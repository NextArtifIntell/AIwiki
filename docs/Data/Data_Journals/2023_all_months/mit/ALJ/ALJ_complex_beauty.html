<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ALJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alj---29">ALJ - 29</h2>
<ul>
<li><details>
<summary>
(2023). Reviewers of volume 29. <em>ALJ</em>, <em>29</em>(4), 512.
(<a href="https://doi.org/10.1162/artl_e_00419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Takaya AritaMartina BacaroChloe BarnesPeter J. BentleyWill BrowneSeth BullockPaco CalvoNicolas CambierFelipe CampeloKeeley CrockettPaul DumouchelLukas EsterleErik FrederiksKetika GargJerzy GoreckiWim HordijkTakashi IkegamiWander JagerMarco JanssenGenaro Juarez MartinezStefan Kambiz BehfarTakeshi KanoAsif KhanHagen LehmannPeter LewisAlexander V. MantzarisStephen MarshChristian Müller-SchloerEce C. MutluYair NeumanEvangelos PournarasSimon PowersDragos RaduAmirarsalan RajabiSebastian RisiJuan C. RochaChristopher RoseTianqi SongPietro SperoniAndreas SpitzPasquale StanoWalter StecheleNathaniel VirgoSebastian von MammenNing WangEdward WeinbergerMeike WillOlaf WitkowskiCharley WuHector Zenil},
  archive      = {J_ALJ},
  doi          = {10.1162/artl_e_00419},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {512},
  shortjournal = {Artif. Life},
  title        = {Reviewers of volume 29},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Does the field of nature-inspired computing contribute to
achieving lifelike features? <em>ALJ</em>, <em>29</em>(4), 487–511. (<a
href="https://doi.org/10.1162/artl_a_00407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main idea behind artificial intelligence was simple: what if we study living systems to develop new, practical computing systems that possess “lifelike” properties? And that’s exactly how evolutionary computing emerged. Researchers came up with ideas inspired by the principles of evolution to develop intelligent methods to tackle hard problems. The efficacy of these methods made researchers seek inspiration in living organisms and systems and extend the evolutionary concept to other nature-inspired ideas. In recent years, nature-inspired computing has exhibited an exponential increase in the number of algorithms that are presented each year. Authors claim that they are inspired by a behavior found in nature to come up with a lifelike algorithm. However, the mathematical background does not match the behavior in the majority of these cases. Thus the question is, do all nature-inspired algorithms remain lifelike? Also, are there any ideas included that contribute to computing? This study aims to (a) present some nature-inspired methods that contribute to achieving lifelike features of computing systems and (b) discuss if there is any need for new lifelike features.},
  archive      = {J_ALJ},
  author       = {Tzanetos, Alexandros},
  doi          = {10.1162/artl_a_00407},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {487-511},
  shortjournal = {Artif. Life},
  title        = {Does the field of nature-inspired computing contribute to achieving lifelike features?},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessing model requirements for explainable AI: A template
and exemplary case study. <em>ALJ</em>, <em>29</em>(4), 468–486. (<a
href="https://doi.org/10.1162/artl_a_00414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sociotechnical settings, human operators are increasingly assisted by decision support systems. By employing such systems, important properties of sociotechnical systems, such as self-adaptation and self-optimization, are expected to improve further. To be accepted by and engage efficiently with operators, decision support systems need to be able to provide explanations regarding the reasoning behind specific decisions. In this article, we propose the use of learning classifier systems (LCSs), a family of rule-based machine learning methods, to facilitate and highlight techniques to improve transparent decision-making. Furthermore, we present a novel approach to assessing application-specific explainability needs for the design of LCS models. For this, we propose an application-independent template of seven questions. We demonstrate the approach’s use in an interview-based case study for a manufacturing scenario. We find that the answers received do yield useful insights for a well-designed LCS model and requirements for stakeholders to engage actively with an intelligent agent.},
  archive      = {J_ALJ},
  author       = {Heider, Michael and Stegherr, Helena and Nordsieck, Richard and Hähner, Jörg},
  doi          = {10.1162/artl_a_00414},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {468-486},
  shortjournal = {Artif. Life},
  title        = {Assessing model requirements for explainable AI: A template and exemplary case study},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial collective intelligence engineering: A survey of
concepts and perspectives. <em>ALJ</em>, <em>29</em>(4), 433–467. (<a
href="https://doi.org/10.1162/artl_a_00408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collectiveness is an important property of many systems—both natural and artificial. By exploiting a large number of individuals, it is often possible to produce effects that go far beyond the capabilities of the smartest individuals or even to produce intelligent collective behavior out of not-so-intelligent individuals. Indeed, collective intelligence, namely, the capability of a group to act collectively in a seemingly intelligent way, is increasingly often a design goal of engineered computational systems—motivated by recent technoscientific trends like the Internet of Things, swarm robotics, and crowd computing, to name only a few. For several years, the collective intelligence observed in natural and artificial systems has served as a source of inspiration for engineering ideas, models, and mechanisms. Today, artificial and computational collective intelligence are recognized research topics, spanning various techniques, kinds of target systems, and application domains. However, there is still a lot of fragmentation in the research panorama of the topic within computer science, and the verticality of most communities and contributions makes it difficult to extract the core underlying ideas and frames of reference. The challenge is to identify, place in a common structure, and ultimately connect the different areas and methods addressing intelligent collectives. To address this gap, this article considers a set of broad scoping questions providing a map of collective intelligence research, mostly by the point of view of computer scientists and engineers. Accordingly, it covers preliminary notions, fundamental concepts, and the main research perspectives, identifying opportunities and challenges for researchers on artificial and computational collective intelligence engineering.},
  archive      = {J_ALJ},
  author       = {Casadei, Roberto},
  doi          = {10.1162/artl_a_00408},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {433-467},
  shortjournal = {Artif. Life},
  title        = {Artificial collective intelligence engineering: A survey of concepts and perspectives},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lessons from the evolutionary computation bestiary.
<em>ALJ</em>, <em>29</em>(4), 421–432. (<a
href="https://doi.org/10.1162/artl_a_00402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of metaheuristics has a long history of finding inspiration in natural systems, starting from evolution strategies, genetic algorithms, and ant colony optimization in the second half of the 20th century. In the last decades, however, the field has experienced an explosion of metaphor-centered methods claiming to be inspired by increasingly absurd natural (and even supernatural) phenomena—several different types of birds, mammals, fish and invertebrates, soccer and volleyball, reincarnation, zombies, and gods. Although metaphors can be powerful inspiration tools, the emergence of hundreds of barely discernible algorithmic variants under different labels and nomenclatures has been counterproductive to the scientific progress of the field, as it neither improves our ability to understand and simulate biological systems nor contributes generalizable knowledge or design principles for global optimization approaches. In this article we discuss some of the possible causes of this trend, its negative consequences for the field, and some efforts aimed at moving the area of metaheuristics toward a better balance between inspiration and scientific soundness.},
  archive      = {J_ALJ},
  author       = {Campelo, Felipe and Aranha, Claus},
  doi          = {10.1162/artl_a_00402},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {421-432},
  shortjournal = {Artif. Life},
  title        = {Lessons from the evolutionary computation bestiary},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The evolution of conformity, malleability, and influence in
simulated online agents. <em>ALJ</em>, <em>29</em>(4), 394–420. (<a
href="https://doi.org/10.1162/artl_a_00413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of artificial intelligence (AI) tools that filter the information given to internet users, such as recommender systems and diverse personalizers , may be creating troubling long-term side effects to the obvious short-term conveniences. Many worry that these automated influencers can subtly and unwittingly nudge individuals toward conformity, thereby (somewhat paradoxically) restricting the choices of each agent and/or the population as a whole. In its various guises, this problem has labels such as filter bubble , echo chamber , and personalization polarization . One key danger of diversity reduction is that it plays into the hands of a cadre of self-interested online actors who can leverage conformity to more easily predict and then control users’ sentiments and behaviors, often in the direction of increased conformity and even greater ease of control. This emerging positive feedback loop and the compliance that fuels it are the focal points of this article, which presents several simple, abstract, agent-based models of both peer-to-peer and AI-to-user influence. One of these AI systems functions as a collaborative filter, whereas the other represents an actor the influential power of which derives directly from its ability to predict user behavior. Many versions of the model, with assorted parameter settings, display emergent polarization or universal convergence, but collaborative filtering exerts a weaker homogenizing force than expected. In addition, the combination of basic agents and a self-interested AI predictor yields an emergent positive feedback that can drive the agent population to complete conformity.},
  archive      = {J_ALJ},
  author       = {Downing, Keith L.},
  doi          = {10.1162/artl_a_00413},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {394-420},
  shortjournal = {Artif. Life},
  title        = {The evolution of conformity, malleability, and influence in simulated online agents},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on lifelike computing systems. <em>ALJ</em>,
<em>29</em>(4), 390–393. (<a
href="https://doi.org/10.1162/artl_e_00425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological systems have been a part of human life since prehistory. Although they initially took the form of passive tools, such as axes and spoons, the Industrial Revolution saw the advent of powered, mechanized technology, operating “under it’s own steam,” without direct human control over every action. By integrating more complex information processing machinery, automation evolved into autonomy as decision-making and self-regulation became features of modern technology. Now, so-called intelligent systems, embodying techniques from the field of artificial intelligence (AI), are designed with the explicit intention of replicating rational behaviors and the sorts of things that minds do, inside technological systems.At the same time, the study of Artificial Life (ALife) (Langton, 1987) has explored the properties of living systems, both as they are found in nature, as they might be, and as humans can build them. This has exposed a large variety of mechanisms that produce qualities typically associated with life. Examples include self-organization, homeostasis, self-replication, evolution, learning, self-awareness, and many others besides.The Lifelike Computing Systems initiative (Stein et al., 2021b) aims to learn from the study of life and living systems to develop new, useful, “lifelike” systems; a further aim is to identify when such features are of value. The focus of this research direction is primarily on engineered technological systems broadly within the domain of computing.The notion of “lifelike computing” is not intended to separate itself from or replace previous initiatives; in a large number of cases, there are already technologies and research efforts that strongly lean toward lifelike computing systems in specific aspects. Building on a long and highly successful tradition in biologically inspired computing, the “lifelike” vision not only seeks inspiration in the living world but also seeks to replicate its qualities explicitly in technological systems. Indeed, we cannot claim that all bio-inspired systems remain lifelike, nor is this in general even always a desirable outcome for those designing bio-inspired systems. The agenda also goes beyond fundamental ALife research, often rightly exploratory in nature, because it focuses explicitly on building purposeful and reliable technological systems for people, based on ALife principles. Therefore the vision of explicit replication of lifelike qualities in technological systems of value to humanity marks a sharpening of focus.This special issue is a follow-up to the workshop series “Lifelike Computing Systems,” held at the International Conference on Artificial Life in 2020 and 2021 (Stein et al., 2021a), and again in 2022 (Stein et al., 2023). The workshop series hosted diverse talks showcasing early-stage research and work in progress, with topics ranging from plasticity in technical systems to artificial DNA, from self-explaining systems to realistic humanoid and animal robots.We have therefore solicited papers that explore and contribute to the discussion on research questions we deem key to be further explored: Which qualities of life are of high relevance and benefit for the engineering of lifelike computing systems useful to people? Why? How?How can we integrate and combine insights and methodological approaches from existing, related research initiatives, such as cybernetics, self-aware computing, organic computing, and autonomic computing?Which methods from domains like artificial life, bio-inspired computing, artificial intelligence, and self-adaptive and self-organizing systems contribute to achieving lifelike features of computing systems?When is more “lifelike” technology appropriate? What are the challenges associated with embedding technology that is more “lifelike” in society? How can these be tackled?This special issue represents an opportunity for more mature work emerging from this line of research to be presented. It contains four papers that together provide a review, analysis, and critique of the integration of lifelike properties into engineered systems, in many cases proposing concrete recommendations for future research directions and methods.In “Lessons from the Evolutionary Computation Bestiary,” Campelo and Aranha explore and critique the explosion of metaphor-centered metaheuristic methods that have been published in recent years and that claim to be inspired by—in their view—increasingly absurd natural phenomena. Examples surveyed include several different types of birds, mammals, fish, and invertebrates; soccer and volleyball; and even reincarnation, zombies, and gods. The authors acknowledge that metaphors can be powerful inspiration and explanatory tools and that, indeed, the field of metaheuristics has a long history of finding inspiration in natural systems, starting from evolution strategies, genetic algorithms, and ant colony optimization. However, they question the value of the emergence of hundreds of highly similar variants of essentially the same algorithm under different labels. The authors have curated a “bestiary” of such variants over the years, and their article in this issue reviews this, arguing that this proliferation has been counterproductive to scientific progress in the field. They argue that it does little to improve our ability to understand and simulate biological systems and that it can actively impede an improved understanding of how to design and analyze global optimization techniques. The article discusses why this social phenomenon in research may have occurred in recent years and its negative consequences, ending with a call to improve the scientific soundness of metaheuristic research.In “Does the Field of Nature-Inspired Computing Contribute to Achieving Lifelike Features?,” Tzanetos asks whether all nature-inspired algorithms remain lifelike. The article considers the history of evolutionary computation and, as in the first article, the proliferation of many so-called nature-inspired techniques in recent years. The author juxtaposes the value of such techniques in solving hard problems with an analysis to support an argument that the mathematics of these techniques often does not match the source behavior faithfully. In these cases, can it be said that the algorithms are indeed “lifelike,” and if not, does that matter, so long as they provide value in terms of their ability to solve problems intelligently? The article argues that historically, there was greater alignment between the algorithmic models and source behaviors, but this is often not seen in more recent attempts. The article ends by discussing if there is a need for new lifelike features of algorithms, concluding that this is not helpful—instead presenting recommendations for future research in nature-inspired computing, which, the authors argue, would move the field in “the right direction.”In “Assessing Model Requirements for Explainable AI: A Template and Exemplary Case Study,” Heider et al. explore the explainability of decision support systems that use evolutionary rule-based machine learning techniques, more precisely, learning classifier systems (LCSs). Self-adaptive and self-optimizing systems are necessarily dynamic, yet for them to be accepted by people in sociotechnical settings, explanations for machine-made decisions are often essential. The authors argue that rule-based machine learning models, such as LCSs, present an opportunity for transparent machine learning models that naturally support access to explanations. To assist with designing and evaluating such models, they also propose a generic and thus broadly applicable questionnaire template. The template is demonstrated to provide valuable insights for the design of such LCS models in specific scenarios. The approach is illustrated in a manufacturing case study.Finally, in “Artificial Collective Intelligence Engineering: A Survey of Concepts and Perspectives,” Casadei surveys computational techniques based on or harnessing “collectiveness,” often seen in many living systems, to produce capabilities beyond what can be achieved with individual or monolithic systems. A key concept common to these techniques is that such systems can exploit a large number of individuals to produce intelligent collective behavior out of not-so-intelligent components. The article argues that there is a trend in some areas of engineering toward this way of designing technological systems, citing examples such as the Internet of Things, swarm robotics, and crowd computing and emphasizing that these technologies span many techniques, systems, and application areas. An essential finding of the review is that there is substantial fragmentation of this research, however, and that the so-called “verticality” of research communities makes a common fundamental understanding of such systems challenging to achieve. The author argues that an important challenge is identifying, placing in a common structure, and ultimately connecting the different areas and methods addressing intelligent collectives. As such, the article presents a set of questions aimed at mapping out collective intelligence research. It uses this to develop a set of preliminary notions, concepts, and perspectives, as well as associated research opportunities, to develop a more fundamental understanding of computational collective intelligence engineering.The guest editors thank the authors of papers submitted to the “Lifelike Computing Systems” special issue as well as the reviewers, who gave valuable feedback to all the authors. We would also like to thank the organizers of the ALife conferences that hosted the Lifelike Computing Systems workshops as well as all the speakers and participants who contributed to many vibrant debates that informed the direction of the final set of articles in this issue. Last, we thank the Board of Editors of Artificial Life for supporting this special issue.},
  archive      = {J_ALJ},
  author       = {Stein, Anthony and Tomforde, Sven and Botev, Jean and Lewis, Peter R.},
  doi          = {10.1162/artl_e_00425},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {390-393},
  shortjournal = {Artif. Life},
  title        = {Special issue on lifelike computing systems},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Editorial: A word from the editors. <em>ALJ</em>,
<em>29</em>(4), 389. (<a
href="https://doi.org/10.1162/artl_e_00422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We start this issue with a research article by Keith L. Downing, “The Evolution of Conformity, Malleability, and Influence in Simulated Online Agents.” The author uses an agent-based model to investigate the potential of positive feedback in personalized recommender systems for driving a population to polarization and conformity.Following this, we include a series of articles forming a special issue from the LIFELIKE Computing Systems Workshop, held at the Artificial Life conference in Prague/Online, 2021. The workshop was co-organized by Peter Lewis, Anthony Stein, Jean Botev, and Sven Tomforde. Lewis serves here as guest editor of the special issue.We close this brief editorial to issue 29:4 by noting that our next issue will be the first of volume 30. We will be celebrating 30 years of Artificial Life by hosting several special anniversary articles giving personal perspectives on the history, development, and future of our subject. We are looking forward to publishing an amazing set of pieces.},
  archive      = {J_ALJ},
  author       = {Dorin, Alan and Stepney, Susan},
  doi          = {10.1162/artl_e_00422},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {389},
  shortjournal = {Artif. Life},
  title        = {Editorial: A word from the editors},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Explorative synthetic biology in AI: Criteria of relevance
and a taxonomy for synthetic models of living and cognitive processes.
<em>ALJ</em>, <em>29</em>(3), 367–387. (<a
href="https://doi.org/10.1162/artl_a_00411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article tackles the topic of the special issue “Biology in AI: New Frontiers in Hardware, Software and Wetware Modeling of Cognition” in two ways. It addresses the problem of the relevance of hardware, software, and wetware models for the scientific understanding of biological cognition, and it clarifies the contributions that synthetic biology, construed as the synthetic exploration of cognition, can offer to artificial intelligence (AI). The research work proposed in this article is based on the idea that the relevance of hardware, software, and wetware models of biological and cognitive processes—that is, the concrete contribution that these models can make to the scientific understanding of life and cognition—is still unclear, mainly because of the lack of explicit criteria to assess in what ways synthetic models can support the experimental exploration of biological and cognitive phenomena. Our article draws on elements from cybernetic and autopoietic epistemology to define a framework of reference, for the synthetic study of life and cognition, capable of generating a set of assessment criteria and a classification of forms of relevance, for synthetic models, able to overcome the sterile, traditional polarization of their evaluation between mere imitation and full reproduction of the target processes. On the basis of these tools, we tentatively map the forms of relevance characterizing wetware models of living and cognitive processes that synthetic biology can produce and outline a programmatic direction for the development of “organizationally relevant approaches” applying synthetic biology techniques to the investigative field of (embodied) AI.},
  archive      = {J_ALJ},
  author       = {Damiano, Luisa and Stano, Pasquale},
  doi          = {10.1162/artl_a_00411},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {367-387},
  shortjournal = {Artif. Life},
  title        = {Explorative synthetic biology in AI: Criteria of relevance and a taxonomy for synthetic models of living and cognitive processes},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding social robots: Attribution of intentional
agency to artificial and biological bodies. <em>ALJ</em>,
<em>29</em>(3), 351–366. (<a
href="https://doi.org/10.1162/artl_a_00404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much research in robotic artificial intelligence (AI) and Artificial Life has focused on autonomous agents as an embodied and situated approach to AI. Such systems are commonly viewed as overcoming many of the philosophical problems associated with traditional computationalist AI and cognitive science, such as the grounding problem (Harnad) or the lack of intentionality (Searle), because they have the physical and sensorimotor grounding that traditional AI was argued to lack. Robot lawn mowers and self-driving cars, for example, more or less reliably avoid obstacles, approach charging stations, and so on—and therefore might be considered to have some form of artificial intentionality or intentional directedness. It should be noted, though, that the fact that robots share physical environments with people does not necessarily mean that they are situated in the same perceptual and social world as humans. For people encountering socially interactive systems, such as social robots or automated vehicles, this poses the nontrivial challenge to interpret them as intentional agents to understand and anticipate their behavior but also to keep in mind that the intentionality of artificial bodies is fundamentally different from their natural counterparts. This requires, on one hand, a “suspension of disbelief ” but, on the other hand, also a capacity for the “suspension of belief.” This dual nature of (attributed) artificial intentionality has been addressed only rather superficially in embodied AI and social robotics research. It is therefore argued that Bourgine and Varela’s notion of Artificial Life as the practice of autonomous systems needs to be complemented with a practice of socially interactive autonomous systems , guided by a better understanding of the differences between artificial and biological bodies and their implications in the context of social interactions between people and technology.},
  archive      = {J_ALJ},
  author       = {Ziemke, Tom},
  doi          = {10.1162/artl_a_00404},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {351-366},
  shortjournal = {Artif. Life},
  title        = {Understanding social robots: Attribution of intentional agency to artificial and biological bodies},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perspectives on computation in plants. <em>ALJ</em>,
<em>29</em>(3), 336–350. (<a
href="https://doi.org/10.1162/artl_a_00396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plants thrive in virtually all natural and human-adapted environments and are becoming popular models for developing robotics systems because of their strategies of morphological and behavioral adaptation. Such adaptation and high plasticity offer new approaches for designing, modeling, and controlling artificial systems acting in unstructured scenarios. At the same time, the development of artifacts based on their working principles reveals how plants promote innovative approaches for preservation and management plans and opens new applications for engineering-driven plant science. Environmentally mediated growth patterns (e.g., tropisms) are clear examples of adaptive behaviors displayed through morphological phenotyping. Plants also create networks with other plants through subterranean roots–fungi symbiosis and use these networks to exchange resources or warning signals. This article discusses the functional behaviors of plants and shows the close similarities with a perceptron-like model that could act as a behavior-based control model in plants. We begin by analyzing communication rules and growth behaviors of plants; we then show how we translated plant behaviors into algorithmic solutions for bioinspired robot controllers; and finally, we discuss how those solutions can be extended to embrace original approaches to networking and robotics control architectures.},
  archive      = {J_ALJ},
  author       = {Del Dottore, Emanuela and Mazzolai, Barbara},
  doi          = {10.1162/artl_a_00396},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {336-350},
  shortjournal = {Artif. Life},
  title        = {Perspectives on computation in plants},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and simulation of a multilayer chemical neural
network that learns via backpropagation. <em>ALJ</em>, <em>29</em>(3),
308–335. (<a href="https://doi.org/10.1162/artl_a_00405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design and implementation of adaptive chemical reaction networks, capable of adjusting their behavior over time in response to experience, is a key goal for the fields of molecular computing and DNA nanotechnology. Mainstream machine learning research offers powerful tools for implementing learning behavior that could one day be realized in a wet chemistry system. Here we develop an abstract chemical reaction network model that implements the backpropagation learning algorithm for a feedforward neural network whose nodes employ the nonlinear “leaky rectified linear unit” transfer function. Our network directly implements the mathematics behind this well-studied learning algorithm, and we demonstrate its capabilities by training the system to learn a linearly inseparable decision surface, specifically, the XOR logic function. We show that this simulation quantitatively follows the definition of the underlying algorithm. To implement this system, we also report ProBioSim, a simulator that enables arbitrary training protocols for simulated chemical reaction networks to be straightforwardly defined using constructs from the host programming language. This work thus provides new insight into the capabilities of learning chemical reaction networks and also develops new computational tools to simulate their behavior, which could be applied in the design and implementations of adaptive artificial life.},
  archive      = {J_ALJ},
  author       = {Lakin, Matthew R.},
  doi          = {10.1162/artl_a_00405},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {308-335},
  shortjournal = {Artif. Life},
  title        = {Design and simulation of a multilayer chemical neural network that learns via backpropagation},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The elements of intelligence. <em>ALJ</em>, <em>29</em>(3),
293–307. (<a href="https://doi.org/10.1162/artl_a_00410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can machines ever be sentient? Could they perceive and feel things, be conscious of their surroundings? What are the prospects of achieving sentience in a machine? What are the dangers associated with such an endeavor, and is it even ethical to embark on such a path to begin with? In the series of articles of this column, I discuss one possible path toward “general intelligence” in machines: to use the process of Darwinian evolution to produce artificial brains that can be grafted onto mobile robotic platforms, with the goal of achieving fully embodied sentient machines.After reviewing the history of Artificial Intelligence research (Adami, 2021) and discussing the components, topology, and optimization methods used in artificial neural network research (Adami, 2022), we now take a step back to ask ourselves, What is intelligence? In our quest to evolve an intelligent system, this is not an idle question. In fact, asking this question will help us focus on essential features of what we call intelligence, rather than being distracted by incidental attributes. Our answer will be guided by the principle that intelligence is an evolutionary response to uncertain environments: that the primary purpose of intelligence is to increase the organism’s fitness.Just as it is unlikely that there will ever be a unique and universal definition of intelligence, it is also unlikely that there will be widespread agreement about what the processes are that contribute to intelligence: the elements of intelligence. The five elements that I will discuss here are rooted in the idea that intelligence is a (biological or computational) trait that enables its bearer to reduce the uncertainty about the world in which it lives (both in time and space) and harness the information it has gained to succeed against its competitors, cooperate with its supporters, and extract the resources it needs from its environment without coming to harm. Recognizing who is friend and who is foe (and using information to defeat the foe and support the friend) ultimately leads to a greater number of offspring.To leverage information in support of organismal fitness, the organism needs to perceive the environment; extract the salient features (those that matter to the organism and can be perceived by its sensory system); make predictions and plans based on the sensed world as well as on what was learned from experience; and, finally, act according to those predictions.1 Such a view of intelligence is very much aligned with the “knowledge-level systems” view of the late 20th century (Anderson, 1983; Newell, 1990), except that those attempts to formulate a “unified theory of cognition” made no attempt to quantify said knowledge in terms of information. An information-theoretic view of intelligence and cognition has the advantage that it can quantify the relation between the “symbols” manipulated by the knowledge system and the things in the physical world that they represent. This is important because, historically, one of the most common criticisms of attempts to formalize (and ultimately engineer) thinking systems conjured up an apparent dichotomy between the “zeros and ones” of computer systems, which are devoid of intrinsic meaning (“strings by themselves can’t have any meaning”; Searle, 1984, p. 31), and the fact that “thoughts are about things.” Information theory quantifies precisely that link, both in computers and in people.Whereas most theories of cognition posit that sensing and acting are integral elements of intelligence because they are clearly part of the “sensory–action loop” (Bongard &amp;amp; Pfeifer, 2001; Clark, 2016; Newell, 1990), here I take the point of view that the sensors and motors themselves are “given” (even though cognition does affect sensing and acting), and I discuss only the elements of intelligence that take place within the neurons of the brain, excluding sensors and motors (often called “peripheral neurons”; McCulloch &amp;amp; Pitts, 1943).2We will see that the elements of intelligence that I will discuss—categorization, memory, prediction, learning, and representation—are all tied explicitly to how information is acquired, shaped, stored, and manipulated.To make sense of the world that we perceive, it is imperative that we can tell one thing from another. How do we do that? How is it that a visual scene (say) evokes in the brain a set of objects and their relationships with each other, rather than a jumbled mishmash of colors and shapes? After all, the shapes and colors we perceive are not discrete but rather form a continuum. For us to be able to differentiate between things, we first need to be able to categorize.Our ability to place objects in the world (and behaviors and ways in which objects relate to behaviors) into categories is a crucial skill that develops early in infancy (Quinn &amp;amp; Eimas, 1997). According to psychologists (see, e.g., Karmiloff-Smith, 1992), categories (collections or classes of objects and events that exist in the world) are formed via a perceptual analysis that filters the raw data, leaving behind an abstract representation in the form of image schemas. We all carry such image schemas with us. If I were to ask you to imagine a chair, for example, you could do that very easily, and even though you might imagine a straight-backed chair with four legs, you would not regard a three-legged stool as something completely outside of this category. In fact, we are able to subsume thousands of different shapes under this one category “chair.” I propose that this ability to form categories is a central element of intelligence: all others that we will discuss build on this one. In particular, building categories allows us to quantify how much there is to know, using the information-theoretic concept of Shannon entropy (Shannon, 1948), a measure of uncertainty.Claude Shannon, the creator of information theory, called his measure entropy because a very similar quantity had been introduced in statistical physics much earlier.3 For the purpose of understanding the concept within the context of cognitive science (and to see its relation with the concept of “information”), I will use the word uncertainty instead of entropy. Shannon defined his uncertainty concept both for continuous (“blurry”) quantities and for discrete (or “sharp”) ones.Let us first write down Shannon’s uncertainty function in the discrete case. To do this, we have to introduce the concept of a variable X that can take on n discrete states xi, i = 1 … n. For the purpose of describing categorization, we might then ask, How do we associate objects in the world that have continuous shapes and colors and features with only one of the n categories defined by variable X? This process (called coarse-graining in the literature; see, e.g., Feynman, 1974), is without a doubt a complex one, involving (as I will describe) a shift from perceptual characters that are described by continuous values to conceptual ones described by discrete values. For discrete categories, Shannon’s (1948) uncertainty function is given byH(X)=−∑i=1np(xi)logp(xi),(1)where p(xi) is the likelihood of encountering an object within category xi in world X. In general, the number of categories (as well as the “distance” between different categories) depends on how useful the differentiation is. For example, in some situations, it might be relevant to make a distinction between two categories (say, “chair“ and “stool”) that is not necessary in others. In other words, the brain tends to operate with just the categories that are necessary to best understand (and predict) the world, given the particular circumstances.How do categories emerge? This is a difficult question to answer, because although they clearly emerge over time via a process of use, feedback, and learning, those processes themselves are somewhat vague. Furthermore, some categories are clearly innate: The fear of the color red in certain birds (Pryke, 2009) is one such example. In a very real sense, categories evolve. Here we will think of the process of categorization as creating a certain number of image schemas that represent the different categories.4 Generally speaking, we can say that categories emerge so that there is a balance between a large enough number of different images to be able to describe the range of salient differences and a small enough number that manipulating these images in one’s head (or wherever they are stored) is not too cumbersome. This emergence of categories is often described by a shift from perceptual representations (images that look very much like the object in question) to conceptual representations that have lost most of the similarities to the actual objects. When representations become so conceptual that there is no resemblance to the original perceptual artifact, such representations are often called symbolic. We should note that even perceptual representations already involve a certain kind of “filtering,” because the sensory system can perceive only a finite range of values. So, for example, whereas the ultraviolet characteristics of flower petals do not enter our perceptual representations of them, they without a doubt do so in hummingbirds (and for all we know, even in mice; see Cronin &amp;amp; Bok, 2016).Perhaps a typical example of the evolution of a perceptual representation from conceptual to a symbolic one is the evolution of the Sumerian cuneiform script. The Sumerian script is the earliest known writing system and is believed to have emerged around 8000 BC (Kramer, 1981). Originally, the “tokens” the Sumerians used were pictograms representing the objects to which they directly refer. Over time, the pictograms became stylized and morphed into symbols, adapting to the method people used to record the symbols, namely, via strokes made by pressing a stylus into clay. A typical sequence that changes the pictogram for “head” into a glyph representing the concept is shown in Figure 1. Note that during the evolution of glyphs, the total number of symbols changes also, usually by eliminating symbols than can be rendered via a combination of others (Kramer, 1981).Although learning about the objects in the world and categorizing them is a necessary precursor to intelligent behavior, it does not specify what action to take to reduce uncertainty. We can say quite generally that what reduces uncertainty is information. Indeed, eons of evolution have stored massive amounts of information about our world in our genome (a very rough estimate suggests about half a billion bits of information5). However, to survive and thrive in a world that changes over a lifetime, we need to store information in a different medium: We need memory.Shannon’s measure for information can be seen as a difference between uncertainties: the uncertainty we have if nothing is known minus our current uncertainty, that is, the remaining uncertainty given all the knowledge we have. So, if we consider again our world X, in which we expect to find objects in categories x1 to xn with probabilities p(xi), then the highest uncertainty would occur if all categories were equally likely in this world: p(xi) = 1/n. In that case, this maximal uncertainty would beHmax=logn.(2)If we know instead that some categories occur with much greater likelihood than others (while some others might even have p(xi) = 0), then this knowledge is quantified by the informationI=Hmax−H(X)=log(n)+∑i=1np(xi)logp(xi).(3)One of the great advantages of information written as a difference of uncertainties is that when discretizing Shannon’s continuous-valued differential entropy to the discrete version, a “renormalization” constant that is related to the level of discretization cancels from the expression of information, telling us that, in the end, only the measurable differences matter.Storing information is crucial to making “informed” decisions. In our brains, information acquired in the past is stored in the connections between neurons and can be retrieved and integrated with the information streaming through our senses. This capacity to remember, and to evaluate the current sensory stream in the context of past experience, is crucial for intelligent behavior. Memory is the mandatory ingredient in two other elements of intelligence I discuss later: learning and representation.Memory allows us to not make the same mistake twice, and it makes it possible that we form models of the world within our brains. But memory, specifically the storage of information, is not something that is automatic. We are so used to information storage in our current world that we often take it for granted. But from the point of view of (classical) physics, information is inherently fragile: The second law of thermodynamics is unforgiving unless specific measures are in place to prevent the deterioration of information.The means by which we store information has changed tremendously over time. In biology, the need to store information through means other than “filing it away” in the organism’s DNA emerged once the world started changing on a timescale faster than an organism’s lifetime. Before the Cambrian explosion, the animal world (mostly concentrated on the seafloor) was fairly predictable: The Ediacaran fauna were mostly sessile, and those that were not roamed the seafloor in idiosyncratic patterns that required no memory (Carbone &amp;amp; Narbonne, 2014). However, once the world became more complex, it was also changing more quickly. To survive in such a world, it is necessary to react promptly to those changes, and it is likely that the spiking neuron evolved precisely for this reason, by coupling sensory information directly to motor activation (Jékely, 2011). But in a world that changes fast, it is also necessary to learn from mistakes. To do this, the brain has to be able to recall prior sequences of events: It needs memory.The simplest form of memory is the preservation (and recall) of the fleeting state of the environment (this is also sometimes called working memory [see Baddeley, 1986], but others simply call it sensory short-term memory [Carruthers, 2014]). For firing neurons, keeping a signal in memory is not a trivial task, because after firing, the neuron returns to its quiescent state, thus erasing the signal. One way to preserve the signal is to stimulate the firing of another neuron, but how can that neuron’s state be preserved? One solution is to arrange for the neuron to stimulate its own state by firing, and in evolutionary experiments with Markov brains,6 this is precisely what emerged (Edlund et al., 2011).More complex processing is required when an organism needs to recall sequences of events (often called episodic memory; Tulving, 2002). The logic of such memory can be understood using the following simple model. Suppose a brain needs to remember a particular temporal sequence of bits. In the simplest case, this is a sequence of two bits; that is, the brain needs to remember (and then recognize) one out of four possible temporal patterns. This can be accomplished by a set of three neurons: a sensory neuron, an actuator neuron, and an intermediary (“hidden”) neuron (see Figure 2). In this simple model, the hidden (intermediary) neuron is necessary to keep time, and a particular logic can be constructed (it will also readily evolve) so that the actuator neuron (the motor unit M) fires if (and only if) a particular temporal sequence is experienced within the sensor S. This logic (shown in Figure 2b) connects the signal S and the state of the hidden unit H at the previous time point to the state of the motor M and the hidden unit at the subsequent time point. This particular logic table implements a simple dynamic: If the input state of S is 0 while H is zero, H remains zero along with M, signaling “no recognition.” Because the sequence to be recognized starts with a 1, so far so good. Indeed, the hidden neuron’s state remains quiescent until a 1 is sensed. If this occurs, the hidden neuron begins to keep time as the pattern 10 makes the hidden neuron fire, while the motor neuron remains quiescent. If the following input is a 1 instead, the table forces a return to the quiescent state 00, which we can interpret as “try again”: The sequence 10 was not detected, and the logic is re-prepared in the initial state to wait for a 1. However, if a 0 did follow that 1 within S, this logic dictates the output pattern 10 within the pair MH: The motor neuron fires to indicate successful recognition of the signal, while the timekeeper is reset to zero to await another sequence.What I have described here is an extremely simple model of time series recognition (it is arguably the simplest), but it can easily be scaled up to handle longer time series. For example, in Figure 3a, we see the logic necessary to recognize a four-bit sequence, which needs two hidden neurons to keep time. The 3-in, 3-out logic gate shown in Figure 3a will recognize the sequence 0001: the binary version of the famous “fate” motif of Beethoven’s Fifth Symphony (if you identify the 0 with G and the 1 with B♭, while ignoring the rhythm of the motif). Logic that recognizes specific time series evolves readily in Markov brains (Hintze et al., 2017). Moreover, a simple reinforcement learning algorithm can quickly change this logic, as I discuss later.Is logic of the sort described here used in actual brains to recognize time series? This is difficult to answer because it is very hard to reconstruct the logic of a set of neurons from the connections and recordings alone. However, given that the average motor neuron in the spinal cord, for example, receives inputs from thousands of other neurons that synapse on the cell body (see Alberts et al., 2002, chapter 11) it is conceivable that the positioning and strength of the synapses on the cell body (which can change with experience) implement precisely such logic.That an accurate prediction of the future is going to be advantageous for any living organism is obvious: There is value in information, in particular, in changing environments (Donaldson-Matasci et al., 2010; Rivoire &amp;amp; Leibler, 2011). The selective pressure to predict the future is intense. For this reason, the brain is often described as a “prediction machine” (Bubic et al., 2010; Clark, 2016; Friston &amp;amp; Kiebel, 2009; Hawkins, 2021; Hawkins &amp;amp; Blakeslee, 2004). But it is not only prediction of the future that is important. The information stored in genes is also used to predict the state of an organism’s environment (for example, to ensure that the right genes are expressed at the right time). Predicting the future state of the environment, however, has several other advantages, for example, it becomes possible to plan ahead. One of the simplest prediction algorithms is not even based on neurons. The chemotaxis pathway that allows bacteria to swim “up” a resource concentration gradient can be viewed as a prediction algorithm that infers the location of the source of the resource from local concentration fluctuations.How do we know that the brain makes predictions? And what are the predictions based on? Rivoire and Leibler (2011) discussed the power of information for predicting changing environments by studying the dynamics of adapting populations (not specifically brains), but their analysis is quite general. Even though they make a number of simplifying assumptions (to make the problem mathematically tractable), the formalism is powerful enough to reveal a fundamental relationship between information (used for prediction) and fitness.Rivoire and Leibler’s model describes agents that make decisions within a changing environment, using the information they inherited (stored in the genome) as well as information acquired from the environment.7 The model implies that if an agent optimally uses the information it has at its disposal, then it maximizes its fitness (measured in terms of the growth rate of the population).Imagine an agent described by a random variable Z that can take on a finite number of states z. The agent can change states over time, so we have to introduce the variable Zt, and the agent’s trajectory over time is described by Z1Z2 ⋯ Zt−1Zt.8 At the same time, we define an environment variable Et that can take on states et. This environment changes over time in an uncorrelated manner,9 that is, the probability that Et takes on state et is P(E = et) = p(et). We now define the fitness function f(zt, et) that represents the expected number of offspring generated by the agent when it is in state zt while the environment is in state et, and we further assume that if the environment is in state et, then there is a particular state zt for which f(zt, et) = f(zt) &amp;gt; 0, while the fitness is zero otherwise. In other words, to be fit, the agent’s state has to “track” the environment. Here we assume that the only way the agent can do this is by sensing the environment. For this, the agent is equipped with a sensor (described by random variable X), and we also assume for simplicity that this sensor is accurate. Consulting this sensor that accurately displays the state of the environment, the agent can change its own state accordingly so that its state always tracks the environment. Rivoire and Leibler showed that if an agent cannot sense the environment (or is incapable of interpreting the sensed value and changing its state accordingly), then the growth rate Γ0 of a population of such agents will beΓ0=〈logf〉−H(E),(4)where 〈log f〉 is the logarithm of the fitness (the logarithm of fitness represents the individual’s growth rate) averaged over the possible states of the environment (the probability distribution p(e) is assumed to be the stationary distribution, that is, the one obtained from p(et) in the long time limit),〈logf〉=∑ep(e)logf(e).(5)H(E), in turn, is the uncertainty that the agent has about the environment, given in terms of the Shannon entropyH(E)=−∑ep(e)logp(e).(6)The interpretation of Equation 4 is straightforward: There is a cost to being blind to the environment’s changes, and the cost in growth rate is exactly equal to the Shannon entropy of the environment.If an agent senses the environment with a perfect sensor X and follows an optimal strategy (a policy that will change the agent’s state according to the sensed value), then the growth rate ΓX will instead beΓX=〈logf〉−H(E∣X),(7)where H(E|X) is the conditional entropy of the environment given the sensed value, that is, the remaining uncertainty after taking the sensor’s state into account. We can write this using the conditional probability p(e|x), which quantifies how likely it is that we will encounter environment e given that we sensed x, asH(E∣X)=−∑e,xp(x)p(e∣x)logp(e∣x).(8)The value of this information for the population (the gain in predictability) is the difference between Equations 7 and 4:ΓX−Γ0=H(E)−H(E∣X)=I(E;X);(9)that is, this value is given precisely by the Shannon information I(E; X) gained by the agent. This information, as I mentioned earlier, is the difference between an unconditional entropy H(E) (what there is to know) and a conditional entropy H(E|X) (what remains to be known once we know X). It quantifies how well an agent can predict the state of E (the environment) armed with the state of X (the sensor).The model (simple as it is) brings the value of prediction into sharp focus: Information can be turned into fitness using an optimal strategy. Evolution, in turn, strives to find that optimal strategy. Multiple experiments evolving artifical brains of agents navigating changing environments (C G et al., 2018; Edlund et al., 2011; Fischer et al., 2020; Hintze et al., 2015; Iliopoulos et al., 2010; Kvam &amp;amp; Hintze, 2018; Marstaller et al., 2013; Olson et al., 2016; Tehrani-Saleh &amp;amp; Adami, 2021; Tehrani-Saleh et al., 2016) have supported that conclusion.Although making good predictions will, on average, increase an organism’s chances for survival, it is also important to have a strategy to deal with situations in which a prediction turned out to be wrong. In general, the algorithm that allows us to change our actions based on environmental feedback is called reinforcement learning (Sutton &amp;amp; Barto, 1998). Clearly, learning is not an intrinsic capacity of an information-processing and -predicting system: This capacity needs to evolve. The literature on learning is vast and spans multiple different disciplines (see, e.g., Gross, 2020). Learning is an important concept in computer science, cognitive science, psychology, animal behavior, and more. Here I will focus on only one specific form of learning: reinforcement learning applied to neural networks. Specifically, I will discuss a very general reinforcement learning algorithm that is powerful yet simple to implement. It is also fast, and arguably close to optimal: the multiplicative weights update algorithm (MWUA; Arora et al., 2012; see also Chastain et al., 2014). This is not a new method: It has been used independently in numerous fields, such as machine learning, decision theory, optimization, and evolutionary game theory. It may very well be an algorithmic description of what is going on in our brains as we learn.In what follows, I describe the Expert Advice Problem: how to (repeatedly) pick out one from a set of n experts and follow that expert’s advice. We can imagine that this expert is giving advice on stock picks, so we are interested in finding the strategy that will maximize our payoff. Given the particular environment (inputs that the expert senses), each expert makes a particular prediction based on whatever information or intuition the expert has. This prediction could be good one day and terrible the next. Every expert is given a weight wi, and at the outset, all of the weights are equal. An algorithm determines which of the experts’ advice will be chosen to pick the stock, after which the weights are changed based on whether the investment led to a gain or a loss. This is the feedback part of the learning algorithm. We would like to know the optimal way to pick an expert based on the weights and how to best change these weights given the feedback. Here we consider only the case in which the likelihood that any particular expert will be picked for advice next time is proportional to the expert’s weight (another option is the “weighted-majority” algorithm, whereby the prediction that has the highest total weight of experts advising it is chosen; Arora et al., 2012).To make it clear from the outset how this relates to artificial brains, we can think of each of the experts as a particular firing pattern of a group of neurons in response to a sensation. Each pattern gives rise to a particular behavior, and the task is to optimize the fitness of the behaving agent given the sensation. In a sense, we are asking, Given a particular sensed pattern (and the behavior it triggers), how should we change the likelihood of each of the possible response patterns given the feedback we receive for the ensuing behavior?For the stock-picking experts, we are looking for the optimal way to adjust the weights given the outcome of the previous pick (a gain or a loss). It turns out that the optimal algorithm is fairly simple (Arora et al., 2012; Chastain et al., 2014). Suppose that at time point t, we pick expert i with a probability proportional to its weight,pi(t)=wi(t)∑inwi(t),(10)and we record a gain gi. If the gain is positive, then we increase the weight of this expert (in a multiplicative manner) by an amount proportional to the gain. If gi is negative, we decrease the weight:wi(t+1)=wi(t)(1+ϵgi(t)),(11)where ϵ is a small constant. We see that if the feedback is positive, then at the next time step this expert will have a higher likelihood of getting picked, while all other experts will be picked with a smaller probability. Translated to brains, we can say that those firing patterns that led to a positive outcome will be reinforced, that is, the likelihood of this pattern to fire given the same sensory input is increased, while other patterns are suppressed. This is reminiscent of the Hebbian learning rule (Hebb, 1949) that reinforces the synaptic connection between neurons if they fired together (and a positive outcome was achieved), only in the MWUA the reinforcement occurs not on the connection between two neurons but rather on the likelihood of a particular firing pattern of a set of (possibly) many neurons together.The rate at which the optimal behavior emerges when accurate feedback is given (the learning rate) depends on the constant ϵ. Although a large ϵ tends to change weights (and therefore firing probabilities) faster, it also makes the algorithm less than optimal. We predict that the expected gain 〈m(t)〉 = ∑i=1npi(t)gi increases quickly given the feedback that discourages calling on patterns that score poorly. Indeed, it is possible to show that the cumulative expected gain 〈G〉, given by the expected gain accumulated over the entire lifetime T, is just a little smaller than the optimal return (Arora et al., 2012):〈G〉=∑t=1T∑i=1npi(t)gi(t)≥(1−ϵ)Gopt−lnnϵ,(12)where n is the number of experts (or the number of possible firing patterns in a particular group of neurons) and Gopt is the optimal gain,Gopt=maxG1,G2,⋯,Gn,(13)which is obtained by choosing the expert who (in hindsight) would have accumulated the largest return. Choosing a large ϵ might make the system learn faster, but the lower bound on the expected return decreases quickly too. Often, the choice ϵ ≈ ln(n)/T is a good trade-off (Chastain et al., 2014).This learning algorithm is easy to implement on Markov brains, as the pi(t) are simply given by the entries in the transition table for the “Markov gates” that we encountered earlier. For example, suppose that the transition table shown in Figure 3b, which, as we recall, implements the simplest algorithm for the recognition of Beethoven’s “fate” motif, has instead probabilities that sum to 1 in each row (a stochastic logic gate). Obviously, random logic (initializing the probabilities with pi = 1/8) does not recognize the motif at all, but we can now “train” this gate using the multiplicative weight update algorithm. Figure 4 shows the result of this training using ϵ = 0.2, and a reward gi = 1 any time the decision neuron was silent when it was supposed to be (or fired when the motif ended), and a penalty of gi = −1 when the decision was wrong. We also changed the rewarded pattern after 1,000 updates to study “reversal learning,” that is, how easily the logic can be retrained to recognize the “opposite” motif (here 1110). Just before the pattern was changed, the probabilities in the logic table were very close to the optimal probabilities shown in Figure 3.Reinforcement learning works well (and is fast) when there is an accurate feedback signal about the quality of the behavior and when the feedback follows the relevant action fairly closely in time. However, unless a reliable indicator is provided to the brain (for example, by a “teacher”), brains have to figure out what environmental variable represents a reliable indicator. It turns out that because reinforcement learning is such a powerful tool, organisms quickly learn which sensory inputs (or combinations of inputs) provide useful feedback. In one such experiment (Sheneman &amp;amp; Hintze, 2017), agents with Markov brains “woke up” every morning with a different wiring of their motor neurons to their motors (the pattern that indicated “forward” the day before now is interpreted as “backward”; what was “left” may now be “right”). Given the task of traversing a plane studded with obstacles, the agents quickly evolved a system in which a few trial actuations followed by checking the resulting movement with sensors allowed the agents to learn the new assignment of actions autonomously, without feedback from a teacher.The reader might ask why this section was titled “Learning and Inference” when we only discussed a particular learning algorithm and did not discuss inference. The answer is that if an agent has learned to associate a particular behavior with a given sensed pattern, we can view this as the agent having inferred the optimal behavior from a series of “measurements,” which are the trial behaviors used to update the weight table for each firing pattern. In the standard (Bayesian) inference loop, a set of prior probabilities is updated (using acquired data) to posteriors, which is precisely what happens in the update step of the MWUA.A “representation,” in the sense of the word coined by cognitive psychologists, is a model of the world that “stands in” for the real world (Clark, 1997). It is an abstracted version of the world stored within the brain that can be used to plan actions in the future without having to resort to the senses. Although some aspects of the world are clearly represented innately (stored in our genes), I will focus exclusively on neural representations here because they influence our behaviors directly, for the most part consciously.It does not take a lot of imagination to realize how important it is to carry around these representations of the world. The vision system, for example, relies on models of what an object might look like, so that we have to check only a few salient aspects of a scene to “fill in the blanks,” so to speak, to come up with an accurate picture (Clark, 2016). (I will return to discussing perception in more detail in the next column.)We can characterize how much a brain knows about the world (including itself) in terms of information theory. If we define a random variable for the world states E (the “environment”) and another variable B for brain states, then what the brain knows about the environment is just the shared entropy I(B; E).10 However, this is not a representation, because the brain states might be directly influenced by what the agent perceives in the sensors (which, in turn, are reflections of the world). In other words, I(B; E) might be nonzero not because the brain is able to model the world but because the brain senses the world. To craft a true measure of representation, we need to keep the sensory states constant, so that they are unable to mimic representations. Specifically, a brain needs to be able to represent the world while there is no variation in the sensory neurons. Such a representation can be quantified by the conditional entropy H(B; E|S), that is, the information that the brain has about the world given the current sensor state (Marstaller et al., 2013). This shared conditional entropy is shown in terms of an entropy Venn diagram in Figure 5. The representation R = H(B; E|S) can be even larger than the information the brain has about the world I(B; E) (given by the sum of H(B; E|S) and I(B; E; S)) because the multi-information I(B; E; S) (see Figure 5) can be negative. This can happen if the sensed symbol unlocks encrypted information (see, e.g., Bohm et al., 2022).Representations are powerful because they are flexible: Learning new relationships between causes and effects allows us to refine our representations, so that our predictions (for example, what we expect to sense next) are more accurate. And indeed, measuring how much a brain represents about the world while it evolves to be more fit shows that representation increases with fitness in lockstep (Marstaller et al., 2013). Thus the representation R = H(B; E|S) could be a powerful objective function for evolution; that is, we can imagine that increasing what we know about the world (given what we sense) will invariably increase our fitness because we can make better predictions.Information-theoretic objective functions for evolving intelligent behavior are not new. Linsker (1989) has described an “infomax principle” based on maximizing the shared entropy between sensors and motors, what we would call I(S; M), where M are motor neurons. A more sophisticated version of this objective function is empowerment, which is a measure of how much one’s actions (encoded in M) have influenced what one will perceive some time in the future (Klyubin et al., 2005). It is also possible to maximize the shared entropy between the sensor’s states at one time point and a subsequent time (the “effective information”), to evolve coordinated behavior. This works to some extent (Martius et al., 2013) but does not give rise to goal-directed behavior. None of these objective functions takes into account internal representations of the world. Phillips and Singer’s (1997) “coherent infomax” principle does take a “context” variable C into account, maximizing I(S; M; C). Because the context here is given by memories of prior events, we can safely say that they play the role of the “brain” variable B. But the problem with coherent infomax is that the coherent information I(S; M; C) can be negative (as pointed out earlier; see the light gray area in Figure 5), and often is (as we see later, when discussing perception). Indeed, it is hard to imagine that a maximization procedure has led to a negative uncertainty. Thus the representation R = H(B; E|S) is still the best objective function that takes internal models into account, and indeed, using an evolutionary algorithm (see, e.g., Adami, 2022, Figure 3) in which representation is maximized as a second objective (besides attempting to maximize task performance) works better than any other method tried so far (Schossau et al., 2016).Intelligence, according to the point of view I am discussing, cannot be condensed into a single number; rather, it is built from multiple elements that work together, build on each other, and have different impacts, depending on the situation we encounter. I have described five elements of intelligence here, but it is clear that the number is arbitrary, and there is significant overlap between these elements. Furthermore, other important elements (such as planning) have been ignored. One could argue that planning is part of prediction, but although there is overlap, we can also argue that making a prediction does not tell us how we should use stored representations to successfully execute future behavior that maximizes fitness. But because striving for completeness in an exposition of the elements of intelligence would be both futile and inappropriate for a column, I have limited myself to these five elements: categorization, memory, prediction, learning, and representation. In the way I have described them, all these elements are rooted in the concept of information, so we can safely say that intelligent behavior is information-driven behavior. Categorization makes memory possible (and allows us to know how much we do not know), memory makes information persistent, prediction uses information to make better decisions in changing worlds, learning allows us to change our behavior if the outcome was detrimental, and representations encode information about the world in our brains so that we can make decisions based on an understanding of the world. Of course, we use these representations also to plan future behavior.Although a focus on the information-theoretic underpinnings of the elements of intelligence is satisfying from a fundamental point of view, it does not immediately shed light on particular algorithms that our brains use for day-to-day activities. It may be true that the fundamental algorithm to remember and recognize time series, described earlier, is used in mammalian brains. It may very well be the case that learning in biological brains fundamentally follows the MWUA rules. In either case, we do not have good experimental evidence for or against such a notion, and much more work is needed. Much more is known about how brains perceive sensory signals, such as visual, tactile, olfactory, or auditory signals. In the next installment, I will discuss some fundamentals of perception and compare evolved solutions to a perception task to how humans perceive time, in simple, dedicated experiments.I am grateful to a large number of students, colleagues, and collaborators who have helped shape my views on the evolution of intelligence, in particular, Larissa Albantakis, Cliff Bohm, Nitash C G, Fred Dyer, Jeffrey Edlund, Jeff Hawkins, Arend Hintze, Doug Kirkpatrick, Christof Koch, Lars Marstaller, Devin McAuley, Charles Ofria, Randal Olson, Anselmo Pontes, Vincent Ragusa, Jory Schossau, Ali Tehrani, and Giulio Tononi. Of those, I am in particular indebted to Arend Hintze, who created Markov brains along with me and independently “discovered” the MWU algorithm (and generated a version of Figure 4) as early as 2009, for an ill-fated grant application to DARPA’s “Physical Intelligence” program. We can only muse about where we would be today if that application had been funded. The actual Figure 4 in this article was generated by Jory Schossau. I also acknowledge the editors in chief of the Artificial Life journal, who have donated their time to read—and expertly criticize—these column articles. Their care has helped shape these articles and eliminated many misconceptions, while all remaining errors are my own.},
  archive      = {J_ALJ},
  author       = {Adami, Christoph},
  doi          = {10.1162/artl_a_00410},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {293-307},
  shortjournal = {Artif. Life},
  title        = {The elements of intelligence},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Biology in AI: New frontiers in hardware, software, and
wetware modeling of cognition. <em>ALJ</em>, <em>29</em>(3), 289–292.
(<a href="https://doi.org/10.1162/artl_e_00412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposal for this special issue was inspired by the main themes around which we organize a series of satellite workshops at Artificial Life conferences (including some of the latest European Conferences on Artificial Life), the title of which is “SB-AI: What can Synthetic Biology (SB) offer to Artificial Intelligence (AI)?” The workshop themes are part of a larger scenario in which we are interested and which we intend to develop. This scenario includes the entire taxonomy of new research frontiers generated within AI, based on the construction and experimental exploration of software, hardware, wetware, and mixed synthetic models to deepen the scientific understanding of biological cognition.Since the early 1990s, basic research in AI, broadly conceived as the study of cognition and intelligence through artificial models, has been developed based on an increasing interest in insights provided by the sciences of life on natural cognitive structures, mechanisms, and processes. The growing importance of biology in AI research relies on recent paradigmatic transformations within this field, which, being enhanced by scientific and technological progress, are generating new research frameworks and directions.The emergence of the “embodied approach” in the cognitive sciences and AI, beginning in the late 1980s, has been increasingly focusing the investigations of this field on the role(s) played by the body’s self-production and interactions in cognition. As we argued in previous works on SB-AI (Damiano &amp;amp; Stano, 2018, 2021) this reintegration of the biological body, directed toward overcoming the Cartesian dichotomy characterizing the classical cognitive sciences and AI, appears to be only partially accomplished through the combination of synthetic hardware and software models—which are traditional in AI—and can be enriched by new approaches, based on the emerging wetware form of the synthetic modeling. Until now the latter has not received sufficient attention as a way of expanding AI research from the embodied perspective. On this basis, a potentially impactful scenario can be developed, through the complementary, critical integration—at both the experimental and the conceptual levels—of the three branches of the synthetic modeling mentioned, that is, hardware, software, and wetware synthetic modeling of life and cognition.By wetware models of biological and cognitive processes we refer to chemical models developed by SB, the nascent field constructing and studying biochemical artificial systems that display features of (minimal) life and (minimal) cognition. Whereas industrial SB aims at constructing parts, systems and/or processes to modify extant biological cells for practical applications, knowledge-oriented SB focuses on constructing artificial systems of various types, to generate, from the bottom up, living and cognitive patterns. As we emphasized in the works we dedicated to SB-AI, the constitution of this new field offers AI a new way of exploring the role the body plays in cognitive processes and converges with computer science and robotics in developing frontier programs in AI (in particular, in embodied AI) aiming at extending its research within the domain of synthetic experimental biology.The theme of this special issue, “Biology in AI: New Frontiers in Hardware, Software, and Wetware Modeling of Cognition,” can be conceived as collecting frontier research topics introduced to stimulate interdisciplinary avant-garde programs in AI involving the whole spectrum of the (hardware, software, wetware, and mixed) synthetic modeling of life and cognition. Some of the most fascinating and potentially productive research subjects connected to the theme of this special issue, are defined by the following questions:Considering SB’s current state of development, can it productively explore cognition and intelligence through the construction and experimental exploration of synthetic biological systems and processes? If yes, how? Under which conditions could this exploration positively contribute to AI research? That is, what can SB offer to AI?What are the groundings, procedures, expected results and impacts of current (or currently realizable) research programs involving SB in AI research? What novelties are they introducing into both the classical and the embodied approach to AI? What are its possibilities of enriching the traditional hardware and software synthetic exploration of cognitive processes?Can we, at the present time, plan concrete collaborations between computer science, robotics, and SB in the scientific study of natural forms of cognition and intelligence? How?What aspects of life and biological cognition can hardware, software, and wetware models, in their specificities and complementary combinations, allow us to explore scientifically and to better understand?What kind of synthetic models can be used to investigate, by means of specific models, relevant aspects of life and cognition genuinely emerging “from the bottom”?Are there innovative synthetic models for investigating (minimal) living and (minimal) cognitive processes, capable of drawing the lines of new theories or experimental approaches for the scientific characterization of life and cognition?Are there approaches able to trigger the transition from purely imitative AI to a form of AI focusing not only on behaviors, movements, and anatomic structures of biological systems but also on their organization?The articles selected for this special issue attempt to offer answers to some of these questions and to develop research directions for “Biology in AI.”The exploration of computational capabilities of chemical reaction networks is a research arena that combines AI and purely chemical systems well and aims at embodying computation into chemical transformations. Its implications are increasingly relevant when minimal forms of life are considered, for example, forms of life that potentially can be built by SB, to plan the construction of responsive and adaptive cognitive systems via a chemical perspective. In his article, Matthew R. Lakin presents an abstract chemical reaction network model that implements the backpropagation learning algorithm for a feedforward neural network whose nodes employ the nonlinear “leaky rectified linear unit” transfer function. The network directly implements the mathematics behind this well-studied learning algorithm, and the author demonstrates that it is possible to train the system to learn a linearly inseparable decision surface, specifically, the XOR logic function. The article, “Design and Simulation of a Multilayer Chemical Neural Network That Learns via Backpropagation” provides insights into chemical reaction networks as wetware machines capable of learning. While experimental biochemical strategies are called to action, and consider these targets as very relevant, yet challenging, research questions, computational approaches like the one developed in this article pave the way to focused projects and ultimately support embodied AI progressions.The article by Emanuela Del Dottore and Barbara Mazzolai, “Perspectives on Computation in Plants,” focuses on an emerging branch of biorobotics that applies the cybernetic paradigm of transdisciplinary circuits to the project of establishing a bidirectional exchange of knowledge between the scientific study of plants and the robotic construction of artifacts. On one hand, the research direction the authors present is engaged in developing robotic systems that model plant processes of morphological and behavioral adaptation to effectively perform in unstructured scenarios. On the other hand, this research line uses the robotic systems it develops as scientific tools, that is, material models whose experimental exploration can deepen the scientific understanding of plants. The ambition is to create a plant science that, based on this generative circuit of exchanges centered on plants, is able to promote both original approaches for preservation and management plans and innovative developments in science and engineering. The article offers a paradigmatic example of this kind of circuit by describing the role that the scientific exploration of functional behaviors of plants—in particular, their growth and communication behaviors—can play in developing algorithmic solutions for bioinspired robot controllers and, furthermore, in generating innovative approaches to robotic control architectures and networking.The article “Understanding Social Robots: Attribution of Intentional Agency to Artificial and Biological Bodies” by Tom Ziemke offers an interesting contribution to the philosophical dimension of embodied AI and Artificial Life by focusing on the specific problem of intentionality posed by socially interactive autonomous systems, for example, (some) social robots, automated vehicles, and behavioral robotized objects. With regard to these systems, Ziemke addresses what he calls the issue of the “dual nature of (attributed) artificial intentionality,” or the challenge of intentionality that they impose on their human users. On one hand, these systems require their users to interpret them as intentional agents to understand and anticipate their behavior, but on the other hand, to effectively interact with them, their users have to remember that these agents’ intentionality, being grounded in artificial bodies, is fundamentally different from human intentionality. Ziemke’s theoretical development of this issue generates a compelling invitation for the community of Artificial Life that proposes to develop the “practice of autonomous systems,” originally brought forward by Francisco Varela and Paul Bourgine (1992) as the core of Artificial Life, by introducing of a “practice of socially interactive autonomous systems.” This new practice is oriented by a deeper scientific understanding of the differences between artificial and biological bodies and the implications that these differences entail in the context of human–technology social interactions.In the final article, “Explorative Synthetic Biology in AI: Criteria of Relevance and a Taxonomy for Synthetic Models of Living and Cognitive Processes,” we address the topic of Biology in AI in two ways. First, we tackle the issue of the relevance of hardware, software, and wetware models, broadly construed as the problem of determining the concrete contribution that these synthetic models can make to the scientific understanding of life and cognition. Elements from cybernetic epistemologies and the epistemological framework of autopoiesis are used to introduce two assessment criteria for synthetic models, whose combination generates a wide, articulated space of possible forms of relevance for these models, located between the terms of the traditional alternative mere imitation vs. full reproduction of the target processes. In the article this space is defined through a detailed taxonomy of different forms of relevance that synthetic (hardware, software, and wetware) models can express in exploring biological and cognitive processes, with particular attention to biological cognition. Second, on the basis of this framework, we propose a tentative map of the forms of relevance characterizing wetware models of living and cognitive processes produced by SB today and define a programmatic direction for the development of “organizationally relevant approaches” applying SB techniques to the field of (embodied) AI.Through these articles, the special issue Biology in AI promotes original directions on AI within and through the scenario of the synthetic modeling of life and cognition. The hope is that, as stepping stones, the articles that we collected in this special issue can inspire and influence novel ideas, debates, collaborations, studies, and research domains across the scientific communities converging into Artificial Life.In addition to the themed articles discussed, this special issue includes the third installment of Chris Adami’s column “The Evolutionary Path to Sentient Machines.” The overall topic is on evolving intelligence. In this installment, Adami discusses five elements of intelligence in terms of information and its acquisition, storage, and manipulation.We acknowledge the productive collaboration of the Artificial Life co-editors-in-chief Susan Stepney and Alan Dorin, and the precious, patient, and supportive help of Lin Reedijk of the editorial office. This is also an opportunity to thank the authors and reviewers of the published articles, as well as all contributors we have hosted as speakers in the SB-AI workshop series over the years. The scientific collaboration, as workshop co-organizer, of Yutetsu Kuruma (Japan Agency for Marine-Earth Science and Technology, Yokosuka) and, as critical commenter and discussant, of Hagen Lehmann (University of Bergamo) is also greatly appreciated.},
  archive      = {J_ALJ},
  author       = {Damiano, Luisa and Stano, Pasquale},
  doi          = {10.1162/artl_e_00412},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {289-292},
  shortjournal = {Artif. Life},
  title        = {Biology in AI: New frontiers in hardware, software, and wetware modeling of cognition},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ansatz for computational undecidability in RNA automata.
<em>ALJ</em>, <em>29</em>(2), 261–288. (<a
href="https://doi.org/10.1162/artl_a_00370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this ansatz we consider theoretical constructions of RNA polymers into automata, a form of computational structure. The bases for transitions in our automata are plausible RNA enzymes that may perform ligation or cleavage. Limited to these operations, we construct RNA automata of increasing complexity; from the Finite Automaton (RNA-FA) to the Turing machine equivalent 2-stack PDA (RNA-2PDA) and the universal RNA-UPDA. For each automaton we show how the enzymatic reactions match the logical operations of the RNA automaton. A critical theme of the ansatz is the self-reference in RNA automata configurations that exploits the program-data duality but results in computational undecidability. We describe how computational undecidability is exemplified in the self-referential Liar paradox that places a boundary on a logical system, and by construction, any RNA automata. We argue that an expansion of the evolutionary space for RNA-2PDA automata can be interpreted as a hierarchical resolution of computational undecidability by a meta-system (akin to Turing’s oracle), in a continual process analogous to Turing’s ordinal logics and Post’s extensible recursively generated logics. On this basis, we put forward the hypothesis that the resolution of undecidable configurations in RNA automata represent a novelty generation mechanism and propose avenues for future investigation of biological automata.},
  archive      = {J_ALJ},
  author       = {Svahn, Adam J. and Prokopenko, Mikhail},
  doi          = {10.1162/artl_a_00370},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {261-288},
  shortjournal = {Artif. Life},
  title        = {An ansatz for computational undecidability in RNA automata},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DigiHive: Artificial chemistry environment for modeling of
self-organization phenomena. <em>ALJ</em>, <em>29</em>(2), 235–260. (<a
href="https://doi.org/10.1162/artl_a_00398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents the DigiHive system, an artificial chemistry simulation environment, and the results of preliminary simulation experiments leading toward building a self-replicating system resembling a living cell. The two-dimensional environment is populated by particles that can bond together and form complexes of particles. Some complexes can recognize and change the structures of surrounding complexes, where the functions they perform are encoded in their structure in the form of Prolog-like language expressions. After introducing the DigiHive environment, we present the results of simulations of two fundamental parts of a self-replicating system, the work of a universal constructor and a copying machine, and the growth and division of a cell-like wall. At the end of the article, the limitations and arising difficulties of modeling in the DigiHive environment are presented, along with a discussion of possible future experiments and applications of this type of modeling.},
  archive      = {J_ALJ},
  author       = {Sienkiewicz, Rafał and Jędruch, Wojciech},
  doi          = {10.1162/artl_a_00398},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {235-260},
  shortjournal = {Artif. Life},
  title        = {DigiHive: Artificial chemistry environment for modeling of self-organization phenomena},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interdependent self-organizing mechanisms for cooperative
survival. <em>ALJ</em>, <em>29</em>(2), 198–234. (<a
href="https://doi.org/10.1162/artl_a_00403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative survival “games” are situations in which, during a sequence of catastrophic events, no one survives unless everyone survives. Such situations can be further exacerbated by uncertainty over the timing and scale of the recurring catastrophes, while the resource management required for survival may depend on several interdependent subgames of resource extraction, distribution, and investment with conflicting priorities and preferences between survivors. In social systems, self-organization has been a critical feature of sustainability and survival; therefore, in this article we use the lens of artificial societies to investigate the effectiveness of socially constructed self-organization for cooperative survival games. We imagine a cooperative survival scenario with four parameters: scale, that is, n in an n -player game; uncertainty, with regard to the occurrence and magnitude of each catastrophe; complexity, concerning the number of subgames to be simultaneously “solved”; and opportunity, with respect to the number of self-organizing mechanisms available to the players. We design and implement a multiagent system for a situation composed of three entangled subgames—a stag hunt game, a common-pool resource management problem, and a collective risk dilemma—and specify algorithms for three self-organizing mechanisms for governance, trading, and forecasting. A series of experiments shows, as perhaps expected, a threshold for a critical mass of survivors and also that increasing dimensions of uncertainty and complexity require increasing opportunity for self-organization. Perhaps less expected are the ways in which self-organizing mechanisms may interact in pernicious but also self-reinforcing ways, highlighting the need for some reflection as a process in collective self-governance for cooperative survival.},
  archive      = {J_ALJ},
  author       = {Scott, Matthew and Pitt, Jeremy},
  doi          = {10.1162/artl_a_00403},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {198-234},
  shortjournal = {Artif. Life},
  title        = {Interdependent self-organizing mechanisms for cooperative survival},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How lévy flights triggered by the presence of defectors
affect evolution of cooperation in spatial games. <em>ALJ</em>,
<em>29</em>(2), 187–197. (<a
href="https://doi.org/10.1162/artl_a_00382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation among individuals has been key to sustaining societies. However, natural selection favors defection over cooperation. Cooperation can be favored when the mobility of individuals allows cooperators to form a cluster (or group). Mobility patterns of animals sometimes follow a Lévy flight. A Lévy flight is a kind of random walk but it is composed of many small movements with a few big movements. The role of Lévy flights for cooperation has been studied by Antonioni and Tomassini, who showed that Lévy flights promoted cooperation combined with conditional movements triggered by neighboring defectors. However, the optimal condition for neighboring defectors and how the condition changes with the intensity of Lévy flights are still unclear. Here, we developed an agent-based model in a square lattice where agents perform Lévy flights depending on the fraction of neighboring defectors. We systematically studied the relationships among three factors for cooperation: sensitivity to defectors, the intensity of Lévy flights, and population density. Results of evolutionary simulations showed that moderate sensitivity most promoted cooperation. Then, we found that the shortest movements were best for cooperation when the sensitivity to defectors was high. In contrast, when the sensitivity was low, longer movements were best for cooperation. Thus, Lévy flights, the balance between short and long jumps, promoted cooperation in any sensitivity, which was confirmed by evolutionary simulations. Finally, as the population density became larger, higher sensitivity was more beneficial for cooperation to evolve. Our study highlights that Lévy flights are an optimal searching strategy not only for foraging but also for constructing cooperative relationships with others.},
  archive      = {J_ALJ},
  author       = {Ichinose, Genki and Miyagawa, Daiki and Chiba, Erika and Sayama, Hiroki},
  doi          = {10.1162/artl_a_00382},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {187-197},
  shortjournal = {Artif. Life},
  title        = {How lévy flights triggered by the presence of defectors affect evolution of cooperation in spatial games},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the stability and behavioral diversity of single and
collective bernoulli balls. <em>ALJ</em>, <em>29</em>(2), 168–186. (<a
href="https://doi.org/10.1162/artl_a_00395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to express diverse behaviors is a key requirement for most biological systems. Underpinning behavioral diversity in the natural world is the embodied interaction between the brain, body, and environment. Dynamical systems form the basis of embodied agents, and can express complex behavioral modalities without any conventional computation. While significant study has focused on designing dynamical systems agents with complex behaviors, for example, passive walking, there is still a limited understanding about how to drive diversity in the behavior of such systems. In this article, we present a novel hardware platform for studying the emergence of individual and collective behavioral diversity in a dynamical system. The platform is based on the so-called Bernoulli ball, an elegant fluid dynamics phenomenon in which spherical objects self-stabilize and hover in an airflow. We demonstrate how behavioral diversity can be induced in the case of a single hovering ball via modulation of the environment. We then show how more diverse behaviors are triggered by having multiple hovering balls in the same airflow. We discuss this in the context of embodied intelligence and open-ended evolution, suggesting that the system exhibits a rudimentary form of evolutionary dynamics in which balls compete for favorable regions of the environment and exhibit intrinsic “alive” and “dead” states based on their positions in or outside of the airflow.},
  archive      = {J_ALJ},
  author       = {Howison, Toby and Crisp, Harriet and Hauser, Simon and Iida, Fumiya},
  doi          = {10.1162/artl_a_00395},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {168-186},
  shortjournal = {Artif. Life},
  title        = {On the stability and behavioral diversity of single and collective bernoulli balls},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emergence in artificial life. <em>ALJ</em>, <em>29</em>(2),
153–167. (<a href="https://doi.org/10.1162/artl_a_00397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even when concepts similar to emergence have been used since antiquity, we lack an agreed definition. However, emergence has been identified as one of the main features of complex systems. Most would agree on the statement “life is complex.” Thus understanding emergence and complexity should benefit the study of living systems. It can be said that life emerges from the interactions of complex molecules. But how useful is this to understanding living systems? Artificial Life (ALife) has been developed in recent decades to study life using a synthetic approach: Build it to understand it. ALife systems are not so complex, be they soft (simulations), hard (robots), or wet(protocells). Thus, we can aim at first understanding emergence in ALife, to then use this knowledge in biology. I argue that to understand emergence and life, it becomes useful to use information as a framework. In a general sense, I define emergence as information that is not present at one scale but present at another. This perspective avoids problems of studying emergence from a materialist framework and can also be useful in the study of self-organization and complexity.},
  archive      = {J_ALJ},
  author       = {Gershenson, Carlos},
  doi          = {10.1162/artl_a_00397},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {153-167},
  shortjournal = {Artif. Life},
  title        = {Emergence in artificial life},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalised dropout mechanism for distributed systems.
<em>ALJ</em>, <em>29</em>(2), 146–152. (<a
href="https://doi.org/10.1162/artl_a_00393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter uses a modified form of the NK model introduced to explore aspects of distributed control. In particular, a previous result suggesting the use of dynamically formed subgroups within the overall system can be more effective than global control is further explored. The conditions under which the beneficial distributed control emerges are more clearly identified, and the reason for the benefit over traditional global control is suggested as a generally applicable dropout mechanism to improve learning in such systems.},
  archive      = {J_ALJ},
  author       = {Bull, Larry and Liu, Haixia},
  doi          = {10.1162/artl_a_00393},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {146-152},
  shortjournal = {Artif. Life},
  title        = {A generalised dropout mechanism for distributed systems},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Editorial: What have large-language models and generative
al got to do with artificial life? <em>ALJ</em>, <em>29</em>(2),
141–145. (<a href="https://doi.org/10.1162/artl_e_00409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accessible generative artificial intelligence (AI) tools like large-language models (LLMs) (e.g., ChatGPT,1 Minerva2) are raising a flurry of questions about the potential and implications of generative algorithms and the ethical use of AI-generated text in a variety of contexts, including open science (Bugbee &amp;amp; Ramachandran, 2023), student assessment (Heidt, 2023), and medicine (Harrer, 2023). Similarly, among the graphic and visual arts communities, the use of generative image synthesis algorithms (e.g., DALL-E,3 Midjourney,4 Stable Diffusion5) that take text prompts as input and produce works in the style of a particular human artist, or no artist who ever lived, are causing consternation and posing challenging questions (Murphy, 2022; Plunkett, 2022). The use of generative AI to create deep fakes has also been in the spotlight (Ruiter, 2021), as has its role in answering scientific research questions directly (Castelvecchi, 2023).To our minds, the questions these technologies are raising do not seem to be of a fundamentally different character to questions asked about AI for many years. They largely concern (a) what is possible, (b) what is right, and (c) the implications of the technology’s use. For instance,Can AI generate documentary “evidence” that is indistinguishable from reality? Can AI generate artifacts that are competitive with (or superior to) those made by a human?How is the concept of “truth” confused or undermined by the output of these technologies? Is it ethical to load examples of a human’s art to generate a model for replicating their style?Who owns the intellectual property rights of AI-generated artifacts? Who is accountable if an AI-generated artifact causes harm? Will I lose my job to an AI; does AI make creative writers or artists redundant?The sense of urgency and public interest in these questions has increased as generative models become practical and ubiquitous tools with levels of skill or expertise that appear to be of above average human competency.Of general relevance to academic publishers, including Artificial Life, is that LLMs can compose text structured in the form of research paper sections or even whole articles. And these texts are often (currently) hard to distinguish from human-written text, even by software trained to detect text generated by language models (LMs). When questioned (on February 20, 2023) “What should a journal editor do about LMs?” You.com’s chatbot suggested,A journal editor should ensure that all submitted manuscripts adhere to the journal’s style guide, and that the language used is appropriate for the journal’s audience. They should also ensure that any language models (LMs) used to assist with the writing process are properly cited and that any generated text is labeled as such. Additionally, the journal editor should ensure that the LMs are used in accordance with the journal’s policies and ethical considerations.That’s a generic response that begins tangentially to the questioner’s intended focus. As any human reader can immediately see, the chatbot is not aware of the social context of the question, nor has it had the opportunity to gauge the questioner’s level of expertise. But the text is “good enough”—it does address relevant issues, and it’s an answer that probably took you a few seconds to read, interpret, and critique. It may take you more time to read and analyze the text and this paragraph than it took for us and the chatbot to generate it. Was this therefore a waste of your time, or ours? Is the chatbot wasting your time? Are we? Some journals and publishers have drafted formal policies that require the use of LLMs for writing submissions to be explicitly acknowledged (e.g., at Springer-Nature; “Tools Such as ChatGPT,” 2023). In practice, their use (or misuse) may be very difficult to detect.Artificial Life, and its publisher MIT Press generally, is also adopting the policy that any use of generative AI, for any part of a submitted work, including but not limited to text, images, sound, data, mathematics, logic, reasoning, programming code, or algorithms, must be prominently, explicitly, and unambiguously labeled and its source formally cited (e.g., via a name, manufacturer, URL, version number, or access date).Journals and publishers have also moved to prevent LLMs from being listed as authors on articles. For instance, Springer-Nature’s policy was online earlier this year, and, although it has now seemingly been removed from its original location, variants of it have been incorporated into the authorship policies of some journals:Large Language Models (LLMs), such as ChatGPT, do not currently satisfy our authorship criteria. Notably an attribution of authorship carries with it accountability for the work, which cannot be effectively applied to LLMs. Use of an LLM should be properly documented in the Methods section (and if a Methods section is not available, in a suitable alternative part) of the manuscript. (Nature, 2023)In a policy that remains online at time of writing, the journal Science states,Text generated from AI, machine learning, or similar algorithmic tools cannot be used in papers published in Science journals, nor can the accompanying figures, images, or graphics be the products of such tools, without explicit permission from the editors. In addition, an AI program cannot be an author of a Science journal paper. A violation of this policy constitutes scientific misconduct. (Science, 2023)Artificial Life and MIT Press are taking an approach in alignment with those of the editorial boards (and publishing house legal teams) of such journals: that authorship is associated with responsibility and accountability for an article. However, for Artificial Life, the issue doesn’t stop there.The implications of generative AI are relevant to a broad spectrum of society. But an interest in generative computational processes is arguably at the center of Artificial Life research. How might LLMs be specifically relevant to Artificial Life, as opposed to the subdiscipline (yes, that’s ironic) of AI? Here are a few ideas.The production of novelty can be explored through the use of an LM that continually takes as its input text composed by humans, other LMs, and its own output. Such a system is relevant to our field’s interests in feedback loops, open-endedness, and the emergence of complexity. Is this system engaged in language acquisition through “social” interactions?LMs might be used to explore questions related to the emergence of meaning in language. Can meaning be generated by an LM, or is it specific to living things? Can LMs evolve to be better interpreters and writers? How does the text LMs generate change the way humans produce and use language?If any work responding to these questions was presented in the form of a formal research experiment documented in an article, then this would naturally fall within the scope of human-authored research. However, an LLM-generated poem, song, or essay can be of value to researchers in Artificial Life exploring these topics (even if it isn’t very good; Cave, 2023). The coauthorship of such a work by an LM and a human as a way of communicating ideas about Artificial Life would be interesting to consider. In this case, the text contributed by the LM would need to be quoted as an “example” within the text of a submission made by a human author who determined that it was worthy of submission. Even though the text itself is a direct, self-referential, and, we would hope, revealing exploration of an LM system’s quirks, capabilities, or limitations, the determination of its relevance must, for the time being at least, be made by and attributed to a human.There is a precedent for such work, much of which has been explored under the banners of cybernetic, generative, and Artificial Life art (e.g., see many historical examples in Benthall, 1972; Ohlenschläger, 2012; Reichardt, 1968; Whitelaw, 2004). In such contexts, the art is published. Associated commentary and/or explanations may come later, and these might not be authored by the same system or person who made the original work.An interview, debate, discussion, or duet between a human and an AI, or between several computer programs, can also challenge our ideas about living systems and their exchange of information, the use of language, or the production of improvised movement and sound. The inclusion of extracts from discussions with computer chatbots dates back at least to the advent of ELIZA in the 1960s: “Men are all alike—IN WHAT WAY—They’re always bugging us about something or other” (Weizenbaum, 1966, p. 36). Likewise collaborative improvisations performed by robots, algorithms, and humans have an established place in music (Bown, 2011; Eldridge, 2005). As far as we know, the journal hasn’t published such works previously. But we could.For something along these lines to be published today, as with any contribution, it would of course need to provide novel perspective or insight. However, the main point here is that in these scenarios, even though we might intuitively feel that the generative AI system warrants the status of contributor at the level of coauthor, we have to insist on a human author of the submission. They would have ultimate responsibility for the work produced by the generative algorithm so that, for instance, if the LLM’s poetry influenced thought in a positive and productive way, or if it incited violence, we would have somebody accountable to thank or blame. If the article’s publication required the payment of an open access fee, we would also have somebody from whom to extract the payment!We haven’t yet received any submissions made by generative AI (as far as we know). But this issue contains novel work by human authors. In fact, we have recently published a spate of varied special issues reporting on the research presented at human gatherings, some in person, some online. These have covered a wide range of exciting activity in the Artificial Life community: Issue 28:2 has extended versions of selected papers from the 2019 Artificial Life conference; 28:3 is a collection of articles on embodied intelligence; 28:4 is the Artificial Life 2021 conference special issue; 29:1 explores agent-based models of human behavior. We extend our thanks to all the guest editors for their hard work in handling the selection and review of articles for their issues. New ideas for special issues in the subdomains of Artificial Life are always welcome. If you have an idea, please contact us.After that run of special issues, we welcome you back to a general issue of contributed research articles that cover a wide range of Artificial Life topics—including distributed control, emergence, dynamical systems, self-organization, game theory, artificial chemistry, and biocomputing—addressed through theory, models, simulations, and physical experiments.We start with a letter from Bull and Liu, on “A Generalised Dropout Mechanism for Distributed Systems.” They use a modified NK model to sharpen the criteria for determining when local control is more beneficial than global control. Next, we have an article from Gershenson, on “Emergence in Artificial Life.” He uses the difference in information present at different levels of a system as the basis for a new definition of emergence, one of the fundamental components of ALife.The article from Howison et al., “On the Stability and Behavioral Diversity of Single and Collective Bernoulli Balls,” describes a platform for investigating how dynamical systems may be used as the basis for designing a variety of agent behaviors. This platform, both in simulation and as a physical system, comprises a collection of “Bernoulli balls” in an airflow, interacting with each other and with the flow. The aim is to develop a dynamical system with a diverse set of possible behaviors.Ichinose et al. present “How Lévy Flights Triggered by the Presence of Defectors Affect Evolution of Cooperation in Spatial Games.” Lévy flights model a kind of random motion with both small and big displacements. Here Lévy flights are combined with game theory concepts in an agent-based model. The authors investigate how the presence of defectors changes the optimal behaviors.Next, Scott and Pitt investigate “Interdependent Self-Organizing Mechanisms for Cooperative Survival.” Complex survival games, where cooperation is needed to survive intermittent catastrophes, need complex strategies. Here the authors look at social self-organization, which, as any complex domain, has aspects that can make the situation better in some cases and, in other cases, worse. They conclude that such systems need to be able to reflect on their own operation through some kind of self-model.Sienkiewicz and Jędruch tell us about “DigiHive: Artificial Chemistry Environment for Modeling of Self-Organization Phenomena.” This two-dimensional continuous space simulation environment supports experiments with the goal of facilitating open-ended simulations. It steers more toward natural physical and biological systems (e.g., it includes energy conservation), rather than toward the more abstract operation of some other artificial chemistries. The authors describe the rationale and operation of the system and use it to investigate aspects of self-organization and self-replication in cellular-like systems.Finally, Svahn and Prokopenko examine “An Ansatz for Computational Undecidability in RNA Automata.” An ansatz is an “educated guess” about the form of the solution to a problem that can be used to provide a stepping-stone to finding the solution. Here the approach uses the known computational power of a set of automaton models as the form of solution and shows how RNA behaviors map to these models to demonstrate the computational power of this biological form of computing.},
  archive      = {J_ALJ},
  author       = {Dorin, Alan and Stepney, Susan},
  doi          = {10.1162/artl_e_00409},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {141-145},
  shortjournal = {Artif. Life},
  title        = {Editorial: What have large-language models and generative al got to do with artificial life?},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social search and resource clustering as emergent stable
states. <em>ALJ</em>, <em>29</em>(1), 118–140. (<a
href="https://doi.org/10.1162/artl_a_00391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social search has stably evolved across various species and is often used by humans to search for resources (such as food, information, social partners). In turn, these resources frequently come distributed in patches or clusters. In the current work, we use an ecologically inspired agent-based model to investigate whether social search and clustering are stable outcomes of the dynamical mutual interactions between the two. While previous research has studied unidirectional influences of social search on resource clustering and vice versa, the current work investigates the consequential patterns emerging from their two-way interactions over time. In our model, consumers evolved search strategies (ranging from competitive to social) as adaptations to their environmental resource structures, and resources varied in distributions (ranging from random to clustered) that were shaped by agents’ consumption patterns. Across four experiments, we systematically analyzed the patterns of influence that search strategies and environment structure have on each other to identify stable attractor states of both. In Experiment 1, we fixed resource clustering at various levels and observed its influence on social search, and in Experiment 2, we observed the influence of social search on resource distribution. In both these experiments we found that increasing levels of one variable produced increases in the other; however, at very high levels of the manipulated variable, the dependent variable tended to fall. Finally in Experiments 3 and 4, we studied the dynamics that arose when resource clustering and social search could both change and mutually influence each other, finding that low levels of social search and clustering were stable attractor states. Our simple 2D model yielded results that qualitatively resemble those across a wide range of search domains (from physical search for food to abstract search for information), highlighting some stable outcomes of mutually interacting consumer/resource systems.},
  archive      = {J_ALJ},
  author       = {Luthra, Mahi and Todd, Peter M.},
  doi          = {10.1162/artl_a_00391},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {118-140},
  shortjournal = {Artif. Life},
  title        = {Social search and resource clustering as emergent stable states},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-isolation and testing behaviour during the COVID-19
pandemic: An agent-based model. <em>ALJ</em>, <em>29</em>(1), 94–117.
(<a href="https://doi.org/10.1162/artl_a_00392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the beginning of the COVID-19 pandemic, various models of virus spread have been proposed. While most of these models focused on the replication of the interaction processes through which the virus is passed on from infected agents to susceptible ones, less effort has been devoted to the process through which agents modify their behaviour as they adapt to the risks posed by the pandemic. Understanding the way agents respond to COVID-19 spread is important, as this behavioural response affects the dynamics of virus spread by modifying interaction patterns. In this article, we present an agent-based model that includes a behavioural module determining agent testing and isolation propensity in order to understand the role of various behavioural parameters in the spread of COVID-19.},
  archive      = {J_ALJ},
  author       = {Gostoli, Umberto and Silverman, Eric},
  doi          = {10.1162/artl_a_00392},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {94-117},
  shortjournal = {Artif. Life},
  title        = {Self-isolation and testing behaviour during the COVID-19 pandemic: An agent-based model},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining the neuroevolution of fighting creatures through
virtual fMRI. <em>ALJ</em>, <em>29</em>(1), 66–93. (<a
href="https://doi.org/10.1162/artl_a_00389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While interest in artificial neural networks (ANNs) has been renewed by the ubiquitous use of deep learning to solve high-dimensional problems, we are still far from general artificial intelligence. In this article, we address the problem of emergent cognitive capabilities and, more crucially, of their detection, by relying on co-evolving creatures with mutable morphology and neural structure. The former is implemented via both static and mobile structures whose shapes are controlled by cubic splines. The latter uses ESHyperNEAT to discover not only appropriate combinations of connections and weights but also to extrapolate hidden neuron distribution. The creatures integrate low-level perceptions (touch/pain proprioceptors, retina-based vision, frequency-based hearing) to inform their actions. By discovering a functional mapping between individual neurons and specific stimuli, we extract a high-level module-based abstraction of a creature’s brain. This drastically simplifies the discovery of relationships between naturally occurring events and their neural implementation. Applying this methodology to creatures resulting from solitary and tag-team co-evolution showed remarkable dynamics such as range-finding and structured communication. Such discovery was made possible by the abstraction provided by the modular ANN which allowed groups of neurons to be viewed as functionally enclosed entities.},
  archive      = {J_ALJ},
  author       = {Godin-Dubois, Kevin and Cussat-Blanc, Sylvain and Duthen, Yves},
  doi          = {10.1162/artl_a_00389},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {66-93},
  shortjournal = {Artif. Life},
  title        = {Explaining the neuroevolution of fighting creatures through virtual fMRI},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expertise, social influence, and knowledge aggregation in
distributed information processing. <em>ALJ</em>, <em>29</em>(1), 37–65.
(<a href="https://doi.org/10.1162/artl_a_00387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many social, cyber-physical, and socio-technical systems, a group of autonomous peers can encounter a knowledge aggregation problem, requiring them to organise themselves, without a centralised authority, as a distributed information processing unit (DIP). In this article, we specify and implement a new algorithm for knowledge aggregation based on Nowak’s psychological theory Regulatory Theory of Social Influence (RTSI). This theory posits that social influence consists of not only sources trying to influence targets, but also targets seeking sources by whom to be influenced and learning what processing rules those sources are using. A multi-agent simulator SMARTSIS is implemented to evaluate the algorithm, using as its base scenario a linear public goods game where the DIP’s decision is a qualitative question of distributive justice. In a series of experiments examining the emergence of expertise, we show how RTSI enhances the effectiveness of the multi-agent DIP as a social group while conserving each agent’s individual resources. Additionally, we identify eight criteria for evaluating the DIP unit’s performance, consisting of four conflicting pairs of systemic drivers, and discuss how RTSI maintains a balanced tension between the four driver pairs through the emergence and divergence of expertise. We conclude by arguing that this shows how psychological theories like RTSI can have a crucial role in informing agent-based models of human behaviour, which in turn may be critically important for effective knowledge management and reflective self-improvement in both cyber-physical and socio-technical systems.},
  archive      = {J_ALJ},
  author       = {Mertzani, Asimina and Pitt, Jeremy and Nowak, Andrzej and Michalak, Tomasz},
  doi          = {10.1162/artl_a_00387},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {37-65},
  shortjournal = {Artif. Life},
  title        = {Expertise, social influence, and knowledge aggregation in distributed information processing},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adapting the exploration–exploitation balance in
heterogeneous swarms: Tracking evasive targets. <em>ALJ</em>,
<em>29</em>(1), 21–36. (<a
href="https://doi.org/10.1162/artl_a_00390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been growing interest in the use of multi-robot systems in various tasks and scenarios. The main attractiveness of such systems is their flexibility, robustness, and scalability. An often overlooked yet promising feature is system modularity, which offers the possibility of harnessing agent specialization, while also enabling system-level upgrades. However, altering the agents’ capacities can change the exploration–exploitation balance required to maximize the system’s performance. Here, we study the effect of a swarm’s heterogeneity on its exploration–exploitation balance while tracking multiple fast-moving evasive targets under the cooperative multi-robot observation of multiple moving targets framework. To this end, we use a decentralized search and tracking strategy with adjustable levels of exploration and exploitation. By indirectly tuning the balance, we first confirm the presence of an optimal balance between these two key competing actions. Next, by substituting slower moving agents with faster ones, we show that the system exhibits a performance improvement without any modifications to the original strategy. In addition, owing to the additional amount of exploitation carried out by the faster agents, we demonstrate that a heterogeneous system’s performance can be further improved by reducing an agent’s level of connectivity, to favor the conduct of exploratory actions. Furthermore, in studying the influence of the density of swarming agents, we show that the addition of faster agents can counterbalance a reduction in the overall number of agents while maintaining the level of tracking performance. Finally, we explore the challenges of using differentiated strategies to take advantage of the heterogeneous nature of the swarm.},
  archive      = {J_ALJ},
  author       = {Kwa, Hian Lee and Babineau, Victor and Philippot, Julien and Bouffanais, Roland},
  doi          = {10.1162/artl_a_00390},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {21-36},
  shortjournal = {Artif. Life},
  title        = {Adapting the Exploration–Exploitation balance in heterogeneous swarms: Tracking evasive targets},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effects of information on the formation of migration
routes and the dynamics of migration. <em>ALJ</em>, <em>29</em>(1),
3–20. (<a href="https://doi.org/10.1162/artl_a_00388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most models of migration simply assume that migrants somehow make their way from their point of origin to their chosen destination. We know, however, that—especially in the case of asylum migration—the migrant journey often is a hazardous, difficult process where migrants make decisions based on limited information and under severe material constraints. Here we investigate the dynamics of the migration journey itself using a spatially explicit, agent-based model. In particular we are interested in the effects of limited information and information exchange. We find that under limited information, migration routes generally become suboptimal, their stochasticity increases, and migrants arrive much less frequently at their preferred destination. Under specific circumstances, self-organised consensus routes emerge that are largely unpredictable. Limited information also strongly reduces the migrants’ ability to react to changes in circumstances. We conclude, first, that information and information exchange is likely to have considerable effects on all aspects of migration and should thus be included in future modelling efforts and, second, that there are many questions in theoretical migration research that are likely to profit from the use of agent-based modelling techniques.},
  archive      = {J_ALJ},
  author       = {Hinsch, Martin and Bijak, Jakub},
  doi          = {10.1162/artl_a_00388},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {3-20},
  shortjournal = {Artif. Life},
  title        = {The effects of information on the formation of migration routes and the dynamics of migration},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The “agent-based modeling for human behavior” special issue.
<em>ALJ</em>, <em>29</em>(1), 1–2. (<a
href="https://doi.org/10.1162/artl_e_00394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We inhabit an obscure planet, in an obscure galaxy, around an obscure sun, but on the other hand, modern human society represents one of the most complex things we know.—David Christian1If human societies are so complex, then how can we hope to understand them? Artificial Life gives us one answer. The field of Artificial Life comprises a diverse set of introspective studies that largely ask the same questions, albeit from many different perspectives: Why are we here? Who are we? Why do we behave as we do? Starting with the origins of life provides us with fascinating answers to some of these questions. However, some researchers choose to bring their studies closer to the present day. We are, after all, human. It has been a few billion years since our ancestors were self-replicating molecules. Thus more direct studies of ourselves and our human societies can reveal truths that may lead to practical knowledge.The articles in this special issue bring together scientists who choose to perform this kind of research. Expanded from submissions to our annual Agent-Based Modelling of Human Behaviour Workshop, the studies share similar methods, all using variations of agent-based modeling (ABM) to ask their own what-if questions. As guest editors, we believe such collections help bring together and enhance such research by sharing ideas. While ABM research—out of necessity—is often highly specialized toward the hypotheses and phenomena under study, the research methodology is shared by all. We formulate our hypothesis, develop our agent-based model of the relevant aspects of reality, and run experiments to gather evidence that may support or refute the hypothesis. An experimental model that supports the hypothesis may not prove that reality follows this approach or agrees with this result, but it indicates that there exists a specific set of conditions that, if found to be true elsewhere, may produce the same result. Modeling tells us about trends, about possible likelihoods. Our ABMs show us what will result if our assumptions are valid and why, whether we are examining civil violence, app stores, the economy, fish markets, language evolution, or energy consumption. When we study human societies, ABMs are the tools of choice for obvious reasons: It is not ethical or safe to play what-if experiments with ourselves. The researchers in this special issue demonstrate the exciting potential in ABM. We can create our own safe virtual worlds and make discoveries that enlighten us about ourselves.In this special issue, we have a fascinating collection of studies that relate to us. For example, from our first article, “The Effects of Information on the Formation of Migration Routes and the Dynamics of Migration” by Hinsch and Bijak, we may discover that as we spread across the planet both physically and culturally, the nature of our interactions affects us in ways we may not expect. The authors develop a spatially explicit agent-based model of human migration and show that information exchange plays a crucial role in affecting all aspects of migration: Too little information results in suboptimal migration routes, increased stochasticity, and migrants frequently not arriving at their preferred destinations, and too much information under certain conditions can lead to less predictable migration routes.Likewise, from our second article, “Adapting the Exploration–Exploitation Balance in Heterogeneous Swarms: Tracking Evasive Targets” by Kwa et al., we might recognize that like the robotic agents, we are diverse in every sense of the word, and that diversity is not only to be celebrated; it is a necessity for the optimal functioning of our societies. In their work, agents with different properties and behaviors are part of the same collective, forming a heterogeneous swarm. The authors use a decentralized search-and-tracking strategy with adjustable levels of exploration and exploitation and find that small, heterogenous swarms can match and outperform homogeneous swarms. They also explore using differentiated strategies to take advantage of heterogenous swarms.Our third contribution, “Expertise, Social Influence, and Knowledge Aggregation in Distributed Information Processing” by Mertzani et al., helps us understand that fields like psychology provide insights into human behavior and how we communicate—a topic close to our hearts as guest editors. Mertzani et al. develop an algorithm for knowledge aggregation based on Nowak’s regulatory theory of social influence. This theory posits that social influence consists not only of sources trying to influence targets but also of targets seeking sources by whom to be influenced and learning what processing rules those sources are using.Our societies face constant challenges that threaten us. Sadly, humans evolved to be violent and to work together for our violent aims, as ongoing geopolitical instability shows all too well. “Explaining Neuroevolution of Fighting Creatures Through Virtual fMRI” by Godin-Dubois et al. investigates where these behaviors come from and what causes different strategies and morphologies. The authors develop artificial creatures that engage in individual and team competitions and investigate three types of reactions—pain, vision, and audition—each with varying response levels but consistent across all evolution types.When we’re not battling each other, human societies also must battle pathogens—most recently in the global COVID-19 pandemic. As the virus becomes endemic and we develop coping strategies, are we responding appropriately to the changing risk levels? “Self-Isolation and Testing Behavior During the COVID-19 Pandemic: An Agent-Based Model” by Gostoli and Silverman models this phenomenon to find out. In this article, the authors develop an agent-based model that includes a behavioral module determining the agents’ testing and isolation propensity. According to the authors, most models focus on the replication of the interactions’ processes through which the virus is passed from infected agents to susceptible ones; in this model, agents modify their behavior as they adapt to the risks posed by the pandemic.Finally, there can be no more fundamental challenge to a society than its constant search for resources to sustain itself. Our final paper, “Social Search and Resource Clustering as Emergent Stable States” by Luthra and Todd, looks at the interplay between resources and consumers of those resources in a 2-D simulated world. Agents evolve strategies to search for food, and the resources resemble plants that grow continuously across the 2-D simulated world. Resources and other consumers produce distinguishable “odors” that are normally distributed around their positions in the grid, and agents can perceive these two types of odors and make movement decisions that take them toward or away from each kind of source.Through models such as these, we can learn why we might behave as we do as a species and whether we can start to mitigate or redirect some of the less-than-optimal patterns of behaviors to which our complex societies might be attracted. We hope you enjoy reading these articles as much as we have and that you might be inspired to ask your own what-if questions using ABM in your work.},
  archive      = {J_ALJ},
  author       = {Lim, Soo Ling and Bentley, Peter J.},
  doi          = {10.1162/artl_e_00394},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Artif. Life},
  title        = {The “Agent-based modeling for human behavior” special issue},
  volume       = {29},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
