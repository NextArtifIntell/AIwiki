<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco---66">NECO - 66</h2>
<ul>
<li><details>
<summary>
(2023). Training a hyperdimensional computing classifier using a
threshold on its confidence. <em>NECO</em>, <em>35</em>(12), 2006–2023.
(<a href="https://doi.org/10.1162/neco_a_01618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional computing (HDC) has become popular for light-weight and energy-efficient machine learning, suitable for wearable Internet-of-Things devices and near-sensor or on-device processing. HDC is computationally less complex than traditional deep learning algorithms and achieves moderate to good classification performance. This letter proposes to extend the training procedure in HDC by taking into account not only wrongly classified samples but also samples that are correctly classified by the HDC model but with low confidence. We introduce a confidence threshold that can be tuned for each data set to achieve the best classification accuracy. The proposed training procedure is tested on UCIHAR, CTG, ISOLET, and HAND data sets for which the performance consistently improves compared to the baseline across a range of confidence threshold values. The extended training procedure also results in a shift toward higher confidence values of the correctly classified samples, making the classifier not only more accurate but also more confident about its predictions.},
  archive      = {J_NECO},
  author       = {Smets, Laura and Van Leekwijck, Werner and Tsang, Ing Jyh and Latré, Steven},
  doi          = {10.1162/neco_a_01618},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2006-2023},
  shortjournal = {Neural Comput.},
  title        = {Training a hyperdimensional computing classifier using a threshold on its confidence},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized low-rank update: Model parameter bounds for
low-rank training data modifications. <em>NECO</em>, <em>35</em>(12),
1970–2005. (<a href="https://doi.org/10.1162/neco_a_01619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we have developed an incremental machine learning (ML) method that efficiently obtains the optimal model when a small number of instances or features are added or removed. This problem holds practical importance in model selection, such as cross-validation (CV) and feature selection. Among the class of ML methods known as linear estimators, there exists an efficient model update framework, the low-rank update , that can effectively handle changes in a small number of rows and columns within the data matrix. However, for ML methods beyond linear estimators, there is currently no comprehensive framework available to obtain knowledge about the updated solution within a specific computational complexity. In light of this, our study introduces a the generalized low-rank update (GLRU) method, which extends the low-rank update framework of linear estimators to ML methods formulated as a certain class of regularized empirical risk minimization, including commonly used methods such as support vector machines and logistic regression. The proposed GLRU method not only expands the range of its applicability but also provides information about the updated solutions with a computational complexity proportional to the number of data set changes. To demonstrate the effectiveness of the GLRU method, we conduct experiments showcasing its efficiency in performing cross-validation and feature selection compared to other baseline methods.},
  archive      = {J_NECO},
  author       = {Hanada, Hiroyuki and Hashimoto, Noriaki and Taji, Kouichi and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01619},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {1970-2005},
  shortjournal = {Neural Comput.},
  title        = {Generalized low-rank update: Model parameter bounds for low-rank training data modifications},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive filter model of cerebellum for biological muscle
control with spike train inputs. <em>NECO</em>, <em>35</em>(12),
1938–1969. (<a href="https://doi.org/10.1162/neco_a_01617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior applications of the cerebellar adaptive filter model have included a range of tasks within simulated and robotic systems. However, this has been limited to systems driven by continuous signals. Here, the adaptive filter model of the cerebellum is applied to the control of a system driven by spiking inputs by considering the problem of controlling muscle force. The performance of the standard adaptive filter algorithm is compared with the algorithm with a modified learning rule that minimizes inputs and a simple proportional-integral-derivative (PID) controller. Control performance is evaluated in terms of the number of spikes, the accuracy of spike input locations, and the accuracy of muscle force output. Results show that the cerebellar adaptive filter model can be applied without change to the control of systems driven by spiking inputs. The cerebellar algorithm results in good agreement between input spikes and force outputs and significantly improves on a PID controller. Input minimization can be used to reduce the number of spike inputs, but at the expense of a decrease in accuracy of spike input location and force output. This work extends the applications of the cerebellar algorithm and demonstrates the potential of the adaptive filter model to be used to improve functional electrical stimulation muscle control.},
  archive      = {J_NECO},
  author       = {Wilson, Emma},
  doi          = {10.1162/neco_a_01617},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {1938-1969},
  shortjournal = {Neural Comput.},
  title        = {Adaptive filter model of cerebellum for biological muscle control with spike train inputs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robustness to transformations across categories: Is
robustness driven by invariant neural representations? <em>NECO</em>,
<em>35</em>(12), 1910–1937. (<a
href="https://doi.org/10.1162/neco_a_01621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (e.g., blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. However, to what extent this hypothesis holds true is an outstanding question, as robustness to transformations could be achieved with properties different from invariance; for example, parts of the network could be specialized to recognize either transformed or nontransformed images. This article investigates the conditions under which invariant neural representations emerge by leveraging that they facilitate robustness to transformations beyond the training distribution. Concretely, we analyze a training paradigm in which only some object categories are seen transformed during training and evaluate whether the DCNN is robust to transformations across categories not seen transformed. Our results with state-of-the-art DCNNs indicate that invariant neural representations do not always drive robustness to transformations, as networks show robustness for categories seen transformed during training even in the absence of invariant neural representations. Invariance emerges only as the number of transformed categories in the training set is increased. This phenomenon is much more prominent with local transformations such as blurring and high-pass filtering than geometric transformations such as rotation and thinning, which entail changes in the spatial arrangement of the object. Our results contribute to a better understanding of invariant neural representations in deep learning and the conditions under which it spontaneously emerges.},
  archive      = {J_NECO},
  author       = {Jang, Hojin and Zaidi, Syed Suleman Abbas and Boix, Xavier and Prasad, Neeraj and Gilad-Gutnick, Sharon and Ben-Ami, Shlomit and Sinha, Pawan},
  doi          = {10.1162/neco_a_01621},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {1910-1937},
  shortjournal = {Neural Comput.},
  title        = {Robustness to transformations across categories: Is robustness driven by invariant neural representations?},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predictive coding as a neuromorphic alternative to
backpropagation: A critical evaluation. <em>NECO</em>, <em>35</em>(12),
1881–1909. (<a href="https://doi.org/10.1162/neco_a_01620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backpropagation has rapidly become the workhorse credit assignment algorithm for modern deep learning methods. Recently, modified forms of predictive coding (PC), an algorithm with origins in computational neuroscience, have been shown to result in approximately or exactly equal parameter updates to those under backpropagation. Due to this connection, it has been suggested that PC can act as an alternative to backpropagation with desirable properties that may facilitate implementation in neuromorphic systems. Here, we explore these claims using the different contemporary PC variants proposed in the literature. We obtain time complexity bounds for these PC variants, which we show are lower bounded by backpropagation. We also present key properties of these variants that have implications for neurobiological plausibility and their interpretations, particularly from the perspective of standard PC as a variational Bayes algorithm for latent probabilistic models. Our findings shed new light on the connection between the two learning frameworks and suggest that in its current forms, PC may have more limited potential as a direct replacement of backpropagation than previously envisioned.},
  archive      = {J_NECO},
  author       = {Zahid, Umais and Guo, Qinghai and Fountas, Zafeirios},
  doi          = {10.1162/neco_a_01620},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {1881-1909},
  shortjournal = {Neural Comput.},
  title        = {Predictive coding as a neuromorphic alternative to backpropagation: A critical evaluation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal feedback control for the proportion of energy cost
in the upper-arm reaching movement. <em>NECO</em>, <em>35</em>(11),
1870–1880. (<a href="https://doi.org/10.1162/neco_a_01614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum expected energy cost model, which has been proposed as one of the optimization principles for movement planning, can reproduce many characteristics of the human upper-arm reaching movement when signal-dependent noise and the co-contraction of the antagonist’s muscles are considered. Regarding the optimization principles, discussion has been mainly based on feedforward control; however, there is debate as to whether the central nervous system uses a feedforward or feedback control process. Previous studies have shown that feedback control based on the modified linear-quadratic gaussian (LQG) control, including multiplicative noise, can reproduce many characteristics of the reaching movement. Although the cost of the LQG control consists of state and energy costs, the relationship between the energy cost and the characteristics of the reaching movement in the LQG control has not been studied. In this work, I investigated how the optimal movement based on the LQG control varied with the proportion of energy cost, assuming that the central nervous system used feedback control. When the cost contained specific proportions of energy cost, the optimal movement reproduced the characteristics of the reaching movement. This result shows that energy cost is essential in both feedforward and feedback control for reproducing the characteristics of the upper-arm reaching movement.},
  archive      = {J_NECO},
  author       = {Taniai, Yoshiaki},
  doi          = {10.1162/neco_a_01614},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {1870-1880},
  shortjournal = {Neural Comput.},
  title        = {Optimal feedback control for the proportion of energy cost in the upper-arm reaching movement},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Winning the lottery with neural connectivity constraints:
Faster learning across cognitive tasks with spatially constrained sparse
RNNs. <em>NECO</em>, <em>35</em>(11), 1850–1869. (<a
href="https://doi.org/10.1162/neco_a_01613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are often used to model circuits in the brain and can solve a variety of difficult computational problems requiring memory, error correction, or selection (Hopfield, 1982 ; Maass et al., 2002 ; Maass, 2011 ). However, fully connected RNNs contrast structurally with their biological counterparts, which are extremely sparse (about 0.1\%). Motivated by the neocortex, where neural connectivity is constrained by physical distance along cortical sheets and other synaptic wiring costs, we introduce locality masked RNNs (LM-RNNs) that use task-agnostic predetermined graphs with sparsity as low as 4\%. We study LM-RNNs in a multitask learning setting relevant to cognitive systems neuroscience with a commonly used set of tasks, 20-Cog-tasks (Yang et al., 2019 ). We show through reductio ad absurdum that 20-Cog-tasks can be solved by a small pool of separated autapses that we can mechanistically analyze and understand. Thus, these tasks fall short of the goal of inducing complex recurrent dynamics and modular structure in RNNs. We next contribute a new cognitive multitask battery, Mod-Cog , consisting of up to 132 tasks that expands by about seven-fold the number of tasks and task complexity of 20-Cog-tasks . Importantly, while autapses can solve the simple 20-Cog-tasks , the expanded task set requires richer neural architectures and continuous attractor dynamics. On these tasks, we show that LM-RNNs with an optimal sparsity result in faster training and better data efficiency than fully connected networks.},
  archive      = {J_NECO},
  author       = {Khona, Mikail and Chandra, Sarthak and Ma, Joy J. and Fiete, Ila R.},
  doi          = {10.1162/neco_a_01613},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {1850-1869},
  shortjournal = {Neural Comput.},
  title        = {Winning the lottery with neural connectivity constraints: Faster learning across cognitive tasks with spatially constrained sparse RNNs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-organization of nonlinearly coupled neural fluctuations
into synergistic population codes. <em>NECO</em>, <em>35</em>(11),
1820–1849. (<a href="https://doi.org/10.1162/neco_a_01612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural activity in the brain exhibits correlated fluctuations that may strongly influence the properties of neural population coding. However, how such correlated neural fluctuations may arise from the intrinsic neural circuit dynamics and subsequently affect the computational properties of neural population activity remains poorly understood. The main difficulty lies in resolving the nonlinear coupling between correlated fluctuations with the overall dynamics of the system. In this study, we investigate the emergence of synergistic neural population codes from the intrinsic dynamics of correlated neural fluctuations in a neural circuit model capturing realistic nonlinear noise coupling of spiking neurons. We show that a rich repertoire of spatial correlation patterns naturally emerges in a bump attractor network and further reveals the dynamical regime under which the interplay between differential and noise correlations leads to synergistic codes. Moreover, we find that negative correlations may induce stable bound states between two bumps, a phenomenon previously unobserved in firing rate models. These noise-induced effects of bump attractors lead to a number of computational advantages including enhanced working memory capacity and efficient spatiotemporal multiplexing and can account for a range of cognitive and behavioral phenomena related to working memory. This study offers a dynamical approach to investigating realistic correlated neural fluctuations and insights to their roles in cortical computations.},
  archive      = {J_NECO},
  author       = {Ma, Hengyuan and Qi, Yang and Gong, Pulin and Zhang, Jie and Lu, Wen-lian and Feng, Jianfeng},
  doi          = {10.1162/neco_a_01612},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {1820-1849},
  shortjournal = {Neural Comput.},
  title        = {Self-organization of nonlinearly coupled neural fluctuations into synergistic population codes},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing catastrophic forgetting with associative learning:
A lesson from fruit flies. <em>NECO</em>, <em>35</em>(11), 1797–1819.
(<a href="https://doi.org/10.1162/neco_a_01615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic forgetting remains an outstanding challenge in continual learning. Recently, methods inspired by the brain, such as continual representation learning and memory replay, have been used to combat catastrophic forgetting. Associative learning (retaining associations between inputs and outputs, even after good representations are learned) plays an important function in the brain; however, its role in continual learning has not been carefully studied. Here, we identified a two-layer neural circuit in the fruit fly olfactory system that performs continual associative learning between odors and their associated valences. In the first layer, inputs (odors) are encoded using sparse, high-dimensional representations, which reduces memory interference by activating nonoverlapping populations of neurons for different odors. In the second layer, only the synapses between odor-activated neurons and the odor’s associated output neuron are modified during learning; the rest of the weights are frozen to prevent unrelated memories from being overwritten. We prove theoretically that these two perceptron-like layers help reduce catastrophic forgetting compared to the original perceptron algorithm, under continual learning. We then show empirically on benchmark data sets that this simple and lightweight architecture outperforms other popular neural-inspired algorithms when also using a two-layer feedforward architecture. Overall, fruit flies evolved an efficient continual associative learning algorithm, and circuit mechanisms from neuroscience can be translated to improve machine computation.},
  archive      = {J_NECO},
  author       = {Shen, Yang and Dasgupta, Sanjoy and Navlakha, Saket},
  doi          = {10.1162/neco_a_01615},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {1797-1819},
  shortjournal = {Neural Comput.},
  title        = {Reducing catastrophic forgetting with associative learning: A lesson from fruit flies},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tutorial on the spectral theory of markov chains.
<em>NECO</em>, <em>35</em>(11), 1713–1796. (<a
href="https://doi.org/10.1162/neco_a_01611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chains are a class of probabilistic models that have achieved widespread application in the quantitative sciences. This is in part due to their versatility, but is compounded by the ease with which they can be probed analytically. This tutorial provides an in-depth introduction to Markov chains and explores their connection to graphs and random walks. We use tools from linear algebra and graph theory to describe the transition matrices of different types of Markov chains, with a particular focus on exploring properties of the eigenvalues and eigenvectors corresponding to these matrices. The results presented are relevant to a number of methods in machine learning and data mining, which we describe at various stages. Rather than being a novel academic study in its own right, this text presents a collection of known results, together with some new concepts. Moreover, the tutorial focuses on offering intuition to readers rather than formal understanding and only assumes basic exposure to concepts from linear algebra and probability theory. It is therefore accessible to students and researchers from a wide variety of disciplines.},
  archive      = {J_NECO},
  author       = {Seabrook, Eddie and Wiskott, Laurenz},
  doi          = {10.1162/neco_a_01611},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {1713-1796},
  shortjournal = {Neural Comput.},
  title        = {A tutorial on the spectral theory of markov chains},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning with singular value decomposition of
multichannel convolution matrices. <em>NECO</em>, <em>35</em>(10),
1678–1712. (<a href="https://doi.org/10.1162/neco_a_01608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of transfer learning using pretrained convolutional neural networks is considered. We propose a convolution-SVD layer to analyze the convolution operators with a singular value decomposition computed in the Fourier domain. Singular vectors extracted from the source domain are transferred to the target domain, whereas the singular values are fine-tuned with a target data set. In this way, dimension reduction is achieved to avoid overfitting, while some flexibility to fine-tune the convolution kernels is maintained. We extend an existing convolution kernel reconstruction algorithm to allow for a reconstruction from an arbitrary set of learned singular values. A generalization bound for a single convolution-SVD layer is devised to show the consistency between training and testing errors. We further introduce a notion of transfer learning gap. We prove that the testing error for a single convolution-SVD layer is bounded in terms of the gap, which motivates us to develop a regularization model with the gap as the regularizer. Numerical experiments are conducted to demonstrate the superiority of the proposed model in solving classification problems and the influence of various parameters. In particular, the regularization is shown to yield a significantly higher prediction accuracy.},
  archive      = {J_NECO},
  author       = {Yeung, Tak Shing Au and Cheung, Ka Chun and Ng, Michael K. and See, Simon and Yip, Andy},
  doi          = {10.1162/neco_a_01608},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1678-1712},
  shortjournal = {Neural Comput.},
  title        = {Transfer learning with singular value decomposition of multichannel convolution matrices},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning intention-aware policies in deep reinforcement
learning. <em>NECO</em>, <em>35</em>(10), 1657–1677. (<a
href="https://doi.org/10.1162/neco_a_01607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) provides an agent with an optimal policy so as to maximize the cumulative rewards. The policy defined in DRL mainly depends on the state, historical memory, and policy model parameters. However, we humans usually take actions according to our own intentions, such as moving fast or slow, besides the elements included in the traditional policy models. In order to make the action-choosing mechanism more similar to humans and make the agent to select actions that incorporate intentions, we propose an intention-aware policy learning method in this letter To formalize this process, we first define an intention-aware policy by incorporating the intention information into the policy model, which is learned by maximizing the cumulative rewards with the mutual information (MI) between the intention and the action. Then we derive an approximation of the MI objective that can be optimized efficiently. Finally, we demonstrate the effectiveness of the intention-aware policy in the classical MuJoCo control task and the multigoal continuous chain walking task.},
  archive      = {J_NECO},
  author       = {Zhao, T. and Wu, S. and Li, G. and Chen, Y. and Niu, G. and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01607},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1657-1677},
  shortjournal = {Neural Comput.},
  title        = {Learning intention-aware policies in deep reinforcement learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring trade-offs in spiking neural networks.
<em>NECO</em>, <em>35</em>(10), 1627–1656. (<a
href="https://doi.org/10.1162/neco_a_01609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have emerged as a promising alternative to traditional deep neural networks for low-power computing. However, the effectiveness of SNNs is not solely determined by their performance but also by their energy consumption, prediction speed, and robustness to noise. The recent method Fast &amp; Deep, along with others, achieves fast and energy-efficient computation by constraining neurons to fire at most once. Known as time-to-first-spike (TTFS), this constraint, however, restricts the capabilities of SNNs in many aspects. In this work, we explore the relationships of performance, energy consumption, speed, and stability when using this constraint. More precisely, we highlight the existence of trade-offs where performance and robustness are gained at the cost of sparsity and prediction latency. To improve these trade-offs, we propose a relaxed version of Fast &amp; Deep that allows for multiple spikes per neuron. Our experiments show that relaxing the spike constraint provides higher performance while also benefiting from faster convergence, similar sparsity, comparable prediction latency, and better robustness to noise compared to TTFS SNNs. By highlighting the limitations of TTFS and demonstrating the advantages of unconstrained SNNs, we provide valuable insight for the development of effective learning strategies for neuromorphic computing.},
  archive      = {J_NECO},
  author       = {Bacho, Florian and Chu, Dominique},
  doi          = {10.1162/neco_a_01609},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1627-1656},
  shortjournal = {Neural Comput.},
  title        = {Exploring trade-offs in spiking neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grid cell percolation. <em>NECO</em>, <em>35</em>(10),
1609–1626. (<a href="https://doi.org/10.1162/neco_a_01606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid cells play a principal role in enabling cognitive representations of ambient environments. The key property of these cells—the regular arrangement of their firing fields—is commonly viewed as a means for establishing spatial scales or encoding specific locations. However, using grid cells’ spiking outputs for deducing geometric orderliness proves to be a strenuous task due to fairly irregular activation patterns triggered by the animal’s sporadic visits to the grid fields. This article addresses statistical mechanisms enabling emergent regularity of grid cell firing activity from the perspective of percolation theory. Using percolation phenomena for modeling the effect of the rat’s moves through the lattices of firing fields sheds new light on the mechanisms of spatial information processing, spatial learning, path integration, and establishing spatial metrics. It is also shown that physiological parameters required for spiking percolation match the experimental range, including the characteristic 2/3 ratio between the grid fields’ size and the grid spacing, pointing at a biological viability of the approach.},
  archive      = {J_NECO},
  author       = {Dabaghian, Yuri},
  doi          = {10.1162/neco_a_01606},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1609-1626},
  shortjournal = {Neural Comput.},
  title        = {Grid cell percolation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A noise-based novel strategy for faster SNN training.
<em>NECO</em>, <em>35</em>(9), 1593–1608. (<a
href="https://doi.org/10.1162/neco_a_01604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are receiving increasing attention due to their low power consumption and strong bioplausibility. Optimization of SNNs is a challenging task. Two main methods, artificial neural network (ANN)-to-SNN conversion and spike-based backpropagation (BP), both have advantages and limitations. ANN-to-SNN conversion requires a long inference time to approximate the accuracy of ANN, thus diminishing the benefits of SNN. With spike-based BP, training high-precision SNNs typically consumes dozens of times more computational resources and time than their ANN counterparts. In this letter, we propose a novel SNN training approach that combines the benefits of the two methods. We first train a single-step SNN( T = 1) by approximating the neural potential distribution with random noise, then convert the single-step SNN( T = 1) to a multistep SNN(T = N ) losslessly. The introduction of gaussian distributed noise leads to a significant gain in accuracy after conversion. The results show that our method considerably reduces the training and inference times of SNNs while maintaining their high accuracy. Compared to the previous two methods, ours can reduce training time by 65\% to 75\% and achieves more than 100 times faster inference speed. We also argue that the neuron model augmented with noise makes it more bioplausible.},
  archive      = {J_NECO},
  author       = {Jiang, Chunming and Zhang, Yilei},
  doi          = {10.1162/neco_a_01604},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1593-1608},
  shortjournal = {Neural Comput.},
  title        = {A noise-based novel strategy for faster SNN training},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On an interpretation of ResNets via gate-network control.
<em>NECO</em>, <em>35</em>(9), 1566–1592. (<a
href="https://doi.org/10.1162/neco_a_01600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter first constructs a typical solution of ResNets for multicategory classifications based on the idea of the gate control of LSTMs, from which a general interpretation of the ResNet architecture is given and the performance mechanism is explained. We also use more solutions to further demonstrate the generality of that interpretation. The classification result is then extended to the universal-approximation capability of the type of ResNet with two-layer gate networks, an architecture that was proposed in an original paper of ResNets and has both theoretical and practical significance.},
  archive      = {J_NECO},
  author       = {Huang, Changcun},
  doi          = {10.1162/neco_a_01600},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1566-1592},
  shortjournal = {Neural Comput.},
  title        = {On an interpretation of ResNets via gate-network control},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Composite optimization algorithms for sigmoid networks.
<em>NECO</em>, <em>35</em>(9), 1543–1565. (<a
href="https://doi.org/10.1162/neco_a_01603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this letter, we use composite optimization algorithms to solve sigmoid networks. We equivalently transfer the sigmoid networks to a convex composite optimization and propose the composite optimization algorithms based on the linearized proximal algorithms and the alternating direction method of multipliers. Under the assumptions of the weak sharp minima and the regularity condition, the algorithm is guaranteed to converge to a globally optimal solution of the objective function even in the case of nonconvex and nonsmooth problems. Furthermore, the convergence results can be directly related to the amount of training data and provide a general guide for setting the size of sigmoid networks. Numerical experiments on Franke’s function fitting and handwritten digit recognition show that the proposed algorithms perform satisfactorily and robustly.},
  archive      = {J_NECO},
  author       = {Chen, Huixiong and Ye, Qi},
  doi          = {10.1162/neco_a_01603},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1543-1565},
  shortjournal = {Neural Comput.},
  title        = {Composite optimization algorithms for sigmoid networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mirror descent of hopfield model. <em>NECO</em>,
<em>35</em>(9), 1529–1542. (<a
href="https://doi.org/10.1162/neco_a_01602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mirror descent is an elegant optimization technique that leverages a dual space of parametric models to perform gradient descent. While originally developed for convex optimization, it has increasingly been applied in the field of machine learning. In this study, we propose a novel approach for using mirror descent to initialize the parameters of neural networks. Specifically, we demonstrate that by using the Hopfield model as a prototype for neural networks, mirror descent can effectively train the model with significantly improved performance compared to traditional gradient descent methods that rely on random parameter initialization. Our findings highlight the potential of mirror descent as a promising initialization technique for enhancing the optimization of machine learning models.},
  archive      = {J_NECO},
  author       = {Soh, Hyungjoon and Kim, Dongyeob and Hwang, Juno and Jo, Junghyo},
  doi          = {10.1162/neco_a_01602},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1529-1542},
  shortjournal = {Neural Comput.},
  title        = {Mirror descent of hopfield model},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mean-field approximations with adaptive coupling for
networks with spike-timing-dependent plasticity. <em>NECO</em>,
<em>35</em>(9), 1481–1528. (<a
href="https://doi.org/10.1162/neco_a_01601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the effect of spike-timing-dependent plasticity (STDP) is key to elucidating how neural networks change over long timescales and to design interventions aimed at modulating such networks in neurological disorders. However, progress is restricted by the significant computational cost associated with simulating neural network models with STDP and by the lack of low-dimensional description that could provide analytical insights. Phase-difference-dependent plasticity (PDDP) rules approximate STDP in phase oscillator networks, which prescribe synaptic changes based on phase differences of neuron pairs rather than differences in spike timing. Here we construct mean-field approximations for phase oscillator networks with STDP to describe part of the phase space for this very high-dimensional system. We first show that single-harmonic PDDP rules can approximate a simple form of symmetric STDP, while multiharmonic rules are required to accurately approximate causal STDP. We then derive exact expressions for the evolution of the average PDDP coupling weight in terms of network synchrony. For adaptive networks of Kuramoto oscillators that form clusters, we formulate a family of low-dimensional descriptions based on the mean-field dynamics of each cluster and average coupling weights between and within clusters. Finally, we show that such a two-cluster mean-field model can be fitted to synthetic data to provide a low-dimensional approximation of a full adaptive network with symmetric STDP. Our framework represents a step toward a low-dimensional description of adaptive networks with STDP, and could for example inform the development of new therapies aimed at maximizing the long-lasting effects of brain stimulation.},
  archive      = {J_NECO},
  author       = {Duchet, Benoit and Bick, Christian and Byrne, Áine},
  doi          = {10.1162/neco_a_01601},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1481-1528},
  shortjournal = {Neural Comput.},
  title        = {Mean-field approximations with adaptive coupling for networks with spike-timing-dependent plasticity},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention in a family of boltzmann machines emerging from
modern hopfield networks. <em>NECO</em>, <em>35</em>(8), 1463–1480. (<a
href="https://doi.org/10.1162/neco_a_01597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hopfield networks and Boltzmann machines (BMs) are fundamental energy-based neural network models. Recent studies on modern Hopfield networks have broadened the class of energy functions and led to a unified perspective on general Hopfield networks, including an attention module. In this letter, we consider the BM counterparts of modern Hopfield networks using the associated energy functions and study their salient properties from a trainability perspective. In particular, the energy function corresponding to the attention module naturally introduces a novel BM, which we refer to as the attentional BM (AttnBM). We verify that AttnBM has a tractable likelihood function and gradient for certain special cases and is easy to train. Moreover, we reveal the hidden connections between AttnBM and some single-layer models, namely the gaussian–Bernoulli restricted BM and the denoising autoencoder with softmax units coming from denoising score matching. We also investigate BMs introduced by other energy functions and show that the energy function of dense associative memory models gives BMs belonging to exponential family harmoniums.},
  archive      = {J_NECO},
  author       = {Ota, Toshihiro and Karakida, Ryo},
  doi          = {10.1162/neco_a_01597},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1463-1480},
  shortjournal = {Neural Comput.},
  title        = {Attention in a family of boltzmann machines emerging from modern hopfield networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximal memory capacity near the edge of chaos in balanced
cortical e-i networks. <em>NECO</em>, <em>35</em>(8), 1430–1462. (<a
href="https://doi.org/10.1162/neco_a_01596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the efficiency of information processing in a balanced excitatory and inhibitory (E-I) network during the developmental critical period, when network plasticity is heightened. A multimodule network composed of E-I neurons was defined, and its dynamics were examined by regulating the balance between their activities. When adjusting E-I activity, both transitive chaotic synchronization with a high Lyapunov dimension and conventional chaos with a low Lyapunov dimension were found. In between, the edge of high-dimensional chaos was observed. To quantify the efficiency of information processing, we applied a short-term memory task in reservoir computing to the dynamics of our network. We found that memory capacity was maximized when optimal E-I balance was realized, underscoring both its vital role and vulnerability during critical periods of brain development.},
  archive      = {J_NECO},
  author       = {Kanamaru, Takashi and Hensch, Takao K. and Aihara, Kazuyuki},
  doi          = {10.1162/neco_a_01596},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1430-1462},
  shortjournal = {Neural Comput.},
  title        = {Maximal memory capacity near the edge of chaos in balanced cortical E-I networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-regularized tensor regression: A domain-aware
framework for interpretable modeling of multiway data on graphs.
<em>NECO</em>, <em>35</em>(8), 1404–1429. (<a
href="https://doi.org/10.1162/neco_a_01598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data analytics applications are increasingly characterized by exceedingly large and multidimensional data sources. This represents a challenge for traditional machine learning models, as the number of model parameters needed to process such data grows exponentially with the data dimensions, an effect known as the curse of dimensionality. Recently, tensor decomposition (TD) techniques have shown promising results in reducing the computational costs associated with large-dimensional models while achieving comparable performance. However, such tensor models are often unable to incorporate the underlying domain knowledge when compressing high-dimensional models. To this end, we introduce a novel graph-regularized tensor regression (GRTR) framework, whereby domain knowledge about intramodal relations is incorporated into the model in the form of a graph Laplacian matrix. This is then used as a regularization tool to promote a physically meaningful structure within the model parameters. By virtue of tensor algebra, the proposed framework is shown to be fully interpretable, both coefficient-wise and dimension-wise. The GRTR model is validated in a multiway regression setting and compared against competing models and is shown to achieve improved performance at reduced computational costs. Detailed visualizations are provided to help readers gain an intuitive understanding of the employed tensor operations.},
  archive      = {J_NECO},
  author       = {Xu, Yao Lei and Konstantinidis, Kriton and Mandic, Danilo P.},
  doi          = {10.1162/neco_a_01598},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1404-1429},
  shortjournal = {Neural Comput.},
  title        = {Graph-regularized tensor regression: A domain-aware framework for interpretable modeling of multiway data on graphs},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal burstiness in populations of spiking neurons
facilitates decoding of decreases in tonic firing. <em>NECO</em>,
<em>35</em>(8), 1363–1403. (<a
href="https://doi.org/10.1162/neco_a_01595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stimulus can be encoded in a population of spiking neurons through any change in the statistics of the joint spike pattern, yet we commonly summarize single-trial population activity by the summed spike rate across cells: the population peristimulus time histogram (pPSTH). For neurons with a low baseline spike rate that encode a stimulus with a rate increase, this simplified representation works well, but for populations with high baseline rates and heterogeneous response patterns, the pPSTH can obscure the response. We introduce a different representation of the population spike pattern, which we call an “information train,” that is well suited to conditions of sparse responses, especially those that involve decreases rather than increases in firing. We use this tool to study populations with varying levels of burstiness in their spiking statistics to determine how burstiness affects the representation of spike decreases (firing “gaps”). Our simulated populations of spiking neurons varied in size, baseline rate, burst statistics, and correlation. Using the information train decoder, we find that there is an optimal level of burstiness for gap detection that is robust to several other parameters of the population. We consider this theoretical result in the context of experimental data from different types of retinal ganglion cells and determine that the baseline spike statistics of a recently identified type support nearly optimal detection of both the onset and strength of a contrast step.},
  archive      = {J_NECO},
  author       = {Durian, Sylvia C. L. and Agrios, Mark and Schwartz, Gregory W.},
  doi          = {10.1162/neco_a_01595},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1363-1403},
  shortjournal = {Neural Comput.},
  title        = {Optimal burstiness in populations of spiking neurons facilitates decoding of decreases in tonic firing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Posterior covariance information criterion for weighted
inference. <em>NECO</em>, <em>35</em>(7), 1340–1361. (<a
href="https://doi.org/10.1162/neco_a_01592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For predictive evaluation based on quasi-posterior distributions, we develop a new information criterion, the posterior covariance information criterion (PCIC). PCIC generalizes the widely applicable information criterion (WAIC) so as to effectively handle predictive scenarios where likelihoods for the estimation and the evaluation of the model may be different. A typical example of such scenarios is the weighted likelihood inference, including prediction under covariate shift and counterfactual prediction. The proposed criterion uses a posterior covariance form and is computed by using only one Markov chain Monte Carlo run. Through numerical examples, we demonstrate how PCIC can apply in practice. Further, we show that PCIC is asymptotically unbiased to the quasi-Bayesian generalization error under mild conditions in weighted inference with both regular and singular statistical models.},
  archive      = {J_NECO},
  author       = {Iba, Yukito and Yano, Keisuke},
  doi          = {10.1162/neco_a_01592},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1340-1361},
  shortjournal = {Neural Comput.},
  title        = {Posterior covariance information criterion for weighted inference},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep clustering with a constraint for topological invariance
based on symmetric InfoNCE. <em>NECO</em>, <em>35</em>(7), 1288–1339.
(<a href="https://doi.org/10.1162/neco_a_01591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the scenario of deep clustering, in which the available prior knowledge is limited. In this scenario, few existing state-of-the-art deep clustering methods can perform well for both noncomplex topology and complex topology data sets. To address the problem, we propose a constraint utilizing symmetric InfoNCE, which helps an objective of the deep clustering method in the scenario of training the model so as to be efficient for not only noncomplex topology but also complex topology data sets. Additionally, we provide several theoretical explanations of the reason that the constraint can enhances the performance of deep clustering methods. To confirm the effectiveness of the proposed constraint, we introduce a deep clustering method named MIST, which is a combination of an existing deep clustering method and our constraint. Our numerical experiments via MIST demonstrate that the constraint is effective. In addition, MIST outperforms other state-of-the-art deep clustering methods for most of the commonly used 10 benchmark data sets.},
  archive      = {J_NECO},
  author       = {Zhang, Yuhui and Wada, Yuichiro and Waida, Hiroki and Goto, Kaito and Hino, Yusaku and Kanamori, Takafumi},
  doi          = {10.1162/neco_a_01591},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1288-1339},
  shortjournal = {Neural Comput.},
  title        = {Deep clustering with a constraint for topological invariance based on symmetric InfoNCE},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization and learning with randomly compressed gradient
updates. <em>NECO</em>, <em>35</em>(7), 1234–1287. (<a
href="https://doi.org/10.1162/neco_a_01588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient descent methods are simple and efficient optimization algorithms with widespread applications. To handle high-dimensional problems, we study compressed stochastic gradient descent (SGD) with low-dimensional gradient updates. We provide a detailed analysis in terms of both optimization rates and generalization rates. To this end, we develop uniform stability bounds for CompSGD for both smooth and nonsmooth problems, based on which we develop almost optimal population risk bounds. Then we extend our analysis to two variants of SGD: batch and mini-batch gradient descent. Furthermore, we show that these variants achieve almost optimal rates compared to their high-dimensional gradient setting. Thus, our results provide a way to reduce the dimension of gradient updates without affecting the convergence rate in the generalization analysis. Moreover, we show that the same result also holds in the differentially private setting, which allows us to reduce the dimension of added noise with “almost free” cost.},
  archive      = {J_NECO},
  author       = {Huang, Zhanliang and Lei, Yunwen and Kabán, Ata},
  doi          = {10.1162/neco_a_01588},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1234-1287},
  shortjournal = {Neural Comput.},
  title        = {Optimization and learning with randomly compressed gradient updates},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conductance-based phenomenological nonspiking model: A
dimensionless and simple model that reliably predicts the effects of
conductance variations on nonspiking neuronal dynamics. <em>NECO</em>,
<em>35</em>(7), 1209–1233. (<a
href="https://doi.org/10.1162/neco_a_01589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modeling of single neurons has proven to be an indispensable tool in deciphering the mechanisms underlying neural dynamics and signal processing. In that sense, two types of single-neuron models are extensively used: the conductance-based models (CBMs) and the so-called phenomenological models, which are often opposed in their objectives and their use. Indeed, the first type aims to describe the biophysical properties of the neuron cell membrane that underlie the evolution of its potential, while the second one describes the macroscopic behavior of the neuron without taking into account all of its underlying physiological processes. Therefore, CBMs are often used to study “low-level” functions of neural systems, while phenomenological models are limited to the description of “high-level” functions. In this letter, we develop a numerical procedure to endow a dimensionless and simple phenomenological nonspiking model with the capability to describe the effect of conductance variations on nonspiking neuronal dynamics with high accuracy. The procedure allows determining a relationship between the dimensionless parameters of the phenomenological model and the maximal conductances of CBMs. In this way, the simple model combines the biological plausibility of CBMs with the high computational efficiency of phenomenological models, and thus may serve as a building block for studying both high-level and low-level functions of nonspiking neural networks. We also demonstrate this capability in an abstract neural network inspired by the retina and C. elegans networks, two important nonspiking nervous tissues.},
  archive      = {J_NECO},
  author       = {Naudin, Loïs and Raison-Aubry, Laetitia and Buhry, Laure},
  doi          = {10.1162/neco_a_01589},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1209-1233},
  shortjournal = {Neural Comput.},
  title        = {Conductance-based phenomenological nonspiking model: A dimensionless and simple model that reliably predicts the effects of conductance variations on nonspiking neuronal dynamics},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic modeling of spike count data with conway-maxwell
poisson variability. <em>NECO</em>, <em>35</em>(7), 1187–1208. (<a
href="https://doi.org/10.1162/neco_a_01593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many areas of the brain, neural spiking activity covaries with features of the external world, such as sensory stimuli or an animal&#39;s movement. Experimental findings suggest that the variability of neural activity changes over time and may provide information about the external world beyond the information provided by the average neural activity. To flexibly track time-varying neural response properties, we developed a dynamic model with Conway-Maxwell Poisson (CMP) observations. The CMP distribution can flexibly describe firing patterns that are both under- and overdispersed relative to the Poisson distribution. Here we track parameters of the CMP distribution as they vary over time. Using simulations, we show that a normal approximation can accurately track dynamics in state vectors for both the centering and shape parameters ( ⁠ λ and ν ⁠ ). We then fit our model to neural data from neurons in primary visual cortex, “place cells” in the hippocampus, and a speed-tuned neuron in the anterior pretectal nucleus. We find that this method outperforms previous dynamic models based on the Poisson distribution. The dynamic CMP model provides a flexible framework for tracking time-varying non-Poisson count data and may also have applications beyond neuroscience.},
  archive      = {J_NECO},
  author       = {Wei, Ganchao and Stevenson, Ian H.},
  doi          = {10.1162/neco_a_01593},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1187-1208},
  shortjournal = {Neural Comput.},
  title        = {Dynamic modeling of spike count data with conway-maxwell poisson variability},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient decoding of compositional structure in holistic
representations. <em>NECO</em>, <em>35</em>(7), 1159–1186. (<a
href="https://doi.org/10.1162/neco_a_01590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the task of retrieving information from compositional distributed representations formed by hyperdimensional computing/vector symbolic architectures and present novel techniques that achieve new information rate bounds. First, we provide an overview of the decoding techniques that can be used to approach the retrieval task. The techniques are categorized into four groups. We then evaluate the considered techniques in several settings that involve, for example, inclusion of external noise and storage elements with reduced precision. In particular, we find that the decoding techniques from the sparse coding and compressed sensing literature (rarely used for hyperdimensional computing/vector symbolic architectures) are also well suited for decoding information from the compositional distributed representations. Combining these decoding techniques with interference cancellation ideas from communications improves previously reported bounds (Hersche et al., 2021 ) of the information rate of the distributed representations from 1.20 to 1.40 bits per dimension for smaller codebooks and from 0.60 to 1.26 bits per dimension for larger codebooks.},
  archive      = {J_NECO},
  author       = {Kleyko, Denis and Bybee, Connor and Huang, Ping-Chen and Kymn, Christopher J. and Olshausen, Bruno A. and Frady, E. Paxon and Sommer, Friedrich T.},
  doi          = {10.1162/neco_a_01590},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1159-1186},
  shortjournal = {Neural Comput.},
  title        = {Efficient decoding of compositional structure in holistic representations},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalization analysis of pairwise learning for ranking
with deep neural networks. <em>NECO</em>, <em>35</em>(6), 1135–1158. (<a
href="https://doi.org/10.1162/neco_a_01585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise learning is widely employed in ranking, similarity and metric learning, area under the ROC curve (AUC) maximization, and many other learning tasks involving sample pairs. Pairwise learning with deep neural networks was considered for ranking, but enough theoretical understanding about this topic is lacking. In this letter, we apply symmetric deep neural networks to pairwise learning for ranking with a hinge loss ϕ h and carry out generalization analysis for this algorithm. A key step in our analysis is to characterize a function that minimizes the risk. This motivates us to first find the minimizer of ϕ h -risk and then design our two-part deep neural networks with shared weights, which induces the antisymmetric property of the networks. We present convergence rates of the approximation error in terms of function smoothness and a noise condition and give an excess generalization error bound by means of properties of the hypothesis space generated by deep neural networks. Our analysis is based on tools from U-statistics and approximation theory.},
  archive      = {J_NECO},
  author       = {Huang, Shuo and Zhou, Junyu and Feng, Han and Zhou, Ding-Xuan},
  doi          = {10.1162/neco_a_01585},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1135-1158},
  shortjournal = {Neural Comput.},
  title        = {Generalization analysis of pairwise learning for ranking with deep neural networks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning solution of the eigenvalue problem for
differential operators. <em>NECO</em>, <em>35</em>(6), 1100–1134. (<a
href="https://doi.org/10.1162/neco_a_01583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the eigenvalue problem for differential operators is a common problem in many scientific fields. Classical numerical methods rely on intricate domain discretization and yield nonanalytic or nonsmooth approximations. We introduce a novel neural network–based solver for the eigenvalue problem of differential self-adjoint operators, where the eigenpairs are learned in an unsupervised end-to-end fashion. We propose several training procedures for solving increasingly challenging tasks toward the general eigenvalue problem. The proposed solver is capable of finding the M smallest eigenpairs for a general differential operator. We demonstrate the method on the Laplacian operator, which is of particular interest in image processing, computer vision, and shape analysis among many other applications. In addition, we solve the Legendre differential equation. Our proposed method simultaneously solves several eigenpairs and can be easily used on free-form domains. We exemplify it on L-shape and circular cut domains. A significant contribution of this work is an analysis of the numerical error of this method. In particular an upper bound for the (unknown) solution error is given in terms of the (measured) truncation error of the partial differential equation and the network structure.},
  archive      = {J_NECO},
  author       = {Ben-Shaul, Ido and Bar, Leah and Fishelov, Dalia and Sochen, Nir},
  doi          = {10.1162/neco_a_01583},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1100-1134},
  shortjournal = {Neural Comput.},
  title        = {Deep learning solution of the eigenvalue problem for differential operators},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic hyperparameter tuning in sparse matrix
factorization. <em>NECO</em>, <em>35</em>(6), 1086–1099. (<a
href="https://doi.org/10.1162/neco_a_01581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of hyperparameter tuning in sparse matrix factorization under a Bayesian framework. In prior work, an analytical solution of sparse matrix factorization with Laplace prior was obtained by a variational Bayes method under several approximations. Based on this solution, we propose a novel numerical method of hyperparameter tuning by evaluating the zero point of the normalization factor in a sparse matrix prior. We also verify that our method shows excellent performance for ground-truth sparse matrix reconstruction by comparing it with the widely used algorithm of sparse principal component analysis.},
  archive      = {J_NECO},
  author       = {Kawasumi, Ryota and Takeda, Koujin},
  doi          = {10.1162/neco_a_01581},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1086-1099},
  shortjournal = {Neural Comput.},
  title        = {Automatic hyperparameter tuning in sparse matrix factorization},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensitivity to control signals in triphasic rhythmic neural
systems: A comparative mechanistic analysis via infinitesimal local
timing response curves. <em>NECO</em>, <em>35</em>(6), 1028–1085. (<a
href="https://doi.org/10.1162/neco_a_01586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar activity patterns may arise from model neural networks with distinct coupling properties and individual unit dynamics. These similar patterns may, however, respond differently to parameter variations and specifically to tuning of inputs that represent control signals. In this work, we analyze the responses resulting from modulation of a localized input in each of three classes of model neural networks that have been recognized in the literature for their capacity to produce robust three-phase rhythms: coupled fast-slow oscillators, near-heteroclinic oscillators, and threshold-linear networks. Triphasic rhythms, in which each phase consists of a prolonged activation of a corresponding subgroup of neurons followed by a fast transition to another phase, represent a fundamental activity pattern observed across a range of central pattern generators underlying behaviors critical to survival, including respiration, locomotion, and feeding. To perform our analysis, we extend the recently developed local timing response curve (lTRC), which allows us to characterize the timing effects due to perturbations, and we complement our lTRC approach with model-specific dynamical systems analysis. Interestingly, we observe disparate effects of similar perturbations across distinct model classes. Thus, this work provides an analytical framework for studying control of oscillations in nonlinear dynamical systems and may help guide model selection in future efforts to study systems exhibiting triphasic rhythmic activity.},
  archive      = {J_NECO},
  author       = {Yu, Zhuojun and Rubin, Jonathan E. and Thomas, Peter J.},
  doi          = {10.1162/neco_a_01586},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1028-1085},
  shortjournal = {Neural Comput.},
  title        = {Sensitivity to control signals in triphasic rhythmic neural systems: A comparative mechanistic analysis via infinitesimal local timing response curves},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable variational inference for low-rank spatiotemporal
receptive fields. <em>NECO</em>, <em>35</em>(6), 995–1027. (<a
href="https://doi.org/10.1162/neco_a_01584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important problem in systems neuroscience is to characterize how a neuron integrates sensory inputs across space and time. The linear receptive field provides a mathematical characterization of this weighting function and is commonly used to quantify neural response properties and classify cell types. However, estimating receptive fields is difficult in settings with limited data and correlated or high-dimensional stimuli. To overcome these difficulties, we propose a hierarchical model designed to flexibly parameterize low-rank receptive fields. The model includes gaussian process priors over spatial and temporal components of the receptive field, encouraging smoothness in space and time. We also propose a new temporal prior, temporal relevance determination, which imposes a variable degree of smoothness as a function of time lag. We derive a scalable algorithm for variational Bayesian inference for both spatial and temporal receptive field components and hyperparameters. The resulting estimator scales to high-dimensional settings in which full-rank maximum likelihood or a posteriori estimates are intractable. We evaluate our approach on neural data from rat retina and primate cortex and show that it substantially outperforms a variety of existing estimators. Our modeling approach will have useful extensions to a variety of other high-dimensional inference problems with smooth or low-rank structure.},
  archive      = {J_NECO},
  author       = {Duncker, Lea and Ruda, Kiersten M. and Field, Greg D. and Pillow, Jonathan W.},
  doi          = {10.1162/neco_a_01584},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {995-1027},
  shortjournal = {Neural Comput.},
  title        = {Scalable variational inference for low-rank spatiotemporal receptive fields},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification from positive and biased negative data with
skewed labeled posterior probability. <em>NECO</em>, <em>35</em>(5),
977–994. (<a href="https://doi.org/10.1162/neco_a_01580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The binary classification problem has a situation where only biased data are observed in one of the classes. In this letter, we propose a new method to approach the positive and biased negative (PbN) classification problem, which is a weakly supervised learning method to learn a binary classifier from positive data and negative data with biased observations. We incorporate a method to correct the negative influence due to a skewed confidence, which is represented by the posterior probability that the observed data are positive. This reduces the distortion of the posterior probability that the data are labeled, which is necessary for the empirical risk minimization of the PbN classification problem. We verified the effectiveness of the proposed method by synthetic and benchmark data experiments.},
  archive      = {J_NECO},
  author       = {Watanabe, Shotaro and Matsui, Hidetoshi},
  doi          = {10.1162/neco_a_01580},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {977-994},
  shortjournal = {Neural Comput.},
  title        = {Classification from positive and biased negative data with skewed labeled posterior probability},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Echo-enhanced embodied visual navigation. <em>NECO</em>,
<em>35</em>(5), 958–976. (<a
href="https://doi.org/10.1162/neco_a_01579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual navigation involves a movable robotic agent striving to reach a point goal (target location) using vision sensory input. While navigation with ideal visibility has seen plenty of success, it becomes challenging in suboptimal visual conditions like poor illumination, where traditional approaches suffer from severe performance degradation. We propose E3VN (echo-enhanced embodied visual navigation) to effectively perceive the surroundings even under poor visibility to mitigate this problem. This is made possible by adopting an echoer that actively perceives the environment via auditory signals. E3VN models the robot agent as playing a cooperative Markov game with that echoer. The action policies of robot and echoer are jointly optimized to maximize the reward in a two-stream actor-critic architecture. During optimization, the reward is also adaptively decomposed into the robot and echoer parts. Our experiments and ablation studies show that E3VN is consistently effective and robust in point goal navigation tasks, especially under nonideal visibility.},
  archive      = {J_NECO},
  author       = {Yu, Yinfeng and Cao, Lele and Sun, Fuchun and Yang, Chao and Lai, Huicheng and Huang, Wenbing},
  doi          = {10.1162/neco_a_01579},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {958-976},
  shortjournal = {Neural Comput.},
  title        = {Echo-enhanced embodied visual navigation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From pavlov conditioning to hebb learning. <em>NECO</em>,
<em>35</em>(5), 930–957. (<a
href="https://doi.org/10.1162/neco_a_01578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hebb&#39;s learning traces its origin in Pavlov&#39;s classical conditioning; however, while the former has been extensively modeled in the past decades (e.g., by the Hopfield model and countless variations on theme), as for the latter, modeling has remained largely unaddressed so far. Furthermore, a mathematical bridge connecting these two pillars is totally lacking. The main difficulty toward this goal lies in the intrinsically different scales of the information involved: Pavlov&#39;s theory is about correlations between concepts that are (dynamically) stored in the synaptic matrix as exemplified by the celebrated experiment starring a dog and a ringing bell; conversely, Hebb&#39;s theory is about correlations between pairs of neurons as summarized by the famous statement that neurons that fire together wire together . In this letter, we rely on stochastic process theory to prove that as long as we keep neurons&#39; and synapses&#39; timescales largely split, Pavlov&#39;s mechanism spontaneously takes place and ultimately gives rise to synaptic weights that recover the Hebbian kernel.},
  archive      = {J_NECO},
  author       = {Agliari, Elena and Aquaro, Miriam and Barra, Adriano and Fachechi, Alberto and Marullo, Chiara},
  doi          = {10.1162/neco_a_01578},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {930-957},
  shortjournal = {Neural Comput.},
  title        = {From pavlov conditioning to hebb learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strong allee effect synaptic plasticity rule in an
unsupervised learning environment. <em>NECO</em>, <em>35</em>(5),
896–929. (<a href="https://doi.org/10.1162/neco_a_01577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synaptic plasticity, or the ability of a brain to change one or more of its functions or structures at the synaptic level, has generated and is still generating a lot of interest from the scientific community especially from neuroscientists. These interests went into high gear after empirical evidence was collected that challenged the established paradigm that human brain structures and functions are set from childhood and only modest changes were expected beyond. Early synaptic plasticity rules or laws to that regard include the basic Hebbian rule that proposed a mechanism for strengthening or weakening of synapses (weights) during learning and memory. This rule, however, did not account for the fact that weights must have bounded growth over time. Thereafter, many other rules that possess other desirable properties were proposed to complement the basic Hebbian rule. In particular, a desirable property in a synaptic plasticity rule is that the ambient system must account for inhibition, which is often achieved if the rule used allows for a lower bound in synaptic weights. To that regard, in this letter, we propose such a synaptic plasticity rule that is inspired by the Allee effect, a phenomenon often observed in population dynamics. We show that properties such as synaptic normalization, competition between weights, decorrelation potential, and dynamic stability are satisfied. We show that in fact, an Allee effect in synaptic plasticity can be construed as an absence of plasticity.},
  archive      = {J_NECO},
  author       = {Kwessi, Eddy},
  doi          = {10.1162/neco_a_01577},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {896-929},
  shortjournal = {Neural Comput.},
  title        = {Strong allee effect synaptic plasticity rule in an unsupervised learning environment},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Body mechanics, optimality, and sensory feedback in the
human control of complex objects. <em>NECO</em>, <em>35</em>(5),
853–895. (<a href="https://doi.org/10.1162/neco_a_01576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are adept at a wide variety of motor skills, including the handling of complex objects and using tools. Advances to understand the control of voluntary goal-directed movements have focused on simple behaviors such as reaching, uncoupled to any additional object dynamics. Under these simplified conditions, basic elements of motor control, such as the roles of body mechanics, objective functions, and sensory feedback, have been characterized. However, these elements have mostly been examined in isolation, and the interactions between these elements have received less attention. This study examined a task with internal dynamics, inspired by the daily skill of transporting a cup of coffee, with additional expected or unexpected perturbations to probe the structure of the controller. Using optimal feedback control (OFC) as the basis, it proved necessary to endow the model of the body with mechanical impedance to generate the kinematic features observed in the human experimental data. The addition of mechanical impedance revealed that simulated movements were no longer sensitively dependent on the objective function, a highly debated cornerstone of optimal control. Further, feedforward replay of the control inputs was similarly successful in coping with perturbations as when feedback, or sensory information, was included. These findings suggest that when the control model incorporates a representation of the mechanical properties of the limb, that is, embodies its dynamics, the specific objective function and sensory feedback become less critical, and complex interactions with dynamic objects can be successfully managed.},
  archive      = {J_NECO},
  author       = {Razavian, Reza Sharif and Sadeghi, Mohsen and Bazzi, Salah and Nayeem, Rashida and Sternad, Dagmar},
  doi          = {10.1162/neco_a_01576},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {853-895},
  shortjournal = {Neural Comput.},
  title        = {Body mechanics, optimality, and sensory feedback in the human control of complex objects},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reward maximization through discrete active inference.
<em>NECO</em>, <em>35</em>(5), 807–852. (<a
href="https://doi.org/10.1162/neco_a_01574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active inference is a probabilistic framework for modeling the behavior of biological and artificial agents, which derives from the principle of minimizing free energy. In recent years, this framework has been applied successfully to a variety of situations where the goal was to maximize reward, often offering comparable and sometimes superior performance to alternative approaches. In this article, we clarify the connection between reward maximization and active inference by demonstrating how and when active inference agents execute actions that are optimal for maximizing reward. Precisely, we show the conditions under which active inference produces the optimal solution to the Bellman equation, a formulation that underlies several approaches to model-based reinforcement learning and control. On partially observed Markov decision processes, the standard active inference scheme can produce Bellman optimal actions for planning horizons of 1 but not beyond. In contrast, a recently developed recursive active inference scheme (sophisticated inference) can produce Bellman optimal actions on any finite temporal horizon. We append the analysis with a discussion of the broader relationship between active inference and reinforcement learning.},
  archive      = {J_NECO},
  author       = {Da Costa, Lancelot and Sajid, Noor and Parr, Thomas and Friston, Karl and Smith, Ryan},
  doi          = {10.1162/neco_a_01574},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {807-852},
  shortjournal = {Neural Comput.},
  title        = {Reward maximization through discrete active inference},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modern artificial neural networks: Is evolution cleverer?
<em>NECO</em>, <em>35</em>(5), 763–806. (<a
href="https://doi.org/10.1162/neco_a_01575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning tools, particularly artificial neural networks (ANN), have become ubiquitous in many scientific disciplines, and machine learning-based techniques flourish not only because of the expanding computational power and the increasing availability of labeled data sets but also because of the increasingly powerful training algorithms and refined topologies of ANN. Some refined topologies were initially motivated by neuronal network architectures found in the brain, such as convolutional ANN. Later topologies of neuronal networks departed from the biological substrate and began to be developed independently as the biological processing units are not well understood or are not transferable to in silico architectures. In the field of neuroscience, the advent of multichannel recordings has enabled recording the activity of many neurons simultaneously and characterizing complex network activity in biological neural networks (BNN). The unique opportunity to compare large neuronal network topologies, processing, and learning strategies with those that have been developed in state-of-the-art ANN has become a reality. The aim of this review is to introduce certain basic concepts of modern ANN, corresponding training algorithms, and biological counterparts. The selection of these modern ANN is prone to be biased (e.g., spiking neural networks are excluded) but may be sufficient for a concise overview.},
  archive      = {J_NECO},
  author       = {Bahmer, Andreas and Gupta, Daya and Effenberger, Felix},
  doi          = {10.1162/neco_a_01575},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {763-806},
  shortjournal = {Neural Comput.},
  title        = {Modern artificial neural networks: Is evolution cleverer?},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference and learning for generative capsule models.
<em>NECO</em>, <em>35</em>(4), 727–761. (<a
href="https://doi.org/10.1162/neco_a_01564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule networks (see Hinton et al., 2018 ) aim to encode knowledge of and reason about the relationship between an object and its parts. In this letter, we specify a generative model for such data and derive a variational algorithm for inferring the transformation of each model object in a scene and the assignments of observed parts to the objects. We derive a learning algorithm for the object models, based on variational expectation maximization (Jordan et al., 1999 ). We also study an alternative inference algorithm based on the RANSAC method of Fischler and Bolles ( 1981 ). We apply these inference methods to data generated from multiple geometric objects like squares and triangles (“constellations”) and data from a parts-based model of faces. Recent work by Kosiorek et al. ( 2019 ) has used amortized inference via stacked capsule autoencoders to tackle this problem; our results show that we significantly outperform them where we can make comparisons (on the constellations data).},
  archive      = {J_NECO},
  author       = {Nazabal, Alfredo and Tsagkas, Nikolaos and Williams, Christopher K. I.},
  doi          = {10.1162/neco_a_01564},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {727-761},
  shortjournal = {Neural Comput.},
  title        = {Inference and learning for generative capsule models},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multistream-based marked point process with decomposed
cumulative hazard functions. <em>NECO</em>, <em>35</em>(4), 699–726. (<a
href="https://doi.org/10.1162/neco_a_01572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When applying a point process to a real-world problem, an appropriate intensity function model should be designed based on physical and mathematical prior knowledge. Recently, a fully trainable deep learning–based approach has been developed for temporal point processes. In this approach, a cumulative hazard function (CHF) capable of systematic computation of adaptive intensity function is modeled in a data-driven manner. However, in this approach, although many applications of point processes generate various kinds of information such as location, magnitude, and depth, the mark information of events is not considered. To overcome this limitation, we propose a fully trainable marked point process method for modeling decomposed CHFs for time and mark prediction using multistream deep neural networks. We demonstrate the effectiveness of the proposed method through experiments with synthetic and real-world event data.},
  archive      = {J_NECO},
  author       = {Hachiya, Hirotaka and Hong, Sujun},
  doi          = {10.1162/neco_a_01572},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {699-726},
  shortjournal = {Neural Comput.},
  title        = {Multistream-based marked point process with decomposed cumulative hazard functions},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Formal verification of deep brain stimulation controllers
for parkinson’s disease treatment. <em>NECO</em>, <em>35</em>(4),
671–698. (<a href="https://doi.org/10.1162/neco_a_01569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep brain stimulation (DBS) is a widely accepted treatment for the Parkinson&#39;s disease (PD). Traditionally, it is done in an open-loop manner, where stimulation is always ON, irrespective of the patient needs. As a consequence, patients can feel some side effects due to the continuous high-frequency stimulation. Closed-loop DBS can address this problem as it allows adjusting stimulation according to the patient need. The selection of open- or closed-loop DBS and an optimal algorithm for closed-loop DBS are some of the main challenges in DBS controller design, and typically the decision is made through sampling based simulations. In this letter, we used model checking, a formal verification technique used to exhaustively explore the complete state space of a system, for analyzing DBS controllers. We analyze the timed automata of the open-loop and closed-loop DBS controllers in response to the basal ganglia (BG) model. Furthermore, we present a formal verification approach for the closed-loop DBS controllers using timed computation tree logic (TCTL) properties, that is, safety, liveness (the property that under certain conditions, some event will eventually occur), and deadlock freeness. We show that the closed-loop DBS significantly outperforms existing open-loop DBS controllers in terms of energy efficiency. Moreover, we formally analyze the closed-loop DBS for energy efficiency and time behavior with two algorithms, the constant update algorithm and the error prediction update algorithm. Our results demonstrate that the closed-loop DBS running the error prediction update algorithm is efficient in terms of time and energy as compared to the constant update algorithm.},
  archive      = {J_NECO},
  author       = {Nawaz, Arooj and Hasan, Osman and Jabeen, Shaista},
  doi          = {10.1162/neco_a_01569},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {671-698},
  shortjournal = {Neural Comput.},
  title        = {Formal verification of deep brain stimulation controllers for parkinson&#39;s disease treatment},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Macroscopic gamma oscillation with bursting neuron model
under stochastic fluctuation. <em>NECO</em>, <em>35</em>(4), 645–670.
(<a href="https://doi.org/10.1162/neco_a_01570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gamma oscillations are thought to play a role in information processing in the brain. Bursting neurons, which exhibit periodic clusters of spiking activity, are a type of neuron that are thought to contribute largely to gamma oscillations. However, little is known about how the properties of bursting neurons affect the emergence of gamma oscillation, its waveforms, and its synchronized characteristics, especially when subjected to stochastic fluctuations. In this study, we proposed a bursting neuron model that can analyze the bursting ratio and the phase response function. Then we theoretically analyzed the neuronal population dynamics composed of bursting excitatory neurons, mixed with inhibitory neurons. The bifurcation analysis of the equivalent Fokker-Planck equation exhibits three types of gamma oscillations of unimodal firing, bimodal firing in the inhibitory population, and bimodal firing in the excitatory population under different interaction strengths. The analyses of the macroscopic phase response function by the adjoint method of the Fokker-Planck equation revealed that the inhibitory doublet facilitates synchronization of the high-frequency oscillations. When we keep the strength of interactions constant, decreasing the bursting ratio of the individual neurons increases the relative high-gamma component of the populational phase-coupling functions. This also improves the ability of the neuronal population model to synchronize with faster oscillatory input. The analytical frameworks in this study provide insight into nontrivial dynamics of the population of bursting neurons, which further suggest that bursting neurons have an important role in rhythmic activities.},
  archive      = {J_NECO},
  author       = {Yoshikai, Yuto and Zheng, Tianyi and Kotani, Kiyoshi and Jimbo, Yasuhiko},
  doi          = {10.1162/neco_a_01570},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {645-670},
  shortjournal = {Neural Comput.},
  title        = {Macroscopic gamma oscillation with bursting neuron model under stochastic fluctuation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heuristic tree-partition-based parallel method for
biophysically detailed neuron simulation. <em>NECO</em>, <em>35</em>(4),
627–644. (<a href="https://doi.org/10.1162/neco_a_01565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biophysically detailed neuron simulation is a powerful tool to explore the mechanisms behind biological experiments and bridge the gap between various scales in neuroscience research. However, the extremely high computational complexity of detailed neuron simulation restricts the modeling and exploration of detailed network models. The bottleneck is solving the system of linear equations. To accelerate detailed simulation, we propose a heuristic tree-partition-based parallel method (HTP) to parallelize the computation of the Hines algorithm, the kernel for solving linear equations, and leverage the strong parallel capability of the graphic processing unit (GPU) to achieve further speedup. We formulate the problem of how to get a fine parallel process as a tree-partition problem. Next, we present a heuristic partition algorithm to obtain an effective partition to efficiently parallelize the equation-solving process in detailed simulation. With further optimization on GPU, our HTP method achieves 2.2 to 8.5 folds speedup compared to the state-of-the-art GPU method and 36 to 660 folds speedup compared to the typical Hines algorithm.},
  archive      = {J_NECO},
  author       = {Zhang, Yichen and Du, Kai and Huang, Tiejun},
  doi          = {10.1162/neco_a_01565},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {627-644},
  shortjournal = {Neural Comput.},
  title        = {Heuristic tree-partition-based parallel method for biophysically detailed neuron simulation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning of temporal abstractions with
slot-based transformers. <em>NECO</em>, <em>35</em>(4), 593–626. (<a
href="https://doi.org/10.1162/neco_a_01567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of reusable subroutines simplifies decision making and planning in complex reinforcement learning problems. Previous approaches propose to learn such temporal abstractions in an unsupervised fashion through observing state-action trajectories gathered from executing a policy. However, a current limitation is that they process each trajectory in an entirely sequential manner, which prevents them from revising earlier decisions about subroutine boundary points in light of new incoming information. In this work, we propose slot-based transformer for temporal abstraction (SloTTAr), a fully parallel approach that integrates sequence processing transformers with a slot attention module to discover subroutines in an unsupervised fashion while leveraging adaptive computation for learning about the number of such subroutines solely based on their empirical distribution. We demonstrate how SloTTAr is capable of outperforming strong baselines in terms of boundary point discovery, even for sequences containing variable amounts of subroutines, while being up to seven times faster to train on existing benchmarks.},
  archive      = {J_NECO},
  author       = {Gopalakrishnan, Anand and Irie, Kazuki and Schmidhuber, Jürgen and van Steenkiste, Sjoerd},
  doi          = {10.1162/neco_a_01567},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {593-626},
  shortjournal = {Neural Comput.},
  title        = {Unsupervised learning of temporal abstractions with slot-based transformers},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneity in neuronal dynamics is learned by gradient
descent for temporal processing tasks. <em>NECO</em>, <em>35</em>(4),
555–592. (<a href="https://doi.org/10.1162/neco_a_01571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual neurons in the brain have complex intrinsic dynamics that are highly diverse. We hypothesize that the complex dynamics produced by networks of complex and heterogeneous neurons may contribute to the brain&#39;s ability to process and respond to temporally complex data. To study the role of complex and heterogeneous neuronal dynamics in network computation, we develop a rate-based neuronal model, the generalized-leaky-integrate-and-fire-rate (GLIFR) model, which is a rate equivalent of the generalized-leaky-integrate-and-fire model. The GLIFR model has multiple dynamical mechanisms, which add to the complexity of its activity while maintaining differentiability. We focus on the role of after-spike currents, currents induced or modulated by neuronal spikes, in producing rich temporal dynamics. We use machine learning techniques to learn both synaptic weights and parameters underlying intrinsic dynamics to solve temporal tasks. The GLIFR model allows the use of standard gradient descent techniques rather than surrogate gradient descent, which has been used in spiking neural networks. After establishing the ability to optimize parameters using gradient descent in single neurons, we ask how networks of GLIFR neurons learn and perform on temporally challenging tasks, such as sequential MNIST. We find that these networks learn diverse parameters, which gives rise to diversity in neuronal dynamics, as demonstrated by clustering of neuronal parameters. GLIFR networks have mixed performance when compared to vanilla recurrent neural networks, with higher performance in pixel-by-pixel MNIST but lower in line-by-line MNIST. However, they appear to be more robust to random silencing. We find that the ability to learn heterogeneity and the presence of after-spike currents contribute to these gains in performance. Our work demonstrates both the computational robustness of neuronal complexity and diversity in networks and a feasible method of training such models using exact gradients.},
  archive      = {J_NECO},
  author       = {Winston, Chloe N. and Mastrovito, Dana and Shea-Brown, Eric and Mihalas, Stefan},
  doi          = {10.1162/neco_a_01571},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {555-592},
  shortjournal = {Neural Comput.},
  title        = {Heterogeneity in neuronal dynamics is learned by gradient descent for temporal processing tasks},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TruthSift: A platform for collective rationality.
<em>NECO</em>, <em>35</em>(3), 536–553. (<a
href="https://doi.org/10.1162/neco_a_01562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TruthSift is a cloud-based platform that logically combines members’ contributions into a collective intelligence. Members add statements and directed connectors to diagrams. TruthSift monitors which statements have been logically established by demonstrations for which every challenge raised has been refuted by an established refutation. When members run out of rational objections, the result is a converged diagram succinctly representing the state of knowledge about a topic, including plausible challenges and how they were refuted. Previous computer systems for collaborative intelligence did not have a qualitatively better solution for combining contributions than voting and are subject to groupthink, interest group capture, and inability to follow a multistep logical argument. They did not settle issues automatically point by point and logically propagate the consequences. I review indications that many practically important statements most people believe to be firmly established will be revealed to be firmly refuted upon computer-assisted scrutiny. TruthSift also supports construction of powerful probabilistic models over networks of causes, implications, tests, and necessary factors.},
  archive      = {J_NECO},
  author       = {Baum, Eric B.},
  doi          = {10.1162/neco_a_01562},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {536-553},
  shortjournal = {Neural Comput.},
  title        = {TruthSift: A platform for collective rationality},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward network intelligence. <em>NECO</em>, <em>35</em>(3),
525–535. (<a href="https://doi.org/10.1162/neco_a_01536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a conceptual framework to guide research in neural computation by relating it to mathematical progress in other fields and to examples illustrative of biological networks. The goal is to provide insight into how biological networks, and possibly large artificial networks such as foundation models, transition from analog computation to an analog approximation of symbolic computation. From the mathematical perspective, I focus on the development of consistent symbolic representations and optimal policies for action selection within network settings. From the biological perspective, I give examples of human and animal social network behavior that may be described using these mathematical models.},
  archive      = {J_NECO},
  author       = {Pentland, Alex},
  doi          = {10.1162/neco_a_01536},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {525-535},
  shortjournal = {Neural Comput.},
  title        = {Toward network intelligence},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IAN: Iterated adaptive neighborhoods for manifold learning
and dimensionality estimation. <em>NECO</em>, <em>35</em>(3), 453–524.
(<a href="https://doi.org/10.1162/neco_a_01566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invoking the manifold assumption in machine learning requires knowledge of the manifold&#39;s geometry and dimension, and theory dictates how many samples are required. However, in most applications, the data are limited, sampling may not be uniform, and the manifold&#39;s properties are unknown; this implies that neighborhoods must adapt to the local structure. We introduce an algorithm for inferring adaptive neighborhoods for data given by a similarity kernel. Starting with a locally conservative neighborhood (Gabriel) graph, we sparsify it iteratively according to a weighted counterpart. In each step, a linear program yields minimal neighborhoods globally, and a volumetric statistic reveals neighbor outliers likely to violate manifold geometry. We apply our adaptive neighborhoods to nonlinear dimensionality reduction, geodesic computation, and dimension estimation. A comparison against standard algorithms using, for example, k -nearest neighbors, demonstrates the usefulness of our approach.},
  archive      = {J_NECO},
  author       = {Dyballa, Luciano and Zucker, Steven W.},
  doi          = {10.1162/neco_a_01566},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {453-524},
  shortjournal = {Neural Comput.},
  title        = {IAN: Iterated adaptive neighborhoods for manifold learning and dimensionality estimation},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to represent part-whole hierarchies in a neural network.
<em>NECO</em>, <em>35</em>(3), 413–452. (<a
href="https://doi.org/10.1162/neco_a_01557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article does not describe a working system. Instead, it presents a single idea about representation that allows advances made by several different groups to be combined into an imaginary system called GLOM. 1 The advances include transformers, neural fields, contrastive representation learning, distillation, and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy that has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.},
  archive      = {J_NECO},
  author       = {Hinton, Geoffrey},
  doi          = {10.1162/neco_a_01557},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {413-452},
  shortjournal = {Neural Comput.},
  title        = {How to represent part-whole hierarchies in a neural network},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward a biomimetic neural circuit model of sensory-motor
processing. <em>NECO</em>, <em>35</em>(3), 384–412. (<a
href="https://doi.org/10.1162/neco_a_01516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational models have been a mainstay of research on smooth pursuit eye movements in monkeys. Pursuit is a sensory-motor system that is driven by the visual motion of small targets. It creates a smooth eye movement that accelerates up to target speed and tracks the moving target essentially perfectly. In this review of my laboratory&#39;s research, I trace the development of computational models of pursuit eye movements from the early control-theory models to the most recent neural circuit models. I outline a combined experimental and computational plan to move the models to the next level. Finally, I explain why research on nonhuman primates is so critical to the development of the neural circuit models I think we need.},
  archive      = {J_NECO},
  author       = {Lisberger, Stephen G.},
  doi          = {10.1162/neco_a_01516},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {384-412},
  shortjournal = {Neural Comput.},
  title        = {Toward a biomimetic neural circuit model of sensory-motor processing},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuromorphic engineering: In memory of misha mahowald.
<em>NECO</em>, <em>35</em>(3), 343–383. (<a
href="https://doi.org/10.1162/neco_a_01553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We review the coevolution of hardware and software dedicated to neuromorphic systems. From modest beginnings, these disciplines have become central to the larger field of computation. In the process, their biological foundations become more relevant, and their realizations increasingly overlap. We identify opportunities for significant steps forward in both the near and more distant future.},
  archive      = {J_NECO},
  author       = {Mead, Carver},
  doi          = {10.1162/neco_a_01553},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {343-383},
  shortjournal = {Neural Comput.},
  title        = {Neuromorphic engineering: In memory of misha mahowald},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large language models and the reverse turing test.
<em>NECO</em>, <em>35</em>(3), 309–342. (<a
href="https://doi.org/10.1162/neco_a_01563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.},
  archive      = {J_NECO},
  author       = {Sejnowski, Terrence J.},
  doi          = {10.1162/neco_a_01563},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {309-342},
  shortjournal = {Neural Comput.},
  title        = {Large language models and the reverse turing test},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding and applying deep learning. <em>NECO</em>,
<em>35</em>(3), 287–308. (<a
href="https://doi.org/10.1162/neco_a_01518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past 10 years have witnessed an explosion in deep learning neural network model development. The most common perceptual models with vision, speech, and text inputs are not general-purpose AI systems but tools. They automatically extract clues from inputs and compute probabilities of class labels. Successful applications require representative training data, an understanding of the limitations and capabilities of deep learning, and careful attention to a complex development process. The goal of this view is to foster an intuitive understanding of convolutional network deep learning models and how to use them with the goal of engaging a wider creative community. A focus is to make it possible for experts in areas such as health, education, poverty, and agriculture to understand the process of deep learning model development so they can help transition effective solutions to practice.},
  archive      = {J_NECO},
  author       = {Lippmann, Richard},
  doi          = {10.1162/neco_a_01518},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {287-308},
  shortjournal = {Neural Comput.},
  title        = {Understanding and applying deep learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feelings are the source of consciousness. <em>NECO</em>,
<em>35</em>(3), 277–286. (<a
href="https://doi.org/10.1162/neco_a_01521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this view, we address the problem of consciousness, and although we focus on its human presentation, we note that the phenomenon is present in numerous nonhuman species and use findings from a variety of animal studies to explain our hypothesis for how consciousness is made. Consciousness occurs when mind contents, such as perceptions and thoughts, are spontaneously identified as belonging to a specific organism/owner. Conscious minds are said to have a self that experiences mental events. We hypothesize that the automatic identification that associates minds and organisms is provided by a continuous flow of homeostatic feelings. Those feelings arise from the uninterrupted process of life regulation and correspond to both salient physiological fluctuations such as hunger, pain, well-being, or malaise, as well as to states closer to metabolic equilibrium and best described as feelings of life/existence, such as breathing or body temperature. We also hypothesize that homeostatic feelings were the inaugural phenomena of consciousness in biological evolution and venture that they were selected because the information they provided regarding the current state of life regulation conferred extraordinary advantages to the organisms so endowed. The “knowledge” carried by conscious homeostatic feelings provided “overt” guidance for life regulation, an advance over the covert regulation present in nonconscious organisms. Finally, we outline a mechanism for the generation of feelings based on a two-way interaction between interoceptive components of the nervous system and a particular set of nonneural components of the organism&#39;s interior, namely, viscera and circulating chemical molecules involved in their operations. Feelings emerge from this interaction as continuous and hybrid phenomena, related simultaneously to two series of events. The first is best described by the terms neural/representational/and mental and the second by the terms nonneural/visceral/and chemical . We note that this account offers a solution for the mind-body problem: homeostatic feelings constitute the “mental” version of bodily processes.},
  archive      = {J_NECO},
  author       = {Damasio, Antonio and Damasio, Hanna},
  doi          = {10.1162/neco_a_01521},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {277-286},
  shortjournal = {Neural Comput.},
  title        = {Feelings are the source of consciousness},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying and localizing multiple objects using artificial
ventral and dorsal cortical visual pathways. <em>NECO</em>,
<em>35</em>(2), 249–275. (<a
href="https://doi.org/10.1162/neco_a_01559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our previous study (Han &amp; Sereno, 2022a), we found that two artificial cortical visual pathways trained for either identity or space actively retain information about both identity and space independently and differently. We also found that this independently and differently retained information about identity and space in two separate pathways may be necessary to accurately and optimally recognize and localize objects. One limitation of our previous study was that there was only one object in each visual image, whereas in reality, there may be multiple objects in a scene. In this study, we find we are able to generalize our findings to object recognition and localization tasks where multiple objects are present in each visual image. We constrain the binding problem by training the identity network pathway to report the identities of objects in a given order according to the relative spatial relationships between the objects, given that most visual cortical areas including high-level ventral steam areas retain spatial information. Under these conditions, we find that the artificial neural networks with two pathways for identity and space have better performance in multiple-objects recognition and localization tasks (higher average testing accuracy, lower testing accuracy variance, less training time) than the artificial neural networks with a single pathway. We also find that the required number of training samples and the required training time increase quickly, and potentially exponentially, when the number of objects in each image increases, and we suggest that binding information from multiple objects simultaneously within any network (cortical area) induces conflict or competition and may be part of the reason why our brain has limited attentional and visual working memory capacities.},
  archive      = {J_NECO},
  author       = {Han, Zhixian and Sereno, Anne},
  doi          = {10.1162/neco_a_01559},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {249-275},
  shortjournal = {Neural Comput.},
  title        = {Identifying and localizing multiple objects using artificial ventral and dorsal cortical visual pathways},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic consolidation for continual learning. <em>NECO</em>,
<em>35</em>(2), 228–248. (<a
href="https://doi.org/10.1162/neco_a_01560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep learning models from a stream of nonstationary data is a critical problem to be solved to achieve general artificial intelligence. As a promising solution, the continual learning (CL) technique aims to build intelligent systems that have the plasticity to learn from new information without forgetting the previously obtained knowledge. Unfortunately, existing CL methods face two nontrivial limitations. First, when updating a model with new data, existing CL methods usually constrain the model parameters within the vicinity of the parameters optimized for old data, limiting the exploration ability of the model; second, the important strength of each parameter (used to consolidate the previously learned knowledge) is fixed and thus is suboptimal for the dynamic parameter updates. To address these limitations, we first relax the vicinity constraints with a global definition of the important strength, which allows us to explore the full parameter space. Specifically, we define the important strength as the sensitivity of the global loss function to the model parameters. Moreover, we propose adjusting the important strength adaptively to align it with the dynamic parameter updates. Through extensive experiments on popular data sets, we demonstrate that our proposed method outperforms the strong baselines by up to 24\% in terms of average accuracy.},
  archive      = {J_NECO},
  author       = {Li, Hang and Ma, Chen and Chen, Xi and Liu, Xue},
  doi          = {10.1162/neco_a_01560},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {228-248},
  shortjournal = {Neural Comput.},
  title        = {Dynamic consolidation for continual learning},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The tensor brain: A unified theory of perception, memory,
and semantic decoding. <em>NECO</em>, <em>35</em>(2), 156–227. (<a
href="https://doi.org/10.1162/neco_a_01552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a unified computational theory of an agent&#39;s perception and memory. In our model, both perception and memory are realized by different operational modes of the oscillating interactions between a symbolic index layer and a subsymbolic representation layer. The two layers form a bilayer tensor network (BTN). The index layer encodes indices for concepts, predicates, and episodic instances. The representation layer broadcasts information and reflects the cognitive brain state; it is our model of what authors have called the “mental canvas” or the “global workspace.” As a bridge between perceptual input and the index layer, the representation layer enables the grounding of indices by their subsymbolic embeddings, which are implemented as connection weights linking both layers. The propagation of activation to earlier perceptual processing layers in the brain can lead to embodiments of indices. Perception and memories first create subsymbolic representations, which are subsequently decoded semantically to produce sequences of activated indices that form symbolic triple statements. The brain is a sampling engine: only activated indices are communicated to the remaining parts of the brain. Triple statements are dynamically embedded in the representation layer and embodied in earlier processing layers: the brain speaks to itself. Although memory appears to be about the past, its main purpose is to support the agent in the present and the future. Recent episodic memory provides the agent with a sense of the here and now. Remote episodic memory retrieves relevant past experiences to provide information about possible future scenarios. This aids the agent in decision making. “Future” episodic memory, based on expected future events, guides planning and action. Semantic memory retrieves specific information, which is not delivered by current perception, and defines priors for future observations. We argue that it is important for the agent to encode individual entities, not just classes and attributes. Perception is learning: episodic memories are constantly being formed, and we demonstrate that a form of self-supervised learning can acquire new concepts and refine existing ones. We test our model on a standard benchmark data set, which we expanded to contain richer representations for attributes, classes, and individuals. Our key hypothesis is that obtaining a better understanding of perception and memory is a crucial prerequisite to comprehending human-level intelligence.},
  archive      = {J_NECO},
  author       = {Tresp, Volker and Sharifzadeh, Sahand and Li, Hang and Konopatzki, Dario and Ma, Yunpu},
  doi          = {10.1162/neco_a_01552},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {156-227},
  shortjournal = {Neural Comput.},
  title        = {The tensor brain: A unified theory of perception, memory, and semantic decoding},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal quadratic binding for relational reasoning in vector
symbolic neural architectures. <em>NECO</em>, <em>35</em>(2), 105–155.
(<a href="https://doi.org/10.1162/neco_a_01558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binding operation is fundamental to many cognitive processes, such as cognitive map formation, relational reasoning, and language comprehension. In these processes, two different modalities, such as location and objects, events and their contextual cues, and words and their roles, need to be bound together, but little is known about the underlying neural mechanisms. Previous work has introduced a binding model based on quadratic functions of bound pairs, followed by vector summation of multiple pairs. Based on this framework, we address the following questions: Which classes of quadratic matrices are optimal for decoding relational structures? And what is the resultant accuracy? We introduce a new class of binding matrices based on a matrix representation of octonion algebra, an eight-dimensional extension of complex numbers. We show that these matrices enable a more accurate unbinding than previously known methods when a small number of pairs are present. Moreover, numerical optimization of a binding operator converges to this octonion binding. We also show that when there are a large number of bound pairs, however, a random quadratic binding performs, as well as the octonion and previously proposed binding methods. This study thus provides new insight into potential neural mechanisms of binding operations in the brain.},
  archive      = {J_NECO},
  author       = {Hiratani, Naoki and Sompolinsky, Haim},
  doi          = {10.1162/neco_a_01558},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {105-155},
  shortjournal = {Neural Comput.},
  title        = {Optimal quadratic binding for relational reasoning in vector symbolic neural architectures},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian process koopman mode decomposition. <em>NECO</em>,
<em>35</em>(1), 82–103. (<a
href="https://doi.org/10.1162/neco_a_01555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a nonlinear probabilistic generative model of Koopman mode decomposition based on an unsupervised gaussian process. Existing data-driven methods for Koopman mode decomposition have focused on estimating the quantities specified by Koopman mode decomposition: eigenvalues, eigenfunctions, and modes. Our model enables the simultaneous estimation of these quantities and latent variables governed by an unknown dynamical system. Furthermore, we introduce an efficient strategy to estimate the parameters of our model by low-rank approximations of covariance matrices. Applying the proposed model to both synthetic data and a real-world epidemiological data set, we show that various analyses are available using the estimated parameters.},
  archive      = {J_NECO},
  author       = {Kawashima, Takahiro and Hino, Hideitsu},
  doi          = {10.1162/neco_a_01555},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {82-103},
  shortjournal = {Neural Comput.},
  title        = {Gaussian process koopman mode decomposition},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning with proper partial labels. <em>NECO</em>,
<em>35</em>(1), 58–81. (<a
href="https://doi.org/10.1162/neco_a_01554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial-label learning is a kind of weakly supervised learning with inexact labels, where for each training example, we are given a set of candidate labels instead of only one true label. Recently, various approaches on partial-label learning have been proposed under different generation models of candidate label sets. However, these methods require relatively strong distributional assumptions on the generation models. When the assumptions do not hold, the performance of the methods is not guaranteed theoretically. In this letter, we propose the notion of properness on partial labels. We show that this proper partial-label learning framework requires a weaker distributional assumption and includes many previous partial-label learning settings as special cases. We then derive a unified unbiased estimator of the classification risk. We prove that our estimator is risk consistent, and we also establish an estimation error bound. Finally, we validate the effectiveness of our algorithm through experiments.},
  archive      = {J_NECO},
  author       = {Wu, Zhenguo and Lv, Jiaqi and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01554},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {58-81},
  shortjournal = {Neural Comput.},
  title        = {Learning with proper partial labels},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive interpretation synthesis: Interpreting task
solving by quantifying previously used and unused information.
<em>NECO</em>, <em>35</em>(1), 38–57. (<a
href="https://doi.org/10.1162/neco_a_01542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep neural network is a good task solver, but it is difficult to make sense of its operation. People have different ideas about how to interpret its operation. We look at this problem from a new perspective where the interpretation of task solving is synthesized by quantifying how much and what previously unused information is exploited in addition to the information used to solve previous tasks. First, after learning several tasks, the network acquires several information partitions related to each task. We propose that the network then learns the minimal information partition that supplements previously learned information partitions to more accurately represent the input. This extra partition is associated with unconceptualized information that has not been used in previous tasks. We manage to identify what unconceptualized information is used and quantify the amount. To interpret how the network solves a new task, we quantify as meta-information how much information from each partition is extracted. We implement this framework with the variational information bottleneck technique. We test the framework with the MNIST and the CLEVR data set. The framework is shown to be able to compose information partitions and synthesize experience-dependent interpretation in the form of meta-information. This system progressively improves the resolution of interpretation upon new experience by converting a part of the unconceptualized information partition to a task-related partition. It can also provide a visual interpretation by imaging what is the part of previously unconceptualized information that is needed to solve a new task.},
  archive      = {J_NECO},
  author       = {He, Zhengqi and Toyoizumi, Taro},
  doi          = {10.1162/neco_a_01542},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {38-57},
  shortjournal = {Neural Comput.},
  title        = {Progressive interpretation synthesis: Interpreting task solving by quantifying previously used and unused information},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visuomotor mismatch responses as a hallmark of explaining
away in causal inference. <em>NECO</em>, <em>35</em>(1), 27–37. (<a
href="https://doi.org/10.1162/neco_a_01546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How are visuomotor mismatch responses in primary visual cortex embedded into cortical processing? We here show that mismatch responses can be understood as the result of a cooperation of motor and visual areas to jointly explain optic flow. This cooperation requires that optic flow is not explained redundantly by both areas, meaning that optic flow inputs to V1 that are predictable from motor neurons should be canceled (i.e., explained away). As a result, neurons in V1 represent only external causes of optic flow, which could allow the animal to easily detect movements that are independent of its own locomotion. We implement the proposed model in a spiking neural network, where coding errors are computed in dendrites and synaptic weights are learned with voltage-dependent plasticity rules. We find that both positive and negative mismatch responses arise, providing an alternative to the prevailing idea that visuomotor mismatch responses are linked to dedicated neurons for error computation. These results also provide a new perspective on several other recent observations of cross-modal neural interactions in cortex.},
  archive      = {J_NECO},
  author       = {Mikulasch, Fabian A. and Rudelt, Lucas and Priesemann, Viola},
  doi          = {10.1162/neco_a_01546},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {27-37},
  shortjournal = {Neural Comput.},
  title        = {Visuomotor mismatch responses as a hallmark of explaining away in causal inference},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the explainability of graph convolutional network with
GCN tangent kernel. <em>NECO</em>, <em>35</em>(1), 1–26. (<a
href="https://doi.org/10.1162/neco_a_01548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network (GCN) is a powerful deep model in dealing with graph data. However, the explainability of GCN remains a difficult problem since the training behaviors for graph neural networks are hard to describe. In this work, we show that for GCN with wide hidden feature dimension, the output for semisupervised problem can be described by a simple differential equation. In addition, the dynamics of the behavior of output is decided by the graph convolutional neural tangent kernel (GCNTK), which is stable when the width of hidden feature tends to be infinite. And the solution of node classification can be explained directly by the differential equation for a semisupervised problem. The experiments on some toy models speak to the consistency of the GCNTK model and GCN.},
  archive      = {J_NECO},
  author       = {Zhou, Xianchen and Wang, Hongxia},
  doi          = {10.1162/neco_a_01548},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Neural Comput.},
  title        = {On the explainability of graph convolutional network with GCN tangent kernel},
  volume       = {35},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
