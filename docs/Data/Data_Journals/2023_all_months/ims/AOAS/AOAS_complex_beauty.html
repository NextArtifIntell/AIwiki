<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aoas---160">AOAS - 160</h2>
<ul>
<li><details>
<summary>
(2023). Continuous-time modelling of behavioural responses in animal
movement. <em>AOAS</em>, <em>17</em>(4), 3570–3588. (<a
href="https://doi.org/10.1214/23-AOAS1776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is great interest in ecology to understand how wild animals are affected by anthropogenic disturbances, such as sounds. For example, behavioural response studies are an important approach to quantify the impact of naval activity on marine mammals. Controlled exposure experiments are undertaken where the behaviour of animals is quantified before, during, and after exposure to a controlled sound source, often using telemetry tags (e.g., accelerometers or satellite trackers). Statistical modelling is required to formally compare patterns before and after exposure, to quantify deviations from baseline behaviour. We propose varying-coefficient stochastic differential equations (SDEs) as a flexible framework to model such data with two components: (1) time-varying baseline dynamics, modelled with nonparametric or random effects of time-varying covariates, and (2) a nonparametric response model, which captures deviations from baseline. SDEs are specified in continuous time, which makes it straightforward to analyse data collected at irregular time intervals, a common situation for animal tracking studies. We describe how the model can be embedded into a state-space modelling framework to account for measurement error. We present inferential methods for model fitting, model checking, and uncertainty quantification (including on the response model). We apply this approach to two behavioural response study data sets on beaked whales: a satellite track and high-resolution depth data. Our results suggest that the whales’ horizontal movement and vertical diving behaviour changed after exposure to the sound source, and future work should evaluate the severity and possible consequences of these responses. These two very different examples showcase the versatility of varying-coefficient SDEs to measure changes in behaviour, and we discuss implications of disturbances for the whales’ energetic balance.},
  archive      = {J_AOAS},
  author       = {Théo Michelot and Richard Glennie and Len Thomas and Nicola Quick and Catriona M. Harris},
  doi          = {10.1214/23-AOAS1776},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3570-3588},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Continuous-time modelling of behavioural responses in animal movement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Debiased lasso for stratified cox models with application to
the national kidney transplant data. <em>AOAS</em>, <em>17</em>(4),
3550–3569. (<a href="https://doi.org/10.1214/23-AOAS1775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Scientific Registry of Transplant Recipients (SRTR) system has become a rich resource for understanding the complex mechanisms of graft failure after kidney transplant, a crucial step for allocating organs effectively and implementing appropriate care. As transplant centers that treated patients might strongly confound graft failures, Cox models stratified by centers can eliminate their confounding effects. Also, since recipient age is a proven nonmodifiable risk factor, a common practice is to fit models separately by recipient age groups. The moderate sample sizes, relative to the number of covariates, in some age groups may lead to biased maximum stratified partial likelihood estimates and unreliable confidence intervals, even when samples still outnumber covariates. To draw reliable inference on a comprehensive list of risk factors measured from both donors and recipients in SRTR, we propose a debiased lasso approach via quadratic programming for fitting stratified Cox models. We establish asymptotic properties and verify via simulations that our method produces consistent estimates and confidence intervals with nominal coverage probabilities. Accounting for nearly 100 confounders in SRTR, the debiased method detects that the graft failure hazard nonlinearly increases with donor’s age among all recipient age groups and that organs from older donors more adversely impact the younger recipients. Our method also delineates the associations between graft failure and many risk factors such as recipients’ primary diagnoses (e.g., polycystic disease, glomerular disease, and diabetes) and donor-recipient mismatches for human leukocyte antigen loci across recipient age groups. These results may inform the refinement of donor-recipient matching criteria for stakeholders.},
  archive      = {J_AOAS},
  author       = {Lu Xia and Bin Nan and Yi Li},
  doi          = {10.1214/23-AOAS1775},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3550-3569},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Debiased lasso for stratified cox models with application to the national kidney transplant data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Association and causation: Attributes and effects of judges
in equal employment opportunity commission litigation outcomes.
<em>AOAS</em>, <em>17</em>(4), 3526–3549. (<a
href="https://doi.org/10.1214/23-AOAS1774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large literature on judicial decision making asks if judges with different features of an attribute (e.g, sex, race) adjudicate cases differently. Researchers estimate models for case outcomes, interpreting coefficients associated with attributes as effects. But attributes are not treatments. While these coefficients indicate how judges with different features adjudicate the different cases they are assigned, ideally, different judges should be compared on a common set of cases. We construct a general methodology for making such comparisons, using it to study whether monetary relief in discrimination cases brought by the Equal Employment Opportunity Commission differs by judges’ race. For all federal judges (treatments) eligible to hear a case (unit), we define potential outcomes, using unit treatment effects between judges with different features to define a unit feature comparison (UFC), then using these to define new population estimands: the average (AFC) and quantile (QFC) feature comparisons. We estimate these quantities by combining observed case outcomes with missing potential outcomes imputed from the posterior predictive distribution of a two-part Bayesian hierarchical model. A case initially assigned to a non-white or African American judge is more likely to result in monetary relief than were that case initially assigned to an eligible white or non-African American judge. For the amount of relief, the 95\% posterior interval for the AFC covers 0, while the upper endpoint of the 95\% posterior interval for the median QFC is negative.},
  archive      = {J_AOAS},
  author       = {Michael E. Sobel and Gregory J. Wawro and Sean Farhang},
  doi          = {10.1214/23-AOAS1774},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3526-3549},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Association and causation: Attributes and effects of judges in equal employment opportunity commission litigation outcomes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accounting for seasonality in extreme sea-level estimation.
<em>AOAS</em>, <em>17</em>(4), 3500–3525. (<a
href="https://doi.org/10.1214/23-AOAS1773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable estimates of sea-level return-levels are crucial for coastal flooding risk assessments and for coastal flood defence design. We describe a novel method for estimating extreme sea-levels that is the first to capture seasonality, interannual variations and longer term changes. We use a joint probabilities method, with skew-surge and peak-tide as two sea-level components. The tidal regime is predictable, but skew-surges are stochastic. We present a statistical model for skew-surges, where the main body of the distribution is modelled empirically while a nonstationary generalised Pareto distribution (GPD) is used for the upper tail. We capture within-year seasonality by introducing a daily covariate to the GPD model and allowing the distribution of peak-tide to change over months and years. Skew-surge-peak-tide dependence is accounted for, via a tidal covariate, in the GPD model, and we adjust for skew-surge temporal dependence through the subasymptotic extremal index. We incorporate spatial prior information in our GPD model to reduce the uncertainty associated with the highest return-level estimates. Our results are an improvement on current return-level estimates, with previous methods typically underestimating. We illustrate our method at four U.K. tide gauges.},
  archive      = {J_AOAS},
  author       = {Eleanor D’Arcy and Jonathan A. Tawn and Amélie Joly and Dafni E. Sifnioti},
  doi          = {10.1214/23-AOAS1773},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3500-3525},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Accounting for seasonality in extreme sea-level estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A statistical approach to estimating adsorption-isotherm
parameters in gradient-elution preparative liquid chromatography.
<em>AOAS</em>, <em>17</em>(4), 3476–3499. (<a
href="https://doi.org/10.1214/23-AOAS1772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the adsorption isotherms is an issue of significant importance in preparative chromatography. A modern technique for estimating adsorption isotherms is to solve an inverse problem so that the simulated batch separation coincides with actual experimental results. However, due to the ill-posedness, the high nonlinearity, and the uncertainty quantification of the corresponding physical model, the existing deterministic inversion methods are usually inefficient in real-world applications. To overcome these difficulties and study the uncertainties of the adsorption-isotherm parameters, in this work, based on the Bayesian sampling framework, we propose a statistical approach for estimating the adsorption isotherms in various chromatography systems. Two modified Markov chain Monte Carlo algorithms are developed for a numerical realization of our statistical approach. Numerical experiments with both synthetic and real data are conducted and described to show the efficiency of the proposed new method.},
  archive      = {J_AOAS},
  author       = {Jiaji Su and Zhigang Yao and Cheng Li and Ye Zhang},
  doi          = {10.1214/23-AOAS1772},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3476-3499},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A statistical approach to estimating adsorption-isotherm parameters in gradient-elution preparative liquid chromatography},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compressed spectral screening for large-scale differential
correlation analysis with application in selecting glioblastoma gene
modules. <em>AOAS</em>, <em>17</em>(4), 3450–3475. (<a
href="https://doi.org/10.1214/23-AOAS1771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential coexpression analysis has been widely applied by scientists in understanding the biological mechanisms of diseases. However, the unknown differential patterns are often complicated; thus, models based on simplified parametric assumptions can be ineffective in identifying the differences. Meanwhile, the gene expression data involved in such analysis are in extremely high dimensions by nature, whose correlation matrices may not even be computable. Such a large scale seriously limits the application of most well-studied statistical methods. This paper introduces a simple yet powerful approach to the differential correlation analysis problem called compressed spectral screening. By leveraging spectral structures and random sampling techniques, our approach could achieve a highly accurate screening of features with complicated differential patterns while maintaining the scalability to analyze correlation matrices of 104–105 variables within a few minutes on a standard personal computer. We have applied this screening approach in comparing a TCGA data set about Glioblastoma with normal subjects. Our analysis successfully identifies multiple functional modules of genes that exhibit different coexpression patterns. The findings reveal new insights about Glioblastoma’s evolving mechanism. The validity of our approach is also justified by a theoretical analysis, showing that the compressed spectral analysis can achieve variable screening consistency.},
  archive      = {J_AOAS},
  author       = {Tianxi Li and Xiwei Tang and Ajay Chatrath},
  doi          = {10.1214/23-AOAS1771},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3450-3475},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Compressed spectral screening for large-scale differential correlation analysis with application in selecting glioblastoma gene modules},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Binned multinomial logistic regression for integrative
cell-type annotation. <em>AOAS</em>, <em>17</em>(4), 3426–3449. (<a
href="https://doi.org/10.1214/23-AOAS1769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Categorizing individual cells into one of many known cell-type categories, also known as cell-type annotation, is a critical step in the analysis of single-cell genomics data. The current process of annotation is time intensive and subjective, which has led to different studies describing cell types with labels of varying degrees of resolution. While supervised learning approaches have provided automated solutions to annotation, there remains a significant challenge in fitting a unified model for multiple datasets with inconsistent labels. In this article we propose a new multinomial logistic regression estimator which can be used to model cell-type probabilities by integrating multiple datasets with labels of varying resolution. To compute our estimator, we solve a nonconvex optimization problem using a blockwise proximal gradient descent algorithm. We show through simulation studies that our approach estimates cell-type probabilities more accurately than competitors in a wide variety of scenarios. We apply our method to 10 single-cell RNA-seq datasets and demonstrate its utility in predicting fine resolution cell-type labels on unlabeled data as well as refining cell-type labels on data with existing coarse resolution annotations. Finally, we demonstrate that our method can lead to novel scientific insights in the context of a differential expression analysis comparing peripheral blood gene expression before and after treatment with interferon-β. An R package implementing the method is available in the Supplementary Material and at https://github.com/keshav-motwani/IBMR, and the collection of datasets we analyze is available at https://github.com/keshav-motwani/AnnotatedPBMC.},
  archive      = {J_AOAS},
  author       = {Keshav Motwani and Rhonda Bacher and Aaron J. Molstad},
  doi          = {10.1214/23-AOAS1769},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3426-3449},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Binned multinomial logistic regression for integrative cell-type annotation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodel ensemble analysis with neural network gaussian
processes. <em>AOAS</em>, <em>17</em>(4), 3403–3425. (<a
href="https://doi.org/10.1214/23-AOAS1768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodel ensemble analysis integrates information from multiple climate models into a unified projection. However, existing integration approaches, based on model averaging, can dilute fine-scale spatial information and incur bias from rescaling low-resolution climate models. We propose a statistical approach, called NN-GPR, using Gaussian process regression (GPR) with an infinitely wide deep neural network based covariance function. NN-GPR requires no assumptions about the relationships between climate models, no interpolation to a common grid, and automatically downscales as part of its prediction algorithm. Model experiments show that NN-GPR can be highly skillful at surface temperature and precipitation forecasting by preserving geospatial signals at multiple scales and capturing interannual variability. Our projections particularly show improved accuracy and uncertainty quantification skill in regions of high variability, which allows us to cheaply assess tail behavior at a 0.44∘/50 km spatial resolution without a regional climate model (RCM). Evaluations on reanalysis data and SSP2-4.5 forced climate models show that NN-GPR produces similar, overall climatologies to the model ensemble while better capturing fine-scale spatial patterns. Finally, we compare NN-GPR’s regional predictions against two RCMs and show that NN-GPR can rival the performance of RCMs using only global model data as input.},
  archive      = {J_AOAS},
  author       = {Trevor Harris and Bo Li and Ryan Sriver},
  doi          = {10.1214/23-AOAS1768},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3403-3425},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multimodel ensemble analysis with neural network gaussian processes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reluctant additive model framework for interpretable
nonlinear individualized treatment rules. <em>AOAS</em>, <em>17</em>(4),
3384–3402. (<a href="https://doi.org/10.1214/23-AOAS1767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rules (ITRs) for treatment recommendation is an important topic for precision medicine as not all beneficial treatments work well for all individuals. Interpretability is a desirable property of ITRs, as it helps practitioners make sense of treatment decisions, yet there is a need for ITRs to be flexible to effectively model complex biomedical data for treatment decision making. Many ITR approaches either focus on linear ITRs, which may perform poorly when true optimal ITRs are nonlinear, or black-box nonlinear ITRs, which may be hard to interpret and can be overly complex. This dilemma indicates a tension between interpretability and accuracy of treatment decisions. Here we propose an additive model-based nonlinear ITR learning method that balances interpretability and flexibility of the ITR. Our approach aims to strike this balance by allowing both linear and nonlinear terms of the covariates in the final ITR. Our approach is parsimonious in that the nonlinear term is included in the final ITR only when it substantially improves the ITR performance. To prevent overfitting, we combine crossfitting and a specialized information criterion for model selection. Through extensive simulations we show that our methods are data-adaptive to the degree of nonlinearity and can favorably balance ITR interpretability and flexibility. We further demonstrate the robust performance of our methods with an application to a cancer drug sensitive study.},
  archive      = {J_AOAS},
  author       = {Jacob M. Maronge and Jared D. Huling and Guanhua Chen},
  doi          = {10.1214/23-AOAS1767},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3384-3402},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A reluctant additive model framework for interpretable nonlinear individualized treatment rules},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint stochastic simulation of extreme coastal and offshore
significant wave heights. <em>AOAS</em>, <em>17</em>(4), 3363–3383. (<a
href="https://doi.org/10.1214/23-AOAS1766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characterisation of future extreme wave events is crucial because of their multiple impacts, covering a broad range of topics such as coastal flood hazard, coastal erosion, reliability of offshore and coastal structures. The main goal of this paper is to propose and study a stochastic simulator that, given offshore conditions (peak direction Dp, peak period Tp and moderately high significant wave heights Hs), produces jointly offshore and coastal extreme Hs, a quantity measuring the wave severity and which represent a key feature in coastal risk analysis. For this purpose we rely on bivariate Peaks over Threshold, and a nonparametric simulation scheme of bivariate GPD is developed. From this joint simulator, a second generator is derived, allowing for conditional simulations of extreme Hs. Finally, to take into account nonstationarities, the extended generalised Pareto model is also adapted, letting the parameters vary with specific sea-state parameters Tp and Dp. The performances of the two proposed generators are illustrated on simulated data and then applied to the simulation of new extreme oceanographic conditions close to the French Brittany coast using hindcast sea-state data. Results show that the proposed algorithms successfully simulate future extreme Hs near the coast in a nonparametric way, jointly or conditionally on sea-state parameters from a coarser model.},
  archive      = {J_AOAS},
  author       = {Juliette Legrand and Pierre Ailliot and Philippe Naveau and Nicolas Raillard},
  doi          = {10.1214/23-AOAS1766},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3363-3383},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Joint stochastic simulation of extreme coastal and offshore significant wave heights},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating covid-19 transmission time using hawkes point
processes. <em>AOAS</em>, <em>17</em>(4), 3349–3362. (<a
href="https://doi.org/10.1214/23-AOAS1765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The question addressed here is whether, using Hawkes models. the distribution of SARS-CoV-2 (Covid-19) transmission times can be estimated accurately with only case-count data. We fit Hawkes models with varying productivities to each of the 50 United States individually, estimating for each state a transmission time density, both nonparametrically and using a normal approximation. We find that, for nearly all states, the estimated transmission times are centered near seven days with a standard deviation of approximately one day. Compared to previous reports, the results here suggest that transmission times for SARS-CoV-2 are somewhat shorter, on average, and the distribution is less diffuse, though the results also suggest the possibility of transmission occurring on the first day of exposure.},
  archive      = {J_AOAS},
  author       = {Frederic Schoenberg},
  doi          = {10.1214/23-AOAS1765},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3349-3362},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating covid-19 transmission time using hawkes point processes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating COVID-19 vaccine protection rates via dynamic
epidemiological models—a study of 10 countries. <em>AOAS</em>,
<em>17</em>(4), 3324–3348. (<a
href="https://doi.org/10.1214/23-AOAS1764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-world performance of vaccines against COVID-19 infections is critically important to counter the pandemics. We propose a varying coefficient stochastic epidemic model to estimate the vaccine protection rates based on the publicly available epidemiological and vaccination data. To tackle the challenges posed by the unobserved state variables, we develop a multistep decentralized estimation procedure that uses different data segments to estimate different parameters. A B-spline structure is used to approximate the underlying infection rates and to facilitate model simulation in obtaining an objective function between the imputed and the simulation-based estimates of the latent state variables, leading to simulation-based estimation of the diagnosis rate using data in the prevaccine period and the vaccine effect parameters using data in the postvaccine periods. The time-varying infection, recovery and death rates are estimated by kernel regressions. We apply the proposed method to analyze the data in ten countries which collectively used eight vaccines. The analysis reveals that the average protection rate of the full vaccination was at least 22\% higher than that of the partial vaccination and was largely above the WHO recognized level of 50\% before November 20, 2021, including the Delta variant dominated period. The protection rates for the booster vaccine in the Omicron period were also provided.},
  archive      = {J_AOAS},
  author       = {Yuru Zhu and Jia Gu and Yumou Qiu and Song Xi Chen},
  doi          = {10.1214/23-AOAS1764},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3324-3348},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating COVID-19 vaccine protection rates via dynamic epidemiological models—a study of 10 countries},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential monte carlo for sampling balanced and compact
redistricting plans. <em>AOAS</em>, <em>17</em>(4), 3300–3323. (<a
href="https://doi.org/10.1214/23-AOAS1763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random sampling of graph partitions under constraints has become a popular tool for evaluating legislative redistricting plans. Analysts detect partisan gerrymandering by comparing a proposed redistricting plan with an ensemble of sampled alternative plans. For successful application sampling methods must scale to maps with a moderate or large number of districts, incorporate realistic legal constraints, and accurately and efficiently sample from a selected target distribution. Unfortunately, most existing methods struggle in at least one of these areas. We present a new sequential Monte Carlo (SMC) algorithm that generates a sample of redistricting plans converging to a realistic target distribution. Because it draws many plans in parallel, the SMC algorithm can efficiently explore the relevant space of redistricting plans better than the existing Markov chain Monte Carlo (MCMC) algorithms that generate plans sequentially. Our algorithm can simultaneously incorporate several constraints commonly imposed in real-world redistricting problems, including equal population, compactness, and preservation of administrative boundaries. We validate the accuracy of the proposed algorithm by using a small map where all redistricting plans can be enumerated. We then apply the SMC algorithm to evaluate the partisan implications of several maps submitted by relevant parties in a recent high-profile redistricting case in the State of Pennsylvania. We find that the proposed algorithm converges faster and with fewer samples than a comparable MCMC algorithm. Open-source software is available for implementing the proposed methodology.},
  archive      = {J_AOAS},
  author       = {Cory McCartan and Kosuke Imai},
  doi          = {10.1214/23-AOAS1763},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3300-3323},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sequential monte carlo for sampling balanced and compact redistricting plans},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic additive and multiplicative effects network model
with application to the united nations voting behaviors. <em>AOAS</em>,
<em>17</em>(4), 3283–3299. (<a
href="https://doi.org/10.1214/23-AOAS1762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a study of United Nations voting behaviors, we introduce a regression model for a series of networks that are correlated over time. Our model is a dynamic extension of the additive and multiplicative effects network model (AMEN) of Hoff (Statist. Sci. 36 (2021) 34–50). In addition to incorporating a temporal structure, the model accommodates two types of missing data and thus allows the size of the network to vary over time. We demonstrate via simulations the necessity of various components of the model. We apply the model to the United Nations General Assembly voting data from 1983 to 2014 (In Routledge Handbook of International Organization (2013) Routledge) to answer interesting research questions regarding international voting behaviors. In addition to finding important factors that could explain the voting behaviors, the model-estimated additive effects, multiplicative effects, and their movements reveal meaningful foreign policy positions and alliances of various countries.},
  archive      = {J_AOAS},
  author       = {Bomin Kim and Xiaoyue Niu and David Hunter and Xun Cao},
  doi          = {10.1214/23-AOAS1762},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3283-3299},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A dynamic additive and multiplicative effects network model with application to the united nations voting behaviors},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general framework for penalized mixed-effects multitask
learning with applications on DNA methylation surrogate biomarkers
creation. <em>AOAS</em>, <em>17</em>(4), 3257–3282. (<a
href="https://doi.org/10.1214/23-AOAS1760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent evidence highlights the usefulness of DNA methylation (DNAm) biomarkers as surrogates for exposure to risk factors for noncommunicable diseases in epidemiological studies and randomized trials. DNAm variability has been demonstrated to be tightly related to lifestyle behavior and exposure to environmental risk factors, ultimately providing an unbiased proxy of an individual state of health. At present, the creation of DNAm surrogates relies on univariate penalized regression models, with elastic-net regularizer being the gold standard when accomplishing the task. Nonetheless, more advanced modeling procedures are required in the presence of multivariate outcomes with a structured dependence pattern among the study samples. In this work we propose a general framework for mixed-effects multitask learning in presence of high-dimensional predictors to develop a multivariate DNAm biomarker from a multicenter study. A penalized estimation scheme, based on an expectation-maximization algorithm, is devised in which any penalty criteria for fixed-effects models can be conveniently incorporated in the fitting process. We apply the proposed methodology to create novel DNAm surrogate biomarkers for multiple correlated risk factors for cardiovascular diseases and comorbidities. We show that the proposed approach, modeling multiple outcomes together, outperforms state-of-the-art alternatives both in predictive power and biomolecular interpretation of the results.},
  archive      = {J_AOAS},
  author       = {Andrea Cappozzo and Francesca Ieva and Giovanni Fiorito},
  doi          = {10.1214/23-AOAS1760},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3257-3282},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A general framework for penalized mixed-effects multitask learning with applications on DNA methylation surrogate biomarkers creation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A penalized complexity prior for deep bayesian transfer
learning with application to materials informatics. <em>AOAS</em>,
<em>17</em>(4), 3241–3256. (<a
href="https://doi.org/10.1214/23-AOAS1759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key task in the emerging field of materials informatics is to use machine learning to predict a material’s properties and functions. A fast and accurate predictive model allows researchers to more efficiently identify or construct a material with desirable properties. As in many fields, deep learning is one of the state-of-the art approaches, but fully training a deep learning model is not always feasible in materials informatics due to limitations on data availability, computational resources, and time. Accordingly, there is a critical need in the application of deep learning to materials informatics problems to develop efficient transfer learning algorithms. The Bayesian framework is natural for transfer learning because the model trained from the source data can be encoded in the prior distribution for the target task of interest. However, the Bayesian perspective on transfer learning is relatively unaccounted for in the literature and is complicated for deep learning because the parameter space is large and the interpretations of individual parameters are unclear. Therefore, rather than subjective prior distributions for individual parameters, we propose a new Bayesian transfer learning approach based on the penalized complexity prior on the Kullback–Leibler divergence between the predictive models of the source and target tasks. We show via simulations that the proposed method outperforms other transfer learning methods across a variety of settings. The proposed method is applied to predict the properties of a molecular crystal, based on its structural properties, and we show improved precision for estimating the band gap of a material compared to state-of-the-art methods currently used in materials science.},
  archive      = {J_AOAS},
  author       = {Mohamed A. Abba and Jonathan P. Williams and Brian J. Reich},
  doi          = {10.1214/23-AOAS1759},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3241-3256},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A penalized complexity prior for deep bayesian transfer learning with application to materials informatics},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A riemann manifold model framework for longitudinal changes
in physical activity patterns. <em>AOAS</em>, <em>17</em>(4), 3216–3240.
(<a href="https://doi.org/10.1214/23-AOAS1758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical activity (PA) is significantly associated with many health outcomes. The wide usage of wearable accelerometer-based activity trackers in recent years has provided a unique opportunity for in-depth research on PA and its relations with health outcomes and interventions. Past analysis of activity tracker data relies heavily on aggregating minute-level PA records into day-level summary statistics in which important information of PA temporal/diurnal patterns is lost. In this paper we propose a novel functional data analysis approach based on Riemann manifolds for modeling PA and its longitudinal changes. We model smoothed minute-level PA of a day as one-dimensional Riemann manifolds and longitudinal changes in PA in different visits as deformations between manifolds. The variability in changes of PA among a cohort of subjects is characterized via variability in the deformation. Functional principal component analysis is further adopted to model the deformations, and PC scores are used as a proxy in modeling the relation between changes in PA and health outcomes and/or interventions. We conduct comprehensive analyses on data from two clinical trials: Reach for Health (RfH) and Metabolism, Exercise and Nutrition at UCSD (MENU), focusing on the effect of interventions on longitudinal changes in PA patterns and how different modes of changes in PA influence weight loss, respectively. The proposed approach reveals unique modes of changes, including overall enhanced PA, boosted morning PA, and shifts of active hours specific to each study cohort. The results bring new insights into the study of longitudinal changes in PA and health and have the potential to facilitate designing of effective health interventions and guidelines.},
  archive      = {J_AOAS},
  author       = {Jingjing Zou and Tuo Lin and Chongzhi Di and John Bellettiere and Marta M. Jankowska and Sheri J. Hartman and Dorothy D. Sears and Andrea Z. LaCroix and Cheryl L. Rock and Loki Natarajan},
  doi          = {10.1214/23-AOAS1758},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3216-3240},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A riemann manifold model framework for longitudinal changes in physical activity patterns},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal sampling designs for multidimensional streaming time
series with application to power grid sensor data. <em>AOAS</em>,
<em>17</em>(4), 3195–3215. (<a
href="https://doi.org/10.1214/23-AOAS1757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) system generates massive high-speed temporally correlated streaming data and is often connected with online inference tasks under computational or energy constraints. Online analysis of these streaming time series data often faces a trade-off between statistical efficiency and computational cost. One important approach to balance this trade-off is sampling, where only a small portion of the sample is selected for the model fitting and update. Motivated by the demands of dynamic relationship analysis of IoT system, we study the data-dependent sample selection and online inference problem for a multidimensional streaming time series, aiming to provide low-cost real-time analysis of high-speed power grid electricity consumption data. Inspired by D-optimality criterion in design of experiments, we propose a class of online data reduction methods that achieve an optimal sampling criterion and improve the computational efficiency of the online analysis. We show that the optimal solution amounts to a strategy that is a mixture of Bernoulli sampling and leverage score sampling. The leverage score sampling involves auxiliary estimations that have a computational advantage over recursive least squares updates. Theoretical properties of the auxiliary estimations involved are also discussed. When applied to European power grid consumption data, the proposed leverage score based sampling methods outperform the benchmark sampling method in online estimation and prediction. The general applicability of the sampling-assisted online estimation method is assessed via simulation studies.},
  archive      = {J_AOAS},
  author       = {Rui Xie and Shuyang Bai and Ping Ma},
  doi          = {10.1214/23-AOAS1757},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3195-3215},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Optimal sampling designs for multidimensional streaming time series with application to power grid sensor data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic declustering of earthquakes with the
spatiotemporal renewal ETAS model. <em>AOAS</em>, <em>17</em>(4),
3173–3194. (<a href="https://doi.org/10.1214/23-AOAS1756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and forecasting earthquakes is challenging due to the complex interplay and clustering of main-shocks and aftershocks. The epidemic-type aftershock sequence (ETAS) model represents the conditional intensity of earthquakes as the superposition of a background and aftershock rate which allows for the declustering of the earthquakes. Its success has led to the development of numerous versions of the ETAS model. Among these extensions is the renewal ETAS (RETAS) model, which has shown promising potential. The RETAS model endows the main-shock arrival process with a renewal process, which serves as an alternative to the homogeneous Poisson process. Model fitting is performed using likelihood-based estimation by directly optimizing the exact likelihood. However, inferring the branching structure from the fitted RETAS model remains a challenging task since the declustering algorithm that is currently available for the ETAS model is not directly applicable. Therefore, this article develops an iterative algorithm to calculate the smoothed main- and aftershock probabilities, conditional on all available information contained in the catalog. Consequently, an estimate of the background spatial intensity function and model parameters can be obtained using an iterative semiparametric procedure with the smoothing parameters selected using information criteria. The methods proposed herein are illustrated on simulated data and a New Zealand earthquake catalog.},
  archive      = {J_AOAS},
  author       = {Tom Stindl and Feng Chen},
  doi          = {10.1214/23-AOAS1756},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3173-3194},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Stochastic declustering of earthquakes with the spatiotemporal renewal ETAS model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying boundaries in spatially continuous risk surfaces
from spatially aggregated disease count data. <em>AOAS</em>,
<em>17</em>(4), 3153–3172. (<a
href="https://doi.org/10.1214/23-AOAS1755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatially aggregated disease-count data relating to a set of nonoverlapping areal units are often used to make inference on population-level disease risk. This includes the identification of risk boundaries, which are locations where there is a sizeable change in risk between geographically neighbouring areal units. Existing studies provide spatially discrete inference on the areal unit footprint, which forces the boundaries to coincide with the entire geographical border between neighbouring units. This paper is the first to relax these assumptions by estimating disease risk and the locations of risk boundaries on a grid of square pixels covering the study region that can be made arbitrarily small to approximate a spatially continuous surface. We propose a two-stage approach that first fits a Bayesian spatiotemporal realignment model to estimate disease risk at the grid level and then identifies boundaries in this surface using edge detection algorithms from computer vision. This novel methodological fusion is motivated by a new study of respiratory hospitalisation risk in Glasgow, Scotland, between 2008 and 2017, and we identify numerous risk boundaries across the city.},
  archive      = {J_AOAS},
  author       = {Duncan Lee},
  doi          = {10.1214/23-AOAS1755},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3153-3172},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Identifying boundaries in spatially continuous risk surfaces from spatially aggregated disease count data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design-based mapping of land use/land cover classes with
bootstrap estimation of precision by nearest-neighbour interpolation.
<em>AOAS</em>, <em>17</em>(4), 3133–3152. (<a
href="https://doi.org/10.1214/23-AOAS1754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land use/land cover mapping is usually performed by classifying satellite imagery (e.g., Landsat, Sentinel) for the whole survey region using classification algorithms implemented with training data. Subsequently, probabilistic samples are usually implemented with the main purpose of assessing the accuracy of these maps by comparing the map class and the ground condition determined for the sampled units. The main proposal of this paper is to directly exploit these probabilistic samples to estimate the land use/land cover class at any location of the survey region in a design-based framework by the well-known nearest-neighbour interpolator. For the first time, the design-based consistency of nearest-neighbour maps (i.e., categorical variables) is theoretically proven and a pseudo-population bootstrap estimator of their precision is proposed and discussed. These nearest-neighbour maps provide the ability to place mapping within a rigorous design-based inference framework, in contrast to most traditional mapping approaches which often are implemented with no inferential basis or by necessity (due to lack of a probabilistic sample) model-based inference. A simulation study is performed on an estimated land use map in Southern Tuscany (Italy)—taken as the true map—to check the finite-sample performance of the proposal as well as the matching of the area coverage estimates arising from the map with those achieved by traditional estimators. The Italian land use map arising from the IUTI surveys and the U.S. land cover map arising from the LCMAP program are considered as case studies.},
  archive      = {J_AOAS},
  author       = {Agnese Marcelli and Rosa Maria Di Biase and Piermaria Corona and Stephen V. Stehman and Lorenzo Fattorini},
  doi          = {10.1214/23-AOAS1754},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3133-3152},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Design-based mapping of land use/land cover classes with bootstrap estimation of precision by nearest-neighbour interpolation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When ecological individual heterogeneity models and large
data collide: An importance sampling approach. <em>AOAS</em>,
<em>17</em>(4), 3112–3132. (<a
href="https://doi.org/10.1214/23-AOAS1753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the challenges that arise when fitting ecological individual heterogeneity models to “large” data sets. In particular, we focus on (continuous-valued) random effect models commonly used to describe individual heterogeneity present in ecological populations within the context of capture–recapture data, although the approach is more widely applicable to more general latent variable models. Within such models the associated likelihood is expressible only as an analytically intractable integral. Common techniques for fitting such models to data include, for example, the use of numerical approximations for the integral or a Bayesian data augmentation approach. However, as the size of the data set increases (i.e., the number of individuals increases), these computational tools may become computationally infeasible. We present an efficient Bayesian model-fitting approach, whereby we initially sample from the posterior distribution of a smaller subsample of the data, before correcting this sample to obtain estimates of the posterior distribution of the full data set using an importance sampling approach. We consider several practical issues, including the subsampling mechanism, computational efficiencies (including the ability to parallelise the algorithm) and combining subsampling estimates using multiple subsampled data sets. We initially demonstrate the feasibility (and accuracy) of the approach via simulated data before considering a challenging real data set of approximately 30,000 guillemots and, using the proposed algorithm, obtain posterior estimates of the model parameters in substantially reduced computational time, compared to the standard Bayesian model-fitting approach.},
  archive      = {J_AOAS},
  author       = {Ruth King and Blanca Sarzo and Víctor Elvira},
  doi          = {10.1214/23-AOAS1753},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3112-3132},
  shortjournal = {Ann. Appl. Stat.},
  title        = {When ecological individual heterogeneity models and large data collide: An importance sampling approach},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven chimney fire risk prediction using machine
learning and point process tools. <em>AOAS</em>, <em>17</em>(4),
3088–3111. (<a href="https://doi.org/10.1214/23-AOAS1752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chimney fires constitute one of the most commonly occurring fire types. Precise prediction and prompt prevention are crucial in reducing the harm they cause. In this paper we develop a combined machine learning and statistical modelling process to predict fire risk. First, we use random forests and permutation importance techniques to identify the most informative explanatory variables. Second, we design a Poisson point process model and employ logistic regression estimation to estimate the parameters. Moreover, we validate the Poisson model assumption using second-order summary statistics and residuals. We implement the modelling process on data collected by the Twente Fire Brigade and obtain plausible predictions. Compared to similar studies, our approach has two advantages: (i) with random forests, we can select explanatory variables nonparametrically considering variable dependence; (ii) using logistic regression estimation, we can fit our statistical model efficiently by tuning it to focus on regions and times that are salient for fire risk.},
  archive      = {J_AOAS},
  author       = {Changqing Lu and Marie-Colette van Lieshout and Maurits de Graaf and Paul Visscher},
  doi          = {10.1214/23-AOAS1752},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3088-3111},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Data-driven chimney fire risk prediction using machine learning and point process tools},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic spatial filtering approach to mitigate
underestimation bias in field calibrated low-cost sensor air pollution
data. <em>AOAS</em>, <em>17</em>(4), 3056–3087. (<a
href="https://doi.org/10.1214/23-AOAS1751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-cost air pollution sensors, offering hyperlocal characterization of pollutant concentrations, are becoming increasingly prevalent in environmental and public health research. However, low-cost air pollution data can be noisy, biased by environmental conditions, and usually need to be field-calibrated by collocating low-cost sensors with reference-grade instruments. We show, theoretically and empirically, that the common procedure of regression-based calibration, using collocated data, systematically underestimates high air pollution concentrations, which are critical to diagnose from a health perspective. Current calibration practices also often fail to utilize the spatial correlation in pollutant concentrations. We propose a novel spatial filtering approach to collocation-based calibration of low-cost networks that mitigates the underestimation issue by using an inverse regression. The inverse regression also allows for incorporating spatial correlations by a second-stage model for the true pollutant concentrations using a conditional Gaussian process. Our approach works with one or more collocated sites in the network and is dynamic, leveraging spatial correlation with the latest available reference data. Through extensive simulations, we demonstrate how the spatial filtering substantially improves estimation of pollutant concentrations and measures peak concentrations with greater accuracy. We apply the methodology for calibration of a low-cost PM2.5 network in Baltimore, Maryland, and diagnose air pollution peaks that are missed by the regression-calibration.},
  archive      = {J_AOAS},
  author       = {Claire Heffernan and Roger Peng and Drew R. Gentner and Kirsten Koehler and Abhirup Datta},
  doi          = {10.1214/23-AOAS1751},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3056-3087},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A dynamic spatial filtering approach to mitigate underestimation bias in field calibrated low-cost sensor air pollution data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian decision framework for optimizing sequential
combination antiretroviral therapy in people with HIV. <em>AOAS</em>,
<em>17</em>(4), 3035–3055. (<a
href="https://doi.org/10.1214/23-AOAS1750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous adverse effects (e.g., depression) have been reported for combination antiretroviral therapy (cART) despite its remarkable success in viral suppression in people with HIV (PWH). To improve long-term health outcomes for PWH, there is an urgent need to design personalized optimal cART with the lowest risk of comorbidity in the emerging field of precision medicine for HIV. Large-scale HIV studies offer researchers unprecedented opportunities to optimize personalized cART in a data-driven manner. However, the large number of possible drug combinations for cART makes the estimation of cART effects a high-dimensional combinatorial problem, imposing challenges in both statistical inference and decision-making. We develop a two-step Bayesian decision framework for optimizing sequential cART assignments. In the first step, we propose a dynamic model for individuals’ longitudinal observations using a multivariate Gaussian process. In the second step, we build a probabilistic generative model for cART assignments and design an uncertainty-penalized policy optimization using the uncertainty quantification from the first step. Applying the proposed method to a dataset from the Women’s Interagency HIV Study, we demonstrate its clinical utility in assisting physicians to make effective treatment decisions, serving the purpose of both viral suppression and comorbidity risk reduction.},
  archive      = {J_AOAS},
  author       = {Wei Jin and Yang Ni and Jane O’Halloran and Amanda B. Spence and Leah H. Rubin and Yanxun Xu},
  doi          = {10.1214/23-AOAS1750},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3035-3055},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian decision framework for optimizing sequential combination antiretroviral therapy in people with HIV},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian group selection with compositional responses for
analysis of radiologic tumor proportions and their genomic determinants.
<em>AOAS</em>, <em>17</em>(4), 3013–3034. (<a
href="https://doi.org/10.1214/23-AOAS1749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volumetric imaging features are used in cancer research to determine the size and the composition of a tumor and have been shown to be prognostic of overall survival. In this paper we focus on the analysis of tumor component proportions of brain cancer patients collected through The Cancer Genome Atlas (TCGA) project. Our main goal is to identify pathways and corresponding genes that can explain the heterogeneity of the composition of a brain tumor. In particular, we focus on the glioblastoma multiform (GBM), as it is the most common malignant brain neoplasm, accounting for 23\% of all primary brain tumors for which it still has very poor prognosis. We propose a Bayesian hierarchical model for variable selection with a group structure in the context of correlated multivariate compositional response variables. More specifically, we model the proportions of the tumor components within the tumor using a Dirichlet model by allowing for straightforward incorporation of available high-dimensional covariate information within a log-linear regression framework. We impose prior distributions that account for the overlapping structure between groups of covariates. Simulations and application to GBM disease show the importance of our approach. We have identified associations between tumor component volume-based features and several important pathways and genes. Some of these genes have previously been shown to be prognostic indicators of overall survival time in GBM.},
  archive      = {J_AOAS},
  author       = {Thierry Chekouo and Francesco C. Stingo and Shariq Mohammed and Arvind Rao and Veerabhadran Baladandayuthapani},
  doi          = {10.1214/23-AOAS1749},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {3013-3034},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian group selection with compositional responses for analysis of radiologic tumor proportions and their genomic determinants},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building a dose toxo-equivalence model from a bayesian
meta-analysis of published clinical trials. <em>AOAS</em>,
<em>17</em>(4), 2993–3012. (<a
href="https://doi.org/10.1214/23-AOAS1748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical practice medications are often interchanged in treatment protocols when a patient negatively reacts to their first line of therapy. Although switching between medications is common, clinicians often lack structured guidance when choosing the initial dose and frequency of a new medication, given the former with respect to risk of adverse events. In this paper we propose to establish this dose toxo-equivalence relationship using published clinical trial results with one or both drugs of interest via a Bayesian meta-analysis model that accounts for both within- and between-study variances. With the posterior parameter samples from this model, we compute median and 95\% credible intervals for equivalent dose pairs of the two drugs that are predicted to produce equal rates of an adverse outcome, relying solely on study-level information. Via extensive simulations, we show that this approach approximates well the true dose toxo-equivalence relationship, considering different study designs, levels of between-study variance, and the inclusion/exclusion of nonconfounder/nonmodifier subject-level covariates in addition to study-level covariates. We compare the performance of this study-level meta-analysis estimate to the equivalent individual patient data meta-analysis model and find comparable bias and minimal efficiency loss in the study-level coefficients used in the dose toxo-equivalence relationship. Finally, we present the findings of our dose toxo-equivalence model applied to two chemotherapy drugs, based on data from 169 published clinical trials.},
  archive      = {J_AOAS},
  author       = {Elizabeth A. Sigworth and Samuel M. Rubinstein and Jeremy L. Warner and Yong Chen and Qingxia Chen},
  doi          = {10.1214/23-AOAS1748},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2993-3012},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Building a dose toxo-equivalence model from a bayesian meta-analysis of published clinical trials},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Targeting underrepresented populations in precision
medicine: A federated transfer learning approach. <em>AOAS</em>,
<em>17</em>(4), 2970–2992. (<a
href="https://doi.org/10.1214/23-AOAS1747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited representation of minorities and disadvantaged populations in large-scale clinical and genomics research poses a significant barrier to translating precision medicine research into practice. Prediction models are likely to underperform in underrepresented populations due to heterogeneity across populations, thereby exacerbating known health disparities. To address this issue, we propose FETA, a two-way data integration method that leverages a federated transfer learning approach to integrate heterogeneous data from diverse populations and multiple healthcare institutions, with a focus on a target population of interest having limited sample sizes. We show that FETA achieves performance comparable to the pooled analysis, where individual-level data is shared across institutions, with only a small number of communications across participating sites. Our theoretical analysis and simulation study demonstrate how FETA’s estimation accuracy is influenced by communication budgets, privacy restrictions, and heterogeneity across populations. We apply FETA to multisite data from the electronic Medical Records and Genomics (eMERGE) Network to construct genetic risk prediction models for extreme obesity. Compared to models trained using target data only, source data only, and all data without accounting for population-level differences, FETA shows superior predictive performance. FETA has the potential to improve estimation and prediction accuracy in underrepresented populations and reduce the gap in model performance across populations.},
  archive      = {J_AOAS},
  author       = {Sai Li and Tianxi Cai and Rui Duan},
  doi          = {10.1214/23-AOAS1747},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2970-2992},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Targeting underrepresented populations in precision medicine: A federated transfer learning approach},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized matrix decomposition regression: Estimation and
inference for two-way structured data. <em>AOAS</em>, <em>17</em>(4),
2944–2969. (<a href="https://doi.org/10.1214/23-AOAS1746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by emerging applications in ecology, microbiology, and neuroscience, this paper studies high-dimensional regression with two-way structured data. To estimate the high-dimensional coefficient vector, we propose the generalized matrix decomposition regression (GMDR) to efficiently leverage auxiliary information on row and column structures. GMDR extends the principal component regression (PCR) to two-way structured data, but unlike PCR, GMDR selects the components that are most predictive of the outcome, leading to more accurate prediction. For inference on regression coefficients of individual variables, we propose the generalized matrix decomposition inference (GMDI), a general high-dimensional inferential framework for a large family of estimators that include the proposed GMDR estimator. GMDI provides more flexibility for incorporating relevant auxiliary row and column structures. As a result, GMDI does not require the true regression coefficients to be sparse but constrains the coordinate system representing the regression coefficients according to the column structure. GMDI also allows dependent and heteroscedastic observations. We study the theoretical properties of GMDI in terms of both the type-I error rate and power and demonstrate the effectiveness of GMDR and GMDI in simulation studies and an application to human microbiome data.},
  archive      = {J_AOAS},
  author       = {Yue Wang and Ali Shojaie and Timothy Randolph and Parker Knight and Jing Ma},
  doi          = {10.1214/23-AOAS1746},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2944-2969},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Generalized matrix decomposition regression: Estimation and inference for two-way structured data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pairwise nonlinear dependence analysis of genomic data.
<em>AOAS</em>, <em>17</em>(4), 2924–2943. (<a
href="https://doi.org/10.1214/23-AOAS1745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In The Cancer Genome Atlas (TCGA) data set, there are many interesting nonlinear dependencies between pairs of genes that reveal important relationships and subtypes of cancer. Such genomic data analysis requires a rapid, powerful, and interpretable detection process, especially in a high-dimensional environment. We study the nonlinear patterns among the expression of pairs of genes from TCGA using a powerful tool called binary expansion testing. We find many nonlinear patterns, some of which are driven by known cancer subtypes, some of which are novel.},
  archive      = {J_AOAS},
  author       = {Siqi Xiang and Wan Zhang and Siyao Liu and Katherine A. Hoadley and Charles M. Perou and Kai Zhang and J. S. Marron},
  doi          = {10.1214/23-AOAS1745},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2924-2943},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Pairwise nonlinear dependence analysis of genomic data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Addressing selection bias and measurement error in COVID-19
case count data using auxiliary information. <em>AOAS</em>,
<em>17</em>(4), 2903–2923. (<a
href="https://doi.org/10.1214/23-AOAS1744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus case-count data has influenced government policies and drives most epidemiological forecasts. Limited testing is cited as the key driver behind minimal information on the COVID-19 pandemic. While expanded testing is laudable, measurement error and selection bias are the two greatest problems limiting our understanding of the COVID-19 pandemic; neither can be fully addressed by increased testing capacity. In this paper we demonstrate their impact on estimation of point prevalence and the effective reproduction number. We show that estimates, based on the millions of molecular tests in the U.S., have the same mean square error as a small simple random sample. To address this, a procedure is presented that combines case-count data and random samples over time to estimate selection propensities based on key covariate information. We then combine these selection propensities with epidemiological forecast models to construct a doubly robust estimation method that accounts for both measurement-error and selection bias. This method is then applied to estimate Indiana’s active infection prevalence using case-count, hospitalization, and death data with demographic information, a statewide random molecular sample collected from April 25–29, 2020, and Delphi’s COVID-19 Trends and Impact Survey. We end with a series of recommendations based on the proposed methodology.},
  archive      = {J_AOAS},
  author       = {Walter Dempsey},
  doi          = {10.1214/23-AOAS1744},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2903-2923},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Addressing selection bias and measurement error in COVID-19 case count data using auxiliary information},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian learning of covid-19 vaccine safety while
incorporating adverse events ontology. <em>AOAS</em>, <em>17</em>(4),
2887–2902. (<a href="https://doi.org/10.1214/23-AOAS1743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While vaccines are crucial to end the COVID-19 pandemic, public confidence in vaccine safety has always been vulnerable. Many statistical methods have been applied to VAERS (Vaccine Adverse Event Reporting System) database to study the safety of COVID-19 vaccines. However, none of these methods considered the adverse event (AE) ontology. AEs are naturally related; for example, events of retching, dysphagia, and reflux are all related to an abnormal digestive system. Explicitly bringing AE relationships into the model can aid in the detection of true AE signals amid the noise while reducing false positives. We propose a Bayesian graph-assisted signal selection (BGrass) model to simultaneously estimate all AEs while incorporating the network of dependence between AEs. Under a fully Bayesian inference framework, we also propose a negative control approach to mitigate the reporting bias and an enrichment approach to detecting AE groups of concern. For posterior computation we construct an equivalent model representation and develop an efficient Gibbs sampler. We evaluate the performance of BGrass via extensive simulations. To study the safety of COVID-19 vaccines, we apply BGrass to analyze approximately one million VAERS reports (01/01/2016–12/24/2021) involving more than 800 AEs. In particular, we found that blood clots (including deep vein thrombosis, thrombosis, and pulmonary embolism) are more likely to be reported after COVID-19 vaccination, compared to influenza vaccines. They are also reported more often for Johnson &amp; Johnson–Janssen vaccine, compared to mRNA-based COVID-19 vaccines. A user-friendly R package BGrass that implements the proposed methods to assess vaccine safety is included in the Supplementary Material and is publicly available at https://github.com/BangyaoZhao/BGrass.},
  archive      = {J_AOAS},
  author       = {Bangyao Zhao and Yuan Zhong and Jian Kang and Lili Zhao},
  doi          = {10.1214/23-AOAS1743},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2887-2902},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian learning of covid-19 vaccine safety while incorporating adverse events ontology},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian hierarchical modeling and analysis for actigraph
data from wearable devices. <em>AOAS</em>, <em>17</em>(4), 2865–2886.
(<a href="https://doi.org/10.1214/23-AOAS1742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of Americans fail to achieve recommended levels of physical activity, which leads to numerous preventable health problems, such as diabetes, hypertension, and heart diseases. This has generated substantial interest in monitoring human activity to gear interventions toward environmental features that may relate to higher physical activity. Wearable devices, such as wrist-worn sensors that monitor gross motor activity (actigraph units) continuously record the activity levels of a subject, producing massive amounts of high-resolution measurements. Analyzing actigraph data needs to account for spatial and temporal information on trajectories or paths traversed by subjects wearing such devices. Inferential objectives include estimating a subject’s physical activity levels along a given trajectory, identifying trajectories that are more likely to produce higher levels of physical activity for a given subject, and predicting expected levels of physical activity in any proposed new trajectory for a given set of health attributes. Here, we devise a Bayesian hierarchical modeling framework for spatial-temporal actigraphy data to deliver fully model-based inference on trajectories while accounting for subject-level health attributes and spatial-temporal dependencies. We undertake a comprehensive analysis of an original dataset from the Physical Activity through Sustainable Transport Approaches in Los Angeles (PASTA-LA) study to ascertain spatial zones and trajectories exhibiting significantly higher levels of physical activity while accounting for various sources of heterogeneity.},
  archive      = {J_AOAS},
  author       = {Pierfrancesco Alaimo Di Loro and Marco Mingione and Jonah Lipsitt and Christina M. Batteate and Michael Jerrett and Sudipto Banerjee},
  doi          = {10.1214/23-AOAS1742},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2865-2886},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian hierarchical modeling and analysis for actigraph data from wearable devices},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Varying impacts of letters of recommendation on college
admissions. <em>AOAS</em>, <em>17</em>(4), 2843–2864. (<a
href="https://doi.org/10.1214/23-AOAS1740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a pilot program during the 2016–17 admissions cycle, the University of California, Berkeley invited many applicants for freshman admission to submit letters of recommendation. This proved controversial within the university, with concerns that this change would further disadvantage applicants from disadvantaged groups. To inform this debate, we use this pilot as the basis for an observational study of the impact of submitting letters of recommendation on subsequent admission, with the goal of estimating how impacts vary across predefined subgroups. Understanding this variation is challenging in an observational setting because estimated impacts reflect both actual treatment effect variation and differences in covariate balance across groups. To address this, we develop balancing weights that directly optimize for “local balance” within subgroups while maintaining global covariate balance between treated and control units. Applying this approach to the UC Berkeley pilot study yields excellent local and global balance, unlike more traditional weighting methods, which fail to balance covariates within subgroups. We find that the impact of letters of recommendation increases with applicant strength. However, we find little average difference for applicants from disadvantaged groups, although this result is more mixed. In the end we conclude that soliciting letters of recommendation from a broader pool of applicants would not meaningfully change the composition of admitted undergraduates.},
  archive      = {J_AOAS},
  author       = {Eli Ben-Michael and Avi Feller and Jesse Rothstein},
  doi          = {10.1214/23-AOAS1740},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2843-2864},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Varying impacts of letters of recommendation on college admissions},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for covariate-specific ROC curve estimation,
with application to biometric recognition. <em>AOAS</em>,
<em>17</em>(4), 2821–2842. (<a
href="https://doi.org/10.1214/23-AOAS1738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric traits, such as fingerprints, facial images, and teeth impressions, are often used in forensic analysis to identify crime suspects. Matching such biometric traits is not perfect, and recent reports have indicated the need for quantifiable measures of error rates for (these) possible matches. Often, comparisons between two sets of a trait are scored with a higher score indicating a higher likelihood that the sets are a match. Adjustment of the cutoff for which a match is declared yields a trade-off between false positive and false negative decisions that can be represented by an ROC curve. In this paper we study modeling of such ROC curves conditional on covariates, for example, demographic information about source subjects, quality properties of the underlying biometric measurements, or characteristics of forensic examiners; quantifying how error rates vary in dependence of such covariates is often considerably more meaningful in biometrics and forensics than the “raw” error rates based on the pooled data. We herein develop a framework for estimating covariate-specific ROC curves that integrates robustness, heteroscedasticity, and stochastic ordering. The latter is of specific relevance in the given application since biometric recognition systems are typically calibrated to assign higher scores to matching pairs than to nonmatching pairs. The proposed methodology is demonstrated on accuracy of face recognition and fingerprint matching and also has potential in other domains of application like medical diagnostics.},
  archive      = {J_AOAS},
  author       = {Xiaochen Zhu and Martin Slawski and Liansheng Tang},
  doi          = {10.1214/23-AOAS1738},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2821-2842},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A framework for covariate-specific ROC curve estimation, with application to biometric recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predictive inference for travel time on transportation
networks. <em>AOAS</em>, <em>17</em>(4), 2796–2820. (<a
href="https://doi.org/10.1214/23-AOAS1737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent statistical methods fitted on large-scale GPS data can provide accurate estimations of the expected travel time between two points. However, little is known about the distribution of travel time, which is key to decision-making across a number of logistic problems. With sufficient data single road-segment travel time can be well approximated. The challenge lies in understanding how to aggregate such information over a route to arrive at the route-distribution of travel time. We develop a novel statistical approach to this problem. We show that, under general conditions and without assuming a distribution of speed, travel time divided by route distance follows a Gaussian distribution with route-invariant population mean and variance. We develop efficient inference methods for these parameters and propose asymptotically tight population prediction intervals for travel time. Using traffic flow information, we further develop a trip-specific Gaussian-based predictive distribution, resulting in tight prediction intervals for short and long trips. Our methods, implemented in an R-package,1 1 Available at https://github.com/melmasri/traveltimeCLT. are illustrated in a real-world case study using mobile GPS data, showing that our trip-specific and population intervals both achieve the 95\% theoretical coverage levels. Compared to alternative approaches, our trip-specific predictive distribution achieves: (a) the theoretical coverage at every level of significance, (b) tighter prediction intervals, (c) less predictive bias, and (d) more efficient estimation and prediction procedures. This makes our approach promising for low-latency, large-scale transportation applications.},
  archive      = {J_AOAS},
  author       = {Mohamad Elmasri and Aurélie Labbe and Denis Larocque and Laurent Charlin},
  doi          = {10.1214/23-AOAS1737},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2796-2820},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Predictive inference for travel time on transportation networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-independent detection of new physics signals using
interpretable SemiSupervised classifier tests. <em>AOAS</em>,
<em>17</em>(4), 2759–2795. (<a
href="https://doi.org/10.1214/22-AOAS1722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central goal in experimental high energy physics is to detect new physics signals that are not explained by known physics. In this paper we aim to search for new signals that appear as deviations from known Standard Model physics in high-dimensional particle physics data. To do this, we determine whether there is any statistically significant difference between the distribution of Standard Model background samples and the distribution of the experimental observations which are a mixture of the background and a potential new signal. Traditionally, one also assumes access to a sample from a model for the hypothesized signal distribution. Here we instead investigate a model-independent method that does not make any assumptions about the signal and uses a semisupervised classifier to detect the presence of the signal in the experimental data. We construct three test statistics using the classifier: an estimated likelihood ratio test (LRT) statistic, a test based on the area under the ROC curve (AUC), and a test based on the misclassification error (MCE). Additionally, we propose a method for estimating the signal strength parameter and explore active subspace methods to interpret the proposed semisupervised classifier in order to understand the properties of the detected signal. We also propose a score test statistic that can be used in the model-dependent setting. We investigate the performance of the methods on a simulated data set related to the search for the Higgs boson at the Large Hadron Collider at CERN. We demonstrate that the semisupervised tests have power competitive with the classical supervised methods for a well-specified signal but much higher power for an unexpected signal which might be entirely missed by the supervised tests.},
  archive      = {J_AOAS},
  author       = {Purvasha Chakravarti and Mikael Kuusela and Jing Lei and Larry Wasserman},
  doi          = {10.1214/22-AOAS1722},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2759-2795},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Model-independent detection of new physics signals using interpretable SemiSupervised classifier tests},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling racial/ethnic differences in COVID-19 incidence
with covariates subject to nonrandom missingness. <em>AOAS</em>,
<em>17</em>(4), 2723–2758. (<a
href="https://doi.org/10.1214/22-AOAS1711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing the cumulative burden of COVID-19 by race/ethnicity is of the utmost importance for public health researchers and policy makers in order to design effective mitigation measures. This analysis is hampered, however, by surveillance case data with substantial missingness in race and ethnicity covariates. Worse yet, this missingness likely depends on the values of these missing covariates; that is, they are not-missing-at-random (NMAR). We propose a Bayesian parametric model that leverages joint information on spatial variation in the disease and covariate missingness processes and can accommodate both MAR and NMAR missingness. We show that the model is locally identifiable when the spatial distribution of the population covariates is known and observed cases can be associated with a spatial unit of observation. We also use a simulation study to investigate the model’s finite-sample performance. We compare our model’s performance on NMAR data against complete-case analysis and multiple imputation (MI), both of which are commonly used by public health researchers when confronted with missing categorical covariates. Finally, we model spatial variation in cumulative COVID-19 incidence in Wayne County, Michigan, using data from the Michigan Department of Health and Human Services. The analysis suggests that population relative risk estimates by race during the early part of the COVID-19 pandemic in Michigan were understated for non-white residents, compared to white residents, when cases missing race were dropped or had these values imputed using MI.},
  archive      = {J_AOAS},
  author       = {Rob Trangucci and Yang Chen and Jon Zelner},
  doi          = {10.1214/22-AOAS1711},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2723-2758},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling racial/ethnic differences in COVID-19 incidence with covariates subject to nonrandom missingness},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiagent reinforcement learning framework for off-policy
evaluation in two-sided markets. <em>AOAS</em>, <em>17</em>(4),
2701–2722. (<a href="https://doi.org/10.1214/22-AOAS1700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-sided markets, such as ride-sharing companies, often involve a group of subjects who are making sequential decisions across time and/or location. With the rapid development of smart phones and internet of things, they have substantially transformed the transportation landscape of human beings. In this paper we consider large-scale fleet management in ride-sharing companies that involve multiple units in different areas receiving sequences of products (or treatments) over time. Major technical challenges, such as policy evaluation, arise in those studies because: (i) spatial and temporal proximities induce interference between locations and times, and (ii) the large number of locations results in the curse of dimensionality. To address both challenges simultaneously, we introduce a multiagent reinforcement learning (MARL) framework for carrying policy evaluation in these studies. We propose novel estimators for mean outcomes under different products that are consistent despite the high dimensionality of state-action space. The proposed estimator works favorably in simulation experiments. We further illustrate our method using a real dataset obtained from a two-sided marketplace company to evaluate the effects of applying different subsidizing policies. A Python implementation of our proposed method is available in the Supplementary Material and also at https://github.com/RunzheStat/CausalMARL.},
  archive      = {J_AOAS},
  author       = {Chengchun Shi and Runzhe Wan and Ge Song and Shikai Luo and Hongtu Zhu and Rui Song},
  doi          = {10.1214/22-AOAS1700},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2701-2722},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A multiagent reinforcement learning framework for off-policy evaluation in two-sided markets},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum modeling biomarker ratios with gamma distributed
components. <em>AOAS</em>, <em>17</em>(3), 2700. (<a
href="https://doi.org/10.1214/23-AOAS1777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOAS},
  author       = {Matthias Schmid},
  doi          = {10.1214/23-AOAS1777},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2700},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Corrigendum modeling biomarker ratios with gamma distributed components},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semiparametric promotion time cure model with support
vector machine. <em>AOAS</em>, <em>17</em>(3), 2680–2699. (<a
href="https://doi.org/10.1214/23-AOAS1741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The promotion time cure rate model (PCM) is an extensively studied model for the analysis of time-to-event data in the presence of a cured subgroup. There are several strategies proposed in the literature to model the latency part of PCM. However, there aren’t many strategies proposed to investigate the effects of covariates on the incidence part of PCM. In this regard most existing studies assume the boundary separating the cured and noncured subjects with respect to the covariates to be linear. As such, they can only capture simple effects of the covariates on the cured/noncured probability. In this manuscript we propose a new promotion time cure model that uses the support vector machine (SVM) to model the incidence part. The proposed model inherits the features of the SVM and provides flexibility in capturing nonlinearity in the data. To the best of our knowledge, this is the first work that integrates the SVM with PCM model. For the estimation of model parameters, we develop an expectation maximization algorithm where we make use of the sequential minimal optimization technique together with the Platt scaling method to obtain the posterior probabilities of cured/uncured. A detailed simulation study shows that the proposed model outperforms the existing logistic regression-based PCM model as well as the spline regression-based PCM model, which is also known to capture nonlinearity in the data. This is true in terms of bias and mean square error of different quantities of interest and also in terms of predictive and classification accuracies of cure. Finally, we illustrate the applicability and superiority of our model using the data from a study on leukemia patients who went through bone marrow transplantation.},
  archive      = {J_AOAS},
  author       = {Suvra Pal and Wisdom Aselisewine},
  doi          = {10.1214/23-AOAS1741},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2680-2699},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A semiparametric promotion time cure model with support vector machine},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using predictability to improve matching of urban locations
in philadelphia. <em>AOAS</em>, <em>17</em>(3), 2659–2679. (<a
href="https://doi.org/10.1214/23-AOAS1739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by theories in urban planning and criminology, we use high-resolution data to investigate the relationship between crime and the built environment in the City of Philadelphia. We develop a novel and flexible matching framework that uses the predictability of the treatment variable within matched pairs to empirically inform both the differential weighting of covariates in the matching as well as the selection of the number of matched pairs to create. We use this matching framework for a series of comparisons, each involving matched pairs of Philadelphia intersections that are highly similar on a set of covariates but restricted to differ on a single aspect of the built environment. Our predictability-based matching framework includes data-driven decisions about differential weighting of covariates and the number of matched pairs to create, which is beneficial in our setting as our urban comparisons involve a large number of potential intersections and a large set of covariates to be balanced. In these comparisons we find substantial heterogeneity in the relationships between crime and different aspects of the built environment as well as some empirical support for historical theories.},
  archive      = {J_AOAS},
  author       = {Colman Humphrey and Ryan Gross and Dylan S. Small and Shane T. Jensen},
  doi          = {10.1214/23-AOAS1739},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2659-2679},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Using predictability to improve matching of urban locations in philadelphia},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A horseshoe mixture model for bayesian screening with an
application to light sheet fluorescence microscopy in brain imaging.
<em>AOAS</em>, <em>17</em>(3), 2639–2658. (<a
href="https://doi.org/10.1214/23-AOAS1736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we focus on identifying differentially activated brain regions using a light sheet fluorescence microscopy—a recently developed technique for whole-brain imaging. Most existing statistical methods solve this problem by partitioning the brain regions into two classes: significantly and nonsignificantly activated. However, for the brain imaging problem at the center of our study, such binary grouping may provide overly simplistic discoveries by filtering out weak but important signals that are typically adulterated by the noise present in the data. To overcome this limitation, we introduce a new Bayesian approach that allows classifying the brain regions into several tiers with varying degrees of relevance. Our approach is based on a combination of shrinkage priors, widely used in regression and multiple hypothesis testing problems, and mixture models, commonly used in model-based clustering. In contrast to the existing regularizing prior distributions, which use either the spike-and-slab prior or continuous scale mixtures, our class of priors is based on a discrete mixture of continuous scale mixtures and devises a cluster shrinkage version of the horseshoe prior. As a result, our approach provides a more general setting for Bayesian sparse estimation, drastically reduces the number of shrinkage parameters needed, and creates a framework for sharing information across units of interest. We show that this approach leads to more biologically meaningful and interpretable results in our brain imaging problem, since it allows the discrimination between active and inactive regions, while at the same time ranking the discoveries into clusters representing tiers of similar importance.},
  archive      = {J_AOAS},
  author       = {Francesco Denti and Ricardo Azevedo and Chelsie Lo and Damian G. Wheeler and Sunil P. Gandhi and Michele Guindani and Babak Shahbaba},
  doi          = {10.1214/23-AOAS1736},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2639-2658},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A horseshoe mixture model for bayesian screening with an application to light sheet fluorescence microscopy in brain imaging},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SNIP: An adaptation of sorted neighborhood methods for
deduplicating pedigree data. <em>AOAS</em>, <em>17</em>(3), 2619–2638.
(<a href="https://doi.org/10.1214/23-AOAS1735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedigree data contain family history information that is used to analyze hereditary diseases. These clinical data sets may contain duplicate records due to the same family visiting a clinic multiple times or a clinician entering multiple versions of the family for testing purposes. Inferences drawn from the data or using them for training or validation without removing the duplicates could lead to invalid conclusions, and hence identifying the duplicates is essential. Since family structures can be complex, direct application of existing deduplication algorithms may not be straightforward. We first motivate the importance of deduplication by examining the impact of pedigree duplicates on model performance when training and validating a familial risk prediction model. We then introduce an unsupervised algorithm, which we call SNIP (Sorted NeIghborhood for Pedigrees), that builds on the sorted neighborhood method to find efficiently and to classify pair comparisons by leveraging the inherent hierarchical nature of the pedigrees. We conduct a simulation study to assess the performance of the algorithm and find parameter configurations where the algorithm is able to accurately detect the duplicates. We then apply the method to data from the Risk Service, which includes over 300,000 pedigrees at high risk of hereditary cancers, and uncover large clusters of potential duplicate families. After removing 104,520 pedigrees (33\% of original data), the resulting Risk Service data set can now be used for future analysis, training, and validation. The algorithm is available as an R package snipR at https://github.com/bayesmendel/snipR.},
  archive      = {J_AOAS},
  author       = {Theodore Huang and Matthew Ploenzke and Danielle Braun},
  doi          = {10.1214/23-AOAS1735},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2619-2638},
  shortjournal = {Ann. Appl. Stat.},
  title        = {SNIP: An adaptation of sorted neighborhood methods for deduplicating pedigree data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating GARCH(1,1) in the presence of missing data.
<em>AOAS</em>, <em>17</em>(3), 2596–2618. (<a
href="https://doi.org/10.1214/23-AOAS1734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum likelihood estimation of the famous GARCH(1,1) model is generally straightforward, given the full observation series. However, when some observations are missing, the marginal likelihood of the observed data is intractable in most cases of interest, also intractable is the likelihood from temporally aggregated data. For both these problems, we propose to approximate the intractable likelihoods through sequential Monte Carlo (SMC). The SMC approximation is done in a smooth manner so that the resulting approximate likelihoods can be numerically optimized to obtain parameter estimates. In the case with data aggregation, the use of SMC is made possible by a novel state space representation of the aggregated GARCH series. Through extensive simulation experiments, the proposed method is found to be computationally feasible and produce more accurate estimators of the model parameters compared with other recently published methods, especially in the case with aggregated data. In addition, the Hessian matrix of the minus logarithm of the approximate likelihood can be inverted to produce fairly accurate standard error estimates. The proposed methodology is applied to the analysis of time series data on several exchange-traded funds on the Australian Stock Exchange with missing prices, due to interruptions such as scheduled trading holidays.},
  archive      = {J_AOAS},
  author       = {Damien C. H. Wee and Feng Chen and William T. M. Dunsmuir},
  doi          = {10.1214/23-AOAS1734},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2596-2618},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating GARCH(1,1) in the presence of missing data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian inference and dynamic prediction for multivariate
longitudinal and survival data. <em>AOAS</em>, <em>17</em>(3),
2574–2595. (<a href="https://doi.org/10.1214/23-AOAS1733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a complex neurological disorder impairing multiple domains such as cognition and daily functions. To better understand the disease and its progression, many AD research studies collect multiple longitudinal outcomes that are strongly predictive of the onset of AD dementia. We propose a joint model based on a multivariate functional mixed model framework (referred to as MFMM-JM) that simultaneously models the multiple longitudinal outcomes and the time to dementia onset. We develop six functional forms to fully investigate the complex association between longitudinal outcomes and dementia onset. Moreover, we use the Bayesian methods for statistical inference and develop a dynamic prediction framework that provides accurate personalized predictions of disease progressions based on new subject-specific data. We apply the proposed MFMM-JM to two large ongoing AD studies, the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and National Alzheimer’s Coordinating Center (NACC), and identify the functional forms with the best predictive performance. Our method is also validated by extensive simulation studies with five settings.},
  archive      = {J_AOAS},
  author       = {Haotian Zou and Donglin Zeng and Luo Xiao and Sheng Luo},
  doi          = {10.1214/23-AOAS1733},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2574-2595},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian inference and dynamic prediction for multivariate longitudinal and survival data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure learning for zero-inflated counts with an
application to single-cell RNA sequencing data. <em>AOAS</em>,
<em>17</em>(3), 2555–2573. (<a
href="https://doi.org/10.1214/23-AOAS1732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of estimating the structure of a graph from observed data is of growing interest in the context of high-throughput genomic data and single-cell RNA sequencing in particular. These, however, are challenging applications, since the data consist of high-dimensional counts with high variance and overabundance of zeros. Here we present a general framework for learning the structure of a graph from single-cell RNA-seq data, based on the zero-inflated negative binomial distribution. We demonstrate with simulations that our approach is able to retrieve the structure of a graph in a variety of settings, and we show the utility of the approach on real data.},
  archive      = {J_AOAS},
  author       = {Thi Kim Hue Nguyen and Koen van den Berge and Monica Chiogna and Davide Risso},
  doi          = {10.1214/23-AOAS1732},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2555-2573},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Structure learning for zero-inflated counts with an application to single-cell RNA sequencing data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint modeling of playing time and purchase propensity in
massively multiplayer online role-playing games using crossed random
effects. <em>AOAS</em>, <em>17</em>(3), 2533–2554. (<a
href="https://doi.org/10.1214/23-AOAS1731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massively multiplayer online role-playing games (MMORPGs) offer a unique blend of a personalized gaming experience and a platform for forging social connections. Managers of these digital products rely on predictions of key player responses, such as playing time and purchase propensity, to design timely interventions for promoting, engaging and monetizing their playing base. However, the longitudinal data associated with these MMORPGs not only exhibit a large set of potential predictors to choose from but often present several other distinctive characteristics that pose significant challenges in developing flexible statistical algorithms that can generate efficient predictions of future player activities. For instance, the existence of virtual communities or “guilds” in these games complicate prediction since players who are part of the same guild have correlated behaviors and the guilds themselves evolve over time and thus have a dynamic effect on the future playing behavior of its members. In this paper we develop a crossed random effects joint modeling (CREJM) framework for analyzing correlated player responses in MMORPGs. Contrary to existing methods that assume player independence, CREJM is flexible enough to incorporate both player dependence as well as time-varying guild effects on the future playing behavior of the guild members. On a large-scale data from a popular MMORPG, CREJM conducts simultaneous selection of fixed and random effects in high-dimensional penalized multivariate mixed models. We study the asymptotic properties of the variable selection procedure in CREJM and establish its selection consistency. Besides providing superior predictions of daily playing time and purchase propensity over competing methods, CREJM also predicts player correlations within each guild which are valuable for optimizing future promotional and reward policies for these virtual communities.},
  archive      = {J_AOAS},
  author       = {Trambak Banerjee and Peng Liu and Gourab Mukherjee and Shantanu Dutta and Hai Che},
  doi          = {10.1214/23-AOAS1731},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2533-2554},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Joint modeling of playing time and purchase propensity in massively multiplayer online role-playing games using crossed random effects},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating HIV epidemics for subnational areas.
<em>AOAS</em>, <em>17</em>(3), 2515–2532. (<a
href="https://doi.org/10.1214/23-AOAS1730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global HIV pandemic enters its fifth decade, increasing numbers of countries use routine HIV testing among pregnant women to monitor their epidemics, allowing governments to look into the epidemics at a finer scale, for example, at subnational levels. Currently, the epidemic model that describes the dynamics of the spread of HIV consists of a set of differential equations and is applied independently to each subnational area. However, the availability of the data varies widely which leads to biased and unreliable estimates for areas with very few data points. We propose to overcome this issue by introducing dependence in the parameters across areas. The proposed method better reconstructs the epidemic trajectories than the independent model as shown in multiple countries in Sub-Saharan Africa. We also offer an approximate method for parameter estimation that is much less computationally burdensome than direct parameter estimation. Compared to direct parameter estimation from the dependent model, the approximate method provides competitive parameter estimation in simulations and the application of HIV subepidemic estimation.},
  archive      = {J_AOAS},
  author       = {Le Bao and Xiaoyue Niu and Mary Mahy and Peter D. Ghys},
  doi          = {10.1214/23-AOAS1730},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2515-2532},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating HIV epidemics for subnational areas},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian growth mixture model for complex survey data:
Clustering postdisaster PTSD trajectories. <em>AOAS</em>,
<em>17</em>(3), 2494–2514. (<a
href="https://doi.org/10.1214/23-AOAS1729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on growth mixture models (GMMs) for analyzing data from a complex sample survey is sparse. Existing methods use pseudo-likelihood in which survey weights are incorporated into the likelihood function, with variance estimated via linearization or resampling techniques. Despite popularity of the pseudo-likelihood approach, weighted estimation introduces the risk of efficiency loss. In this paper we propose a Bayesian GMM for complex survey data in which sample design features, such as stratification, clustering, and unequal probability of selection, are incorporated as covariates or hierarchical variance components. The Bayesian GMM can yield a reduction in bias in the estimation of regression coefficients when design features are associated with survey outcomes, and can lead to more efficient estimates than the pseudo-likelihood estimators when the design is noninformative. We develop an efficient Gibbs sampler that includes only closed-form full conditional distributions for model fitting. We present the results of a careful analysis of data from the Galveston Bay Recovery Study (GBRS) which used a stratified multi-stage cluster sample design. Using our proposed Bayesian GMM, we characterize longitudinal trajectories of post-traumatic stress disorder (PTSD) among residents of southeastern Texas in the aftermath of Hurricane Ike. We identify four clinically meaningful PTSD trajectory subgroups and characterize risk factors associated with subgroup membership. In the absence of existing software that can be used to implement our proposed Bayesian GMM for complex survey data, we built the R package Bsvygmm for model fitting, selection, and checking which can be downloaded from https://github.com/anthopolos/Bsvygmm.},
  archive      = {J_AOAS},
  author       = {Rebecca Anthopolos and Qixuan Chen and Joseph Sedransk and Mary Thompson and Gang Meng and Sandro Galea},
  doi          = {10.1214/23-AOAS1729},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2494-2514},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian growth mixture model for complex survey data: Clustering postdisaster PTSD trajectories},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient doubly-robust imputation framework for
longitudinal dropout, with an application to an alzheimer’s clinical
trial. <em>AOAS</em>, <em>17</em>(3), 2473–2493. (<a
href="https://doi.org/10.1214/23-AOAS1728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel doubly-robust (DR) imputation framework for longitudinal studies with monotone dropout, motivated by the informative dropout that is common in FDA-regulated trials for Alzheimer’s disease. In this approach the missing data are first imputed using a doubly-robust augmented inverse probability weighting (AIPW) estimator; then the imputed completed data are substituted into a full-data estimating equation, and the estimate is obtained using standard software. The imputed completed data may be inspected and compared to the observed data, and standard model diagnostics are available. The same imputed completed data can be used for several different estimands, such as subgroup analyses in a clinical trial, allowing for reduced computation and increased consistency across analyses. We present two specific DR imputation estimators, AIPW-I and AIPW-S, study their theoretical properties, and investigate their performance by simulation. AIPW-S has substantially reduced computational burden, compared to many other DR estimators, at the cost of some loss of efficiency and the requirement of stronger assumptions. Simulation studies support the theoretical properties and good performance of the DR imputation framework. Importantly, we demonstrate their ability to address time-varying covariates, such as a time by treatment interaction. We illustrate using data from a large randomized Phase III trial, investigating the effect of donepezil in Alzheimer’s disease, from the Alzheimer’s Disease Cooperative Study (ADCS) group.},
  archive      = {J_AOAS},
  author       = {Yuqi Qiu and Karen Messer},
  doi          = {10.1214/23-AOAS1728},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2473-2493},
  shortjournal = {Ann. Appl. Stat.},
  title        = {An efficient doubly-robust imputation framework for longitudinal dropout, with an application to an alzheimer’s clinical trial},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imputation scores. <em>AOAS</em>, <em>17</em>(3), 2452–2472.
(<a href="https://doi.org/10.1214/22-AOAS1727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the prevalence of missing data in modern statistical research, a broad range of methods is available for any given imputation task. How does one choose the “best” imputation method in a given application? The standard approach is to select some observations, set their status to missing, and compare prediction accuracy of the methods under consideration of these observations. Besides having to somewhat artificially mask observations, a shortcoming of this approach is that imputations based on the conditional mean will rank highest if predictive accuracy is measured with quadratic loss. In contrast, we want to rank highest an imputation that can sample from the true conditional distributions. In this paper we develop a framework called “Imputation Scores” (I-Scores) for assessing missing value imputations. We provide a specific I-Score, based on density ratios and projections, that is applicable to discrete and continuous data. It does not require to mask additional observations for evaluations and is also applicable if there are no complete observations. The population version is shown to be proper in the sense that the highest rank is assigned to an imputation method that samples from the correct conditional distribution. The propriety is shown under the missing completely at random (MCAR) assumption but is also shown to be valid under missing at random (MAR) with slightly more restrictive assumptions. We show empirically on a range of data sets and imputation methods that our score consistently ranks true data high(est) and is able to avoid pitfalls usually associated with performance measures such as RMSE. Finally, we provide the R-package Iscores available on CRAN with an implementation of our method.},
  archive      = {J_AOAS},
  author       = {Jeffrey Näf and Meta-Lina Spohn and Loris Michel and Nicolai Meinshausen},
  doi          = {10.1214/22-AOAS1727},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2452-2472},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Imputation scores},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the use of generalized dynamic weighted ordinary
least squares for individualized HIV treatment strategies.
<em>AOAS</em>, <em>17</em>(3), 2432–2451. (<a
href="https://doi.org/10.1214/22-AOAS1726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamic treatment regimes (DTR) represents a statistical paradigm in precision medicine which aims to optimize patient outcomes by individualizing treatments. At its simplest, a DTR may require only a single decision to be made; this special case is called an individualized treatment rule (ITR) and is often used to maximize short-term rewards. Generalized dynamic weighted ordinary least squares (G-dWOLS), a DTR estimation method that offers theoretical advantages such as double robustness of parameter estimators in the decision rules, has been recently extended to accommodate categorical treatments. In this work G-dWOLS is applied to longitudinal data to estimate an optimal ITR. This novel method is demonstrated in simulations and is then applied to a population affected by HIV, whereby an ITR for the administration of Interleukin 7 (IL-7) is devised to maximize the duration where the CD4 load is above a healthy threshold (500 cells/μL) while preventing the administration of unnecessary injections.},
  archive      = {J_AOAS},
  author       = {Larry Dong and Erica E. M. Moodie and Laura Villain and Rodolphe Thiébaut},
  doi          = {10.1214/22-AOAS1726},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2432-2451},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Evaluating the use of generalized dynamic weighted ordinary least squares for individualized HIV treatment strategies},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Signal-noise ratio of genetic associations and statistical
power of SNP-set tests. <em>AOAS</em>, <em>17</em>(3), 2410–2431. (<a
href="https://doi.org/10.1214/22-AOAS1725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SNP-set analysis is a powerful tool for dissecting the genetics of complex human diseases. There are three fundamental genetic association approaches to SNR-set analysis: the marginal model fitting approach, the joint model fitting approach, and the decorrelation approach. A problem of primary interest is how these approaches compare with each other. To address this problem, we develop a theoretical platform to compare the signal-to-noise ratio (SNR) of these approaches under the generalized linear model. We elaborate how causal genetic effects give rise to statistically detectable association signals and show that, when causal effects spread over blocks of strong linkage disequilibrium (LD), the SNR of the marginal model fitting is usually higher than that of the decorrelation approach which, in turn, is higher than that of the unbiased joint model fitting approach. We also scrutinize dense effects and LDs by a bivariate model and extensive simulations using the 1000 Genome Project data. Last, we compare the statistical power of two generic types of SNP-set tests (summation-based and supremum-based) by simulations and an osteoporosis study using large data from UK Biobank. Our results help develop powerful tools for SNP-set analysis and understand the signal detection problem in the presence of colored noise.},
  archive      = {J_AOAS},
  author       = {Hong Zhang and Ming Liu and Jiashun Jin and Zheyang Wu},
  doi          = {10.1214/22-AOAS1725},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2410-2431},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Signal-noise ratio of genetic associations and statistical power of SNP-set tests},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doubly-online changepoint detection for monitoring health
status during sports activities. <em>AOAS</em>, <em>17</em>(3),
2387–2409. (<a href="https://doi.org/10.1214/22-AOAS1724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide an online framework for analyzing data recorded by smart watches during running activities. In particular, we focus on identifying variations in the behavior of one or more measurements caused by changes in physical condition, such as physical discomfort, periods of prolonged de-training, or even the malfunction of measuring devices. Our framework considers data as a sequence of running activities represented by multivariate time series of physical and biometric data. We combine classical changepoint detection models with an unknown number of components with Gaussian state space models to detect distributional changes between a sequence of activities. The model considers multiple sources of dependence due to the sequential nature of subsequent activities, the autocorrelation structure within each activity, and the contemporaneous dependence between different variables. We provide an online expectation-maximization (EM) algorithm involving a sequential Monte Carlo (SMC) approximation of changepoint predicted probabilities. As a byproduct of our model assumptions, our proposed approach processes sequences of multivariate time series in a doubly-online framework. While classical changepoint models detect changes between subsequent activities, the state space framework, coupled with the online EM algorithm, provides the additional benefit of estimating the real-time probability that a current activity is a changepoint.},
  archive      = {J_AOAS},
  author       = {Mattia Stival and Mauro Bernardi and Petros Dellaportas},
  doi          = {10.1214/22-AOAS1724},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2387-2409},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Doubly-online changepoint detection for monitoring health status during sports activities},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subbotin graphical models for extreme value dependencies
with applications to functional neuronal connectivity. <em>AOAS</em>,
<em>17</em>(3), 2364–2386. (<a
href="https://doi.org/10.1214/22-AOAS1723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With modern calcium imaging technology, activities of thousands of neurons can be recorded in vivo. These experiments can potentially provide new insights into intrinsic functional neuronal connectivity, defined as contemporaneous correlations between neuronal activities. As a common tool for estimating conditional dependencies in high-dimensional settings, graphical models are a natural choice for estimating functional connectivity networks. However, raw neuronal activity data presents a unique challenge: the relevant information in the data lies in rare extreme value observations that indicate neuronal firing rather than in the observations near the mean. Existing graphical modeling techniques for extreme values rely on binning or thresholding observations which may not be appropriate for calcium imaging data. In this paper we develop a novel class of graphical models, called the Subbotin graphical model, which finds sparse conditional dependency structures with respect to the extreme value observations without requiring data pre-processing. We first derive the form of the Subbotin graphical model and show the conditions under which it is normalizable. We then study the empirical performance of the Subbotin graphical model and compare it to existing extreme value graphical modeling techniques and functional connectivity models from neuroscience through several simulation studies as well as a real-world calcium imaging data example.},
  archive      = {J_AOAS},
  author       = {Andersen Chang and Genevera I. Allen},
  doi          = {10.1214/22-AOAS1723},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2364-2386},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Subbotin graphical models for extreme value dependencies with applications to functional neuronal connectivity},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Penalized estimating equations for generalized linear models
with multiple imputation. <em>AOAS</em>, <em>17</em>(3), 2345–2363. (<a
href="https://doi.org/10.1214/22-AOAS1721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values among variables present a challenge in variable selection in the generalized linear model. Common strategies that delete observations with missing information may cause serious information loss. Multiple imputation has been widely used in recent years because it provides unbiased statistical results given a correctly specified imputation model and considers the uncertainty of the missing data. However, variable selection methods in the generalized linear model with multiply-imputed data have not yet been studied widely. In this study, we introduce penalized estimating equations for generalized linear models with multiple imputation (PEE–MI), which incorporates the correlation of multiple imputed observations into the objective function. The theoretical performance of the proposed PEE–MI depends on the penalized function adopted. We use the adaptive least absolute shrinkage and selection operator (adaptive LASSO) as an illustrating example. Simulations show that PEE–MI outperforms the alternatives. The proposed method is shown to select variables with clinical relevance when applied to a database of laboratory-diagnosed A/H7N9 patients in the Zhejiang province, China.},
  archive      = {J_AOAS},
  author       = {Yang Li and Haoyu Yang and Haochen Yu and Hanwen Huang and Ye Shen},
  doi          = {10.1214/22-AOAS1721},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2345-2363},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Penalized estimating equations for generalized linear models with multiple imputation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic screening algorithm for hierarchical binary
marketing data. <em>AOAS</em>, <em>17</em>(3), 2326–2344. (<a
href="https://doi.org/10.1214/22-AOAS1720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications of business and marketing analytics, predictive models are fit using hierarchically structured data: common characteristics of products, customers, or web pages are represented as categorical variables, and each category can be split up into multiple subcategories at a lower level of the hierarchy. The model may thus contain hundreds of thousands of binary variables, necessitating the use of variable selection to screen out large numbers of irrelevant or insignificant features. We propose a new dynamic screening method, based on the distance correlation criterion, designed for hierarchical binary data. Our method can screen out large parts of the hierarchy at the higher levels, avoiding the need to explore many lower-level features and greatly reducing the computational cost of screening. The practical potential of the method is demonstrated in a case application on user-brand interaction data from Facebook.},
  archive      = {J_AOAS},
  author       = {Yimei Fan and Yuan Liao and Ilya O. Ryzhov and Kunpeng Zhang},
  doi          = {10.1214/22-AOAS1720},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2326-2344},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A dynamic screening algorithm for hierarchical binary marketing data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial quantile autoregression for season within year daily
maximum temperature data. <em>AOAS</em>, <em>17</em>(3), 2305–2325. (<a
href="https://doi.org/10.1214/22-AOAS1719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression is the most widely used modeling tool in statistics. Quantile regression offers a strategy for enhancing the regression picture beyond customary mean regression. With time-series data, we move to quantile autoregression and, finally, with spatially referenced time series, we move to space-time quantile regression. Here, we are concerned with the spatiotemporal evolution of daily maximum temperature, particularly with regard to extreme heat. Our motivating data set is 60 years of daily summer maximum temperature data over Aragón in Spain. Hence, we work with time on two scales—days within summer season across years—collected at geocoded station locations. For a specified quantile, we fit a very flexible, mixed-effects autoregressive model, introducing four spatial processes. We work with asymmetric Laplace errors to take advantage of the available conditional Gaussian representation for these distributions. Further, while the autoregressive model yields conditional quantiles, we demonstrate how to extract marginal quantiles with the asymmetric Laplace specification. Thus, we are able to interpolate quantiles for any days within years across our study region.},
  archive      = {J_AOAS},
  author       = {Jorge Castillo-Mateo and Jesús Asín and Ana C. Cebrián and Alan E. Gelfand and Jesús Abaurrea},
  doi          = {10.1214/22-AOAS1719},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2305-2325},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatial quantile autoregression for season within year daily maximum temperature data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The bayesian nested lasso for mixed frequency regression
models. <em>AOAS</em>, <em>17</em>(3), 2279–2304. (<a
href="https://doi.org/10.1214/22-AOAS1718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though many time series are sampled at different frequencies, their joint evolution is usually modeled and analyzed at a common low frequency. The mixed data sampling (MIDAS) framework was developed to enable joint modeling of mixed frequency temporally evolving data with GDP forecasting as a key motivating application. In this paper we develop a fully Bayesian method to jointly estimate both the appropriate lag as well as the regression coefficients in linear models wherein the response is measured at a lower frequency than the predictors. This is accomplished through a novel prior distribution, coined the Bayesian nested lasso (BNL), that leads to principled selection of the lag of the predictors, reduces the effective number of model parameters through sparsity induced by the lasso component and finally incorporates desirable decay patterns over time lags in the magnitude of the corresponding regression coefficients. Further, it is easy to obtain samples from the posterior distribution due to the closed form expressions for the conditional distributions of the model parameters. Numerical results, obtained from synthetic and macroeconomic data, illustrate the good performance of the proposed Bayesian framework in parameter selection and estimation and in the key task of GDP forecasting.},
  archive      = {J_AOAS},
  author       = {Satyajit Ghosh and Kshitij Khare and George Michailidis},
  doi          = {10.1214/22-AOAS1718},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2279-2304},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The bayesian nested lasso for mixed frequency regression models},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian nonparametric mixture modeling for temporal
dynamics of gender stereotypes. <em>AOAS</em>, <em>17</em>(3),
2256–2278. (<a href="https://doi.org/10.1214/22-AOAS1717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of temporal dynamics of gender and ethnic stereotypes is an important topic in many disciplines at the intersection between statistics and social sciences. In this paper we make use of word “embeddings,” a common tool in natural language processing and of Bayesian nonparametric mixture modeling for the analysis of temporal dynamics of gender stereotypes in adjectives and occupation over the 20th and 21st centuries in the United States. Our Bayesian nonparametric approach relies on a novel dependent Dirichlet process prior, and it allows for both dynamic density estimation and dynamic clustering of adjective embedding and occupation embedding biases in a hierarchical setting. Posterior inference is performed through a particle Markov chain Monte Carlo algorithm, which is simple and computationally efficient. An application to time-dependent data for adjective embedding bias and for occupation embedding bias shows that our approach enables the quantification of historical trends of gender stereotypes and hence allows to identify how specific adjectives and occupations have become more closely associated with a female rather than male over time.},
  archive      = {J_AOAS},
  author       = {Maria De Iorio and Stefano Favaro and Alessandra Guglielmi and Lifeng Ye},
  doi          = {10.1214/22-AOAS1717},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2256-2278},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian nonparametric mixture modeling for temporal dynamics of gender stereotypes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using proxies to improve forecast evaluation. <em>AOAS</em>,
<em>17</em>(3), 2236–2255. (<a
href="https://doi.org/10.1214/22-AOAS1716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative evaluation of forecasts of statistical functionals relies on comparing averaged losses of competing forecasts after the realization of the quantity Y, on which the functional is based, has been observed. Motivated by high-frequency finance, in this paper we investigate how proxies Y˜ for Y—say volatility proxies—which are observed together with Y can be utilized to improve forecast comparisons. We extend previous results on robustness of loss functions for the mean to general moments and ratios of moments, and show in terms of the variance of differences of losses that using proxies will increase the power in comparative forecast tests. These results apply both to testing conditional as well as unconditional dominance. Finally, we numerically illustrate the theoretical results, both for simulated high-frequency data as well as for high-frequency log returns of several cryptocurrencies.},
  archive      = {J_AOAS},
  author       = {Hajo Holzmann and Bernhard Klar},
  doi          = {10.1214/22-AOAS1716},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2236-2255},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Using proxies to improve forecast evaluation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian combinatorial MultiStudy factor analysis.
<em>AOAS</em>, <em>17</em>(3), 2212–2235. (<a
href="https://doi.org/10.1214/22-AOAS1715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutations in the BRCA1 and BRCA2 genes are known to be highly associated with breast cancer. Identifying both shared and unique transcript expression patterns in blood samples from these groups can shed insight into if and how the disease mechanisms differ among individuals by mutation status, but this is challenging in the high-dimensional setting. A recent method, Bayesian multistudy factor analysis (BMSFA), identifies latent factors common to all studies (or equivalently, groups) and latent factors specific to individual studies. However, BMSFA does not allow for factors shared by more than one but less than all studies. This is critical in our context, as we may expect some but not all signals to be shared by BRCA1- and BRCA2-mutation carriers but not necessarily other high-risk groups. We extend BMSFA by introducing a new method, Tetris, for Bayesian combinatorial multistudy factor analysis which identifies latent factors that any combination of studies or groups can share. We model the subsets of studies that share latent factors with an Indian buffet process and offer a way to summarize uncertainty in the sharing patterns using credible balls. We test our method with an extensive range of simulations and showcase its utility not only in dimension reduction but also in covariance estimation. When applied to transcript expression data from high-risk families grouped by mutation status, Tetris reveals the features and pathways characterizing each group and the sharing patterns among them. Finally, we further extend Tetris to discover groupings of samples when group labels are not provided which can elucidate additional structure in these data.},
  archive      = {J_AOAS},
  author       = {Isabella N. Grabski and Roberta De Vito and Lorenzo Trippa and Giovanni Parmigiani},
  doi          = {10.1214/22-AOAS1715},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2212-2235},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian combinatorial MultiStudy factor analysis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using persistent homology topological features to
characterize medical images: Case studies on lung and brain cancers.
<em>AOAS</em>, <em>17</em>(3), 2192–2211. (<a
href="https://doi.org/10.1214/22-AOAS1714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tumor shape is a key factor that affects tumor growth and metastasis. This paper proposes a topological feature computed by persistent homology to characterize tumor progression from digital pathology and radiology images and examines its effect on the time-to-event data. The proposed topological features are invariant to scale-preserving transformation and can summarize various tumor shape patterns. The topological features are represented in functional space and used as functional predictors in a functional Cox proportional hazards model. The proposed model enables interpretable inference about the association between topological shape features and survival risks. Two case studies are conducted using consecutive 133 lung cancer and 77 brain tumor patients. The results of both studies show that the topological features predict survival prognosis after adjusting clinical variables, and the predicted high-risk groups have worse survival outcomes than the low-risk groups. Also, the topological shape features found to be positively associated with survival hazards are irregular and heterogeneous shape patterns which are known to be related to tumor progression.},
  archive      = {J_AOAS},
  author       = {Chul Moon and Qiwei Li and Guanghua Xiao},
  doi          = {10.1214/22-AOAS1714},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2192-2211},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Using persistent homology topological features to characterize medical images: Case studies on lung and brain cancers},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating causal effects of HIV prevention interventions
with interference in network-based studies among people who inject
drugs. <em>AOAS</em>, <em>17</em>(3), 2165–2191. (<a
href="https://doi.org/10.1214/22-AOAS1713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating causal effects in the presence of interference is challenging in network-based studies of hard-to-reach populations. Like many such populations, people who inject drugs (PWID) are embedded in social networks and one person’s treatment can affect the outcomes of others in the network. In our setting, the study design is observational with a nonrandomized network-based HIV prevention intervention. Information is available on each participant and their connections that confer possible HIV risk through injection and sexual behaviors. We considered two inverse probability weighted (IPW) estimators to quantify the population-level spillover effects of nonrandomized interventions on subsequent health outcomes. We demonstrated that these two IPW estimators are consistent, asymptotically normal, and derived a closed-form estimator for the asymptotic variance, while allowing for overlapping interference sets (groups of individuals in which the interference is assumed possible). A simulation study was conducted to evaluate the finite-sample performance of the estimators. We analyzed data from the Transmission Reduction Intervention Project which ascertained a network of PWID and their contacts in Athens, Greece, from 2013 to 2015. We evaluated the effects of community alerts on subsequent HIV risk behavior in this observed network, where the connections or links between participants were defined by using substances or having unprotected sex together. In the study, community alerts were distributed to inform people of recent HIV infections among individuals in close proximity in the observed network. The estimates of the risk differences for spillover, using either IPW estimator demonstrated a protective effect. The results suggest that HIV risk behavior could be mitigated by exposure to a community alert when an increased risk of HIV is detected in the network.},
  archive      = {J_AOAS},
  author       = {TingFang Lee and Ashley L. Buchanan and Natallia V. Katenka and Laura Forastiere and M. Elizabeth Halloran and Samuel R. Friedman and Georgios Nikolopoulos},
  doi          = {10.1214/22-AOAS1713},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2165-2191},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating causal effects of HIV prevention interventions with interference in network-based studies among people who inject drugs},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging population outcomes to improve the generalization
of experimental results: Application to the JTPA study. <em>AOAS</em>,
<em>17</em>(3), 2139–2164. (<a
href="https://doi.org/10.1214/22-AOAS1712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizing causal estimates in randomized experiments to a broader target population is essential for guiding decisions by policymakers and practitioners in the social and biomedical sciences. While recent papers have developed various weighting estimators for the population average treatment effect (PATE), many of these methods result in large variance because the experimental sample often differs substantially from the target population and estimated sampling weights are extreme. We investigate this practical problem motivated by an evaluation study of the Job Training Partnership Act (JTPA), where we examine how well we can generalize the causal effect of job training programs beyond a specific population of economically disadvantaged adults and youths. In particular, we propose post-residualized weighting in which we use the outcome measured in the observational population data to build a flexible predictive model (e.g., with machine learning) and residualize the outcome in the experimental data before using conventional weighting methods. We show that the proposed PATE estimator is consistent under the same assumptions required for existing weighting methods, importantly without assuming the correct specification of the predictive model. We demonstrate the efficiency gains from this approach through our JTPA application: we find a reduction of between 5\% and 25\% in variance.},
  archive      = {J_AOAS},
  author       = {Melody Huang and Naoki Egami and Erin Hartman and Luke Miratrix},
  doi          = {10.1214/22-AOAS1712},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2139-2164},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Leveraging population outcomes to improve the generalization of experimental results: Application to the JTPA study},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian model selection: Application to the adjustment of
fundamental physical constants. <em>AOAS</em>, <em>17</em>(3),
2118–2138. (<a href="https://doi.org/10.1214/22-AOAS1710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method originally suggested by Raymond Birge, using what came to be known as the Birge ratio, has been widely used in metrology and physics for the adjustment of fundamental physical constants, particularly in the periodic reevaluation carried out by the Task Group on Fundamental Physical Constants of CODATA (the Committee on Data of the International Science Council). The method involves increasing the reported uncertainties by a multiplicative factor large enough to make the measurement results mutually consistent. An alternative approach, predominant in the meta-analysis of medical studies, involves inflating the reported uncertainties by combining them, using the root sum of squares, with a sufficiently large constant (often dubbed dark uncertainty) that is estimated from the data. In this contribution we establish a connection between the method based on the Birge ratio and the location-scale model, which allows one to combine the results of various studies, while the additive adjustment is reviewed in the usual context of random-effects models. Framing these alternative approaches as statistical models facilitates a quantitative comparison of them using statistical tools for model comparison. The intrinsic Bayes factor (IBF) is derived for the Berger and Bernardo reference prior, and then it is used to select a model for a set of measurements of the Newtonian constant of gravitation (“Big G”) to estimate a consensus value for this constant and to evaluate the associated uncertainty. Our empirical findings support the method based on the Birge ratio. The same conclusion is reached when the IBF corresponding to the Jeffreys prior is used and also when the comparison is based on the Akaike information criterion (AIC). Finally, the results of a simulation study indicate that the suggested procedure for model selection provides clear guidance, even when the data comprise only a small number of measurements.},
  archive      = {J_AOAS},
  author       = {Olha Bodnar and Viktor Eriksson},
  doi          = {10.1214/22-AOAS1710},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2118-2138},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian model selection: Application to the adjustment of fundamental physical constants},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-aware modeling of brain connectivity networks.
<em>AOAS</em>, <em>17</em>(3), 2095–2117. (<a
href="https://doi.org/10.1214/22-AOAS1709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional connections in the brain are frequently represented by weighted networks, with nodes representing locations in the brain and edges representing the strength of connectivity between these locations. One challenge in analyzing such data is that inference at the individual edge level is not particularly biologically meaningful; interpretation is more useful at the level of so-called functional systems or groups of nodes and connections between them; this is often called “graph-aware” inference in the neuroimaging literature. However, pooling over functional regions leads to significant loss of information and lower accuracy. Another challenge is correlation among edge weights within a subject which makes inference based on independence assumptions unreliable. We address both of these challenges with a linear mixed effects model, which accounts for functional systems and for edge dependence, while still modeling individual edge weights to avoid loss of information. The model allows for comparing two populations, such as patients and healthy controls, both at the functional regions level and at individual edge level, leading to biologically meaningful interpretations. We fit this model to resting state fMRI data on schizophrenic patients and healthy controls, obtaining interpretable results consistent with the schizophrenia literature.},
  archive      = {J_AOAS},
  author       = {Yura Kim and Daniel Kessler and Elizaveta Levina},
  doi          = {10.1214/22-AOAS1709},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2095-2117},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Graph-aware modeling of brain connectivity networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Log-gaussian cox process modeling of large spatial lightning
data using spectral and laplace approximations. <em>AOAS</em>,
<em>17</em>(3), 2078–2094. (<a
href="https://doi.org/10.1214/22-AOAS1708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightning is a destructive and highly visible product of severe storms, yet there is still much to be learned about the conditions under which lightning is most likely to occur. The GOES-16 and GOES-17 satellites, launched in 2016 and 2018 by NOAA and NASA, collect a wealth of data regarding individual lightning strike occurrence and potentially related atmospheric variables. The acute nature and inherent spatial correlation in lightning data renders standard regression analyses inappropriate. Further, computational considerations are foregrounded by the desire to analyze the immense and rapidly increasing volume of lightning data. We present a new computationally feasible method that combines spectral and Laplace approximations in an EM algorithm, denoted SLEM, to fit the widely popular log-Gaussian Cox process model to large spatial point pattern datasets. In simulations we find SLEM is competitive with contemporary techniques in terms of speed and accuracy. When applied to two lightning datasets, SLEM provides better out-of-sample prediction scores and quicker runtimes, suggesting its particular usefulness for analyzing lightning data which tend to have sparse signals.},
  archive      = {J_AOAS},
  author       = {Megan L. Gelsinger and Maryclare Griffin and David Matteson and Joseph Guinness},
  doi          = {10.1214/22-AOAS1708},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2078-2094},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Log-gaussian cox process modeling of large spatial lightning data using spectral and laplace approximations},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Postelection analysis of presidential election/poll data.
<em>AOAS</em>, <em>17</em>(3), 2059–2077. (<a
href="https://doi.org/10.1214/22-AOAS1707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns analyses of the 2016 and 2020 U.S. presidential election data, including the data of preelection polls and the actual elections. Our analyses unveil statistical evidence of discrepancy between the polls and real elections that is consistent across these two elections. Specifically, the polls had consistently overestimated advantages of the Democratic candidates or, equivalently, underestimated the true population support of the Republican candidate, Donald Trump, in both elections. The analyses are stratified by state, reflecting the U.S. electoral college system by the means of small area estimation. We have found recurrent patterns suggesting that the polls have been underestimating the Republican candidate, especially in swing states of critical importance. Our findings also suggest an improvement of the 2020 polling methods to mitigate the size of underestimation. We show that a small-area model built upon the actual election data from one election can provide a better prediction than the poll-based projection to another election involving the same Republican candidate. Ranking of pollsters, based on prediction bias, using mixed model prediction is also considered.},
  archive      = {J_AOAS},
  author       = {Jiming Jiang and Yuanyuan Li and Peter X. K. Song},
  doi          = {10.1214/22-AOAS1707},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2059-2077},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Postelection analysis of presidential election/poll data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic prediction of residual life with longitudinal
covariates using long short-term memory networks. <em>AOAS</em>,
<em>17</em>(3), 2039–2058. (<a
href="https://doi.org/10.1214/22-AOAS1706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis, a complex medical condition that involves severe infections with life-threatening organ dysfunction, is a leading cause of death worldwide. Treatment of sepsis is highly challenging. When making treatment decisions, clinicians and patients desire accurate predictions of mean residual life (MRL) that leverage all available patient information, including longitudinal biomarker data. Biomarkers are biological, clinical, and other variables reflecting disease progression that are often measured repeatedly on patients in the clinical setting. Dynamic prediction methods leverage accruing biomarker measurements to improve performance, providing updated predictions as new measurements become available. We introduce two methods for dynamic prediction of MRL using longitudinal biomarkers. In both methods, we begin by using long short-term memory networks (LSTMs) to construct encoded representations of the biomarker trajectories, referred to as “context vectors.” In our first method, the LSTM-GLM, we dynamically predict MRL via a transformed MRL model that includes the context vectors as covariates. In our second method, the LSTM-NN, we dynamically predict MRL from the context vectors using a feed-forward neural network. We demonstrate the improved performance of both proposed methods relative to competing methods in simulation studies. We apply the proposed methods to dynamically predict the restricted mean residual life (RMRL) of septic patients in the intensive care unit using electronic medical record data. We demonstrate that the LSTM-GLM and the LSTM-NN are useful tools for producing individualized, real-time predictions of RMRL that can help inform the treatment decisions of septic patients.},
  archive      = {J_AOAS},
  author       = {Grace Rhodes and Marie Davidian and Wenbin Lu},
  doi          = {10.1214/22-AOAS1706},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2039-2058},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Dynamic prediction of residual life with longitudinal covariates using long short-term memory networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-adaptive discriminative feature localization with
statistically guaranteed interpretation. <em>AOAS</em>, <em>17</em>(3),
2019–2038. (<a href="https://doi.org/10.1214/22-AOAS1705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In explainable artificial intelligence, discriminative feature localization is critical to reveal a black-box model’s decision-making process from raw data to prediction. In this article we use two real datasets, the MNIST handwritten digits and MIT-BIH electrocardiogram (ECG) signals, to motivate key characteristics of discriminative features, namely, adaptiveness, predictive importance and effectiveness. Then we develop a localization framework, based on adversarial attacks, to effectively localize discriminative features. In contrast to existing heuristic methods, we also provide a statistically guaranteed interpretability of the localized features by measuring a generalized partial R2. We apply the proposed method to the MNIST dataset and the MIT-BIH dataset with a convolutional autoencoder. In the first, the compact image regions localized by the proposed method are visually appealing. Similarly, in the second, the identified ECG features are biologically plausible and consistent with cardiac electrophysiological principles while locating subtle anomalies in a QRS complex that may not be discernible by the naked eye. Overall, the proposed method compares favorably with state-of-the-art competitors. Accompanying this paper is a Python library dnn-locate that implements the proposed approach.},
  archive      = {J_AOAS},
  author       = {Ben Dai and Xiaotong Shen and Lin Yee Chen and Chunlin Li and Wei Pan},
  doi          = {10.1214/22-AOAS1705},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2019-2038},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Data-adaptive discriminative feature localization with statistically guaranteed interpretation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint point and variance estimation under a hierarchical
bayesian model for survey count data. <em>AOAS</em>, <em>17</em>(3),
2002–2018. (<a href="https://doi.org/10.1214/22-AOAS1704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Bayesian framework for the joint modeling of survey point and variance estimates for count data. The approach incorporates an induced prior distribution on the modeled true variance that sets it equal to the generating variance of the point estimate, a key property more readily achieved for continuous data response type models. Our count data model formulation allows the input of domains at multiple resolutions (e.g., states, regions, nation) and simultaneously benchmarks modeled estimates at higher resolutions (e.g., states) to those at lower resolutions (e.g., regions) in a fashion that borrows more strength to sharpen our domain estimates at higher resolutions. We conduct a simulation study that generates a population of units within domains to produce ground truth statistics to compare to direct and modeled estimates performed on samples taken from the population where we show improved reductions in error across domains. The model is applied to the job openings variable and other data items published in the Job Openings and Labor Turnover Survey administered by the U.S. Bureau of Labor Statistics.},
  archive      = {J_AOAS},
  author       = {Terrance D. Savitsky and Julie Gershunskaya and Mark Crankshaw},
  doi          = {10.1214/22-AOAS1704},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2002-2018},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Joint point and variance estimation under a hierarchical bayesian model for survey count data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian hierarchical model framework to quantify
uncertainty of tropical cyclone precipitation forecasts. <em>AOAS</em>,
<em>17</em>(3), 1984–2001. (<a
href="https://doi.org/10.1214/22-AOAS1703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tropical cyclones present a serious threat to many coastal communities around the world. Many numerical weather prediction models provide deterministic forecasts with limited measures of their forecast uncertainty. Standard postprocessing techniques may struggle with extreme events or use a 30-day training window that will not adequately characterize the uncertainty of a tropical cyclone forecast. We propose a novel approach that leverages information from past storm events, using a hierarchical model to quantify uncertainty in the spatial correlation parameters of the forecast errors (modeled as Gaussian processes) for a numerical weather prediction model. This approach addresses a massive data problem by implementing a drastic dimension reduction through the assumption that the MLE and Hessian matrix represent all useful information from each tropical cyclone. From this, simulated forecast errors provide uncertainty quantification for future tropical cyclone forecasts. We apply this method to the North American Mesoscale model forecasts and use observations based on the Stage IV data product for 47 tropical cyclones between 2004 and 2017. For an incoming storm, our hierarchical framework combines the forecast from the North American Mesoscale model with the information from previous storms to create 95\% and 99\% prediction maps of rain. For six test storms from 2018 and 2019, these maps provide appropriate probabilistic coverage of observations. We show evidence from the log scoring rule that the proposed hierarchical framework performs best among competing methods.},
  archive      = {J_AOAS},
  author       = {Stephen Walsh and Marco A. R. Ferreira and David Higdon and Stephanie Zick},
  doi          = {10.1214/22-AOAS1703},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1984-2001},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian hierarchical model framework to quantify uncertainty of tropical cyclone precipitation forecasts},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The scalable birth–death MCMC algorithm for mixed graphical
model learning with application to genomic data integration.
<em>AOAS</em>, <em>17</em>(3), 1958–1983. (<a
href="https://doi.org/10.1214/22-AOAS1701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in biological research have seen the emergence of high-throughput technologies with numerous applications that allow the study of biological mechanisms at an unprecedented depth and scale. A large amount of genomic data is now distributed through consortia like The Cancer Genome Atlas (TCGA), where specific types of biological information on specific type of tissue or cell are available. In cancer research the challenge is now to perform integrative analyses of high-dimensional multiomic data with the goal to better understand genomic processes that correlate with cancer outcomes, for example, elucidate gene networks that discriminate a specific cancer subgroups (cancer subtyping) or discovering gene networks that overlap across different cancer types (pan-cancer studies). In this paper we propose a novel mixed graphical model approach to analyze multiomic data of different types (continuous, discrete and count) and perform model selection by extending the birth–death MCMC (BDMCMC) algorithm initially proposed by Stephens (Ann. Statist. 28 (2000) 40–74) and later developed by Mohammadi and Wit (Bayesian Anal. 10 (2015) 109–138). Using simulations, we compare the performance of our method to the LASSO method and the standard BDMCMC method and find that our method is superior in terms of both computational efficiency and the accuracy of the model selection results. Finally, an application to the TCGA breast cancer data shows that integrating genomic information at different levels (mutation and expression data) leads to better subtyping of breast cancers.},
  archive      = {J_AOAS},
  author       = {Nanwei Wang and Hélène Massam and Xin Gao and Laurent Briollais},
  doi          = {10.1214/22-AOAS1701},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1958-1983},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The scalable birth–death MCMC algorithm for mixed graphical model learning with application to genomic data integration},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian additive regression trees for genotype by
environment interaction models. <em>AOAS</em>, <em>17</em>(3),
1936–1957. (<a href="https://doi.org/10.1214/22-AOAS1698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of models for the estimation of genotype by environment (GxE) interactions in plant-based genetics. Our approach, named AMBARTI, uses semiparametric Bayesian additive regression trees to accurately capture marginal genotypic and environment effects along with their interaction in a cut Bayesian framework. We demonstrate that our approach is competitive or superior to similar models widely used in the literature via both simulation and a real world dataset. Furthermore, we introduce new types of visualisation to properly assess both the marginal and interactive predictions from the model. An R package that implements our approach is also available at https://github.com/ebprado/ambarti.},
  archive      = {J_AOAS},
  author       = {Danilo A. Sarti and Estevão B. Prado and Alan N. Inglis and Antônia A. L. dos Santos and Catherine B. Hurley and Rafael A. Moral and Andrew C. Parnell},
  doi          = {10.1214/22-AOAS1698},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1936-1957},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian additive regression trees for genotype by environment interaction models},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequentially valid tests for forecast calibration.
<em>AOAS</em>, <em>17</em>(3), 1909–1935. (<a
href="https://doi.org/10.1214/22-AOAS1697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting and forecast evaluation are inherently sequential tasks. Predictions are often issued on a regular basis, such as every hour, day, or month, and their quality is monitored continuously. However, the classical statistical tools for forecast evaluation are static, in the sense that statistical tests for forecast calibration are only valid if the evaluation period is fixed in advance. Recently, e-values have been introduced as a new, dynamic method for assessing statistical significance. An e-value is a nonnegative random variable with expected value, at most, one under a null hypothesis. Large e-values give evidence against the null hypothesis, and the multiplicative inverse of an e-value is a conservative p-value. Since they naturally lead to statistical tests which are valid under optional stopping, e-values are particularly suitable for sequential forecast evaluation. This article proposes e-values for testing probabilistic calibration of forecasts which is one of the most important notions of calibration. The proposed methods are also more generally applicable for sequential goodness-of-fit testing. We demonstrate in a simulation study that the e-values are competitive in terms of power, when compared to extant methods which do not allow for sequential testing. In this context we introduce test power heat matrices, a graphical tool to compactly visualize results of simulation studies on test power. In a case study we show that the e-values provide important and new useful insights in the evaluation of probabilistic weather forecasts.},
  archive      = {J_AOAS},
  author       = {Sebastian Arnold and Alexander Henzi and Johanna F. Ziegel},
  doi          = {10.1214/22-AOAS1697},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1909-1935},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sequentially valid tests for forecast calibration},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic learning of treatment trees in cancer.
<em>AOAS</em>, <em>17</em>(3), 1884–1908. (<a
href="https://doi.org/10.1214/22-AOAS1696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of synergistic treatment combinations and their underlying biological mechanisms is critical across many disease domains, especially cancer. In translational oncology research, preclinical systems, such as patient-derived xenografts (PDX), have emerged as a unique study design evaluating multiple treatments administered to samples from the same human tumor implanted into genetically identical mice. In this paper we propose a novel Bayesian probabilistic tree-based framework for PDX data to investigate the hierarchical relationships between treatments by inferring treatment cluster trees, referred to as treatment trees (Rx-tree). The framework motivates a new metric of mechanistic similarity between two or more treatments, accounting for inherent uncertainty in tree estimation; treatments with a high estimated similarity have potentially high mechanistic synergy. Building upon Dirichlet diffusion trees, we derive a closed-form marginal likelihood, encoding the tree structure, which facilitates computationally efficient posterior inference via a new two-stage algorithm. Simulation studies demonstrate superior performance of the proposed method in recovering the tree structure and treatment similarities. Our analyses of a recently collated PDX dataset produce treatment similarity estimates that show a high degree of concordance with known biological mechanisms across treatments in five different cancers. More importantly, we uncover new and potentially effective combination therapies that confer synergistic regulation of specific downstream biological pathways for future clinical investigations. Our accompanying code, data, and shiny application for visualization of results are available at: https://github.com/bayesrx/RxTree.},
  archive      = {J_AOAS},
  author       = {Tsung-Hung Yao and Zhenke Wu and Karthik Bharath and Jinju Li and Veerabhadran Baladandayuthapani},
  doi          = {10.1214/22-AOAS1696},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1884-1908},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Probabilistic learning of treatment trees in cancer},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian modeling of interaction between features in sparse
multivariate count data with application to microbiome study.
<em>AOAS</em>, <em>17</em>(3), 1861–1883. (<a
href="https://doi.org/10.1214/22-AOAS1690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical methods have been developed for the analysis of microbial community profiles, but due to the complexity of typical microbiome measurements, inference of interactions between microbial features remains challenging. We develop a Bayesian zero-inflated rounded log-normal kernel method to model interaction between microbial features in a community using multivariate count data in the presence of covariates and excess zeros. The model carefully constructs the interaction structure by imposing joint sparsity on the covariance matrix of the kernel and obtains a reliable estimate of the structure with a small sample size. The model also includes zero inflation to account for excess zeros observed in data and infers differential abundance of microbial features associated with covariates through log-linear regression. We provide simulation studies and real data analysis examples to demonstrate the developed model. Comparison of the model to a simpler model and popular alternatives in simulation studies shows that, in addition to an added and important insight on the feature interaction, it yields superior parameter estimates and model fit in various settings.},
  archive      = {J_AOAS},
  author       = {Shuangjie Zhang and Yuning Shen and Irene A. Chen and Juhee Lee},
  doi          = {10.1214/22-AOAS1690},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1861-1883},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian modeling of interaction between features in sparse multivariate count data with application to microbiome study},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tracking hematopoietic stem cell evolution in a
wiskott–aldrich clinical trial. <em>AOAS</em>, <em>17</em>(3),
1841–1860. (<a href="https://doi.org/10.1214/22-AOAS1686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hematopoietic stem cells (HSC) are the cells that give rise to all other blood cells and, as such, they are crucial in the healthy development of individuals. Wiskott–Aldrich Syndrome (WAS) is a severe disorder affecting the regulation of hematopoietic cells and is caused by mutations in the WASP gene. We consider data from a revolutionary gene therapy clinical trial, where HSC harvested from three WAS patients’ bone marrow have been edited and corrected using viral vectors. Upon reinfusion into the patient, the HSC multiply and differentiate into other cell types. The aim is to unravel the cell multiplication and cell differentiation process, which has until now remained elusive. This paper models the replenishment of blood lineages resulting from corrected HSC via a multivariate, density-dependent Markov process and develops an inferential procedure to estimate the dynamic parameters, given a set of temporally sparsely observed trajectories. Starting from the master equation, we derive a system of nonlinear differential equations for the evolution of the first- and second-order moments over time. We use these moment equations in a generalized method-of-moments framework to perform inference. The performance of our proposal has been evaluated by considering different sampling scenarios and measurement errors of various strengths using a simulation study. We also compared it to another state-of-the-art approach and found that our method is statistically more efficient. By applying our method to the WAS gene therapy data, we found strong evidence for a myeloid-based developmental pathway of hematopoietic cells where fates of lymphoid and myeloid cells remain coupled, even after the loss of erythroid potential. All code used in this manuscript can be found in the online Supplementary Material, and the latest version of the code is available at https://github.com/dp3ll1n/SLCDP_v1.0.},
  archive      = {J_AOAS},
  author       = {Danilo Pellin and Luca Biasco and Serena Scala and Clelia Di Serio and Ernst C. Wit},
  doi          = {10.1214/22-AOAS1686},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1841-1860},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Tracking hematopoietic stem cell evolution in a Wiskott–Aldrich clinical trial},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calibration of SpatioTemporal forecasts from citizen science
urban air pollution data with sparse recurrent neural networks.
<em>AOAS</em>, <em>17</em>(3), 1820–1840. (<a
href="https://doi.org/10.1214/22-AOAS1683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With their continued increase in coverage and quality, data collected from personal air quality monitors has become an increasingly valuable tool to complement existing public health monitoring systems over urban areas. However, the potential of using such “citizen science data” for automatic early warning systems is hampered by the lack of models able to capture the high-resolution, nonlinear spatiotemporal features stemming from local emission sources such as traffic, residential heating and commercial activities. In this work we propose a machine-learning approach to forecast high-frequency spatial fields which has two distinctive advantages from standard neural network methods in time: (1) sparsity of the neural network via a spike-and-slab prior and (2) a small parametric space. The introduction of stochastic neural networks generates additional uncertainty, and in this work we propose a fast approach for ensure that the forecast is correctly assessed (calibration), both marginally and spatially. We focus on assessing exposure to urban air pollution in San Francisco, and our results suggest an improvement of over 58\% in the mean squared error over standard time-series approach with a calibrated forecast for up to five days.},
  archive      = {J_AOAS},
  author       = {Matthew Bonas and Stefano Castruccio},
  doi          = {10.1214/22-AOAS1683},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1820-1840},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Calibration of SpatioTemporal forecasts from citizen science urban air pollution data with sparse recurrent neural networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time mechanistic bayesian forecasts of COVID-19
mortality. <em>AOAS</em>, <em>17</em>(3), 1801–1819. (<a
href="https://doi.org/10.1214/22-AOAS1671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic emerged in late December 2019. In the first six months of the global outbreak, the U.S. reported more cases and deaths than any other country in the world. Effective modeling of the course of the pandemic can help assist with public health resource planning, intervention efforts, and vaccine clinical trials. However, building applied forecasting models presents unique challenges during a pandemic. First, case data available to models in real time represent a nonstationary fraction of the true case incidence due to changes in available diagnostic tests and test-seeking behavior. Second, interventions varied across time and geography leading to large changes in transmissibility over the course of the pandemic. We propose a mechanistic Bayesian model that builds upon the classic compartmental susceptible–exposed–infected–recovered (SEIR) model to operationalize COVID-19 forecasting in real time. This framework includes nonparametric modeling of varying transmission rates, nonparametric modeling of case and death discrepancies due to testing and reporting issues, and a joint observation likelihood on new case counts and new deaths; it is implemented in a probabilistic programming language to automate the use of Bayesian reasoning for quantifying uncertainty in probabilistic forecasts. The model has been used to submit forecasts to the U.S. Centers for Disease Control through the COVID-19 Forecast Hub under the name MechBayes. We examine the performance relative to a baseline model as well as alternate models submitted to the forecast hub. Additionally, we include an ablation test of our extensions to the classic SEIR model. We demonstrate a significant gain in both point and probabilistic forecast scoring measures using MechBayes, when compared to a baseline model, and show that MechBayes ranks as one of the top two models out of nine which regularly submitted to the COVID-19 Forecast Hub for the duration of the pandemic, trailing only the COVID-19 Forecast Hub ensemble model of which which MechBayes is a part.},
  archive      = {J_AOAS},
  author       = {Graham C. Gibson and Nicholas G. Reich and Daniel Sheldon},
  doi          = {10.1214/22-AOAS1671},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1801-1819},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Real-time mechanistic bayesian forecasts of COVID-19 mortality},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bivariate hierarchical bayesian model for combining summary
measures and their uncertainties from multiple sources. <em>AOAS</em>,
<em>17</em>(2), 1782–1800. (<a
href="https://doi.org/10.1214/22-AOAS1699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often of interest to combine available estimates of a similar quantity from multiple data sources. When the corresponding variances of each estimate are also available, a model should take into account the uncertainty of the estimates themselves as well as the uncertainty in the estimation of variances. In addition, if there exists a strong association between estimates and their variances, the correlation between these two quantities should also be considered. In this paper we propose a bivariate hierarchical Bayesian model that jointly models the estimates and their estimated variances, assuming a correlation between these two measures. We conduct simulations to explore the performance of the proposed bivariate Bayesian model and compare it to other commonly used methods under different correlation scenarios. The proposed bivariate Bayesian model has a wide range of applications. We illustrate its application in three very different areas: PET brain imaging studies, meta-analysis, and small area estimation.},
  archive      = {J_AOAS},
  author       = {Yujing Yao and R. Todd Ogden and Chubing Zeng and Qixuan Chen},
  doi          = {10.1214/22-AOAS1699},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1782-1800},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bivariate hierarchical bayesian model for combining summary measures and their uncertainties from multiple sources},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging hardy–weinberg disequilibrium for association
testing in case-control studies. <em>AOAS</em>, <em>17</em>(2),
1764–1781. (<a href="https://doi.org/10.1214/22-AOAS1695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern genome-wide association studies (GWAS) remove single nucleotide polymorphisms (SNPs) that are in Hardy–Weinberg disequilibrium (HWD), despite limited rigor for this practice. In a case-control GWAS, although HWD in the control sample is an evidence for genotyping error, a truly associated SNP may be in HWD in the case and/or control populations. We, therefore, develop a new case-control association test that: (i) leverages HWD attributed to true association to increase power, (ii) is robust to HWD caused by genotyping error, and (iii) is easy-to-implement at the genome-wide level. The proposed robust allele-based joint test incorporates the difference in HWD between the case and control samples into the traditional association measure to gain power. We provide the asymptotic distribution of the proposed test statistic under the null hypothesis. We evaluate its type 1 error control at the genome-wide significance level of 5×10−8 in the presence of HWD attributed to factors unrelated to phenotype-genotype association, such as genotyping error. Finally, we demonstrate that the power of the proposed allele-based joint test is higher than the standard association test for a variety of genetic models, through derivations of the noncentrality parameters of the tests, as well as simulation and application studies.},
  archive      = {J_AOAS},
  author       = {Lin Zhang and Lisa J. Strug and Lei Sun},
  doi          = {10.1214/22-AOAS1695},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1764-1781},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Leveraging Hardy–Weinberg disequilibrium for association testing in case-control studies},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Marginally calibrated response distributions for end-to-end
learning in autonomous driving. <em>AOAS</em>, <em>17</em>(2),
1740–1763. (<a href="https://doi.org/10.1214/22-AOAS1693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end learners for autonomous driving are deep neural networks that predict the instantaneous steering angle directly from images of the street ahead. These learners must provide reliable uncertainty estimates for their predictions in order to meet safety requirements and to initiate a switch to manual control in areas of high uncertainty. However, end-to-end learners typically only deliver point predictions, since distributional predictions are associated with large increases in training time or additional computational resources during prediction. To address this shortcoming, we investigate efficient and scalable approximate inference for the deep distributional model of Klein, Nott and Smith (J. Comput. Graph. Statist. 30 (2021) 467–483) in order to quantify uncertainty for the predictions of end-to-end learners. A special merit of this model, which we refer to as implicit copula neural linear model (IC-NLM), is that it produces densities for the steering angle that are marginally calibrated, that is, the average of the estimated densities equals the empirical distribution of steering angles. To ensure the scalability to large n regimes, we develop efficient estimation based on variational inference as a fast alternative to computationally intensive, exact inference via Hamiltonian Monte Carlo. We demonstrate the accuracy and speed of the variational approach on two end-to-end learners trained for highway driving using the comma2k19 dataset. The IC-NLM is competitive with other established uncertainty quantification methods for end-to-end learning in terms of nonprobabilistic predictive performance and outperforms them in terms of marginal calibration for in-distribution prediction. Our proposed approach also allows the identification of overconfident learners and contributes to the explainability of black-box end-to-end learners by using the predictive densities to understand which steering actions the learner sees as valid.},
  archive      = {J_AOAS},
  author       = {Clara Hoffmann and Nadja Klein},
  doi          = {10.1214/22-AOAS1693},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1740-1763},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Marginally calibrated response distributions for end-to-end learning in autonomous driving},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating multiple built environment data sources.
<em>AOAS</em>, <em>17</em>(2), 1722–1739. (<a
href="https://doi.org/10.1214/22-AOAS1692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies examining the contribution of the built environment to health often rely on commercial data sources to derive exposure measures, such as the number of specific food outlets in study participants’ neighborhoods. Data on the location of community amenities (e.g., food outlets) can be collected from multiple sources. However, these commercial listings are known to have ascertainment errors and thus provide conflicting claims about the number and location of amenities. We propose a method that integrates exposure measures from different databases, while accounting for ascertainment errors, and obtains unbiased health effects of latent exposure. We frame the problem of conflicting exposure measures as a problem of two contingency tables with partially known margins, with the entries of the tables modeled using a multinomial distribution. Available estimates of source quality were embedded in a joint model for observed exposure counts, latent exposures, and health outcomes. Simulations show that our modeling framework yields substantially improved inferences regarding the health effects. We used the proposed method to estimate the association between children’s body mass index (BMI) and the concentration of food outlets near their schools when both the NETS and Reference USA databases are available.},
  archive      = {J_AOAS},
  author       = {Jung Yeon Won and Michael R. Elliott and Emma V. Sanchez-Vaznaugh and Brisa N. Sánchez},
  doi          = {10.1214/22-AOAS1692},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1722-1739},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Integrating multiple built environment data sources},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The risk of maternal complications after cesarean delivery:
Near-far matching for instrumental variables study designs with large
observational datasets. <em>AOAS</em>, <em>17</em>(2), 1701–1721. (<a
href="https://doi.org/10.1214/22-AOAS1691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cesarean delivery is used when there are problems with the placenta or umbilical cord, for twin pregnancies, and breech births. However, research has found that Cesarean delivery increases the risk of maternal complications like blood transfusions and admission to the intensive care unit. Here, using an instrumental variables study design to reduce bias from unobserved confounders, we study whether Cesarean delivery increases the risk of maternal complications. We use a variant of matching—near-far matching—to render our study design more plausible. In a near-far match the investigator seeks to strengthen the effect of the instrument on the exposure while balancing observable characteristics between groups of subjects with low and high values of the instrument. Extant near-far matching methods are computationally intensive for large data sets, and computing time can be very lengthy. To reduce the computational complexity of near-far matching in large observational studies, we apply an iterative form of Glover’s algorithm for a doubly convex bipartite graph to determine an optimal reverse caliper for the instrument which reduces the number of candidate matches and allows for an optimal match in a large but much sparser graph. We also incorporate a variety of balance constraints, including exact matching, fine and near-fine balance, and covariate balance prioritization. We illustrate this new matching method using medical claims data from Pennsylvania, New York, and Florida. In our application we match on physician’s preferences for delivery via Cesarean section which is the instrument in our study. We compare the computing time from our match to extant methods, and we find that we can reduce the computational time required for the match by more than 11 hours. If our matched sample came from a paired randomized experiment, we could conclude that Cesarean delivery elevates the risk of maternal complications and increases the time spent in the hospital. Sensitivity analysis shows that the estimates for complications could be the result of a minor amount of confounding due to an unobserved covariate. The effects on the length of stay outcome, however, are more insensitive to hidden confounders.},
  archive      = {J_AOAS},
  author       = {Ruoqi Yu and Rachel Kelz and Scott Lorch and Luke J. Keele},
  doi          = {10.1214/22-AOAS1691},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1701-1721},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The risk of maternal complications after cesarean delivery: Near-far matching for instrumental variables study designs with large observational datasets},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distinct: A novel approach to differential distribution
analyses. <em>AOAS</em>, <em>17</em>(2), 1681–1700. (<a
href="https://doi.org/10.1214/22-AOAS1689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present distinct, a general method for differential analysis of full distributions that is well suited to applications on single-cell data, such as single-cell RNA sequencing and high-dimensional flow or mass cytometry data. High-throughput single-cell data reveal an unprecedented view of cell identity and allow complex variations between conditions to be discovered; nonetheless, most methods for differential expression target differences in the mean and struggle to identify changes where the mean is only marginally affected. distinct is based on a hierarchical nonparametric permutation approach and, by comparing empirical cumulative distribution functions, identifies both differential patterns involving changes in the mean as well as more subtle variations that do not involve the mean. We performed extensive benchmarks across both simulated and experimental datasets from single-cell RNA sequencing and mass cytometry data, where distinct shows favourable performance, identifies more differential patterns than competitors, and displays good control of false positive and false discovery rates. distinct is available as a Bioconductor R package.},
  archive      = {J_AOAS},
  author       = {Simone Tiberi and Helena L. Crowell and Pantelis Samartsidis and Lukas M. Weber and Mark D. Robinson},
  doi          = {10.1214/22-AOAS1689},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1681-1700},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Distinct: A novel approach to differential distribution analyses},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Truncated rank-based tests for two-part models with
excessive zeros and applications to microbiome data. <em>AOAS</em>,
<em>17</em>(2), 1663–1680. (<a
href="https://doi.org/10.1214/22-AOAS1688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput sequencing technology allows us to test the compositional difference of bacteria in different populations. One important feature of human microbiome data is that it often includes a large number of zeros. Such data can be treated as being generated from a two-part model that includes a zero-point mass. Motivated by analysis of such nonnegative data with excessive zeros, we introduce several truncated rank-based two-group and multigroup tests, including a truncated rank-based Wilcoxon rank-sum test for two-group comparison and two truncated Kruskal–Wallis tests for multigroup comparisons. We show, both analytically through asymptotic relative efficiency analysis and by simulations, that the proposed tests have higher power than the standard rank-based tests in typical microbiome data settings, especially when the proportion of zeros in the data is high. The tests can also be applied to repeated measurements of compositional data via simple within-subject permutations. In a simple before-and-after treatment experiment, the within-subject permutation is similar to the paired rank test. However, the proposed tests handle the excessive zeros which leads to a better power. We apply the tests to compare the microbiome compositions of healthy children and pediatric Crohn’s disease patients and to assess the treatment effects on microbiome compositions. We identify several bacterial genera that are missed by the standard rank-based tests.},
  archive      = {J_AOAS},
  author       = {Wanjie Wang and Eric Chen and Hongzhe Li},
  doi          = {10.1214/22-AOAS1688},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1663-1680},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Truncated rank-based tests for two-part models with excessive zeros and applications to microbiome data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How close and how much? Linking health outcomes to built
environment spatial distributions. <em>AOAS</em>, <em>17</em>(2),
1641–1662. (<a href="https://doi.org/10.1214/22-AOAS1687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Built environment features (BEFs) refer to aspects of the human constructed environment which may, in turn, support or restrict health related behaviors and thus impact health. In this paper we are interested in understanding whether the spatial distribution and quantity of fast-food restaurants (FFRs) influence the risk of obesity in schoolchildren. To achieve this goal, we propose a two-stage Bayesian hierarchical modeling framework. In the first stage, examining the position of FFRs relative to that of some reference locations—in our case, schools—we model the distances of FFRs from these reference locations as realizations of inhomogenous Poisson processes (IPP). With the goal of identifying representative spatial patterns of exposure to FFRs, we model the intensity functions of the IPPs using a Bayesian nonparametric model, specifying a nested Dirichlet process prior. The second-stage model relates exposure patterns to obesity. We offer two different approaches to carry out the second stage; they differ in how they accommodate uncertainty in the exposure patterns. In the first approach, the odds of obesity at the school level is regressed on cluster indicators, each representing a major pattern of exposure to FFRs. In the second, we employ Bayesian kernel machine regression to relate the odds of obesity to the multivariate vector reporting the degree of similarity of a given school to all other schools. Our analysis on the influence of patterns of FFR occurrence on obesity among Californian schoolchildren has indicated that, in 2010, among schools that are consistently assigned to a cluster, there is a lower odds of obesity among ninth graders who attend schools with most distant FFR occurrences in a one-mile radius, as compared to others.},
  archive      = {J_AOAS},
  author       = {Adam T. Peterson and Veronica J. Berrocal and Emma V. Sanchez-Vaznaugh and Brisa N. Sánchez},
  doi          = {10.1214/22-AOAS1687},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1641-1662},
  shortjournal = {Ann. Appl. Stat.},
  title        = {How close and how much? linking health outcomes to built environment spatial distributions},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational bayesian analysis of nonhomogeneous hidden
markov models with long and ultralong sequences. <em>AOAS</em>,
<em>17</em>(2), 1615–1640. (<a
href="https://doi.org/10.1214/22-AOAS1685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonhomogeneous hidden Markov models (NHMMs) are useful in modeling sequential and autocorrelated data. Bayesian approaches, particularly Markov chain Monte Carlo (MCMC) methods, are principal statistical inference tools for NHMMs. However, MCMC sampling is computationally demanding, especially for long observation sequences. We develop a variational Bayes (VB) method for NHMMs, which utilizes a structured variational family of Gaussian distributions with factorized covariance matrices to approximate target posteriors, combining a forward-backward algorithm and stochastic gradient ascent in estimation. To improve efficiency and handle ultralong sequences, we further propose a subsequence VB (SVB) method that works on subsamples. The SVB method exploits the memory decay property of NHMMs and uses buffers to control for bias caused by breaking sequential dependence from subsampling. We highlight that the local nonhomogeneity of NHMMs substantially affects the required buffer lengths and propose the use of local Lyapunov exponents that characterize local memory decay rates of NHMMs and adaptively determine buffer lengths. Our methods are validated in simulation studies and in modeling ultralong sequences of customers’ telecom records to uncover the relationship between their mobile Internet usage behaviors and conventional telecommunication behaviors.},
  archive      = {J_AOAS},
  author       = {Xinyuan Chen and Yiwei Li and Xiangnan Feng and Joseph T. Chang},
  doi          = {10.1214/22-AOAS1685},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1615-1640},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Variational bayesian analysis of nonhomogeneous hidden markov models with long and ultralong sequences},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Randomization inference for cluster-randomized test-negative
designs with application to dengue studies: Unbiased estimation, partial
compliance, and stepped-wedge design. <em>AOAS</em>, <em>17</em>(2),
1592–1614. (<a href="https://doi.org/10.1214/22-AOAS1684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2019, the World Health Organization identified dengue as one of the top 10 global health threats. For the control of dengue, the Applying Wolbachia to Eliminate Dengue (AWED) study group conducted a cluster-randomized trial in Yogyakarta, Indonesia, and used a novel design, called the cluster-randomized test-negative design (CR-TND). This design can yield valid statistical inference with data collected by a passive surveillance system and thus has the advantage of cost-efficiency compared to traditional cluster-randomized trials. We investigate the statistical assumptions and properties of CR-TND under a randomization inference framework, which is known to be robust for small-sample problems. We find that, when the differential healthcare-seeking behavior comparing intervention and control varies across clusters (in contrast to the setting of Dufault and Jewell (Stat. Med. 39 (2020a) 1429–1439) where the differential healthcare-seeking behavior is constant across clusters), current analysis methods for CR-TND can be biased and have inflated type I error. We propose the log-contrast estimator that can eliminate such bias and improve precision by adjusting for covariates. Furthermore, we extend our methods to handle partial intervention compliance and a stepped-wedge design, both of which appear frequently in cluster-randomized trials. Finally, we demonstrate our results by simulation studies and reanalysis of the AWED study.},
  archive      = {J_AOAS},
  author       = {Bingkai Wang and Suzanne M. Dufault and Dylan S. Small and Nicholas P. Jewell},
  doi          = {10.1214/22-AOAS1684},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1592-1614},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Randomization inference for cluster-randomized test-negative designs with application to dengue studies: Unbiased estimation, partial compliance, and stepped-wedge design},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation and inference for exposure effects with latency
in the cox proportional hazards model in the presence of exposure
measurement error. <em>AOAS</em>, <em>17</em>(2), 1574–1591. (<a
href="https://doi.org/10.1214/22-AOAS1682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers are often interested in estimating the effects of time-varying exposures on health outcomes. The latency period, defined as the critical period of susceptibility, can be an important component of exposure effect assessment. Although it is widely known that many environmental, nutritional, and other exposure measurements are prone to error and are also likely to act only during a critical time window of susceptibility, no one has yet considered the impact of this on the estimation of latency parameters in survival models. In this paper we derived methods for point and interval estimation for the latency parameter and the regression coefficients in rare disease situations. Under a linear measurement model, although the estimated hazard ratios are biased, as has been previously demonstrated, we show that the latency parameter is approximately unbiased. Simulations and an illustrative example investigating the prospective association between PM2.5 and lung cancer incidence in the Nurses’ Health Study are included to evaluate the performance of our method.},
  archive      = {J_AOAS},
  author       = {Sarah B. Peskoe and Ning Zhang and Donna Spiegelman and Molin Wang},
  doi          = {10.1214/22-AOAS1682},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1574-1591},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimation and inference for exposure effects with latency in the cox proportional hazards model in the presence of exposure measurement error},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian panel vector autoregression to analyze the impact
of climate shocks on high-income economies. <em>AOAS</em>,
<em>17</em>(2), 1543–1573. (<a
href="https://doi.org/10.1214/22-AOAS1681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we assess the impact of climate shocks on futures markets for agricultural commodities and a set of macroeconomic quantities for multiple high-income economies. To capture relations among countries, markets, and climate shocks, this paper proposes parsimonious methods to estimate high-dimensional panel vector autoregressions. We assume that coefficients associated with domestic lagged endogenous variables arise from a Gaussian mixture model while further parsimony is achieved using suitable global-local shrinkage priors on several regions of the parameter space. Our results point toward pronounced global reactions of key macroeconomic quantities to climate shocks. Moreover, the empirical findings highlight substantial linkages between regionally located shocks and global commodity markets.},
  archive      = {J_AOAS},
  author       = {Florian Huber and Tamás Krisztin and Michael Pfarrhofer},
  doi          = {10.1214/22-AOAS1681},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1543-1573},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian panel vector autoregression to analyze the impact of climate shocks on high-income economies},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent variable models for multivariate dyadic data with
zero inflation: Analysis of intergenerational exchanges of family
support. <em>AOAS</em>, <em>17</em>(2), 1521–1542. (<a
href="https://doi.org/10.1214/22-AOAS1680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the help and support that is exchanged between family members of different generations is of increasing importance, with research questions in sociology and social policy focusing on both predictors of the levels of help given and received, and on reciprocity between them. We propose general latent variable models for analysing such data, when helping tendencies in each direction are measured by multiple binary indicators of specific types of help. The model combines two continuous latent variables, which represent the helping tendencies, with two binary latent class variables which allow for high proportions of responses where no help of any kind is given or received. This defines a multivariate version of a zero-inflation model. The main part of the models is estimated using MCMC methods, with a bespoke data augmentation algorithm. We apply the models to analyse exchanges of help between adult individuals and their noncoresident parents, using survey data from the UK Household Longitudinal Study.},
  archive      = {J_AOAS},
  author       = {Jouni Kuha and Siliang Zhang and Fiona Steele},
  doi          = {10.1214/22-AOAS1680},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1521-1542},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Latent variable models for multivariate dyadic data with zero inflation: Analysis of intergenerational exchanges of family support},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatiotemporal local interpolation of global ocean heat
transport using argo floats: A debiased latent gaussian process
approach. <em>AOAS</em>, <em>17</em>(2), 1491–1520. (<a
href="https://doi.org/10.1214/22-AOAS1679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world ocean plays a key role in redistributing heat in the climate system and hence in regulating Earth’s climate. Yet statistical analysis of ocean heat transport suffers from partially incomplete large-scale data intertwined with complex spatiotemporal dynamics as well as from potential model misspecification. We present a comprehensive spatiotemporal statistical framework tailored to interpolating the global ocean heat transport using in situ Argo profiling float measurements. We formalize the statistical challenges using latent local Gaussian process regression accompanied by a two-stage fitting procedure. We introduce an approximate expectation-maximization algorithm to jointly estimate both the mean field and the covariance parameters, and refine the potentially underspecified mean field model with a debiasing procedure. This approach provides data-driven global ocean heat transport fields that vary in both space and time and can provide insights into crucial dynamical phenomena, such as El Niño &amp; La Niña, as well as the global climatological mean heat transport field which by itself is of scientific interest. The proposed framework and the Argo-based estimates are thoroughly validated with state-of-the-art multimission satellite products and shown to yield realistic subsurface ocean heat transport estimates.},
  archive      = {J_AOAS},
  author       = {Beomjo Park and Mikael Kuusela and Donata Giglio and Alison Gray},
  doi          = {10.1214/22-AOAS1679},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1491-1520},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatiotemporal local interpolation of global ocean heat transport using argo floats: A debiased latent gaussian process approach},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balancing weights for region-level analysis: The effect of
medicaid expansion on the uninsurance rate among states that did not
expand medicaid. <em>AOAS</em>, <em>17</em>(2), 1469–1490. (<a
href="https://doi.org/10.1214/22-AOAS1678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We predict the average effect of Medicaid expansion on the nonelderly adult uninsurance rate among states that did not expand Medicaid in 2014, as if they had expanded their Medicaid eligibility requirements. Using American Community Survey data aggregated to the region level, we estimate this effect by reweighting the expansion regions to approximately match the covariate distribution of the nonexpansion regions. Existing methods to estimate balancing weights often assume that the covariates are measured without error and do not account for dependencies in the outcome model. Our covariates have random noise that is uncorrelated with the outcome errors, and our assumed outcome model contains state-level random effects. To correct for measurement error induced bias, we propose generating weights on a linear approximation to the true covariates, extending an idea from the measurement error literature known as “regression calibration.” This requires auxiliary data to estimate the measurement error variability. We also propose an objective function to reduce the variance of our estimator when the outcome model errors are homoskedastic and equicorrelated within states. We then estimate that Medicaid expansion would have caused a −2.33 (−3.54, −1.11) percentage point change in the adult uninsurance rate among states that did not expand Medicaid.},
  archive      = {J_AOAS},
  author       = {Max Rubinstein and Amelia Haviland and David Choi},
  doi          = {10.1214/22-AOAS1678},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1469-1490},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Balancing weights for region-level analysis: The effect of medicaid expansion on the uninsurance rate among states that did not expand medicaid},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-clustering of spatially resolved transcriptomic data.
<em>AOAS</em>, <em>17</em>(2), 1444–1468. (<a
href="https://doi.org/10.1214/22-AOAS1677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial transcriptomics is a groundbreaking technology that allows the measurement of the activity of thousands of genes in a tissue sample and maps where the activity occurs. This technology has enabled the study of the spatial variation of the genes across the tissue. Comprehending gene functions and interactions in different areas of the tissue is of great scientific interest, as it might lead to a deeper understanding of several key biological mechanisms, such as cell-cell communication or tumor-microenvironment interaction. To do so, one can group cells of the same type and genes that exhibit similar expression patterns. However, adequate statistical tools that exploit the previously unavailable spatial information to more coherently group cells and genes are still lacking. In this work we introduce SpaRTaCo, a new statistical model that clusters the spatial expression profiles of the genes according to a partition of the tissue. This is accomplished by performing a co-clustering, that is, inferring the latent block structure of the data and inducing two types of clustering: of the genes, using their expression across the tissue, and of the image areas, using the gene expression in the spots where the RNA is collected. Our proposed methodology is validated with a series of simulation experiments, and its usefulness in responding to specific biological questions is illustrated with an application to a human brain tissue sample processed with the 10X-Visium protocol.},
  archive      = {J_AOAS},
  author       = {Andrea Sottosanti and Davide Risso},
  doi          = {10.1214/22-AOAS1677},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1444-1468},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Co-clustering of spatially resolved transcriptomic data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lagged couplings diagnose markov chain monte carlo
phylogenetic inference. <em>AOAS</em>, <em>17</em>(2), 1419–1443. (<a
href="https://doi.org/10.1214/22-AOAS1676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylogenetic inference is an intractable statistical problem on a complex space. Markov chain Monte Carlo methods are the primary tool for Bayesian phylogenetic inference, but it is challenging to construct efficient schemes to explore the associated posterior distribution or assess their performance. Existing approaches are unable to diagnose mixing or convergence of Markov schemes jointly across all components of a phylogenetic model. Lagged couplings of Markov chain Monte Carlo algorithms have recently been developed on simpler spaces to diagnose convergence and construct unbiased estimators. We describe a contractive coupling of Markov chains targeting a posterior distribution over a space of phylogenetic trees with branch lengths, scalar parameters and latent variables. We use these couplings to assess mixing and convergence of Markov chains jointly across all components of the phylogenetic model on trees with up to 200 leaves. Samples from our coupled chains may also be used to construct unbiased estimators.},
  archive      = {J_AOAS},
  author       = {Luke J. Kelly and Robin J. Ryder and Grégoire Clarté},
  doi          = {10.1214/22-AOAS1676},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1419-1443},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Lagged couplings diagnose markov chain monte carlo phylogenetic inference},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixed-frequency extreme value regression: Estimating the
effect of mesoscale convective systems on extreme rainfall intensity.
<em>AOAS</em>, <em>17</em>(2), 1398–1418. (<a
href="https://doi.org/10.1214/22-AOAS1675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and modeling the determinants of extreme hourly rainfall intensity is of utmost importance for the management of flash-flood risk. Increasing evidence shows that mesoscale convective systems (MCS) are the principal driver of extreme rainfall intensity in the United States. We use extreme value statistics to investigate the relationship between MCS activity and extreme hourly rainfall intensity in Greater St. Louis, an area particularly vulnerable to flash floods. Using a block maxima approach with monthly blocks, we find that the impact of MCS activity on monthly maxima is not homogeneous within the month/block. To appropriately capture this relationship, we develop a mixed-frequency extreme value regression framework accommodating a covariate sampled at a frequency higher than that of the extreme observation.},
  archive      = {J_AOAS},
  author       = {Debbie J. Dupuis and Luca Trapin},
  doi          = {10.1214/22-AOAS1675},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1398-1418},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Mixed-frequency extreme value regression: Estimating the effect of mesoscale convective systems on extreme rainfall intensity},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic risk prediction triggered by intermediate events
using survival tree ensembles. <em>AOAS</em>, <em>17</em>(2), 1375–1397.
(<a href="https://doi.org/10.1214/22-AOAS1674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the availability of massive amounts of data from electronic health records and registry databases, incorporating time-varying patient information to improve risk prediction has attracted great attention. To exploit the growing amount of predictor information over time, we develop a unified framework for landmark prediction, using survival tree ensembles, where an updated prediction can be performed when new information becomes available. Compared to conventional landmark prediction with fixed landmark times, our methods allow the landmark times to be subject-specific and triggered by an intermediate clinical event. Moreover, the nonparametric approach circumvents the thorny issue of model incompatibility at different landmark times. In our framework, both the longitudinal predictors and the event time outcome are subject to right censoring, and thus existing tree-based approaches cannot be directly applied. To tackle the analytical challenges, we propose a risk-set-based ensemble procedure by averaging martingale estimating equations from individual trees. Extensive simulation studies are conducted to evaluate the performance of our methods. The methods are applied to the Cystic Fibrosis Foundation Patient Registry (CFFPR) data to perform dynamic prediction of lung disease in cystic fibrosis patients and to identify important prognosis factors.},
  archive      = {J_AOAS},
  author       = {Yifei Sun and Sy Han Chiou and Colin O. Wu and Meghan E. McGarry and Chiung-Yu Huang},
  doi          = {10.1214/22-AOAS1674},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1375-1397},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Dynamic risk prediction triggered by intermediate events using survival tree ensembles},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating global and country-specific excess mortality
during the covid-19 pandemic. <em>AOAS</em>, <em>17</em>(2), 1353–1374.
(<a href="https://doi.org/10.1214/22-AOAS1673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the true mortality burden of COVID-19 for every country in the world is a difficult, but crucial, public health endeavor. Attributing deaths, direct or indirect, to COVID-19 is problematic. A more attainable target is the “excess deaths,” the number of deaths in a particular period, relative to that expected during “normal times,” and we develop a model for this endeavor. The excess mortality requires two numbers, the total deaths and the expected deaths, but the former is unavailable for many countries, and so modeling is required for such countries. The expected deaths are based on historic data, and we develop a model for producing estimates of these deaths for all countries. We allow for uncertainty in the modeled expected numbers when calculating the excess. The methods we describe were used to produce the World Health Organization (WHO) excess death estimates. To achieve both interpretability and transparency we developed a relatively simple overdispersed Poisson count framework within which the various data types can be modeled. We use data from countries with national monthly data to build a predictive log-linear regression model with time-varying coefficients for countries without data. For a number of countries, subnational data only are available, and we construct a multinomial model for such data, based on the assumption that the fractions of deaths in subregions remain approximately constant over time. Our inferential approach is Bayesian, with the covariate predictive model being implemented in the fast and accurate INLA software. The subnational modeling was carried out using MCMC in Stan. Based on our modeling, the point estimate for global excess mortality during 2020–2021 is 14.8 million, with a 95\% credible interval of (13.2, 16.6) million.},
  archive      = {J_AOAS},
  author       = {Victoria Knutson and Serge Aleshin-Guendel and Ariel Karlinsky and William Msemburi and Jon Wakefield},
  doi          = {10.1214/22-AOAS1673},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1353-1374},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating global and country-specific excess mortality during the covid-19 pandemic},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulating flood event sets using extremal principal
components. <em>AOAS</em>, <em>17</em>(2), 1333–1352. (<a
href="https://doi.org/10.1214/22-AOAS1672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazard event sets, a collection of synthetic extreme events over a given period, are important for catastrophe modelling. This paper addresses the issue of generating event sets of extreme river flow for northern England and southern Scotland, a region which has been particularly affected by severe flooding over the past 20 years. We start by analysing historical extreme river flow across 45 gauges, using methods from extreme value analysis, including the concept of extremal principal components. Our analysis reveals interesting connections between the extremal dependence structure and the region’s topography/climate. We then introduce a framework which is based on modelling the distribution of the extremal principal components in order to generate synthetic events of extreme river flow. The generative framework is dimension-reducing in that it distinctly handles the principal components based on their contribution to describing the nature of extreme river flow across the study region. We also detail a data-driven approach to select the optimal dimension. Synthetic flood events are subsequently generated efficiently by sampling from the fitted distribution. Our results indicate good agreement between the observed and simulated extreme river flow dynamics and, therefore, illustrate the usefulness of our approach to practitioners. For the considered application, we also find that our approach outperforms existing statistical approaches for generating hazard event sets.},
  archive      = {J_AOAS},
  author       = {Christian Rohrbeck and Daniel Cooley},
  doi          = {10.1214/22-AOAS1672},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1333-1352},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Simulating flood event sets using extremal principal components},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LOCUS: A regularized blind source separation method with
low-rank structure for investigating brain connectivity. <em>AOAS</em>,
<em>17</em>(2), 1307–1332. (<a
href="https://doi.org/10.1214/22-AOAS1670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-oriented research has been increasingly popular in many scientific areas. In neuroscience research, imaging-based network connectivity measures have become the key for understanding brain organizations, potentially serving as individual neural fingerprints. There are major challenges in analyzing connectivity matrices, including the high dimensionality of brain networks, unknown latent sources underlying the observed connectivity, and the large number of brain connections leading to spurious findings. In this paper we propose a novel blind source separation method with low-rank structure and uniform sparsity (LOCUS) as a fully data-driven decomposition method for network measures. Compared with the existing method that vectorizes connectivity matrices ignoring brain network topology, LOCUS achieves more efficient and accurate source separation for connectivity matrices using low-rank structure. We propose a novel angle-based uniform sparsity regularization that demonstrates better performance than the existing sparsity controls for low-rank tensor methods. We propose a highly efficient iterative node-rotation algorithm that exploits the block multiconvexity of the objective function to solve the nonconvex optimization problem for learning LOCUS. We illustrate the advantage of LOCUS through extensive simulation studies. Application of LOCUS to Philadelphia Neurodevelopmental Cohort neuroimaging study reveals biologically insightful connectivity traits which are not found using the existing method.},
  archive      = {J_AOAS},
  author       = {Yikai Wang and Ying Guo},
  doi          = {10.1214/22-AOAS1670},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1307-1332},
  shortjournal = {Ann. Appl. Stat.},
  title        = {LOCUS: A regularized blind source separation method with low-rank structure for investigating brain connectivity},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian accelerated failure time model for interval
censored three-state screening outcomes. <em>AOAS</em>, <em>17</em>(2),
1285–1306. (<a href="https://doi.org/10.1214/22-AOAS1669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Women infected by the human papillomavirus are at an increased risk to develop cervical intraepithelial neoplasia lesions (CIN). CIN are classified into three grades of increasing severity (CIN-1, CIN-2, and CIN-3) and can eventually develop into cervical cancer. The main purpose of screening is detecting CIN-2 and CIN-3 cases which are usually removed surgically. Screening data from the POBASCAM trial involving 1454 HPV-positive women are analyzed with two objectives, estimate: (a) the transition time from HPV diagnosis to CIN-3 and (b) the transition time from CIN-2 to CIN-3. The screening data have two key characteristics. First, the CIN state is monitored in an interval censored sequence of screening times. Second, a woman’s progression to CIN-3 is only observed if the woman progresses to, both, CIN-2 and from CIN-2 to CIN-3 in the same screening interval. We propose a Bayesian accelerated failure time model for the two transition times in this three-state model. To deal with the unusual censoring structure of the screening data, we develop a Metropolis-within-Gibbs algorithm with data augmentation from the truncated transition time distributions.},
  archive      = {J_AOAS},
  author       = {Thomas Klausch and Eddymurphy U. Akwiwu and Mark A. van de Wiel and Veerle M. H. Coupé and Johannes Berkhof},
  doi          = {10.1214/22-AOAS1669},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1285-1306},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian accelerated failure time model for interval censored three-state screening outcomes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting distributional differences in labeled sequence
data with application to tropical cyclone satellite imagery.
<em>AOAS</em>, <em>17</em>(2), 1260–1284. (<a
href="https://doi.org/10.1214/22-AOAS1668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal is to quantify whether, and if so how, spatiotemporal patterns in tropical cyclone (TC) satellite imagery signal an upcoming rapid intensity change event. To address this question, we propose a new nonparametric test of association between a time series of images and a series of binary event labels. We ask whether there is a difference in distribution between (dependent but identically distributed) 24-hour sequences of images preceding an event vs. a nonevent. By rewriting the statistical test as a regression problem, we leverage neural networks to infer modes of structural evolution of TC convection that are representative of the lead-up to rapid intensity change events. Dependencies between nearby sequences are handled by a bootstrap procedure that estimates the marginal distribution of the label series. We prove that type I error control is guaranteed as long as the distribution of the label series is well estimated which is made easier by the extensive historical data for binary TC event labels. We show empirical evidence that our proposed method identifies archetypes of infrared imagery associated with elevated rapid intensification risk, typically marked by deep or deepening core convection over time. Such results provide a foundation for improved forecasts of rapid intensification.},
  archive      = {J_AOAS},
  author       = {Trey McNeely and Galen Vincent and Kimberly M. Wood and Rafael Izbicki and Ann B. Lee},
  doi          = {10.1214/22-AOAS1668},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1260-1284},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Detecting distributional differences in labeled sequence data with application to tropical cyclone satellite imagery},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate marker assessment using mediation and instrumental
variable analyses in a case-cohort design. <em>AOAS</em>,
<em>17</em>(2), 1239–1259. (<a
href="https://doi.org/10.1214/22-AOAS1667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of surrogate markers for gold standard outcomes in clinical trials enables future cost-effective trials that target the identified markers. Due to resource limitations, these surrogate markers may be collected only for cases and for a subset of the trial cohort, giving rise to what is termed the case-cohort design. Motivated by a COVID-19 vaccine trial, we propose methods of assessing the surrogate markers for a time-to-event outcome in a case-cohort design by using mediation and instrumental variable (IV) analyses. In the mediation analysis we decomposed the vaccine effect on COVID-19 risk into an indirect effect (the effect mediated through the surrogate marker such as neutralizing antibodies) and a direct effect (the effect not mediated by the marker), and we propose that the mediation proportions are surrogacy indices. In the IV analysis we aimed to quantify the causal effect of the surrogate marker on disease risk in the presence of surrogate-disease confounding which is unavoidable even in randomized trials. We employed weighted estimating equations derived from nonparametric maximum likelihood estimators (NPMLEs) under semiparametric probit models for the time-to-disease outcome. We plugged in the weighted NPMLEs to construct estimators for the aforementioned causal effects and surrogacy indices, and we determined the asymptotic properties of the proposed estimators. Finite sample performance was evaluated in numerical simulations. Applying the proposed mediation and IV analyses to a mock COVID-19 vaccine trial data, we found that 84.2\% of the vaccine efficacy was mediated by 50\% pseudovirus neutralizing antibody and that neutralizing antibodies had significant protective effects for COVID-19 risk.},
  archive      = {J_AOAS},
  author       = {Yen-Tsung Huang and Jih-Chang Yu and Jui-Hsiang Lin},
  doi          = {10.1214/22-AOAS1667},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1239-1259},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Surrogate marker assessment using mediation and instrumental variable analyses in a case-cohort design},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian analysis for imbalanced positive-unlabelled
diagnosis codes in electronic health records. <em>AOAS</em>,
<em>17</em>(2), 1220–1238. (<a
href="https://doi.org/10.1214/22-AOAS1666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing availability of electronic health records (EHR), significant progress has been made on developing predictive inference and algorithms by health-data analysts and researchers. However, the EHR data are notoriously noisy, due to missing and inaccurate inputs, despite abundant information. One serious problem is that only a small portion of patients in the database has confirmatory diagnoses, while many other patients remain undiagnosed because they did not comply with the recommended examinations. The phenomenon leads to a so-called positive-unlabelled situation, and the labels are extremely imbalanced. In this paper we propose a model-based approach to classify the unlabelled patients by using a Bayesian finite mixture model. We also discuss the label switching issue for the imbalanced data and propose a consensus Monte Carlo approach to address the imbalance issue and improve computational efficiency simultaneously. Simulation studies show that our proposed model-based approach outperforms existing positive-unlabelled learning algorithms. The proposed method is applied on the Cerner EHR for detecting diabetic retinopathy (DR) patients using laboratory measurements. With only 3\% confirmatory diagnoses in the EHR database, we estimate the actual DR prevalence to be 25\% which coincides with reported findings in the medical literature.},
  archive      = {J_AOAS},
  author       = {Ru Wang and Ye Liang and Zhuqi Miao and Tieming Liu},
  doi          = {10.1214/22-AOAS1666},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1220-1238},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian analysis for imbalanced positive-unlabelled diagnosis codes in electronic health records},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of immune response combinations associated
with heterogeneous infection risk in the immune correlates analysis of
HIV vaccine studies. <em>AOAS</em>, <em>17</em>(2), 1199–1219. (<a
href="https://doi.org/10.1214/22-AOAS1665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In HIV vaccine/prevention research, probing into the vaccine-induced immune responses that can help to predict the risk of HIV infection provides valuable information for the development of vaccine regimens. Previous correlate analysis of the Thai vaccine trial aided the discovery of interesting immune correlates related to the risk of developing an HIV infection. The present study aimed to identify the combinations of immune responses associated with the heterogeneous infection risk. We explored a “change-plane” via combination of a subset of immune responses that could help separate vaccine recipients into two heterogeneous subgroups in terms of the association between immune responses and the risk of developing infection. Additionally, we developed a new variable selection algorithm through a penalized likelihood approach to investigate a parsimonious marker combination for the change-plane. The resulting marker combinations can serve as candidate correlates of protection and can be used for predicting the protective effect of the vaccine against HIV infection. The application of the proposed statistical approach to the Thai trial has been presented, wherein the marker combinations were explored among several immune responses and antigens.},
  archive      = {J_AOAS},
  author       = {Chaeryon Kang and Ying Huang},
  doi          = {10.1214/22-AOAS1665},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1199-1219},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Identification of immune response combinations associated with heterogeneous infection risk in the immune correlates analysis of HIV vaccine studies},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent multivariate log-gamma models for high-dimensional
MultiType responses with application to daily fine particulate matter
and mortality counts. <em>AOAS</em>, <em>17</em>(2), 1175–1198. (<a
href="https://doi.org/10.1214/22-AOAS1664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise estimation of daily fine particulate matter with a diameter ≤2.5 microns (PM2.5) and mortality in the U.S. is an important research challenge in public health because high levels of PM2.5 have been linked to several serious health problems, including lung disease, cardiovascular disease, and stroke. This motivates us to develop a joint Bayesian hierarchical model for bivariate spatial data to obtain precise spatial predictions of two types of responses, continuous skewed PM2.5 levels, and discrete mortality counts over U.S. counties. Our novel modeling framework address several challenges in the area of spatial prediction of mortality counts and PM2.5 levels. Specifically, our model allows for spatial variability and dependence of two types of responses, accommodate an unknown nonlinear spatial relationship between mortality and PM2.5 through basis function expansions, improve the precision of predictions at counties with undisclosed/missing observations, and allow for different missing data patterns for mortality and PM2.5. Furthermore, we introduce a new local measure of association for the cross-dependence between mortality and PM2.5 level. To address the burden of Bayesian computation for large databases, we use the dimension reduction tool and the shared conjugate structure between the Weibull distribution, Poisson distribution, and the multivariate log-gamma distribution. We provide a simulation study to illustrate the performance of our method. Our joint spatial model of “multitype responses” (discrete and continuous responses) and associated Bayesian method are used to analyze bivariate spatial data of daily averaged PM2.5 levels in air and mortality counts (due to diseases related to lung, cardiovascular, respiratory, and stroke) from the Centers for Disease Control and Prevention (CDC) database.},
  archive      = {J_AOAS},
  author       = {Zhixing Xu and Jonathan R. Bradley and Debajyoti Sinha},
  doi          = {10.1214/22-AOAS1664},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1175-1198},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Latent multivariate log-gamma models for high-dimensional MultiType responses with application to daily fine particulate matter and mortality counts},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knockoffs with side information. <em>AOAS</em>,
<em>17</em>(2), 1152–1174. (<a
href="https://doi.org/10.1214/22-AOAS1663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of assessing the importance of multiple variables or factors from a dataset when side information is available. In principle, using side information can allow the statistician to pay attention to variables with a greater potential which, in turn, may lead to more discoveries. We introduce an adaptive knockoff filter, which generalizes the knockoff procedure (Ann. Statist. 43 (2015) 2055–2085; J. R. Stat. Soc. Ser. B. Stat. Methodol. 80 (2018) 551–577), in that it uses both the data at hand and side information to adaptively order the variables under study and focus on those that are most promising. The adaptive knockoffs procedure controls the finite-sample false discovery rate (FDR), and we demonstrate its power by comparing it with other structured multiple testing methods. We also apply our methodology to real genetic data in order to find associations between genetic variants and various phenotypes such as Crohn’s disease and lipid levels. Here, the adaptive knockoffs method makes more discoveries than reported in previous studies on the same datasets.},
  archive      = {J_AOAS},
  author       = {Zhimei Ren and Emmanuel Candès},
  doi          = {10.1214/22-AOAS1663},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1152-1174},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Knockoffs with side information},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A rotation-based feature and bayesian hierarchical model for
the forensic evaluation of handwriting evidence in a closed set.
<em>AOAS</em>, <em>17</em>(2), 1127–1151. (<a
href="https://doi.org/10.1214/22-AOAS1662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forensic handwriting examiners are often tasked with identifying the writer of a particular document. Examples of handwriting evidence include ransom notes, forged documents and signatures, and threatening letters. At present, examiners rely on visual inspection of similarities and differences between the questioned document and reference writing samples. Here, we propose a principled modeling approach to compute the posterior predictive probability of writership when the author of the questioned document is part of a closed set of writers. Given a handwritten document, we extract measurements, including rotation angles that are related to the slant of writing, which are the response variables in a multilevel model. We fit the model and test its posterior predictive performance using writing samples from the United States and from Europe. We find that, as long as the questioned document is longer than a sentence or two, it is possible to correctly associate a writer with a document that he or she wrote with high probability. Earlier versions of this work have been well received by the community of forensic document examiners.},
  archive      = {J_AOAS},
  author       = {Amy M. Crawford and Danica M. Ommen and Alicia L. Carriquiry},
  doi          = {10.1214/22-AOAS1662},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1127-1151},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A rotation-based feature and bayesian hierarchical model for the forensic evaluation of handwriting evidence in a closed set},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tensor decomposition model for longitudinal microbiome
studies. <em>AOAS</em>, <em>17</em>(2), 1105–1126. (<a
href="https://doi.org/10.1214/22-AOAS1661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal microbiome studies can help delineate true biological signals from the high interindividual variability that is common in microbiome data. However, there are few methods available for unsupervised dimension reduction of time course microbial abundance observations. Existing methods do not fully observe the distribution characteristics of such data types, namely, zero inflation, compositionality, and overdispersion. We present a tensor decomposition model and a semiparametric quasi-likelihood estimation method for the decomposition of longitudinal microbiome data by generalizing existing approaches in tensor decomposition of Gaussian data. Optimization is performed through projected gradient descent, additionally allowing interpretability constraints. We show through simulation studies that our method is able to recover low-rank structures from microbiome time-course data better than existing approaches. Lastly, we apply our method to two existing longitudinal microbiome studies to detect global microbial changes associated with dietary and pharmaceutical effects as well as infant birth modes.},
  archive      = {J_AOAS},
  author       = {Siyuan Ma and Hongzhe Li},
  doi          = {10.1214/22-AOAS1661},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1105-1126},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A tensor decomposition model for longitudinal microbiome studies},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CytOpT: Optimal transport with domain adaptation for
interpreting flow cytometry data. <em>AOAS</em>, <em>17</em>(2),
1086–1104. (<a href="https://doi.org/10.1214/22-AOAS1660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated analysis of flow cytometry measurements is an active research field. We introduce a new algorithm, referred to as CytOpT, using regularized optimal transport to directly estimate the different cell population proportions from a biological sample characterized with flow cytometry measurements. We rely on the regularized Wasserstein metric to compare cytometry measurements from different samples, thus accounting for possible misalignment of a given cell population across samples (due to technical variability from the technology of measurements). In this work we rely on a supervised learning technique, based on the Wasserstein metric, that is used to estimate an optimal reweighting of class proportions in a mixture model from a source distribution (with known segmentation into cell sub-populations) to fit a target distribution with unknown segmentation. Due to the high dimensionality of flow cytometry data, we use stochastic algorithms to approximate the regularized Wasserstein metric to solve the optimization problem involved in the estimation of optimal weights representing the cell population proportions in the target distribution. Several flow cytometry data sets are used to illustrate the performances of CytOpT that are also compared to those of existing algorithms for automatic gating based on supervised learning.},
  archive      = {J_AOAS},
  author       = {Paul Freulon and Jérémie Bigot and Boris P. Hejblum},
  doi          = {10.1214/22-AOAS1660},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1086-1104},
  shortjournal = {Ann. Appl. Stat.},
  title        = {CytOpT: Optimal transport with domain adaptation for interpreting flow cytometry data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian cox regression for large-scale inference with
applications to electronic health records. <em>AOAS</em>,
<em>17</em>(2), 1064–1085. (<a
href="https://doi.org/10.1214/22-AOAS1658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox model is an indispensable tool for time-to-event analysis, particularly in biomedical research. However, medicine is undergoing a profound transformation, generating data at an unprecedented scale, which opens new frontiers to study and understand diseases. With the wealth of data collected, new challenges for statistical inference arise, as datasets are often high dimensional, exhibit an increasing number of measurements at irregularly spaced time points, and are simply too large to fit in memory. Many current implementations for time-to-event analysis are ill-suited for these problems, as inference is computationally demanding and requires access to the full data at once. Here, we propose a Bayesian version for the counting process representation of Cox’s partial likelihood for efficient inference on large-scale datasets with millions of data points and thousands of time-dependent covariates. Through the combination of stochastic variational inference and a reweighting of the log-likelihood, we obtain an approximation for the posterior distribution that factorizes over subsamples of the data, enabling the analysis in big data settings. Crucially, the method produces viable uncertainty estimates for large-scale and high-dimensional datasets. We show the utility of our method through a simulation study and an application to myocardial infarction in the UK Biobank, where we characterize the multivariate effects of risk factors and replicate results from individual studies. Our framework extends the Cox model to new data sources, like biobanks and EHR, the combination of which can provide new insights into our understanding of diseases.},
  archive      = {J_AOAS},
  author       = {Alexander Wolfgang Jung and Moritz Gerstung},
  doi          = {10.1214/22-AOAS1658},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1064-1085},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian cox regression for large-scale inference with applications to electronic health records},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian decision theory for tree-based adaptive screening
tests with an application to youth delinquency. <em>AOAS</em>,
<em>17</em>(2), 1038–1063. (<a
href="https://doi.org/10.1214/22-AOAS1657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crime prevention strategies based on early intervention depend on accurate risk assessment instruments for identifying high-risk youth. It is important in this context that the instruments be convenient to administer, which means, in particular, that they should also be reasonably brief; adaptive screening tests are useful for this purpose. Adaptive tests constructed using classification and regression trees are becoming a popular alternative to traditional item response theory (IRT) approaches for adaptive testing. However, tree-based adaptive tests lack a principled criterion for terminating the test. This paper develops a Bayesian decision theory framework for measuring the trade-off between brevity and accuracy when considering tree-based adaptive screening tests of different lengths. We also present a novel method for designing tree-based adaptive tests, motivated by this framework. The framework and associated adaptive test method are demonstrated through an application to youth delinquency risk assessment in Honduras; it is shown that an adaptive test requiring a subject to answer fewer than 10 questions can identify high-risk youth nearly as accurately as an unabridged survey containing 173 items.},
  archive      = {J_AOAS},
  author       = {Chelsea Krantsevich and P. Richard Hahn and Yi Zheng and Charles Katz},
  doi          = {10.1214/22-AOAS1657},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1038-1063},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian decision theory for tree-based adaptive screening tests with an application to youth delinquency},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust joint modelling of left-censored longitudinal data
and survival data with application to HIV vaccine studies.
<em>AOAS</em>, <em>17</em>(2), 1017–1037. (<a
href="https://doi.org/10.1214/22-AOAS1656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In jointly modelling longitudinal and survival data, the longitudinal data may be complex in the sense that they may contain outliers and may be left censored. Motivated from an HIV vaccine study, we propose a robust method for joint models of longitudinal and survival data, where the outliers in longitudinal data are addressed using a multivariate t-distribution for b-outliers and using an M-estimator for e-outliers. We also propose a computationally efficient method for approximate likelihood inference. The proposed method is evaluated by simulation studies. Based on the proposed models and method, we analyze the HIV vaccine data and find a strong association between longitudinal biomarkers and the risk of HIV infection.},
  archive      = {J_AOAS},
  author       = {Tingting Yu and Lang Wu and Jin Qiu and Peter B. Gilbert},
  doi          = {10.1214/22-AOAS1656},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1017-1037},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Robust joint modelling of left-censored longitudinal data and survival data with application to HIV vaccine studies},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the effects of a california gun control program
with multitask gaussian processes. <em>AOAS</em>, <em>17</em>(2),
985–1016. (<a href="https://doi.org/10.1214/22-AOAS1654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gun violence is a critical public safety concern in the United States. In 2006, California implemented a unique firearm monitoring program, the Armed and Prohibited Persons System (APPS), to address gun violence in the state. The APPS program first identifies those firearm owners who become prohibited from owning one, due to federal or state law, then confiscates their firearms. Our goal is to assess the effect of APPS on California murder rates using annual, state-level crime data across the U.S. for the years before and after the introduction of the program. To do so, we adapt a nonparametric Bayesian approach, multitask Gaussian processes (MTGPs), to the panel data setting. MTGPs allow for flexible and parsimonious panel data models that nest many existing approaches and allow for direct control over both dependence across time and dependence across units as well as natural uncertainty quantification. We extend this approach to incorporate non-Normal outcomes, auxiliary covariates, and multiple outcome series, which are all important in our application. We also show that this approach has attractive Frequentist properties, including a representation as a weighting estimator with separate weights over units and time periods. Applying this approach, we find that the increased monitoring and enforcement from the APPS program substantially decreased homicides in California. We also find that the effect on murder is driven entirely by declines in gun-related murder with no measurable effect on non-gun murder. Estimated cost per murder avoided are substantially lower than conventional estimates of the value of a statistical life, suggesting a very high benefit-cost ratio for this enforcement effort.},
  archive      = {J_AOAS},
  author       = {Eli Ben-Michael and David Arbour and Avi Feller and Alexander Franks and Steven Raphael},
  doi          = {10.1214/22-AOAS1654},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {985-1016},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating the effects of a california gun control program with multitask gaussian processes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust sensible adversarial learning of deep neural networks
for image classification. <em>AOAS</em>, <em>17</em>(2), 961–984. (<a
href="https://doi.org/10.1214/22-AOAS1637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of robustness is central and critical to modern statistical analysis. However, despite the recent advances of deep neural networks (DNNs), many studies have shown that DNNs are vulnerable to adversarial attacks. Making imperceptible changes to an image can cause DNN models to make the wrong classification with high confidence, such as classifying a benign mole as a malignant tumor and a stop sign as a speed limit sign. The trade-off between robustness and standard accuracy is common for DNN models. In this paper we introduce sensible adversarial learning and demonstrate the synergistic effect between pursuits of standard natural accuracy and robustness. Specifically, we define a sensible adversary, which is useful for learning a robust model, while keeping high natural accuracy. We theoretically establish that the Bayes classifier is the most robust multiclass classifier with the 0−1 loss under sensible adversarial learning. We propose a novel and efficient algorithm that trains a robust model using implicit loss truncation. We apply sensible adversarial learning for large-scale image classification to a handwritten digital image dataset, called MNIST, and an object recognition colored image dataset, called CIFAR10. We have performed an extensive comparative study to compare our method with other competitive methods. Our experiments empirically demonstrate that our method is not sensitive to its hyperparameter and does not collapse even with a small model capacity while promoting robustness against various attacks and keeping high natural accuracy. The sensible adversarial learning software is available as a Python package at https://github.com/JungeumKim/SENSE.},
  archive      = {J_AOAS},
  author       = {Jungeum Kim and Xiao Wang},
  doi          = {10.1214/22-AOAS1637},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {961-984},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Robust sensible adversarial learning of deep neural networks for image classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of gaussian directed acyclic graphs using partial
ordering information with applications to DREAM3 networks and dairy
cattle data. <em>AOAS</em>, <em>17</em>(2), 929–960. (<a
href="https://doi.org/10.1214/22-AOAS1636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating a directed acyclic graph (DAG) from observational data represents a canonical learning problem and has generated a lot of interest in recent years. Research has focused mostly on the following two cases: when no information regarding the ordering of the nodes in the DAG is available and when a domain-specific complete ordering of the nodes is available. In this paper, motivated by a recent application in dairy science, we develop a method for DAG estimation for the middle scenario, where partition-based partial ordering of the nodes is known based on domain-specific knowledge. We develop an efficient algorithm that solves the posited problem, coined Partition-DAG. Through extensive simulations, using the DREAM3 Yeast networks, we illustrate that Partition-DAG effectively incorporates the partial ordering information to improve both speed and accuracy. We then illustrate the usefulness of Partition-DAG by applying it to recently collected dairy cattle data, and inferring relationships between various variables involved in dairy agroecosystems.},
  archive      = {J_AOAS},
  author       = {Syed Rahman and Kshitij Khare and George Michailidis and Carlos Martínez and Juan Carulla},
  doi          = {10.1214/22-AOAS1636},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {929-960},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimation of gaussian directed acyclic graphs using partial ordering information with applications to DREAM3 networks and dairy cattle data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hospital quality risk standardization via approximate
balancing weights. <em>AOAS</em>, <em>17</em>(2), 901–928. (<a
href="https://doi.org/10.1214/22-AOAS1629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparing outcomes across hospitals, often to identify underperforming hospitals, is a critical task in health services research. However, naive comparisons of average outcomes, such as surgery complication rates, can be misleading because hospital case mixes differ—a hospital’s overall complication rate may be lower simply because the hospital serves a healthier population overall. In this paper we develop a method of “direct standardization” where we reweight each hospital patient population to be representative of the overall population and then compare the weighted averages across hospitals. Adapting methods from survey sampling and causal inference, we find weights that directly control for imbalance between the hospital patient mix and the target population, even across many patient attributes. Critically, these balancing weights can also be tuned to preserve sample size for more precise estimates. We also derive principled measures of statistical uncertainty and use outcome modeling and Bayesian shrinkage to increase precision and account for variation in hospital size. We demonstrate these methods using claims data from Pennsylvania, Florida, and New York, estimating standardized hospital complication rates for general surgery patients. We conclude with a discussion of how to detect low performing hospitals.},
  archive      = {J_AOAS},
  author       = {Luke J. Keele and Eli Ben-Michael and Avi Feller and Rachel Kelz and Luke Miratrix},
  doi          = {10.1214/22-AOAS1629},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {901-928},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Hospital quality risk standardization via approximate balancing weights},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An extension of estimating equations to model longitudinal
medical cost trajectory with medicare claims data linked to SEER cancer
registry. <em>AOAS</em>, <em>17</em>(1), 881–899. (<a
href="https://doi.org/10.1214/22-AOAS1659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insurance claims’ data is an increasingly important health policy research resource, given its longitudinal assessment of cancer care clinical outcomes. Population-level information on medical cost trajectory from disease diagnosis to terminal events, such as death, specifically interests policy makers. Estimating the mean cost trajectory has statistical challenges. The shape of the trajectory is usually highly nonlinear with varying durations, depending on the diagnosis-to-death population time distribution. The terminal event may be right censored, resulting in missing subsequent costs. Medical costs often have skewed distributions with zero inflation and heteroscedasticity which may not fit well with the commonly used parametric family of distributions. In this paper we propose a flexible semiparametric model to address challenges without imposing a cost data distributional assumption. The estimation procedure is based on generalized estimating equations with censored covariates. The proposed model adopts a bivariate surface that quantifies the interrelationship between longitudinal medical costs and survival, and results in the nonlinear population mean cost trajectories given survival time. We develop a novel generalized estimating equations algorithm to accommodate covariates subject to right censoring without fully specifying the joint distribution of the cost and survival data. We provide theoretical and simulation-based justification for the proposed approach and apply the methods to estimate prostate cancer patient cost trajectories from the Surveillance, Epidemiology, and End Results (SEER)-Medicare linked database.},
  archive      = {J_AOAS},
  author       = {Shikun Wang and Jing Ning and Ying Xu and Ya-Chen Tina Shih and Yu Shen and Liang Li},
  doi          = {10.1214/22-AOAS1659},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {881-899},
  shortjournal = {Ann. Appl. Stat.},
  title        = {An extension of estimating equations to model longitudinal medical cost trajectory with medicare claims data linked to SEER cancer registry},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling a nonlinear biophysical trend followed by
long-memory equilibrium with unknown change point. <em>AOAS</em>,
<em>17</em>(1), 860–880. (<a
href="https://doi.org/10.1214/22-AOAS1655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurements of many biological processes are characterized by an initial trend period followed by an equilibrium period. Scientists may wish to quantify features of the two periods as well as the timing of the change point. Specifically, we are motivated by problems in the study of electrical cell-substrate impedance sensing (ECIS) data. ECIS is a popular new technology which measures cell behavior noninvasively. Previous studies using ECIS data have found that different cell types can be classified by their equilibrium behavior. However, it can be challenging to identify when equilibrium has been reached and to quantify the relevant features of cells’ equilibrium behavior. In this paper we assume that measurements during the trend period are independent deviations from a smooth nonlinear function of time, and that measurements during the equilibrium period are characterized by a simple long memory model. We propose a method to simultaneously estimate the parameters of the trend and equilibrium processes and locate the change point between the two. We find that this method performs well in simulations and in practice. When applied to ECIS data, it produces estimates of change points and measures of cell equilibrium behavior which offer improved classification of infected and uninfected cells.},
  archive      = {J_AOAS},
  author       = {Wenyu Zhang and Maryclare Griffin and David S. Matteson},
  doi          = {10.1214/22-AOAS1655},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {860-880},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling a nonlinear biophysical trend followed by long-memory equilibrium with unknown change point},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Many-to-one indirect sampling with application to the french
postal traffic estimation. <em>AOAS</em>, <em>17</em>(1), 838–859. (<a
href="https://doi.org/10.1214/22-AOAS1653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social and economic surveys, it can be difficult to directly reach units of the target population, and indirect sampling is often advocated to solve this issue. In indirect sampling, the sample is drawn from a frame population that is linked to the target population, and estimation of target population parameters is typically achieved through the generalized weight share method (GWSM). This method provides a weight, for every unit of the target population, that depends on the one hand, on the sampling weights in the frame population and, on the other hand, on the link weights between the frame population and the target population. In the present study, we focus on the situation in which the units from the frame population are linked to one and only one unit from the target population (Many-to-One case). This situation is encountered at the French postal service where addresses are sampled instead of postman rounds. We aim at understanding of the impact of the link weights on the efficiency of the GWSM estimators. We derive variance expressions and optimality results for a large class of sampling designs. Moreover, we note that the Many-to-One case can lead to too many links to observe. We alleviate the problem by introducing an intermediate population and double indirect sampling. The question of the loss of precision in this situation is discussed in detail through theoretical results and simulations. These findings help to explain the loss of precision of double GWSM estimators observed recently at the French postal service.},
  archive      = {J_AOAS},
  author       = {Estelle Medous and Camelia Goga and Anne Ruiz-Gazen and Jean-François Beaumont and Alain Dessertaine and Pauline Puech},
  doi          = {10.1214/22-AOAS1653},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {838-859},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Many-to-one indirect sampling with application to the french postal traffic estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PALM: Patient-centered treatment ranking via large-scale
multivariate network meta-analysis. <em>AOAS</em>, <em>17</em>(1),
815–837. (<a href="https://doi.org/10.1214/22-AOAS1652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing number of available treatment options has led to urgent needs for reliable answers when choosing the best course of treatment for a patient. As it is often infeasible to compare a large number of treatments in a single randomized controlled trial, multivariate network meta-analyses (NMAs) are used to synthesize evidence from trials of a subset of the treatments, where both efficacy and safety related outcomes are considered simultaneously. However, these large-scale multiple-outcome NMAs have created challenges to existing methods due to the increasing complexity of the unknown correlations between outcomes and treatment comparisons. In this paper, we proposed a new framework for PAtient-centered treatment ranking via Large-scale Multivariate network meta-analysis, termed as PALM, which includes a parsimonious modeling approach, a fast algorithm for parameter estimation and inference, a novel visualization tool for presenting multivariate outcomes, termed as the origami plot, as well as personalized treatment ranking procedures taking into account the individual’s considerations on multiple outcomes. In application to an NMA that compares 14 treatment options for labor induction, we provided a comprehensive illustration of the proposed framework and demonstrated its computational efficiency and practicality, and we obtained new insights and evidence to support patient-centered clinical decision making.},
  archive      = {J_AOAS},
  author       = {Rui Duan and Jiayi Tong and Lifeng Lin and Lisa Levine and Mary Sammel and Joel Stoddard and Tianjing Li and Christopher H Schmid and Haitao Chu and Yong Chen},
  doi          = {10.1214/22-AOAS1652},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {815-837},
  shortjournal = {Ann. Appl. Stat.},
  title        = {PALM: Patient-centered treatment ranking via large-scale multivariate network meta-analysis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handling categorical features with many levels using a
product partition model. <em>AOAS</em>, <em>17</em>(1), 786–814. (<a
href="https://doi.org/10.1214/22-AOAS1651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common difficulty in data analysis is how to handle categorical predictors with a large number of levels or categories. Few proposals have been developed to tackle this important and frequent problem. We introduce a generative model that simultaneously carries out the model fitting and the aggregation of the categorical levels into larger groups. We represent the categorical predictor by a graph where the nodes are the categories and establish a probability distribution over meaningful partitions of this graph. Conditionally on the observed data, we obtain a posterior distribution for the levels aggregation, allowing the inference about the most probable clustering for the categories. Simultaneously, we extract inference about all the other regression model parameters. We compare our and state-of-art methods showing that it has equally good predictive performance and more interpretable results. Our approach balances out accuracy vs. interpretability, a current important concern in statistics and machine learning.},
  archive      = {J_AOAS},
  author       = {Tulio L. Criscuolo and Renato M. Assunção and Rosangela H. Loschi and Wagner Meira Jr. and Danna Cruz-Reyes},
  doi          = {10.1214/22-AOAS1651},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {786-814},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Handling categorical features with many levels using a product partition model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nested conformal prediction sets for classification with
applications to probation data. <em>AOAS</em>, <em>17</em>(1), 761–785.
(<a href="https://doi.org/10.1214/22-AOAS1650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk assessments to help inform criminal justice decisions have been used in the United States since the 1920s. Over the past several years, statistical learning risk algorithms have been introduced amid much controversy about fairness, transparency and accuracy. In this paper we focus on accuracy for a large department of probation and parole that is considering a major revision of its current, statistical learning risk methods. Because the content of each offender’s supervision is substantially shaped by a forecast of subsequent conduct, forecasts have real consequences. Here, we consider the probability that risk forecasts are correct. We augment standard statistical learning estimates of forecasting uncertainty (i.e., confusion tables) with uncertainty estimates from nested conformal prediction sets. In a demonstration of concept using data from the department of probation and parole, we show that the standard uncertainty measures and uncertainty measures from nested conformal prediction sets can differ dramatically in formulation and output. We also provide a modification of nested conformal, called the localized conformal method, to match confusion tables more closely. A strong case can be made favoring the nested conformal approach. As best, we can tell our formulation of such comparisons and consequent recommendations is novel.},
  archive      = {J_AOAS},
  author       = {Arun K. Kuchibhotla and Richard A. Berk},
  doi          = {10.1214/22-AOAS1650},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {761-785},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Nested conformal prediction sets for classification with applications to probation data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-discretization approximation enriches continuous-time
discrete-space models for animal movement. <em>AOAS</em>,
<em>17</em>(1), 740–760. (<a
href="https://doi.org/10.1214/22-AOAS1649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous time discrete state models are a valuable tool for explaining animal movement. However, data collection to fit such models over a specified window of time can be misaligned with the actual realization of the movement process. This necessitates approximate model fitting, at present, through approximate imputation distributions (AIDs). Here, we propose a direct time-discretization approximation to the likelihood. The approach employs familiar ideas from hidden Markov modeling. Computation is implemented through the induced infinitesimal generator matrix. Linearization of this matrix expedites computation time. Through simulation and a real data application involving whale movement, we demonstrate that this model fitting strategy can outperform AID approaches.},
  archive      = {J_AOAS},
  author       = {Joshua Hewitt and Alan E. Gelfand and Robert S. Schick},
  doi          = {10.1214/22-AOAS1649},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {740-760},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Time-discretization approximation enriches continuous-time discrete-space models for animal movement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Do forecasts of bankruptcy cause bankruptcy? A machine
learning sensitivity analysis. <em>AOAS</em>, <em>17</em>(1), 711–739.
(<a href="https://doi.org/10.1214/22-AOAS1648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely speculated that auditors’ public forecasts of bankruptcy are, at least in part, self-fulfilling prophecies in the sense that they actually cause bankruptcies that would not have otherwise occurred. This conjecture is hard to prove, however, because the strong association between bankruptcies and bankruptcy forecasts could simply indicate that auditors are skillful forecasters with unique access to highly predictive covariates. In this paper we investigate the causal effect of bankruptcy forecasts on bankruptcy using nonparametric sensitivity analysis. We contrast our analysis with two alternative approaches: a linear bivariate probit model with an endogenous regressor and a recently developed bound on risk ratios called E-values. Additionally, our machine learning approach incorporates a monotonicity constraint corresponding to the assumption that bankruptcy forecasts do not make bankruptcies less likely. Finally, a tree-based posterior summary of the treatment effect estimates allows us to explore which observable firm characteristics moderate the inducement effect.},
  archive      = {J_AOAS},
  author       = {Demetrios Papakostas and P. Richard Hahn and Jared Murray and Frank Zhou and Joseph Gerakos},
  doi          = {10.1214/22-AOAS1648},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {711-739},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Do forecasts of bankruptcy cause bankruptcy? a machine learning sensitivity analysis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model selection uncertainty and stability in beta regression
models: A study of bootstrap-based model averaging with an empirical
application to clickstream data. <em>AOAS</em>, <em>17</em>(1), 680–710.
(<a href="https://doi.org/10.1214/22-AOAS1647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical model development is a central feature of many scientific investigations with a vast methodological landscape. However, uncertainty in the model development process has received less attention and is frequently resolved nonrigorously through beliefs about generalizability, practical usefulness, and computational ease. This is particularly problematic in settings of abundant data, such as clickstream data, as model selection routinely admits multiple models and imposes a source of uncertainty, unacknowledged and unknown by many, on all postselection conclusions. Regression models, based on the beta distribution, are a class of nonlinear models, attractive because of their great flexibility and potential explanatory power, but have not been investigated from the standpoint of multimodel uncertainty and model averaging. For this reason a formalized tool that can combine model selection uncertainty and beta regression modeling is presented in this work. The tool combines bootstrap model averaging, model selection, and asymptotic theory to yield a procedure that can perform joint modeling of the mean and precision parameters, capture sources of variability in the data, and achieve more accurate claims of estimate precision, variable importance, generalization performance, and model stability. Practical utility of the tool is demonstrated through a study of model selection consistency and variable importance in average exit and bounce rate statistical models. This work emphasizes the necessity of a departure from the all-too-common practice of ignoring model selection uncertainty and introduces an accessible technique to handle frequently neglected aspects of the modeling pipeline.},
  archive      = {J_AOAS},
  author       = {Corban Allenbrand and Ben Sherwood},
  doi          = {10.1214/22-AOAS1647},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {680-710},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Model selection uncertainty and stability in beta regression models: A study of bootstrap-based model averaging with an empirical application to clickstream data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ALPHA: Audit that learns from previously hand-audited
ballots. <em>AOAS</em>, <em>17</em>(1), 641–679. (<a
href="https://doi.org/10.1214/22-AOAS1646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A risk-limiting election audit (RLA) offers a statistical guarantee: if the reported electoral outcome is incorrect, the audit has a known maximum chance (the risk limit) of not correcting it before it becomes final. BRAVO (Lindeman, Stark and Yates (In Proceedings of the 2011 Electronic Voting Technology Workshop/Workshop on Trustworthy Elections (EVT/WOTE’11) (2012) USENIX)), based on Wald’s sequential probability ratio test for the Bernoulli parameter, is the simplest and most widely tried method for RLAs, but it has limitations. It cannot accommodate sampling without replacement or stratified sampling which can improve efficiency and are sometimes required by law. It applies only to ballot-polling audits which are less efficient than comparison audits. It applies to plurality, majority, supermajority, proportional representation, and instant-runoff voting (IRV, using RAIRE (Blom, Stuckey and Teague (In Electronic Voting (2018) 17–34 Springer))) but not to other social choice functions for which there are RLA methods. And while BRAVO has the smallest expected sample size among sequentially valid ballot-polling-with-replacement methods when the reported vote shares are exactly correct, it can require arbitrarily large samples when the reported reported winner(s) really won but the reported vote shares are incorrect. ALPHA is a simple generalization of BRAVO that: (i) works for sampling with and without replacement, with and without weights, with and without stratification, and for Bernoulli sampling; (ii) works not only for ballot polling but also for ballot-level comparison, batch polling, and batch-level comparison audits; (iii) works for all social choice functions covered by SHANGRLA (Stark (In Financial Cryptography and Data Security (2020) Springer)), including approval voting, STAR-Voting, proportional representation schemes, such as D’Hondt and Hamilton, IRV, Borda count, and all scoring rules, and (iv) in situations where both ALPHA and BRAVO apply, requires smaller samples than BRAVO when the reported vote shares are wrong but the outcome is correct—five orders of magnitude in some examples. ALPHA includes the family of betting martingale tests in RiLACS (Waudby-Smith, Stark and Ramdas (In Electronic Voting. E-Vote-ID 2021 (2021) Springer)) with a different betting strategy parametrized as an estimator of the population mean and explicit flexibility to accommodate sampling weights and population bounds that change with each draw. A Python implementation is provided.},
  archive      = {J_AOAS},
  author       = {Philip B. Stark},
  doi          = {10.1214/22-AOAS1646},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {641-679},
  shortjournal = {Ann. Appl. Stat.},
  title        = {ALPHA: Audit that learns from previously hand-audited ballots},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TEAM: A multiple testing algorithm on the aggregation tree
for flow cytometry analysis. <em>AOAS</em>, <em>17</em>(1), 621–640. (<a
href="https://doi.org/10.1214/22-AOAS1645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In immunology studies, flow cytometry is a commonly used multivariate single-cell assay. One key goal in flow cytometry analysis is to detect the immune cells responsive to certain stimuli. Statistically, this problem can be translated into comparing two protein expression probability density functions (pdfs) before and after the stimulus; the goal is to pinpoint the regions where these two pdfs differ. Further screening of these differential regions can be performed to identify enriched sets of responsive cells. In this paper we model identifying differential density regions as a multiple testing problem. First, we partition the sample space into small bins. In each bin we form a hypothesis to test the existence of differential pdfs. Second, we develop a novel multiple testing method, called TEAM (testing on the aggregation tree method), to identify those bins that harbor differential pdfs while controlling the false discovery rate (FDR) under the desired level. TEAM embeds the testing procedure into an aggregation tree to test from fine- to coarse-resolution. The procedure achieves the statistical goal of pinpointing density differences to the smallest possible regions. TEAM is computationally efficient, capable of analyzing large flow cytometry data sets in much shorter time compared with competing methods. We applied TEAM and competing methods on a flow cytometry data set to identify T cells responsive to the cytomegalovirus (CMV)-pp65 antigen stimulation. With additional downstream screening, TEAM successfully identified enriched sets containing monofunctional, bifunctional, and polyfunctional T cells. Competing methods either did not finish in a reasonable time frame or provided less interpretable results. Numerical simulations and theoretical justifications demonstrate that TEAM has asymptotically valid, powerful, and robust performance. Overall, TEAM is a computationally efficient and statistically powerful algorithm that can yield meaningful biological insights in flow cytometry studies.},
  archive      = {J_AOAS},
  author       = {John A. Pura and Xuechan Li and Cliburn Chan and Jichun Xie},
  doi          = {10.1214/22-AOAS1645},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {621-640},
  shortjournal = {Ann. Appl. Stat.},
  title        = {TEAM: A multiple testing algorithm on the aggregation tree for flow cytometry analysis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing experiments for estimating an appropriate outlet
size for a silo type problem. <em>AOAS</em>, <em>17</em>(1), 606–620.
(<a href="https://doi.org/10.1214/22-AOAS1644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jam formation is a problem that may occur when granular material is discharged by gravity from a silo. The estimation of the minimum outlet size, which guarantees that the time to the next jamming event is long enough, can be crucial in the industry. The time is modeled by an exponential distribution with two unknown parameters, and this goal translates to precise estimation of a nonlinear transformation of the parameters. We obtain c-optimum experimental designs with that purpose, applying the graphic Elfving method. Because the optimal experimental designs depend on the nominal values of the parameters, we conduct a sensitivity analysis on our dataset. Finally, a simulation study checks the performance of the approximations, first with the Fisher Information matrix, then with the linearization of the function to be estimated. The results are useful for experimenting in a laboratory and then translating the results to a real scenario. From the application we develop a general methodology for estimating a one-dimensional transformation of the parameters of a nonlinear model.},
  archive      = {J_AOAS},
  author       = {Jesus Lopez-Fidalgo and Caterina May and Jose Antonio Moler},
  doi          = {10.1214/22-AOAS1644},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {606-620},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Designing experiments for estimating an appropriate outlet size for a silo type problem},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian clustering of spatial functional data with
application to a human mobility study during COVID-19. <em>AOAS</em>,
<em>17</em>(1), 583–605. (<a
href="https://doi.org/10.1214/22-AOAS1643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus (COVID-19) global pandemic has made a significant impact on people’s social activities. Cell phone mobility data provide unique and rich information on studying this impact. The motivating dataset of this study is the daily leaving-home index data at Harris County in Texas provided by SafeGraph. To study changes in daily leaving-home index and how they relate to public policy and sociodemographic variables, we propose a new Bayesian wavelet model for modeling and clustering spatial functional data, where domain partitioning is achieved by operating on the spanning trees. The resulting clusters can have arbitrary shapes and are spatially contiguous in the input domain. An efficient tailored reversible jump Markov chain Monte Carlo algorithm is proposed to implement the model. The method is applied to the spatial functional data of the daily percentages of people who left home. We focus on the time period covering both lockdown and phased reopening in Texas during the COVID-19 pandemic and study the changing behaviors of those functional curves. By linking the clustering results with the sociodemographic information, we identify several covariates of census blocks that have a noticeable impact on the clustering patterns of people’s mobility behaviors.},
  archive      = {J_AOAS},
  author       = {Bohai Zhang and Huiyan Sang and Zhao Tang Luo and Hui Huang},
  doi          = {10.1214/22-AOAS1643},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {583-605},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian clustering of spatial functional data with application to a human mobility study during COVID-19},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatiotemporal wildfire modeling through point processes
with moderate and extreme marks. <em>AOAS</em>, <em>17</em>(1), 560–582.
(<a href="https://doi.org/10.1214/22-AOAS1642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate spatiotemporal modeling of conditions leading to moderate and large wildfires provides better understanding of mechanisms driving fire-prone ecosystems and improves risk management. Here, we develop a joint model for the occurrence intensity and the wildfire size distribution, by combining extreme-value theory and point processes within a novel Bayesian hierarchical model, and use it to study daily summer wildfire data for the French Mediterranean basin during 1995–2018. The occurrence component models wildfire ignitions as a spatiotemporal log-Gaussian Cox process. Burnt areas are numerical marks attached to points and are considered as extreme if they exceed a high threshold. The size component is a two-component mixture varying in space and time that jointly models moderate and extreme fires. We capture nonlinear influence of covariates (Fire Weather Index, forest cover) through component-specific smooth functions which may vary with season. We propose estimating shared random effects between model components to reveal and interpret common drivers of different aspects of wildfire activity. This increases parsimony and reduces estimation uncertainty, giving better predictions. Specific stratified subsampling of zero counts is implemented to cope with large observation vectors. We compare and validate models through predictive scores and visual diagnostics. Our methodology provides a holistic approach to explaining and predicting the drivers of wildfire activity and associated uncertainties.},
  archive      = {J_AOAS},
  author       = {Jonathan Koh and François Pimont and Jean-Luc Dupuy and Thomas Opitz},
  doi          = {10.1214/22-AOAS1642},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {560-582},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatiotemporal wildfire modeling through point processes with moderate and extreme marks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subject-specific dirichlet-multinomial regression for
multi-district microbiota data analysis. <em>AOAS</em>, <em>17</em>(1),
539–559. (<a href="https://doi.org/10.1214/22-AOAS1641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many environments within the human body host a collection of micro-organisms called microbiota. Recent findings have linked the composition of the microbiota to the development of different human diseases, including cancer. Motivated by a recent colorectal cancer (crc) study, we investigate the effect of clinical factors and diet-related covariates on the microbiota compositions; for the patients enrolled in this study, microbiota abundance counts are collected from three different districts, namely, tumor, fecal and salivary samples. Building upon the Dirichlet-multinomial regression framework, we develop a high-dimensional Bayesian hierarchical model that exploits subject-specific regression coefficients to simultaneously borrow strength across districts and include complex interactions between diet and clinical factors if supported by the data. The proposed method identifies relevant associations through model selection priors and thresholding mechanisms. Posterior inference is performed via a Markov chain Monte Carlo algorithm. We use simulation studies to assess the performance of our method, and found our approach to outperform competing methods that do not account for complex interactions. Finally, a thorough analysis of the crc data illustrates the benefits of the proposed approach.},
  archive      = {J_AOAS},
  author       = {Matteo Pedone and Amedeo Amedei and Francesco C. Stingo},
  doi          = {10.1214/22-AOAS1641},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {539-559},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Subject-specific dirichlet-multinomial regression for multi-district microbiota data analysis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A variational bayesian approach to identifying whole-brain
directed networks with fMRI data. <em>AOAS</em>, <em>17</em>(1),
518–538. (<a href="https://doi.org/10.1214/22-AOAS1640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain is a high-dimensional directed network system, as it consists of many regions as network nodes that exert influence on each other. The directed influence exerted by one region on another is referred to as directed connectivity. We aim to reveal whole-brain directed networks based on resting-state functional magnetic resonance imaging (fMRI) data of many subjects. However, it is both statistically and computationally challenging to produce scientifically meaningful estimates of whole-brain directed networks. To address the statistical modeling challenge, we assume modular brain networks which reflect functional specialization and functional integration of the brain. We address the computational challenge by developing a variational Bayesian method to estimate the new model. We apply our method to resting-state fMRI data of many subjects and identify modules and directed connections in whole-brain directed networks. The identified modules are accordant with functional brain systems specialized for different functions. We also detect directed connections between functionally specialized modules, which is not attainable by existing network methods, based on functional connectivity. In summary, this paper presents a new computationally efficient and flexible method for directed network studies of the brain as well as new scientific findings regarding the functional organization of the human brain.},
  archive      = {J_AOAS},
  author       = {Yaotian Wang and Guofen Yan and Xiaofeng Wang and Shuoran Li and Lingyi Peng and Dana L. Tudorascu and Tingting Zhang},
  doi          = {10.1214/22-AOAS1640},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {518-538},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A variational bayesian approach to identifying whole-brain directed networks with fMRI data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling panels of extremes. <em>AOAS</em>, <em>17</em>(1),
498–517. (<a href="https://doi.org/10.1214/22-AOAS1639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme value applications commonly employ regression techniques to capture cross-sectional heterogeneity or time variation in the data. Estimation of the parameters of an extreme value regression model is notoriously challenging due to the small number of observations that are usually available in applications. When repeated extreme measurements are collected on the same individuals, that is, a panel of extremes is available, pooling the observations in groups can improve the statistical inference. We study three data sets related to risk assessment in finance, climate science, and hydrology. In all three cases the problem can be formulated as an extreme value panel regression model with a latent group structure and group-specific parameters. We propose a new algorithm that jointly assigns the individuals to the latent groups and estimates the parameters of the regression model inside each group. Our method efficiently recovers the underlying group structure without prior information, and for the three data sets it provides improved return level estimates and helps answer important domain-specific questions.},
  archive      = {J_AOAS},
  author       = {Debbie J. Dupuis and Sebastian Engelke and Luca Trapin},
  doi          = {10.1214/22-AOAS1639},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {498-517},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling panels of extremes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating promotion effects in email marketing using a
large-scale cross-classified bayesian joint model for nested imbalanced
data. <em>AOAS</em>, <em>17</em>(1), 476–497. (<a
href="https://doi.org/10.1214/22-AOAS1638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a large-scale, cross-classified nested (CRON) joint model for modeling customer responses to opening, clicking, and purchasing from promotion emails. Our logistic regression-based joint model contains crossing of promotions and customer effects and allows estimation of the heterogeneous effects of different promotion emails, after adjusting for customer preferences, attributes, and historical behaviors. Using data from an email marketing campaign of an apparel retailer, we exhibit the varying effects of promotions not only based on the contents of the email but also across the three different stages, viz. open, click, and purchase of the conversion funnel. We conduct Bayesian estimation of the parameters in the joint model by using a block Metropolis–Hastings algorithm that not only incorporates nested subsampling to tackle the severe imbalance between conversions and no conversions but also uses additive transformation-based modifications of random walk Metropolis to scale estimation for large numbers of customers. We extend our approach to a segmented cross-classified nested (SCRON) joint model that encompasses the possibility of varying promotion effects across different customer segments. The resultant high-dimensional model is estimated using spike-and-slab priors on the promotion and customer segment interactions. Our nested joint model accounts for the correlations in customer preferences across the conversion funnel. Based on the promotion estimates from the model, we demonstrate how marketers can use different priced, nonpriced, and combination of price and nonprice promotions to increase brand awareness or increase purchases. Comparing estimates from CRON and SCRON models, we display the benefits of targeted marketing by using email promotion lists which are separately optimized for the different customer segments.},
  archive      = {J_AOAS},
  author       = {Sabyasachi Mukhopadhyay and Wreetabrata Kar and Gourab Mukherjee},
  doi          = {10.1214/22-AOAS1638},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {476-497},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating promotion effects in email marketing using a large-scale cross-classified bayesian joint model for nested imbalanced data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical matching and subclassification with a continuous
dose: Characterization, algorithm, and application to a health outcomes
study. <em>AOAS</em>, <em>17</em>(1), 454–475. (<a
href="https://doi.org/10.1214/22-AOAS1635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subclassification and matching are often used in empirical studies to adjust for observed covariates; however, they are largely restricted to relatively simple study designs with a binary treatment and less developed for designs with a continuous exposure. Matching with exposure doses is particularly useful in instrumental variable designs and in understanding the dose-response relationships. In this article we propose two criteria for optimal subclassification based on subclass homogeneity, in the context of having a continuous exposure dose, and propose an efficient polynomial-time algorithm that is guaranteed to find an optimal subclassification with respect to one criterion and serves as a 2-approximation algorithm for the other criterion. We discuss how to incorporate dose and use appropriate penalties to control the number of subclasses in the design. Via extensive simulations, we systematically compare our proposed design to optimal nonbipartite pair matching and demonstrate that combining our proposed subclassification scheme with regression adjustment helps to reduce model dependence for parametric causal inference with a continuous dose. We apply the new design and associated randomization-based inferential procedure to study the effect of transesophageal echocardiography (TEE) monitoring during coronary artery bypass graft (CABG) surgery on patients’ postsurgery clinical outcomes, using Medicare and Medicaid claims data, and find evidence that TEE monitoring lowers patients’ all-cause 30-day mortality rate.},
  archive      = {J_AOAS},
  author       = {Bo Zhang and Emily J. Mackay and Mike Baiocchi},
  doi          = {10.1214/22-AOAS1635},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {454-475},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Statistical matching and subclassification with a continuous dose: Characterization, algorithm, and application to a health outcomes study},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individualized risk assessment of preoperative opioid use by
interpretable neural network regression. <em>AOAS</em>, <em>17</em>(1),
434–453. (<a href="https://doi.org/10.1214/22-AOAS1634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preoperative opioid use has been reported to be associated with higher preoperative opioid demand, worse postoperative outcomes, and increased postoperative healthcare utilization and expenditures. Understanding the risk of preoperative opioid use helps establish patient-centered pain management. In the field of machine learning, deep neural network (DNN) has emerged as a powerful means for risk assessment because of its superb prediction power; however, the blackbox algorithms may make the results less interpretable than statistical models. Bridging the gap between the statistical and machine learning fields, we propose a novel interpretable neural network regression (INNER) which combines the strengths of statistical and DNN models. We use the proposed INNER to conduct individualized risk assessment of preoperative opioid use. Intensive simulations and an analysis of 34,186 patients expecting surgery in the Analgesic Outcomes Study (AOS) show that the proposed INNER not only can accurately predict the preoperative opioid use using preoperative characteristics as DNN but also can estimate the patient-specific odds of opioid use without pain and the odds ratio of opioid use for a unit increase in the reported overall body pain, leading to more straightforward interpretations of the tendency to use opioids than DNN. Our results identify the patient characteristics that are strongly associated with opioid use and is largely consistent with the previous findings, providing evidence that INNER is a useful tool for individualized risk assessment of preoperative opioid use.},
  archive      = {J_AOAS},
  author       = {Yuming Sun and Jian Kang and Chad Brummett and Yi Li},
  doi          = {10.1214/22-AOAS1634},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {434-453},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Individualized risk assessment of preoperative opioid use by interpretable neural network regression},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topological learning for brain networks. <em>AOAS</em>,
<em>17</em>(1), 403–433. (<a
href="https://doi.org/10.1214/22-AOAS1633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel topological learning framework that integrates networks of different sizes and topology through persistent homology. Such challenging task is made possible through the introduction of a computationally efficient topological loss. The use of the proposed loss bypasses the intrinsic computational bottleneck associated with matching networks. We validate the method in extensive statistical simulations to assess its effectiveness when discriminating networks with different topology. The method is further demonstrated in a twin brain imaging study where we determine if brain networks are genetically heritable. The challenge here is due to the difficulty of overlaying the topologically different functional brain networks obtained from resting-state functional MRI onto the template structural brain network obtained through diffusion MRI.},
  archive      = {J_AOAS},
  author       = {Tananun Songdechakraiwut and Moo K. Chung},
  doi          = {10.1214/22-AOAS1633},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {403-433},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Topological learning for brain networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized fiducial factor: An alternative to the bayes
factor for forensic identification of source problems. <em>AOAS</em>,
<em>17</em>(1), 378–402. (<a
href="https://doi.org/10.1214/22-AOAS1632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One formulation of forensic identification of source problems is to determine the source of trace evidence, for instance, glass fragments found on a suspect for a crime. The current state of the science is to compute a Bayes factor comparing the marginal distribution of measurements of trace evidence under two competing propositions for whether or not the unknown source evidence originated from a specific source. The obvious problem with such an approach is the ability to tailor the prior distributions (placed on the features/parameters of the statistical model for the measurements of trace evidence) in favor of the defense or prosecution which is further complicated by the fact that the typical number of measurements of trace evidence is typically sufficiently small that prior choice/specification has a strong influence on the value of the Bayes factor. To remedy this problem of prior specification and choice, we develop an alternative to the Bayes factor, within the framework of generalized fiducial inference, that we term a generalized fiducial factor. Furthermore, we demonstrate empirically, on synthetic and real Netherlands Forensic Institute casework data, deficiencies in Bayes factor and classical/frequentist likelihood ratio approaches.},
  archive      = {J_AOAS},
  author       = {Jonathan P. Williams and Danica M. Ommen and Jan Hannig},
  doi          = {10.1214/22-AOAS1632},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {378-402},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Generalized fiducial factor: An alternative to the bayes factor for forensic identification of source problems},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling cell populations measured by flow cytometry with
covariates using sparse mixture of regressions. <em>AOAS</em>,
<em>17</em>(1), 357–377. (<a
href="https://doi.org/10.1214/22-AOAS1631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ocean is filled with microscopic microalgae, called phytoplankton, which together are responsible for as much photosynthesis as all plants on land combined. Our ability to predict their response to the warming ocean relies on understanding how the dynamics of phytoplankton populations is influenced by changes in environmental conditions. One powerful technique to study the dynamics of phytoplankton is flow cytometry which measures the optical properties of thousands of individual cells per second. Today, oceanographers are able to collect flow cytometry data in real time onboard a moving ship, providing them with fine-scale resolution of the distribution of phytoplankton across thousands of kilometers. One of the current challenges is to understand how these small- and large-scale variations relate to environmental conditions, such as nutrient availability, temperature, light and ocean currents. In this paper we propose a novel sparse mixture of multivariate regressions model to estimate the time-varying phytoplankton subpopulations while simultaneously identifying the specific environmental covariates that are predictive of the observed changes to these subpopulations. We demonstrate the usefulness and interpretability of the approach using both synthetic data and real observations collected on an oceanographic cruise conducted in the northeast Pacific in the spring of 2017.},
  archive      = {J_AOAS},
  author       = {Sangwon Hyun and Mattias Rolf Cape and Francois Ribalet and Jacob Bien},
  doi          = {10.1214/22-AOAS1631},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {357-377},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling cell populations measured by flow cytometry with covariates using sparse mixture of regressions},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian non-homogeneous hidden markov model with variable
selection for investigating drivers of seizure risk cycling.
<em>AOAS</em>, <em>17</em>(1), 333–356. (<a
href="https://doi.org/10.1214/22-AOAS1630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major issue in the clinical management of epilepsy is the unpredictability of seizures. Yet, traditional approaches to seizure forecasting and risk assessment in epilepsy rely heavily on raw seizure frequencies which are a stochastic measurement of seizure risk. We consider a Bayesian nonhomogeneous hidden Markov model for unsupervised clustering of zero-inflated seizure count data. The proposed model allows for a probabilistic estimate of the sequence of seizure risk states at the individual level. It also offers significant improvement over prior approaches by incorporating a variable selection prior for the identification of clinical covariates that drive seizure risk changes and accommodating highly granular data. For inference, we implement an efficient sampler that employs stochastic search and data augmentation techniques. We evaluate model performance on simulated seizure count data. We then demonstrate the clinical utility of the proposed model by analyzing daily seizure count data from 133 patients with Dravet syndrome collected through the Seizure TrackerTM system, a patient-reported electronic seizure diary. We report on the dynamics of seizure risk cycling, including validation of several known pharmacologic relationships. We also uncover novel findings characterizing the presence and volatility of risk states in Dravet syndrome which may directly inform counseling to reduce the unpredictability of seizures for patients with this devastating cause of epilepsy.},
  archive      = {J_AOAS},
  author       = {Emily T. Wang and Sharon Chiang and Zulfi Haneef and Vikram R. Rao and Robert Moss and Marina Vannucci},
  doi          = {10.1214/22-AOAS1630},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {333-356},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian non-homogeneous hidden markov model with variable selection for investigating drivers of seizure risk cycling},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model selection for maternal hypertensive disorders with
symmetric hierarchical dirichlet processes. <em>AOAS</em>,
<em>17</em>(1), 313–332. (<a
href="https://doi.org/10.1214/22-AOAS1628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypertensive disorders of pregnancy occur in about 10\% of pregnant women around the world. Though there is evidence that hypertension impacts maternal cardiac functions, the relation between hypertension and cardiac dysfunctions is only partially understood. The study of this relationship can be framed as a joint inferential problem on multiple populations, each corresponding to a different hypertensive disorder diagnosis, that combines multivariate information provided by a collection of cardiac function indexes. A Bayesian nonparametric approach seems particularly suited for this setup, and we demonstrate it on a dataset consisting of transthoracic echocardiography results of a cohort of Indian pregnant women. We are able to perform model selection, provide density estimates of cardiac function indexes and a latent clustering of patients: these readily interpretable inferential outputs allow to single out modified cardiac functions in hypertensive patients, compared to healthy subjects, and progressively increased alterations with the severity of the disorder. The analysis is based on a Bayesian nonparametric model that relies on a novel hierarchical structure, called symmetric hierarchical Dirichlet process. This is suitably designed so that the mean parameters are identified and used for model selection across populations, a penalization for multiplicity is enforced, and the presence of unobserved relevant factors is investigated through a latent clustering of subjects. Posterior inference relies on a suitable Markov chain Monte Carlo algorithm, and the model behaviour is also showcased on simulated data.},
  archive      = {J_AOAS},
  author       = {Beatrice Franzolini and Antonio Lijoi and Igor Prünster},
  doi          = {10.1214/22-AOAS1628},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {313-332},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Model selection for maternal hypertensive disorders with symmetric hierarchical dirichlet processes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the average treatment effect in randomized
clinical trials with all-or-none compliance. <em>AOAS</em>,
<em>17</em>(1), 294–312. (<a
href="https://doi.org/10.1214/22-AOAS1627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noncompliance is a common intercurrent event in randomized clinical trials that raises important questions about analytical objectives and approaches. Motivated by the Multiple Risk Factor Intervention Trial (MRFIT), we consider how to estimate the average treatment effect (ATE) in randomized trials with all-or-none compliance. Confounding is a major challenge in estimating the ATE, and conventional methods for confounding adjustment typically require the assumption of no unmeasured confounders which may be difficult to justify. Using randomized treatment assignment as an instrumental variable, the ATE can be identified in the presence of unmeasured confounders under suitable assumptions, including an assumption that limits the effect-modifying activities of unmeasured confounders. We describe and compare several estimation methods based on different modeling assumptions. Some of these methods are able to incorporate information from auxiliary covariates for improved efficiency without introducing bias. The different methods are compared in a simulation study and applied to the MRFIT.},
  archive      = {J_AOAS},
  author       = {Zhiwei Zhang and Zonghui Hu and Dean Follmann and Lei Nie},
  doi          = {10.1214/22-AOAS1627},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {294-312},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating the average treatment effect in randomized clinical trials with all-or-none compliance},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized theme dictionary models for association pattern
discovery. <em>AOAS</em>, <em>17</em>(1), 269–293. (<a
href="https://doi.org/10.1214/22-AOAS1626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering association patterns of items from a collection of baskets composed of different items is an important problem in various fields. Assuming that each basket is composed of themes of items randomly sampled from a theme dictionary, the theme dictionary model provides a general framework to achieve efficient association pattern discovery with statistical inference. This paper extends the original theme dictionary model by allowing more than one category of items in a basket and only presence/absence of items is observed for each basket with all quantitative information missing. The extended models can solve a larger range of practical problems that cannot be handled by the original theme dictionary model. Both simulation studies and real data applications confirm the superiority of the proposed methods over the existing ones.},
  archive      = {J_AOAS},
  author       = {Yang Yang and Ke Deng},
  doi          = {10.1214/22-AOAS1626},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {269-293},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Generalized theme dictionary models for association pattern discovery},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multivariate frequency-severity framework for healthcare
data breaches. <em>AOAS</em>, <em>17</em>(1), 240–268. (<a
href="https://doi.org/10.1214/22-AOAS1625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data breaches in healthcare have become a substantial concern in recent years and cause millions of dollars in financial losses each year. It is fundamental for government regulators, insurance companies, and stakeholders to understand the breach frequency and the number of affected individuals in each state, as these are directly related to the federal Health Insurance Portability and Accountability Act (HIPAA) and state data breach laws. However, an obstacle to studying data breaches in healthcare is the lack of suitable statistical approaches. We develop a novel multivariate frequency-severity framework to analyze breach frequency and the number of affected individuals at the state level. A mixed effects model is developed to model the square root transformed frequency, and the log-gamma distribution is proposed to capture the skewness and heavy tail exhibited by the distribution of numbers of affected individuals. We further discover a positive nonlinear dependence between the transformed frequency and the log-transformed numbers of affected individuals (i.e., severity). In particular, we propose to use a D-vine copula to capture the multivariate dependence among conditional severities, given frequencies due to its inherent temporal structure and rich bivariate copula families. The rejection sampling technique is developed to simulate the predictive distributions. Both the in-sample and out-of-sample studies show that the proposed multivariate frequency-severity model that accommodates nonlinear dependence has satisfactory fitting and prediction performances.},
  archive      = {J_AOAS},
  author       = {Hong Sun and Maochao Xu and Peng Zhao},
  doi          = {10.1214/22-AOAS1625},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {240-268},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A multivariate frequency-severity framework for healthcare data breaches},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularized fingerprinting in detection and attribution of
climate change with weight matrix optimizing the efficiency in scaling
factor estimation. <em>AOAS</em>, <em>17</em>(1), 225–239. (<a
href="https://doi.org/10.1214/22-AOAS1624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection and attribution analyses play a central role in establishing the causal effect of human activities on global warming. The most commonly used method in such analyses, optimal fingerprinting, is a multiple regression where each covariate has a measurement error whose covariance matrix is the same as that of the regression error up to a known scale. Inferences about the regression coefficients are critical not only for making statements about detection and attribution but also for quantifying the uncertainty in important outcomes derived from detection and attribution analyses. When there are no errors-in-variables (EIV), the optimal weight matrix in estimating the regression coefficients is the precision matrix of the regression error. This matrix, however, is never known and has to be estimated from climate model simulations with regularization. The consequence is that the optimal fingerprinting method is not optimal, as believed in practice. We construct a weight matrix by inverting a nonlinear shrinkage estimate of the error covariance matrix that minimizes loss functions directly targeting the uncertainty of the resulting regression coefficient estimator. The resulting estimator of the regression coefficients is asymptotically optimal as the sample size of the climate model simulations and the matrix dimension go to infinity together with a limiting ratio. When EIVs are present, the estimator of the regression coefficients, based on the proposed weight matrix, is asymptotically more efficient than that based on the inverse of the existing linear shrinkage estimator of the error covariance matrix. The performance of the method is confirmed in finite sample simulation studies mimicking realistic situations in terms of the length of the confidence intervals and empirical coverage rates for the regression coefficients. In an application to detection and attribution analyses of the mean temperature at different spatial scales, the method yielded shorter confidence intervals which are important for such analyses in practice.},
  archive      = {J_AOAS},
  author       = {Yan Li and Kun Chen and Jun Yan and Xuebin Zhang},
  doi          = {10.1214/22-AOAS1624},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {225-239},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Regularized fingerprinting in detection and attribution of climate change with weight matrix optimizing the efficiency in scaling factor estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian classification, anomaly detection, and survival
analysis using network inputs with application to the microbiome.
<em>AOAS</em>, <em>17</em>(1), 199–224. (<a
href="https://doi.org/10.1214/22-AOAS1623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the study of a single network is well established, technological advances now allow for the collection of multiple networks with relative ease. Increasingly, anywhere from several to thousands of networks can be created from brain imaging, gene coexpression data, or microbiome measurements. And these networks, in turn, are being looked to as potentially powerful features to be used in modeling. However, with networks being non-Euclidean in nature, how best to incorporate them into standard modeling tasks is not obvious. In this paper we propose a Bayesian modeling framework that provides a unified approach to binary classification, anomaly detection, and survival analysis with network inputs. We encode the networks in the kernel of a Gaussian process prior via their pairwise differences, and we discuss several choices of provably positive definite kernel that can be plugged into our models. Although our methods are widely applicable, we are motivated here, in particular, by microbiome research (where network analysis is emerging as the standard approach for capturing the interconnectedness of microbial taxa across both time and space) and its potential for reducing preterm delivery and improving personalization of prenatal care.},
  archive      = {J_AOAS},
  author       = {Nathaniel Josephs and Lizhen Lin and Steven Rosenberg and Eric D. Kolaczyk},
  doi          = {10.1214/22-AOAS1623},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {199-224},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian classification, anomaly detection, and survival analysis using network inputs with application to the microbiome},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conex–connect: Learning patterns in extremal brain
connectivity from MultiChannel EEG data. <em>AOAS</em>, <em>17</em>(1),
178–198. (<a href="https://doi.org/10.1214/22-AOAS1621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a chronic neurological disorder; it affects more than 50 million people globally. An epileptic seizure acts like a temporary shock to the neuronal system, disrupting normal electrical activity in the brain. Epilepsy is frequently diagnosed with electroencephalograms (EEGs). Current methods study only the time-varying spectra and coherence but do not directly model changes in extreme behavior, neglecting the fact that neuronal oscillations exhibit non-Gaussian heavy-tailed probability distributions. To overcome this limitation, we propose a new approach to characterize brain connectivity based on the joint tail (i.e., extreme) behavior of the EEGs. Our proposed method, the conditional extremal dependence for brain connectivity (Conex–Connect), is a pioneering approach that links the association between extreme values of higher oscillations at a reference channel with the other brain network channels. Using the Conex–Connect method, we discover changes in the extremal dependence driven by the activity at the foci of the epileptic seizure. Our model-based approach reveals that, preseizure, the dependence is notably stable for all channels when conditioning on extreme values of the focal seizure area. By contrast, the dependence between channels is weaker during the seizure, and dependence patterns are more “chaotic.” Using the Conex–Connect method, we identified the high-frequency oscillations as the most relevant features, explaining the conditional extremal dependence of brain connectivity.},
  archive      = {J_AOAS},
  author       = {Matheus B. Guerrero and Raphaël Huser and Hernando Ombao},
  doi          = {10.1214/22-AOAS1621},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {178-198},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Conex–Connect: Learning patterns in extremal brain connectivity from MultiChannel EEG data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential sampling in prospective observational studies.
<em>AOAS</em>, <em>17</em>(1), 153–177. (<a
href="https://doi.org/10.1214/22-AOAS1620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many factors must be taken into account when designing an observational study. Unlike controlled studies, observational studies cannot mitigate the effects of confounding through randomization, and such factors should be incorporated into both the study analysis and the study design. Unfortunately, there is often little data available on most of these factors at the design stage, rendering it infeasible to reliably postulate the impact of these factors on the treatment effect estimate and precision of such an effect. In this work we demonstrate how failure to account for adjustment covariates in the design stage of observational studies utilizing group sequential designs has deleterious effects on the study’s observed power. We do this by constructing a hypothetical study of Alzheimer’s disease biomarkers on cognition, using data from the Alzheimer’s Disease Neuroimaging Initiative. We then outline a constrained boundaries procedure that uses data collected at interim analyses to better estimate variance and correct stopping boundaries to maintain power and type I error, while allowing for early study termination.},
  archive      = {J_AOAS},
  author       = {Mary M. Ryan and Daniel L. Gillen},
  doi          = {10.1214/22-AOAS1620},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {153-177},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sequential sampling in prospective observational studies},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing animal escape data with circular nonparametric
multimodal regression. <em>AOAS</em>, <em>17</em>(1), 130–152. (<a
href="https://doi.org/10.1214/22-AOAS1619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the escape direction of animals subject to covariates is a problem that requires statistical techniques beyond classical regression methods. Apart from the periodicity of the angle of direction, which demands the use of circular statistics, animal escape data usually call for the exploration of the preferred orientations rather than the expected orientation. In this paper we propose the use of a nonparametric method to estimate the conditional local modes of the escape directions of animals from a regression perspective. We present the estimation algorithms and study the asymptotic properties of the estimators as well as its finite sample performance through some simulation experiments. Our proposal is used to model the escape behavior of a group of larval zebrafish escaping from a robot predator. More broadly, the approach presented in this paper can be applied to many existing problems related to animal behavior or other fields.},
  archive      = {J_AOAS},
  author       = {María Alonso-Pena and Rosa M. Crujeiras},
  doi          = {10.1214/22-AOAS1619},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {130-152},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Analyzing animal escape data with circular nonparametric multimodal regression},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic HIV recency classification—a logistic
regression without labeled individual level training data.
<em>AOAS</em>, <em>17</em>(1), 108–129. (<a
href="https://doi.org/10.1214/22-AOAS1618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate HIV incidence estimation, based on individual recent infection status (recent vs long-term infection), is important for monitoring the epidemic, targeting interventions to those at greatest risk of new infection, and evaluating existing programs of prevention and treatment. Starting from 2015, the population-based HIV impact assessment (PHIA) individual-level surveys are implemented in the most-affected countries in sub-Saharan Africa. PHIA is a nationally-representative HIV-focused survey that combines household visits with key questions and cutting-edge technologies, such as biomarker tests for HIV antibody and HIV viral load which offer the unique opportunity of distinguishing between recent infection and long-term infection, and providing relevant HIV information by age, gender, and location. In this article we propose a semisupervised logistic regression model for estimating individual level HIV recency status. It incorporates information from multiple data sources—the PHIA survey, where the true HIV recency status is unknown, and the cohort studies provided in the literature where the relationship between HIV recency status and the covariates are presented in the form of a contingency table. It also utilizes the national level HIV incidence estimates from the epidemiology model. Applying the proposed model to Malawi PHIA data, we demonstrate that our approach is more accurate for the individual level estimation and more appropriate for estimating HIV recency rates at aggregated levels than the current practice—the binary classification tree (BCT).},
  archive      = {J_AOAS},
  author       = {Ben Sheng and Changcheng Li and Le Bao and Runze Li},
  doi          = {10.1214/22-AOAS1618},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {108-129},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Probabilistic HIV recency classification—a logistic regression without labeled individual level training data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social order statistics models for ranking data with
analysis of preferences in social networks. <em>AOAS</em>,
<em>17</em>(1), 89–107. (<a
href="https://doi.org/10.1214/22-AOAS1617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the National Longitudinal Study of Adolescent to Adult Health (Add Health) in the United States during the 1994–95 school year, sampled high school students were asked to rank 17 romantic relationship activities in ideal chronological order and to nominate up to five male and five female best friends within the school. It is natural to see that the students’ rank-order progression of social, romantic and sexual relationships may be influenced strongly by their peers or friends. So far, traditional ranking models do not account for such social network dependency. In this article we introduce a new class of models, called social order statistics (SOS) models, to learn ranking data in social networks. The new models combine the order statistics models and spatial autoregressive models to account for social dependencies among the individuals. A flexible formulation of weight matrices in the spatial model is adopted to provide diverse network effects among the individuals for different items. Efficient and scalable MCMC algorithms are developed to perform Bayesian inference in a parallel manner for large networks with even a few thousand nodes. Analysis of the Add Health dataset reveals that social network effects are different for students’ preferences toward different activities in a relationship. In particular, students’ preferences on romantic and sexual events generally have stronger peer effects than those on social events, and opinions of close friends of the same gender tend to have a larger impact on the preferences on sexual events.},
  archive      = {J_AOAS},
  author       = {Jiaqi Gu and Philip L. H. Yu},
  doi          = {10.1214/22-AOAS1617},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {89-107},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Social order statistics models for ranking data with analysis of preferences in social networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex discontinuity designs using covariates: Impact of
school grade retention on later life outcomes in chile. <em>AOAS</em>,
<em>17</em>(1), 67–88. (<a
href="https://doi.org/10.1214/22-AOAS1616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression discontinuity designs are extensively used for causal inference in observational studies. However, they are usually confined to settings with simple treatment rules and determined by a single running variable with a single cutoff. Motivated by the problem of estimating the impact of grade retention on educational and juvenile crime outcomes in Chile, we propose a framework and methods for complex discontinuity designs that encompass multiple treatment rules. In this framework the observed covariates play a central role for identification, estimation, and generalization of causal effects. Identification is nonparametric and relies on a local strong ignorability assumption. Estimation proceeds, as in any observational study, under strong ignorability, yet in a neighborhood of the cutoffs of the running variables. We discuss estimation approaches based on matching and weighting, including complementary regression modeling adjustments. We present assumptions for generalization, that is, for identification and estimation of average treatment effects for target populations. We also describe two approaches to select the neighborhood for analysis. We find that grade retention in Chile has a negative impact on future grade retention but is not associated with dropping out of school or committing a juvenile crime.},
  archive      = {J_AOAS},
  author       = {Juan D. Díaz and José R. Zubizarreta},
  doi          = {10.1214/22-AOAS1616},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {67-88},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Complex discontinuity designs using covariates: Impact of school grade retention on later life outcomes in chile},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Control charts for dynamic process monitoring with an
application to air pollution surveillance. <em>AOAS</em>,
<em>17</em>(1), 47–66. (<a
href="https://doi.org/10.1214/22-AOAS1615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution is a major global public health risk factor. Among all air pollutants, PM2.5 is especially harmful. It has been well demonstrated that chronic exposure to PM2.5 can cause many health problems, including asthma, lung cancer and cardiovascular diseases. To tackle problems caused by air pollution, governments have put a huge amount of resources to improve air quality and reduce the impact of air pollution on public health. In this effort it is extremely important to develop an air pollution surveillance system to constantly monitor the air quality over time and to give a signal promptly once the air quality is found to deteriorate so that a timely government intervention can be implemented. To monitor a sequential process, a major statistical tool is the statistical process control (SPC) chart. However, traditional SPC charts are based on the assumptions that process observations at different time points are independent and identically distributed. These assumptions are rarely valid in environmental data because seasonality and serial correlation are common in such data. To overcome this difficulty, we suggest a new control chart in this paper, which can properly accommodate dynamic temporal pattern and serial correlation in a sequential process. Thus, it can be used for effective air pollution surveillance. This method is demonstrated by an application to monitor the daily average PM2.5 levels in Beijing and shown to be effective and reliable in detecting the increase of PM2.5 levels.},
  archive      = {J_AOAS},
  author       = {Xiulin Xie and Peihua Qiu},
  doi          = {10.1214/22-AOAS1615},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {47-66},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Control charts for dynamic process monitoring with an application to air pollution surveillance},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social distancing and COVID-19: Randomization inference for
a structured dose-response relationship. <em>AOAS</em>, <em>17</em>(1),
23–46. (<a href="https://doi.org/10.1214/22-AOAS1613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social distancing is widely acknowledged as an effective public health policy combating the novel coronavirus. But extreme forms of social distancing, like isolation and quarantine, have costs, and it is not clear how much social distancing is needed to achieve public health effects. In this article we develop a design-based framework to test the causal null hypothesis and make inference about the dose-response relationship between reduction in social mobility and COVID-19 related public health outcomes. We first discuss how to embed observational data with a time-independent, continuous treatment dose into an approximate randomized experiment and develop a randomization-based procedure that tests if a structured dose-response relationship fits the data. We then generalize the design and testing procedure to a longitudinal setting and apply them to investigate the effect of social distancing during the first phased reopening in the United States on public health outcomes using data compiled from Unacast™, the United States Census Bureau, and the County Health Rankings and Roadmaps Program. We rejected a primary analysis null hypothesis that stated the social distancing from April 27, 2020 to June 28, 2020, had no effect on the COVID-19-related death toll from June 29, 2020 to August 2, 2020 (p-value &lt; 0.001), and found that it took more reduction in mobility to prevent exponential growth in case numbers for nonrural counties compared to rural counties.},
  archive      = {J_AOAS},
  author       = {Bo Zhang and Siyu Heng and Ting Ye and Dylan S. Small},
  doi          = {10.1214/22-AOAS1613},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {23-46},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Social distancing and COVID-19: Randomization inference for a structured dose-response relationship},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fitting stochastic epidemic models to gene genealogies using
linear noise approximation. <em>AOAS</em>, <em>17</em>(1), 1–22. (<a
href="https://doi.org/10.1214/21-AOAS1583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylodynamics is a set of population genetics tools that aim at reconstructing demographic history of a population based on molecular sequences of individuals sampled from the population of interest. One important task in phylodynamics is to estimate changes in (effective) population size. When applied to infectious disease sequences, such estimation of population size trajectories can provide information about changes in the number of infections. To model changes in the number of infected individuals, current phylodynamic methods use nonparametric approaches (e.g., Bayesian curve-fitting based on change-point models or Gaussian process priors), parametric approaches (e.g., based on differential equations), and stochastic modeling in conjunction with likelihood-free Bayesian methods. The first class of methods yields results that are hard to interpret epidemiologically. The second class of methods provides estimates of important epidemiological parameters, such as infection and removal/recovery rates, but ignores variation in the dynamics of infectious disease spread. The third class of methods is the most advantageous statistically but relies on computationally intensive particle filtering techniques that limits its applications. We propose a Bayesian model that combines phylodynamic inference and stochastic epidemic models and achieves computational tractability by using a linear noise approximation (LNA)—a technique that allows us to approximate probability densities of stochastic epidemic model trajectories. LNA opens the door for using modern Markov chain Monte Carlo tools to approximate the joint posterior distribution of the disease transmission parameters and of high dimensional vectors describing unobserved changes in the stochastic epidemic model compartment sizes (e.g., numbers of infectious and susceptible individuals). In a simulation study we show that our method can successfully recover parameters of stochastic epidemic models. We apply our estimation technique to Ebola genealogies estimated using viral genetic data from the 2014 epidemic in Sierra Leone and Liberia.},
  archive      = {J_AOAS},
  author       = {Mingwei Tang and Gytis Dudas and Trevor Bedford and Vladimir N. Minin},
  doi          = {10.1214/21-AOAS1583},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Fitting stochastic epidemic models to gene genealogies using linear noise approximation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
