<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aos---97">AOS - 97</h2>
<ul>
<li><details>
<summary>
(2023). Nonlinear network autoregression. <em>AOS</em>,
<em>51</em>(6), 2526–2552. (<a
href="https://doi.org/10.1214/23-AOS2345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study general nonlinear models for time series networks of integer and continuous-valued data. The vector of high-dimensional responses, measured on the nodes of a known network, is regressed nonlinearly on its lagged value and on lagged values of the neighboring nodes by employing a smooth link function. We study stability conditions for such multivariate process and develop quasi-maximum likelihood inference when the network dimension is increasing. In addition, we study linearity score tests by treating separately the cases of identifiable and nonidentifiable parameters. In the case of identifiability, the test statistic converges to a chi-square distribution. When the parameters are not identifiable, we develop a supremum-type test whose p-values are approximated adequately by employing a feasible bound and bootstrap methodology. Simulations and data examples support further our findings.},
  archive      = {J_AOS},
  author       = {Mirko Armillotta and Konstantinos Fokianos},
  doi          = {10.1214/23-AOS2345},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2526-2552},
  shortjournal = {Ann. Statist.},
  title        = {Nonlinear network autoregression},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Projective, sparse and learnable latent position network
models. <em>AOS</em>, <em>51</em>(6), 2506–2525. (<a
href="https://doi.org/10.1214/23-AOS2340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When modeling network data using a latent position model, it is typical to assume that the nodes’ positions are independently and identically distributed. However, this assumption implies the average node degree grows linearly with the number of nodes, which is inappropriate when the graph is thought to be sparse. We propose an alternative assumption—that the latent positions are generated according to a Poisson point process—and show that it is compatible with various levels of sparsity. Unlike other notions of sparse latent position models in the literature, our framework also defines a projective sequence of probability models, thus ensuring consistency of statistical inference across networks of different sizes. We establish conditions for consistent estimation of the latent positions, and compare our results to existing frameworks for modeling sparse networks.},
  archive      = {J_AOS},
  author       = {Neil A. Spencer and Cosma Rohilla Shalizi},
  doi          = {10.1214/23-AOS2340},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2506-2525},
  shortjournal = {Ann. Statist.},
  title        = {Projective, sparse and learnable latent position network models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian process regression in the flat limit. <em>AOS</em>,
<em>51</em>(6), 2471–2505. (<a
href="https://doi.org/10.1214/23-AOS2336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) regression is a fundamental tool in Bayesian statistics. It is also known as kriging and is the Bayesian counterpart to the frequentist kernel ridge regression. Most of the theoretical work on GP regression has focused on a large-n asymptotics, characterising the behaviour of GP regression as the amount of data increases. Fixed-sample analysis is much more difficult outside of simple cases, such as locations on a regular grid. In this work, we perform a fixed-sample analysis that was first studied in the context of approximation theory by Fornberg and Driscoll (2002), called the “flat limit”. In flat-limit asymptotics, the goal is to characterise kernel methods as the length-scale of the kernel function tends to infinity, so that kernels appear flat over the range of the data. Surprisingly, this limit is well-defined, and displays interesting behaviour: Driscoll and Fornberg showed that radial basis interpolation converges in the flat limit to polynomial interpolation, if the kernel is Gaussian. Subsequent work showed that this holds true in the multivariate setting as well, but that kernels other than the Gaussian may have (polyharmonic) splines as the limit interpolant. Leveraging recent results on the spectral behaviour of kernel matrices in the flat limit, we study the flat limit of Gaussian process regression. Results show that Gaussian process regression tends in the flat limit to (multivariate) polynomial regression, or (polyharmonic) spline regression, depending on the kernel. Importantly, this holds for both the predictive mean and the predictive variance, so that the posterior predictive distributions become equivalent. For the proof, we introduce the notion of prediction-equivalence of semiparametric models, which lets us state flat-limit results in a compact and unified manner. Our results have practical consequences: for instance, they show that optimal GP predictions in the sense of leave-one-out loss may occur at very large length-scales, which would be invisible to current implementations because of numerical difficulties.},
  archive      = {J_AOS},
  author       = {Simon Barthelmé and Pierre-Olivier Amblard and Nicolas Tremblay and Konstantin Usevich},
  doi          = {10.1214/23-AOS2336},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2471-2505},
  shortjournal = {Ann. Statist.},
  title        = {Gaussian process regression in the flat limit},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient shape-constrained inference for the autocovariance
sequence from a reversible markov chain. <em>AOS</em>, <em>51</em>(6),
2440–2470. (<a href="https://doi.org/10.1214/23-AOS2335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of estimating the autocovariance sequence resulting from a reversible Markov chain. A motivating application for studying this problem is the estimation of the asymptotic variance in central limit theorems for Markov chains. We propose a novel shape-constrained estimator of the autocovariance sequence, which is based on the key observation that the representability of the autocovariance sequence as a moment sequence imposes certain shape constraints. We examine the theoretical properties of the proposed estimator and provide strong consistency guarantees for our estimator. In particular, for geometrically ergodic reversible Markov chains, we show that our estimator is strongly consistent for the true autocovariance sequence with respect to an ℓ2 distance, and that our estimator leads to strongly consistent estimates of the asymptotic variance. Finally, we perform empirical studies to illustrate the theoretical properties of the proposed estimator as well as to demonstrate the effectiveness of our estimator in comparison with other current state-of-the-art methods for Markov chain Monte Carlo variance estimation, including batch means, spectral variance estimators, and the initial convex sequence estimator.},
  archive      = {J_AOS},
  author       = {Stephen Berg and Hyebin Song},
  doi          = {10.1214/23-AOS2335},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2440-2470},
  shortjournal = {Ann. Statist.},
  title        = {Efficient shape-constrained inference for the autocovariance sequence from a reversible markov chain},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). S-estimation in linear models with structured covariance
matrices. <em>AOS</em>, <em>51</em>(6), 2415–2439. (<a
href="https://doi.org/10.1214/23-AOS2334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a unified approach to S-estimation in balanced linear models with structured covariance matrices. Of main interest are S-estimators for linear mixed effects models, but our approach also includes S-estimators in several other standard multivariate models, such as multiple regression, multivariate regression and multivariate location and scatter. We provide sufficient conditions for the existence of S-functionals and S-estimators, establish asymptotic properties such as consistency and asymptotic normality, and derive their robustness properties in terms of breakdown point and influence function. All the results are obtained for general identifiable covariance structures and are established under mild conditions on the distribution of the observations, which goes far beyond models with elliptically contoured densities. Some of our results are new and others are more general than existing ones in the literature. In this way, this manuscript completes and improves results on S-estimation in a wide variety of multivariate models. We illustrate our results by means of a simulation study and an application to data from a trial on the treatment of lead-exposed children.},
  archive      = {J_AOS},
  author       = {Hendrik Paul Lopuhaä and Valerie Gares and Anne Ruiz-Gazen},
  doi          = {10.1214/23-AOS2334},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2415-2439},
  shortjournal = {Ann. Statist.},
  title        = {S-estimation in linear models with structured covariance matrices},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local whittle estimation of high-dimensional long-run
variance and precision matrices. <em>AOS</em>, <em>51</em>(6),
2386–2414. (<a href="https://doi.org/10.1214/23-AOS2330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work develops nonasymptotic theory for estimation of the long-run variance matrix and its inverse, the so-called precision matrix, for high-dimensional time series under general assumptions on the dependence structure including long-range dependence. The estimation involves shrinkage techniques, which are thresholding and penalizing versions of the classical multivariate local Whittle estimator. The results ensure consistent estimation in a double asymptotic regime where the number of component time series is allowed to grow with the sample size as long as the true model parameters are sparse. The key technical result is a concentration inequality of the local Whittle estimator for the long-run variance matrix around the true model parameters. In particular, it handles simultaneously the estimation of the memory parameters, which enter the underlying model. Novel algorithms for the considered procedures are proposed, and a simulation study and a data application are also provided.},
  archive      = {J_AOS},
  author       = {Changryong Baek and Marie-Christine Düker and Vladas Pipiras},
  doi          = {10.1214/23-AOS2330},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2386-2414},
  shortjournal = {Ann. Statist.},
  title        = {Local whittle estimation of high-dimensional long-run variance and precision matrices},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adjusted chi-square test for degree-corrected block models.
<em>AOS</em>, <em>51</em>(6), 2366–2385. (<a
href="https://doi.org/10.1214/23-AOS2329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a goodness-of-fit test for degree-corrected stochastic block models (DCSBM). The test is based on an adjusted chi-square statistic for measuring equality of means among groups of n multinomial distributions with d1,…,dn observations. In the context of network models, the number of multinomials, n, grows much faster than the number of observations, di, corresponding to the degree of node i, hence the setting deviates from classical asymptotics. We show that a simple adjustment allows the statistic to converge in distribution, under null, as long as the harmonic mean of {di} grows to infinity. When applied sequentially, the test can also be used to determine the number of communities. The test operates on a compressed version of the adjacency matrix, conditional on the degrees, and as a result is highly scalable to large sparse networks. We incorporate a novel idea of compressing the rows based on a (K+1)-community assignment when testing for K communities. This approach increases the power in sequential applications without sacrificing computational efficiency, and we prove its consistency in recovering the number of communities. Since the test statistic does not rely on a specific alternative, its utility goes beyond sequential testing and can be used to simultaneously test against a wide range of alternatives outside the DCSBM family. In particular, we prove that the test is consistent against a general family of latent-variable network models with community structure. We show the effectiveness of the approach by extensive numerical experiments with simulated and real data. In particular, applying the test to the Facebook-100 data set, a collection of one hundred social networks, we find that a DCSBM with a small number of communities (say &lt;25) is far from a good fit in almost all cases.},
  archive      = {J_AOS},
  author       = {Linfan Zhang and Arash A. Amini},
  doi          = {10.1214/23-AOS2329},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2366-2385},
  shortjournal = {Ann. Statist.},
  title        = {Adjusted chi-square test for degree-corrected block models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal subgroup selection. <em>AOS</em>, <em>51</em>(6),
2342–2365. (<a href="https://doi.org/10.1214/23-AOS2328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials and other applications, we often see regions of the feature space that appear to exhibit interesting behaviour, but it is unclear whether these observed phenomena are reflected at the population level. Focusing on a regression setting, we consider the subgroup selection challenge of identifying a region of the feature space on which the regression function exceeds a pre-determined threshold. We formulate the problem as one of constrained optimisation, where we seek a low-complexity, data-dependent selection set on which, with a guaranteed probability, the regression function is uniformly at least as large as the threshold; subject to this constraint, we would like the region to contain as much mass under the marginal feature distribution as possible. This leads to a natural notion of regret, and our main contribution is to determine the minimax optimal rate for this regret in both the sample size and the Type I error probability. The rate involves a delicate interplay between parameters that control the smoothness of the regression function, as well as exponents that quantify the extent to which the optimal selection set at the population level can be approximated by families of well-behaved subsets. Finally, we expand the scope of our previous results by illustrating how they may be generalised to a treatment and control setting, where interest lies in the heterogeneous treatment effect.},
  archive      = {J_AOS},
  author       = {Henry W. J. Reeve and Timothy I. Cannings and Richard J. Samworth},
  doi          = {10.1214/23-AOS2328},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2342-2365},
  shortjournal = {Ann. Statist.},
  title        = {Optimal subgroup selection},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Carving model-free inference. <em>AOS</em>, <em>51</em>(6),
2318–2341. (<a href="https://doi.org/10.1214/23-AOS2318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex studies involve many steps. Selecting promising findings based on pilot data is a first step. As more observations are collected, the investigator must decide how to combine the new data with the pilot data to construct valid selective inference. Carving, introduced by Fithian, Sun and Taylor (2014), enables the reuse of pilot data during selective inference and accounts for overoptimism from the selection process. However, currently, carving is only justified for parametric models such as the commonly used Gaussian model. In this paper, we develop the asymptotic theory to substantiate the use of carving beyond Gaussian models. Our results indicate that carving produces valid and tight confidence intervals within a model-free setting, as demonstrated through simulated and real instances.},
  archive      = {J_AOS},
  author       = {Snigdha Panigrahi},
  doi          = {10.1214/23-AOS2318},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2318-2341},
  shortjournal = {Ann. Statist.},
  title        = {Carving model-free inference},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing nonparametric shape restrictions. <em>AOS</em>,
<em>51</em>(6), 2299–2317. (<a
href="https://doi.org/10.1214/23-AOS2311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe and examine a test for a general class of shape constraints, such as signs of derivatives, U-shape, quasi-convexity, log-convexity, among others, in a nonparametric framework using partial sums empirical processes. We show that, after a suitable transformation, its asymptotic distribution is a functional of a Brownian motion index by the c.d.f. of the regressor. As a result, the test is distribution-free and critical values are readily available. However, due to the possible poor approximation of the asymptotic critical values to the finite sample ones, we also describe a valid bootstrap algorithm.},
  archive      = {J_AOS},
  author       = {Tatiana Komarova and Javier Hidalgo},
  doi          = {10.1214/23-AOS2311},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {2299-2317},
  shortjournal = {Ann. Statist.},
  title        = {Testing nonparametric shape restrictions},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of expected euler characteristic curves of
nonstationary smooth random fields. <em>AOS</em>, <em>51</em>(5),
2272–2297. (<a href="https://doi.org/10.1214/23-AOS2337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expected Euler characteristic (EEC) of excursion sets of a smooth Gaussian-related random field over a compact manifold approximates the distribution of its supremum for high thresholds. Viewed as a function of the excursion threshold, the EEC of a Gaussian-related field is expressed by the Gaussian kinematic formula (GKF) as a finite sum of known functions multiplied by the Lipschitz–Killing curvatures (LKCs) of the generating Gaussian field. This paper proposes consistent estimators of the LKCs as linear projections of “pinned” Euler characteristic (EC) curves obtained from realizations of zero-mean, unit variance Gaussian processes. As observed, data seldom is Gaussian and the exact mean and variance is unknown, yet the statistic of interest often satisfies a CLT with a Gaussian limit process; we adapt our LKC estimators to this scenario using a Gaussian multiplier bootstrap approach. This yields consistent estimates of the LKCs of the possibly nonstationary Gaussian limiting field that have low variance and are computationally efficient for complex underlying manifolds. For the EEC of the limiting field, a parametric plug-in estimator is presented, which is more efficient than the nonparametric average of EC curves. The proposed methods are evaluated using simulations of 2D fields, and illustrated on cosmological observations and simulations on the 2-sphere and 3D fMRI volumes.},
  archive      = {J_AOS},
  author       = {Fabian J. E. Telschow and Dan Cheng and Pratyush Pranav and Armin Schwartzman},
  doi          = {10.1214/23-AOS2337},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2272-2297},
  shortjournal = {Ann. Statist.},
  title        = {Estimation of expected euler characteristic curves of nonstationary smooth random fields},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A CLT for the LSS of large-dimensional sample covariance
matrices with diverging spikes. <em>AOS</em>, <em>51</em>(5), 2246–2271.
(<a href="https://doi.org/10.1214/23-AOS2333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish the central limit theorem (CLT) for linear spectral statistics (LSSs) of a large-dimensional sample covariance matrix when the population covariance matrices are involved with diverging spikes. This constitutes a nontrivial extension of the Bai–Silverstein theorem (BST) (Ann. Probab. 32 (2004) 553–605), a theorem that has strongly influenced the development of high-dimensional statistics, especially in the applications of random matrix theory to statistics. Recently, there has been a growing realization that the assumption of uniform boundedness of the population covariance matrices in the BST is not satisfied in some fields, such as economics, where the variances of principal components may diverge as the dimension tends to infinity. Therefore, in this paper, we aim to eliminate this obstacle to applications of the BST. Our new CLT accommodates spiked eigenvalues, which may either be bounded or tend to infinity. A distinguishing feature of our result is that the variance in the new CLT is related to both spiked eigenvalues and bulk eigenvalues, with dominance being determined by the divergence rate of the largest spiked eigenvalues. The new CLT for LSS is then applied to test the hypothesis that the population covariance matrix is the identity matrix or a generalized spiked model. The asymptotic distributions of the corrected likelihood ratio test statistic and the corrected Nagao’s trace test statistic are derived under the alternative hypothesis. Moreover, we present power comparisons between these two LSSs and Roy’s largest root test. In particular, we demonstrate that except for the case in which there is only one spike, the LSSs could exhibit higher asymptotic power than Roy’s largest root test.},
  archive      = {J_AOS},
  author       = {Zhijun Liu and Jiang Hu and Zhidong Bai and Haiyan Song},
  doi          = {10.1214/23-AOS2333},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2246-2271},
  shortjournal = {Ann. Statist.},
  title        = {A CLT for the LSS of large-dimensional sample covariance matrices with diverging spikes},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial quantiles on the hypersphere. <em>AOS</em>,
<em>51</em>(5), 2221–2245. (<a
href="https://doi.org/10.1214/23-AOS2332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a concept of quantiles for probability measures on the unit hypersphere Sd−1 of Rd. The innermost quantile is the Fréchet median, that is, the L1-analog of the Fréchet mean. The proposed quantiles μα,um are directional in nature: they are indexed by a scalar order α∈[0,1] and a unit vector u in the tangent space TmSd−1 to Sd−1 at m. To ensure computability in any dimension d, our quantiles are essentially obtained by considering the Euclidean (Chaudhuri (J. Amer. Statist. Assoc. 91 (1996) 862–872)) spatial quantiles in a suitable stereographic projection of Sd−1 onto TmSd−1. Despite this link with Euclidean spatial quantiles, studying the proposed spherical quantiles requires understanding the nature of the (Chaudhuri (1996)) quantiles in a version of the projective space where all points at infinity are identified. We thoroughly investigate the structural properties of our quantiles and we further study the asymptotic behavior of their sample versions, which requires controlling the impact of estimating m. Our spherical quantile concept also allows for companion concepts of ranks and depth on the hypersphere. We illustrate the relevance of our construction by considering two inferential applications, related to supervised classification and to testing for rotational symmetry.},
  archive      = {J_AOS},
  author       = {Dimitri Konen and Davy Paindaveine},
  doi          = {10.1214/23-AOS2332},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2221-2245},
  shortjournal = {Ann. Statist.},
  title        = {Spatial quantiles on the hypersphere},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The lasso with general gaussian designs with applications to
hypothesis testing. <em>AOS</em>, <em>51</em>(5), 2194–2220. (<a
href="https://doi.org/10.1214/23-AOS2327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lasso is a method for high-dimensional regression, which is now commonly used when the number of covariates p is of the same order or larger than the number of observations n. Classical asymptotic normality theory does not apply to this model due to two fundamental reasons: (1) The regularized risk is nonsmooth; (2) The distance between the estimator θˆ and the true parameters vector θ∗ cannot be neglected. As a consequence, standard perturbative arguments that are the traditional basis for asymptotic normality fail. On the other hand, the Lasso estimator can be precisely characterized in the regime in which both n and p are large and n/p is of order one. This characterization was first obtained in the case of Gaussian designs with i.i.d. covariates: here we generalize it to Gaussian correlated designs with non-singular covariance structure. This is expressed in terms of a simpler “fixed-design” model. We establish nonasymptotic bounds on the distance between the distribution of various quantities in the two models, which hold uniformly over signals θ∗ in a suitable sparsity class and over values of the regularization parameter. As an application, we study the distribution of the debiased Lasso and show that a degrees-of-freedom correction is necessary for computing valid confidence intervals.},
  archive      = {J_AOS},
  author       = {Michael Celentano and Andrea Montanari and Yuting Wei},
  doi          = {10.1214/23-AOS2327},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2194-2220},
  shortjournal = {Ann. Statist.},
  title        = {The lasso with general gaussian designs with applications to hypothesis testing},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Optimal nonparametric testing of missing completely at
random and its connections to compatibility. <em>AOS</em>,
<em>51</em>(5), 2170–2193. (<a
href="https://doi.org/10.1214/23-AOS2326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of incomplete observations, we study the nonparametric problem of testing whether data are Missing Completely At Random (MCAR). Our first contribution is to characterise precisely the set of alternatives that can be distinguished from the MCAR null hypothesis. This reveals interesting and novel links to the theory of Fréchet classes (in particular, compatible distributions) and linear programming, that allow us to propose MCAR tests that are consistent against all detectable alternatives. We define an incompatibility index as a natural measure of ease of detectability, establish its key properties and show how it can be computed exactly in some cases and bounded in others. Moreover, we prove that our tests can attain the minimax separation rate according to this measure, up to logarithmic factors. Our methodology does not require any complete cases to be effective, and is available in the R package MCARtest.},
  archive      = {J_AOS},
  author       = {Thomas B. Berrett and Richard J. Samworth},
  doi          = {10.1214/23-AOS2326},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2170-2193},
  shortjournal = {Ann. Statist.},
  title        = {Optimal nonparametric testing of missing completely at random and its connections to compatibility},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On backward smoothing algorithms. <em>AOS</em>,
<em>51</em>(5), 2145–2169. (<a
href="https://doi.org/10.1214/23-AOS2324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of state-space models, skeleton-based smoothing algorithms rely on a backward sampling step, which by default, has a O(N2) complexity (where N is the number of particles). Existing improvements in the literature are unsatisfactory: a popular rejection sampling-based approach, as we shall show, might lead to badly behaved execution time; another rejection sampler with stopping lacks complexity analysis; yet another MCMC-inspired algorithm comes with no stability guarantee. We provide several results that close these gaps. In particular, we prove a novel nonasymptotic stability theorem, thus enabling smoothing with truly linear complexity and adequate theoretical justification. We propose a general framework, which unites most skeleton-based smoothing algorithms in the literature and allows to simultaneously prove their convergence and stability, both in online and offline contexts. Furthermore, we derive, as a special case of that framework, a new coupling-based smoothing algorithm applicable to models with intractable transition densities. We elaborate practical recommendations and confirm those with numerical experiments.},
  archive      = {J_AOS},
  author       = {Hai-Dang Dau and Nicolas Chopin},
  doi          = {10.1214/23-AOS2324},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2145-2169},
  shortjournal = {Ann. Statist.},
  title        = {On backward smoothing algorithms},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric conditional local independence testing.
<em>AOS</em>, <em>51</em>(5), 2116–2144. (<a
href="https://doi.org/10.1214/23-AOS2323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional local independence is an asymmetric independence relation among continuous time stochastic processes. It describes whether the evolution of one process is directly influenced by another process given the histories of additional processes, and it is important for the description and learning of causal relations among processes. We develop a model-free framework for testing the hypothesis that a counting process is conditionally locally independent of another process. To this end, we introduce a new functional parameter called the Local Covariance Measure (LCM), which quantifies deviations from the hypothesis. Following the principles of double machine learning, we propose an estimator of the LCM and a test of the hypothesis using nonparametric estimators and sample splitting or cross-fitting. We call this test the (cross-fitted) Local Covariance Test ((X)-LCT), and we show that its level and power can be controlled uniformly, provided that the nonparametric estimators are consistent with modest rates. We illustrate the theory by an example based on a marginalized Cox model with time-dependent covariates, and we show in simulations that when double machine learning is used in combination with cross-fitting, then the test works well without restrictive parametric assumptions.},
  archive      = {J_AOS},
  author       = {Alexander Mangulad Christgau and Lasse Petersen and Niels Richard Hansen},
  doi          = {10.1214/23-AOS2323},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2116-2144},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric conditional local independence testing},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust high-dimensional tuning free multiple testing.
<em>AOS</em>, <em>51</em>(5), 2093–2115. (<a
href="https://doi.org/10.1214/23-AOS2322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stylized feature of high-dimensional data is that many variables have heavy tails, and robust statistical inference is critical for valid large-scale statistical inference. Yet, the existing developments such as Winsorization, Huberization and median of means require the bounded second moments and involve variable-dependent tuning parameters, which hamper their fidelity in applications to large-scale problems. To liberate these constraints, this paper revisits the celebrated Hodges–Lehmann (HL) estimator for estimating location parameters in both the one- and two-sample problems, from a nonasymptotic perspective. Our study develops Berry–Esseen inequality and Cramér-type moderate deviation for the HL estimator based on newly developed nonasymptotic Bahadur representation and builds data-driven confidence intervals via a weighted bootstrap approach. These results allow us to extend the HL estimator to large-scale studies and propose tuning-free and moment-free high-dimensional inference procedures for testing global null and for large-scale multiple testing with false discovery proportion control. It is convincingly shown that the resulting tuning-free and moment-free methods control false discovery proportion at a prescribed level. The simulation studies lend further support to our developed theory.},
  archive      = {J_AOS},
  author       = {Jianqing Fan and Zhipeng Lou and Mengxin Yu},
  doi          = {10.1214/23-AOS2322},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2093-2115},
  shortjournal = {Ann. Statist.},
  title        = {Robust high-dimensional tuning free multiple testing},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentially private inference via noisy optimization.
<em>AOS</em>, <em>51</em>(5), 2067–2092. (<a
href="https://doi.org/10.1214/23-AOS2321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general optimization-based framework for computing differentially private M-estimators and a new method for constructing differentially private confidence regions. First, we show that robust statistics can be used in conjunction with noisy gradient descent or noisy Newton methods in order to obtain optimal private estimators with global linear or quadratic convergence, respectively. We establish local and global convergence guarantees, under both local strong convexity and self-concordance, showing that our private estimators converge with high probability to a small neighborhood of the nonprivate M-estimators. Second, we tackle the problem of parametric inference by constructing differentially private estimators of the asymptotic variance of our private M-estimators. This naturally leads to approximate pivotal statistics for constructing confidence regions and conducting hypothesis testing. We demonstrate the effectiveness of a bias correction that leads to enhanced small-sample empirical performance in simulations. We illustrate the benefits of our methods in several numerical examples.},
  archive      = {J_AOS},
  author       = {Marco Avella-Medina and Casey Bradshaw and Po-Ling Loh},
  doi          = {10.1214/23-AOS2321},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2067-2092},
  shortjournal = {Ann. Statist.},
  title        = {Differentially private inference via noisy optimization},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference for extremal regression with dependent
heavy-tailed data. <em>AOS</em>, <em>51</em>(5), 2040–2066. (<a
href="https://doi.org/10.1214/23-AOS2320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric inference on tail conditional quantiles and their least squares analogs, expectiles, remains limited to i.i.d. data. We develop a fully operational inferential theory for extreme conditional quantiles and expectiles in the challenging framework of α-mixing, conditional heavy-tailed data whose tail index may vary with covariate values. This requires a dedicated treatment to deal with data sparsity in the far tail of the response, in addition to handling difficulties inherent to mixing, smoothing and sparsity associated to covariate localization. We prove the pointwise asymptotic normality of our estimators and obtain optimal rates of convergence reminiscent of those found in the i.i.d. regression setting, but which had not been established in the conditional extreme value literature. Our assumptions hold in a wide range of models. We propose full bias and variance reduction procedures, and simple but effective data-based rules for selecting tuning hyperparameters. Our inference strategy is shown to perform well in finite samples and is showcased in applications to stock returns and tornado loss data.},
  archive      = {J_AOS},
  author       = {Abdelaati Daouia and Gilles Stupfler and Antoine Usseglio-Carleve},
  doi          = {10.1214/23-AOS2320},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2040-2066},
  shortjournal = {Ann. Statist.},
  title        = {Inference for extremal regression with dependent heavy-tailed data},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive and robust multi-task learning. <em>AOS</em>,
<em>51</em>(5), 2015–2039. (<a
href="https://doi.org/10.1214/23-AOS2319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the multitask learning problem that aims to simultaneously analyze multiple data sets collected from different sources and learn one model for each of them. We propose a family of adaptive methods that automatically utilize possible similarities among those tasks while carefully handling their differences. We derive sharp statistical guarantees for the methods and prove their robustness against outlier tasks. Numerical experiments on synthetic and real data sets demonstrate the efficacy of our new methods.},
  archive      = {J_AOS},
  author       = {Yaqi Duan and Kaizheng Wang},
  doi          = {10.1214/23-AOS2319},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2015-2039},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive and robust multi-task learning},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assigning topics to documents by successive projections.
<em>AOS</em>, <em>51</em>(5), 1989–2014. (<a
href="https://doi.org/10.1214/23-AOS2316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic models provide a useful tool to organize and understand the structure of large corpora of text documents, in particular, to discover hidden thematic structure. Clustering documents from big unstructured corpora into topics is an important task in various fields, such as image analysis, e-commerce, social networks and population genetics. Since the number of topics is typically substantially smaller than the size of the corpus and of the dictionary, the methods of topic modeling can lead to a dramatic dimension reduction. We study the problem of estimating the topic-document matrix, which gives the topics distribution for each document in a given corpus, that is, we focus on the clustering aspect of the problem. We introduce an algorithm that we call Successive Projection Overlapping Clustering (SPOC) inspired by the successive projection algorithm for separable matrix factorization. This algorithm is simple to implement and computationally fast. We establish upper bounds on the performance of the SPOC algorithm for estimation of the topic-document matrix, as well as near matching minimax lower bounds. We also propose a method that achieves analogous results when the number of topics is unknown and provides an estimate of the number of topics. Our theoretical results are complemented with a numerical study on synthetic and semisynthetic data.},
  archive      = {J_AOS},
  author       = {Olga Klopp and Maxim Panov and Suzanne Sigalla and Alexandre B. Tsybakov},
  doi          = {10.1214/23-AOS2316},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {1989-2014},
  shortjournal = {Ann. Statist.},
  title        = {Assigning topics to documents by successive projections},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient estimation of the maximal association between
multiple predictors and a survival outcome. <em>AOS</em>,
<em>51</em>(5), 1965–1988. (<a
href="https://doi.org/10.1214/23-AOS2313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new approach to post-selection inference for screening high-dimensional predictors of survival outcomes. Post-selection inference for right-censored outcome data has been investigated in the literature, but much remains to be done to make the methods both reliable and computationally-scalable in high dimensions. Machine learning tools are commonly used to provide predictions of survival outcomes, but the estimated effect of a selected predictor suffers from confirmation bias unless the selection is taken into account. The new approach involves the construction of semiparametrically efficient estimators of the linear association between the predictors and the survival outcome, which are used to build a test statistic for detecting the presence of an association between any of the predictors and the outcome. Further, a stabilization technique reminiscent of bagging allows a normal calibration for the resulting test statistic, which enables the construction of confidence intervals for the maximal association between predictors and the outcome and also greatly reduces computational cost. Theoretical results show that this testing procedure is valid even when the number of predictors grows superpolynomially with sample size, and our simulations support this asymptotic guarantee at moderate sample sizes. The new approach is applied to the problem of identifying patterns in viral gene expression associated with the potency of an antiviral drug.},
  archive      = {J_AOS},
  author       = {Tzu-Jung Huang and Alex Luedtke and Ian W. McKeague},
  doi          = {10.1214/23-AOS2313},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {1965-1988},
  shortjournal = {Ann. Statist.},
  title        = {Efficient estimation of the maximal association between multiple predictors and a survival outcome},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of mixed fractional stable processes using
high-frequency data. <em>AOS</em>, <em>51</em>(5), 1946–1964. (<a
href="https://doi.org/10.1214/23-AOS2312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linear fractional stable motion generalizes two prominent classes of stochastic processes, namely stable Lévy processes, and fractional Brownian motion. For this reason, it may be regarded as a basic building block for continuous time models. We study a stylized model consisting of a superposition of independent linear fractional stable motions and our focus is on parameter estimation of the model. Applying an estimating equations approach, we construct estimators for the whole set of parameters and derive their asymptotic normality in a high-frequency regime. The conditions for consistency turn out to be sharp for two prominent special cases: (i) for Lévy processes, that is, for the estimation of the successive Blumenthal–Getoor indices and (ii) for the mixed fractional Brownian motion introduced by Cheridito. In the remaining cases, our results reveal a delicate interplay between the Hurst parameters and the indices of stability. Our asymptotic theory is based on new limit theorems for multiscale moving average processes.},
  archive      = {J_AOS},
  author       = {Fabian Mies and Mark Podolskij},
  doi          = {10.1214/23-AOS2312},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {1946-1964},
  shortjournal = {Ann. Statist.},
  title        = {Estimation of mixed fractional stable processes using high-frequency data},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sharp optimality for high-dimensional covariance testing
under sparse signals. <em>AOS</em>, <em>51</em>(5), 1921–1945. (<a
href="https://doi.org/10.1214/23-AOS2310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers one-sample testing of a high-dimensional covariance matrix by deriving the detection boundary as a function of the signal sparsity and signal strength under the sparse alternative hypotheses. It first shows that the optimal detection boundary for testing sparse means is the minimax detection lower boundary for testing the covariance matrix. A multilevel thresholding test is proposed and is shown to be able to attain the detection lower boundary over a substantial range of the sparsity parameter, implying that the multilevel thresholding test is sharp optimal in the minimax sense over the range. The asymptotic distribution of the multilevel thresholding statistic for covariance matrices is derived under both Gaussian and non-Gaussian distributions by developing a novel U-statistic decomposition in conjunction with the matrix blocking and the coupling techniques to handle the complex dependence among the elements of the sample covariance matrix. The superiority in the detection boundary of the multilevel thresholding test over the existing tests is also demonstrated.},
  archive      = {J_AOS},
  author       = {Song Xi Chen and Yumou Qiu and Shuyi Zhang},
  doi          = {10.1214/23-AOS2310},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {1921-1945},
  shortjournal = {Ann. Statist.},
  title        = {Sharp optimality for high-dimensional covariance testing under sparse signals},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The impacts of unobserved covariates on covariate-adaptive
randomized experiments. <em>AOS</em>, <em>51</em>(5), 1895–1920. (<a
href="https://doi.org/10.1214/23-AOS2308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-adaptive randomization (CAR) is commonly implemented in clinical trials to balance observed covariates. Recent studies have demonstrated the advantages of CAR procedures in balancing covariates and improving the subsequent statistical analysis. Covariate balance is crucial, but it is not a panacea for the valid statistical inferences. If the response to a treatment interacts with some unobserved covariates, the conclusion drawn from a CAR experiment may be affected, and thus, be inconsistent with other evidence. This paper aims to demonstrate the relationships between unobserved covariates and the analysis of treatment and covariate effects in CAR experiments. We first derive the asymptotic properties of the statistical methods based on a linear model framework with interactions between the treatment and an unobserved covariate. We also provide sufficient conditions for the identifiability of the treatment and covariate effects. Our results theoretically explain how inconsistent estimations are generated in CAR experiments when some important covariates are unobserved. Under these sufficient conditions, we show that the tests for the treatment and covariate effects can have reduced Type I errors under CAR procedures. A residual-based adjusted test is proposed to recover the Type I error when the effect can be correctly estimated. Numerical studies are conducted to evaluate the performance of our proposed procedure and theoretical findings.},
  archive      = {J_AOS},
  author       = {Yang Liu and Feifang Hu},
  doi          = {10.1214/23-AOS2308},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {1895-1920},
  shortjournal = {Ann. Statist.},
  title        = {The impacts of unobserved covariates on covariate-adaptive randomized experiments},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Order-of-addition orthogonal arrays to study the effect of
treatment ordering. <em>AOS</em>, <em>51</em>(4), 1877–1894. (<a
href="https://doi.org/10.1214/23-AOS2317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effect of the order in which a set of m treatments is applied can be modeled by relative-position factors that indicate whether treatment i is carried out before or after treatment j, or by the absolute position for treatment i in the sequence. A design with the same normalized information matrix as the design with all m! sequences is D- and G-optimal for the main-effects model involving the relative-position factors. We prove that such designs are also I-optimal for this model and D-optimal as well as G- and I-optimal for the first-order model in the absolute-position factors. We propose a methodology for a complete or partial enumeration of nonequivalent designs that are optimal for both models.},
  archive      = {J_AOS},
  author       = {Eric D. Schoen and Robert W. Mee},
  doi          = {10.1214/23-AOS2317},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1877-1894},
  shortjournal = {Ann. Statist.},
  title        = {Order-of-addition orthogonal arrays to study the effect of treatment ordering},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relaxing the i.i.d. Assumption: Adaptively minimax optimal
regret via root-entropic regularization. <em>AOS</em>, <em>51</em>(4),
1850–1876. (<a href="https://doi.org/10.1214/23-AOS2315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider prediction with expert advice when data are generated from distributions varying arbitrarily within an unknown constraint set. This semi-adversarial setting includes (at the extremes) the classical i.i.d. setting, when the unknown constraint set is restricted to be a singleton, and the unconstrained adversarial setting, when the constraint set is the set of all distributions. The Hedge algorithm—long known to be minimax (rate) optimal in the adversarial regime—was recently shown to be simultaneously minimax optimal for i.i.d. data. In this work, we propose to relax the i.i.d. assumption by seeking adaptivity at all levels of a natural ordering on constraint sets. We provide matching upper and lower bounds on the minimax regret at all levels, show that Hedge with deterministic learning rates is suboptimal outside of the extremes and prove that one can adaptively obtain minimax regret at all levels. We achieve this optimal adaptivity using the follow-the-regularized-leader (FTRL) framework, with a novel adaptive regularization scheme that implicitly scales as the square root of the entropy of the current predictive distribution, rather than the entropy of the initial predictive distribution. Finally, we provide novel technical tools to study the statistical performance of FTRL along the semi-adversarial spectrum.},
  archive      = {J_AOS},
  author       = {Blair Bilodeau and Jeffrey Negrea and Daniel M. Roy},
  doi          = {10.1214/23-AOS2315},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1850-1876},
  shortjournal = {Ann. Statist.},
  title        = {Relaxing the i.i.d. assumption: Adaptively minimax optimal regret via root-entropic regularization},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical inference on a changing extreme value dependence
structure. <em>AOS</em>, <em>51</em>(4), 1824–1849. (<a
href="https://doi.org/10.1214/23-AOS2314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the extreme value dependence of independent, not necessarily identically distributed multivariate regularly varying random vectors. More specifically, we propose estimators of the spectral measure locally at some time point and of the spectral measures integrated over time. The uniform asymptotic normality of these estimators is proved under suitable nonparametric smoothness and regularity assumptions. We then use the process convergence of the integrated spectral measure to devise consistent tests for the null hypothesis that the spectral measure does not change over time.},
  archive      = {J_AOS},
  author       = {Holger Drees},
  doi          = {10.1214/23-AOS2314},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1824-1849},
  shortjournal = {Ann. Statist.},
  title        = {Statistical inference on a changing extreme value dependence structure},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Universality of regularized regression estimators in high
dimensions. <em>AOS</em>, <em>51</em>(4), 1799–1823. (<a
href="https://doi.org/10.1214/23-AOS2309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Convex Gaussian Min–Max Theorem (CGMT) has emerged as a prominent theoretical tool for analyzing the precise stochastic behavior of various statistical estimators in the so-called high-dimensional proportional regime, where the sample size and the signal dimension are of the same order. However, a well-recognized limitation of the existing CGMT machinery rests in its stringent requirement on the exact Gaussianity of the design matrix, therefore rendering the obtained precise high-dimensional asymptotics, largely a specific Gaussian theory in various important statistical models. This paper provides a structural universality framework for a broad class of regularized regression estimators that is particularly compatible with the CGMT machinery. Here, universality means that if a “structure” is satisfied by the regression estimator μˆG for a standard Gaussian design G, then it will also be satisfied by μˆA for a general non-Gaussian design A with independent entries. In particular, we show that with a good enough ℓ∞ bound for the regression estimator μˆA, any “structural property” that can be detected via the CGMT for μˆG also holds for μˆA under a general design A with independent entries. As a proof of concept, we demonstrate our new universality framework in three key examples of regularized regression estimators: the Ridge, Lasso and regularized robust regression estimators, where new universality properties of risk asymptotics and/or distributions of regression estimators and other related quantities are proved. As a major statistical implication of the Lasso universality results, we validate inference procedures using the degrees-of-freedom adjusted debiased Lasso under general design and error distributions. We also provide a counterexample, showing that universality properties for regularized regression estimators do not extend to general isotropic designs. The proof of our universality results relies on new comparison inequalities for the optimum of a broad class of cost functions and Gordon’s max–min (or min–max) costs, over arbitrary structure sets subject to ℓ∞ constraints. These results may be of independent interest and broader applicability.},
  archive      = {J_AOS},
  author       = {Qiyang Han and Yandi Shen},
  doi          = {10.1214/23-AOS2309},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1799-1823},
  shortjournal = {Ann. Statist.},
  title        = {Universality of regularized regression estimators in high dimensions},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single index fréchet regression. <em>AOS</em>,
<em>51</em>(4), 1770–1798. (<a
href="https://doi.org/10.1214/23-AOS2307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single index models provide an effective dimension reduction tool in regression, especially for high-dimensional data, by projecting a general multivariate predictor onto a direction vector. We propose a novel single-index model for regression models where metric space-valued random object responses are coupled with multivariate Euclidean predictors. The responses in this regression model include complex, non-Euclidean data, including covariance matrices, graph Laplacians of networks and univariate probability distribution functions, among other complex objects that lie in abstract metric spaces. While Fréchet regression has proved useful for modeling the conditional mean of such random objects given multivariate Euclidean vectors, it does not provide for regression parameters such as slopes or intercepts, since the metric space-valued responses are not amenable to linear operations. As a consequence, distributional results for Fréchet regression have been elusive. We show here that for the case of multivariate Euclidean predictors, the parameters that define a single index and projection vector can be used to substitute for the inherent absence of parameters in Fréchet regression. Specifically, we derive the asymptotic distribution of suitable estimates of these parameters, which then can be utilized to test linear hypotheses for the parameters, subject to an identifiability condition. Consistent estimation of the link function of the single index Fréchet regression model is obtained through local linear Fréchet regression. We demonstrate the finite sample performance of estimation and inference for the proposed single index Fréchet regression model through simulation studies, including the special cases where responses are probability distributions and graph adjacency matrices. The method is illustrated for resting-state functional Magnetic Resonance Imaging (fMRI) data from the ADNI study.},
  archive      = {J_AOS},
  author       = {Satarupa Bhattacharjee and Hans-Georg Müller},
  doi          = {10.1214/23-AOS2307},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1770-1798},
  shortjournal = {Ann. Statist.},
  title        = {Single index fréchet regression},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning low-dimensional nonlinear structures from
high-dimensional noisy data: An integral operator approach.
<em>AOS</em>, <em>51</em>(4), 1744–1769. (<a
href="https://doi.org/10.1214/23-AOS2306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a kernel-spectral embedding algorithm for learning low-dimensional nonlinear structures from noisy and high-dimensional observations, where the data sets are assumed to be sampled from a nonlinear manifold model and corrupted by high-dimensional noise. The algorithm employs an adaptive bandwidth selection procedure which does not rely on prior knowledge of the underlying manifold. The obtained low-dimensional embeddings can be further utilized for downstream purposes such as data visualization, clustering and prediction. Our method is theoretically justified and practically interpretable. Specifically, for a general class of kernel functions, we establish the convergence of the final embeddings to their noiseless counterparts when the dimension grows polynomially with the size, and characterize the effect of the signal-to-noise ratio on the rate of convergence and phase transition. We also prove the convergence of the embeddings to the eigenfunctions of an integral operator defined by the kernel map of some reproducing kernel Hilbert space capturing the underlying nonlinear structures. Our results hold even when the dimension of the manifold grows with the sample size. Numerical simulations and analysis of real data sets show the superior empirical performance of the proposed method, compared to many existing methods, on learning various nonlinear manifolds in diverse applications.},
  archive      = {J_AOS},
  author       = {Xiucai Ding and Rong Ma},
  doi          = {10.1214/23-AOS2306},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1744-1769},
  shortjournal = {Ann. Statist.},
  title        = {Learning low-dimensional nonlinear structures from high-dimensional noisy data: An integral operator approach},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matching recovery threshold for correlated random graphs.
<em>AOS</em>, <em>51</em>(4), 1718–1743. (<a
href="https://doi.org/10.1214/23-AOS2305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For two correlated graphs which are independently sub-sampled from a common Erdős–Rényi graph G(n,p), we wish to recover their latent vertex matching from the observation of these two graphs without labels. When p=n−α+o(1) for α∈(0,1], we establish a sharp information-theoretic threshold for whether it is possible to correctly match a positive fraction of vertices. Our result sharpens a constant factor in a recent work by Wu, Xu and Yu.},
  archive      = {J_AOS},
  author       = {Jian Ding and Hang Du},
  doi          = {10.1214/23-AOS2305},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1718-1743},
  shortjournal = {Ann. Statist.},
  title        = {Matching recovery threshold for correlated random graphs},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bridging factor and sparse models. <em>AOS</em>,
<em>51</em>(4), 1692–1717. (<a
href="https://doi.org/10.1214/23-AOS2304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor and sparse models are widely used to impose a low-dimensional structure in high-dimensions. However, they are seemingly mutually exclusive. We propose a lifting method that combines the merits of these two models in a supervised learning methodology that allows for efficiently exploring all the information in high-dimensional datasets. The method is based on a flexible model for high-dimensional panel data with observable and/or latent common factors and idiosyncratic components. The model is called the factor-augmented regression model. It includes principal components and sparse regression as specific models, significantly weakens the cross-sectional dependence, and facilitates model selection and interpretability. The method consists of several steps and a novel test for (partial) covariance structure in high dimensions to infer the remaining cross-section dependence at each step. We develop the theory for the model and demonstrate the validity of the multiplier bootstrap for testing a high-dimensional (partial) covariance structure. A simulation study and applications support the theory.},
  archive      = {J_AOS},
  author       = {Jianqing Fan and Ricardo P. Masini and Marcelo C. Medeiros},
  doi          = {10.1214/23-AOS2304},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1692-1717},
  shortjournal = {Ann. Statist.},
  title        = {Bridging factor and sparse models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Post-selection inference via algorithmic stability.
<em>AOS</em>, <em>51</em>(4), 1666–1691. (<a
href="https://doi.org/10.1214/23-AOS2303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the target of statistical inference is chosen in a data-driven manner, the guarantees provided by classical theories vanish. We propose a solution to the problem of inference after selection by building on the framework of algorithmic stability, in particular its branch with origins in the field of differential privacy. Stability is achieved via randomization of selection and it serves as a quantitative measure that is sufficient to obtain nontrivial post-selection corrections for classical confidence intervals. Importantly, the underpinnings of algorithmic stability translate directly into computational efficiency—our method computes simple corrections for selective inference without recourse to Markov chain Monte Carlo sampling.},
  archive      = {J_AOS},
  author       = {Tijana Zrnic and Michael I. Jordan},
  doi          = {10.1214/23-AOS2303},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1666-1691},
  shortjournal = {Ann. Statist.},
  title        = {Post-selection inference via algorithmic stability},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Projected state-action balancing weights for offline
reinforcement learning. <em>AOS</em>, <em>51</em>(4), 1639–1665. (<a
href="https://doi.org/10.1214/23-AOS2302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-policy evaluation is considered a fundamental and challenging problem in reinforcement learning (RL). This paper focuses on value estimation of a target policy based on pre-collected data generated from a possibly different policy, under the framework of infinite-horizon Markov decision processes. Motivated by the recently developed marginal importance sampling method in RL and the covariate balancing idea in causal inference, we propose a novel estimator with approximately projected state-action balancing weights for the policy value estimation. We obtain the convergence rate of these weights, and show that the proposed value estimator is asymptotically normal under technical conditions. In terms of asymptotics, our results scale with both the number of trajectories and the number of decision points at each trajectory. As such, consistency can still be achieved with a limited number of subjects when the number of decision points diverges. In addition, we develop a necessary and sufficient condition for establishing the well-posedness of the operator that relates to the nonparametric Q-function estimation in the off-policy setting, which characterizes the difficulty of Q-function estimation and may be of independent interest. Numerical experiments demonstrate the promising performance of our proposed estimator.},
  archive      = {J_AOS},
  author       = {Jiayi Wang and Zhengling Qi and Raymond K. W. Wong},
  doi          = {10.1214/23-AOS2302},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1639-1665},
  shortjournal = {Ann. Statist.},
  title        = {Projected state-action balancing weights for offline reinforcement learning},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noisy linear inverse problems under convex constraints:
Exact risk asymptotics in high dimensions. <em>AOS</em>, <em>51</em>(4),
1611–1638. (<a href="https://doi.org/10.1214/23-AOS2301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the standard Gaussian linear measurement model Y=Xμ0+ξ∈Rm with a fixed noise level σ&gt;0, we consider the problem of estimating the unknown signal μ0 under a convex constraint μ0∈K, where K is a closed convex set in Rn. We show that the risk of the natural convex constrained least squares estimator (LSE) μˆ(σ) can be characterized exactly in high-dimensional limits, by that of the convex constrained LSE μˆKseq in the corresponding Gaussian sequence model at a different noise level. Formally, we show that ‖μˆ(σ)−μ0‖2/(nrn2)→1in probability, where rn 2&gt;0 solves the fixed-point equation E‖μˆKseq( (rn2+σ2)/(m/n))−μ0‖2=nrn2. This characterization holds (uniformly) for risks rn2 in the maximal regime that ranges from constant order all the way down to essentially the parametric rate, as long as certain necessary nondegeneracy condition is satisfied for μˆ(σ). The precise risk characterization reveals a fundamental difference between noiseless (or low noise limit) and noisy linear inverse problems in terms of the sample complexity for signal recovery. A concrete example is given by the isotonic regression problem: While exact recovery of a general monotone signal requires m≫n1/3 samples in the noiseless setting, consistent signal recovery in the noisy setting requires as few as m≫logn samples. Such a discrepancy occurs when the low and high noise risk behavior of μˆKseq differ significantly. In statistical languages, this occurs when μˆKseq estimates 0 at a faster “adaptation rate” than the slower “worst-case rate” for general signals. Several other examples, including nonnegative least squares and generalized Lasso (in constrained forms), are also worked out to demonstrate the concrete applicability of the theory in problems of different types. The proof relies on a collection of new analytic and probabilistic results concerning estimation error, log likelihood ratio test statistics and degree-of-freedom associated with μˆKseq, regarded as stochastic processes indexed by the noise level. These results are of independent interest in and of themselves.},
  archive      = {J_AOS},
  author       = {Qiyang Han},
  doi          = {10.1214/23-AOS2301},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1611-1638},
  shortjournal = {Ann. Statist.},
  title        = {Noisy linear inverse problems under convex constraints: Exact risk asymptotics in high dimensions},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal change-point detection and localization.
<em>AOS</em>, <em>51</em>(4), 1586–1610. (<a
href="https://doi.org/10.1214/23-AOS2297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a times series Y in Rn, with a piecewise constant mean and independent components, the twin problems of change-point detection and change-point localization, respectively amount to detecting the existence of times where the mean varies and estimating the positions of those change-points. In this work, we tightly characterize optimal rates for both problems and uncover the phase transition phenomenon from a global testing problem to a local estimation problem. Introducing a suitable definition of the energy of a change-point, we first establish in the single change-point setting that the optimal detection threshold is 2loglog(n). When the energy is just above the detection threshold, then the problem of localizing the change-point becomes purely parametric: it only depends on the difference in means and not on the position of the change-point anymore. Interestingly, for most change-point positions, including all those away from the endpoints of the time series, it is possible to detect and localize them at a much smaller energy level. In the multiple change-point setting, we establish the energy detection threshold and show similarly that the optimal localization error of a specific change-point becomes purely parametric. Along the way, tight minimax rates for Hausdorff and l 1 estimation losses of the vector of all change-points positions are also established. Two procedures achieving these optimal rates are introduced. The first one is a least-squares estimator with a new multiscale penalty that favours well spread change-points. The second one is a two-step multiscale post-processing procedure whose computational complexity can be as low as O(nlog(n)). Notably, these two procedures accommodate with the presence of possibly many low-energy and therefore undetectable change-points and are still able to detect and localize high-energy change-points even with the presence of those nuisance parameters.},
  archive      = {J_AOS},
  author       = {Nicolas Verzelen and Magalie Fromont and Matthieu Lerasle and Patricia Reynaud-Bouret},
  doi          = {10.1214/23-AOS2297},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1586-1610},
  shortjournal = {Ann. Statist.},
  title        = {Optimal change-point detection and localization},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Off-policy evaluation in partially observed markov decision
processes under sequential ignorability. <em>AOS</em>, <em>51</em>(4),
1561–1585. (<a href="https://doi.org/10.1214/23-AOS2287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider off-policy evaluation of dynamic treatment rules under sequential ignorability, given an assumption that the underlying system can be modeled as a partially observed Markov decision process (POMDP). We propose an estimator, partial history importance weighting, and show that it can consistently estimate the stationary mean rewards of a target policy, given long enough draws from the behavior policy. We provide an upper bound on its error that decays polynomially in the number of observations (i.e., the number of trajectories times their length) with an exponent that depends on the overlap of the target and behavior policies as well as the mixing time of the underlying system. Furthermore, we show that this rate of convergence is minimax, given only our assumptions on mixing and overlap. Our results establish that off-policy evaluation in POMDPs is strictly harder than off-policy evaluation in (fully observed) Markov decision processes but strictly easier than model-free off-policy evaluation.},
  archive      = {J_AOS},
  author       = {Yuchen Hu and Stefan Wager},
  doi          = {10.1214/23-AOS2287},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1561-1585},
  shortjournal = {Ann. Statist.},
  title        = {Off-policy evaluation in partially observed markov decision processes under sequential ignorability},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cross-validation framework for signal denoising with
applications to trend filtering, dyadic CART and beyond. <em>AOS</em>,
<em>51</em>(4), 1534–1560. (<a
href="https://doi.org/10.1214/23-AOS2283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper formulates a general cross-validation framework for signal denoising. The general framework is then applied to nonparametric regression methods such as trend filtering and dyadic CART. The resulting cross-validated versions are then shown to attain nearly the same rates of convergence as are known for the optimally tuned analogues. There did not exist any previous theoretical analyses of cross-validated versions of trend filtering or dyadic CART. To illustrate the generality of the framework, we also propose and study cross-validated versions of two fundamental estimators; lasso for high-dimensional linear regression and singular value thresholding for matrix estimation. Our general framework is inspired by the ideas in Chatterjee and Jafarov (2015) and is potentially applicable to a wide range of estimation methods which use tuning parameters.},
  archive      = {J_AOS},
  author       = {Anamitra Chaudhuri and Sabyasachi Chatterjee},
  doi          = {10.1214/23-AOS2283},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1534-1560},
  shortjournal = {Ann. Statist.},
  title        = {A cross-validation framework for signal denoising with applications to trend filtering, dyadic CART and beyond},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On lower bounds for the bias-variance trade-off.
<em>AOS</em>, <em>51</em>(4), 1510–1533. (<a
href="https://doi.org/10.1214/23-AOS2279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a common phenomenon that for high-dimensional and nonparametric statistical models, rate-optimal estimators balance squared bias and variance. Although this balancing is widely observed, little is known whether methods exist that could avoid the trade-off between bias and variance. We propose a general strategy to obtain lower bounds on the variance of any estimator with bias smaller than a prespecified bound. This shows to which extent the bias-variance trade-off is unavoidable and allows to quantify the loss of performance for methods that do not obey it. The approach is based on a number of abstract lower bounds for the variance involving the change of expectation with respect to different probability measures as well as information measures such as the Kullback–Leibler or χ2-divergence. Some of these inequalities rely on a new concept of information matrices. In a second part of the article, the abstract lower bounds are applied to several statistical models including the Gaussian white noise model, a boundary estimation problem, the Gaussian sequence model and the high-dimensional linear regression model. For these specific statistical applications, different types of bias-variance trade-offs occur that vary considerably in their strength. For the trade-off between integrated squared bias and integrated variance in the Gaussian white noise model, we propose to combine the general strategy for lower bounds with a reduction technique. This allows us to reduce the original problem to a lower bound on the bias-variance trade-off for estimators with additional symmetry properties in a simpler statistical model. In the Gaussian sequence model, different phase transitions of the bias-variance trade-off occur. Although there is a non-trivial interplay between bias and variance, the rate of the squared bias and the variance do not have to be balanced in order to achieve the minimax estimation rate.},
  archive      = {J_AOS},
  author       = {Alexis Derumigny and Johannes Schmidt-Hieber},
  doi          = {10.1214/23-AOS2279},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1510-1533},
  shortjournal = {Ann. Statist.},
  title        = {On lower bounds for the bias-variance trade-off},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bootstrapping persistent betti numbers and other stabilizing
statistics. <em>AOS</em>, <em>51</em>(4), 1484–1509. (<a
href="https://doi.org/10.1214/23-AOS2277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate multivariate bootstrap procedures for general stabilizing statistics, with specific application to topological data analysis. The work relates to other general results in the area of stabilizing statistics, including central limit theorems for geometric and topological functionals of Poisson and binomial processes in the critical regime, where limit theorems prove difficult to use in practice, motivating the use of a bootstrap approach. A smoothed bootstrap procedure is shown to give consistent estimation in these settings. Specific statistics considered include the persistent Betti numbers of Čech and Vietoris–Rips complexes over point sets in Rd, along with Euler characteristics, and the total edge length of the k-nearest neighbor graph. Special emphasis is given to weakening the necessary conditions needed to establish bootstrap consistency. In particular, the assumption of a continuous underlying density is not required. Numerical studies illustrate the performance of the proposed method.},
  archive      = {J_AOS},
  author       = {Benjamin Roycraft and Johannes Krebs and Wolfgang Polonik},
  doi          = {10.1214/23-AOS2277},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1484-1509},
  shortjournal = {Ann. Statist.},
  title        = {Bootstrapping persistent betti numbers and other stabilizing statistics},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graphical models for nonstationary time series.
<em>AOS</em>, <em>51</em>(4), 1453–1483. (<a
href="https://doi.org/10.1214/22-AOS2205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose NonStGM, a general nonparametric graphical modeling framework, for studying dynamic associations among the components of a nonstationary multivariate time series. It builds on the framework of Gaussian graphical models (GGM) and stationary time series graphical models (StGM) and complements existing works on parametric graphical models based on change point vector autoregressions (VAR). Analogous to StGM, the proposed framework captures conditional noncorrelations (both intertemporal and contemporaneous) in the form of an undirected graph. In addition, to describe the more nuanced nonstationary relationships among the components of the time series, we introduce the new notion of conditional nonstationarity/stationarity and incorporate it within the graph. This can be used to search for small subnetworks that serve as the “source” of nonstationarity in a large system. We explicitly connect conditional noncorrelation and stationarity between and within components of the multivariate time series to zero and Toeplitz embeddings of an infinite-dimensional inverse covariance operator. In the Fourier domain, conditional stationarity and noncorrelation relationships in the inverse covariance operator are encoded with a specific sparsity structure of its integral kernel operator. We show that these sparsity patterns can be recovered from finite-length time series by nodewise regression of discrete Fourier transforms (DFT) across different Fourier frequencies. We demonstrate the feasibility of learning NonStGM structure from data using simulation studies.},
  archive      = {J_AOS},
  author       = {Sumanta Basu and Suhasini Subba Rao},
  doi          = {10.1214/22-AOS2205},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1453-1483},
  shortjournal = {Ann. Statist.},
  title        = {Graphical models for nonstationary time series},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymptotic normality for eigenvalue statistics of a general
sample covariance matrix when p/n→∞ and applications. <em>AOS</em>,
<em>51</em>(3), 1427–1451. (<a
href="https://doi.org/10.1214/23-AOS2300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The asymptotic normality for a large family of eigenvalue statistics of a general sample covariance matrix is derived under the ultrahigh-dimensional setting, that is, when the dimension to sample size ratio p/n→∞. Based on this CLT result, we extend the covariance matrix test problem to the new ultra-high-dimensional context, and apply it to test a matrix-valued white noise. Simulation experiments are conducted for the investigation of finite-sample properties of the general asymptotic normality of eigenvalue statistics, as well as the two developed tests.},
  archive      = {J_AOS},
  author       = {Jiaxin Qiu and Zeng Li and Jianfeng Yao},
  doi          = {10.1214/23-AOS2300},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1427-1451},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic normality for eigenvalue statistics of a general sample covariance matrix when p/n→∞ and applications},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Universal regression with adversarial responses.
<em>AOS</em>, <em>51</em>(3), 1401–1426. (<a
href="https://doi.org/10.1214/23-AOS2299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide algorithms for regression with adversarial responses under large classes of non-i.i.d. instance sequences, on general separable metric spaces, with provably minimal assumptions. We also give characterizations of learnability in this regression context. We consider universal consistency, which asks for strong consistency of a learner without restrictions on the value responses. Our analysis shows that such an objective is achievable for a significantly larger class of instance sequences than stationary processes, and unveils a fundamental dichotomy between value spaces: whether finite-horizon mean estimation is achievable or not. We further provide optimistically universal learning rules, that is, such that if they fail to achieve universal consistency, any other algorithms will fail as well. For unbounded losses, we propose a mild integrability condition under which there exist algorithms for adversarial regression under large classes of non-i.i.d. instance sequences. In addition, our analysis also provides a learning rule for mean estimation in general metric spaces that is consistent under adversarial responses without any moment conditions on the sequence, a result of independent interest.},
  archive      = {J_AOS},
  author       = {Moïse Blanchard and Patrick Jaillet},
  doi          = {10.1214/23-AOS2299},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1401-1426},
  shortjournal = {Ann. Statist.},
  title        = {Universal regression with adversarial responses},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coverage of credible intervals in bayesian multivariate
isotonic regression. <em>AOS</em>, <em>51</em>(3), 1376–1400. (<a
href="https://doi.org/10.1214/23-AOS2298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the nonparametric multivariate isotonic regression problem, where the regression function is assumed to be nondecreasing with respect to each predictor. Our goal is to construct a Bayesian credible interval for the function value at a given interior point with assured limiting frequentist coverage. A natural prior on the regression function is given by a random step function with a suitable prior on increasing step-heights, but the resulting posterior distribution is hard to analyze theoretically due to the complicated order restriction on the coefficients. We instead put a prior on unrestricted step-functions, but make inference using the induced posterior measure by an “immersion map” from the space of unrestricted functions to that of multivariate monotone functions. This allows for maintaining the natural conjugacy for posterior sampling. A natural immersion map to use is a projection with respect to a distance function, but in the present context, a block isotonization map is found to be more useful. The approach of using the induced “immersion posterior” measure instead of the original posterior to make inference provides a useful extension of the Bayesian paradigm, particularly helpful when the model space is restricted by some complex relations. We establish a key weak convergence result for the posterior distribution of the function at a point in terms of some functional of a multiindexed Gaussian process that leads to an expression for the limiting coverage of the Bayesian credible interval. Analogous to a recent result for univariate monotone functions, we find that the limiting coverage is slightly higher than the credibility, the opposite of a phenomenon observed in smoothing problems. Interestingly, the relation between credibility and limiting coverage does not involve any unknown parameter. Hence, by a recalibration procedure, we can get a predetermined asymptotic coverage by choosing a suitable credibility level smaller than the targeted coverage, and thus also shorten the credible intervals.},
  archive      = {J_AOS},
  author       = {Kang Wang and Subhashis Ghosal},
  doi          = {10.1214/23-AOS2298},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1376-1400},
  shortjournal = {Ann. Statist.},
  title        = {Coverage of credible intervals in bayesian multivariate isotonic regression},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finite-sample complexity of sequential monte carlo
estimators. <em>AOS</em>, <em>51</em>(3), 1357–1375. (<a
href="https://doi.org/10.1214/23-AOS2295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present bounds for the finite-sample error of sequential Monte Carlo samplers on static spaces. Our approach explicitly relates the performance of the algorithm to properties of the chosen sequence of distributions and mixing properties of the associated Markov kernels. This allows us to give the first finite-sample comparison to other Monte Carlo schemes. We obtain bounds for the complexity of sequential Monte Carlo approximations for a variety of target distributions such as finite spaces, product measures and log-concave distributions including Bayesian logistic regression. The bounds obtained are within a logarithmic factor of similar bounds obtainable for Markov chain Monte Carlo.},
  archive      = {J_AOS},
  author       = {Joe Marion and Joseph Mathews and Scott C. Schmidler},
  doi          = {10.1214/23-AOS2295},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1357-1375},
  shortjournal = {Ann. Statist.},
  title        = {Finite-sample complexity of sequential monte carlo estimators},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extreme value inference for heterogeneous power law data.
<em>AOS</em>, <em>51</em>(3), 1331–1356. (<a
href="https://doi.org/10.1214/23-AOS2294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend extreme value statistics to independent data with possibly very different distributions. In particular, we present novel asymptotic normality results for the Hill estimator, which now estimates the extreme value index of the average distribution. Due to the heterogeneity, the asymptotic variance can be substantially smaller than that in the i.i.d. case. As a special case, we consider a heterogeneous scales model where the asymptotic variance can be calculated explicitly. The primary tool for the proofs is the functional central limit theorem for a weighted tail empirical process. We also present asymptotic normality results for the extreme quantile estimator. A simulation study shows the good finite-sample behavior of our limit theorems. We also present applications to assess the tail heaviness of earthquake energies and of cross-sectional stock market losses.},
  archive      = {J_AOS},
  author       = {John H.J. Einmahl and Yi He},
  doi          = {10.1214/23-AOS2294},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1331-1356},
  shortjournal = {Ann. Statist.},
  title        = {Extreme value inference for heterogeneous power law data},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference for low-rank models. <em>AOS</em>, <em>51</em>(3),
1309–1330. (<a href="https://doi.org/10.1214/23-AOS2293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies inference in linear models with a high-dimensional parameter matrix that can be well approximated by a “spiked low-rank matrix.” A spiked low-rank matrix has rank that grows slowly compared to its dimensions and nonzero singular values that diverge to infinity. We show that this framework covers a broad class of models of latent variables, which can accommodate matrix completion problems, factor models, varying coefficient models and heterogeneous treatment effects. For inference, we apply a procedure that relies on an initial nuclear-norm penalized estimation step followed by two ordinary least squares regressions. We consider the framework of estimating incoherent eigenvectors and use a rotation argument to argue that the eigenspace estimation is asymptotically unbiased. Using this framework, we show that our procedure provides asymptotically normal inference and achieves the semiparametric efficiency bound. We illustrate our framework by providing low-level conditions for its application in a treatment effects context where treatment assignment might be strongly dependent.},
  archive      = {J_AOS},
  author       = {Victor Chernozhukov and Christian Hansen and Yuan Liao and Yinchu Zhu},
  doi          = {10.1214/23-AOS2293},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1309-1330},
  shortjournal = {Ann. Statist.},
  title        = {Inference for low-rank models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimax rate of distribution estimation on unknown
submanifolds under adversarial losses. <em>AOS</em>, <em>51</em>(3),
1282–1308. (<a href="https://doi.org/10.1214/23-AOS2291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inference from high-dimensional data with low-dimensional structures has recently attracted a lot of attention. In machine learning, deep generative modelling approaches implicitly estimate distributions of complex objects by creating new samples from the underlying distribution, and have achieved great success in generating synthetic realistic-looking images and texts. A key step in these approaches is the extraction of latent features or representations (encoding) that can be used for accurately reconstructing the original data (decoding). In other words, low-dimensional manifold structure is implicitly assumed and utilized in the distribution modelling and estimation. To understand the benefit of low-dimensional manifold structure in generative modelling, we build a general minimax framework for distribution estimation on unknown submanifold under adversarial losses, with suitable smoothness assumptions on the target distribution and the manifold. The established minimax rate elucidates how various problem characteristics, including intrinsic dimensionality of the data and smoothness levels of the target distribution and the manifold, affect the fundamental limit of high-dimensional distribution estimation. To prove the minimax upper bound, we construct an estimator based on a mixture of locally fitted generative models, which is motivated by the partition of unity technique from differential geometry and is necessary to cover cases where the underlying data manifold does not admit a global parametrization. We also propose a data-driven adaptive estimator that is shown to simultaneously attain within a logarithmic factor of the optimal rate over a large collection of distribution classes.},
  archive      = {J_AOS},
  author       = {Rong Tang and Yun Yang},
  doi          = {10.1214/23-AOS2291},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1282-1308},
  shortjournal = {Ann. Statist.},
  title        = {Minimax rate of distribution estimation on unknown submanifolds under adversarial losses},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dispersal density estimation across scales. <em>AOS</em>,
<em>51</em>(3), 1258–1281. (<a
href="https://doi.org/10.1214/23-AOS2290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a space structured population model generated by two-point clouds: a homogeneous Poisson process M with intensity n→∞ as a model for a parent generation together with a Cox point process N as offspring generation, with conditional intensity given by the convolution of M with a scaled dispersal density σ−1f(·/σ). Based on a realisation of M and N, we study the nonparametric estimation of f and the estimation of the physical scale parameter σ&gt;0 simultaneously for all regimes σ=σn. We establish that the optimal rates of convergence do not depend monotonously on the scale and we construct minimax estimators accordingly whether σ is known or considered as a nuisance, in which case we can estimate it and achieve asymptotic minimaxity by plug-in. The statistical reconstruction exhibits a competition between a direct and a deconvolution problem. Our study reveals in particular the existence of a least favorable intermediate inference scale, a phenomenon that seems to be new.},
  archive      = {J_AOS},
  author       = {Marc Hoffmann and Mathias Trabs},
  doi          = {10.1214/23-AOS2290},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1258-1281},
  shortjournal = {Ann. Statist.},
  title        = {Dispersal density estimation across scales},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal discriminant analysis in high-dimensional latent
factor models. <em>AOS</em>, <em>51</em>(3), 1232–1257. (<a
href="https://doi.org/10.1214/23-AOS2289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional classification problems, a commonly used approach is to first project the high-dimensional features into a lower-dimensional space, and base the classification on the resulting lower-dimensional projections. In this paper, we formulate a latent-variable model with a hidden low-dimensional structure to justify this two-step procedure and to guide which projection to choose. We propose a computationally efficient classifier that takes certain principal components (PCs) of the observed features as projections, with the number of retained PCs selected in a data-driven way. A general theory is established for analyzing such two-step classifiers based on any projections. We derive explicit rates of convergence of the excess risk of the proposed PC-based classifier. The obtained rates are further shown to be optimal up to logarithmic factors in the minimax sense. Our theory allows the lower dimension to grow with the sample size and is also valid even when the feature dimension (greatly) exceeds the sample size. Extensive simulations corroborate our theoretical findings. The proposed method also performs favorably relative to other existing discriminant methods on three real data examples.},
  archive      = {J_AOS},
  author       = {Xin Bing and Marten Wegkamp},
  doi          = {10.1214/23-AOS2289},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1232-1257},
  shortjournal = {Ann. Statist.},
  title        = {Optimal discriminant analysis in high-dimensional latent factor models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoRegressive approximations to nonstationary time series
with inference and applications. <em>AOS</em>, <em>51</em>(3),
1207–1231. (<a href="https://doi.org/10.1214/23-AOS2288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the time-varying structure of complex temporal systems is one of the main challenges of modern time-series analysis. In this paper, we show that every uniformly-positive-definite-in-covariance and sufficiently short-range dependent nonstationary and nonlinear time series can be well approximated globally by a white-noise-driven autoregressive (AR) process of slowly diverging order. To our best knowledge, it is the first time such a structural approximation result is established for general classes of nonstationary time series. A high-dimensional L2 test and an associated multiplier bootstrap procedure are proposed for the inference of the AR approximation coefficients. In particular, an adaptive stability test is proposed to check whether the AR approximation coefficients are time-varying, a frequently encountered question for practitioners and researchers of time series. As an application, globally optimal sffollowing hort-term forecasting theory and methodology for a wide class of locally stationary time series are established via the method of sieves.},
  archive      = {J_AOS},
  author       = {Xiucai Ding and Zhou Zhou},
  doi          = {10.1214/23-AOS2288},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1207-1231},
  shortjournal = {Ann. Statist.},
  title        = {AutoRegressive approximations to nonstationary time series with inference and applications},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference in ising models on dense regular graphs.
<em>AOS</em>, <em>51</em>(3), 1183–1206. (<a
href="https://doi.org/10.1214/23-AOS2286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we derive the limit of experiments for one-parameter Ising models on dense regular graphs. In particular, we show that the limiting experiment is Gaussian in the “low temperature” regime, and non-Gaussian in the “critical” regime. We also derive the limiting distributions of the maximum likelihood and maximum pseudolikelihood estimators, and study limiting power for tests of hypothesis against contiguous alternatives. To the best of our knowledge, this is the first attempt at establishing the classical limits of experiments for Ising models (and more generally, Markov random fields).},
  archive      = {J_AOS},
  author       = {Yuanzhe Xu and Sumit Mukherjee},
  doi          = {10.1214/23-AOS2286},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1183-1206},
  shortjournal = {Ann. Statist.},
  title        = {Inference in ising models on dense regular graphs},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficiency of estimators for locally asymptotically normal
quantum statistical models. <em>AOS</em>, <em>51</em>(3), 1159–1182. (<a
href="https://doi.org/10.1214/23-AOS2285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We herein establish an asymptotic representation theorem for locally asymptotically normal quantum statistical models. This theorem enables us to study the asymptotic efficiency of quantum estimators, such as quantum regular estimators and quantum minimax estimators, leading to a universal tight lower bound beyond the i.i.d. assumption. This formulation complements the theory of quantum contiguity developed in the previous paper [Fujiwara and Yamagata, Bernoulli 26 (2020) 2105–2141], providing a solid foundation of the theory of weak quantum local asymptotic normality.},
  archive      = {J_AOS},
  author       = {Akio Fujiwara and Koichi Yamagata},
  doi          = {10.1214/23-AOS2285},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1159-1182},
  shortjournal = {Ann. Statist.},
  title        = {Efficiency of estimators for locally asymptotically normal quantum statistical models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pairwise interaction function estimation of stationary gibbs
point processes using basis expansion. <em>AOS</em>, <em>51</em>(3),
1134–1158. (<a href="https://doi.org/10.1214/23-AOS2284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class of Gibbs point processes (GPP) is a large class of spatial point processes able to model both clustered and repulsive point patterns. They are specified by their conditional intensity, which for a point pattern x and a location u, is roughly speaking the probability that an event occurs in an infinitesimal ball around u given the rest of the configuration is x. The most simple and natural class of models is the class of pairwise interaction point processes where the conditional intensity depends on the number of points and pairwise distances between them. This paper is concerned with the problem of estimating the pairwise interaction function nonparametrically. We propose to estimate it using an orthogonal series expansion of its logarithm. Such an approach has numerous advantages compared to existing ones. The estimation procedure is simple, fast and completely data-driven. We provide asymptotic properties such as consistency and asymptotic normality and show the efficiency of the procedure through simulation experiments and illustrate it with several data sets.},
  archive      = {J_AOS},
  author       = {Ismaïla Ba and Jean-François Coeurjolly and Francisco Cuevas-Pacheco},
  doi          = {10.1214/23-AOS2284},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1134-1158},
  shortjournal = {Ann. Statist.},
  title        = {Pairwise interaction function estimation of stationary gibbs point processes using basis expansion},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Universal rank inference via residual subsampling with
application to large networks. <em>AOS</em>, <em>51</em>(3), 1109–1133.
(<a href="https://doi.org/10.1214/23-AOS2282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the precise rank is an important problem in many large-scale applications with matrix data exploiting low-rank plus noise models. In this paper, we suggest a universal approach to rank inference via residual subsampling (RIRS) for testing and estimating rank in a wide family of models, including many popularly used network models such as the degree corrected mixed membership model as a special case. Our procedure constructs a test statistic via subsampling entries of the residual matrix after extracting the spiked components. The test statistic converges in distribution to the standard normal under the null hypothesis, and diverges to infinity with asymptotic probability one under the alternative hypothesis. The effectiveness of RIRS procedure is justified theoretically, utilizing the asymptotic expansions of eigenvectors and eigenvalues for large random matrices recently developed in (J. Amer. Statist. Assoc. 117 (2022) 996–1009) and (J. R. Stat. Soc. Ser. B. Stat. Methodol. 84 (2022) 630–653). The advantages of the newly suggested procedure are demonstrated through several simulation and real data examples.},
  archive      = {J_AOS},
  author       = {Xiao Han and Qing Yang and Yingying Fan},
  doi          = {10.1214/23-AOS2282},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1109-1133},
  shortjournal = {Ann. Statist.},
  title        = {Universal rank inference via residual subsampling with application to large networks},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal reach estimation and metric learning. <em>AOS</em>,
<em>51</em>(3), 1086–1108. (<a
href="https://doi.org/10.1214/23-AOS2281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the estimation of the reach, an ubiquitous regularity parameter in manifold estimation and geometric data analysis. Given an i.i.d. sample over an unknown d-dimensional Ck-smooth submanifold M of RD, we provide optimal nonasymptotic bounds for the estimation of its reach. We build upon a formulation of the reach in terms of maximal curvature on one hand and geodesic metric distortion on the other. The derived rates are adaptive, with rates depending on whether the reach of M arises from curvature or from a bottleneck structure. In the process we derive optimal geodesic metric estimation bounds.},
  archive      = {J_AOS},
  author       = {Eddie Aamari and Clément Berenfeld and Clément Levrard},
  doi          = {10.1214/23-AOS2281},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1086-1108},
  shortjournal = {Ann. Statist.},
  title        = {Optimal reach estimation and metric learning},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complexity analysis of bayesian learning of high-dimensional
DAG models and their equivalence classes. <em>AOS</em>, <em>51</em>(3),
1058–1085. (<a href="https://doi.org/10.1214/23-AOS2280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structure learning via MCMC sampling is known to be very challenging because of the enormous search space and the existence of Markov equivalent DAGs. Theoretical results on the mixing behavior are lacking. In this work, we prove the rapid mixing of a random walk Metropolis–Hastings algorithm, which reveals that the complexity of Bayesian learning of sparse equivalence classes grows only polynomially in n and p, under some high-dimensional assumptions. A series of high-dimensional consistency results is obtained, including the strong selection consistency of an empirical Bayes model for structure learning. Our proof is based on two new results. First, we derive a general mixing time bound on finite-state spaces, which can be applied to local MCMC schemes for other model selection problems. Second, we construct high-probability search paths on the space of equivalence classes with node degree constraints by proving a combinatorial property of DAG comparisons. Simulation studies on the proposed MCMC sampler are conducted to illustrate the main theoretical findings.},
  archive      = {J_AOS},
  author       = {Quan Zhou and Hyunwoong Chang},
  doi          = {10.1214/23-AOS2280},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1058-1085},
  shortjournal = {Ann. Statist.},
  title        = {Complexity analysis of bayesian learning of high-dimensional DAG models and their equivalence classes},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general characterization of optimal tie-breaker designs.
<em>AOS</em>, <em>51</em>(3), 1030–1057. (<a
href="https://doi.org/10.1214/23-AOS2275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tie-breaker designs trade off a measure of statistical efficiency against a short-term gain from preferentially assigning a binary treatment to subjects with higher values of a running variable x. The efficiency measure can be any continuous function of the expected information matrix in a two-line regression model. The short-term gain is expressed as the covariance between the running variable and the treatment indicator. We investigate how to choose design functions p(x) specifying the probability of treating a subject with running variable x in order to optimize these competing objectives, under external constraints on the number of subjects receiving treatment. Our results include sharp existence and uniqueness guarantees, while accommodating the ethically appealing requirement that p(x) be nondecreasing in x. Under this condition, there is always an optimal treatment probability function p(x) that is constant on the sets (−∞,t) and (t,∞) for some threshold t and generally discontinuous at x=t. When the running variable distribution is not symmetric or the fraction of subjects receiving the treatment is not 1/2, our optimal designs improve upon a D-optimality objective without sacrificing short-term gain, compared to a typical three-level tie-breaker design that fixes treatment probabilities at 0, 1/2 and 1. We illustrate our optimal designs with data from Head Start, an early childhood government intervention program.},
  archive      = {J_AOS},
  author       = {Harrison H. Li and Art B. Owen},
  doi          = {10.1214/23-AOS2275},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1030-1057},
  shortjournal = {Ann. Statist.},
  title        = {A general characterization of optimal tie-breaker designs},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A power analysis for model-x knockoffs with ℓp-regularized
statistics. <em>AOS</em>, <em>51</em>(3), 1005–1029. (<a
href="https://doi.org/10.1214/23-AOS2274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection properties of procedures utilizing penalized-likelihood estimates is a central topic in the study of high-dimensional linear regression problems. Existing literature emphasizes the quality of ranking of the variables by such procedures as reflected in the receiver operating characteristic curve or in prediction performance. Specifically, recent works have harnessed modern theory of approximate message-passing (AMP) to obtain, in a particular setting, exact asymptotic predictions of the type I, type II error tradeoff for selection procedures that rely on ℓp-regularized estimators. In practice, effective ranking by itself is often not sufficient because some calibration for Type I error is required. In this work, we study theoretically the power of selection procedures that similarly rank the features by the size of an ℓp-regularized estimator, but further use Model-X knockoffs to control the false discovery rate in the realistic situation where no prior information about the signal is available. In analyzing the power of the resulting procedure, we extend existing results in AMP theory to handle the pairing between original variables and their knockoffs. This is used to derive exact asymptotic predictions for power. We apply the general results to compare the power of the knockoffs versions of Lasso and thresholded-Lasso selection, and demonstrate that in the i.i.d. covariate setting under consideration, tuning by cross-validation on the augmented design matrix is nearly optimal. We further demonstrate how the techniques allow to analyze also the Type S error, and a corresponding notion of power, when selections are supplemented with a decision on the sign of the coefficient.},
  archive      = {J_AOS},
  author       = {Asaf Weinstein and Weijie J. Su and Małgorzata Bogdan and Rina Foygel Barber and Emmanuel J. Candès},
  doi          = {10.1214/23-AOS2274},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1005-1029},
  shortjournal = {Ann. Statist.},
  title        = {A power analysis for model-X knockoffs with ℓp-regularized statistics},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Total positivity in multivariate extremes. <em>AOS</em>,
<em>51</em>(3), 962–1004. (<a
href="https://doi.org/10.1214/23-AOS2272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive dependence is present in many real world data sets and has appealing stochastic properties that can be exploited in statistical modeling and in estimation. In particular, the notion of multivariate total positivity of order 2 (MTP2) is a convex constraint and acts as an implicit regularizer in the Gaussian case. We study positive dependence in multivariate extremes and introduce EMTP2, an extremal version of MTP2. This notion turns out to appear prominently in extremes, and in fact, it is satisfied by many classical models. For a Hüsler–Reiss distribution, the analogue of a Gaussian distribution in extremes, we show that it is EMTP2 if and only if its precision matrix is a Laplacian of a connected graph. We propose an estimator for the parameters of the Hüsler–Reiss distribution under EMTP2 as the solution of a convex optimization problem with Laplacian constraint. We prove that this estimator is consistent and typically yields a sparse model with possibly nondecomposable extremal graphical structure. Applying our methods to a data set of Danube River flows, we illustrate this regularization and the superior performance compared to existing methods.},
  archive      = {J_AOS},
  author       = {Frank Röttger and Sebastian Engelke and Piotr Zwiernik},
  doi          = {10.1214/23-AOS2272},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {962-1004},
  shortjournal = {Ann. Statist.},
  title        = {Total positivity in multivariate extremes},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal permutation estimation in CrowdSourcing problems.
<em>AOS</em>, <em>51</em>(3), 935–961. (<a
href="https://doi.org/10.1214/23-AOS2271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by crowdsourcing applications, we consider a model where we have partial observations from a bivariate isotonic n×d matrix with an unknown permutation π∗ acting on its rows. Focusing on the twin problems of recovering the permutation π∗ and estimating the unknown matrix, we introduce a polynomial-time procedure achieving the minimax risk for these two problems, this for all possible values of n, d, and all possible sampling efforts. Along the way we establish that, in some regimes, recovering the unknown permutation π∗ is considerably simpler than estimating the matrix.},
  archive      = {J_AOS},
  author       = {Emmanuel Pilliat and Alexandra Carpentier and Nicolas Verzelen},
  doi          = {10.1214/23-AOS2271},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {935-961},
  shortjournal = {Ann. Statist.},
  title        = {Optimal permutation estimation in CrowdSourcing problems},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal high-dimensional and nonparametric distributed
testing under communication constraints. <em>AOS</em>, <em>51</em>(3),
909–934. (<a href="https://doi.org/10.1214/23-AOS2269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive minimax testing errors in a distributed framework where the data is split over multiple machines and their communication to a central machine is limited to b bits. We investigate both the d- and infinite-dimensional signal detection problem under Gaussian white noise. We also derive distributed testing algorithms reaching the theoretical lower bounds. Our results show that distributed testing is subject to fundamentally different phenomena that are not observed in distributed estimation. Among our findings we show that testing protocols that have access to shared randomness can perform strictly better in some regimes than those that do not. We also observe that consistent nonparametric distributed testing is always possible, even with as little as one bit of communication, and the corresponding test outperforms the best local test using only the information available at a single local machine. Furthermore, we also derive adaptive nonparametric distributed testing strategies and the corresponding theoretical lower bounds.},
  archive      = {J_AOS},
  author       = {Botond Szabó and Lasse Vuursteen and Harry van Zanten},
  doi          = {10.1214/23-AOS2269},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {909-934},
  shortjournal = {Ann. Statist.},
  title        = {Optimal high-dimensional and nonparametric distributed testing under communication constraints},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Orthogonal statistical learning. <em>AOS</em>,
<em>51</em>(3), 879–908. (<a
href="https://doi.org/10.1214/23-AOS2258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide nonasymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker assumptions than in previous works and accommodate settings in which the target parameter belongs to a complex nonparametric class. We provide conditions on the metric entropy of the nuisance and target classes such that oracle rates of the same order, as if we knew the nuisance parameter, are achieved.},
  archive      = {J_AOS},
  author       = {Dylan J. Foster and Vasilis Syrgkanis},
  doi          = {10.1214/23-AOS2258},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {879-908},
  shortjournal = {Ann. Statist.},
  title        = {Orthogonal statistical learning},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rate-optimal robust estimation of high-dimensional vector
autoregressive models. <em>AOS</em>, <em>51</em>(2), 846–877. (<a
href="https://doi.org/10.1214/23-AOS2278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional time series data appear in many scientific areas in the current data-rich environment. Analysis of such data poses new challenges to data analysts because of not only the complicated dynamic dependence between the series, but also the existence of aberrant observations, such as missing values, contaminated observations, and heavy-tailed distributions. For high-dimensional vector autoregressive (VAR) models, we introduce a unified estimation procedure that is robust to model misspecification, heavy-tailed noise contamination, and conditional heteroscedasticity. The proposed methodology enjoys both statistical optimality and computational efficiency, and can handle many popular high-dimensional models, such as sparse, reduced-rank, banded, and network-structured VAR models. With proper regularization and data truncation, the estimation convergence rates are shown to be almost optimal in the minimax sense under a bounded (2+2ϵ)th moment condition. When ϵ≥1, the rates of convergence match those obtained under the sub-Gaussian assumption. Consistency of the proposed estimators is also established for some ϵ∈(0,1), with minimax optimal convergence rates associated with ϵ. The efficacy of the proposed estimation methods is demonstrated by simulation and a U.S. macroeconomic example.},
  archive      = {J_AOS},
  author       = {Di Wang and Ruey S. Tsay},
  doi          = {10.1214/23-AOS2278},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {846-877},
  shortjournal = {Ann. Statist.},
  title        = {Rate-optimal robust estimation of high-dimensional vector autoregressive models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conformal prediction beyond exchangeability. <em>AOS</em>,
<em>51</em>(2), 816–845. (<a
href="https://doi.org/10.1214/23-AOS2276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction is a popular, modern technique for providing valid predictive inference for arbitrary machine learning models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice. For example, if the data distribution drifts over time, then the data points are no longer exchangeable; moreover, in such settings, we might want to use a nonsymmetric algorithm that treats recent observations as more relevant. This paper generalizes conformal prediction to deal with both aspects: we employ weighted quantiles to introduce robustness against distribution drift, and design a new randomization technique to allow for algorithms that do not treat data points symmetrically. Our new methods are provably robust, with substantially less loss of coverage when exchangeability is violated due to distribution drift or other challenging features of real data, while also achieving the same coverage guarantees as existing conformal prediction methods if the data points are in fact exchangeable. We demonstrate the practical utility of these new tools with simulations and real-data experiments on electricity and election forecasting.},
  archive      = {J_AOS},
  author       = {Rina Foygel Barber and Emmanuel J. Candès and Aaditya Ramdas and Ryan J. Tibshirani},
  doi          = {10.1214/23-AOS2276},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {816-845},
  shortjournal = {Ann. Statist.},
  title        = {Conformal prediction beyond exchangeability},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference on the maximal rank of time-varying covariance
matrices using high-frequency data. <em>AOS</em>, <em>51</em>(2),
791–815. (<a href="https://doi.org/10.1214/23-AOS2273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the rank of the instantaneous or spot covariance matrix ΣX(t) of a multidimensional process X(t). Given high-frequency observations X(i/n), i=0,…,n, we test the null hypothesis rank(ΣX(t))≤r for all t against local alternatives where the average (r+1)st eigenvalue is larger than some signal detection rate vn. A major problem is that the inherent averaging in local covariance statistics produces a bias that distorts the rank statistics. We show that the bias depends on the regularity and spectral gap of ΣX(t). We establish explicit matrix perturbation and concentration results that provide nonasymptotic uniform critical values and optimal signal detection rates vn. This leads to a rank estimation method via sequential testing. For a class of stochastic volatility models, we determine data-driven critical values via normed p-variations of estimated local covariance matrices. The methods are illustrated by simulations and an application to high-frequency data of U.S. government bonds.},
  archive      = {J_AOS},
  author       = {Markus Reiss and Lars Winkelmann},
  doi          = {10.1214/23-AOS2273},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {791-815},
  shortjournal = {Ann. Statist.},
  title        = {Inference on the maximal rank of time-varying covariance matrices using high-frequency data},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimax rates for conditional density estimation via
empirical entropy. <em>AOS</em>, <em>51</em>(2), 762–790. (<a
href="https://doi.org/10.1214/23-AOS2270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of estimating a conditional density using i.i.d. samples from a joint distribution, which is a fundamental problem with applications in both classification and uncertainty quantification for regression. For joint density estimation, minimax rates have been characterized for general density classes in terms of uniform (metric) entropy, a well-studied notion of statistical capacity. When applying these results to conditional density estimation, the use of uniform entropy—which is infinite when the covariate space is unbounded and suffers from the curse of dimensionality—can lead to suboptimal rates. Consequently, minimax rates for conditional density estimation cannot be characterized using these classical results. We resolve this problem for well-specified models, obtaining matching (within logarithmic factors) upper and lower bounds on the minimax Kullback–Leibler risk in terms of the empirical Hellinger entropy for the conditional density class. The use of empirical entropy allows us to appeal to concentration arguments based on local Rademacher complexity, which—in contrast to uniform entropy—leads to matching rates for large, potentially nonparametric classes and captures the correct dependence on the complexity of the covariate space. Our results require only that the conditional densities are bounded above, and do not require that they are bounded below or otherwise satisfy any tail conditions.},
  archive      = {J_AOS},
  author       = {Blair Bilodeau and Dylan J. Foster and Daniel M. Roy},
  doi          = {10.1214/23-AOS2270},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {762-790},
  shortjournal = {Ann. Statist.},
  title        = {Minimax rates for conditional density estimation via empirical entropy},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimally tackling covariate shift in RKHS-based
nonparametric regression. <em>AOS</em>, <em>51</em>(2), 738–761. (<a
href="https://doi.org/10.1214/23-AOS2268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the covariate shift problem in the context of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We focus on two natural families of covariate shift problems defined using the likelihood ratios between the source and target distributions. When the likelihood ratios are uniformly bounded, we prove that the kernel ridge regression (KRR) estimator with a carefully chosen regularization parameter is minimax rate-optimal (up to a log factor) for a large family of RKHSs with regular kernel eigenvalues. Interestingly, KRR does not require full knowledge of the likelihood ratio apart from an upper bound on it. In striking contrast to the standard statistical setting without covariate shift, we also demonstrate that a naïve estimator, which minimizes the empirical risk over the function class, is strictly suboptimal under covariate shift as compared to KRR. We then address the larger class of covariate shift problems where likelihood ratio is possibly unbounded yet has a finite second moment. Here, we propose a reweighted KRR estimator that weights samples based on a careful truncation of the likelihood ratios. Again, we are able to show that this estimator is minimax optimal, up to logarithmic factors.},
  archive      = {J_AOS},
  author       = {Cong Ma and Reese Pathak and Martin J. Wainwright},
  doi          = {10.1214/23-AOS2268},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {738-761},
  shortjournal = {Ann. Statist.},
  title        = {Optimally tackling covariate shift in RKHS-based nonparametric regression},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On robustness and local differential privacy. <em>AOS</em>,
<em>51</em>(2), 717–737. (<a
href="https://doi.org/10.1214/23-AOS2267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of soaring demand to develop statistical analysis tools that are robust against contamination as well as preserving individual data owners’ privacy. In spite of the fact that both topics host a rich body of literature, to the best of our knowledge, we are the first to systematically study the connections between the optimality under Huber’s contamination model and the local differential privacy (LDP) constraints. In this paper, we start with a general minimax lower bound result, which disentangles the costs of being robust against Huber contamination and preserving LDP. We further study four concrete examples: a two-point testing problem, a potentially diverging mean estimation problem, a nonparametric density estimation problem and a univariate median estimation problem. For each problem, we demonstrate procedures that are optimal in the presence of both contamination and LDP constraints, comment on the connections with the state-of-the-art methods that are only studied under either contamination or privacy constraints, and unveil the connections between robustness and LDP via partially answering whether LDP procedures are robust and whether robust procedures can be efficiently privatised. Overall, our work showcases a promising prospect of joint study for robustness and local differential privacy.},
  archive      = {J_AOS},
  author       = {Mengchu Li and Thomas B. Berrett and Yi Yu},
  doi          = {10.1214/23-AOS2267},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {717-737},
  shortjournal = {Ann. Statist.},
  title        = {On robustness and local differential privacy},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep nonparametric regression on approximate manifolds:
Nonasymptotic error bounds with polynomial prefactors. <em>AOS</em>,
<em>51</em>(2), 691–716. (<a
href="https://doi.org/10.1214/23-AOS2266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the properties of nonparametric least squares regression using deep neural networks. We derive nonasymptotic upper bounds for the excess risk of the empirical risk minimizer of feedforward deep neural regression. Our error bounds achieve minimax optimal rate and improve over the existing ones in the sense that they depend polynomially on the dimension of the predictor, instead of exponentially on dimension. We show that the neural regression estimator can circumvent the curse of dimensionality under the assumption that the predictor is supported on an approximate low-dimensional manifold or a set with low Minkowski dimension. We also establish the optimal convergence rate under the exact manifold support assumption. We investigate how the prediction error of the neural regression estimator depends on the structure of neural networks and propose a notion of network relative efficiency between two types of neural networks, which provides a quantitative measure for evaluating the relative merits of different network structures. To establish these results, we derive a novel approximation error bound for the Hölder smooth functions using ReLU activated neural networks, which may be of independent interest. Our results are derived under weaker assumptions on the data distribution and the neural network structure than those in the existing literature.},
  archive      = {J_AOS},
  author       = {Yuling Jiao and Guohao Shen and Yuanyuan Lin and Jian Huang},
  doi          = {10.1214/23-AOS2266},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {691-716},
  shortjournal = {Ann. Statist.},
  title        = {Deep nonparametric regression on approximate manifolds: Nonasymptotic error bounds with polynomial prefactors},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Efficient functional estimation and the super-oracle
phenomenon. <em>AOS</em>, <em>51</em>(2), 668–690. (<a
href="https://doi.org/10.1214/23-AOS2265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation of two-sample integral functionals, of the type that occur naturally, for example, when the object of interest is a divergence between unknown probability densities. Our first main result is that, in wide generality, a weighted nearest neighbour estimator is efficient, in the sense of achieving the local asymptotic minimax lower bound. Moreover, we also prove a corresponding central limit theorem, which facilitates the construction of asymptotically valid confidence intervals for the functional, having asymptotically minimal width. One interesting consequence of our results is the discovery that, for certain functionals, the worst-case performance of our estimator may improve on that of the natural ‘oracle’ estimator, which itself can be optimal in the related problem where the data consist of the values of the unknown densities at the observations.},
  archive      = {J_AOS},
  author       = {Thomas B. Berrett and Richard J. Samworth},
  doi          = {10.1214/23-AOS2265},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {668-690},
  shortjournal = {Ann. Statist.},
  title        = {Efficient functional estimation and the super-oracle phenomenon},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal estimation and computational limit of low-rank
gaussian mixtures. <em>AOS</em>, <em>51</em>(2), 646–667. (<a
href="https://doi.org/10.1214/23-AOS2264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural matrix-variate observations routinely arise in diverse fields such as multilayer network analysis and brain image clustering. While data of this type have been extensively investigated with fruitful outcomes being delivered, the fundamental questions like its statistical optimality and computational limit are largely under-explored. In this paper, we propose a low-rank Gaussian mixture model (LrMM) assuming each matrix-valued observation has a planted low-rank structure. Minimax lower bounds for estimating the underlying low-rank matrix are established allowing a whole range of sample sizes and signal strength. Under a minimal condition on signal strength, referred to as the information-theoretical limit or statistical limit, we prove the minimax optimality of a maximum likelihood estimator which, in general, is computationally infeasible. If the signal is stronger than a certain threshold, called the computational limit, we design a computationally fast estimator based on spectral aggregation and demonstrate its minimax optimality. Moreover, when the signal strength is smaller than the computational limit, we provide evidences based on the low-degree likelihood ratio framework to claim that no polynomial-time algorithm can consistently recover the underlying low-rank matrix. Our results reveal multiple phase transitions in the minimax error rates and the statistical-to-computational gap. Numerical experiments confirm our theoretical findings. We further showcase the merit of our spectral aggregation method on the worldwide food trading dataset.},
  archive      = {J_AOS},
  author       = {Zhongyuan Lyu and Dong Xia},
  doi          = {10.1214/23-AOS2264},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {646-667},
  shortjournal = {Ann. Statist.},
  title        = {Optimal estimation and computational limit of low-rank gaussian mixtures},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On singular values of data matrices with general independent
columns. <em>AOS</em>, <em>51</em>(2), 624–645. (<a
href="https://doi.org/10.1214/23-AOS2263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the singular values of a large p×n data matrix Xn=(xn1,…,xnn), where the columns {xnj} are independent p-dimensional vectors, possibly with different distributions. Assuming that the covariance matrices Σnj=Cov(xnj) of the column vectors can be asymptotically simultaneously diagonalized, with appropriately converging spectra, we establish a limiting spectral distribution (LSD) for the singular values of Xn when both dimensions p and n grow to infinity in comparable magnitudes. Our matrix model goes beyond and includes many different types of sample covariance matrices in existing work, such as weighted sample covariance matrices, Gram matrices, and sample covariance matrices of a linear time series model. Furthermore, three applications of our general approach are developed. First, we obtain the existence and uniqueness of the LSD for realized covariance matrices of a multi-dimensional diffusion process with anisotropic time-varying co-volatility. Second, we derive the LSD for singular values of data matrices from a recent matrix-valued auto-regressive model. Finally, we also obtain the LSD for singular values of data matrices from a generalized finite mixture model.},
  archive      = {J_AOS},
  author       = {Tianxing Mei and Chen Wang and Jianfeng Yao},
  doi          = {10.1214/23-AOS2263},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {624-645},
  shortjournal = {Ann. Statist.},
  title        = {On singular values of data matrices with general independent columns},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning sparse graphons and the generalized kesten–stigum
threshold. <em>AOS</em>, <em>51</em>(2), 599–623. (<a
href="https://doi.org/10.1214/23-AOS2262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of learning graphons has attracted considerable attention across several scientific communities, with significant progress over the recent years in sparser regimes. Yet, the current techniques still require diverging degrees in order to succeed with efficient algorithms in the challenging cases where the local structure of the graph is homogeneous. This paper provides an efficient algorithm to learn graphons in the constant expected degree regime. The algorithm is shown to succeed in estimating the rank-k projection of a graphon in the L2 metric if the top k eigenvalues of the graphon satisfy a generalized Kesten–Stigum condition.},
  archive      = {J_AOS},
  author       = {Emmanuel Abbe and Shuangping Li and Allan Sly},
  doi          = {10.1214/23-AOS2262},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {599-623},
  shortjournal = {Ann. Statist.},
  title        = {Learning sparse graphons and the generalized Kesten–Stigum threshold},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the disjoint and sliding block maxima method for
piecewise stationary time series. <em>AOS</em>, <em>51</em>(2), 573–598.
(<a href="https://doi.org/10.1214/23-AOS2260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling univariate block maxima by the generalized extreme value distribution constitutes one of the most widely applied approaches in extreme value statistics. It has recently been found that, for an underlying stationary time series, respective estimators may be improved by calculating block maxima in an overlapping way. A proof of concept is provided that the latter finding also holds in situations that involve certain piecewise stationarities. A weak convergence result for an empirical process of central interest is provided, and further details are examplarily worked out for the probability weighted moment estimator. Irrespective of the serial dependence, the asymptotic estimation variance is shown to be smaller for the new estimator. In extensive simulation experiments, the finite-sample variance was typically found to be smaller as well, while the bias stays approximately the same. The results are illustrated by Monte Carlo simulation experiments and are applied to a common situation involving temperature extremes in a changing climate.},
  archive      = {J_AOS},
  author       = {Axel Bücher and Leandra Zanger},
  doi          = {10.1214/23-AOS2260},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {573-598},
  shortjournal = {Ann. Statist.},
  title        = {On the disjoint and sliding block maxima method for piecewise stationary time series},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Canonical noise distributions and private hypothesis tests.
<em>AOS</em>, <em>51</em>(2), 547–572. (<a
href="https://doi.org/10.1214/23-AOS2259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {f-DP has recently been proposed as a generalization of differential privacy allowing a lossless analysis of composition, post-processing, and privacy amplification via subsampling. In the setting of f-DP, we propose the concept of a canonical noise distribution (CND), the first mechanism designed for an arbitrary f-DP guarantee. The notion of CND captures whether an additive privacy mechanism perfectly matches the privacy guarantee of a given f. We prove that a CND always exists, and give a construction that produces a CND for any f. We show that private hypothesis tests are intimately related to CNDs, allowing for the release of private p-values at no additional privacy cost, as well as the construction of uniformly most powerful (UMP) tests for binary data, within the general f-DP framework. We apply our techniques to the problem of difference-of-proportions testing, and construct a UMP unbiased (UMPU) “semiprivate” test which upper bounds the performance of any f-DP test. Using this as a benchmark, we propose a private test based on the inversion of characteristic functions, which allows for optimal inference on the two population parameters and is nearly as powerful as the semiprivate UMPU. When specialized to the case of (ϵ,0)-DP, we show empirically that our proposed test is more powerful than any (ϵ/ 2)-DP test and has more accurate type I errors than the classic normal approximation test.},
  archive      = {J_AOS},
  author       = {Jordan Awan and Salil Vadhan},
  doi          = {10.1214/23-AOS2259},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {547-572},
  shortjournal = {Ann. Statist.},
  title        = {Canonical noise distributions and private hypothesis tests},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local convexity of the TAP free energy and AMP convergence
for z2-synchronization. <em>AOS</em>, <em>51</em>(2), 519–546. (<a
href="https://doi.org/10.1214/23-AOS2257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study mean-field variational Bayesian inference using the TAP approach, for Z2-synchronization as a prototypical example of a high-dimensional Bayesian model. We show that for any signal strength λ&gt;1 (the weak-recovery threshold), there exists a unique local minimizer of the TAP free energy functional near the mean of the Bayes posterior law. Furthermore, the TAP free energy in a local neighborhood of this minimizer is strongly convex. Consequently, a natural-gradient/mirror-descent algorithm achieves linear convergence to this minimizer from a local initialization, which may be obtained by a constant number of iterations of Approximate Message Passing (AMP). This provides a rigorous foundation for variational inference in high dimensions via minimization of the TAP free energy. We also analyze the finite-sample convergence of AMP, showing that AMP is asymptotically stable at the TAP minimizer for any λ&gt;1, and is linearly convergent to this minimizer from a spectral initialization for sufficiently large λ. Such a guarantee is stronger than results obtainable by state evolution analyses, which only describe a fixed number of AMP iterations in the infinite-sample limit. Our proofs combine the Kac–Rice formula and Sudakov–Fernique Gaussian comparison inequality to analyze the complexity of critical points that satisfy strong convexity and stability conditions within their local neighborhoods.},
  archive      = {J_AOS},
  author       = {Michael Celentano and Zhou Fan and Song Mei},
  doi          = {10.1214/23-AOS2257},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {519-546},
  shortjournal = {Ann. Statist.},
  title        = {Local convexity of the TAP free energy and AMP convergence for z2-synchronization},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonlinear independent component analysis for discrete-time
and continuous-time signals. <em>AOS</em>, <em>51</em>(2), 487–518. (<a
href="https://doi.org/10.1214/23-AOS2256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the classical problem of recovering a multidimensional source signal from observations of nonlinear mixtures of this signal. We show that this recovery is possible (up to a permutation and monotone scaling of the source’s original component signals) if the mixture is due to a sufficiently differentiable and invertible but otherwise arbitrarily nonlinear function and the component signals of the source are statistically independent with ‘nondegenerate’ second-order statistics. The latter assumption requires the source signal to meet one of three regularity conditions which essentially ensure that the source is sufficiently far away from the nonrecoverable extremes of being deterministic or constant in time. These assumptions, which cover many popular time series models and stochastic processes, allow us to reformulate the initial problem of nonlinear blind source separation as a simple-to-state problem of optimisation-based function approximation. We propose to solve this approximation problem by minimizing a novel type of objective function that efficiently quantifies the mutual statistical dependence between multiple stochastic processes via cumulant-like statistics. This yields a scalable and direct new method for nonlinear Independent Component Analysis with widely applicable theoretical guarantees and for which our experiments indicate good performance.},
  archive      = {J_AOS},
  author       = {Alexander Schell and Harald Oberhauser},
  doi          = {10.1214/23-AOS2256},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {487-518},
  shortjournal = {Ann. Statist.},
  title        = {Nonlinear independent component analysis for discrete-time and continuous-time signals},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive versus noninteractive locally differentially
private estimation: Two elbows for the quadratic functional.
<em>AOS</em>, <em>51</em>(2), 464–486. (<a
href="https://doi.org/10.1214/22-AOS2254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local differential privacy has recently received increasing attention from the statistics community as a valuable tool to protect the privacy of individual data owners without the need of a trusted third party. Similar to the classical notion of randomized response, the idea is that data owners randomize their true information locally and only release the perturbed data. Many different protocols for such local perturbation procedures can be designed. In most estimation problems studied in the literature so far, however, no significant difference in terms of minimax risk between purely noninteractive protocols and protocols that allow for some amount of interaction between individual data providers could be observed. In this paper, we show that for estimating the integrated square of a density, sequentially interactive procedures improve substantially over the best possible noninteractive procedure in terms of minimax rate of estimation. In particular, in the noninteractive scenario we identify an elbow in the minimax rate at s=34, whereas in the sequentially interactive scenario the elbow is at s=12. This is markedly different from both, the case of direct observations, where the elbow is well known to be at s=14, as well as from the case where Laplace noise is added to the original data, where an elbow at s=94 is obtained. We also provide adaptive estimators that achieve the optimal rate up to log-factors, we draw connections to nonparametric goodness-of-fit testing and estimation of more general integral functionals and conduct a series of numerical experiments. The fact that a particular locally differentially private, but interactive, mechanism improves over the simple noninteractive one is also of great importance for practical implementations of local differential privacy.},
  archive      = {J_AOS},
  author       = {Cristina Butucea and Angelika Rohde and Lukas Steinberger},
  doi          = {10.1214/22-AOS2254},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {464-486},
  shortjournal = {Ann. Statist.},
  title        = {Interactive versus noninteractive locally differentially private estimation: Two elbows for the quadratic functional},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional sequential monte carlo in high dimensions.
<em>AOS</em>, <em>51</em>(2), 437–463. (<a
href="https://doi.org/10.1214/22-AOS2252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The iterated conditional sequential Monte Carlo (i-CSMC) algorithm from Andrieu, Doucet and Holenstein (J. R. Stat. Soc. Ser. B Stat. Methodol. 72 (2010) 269–342) is an MCMC approach for efficiently sampling from the joint posterior distribution of the T latent states in challenging time-series models, for example, in nonlinear or non-Gaussian state-space models. It is also the main ingredient in particle Gibbs samplers which infer unknown model parameters alongside the latent states. In this work, we first prove that the i-CSMC algorithm suffers from a curse of dimension in the dimension of the states, D: it breaks down unless the number of samples (‘particles’), N, proposed by the algorithm grows exponentially with D. Then we present a novel ‘local’ version of the algorithm which proposes particles using Gaussian random-walk moves that are suitably scaled with D. We prove that this iterated random-walk conditional sequential Monte Carlo (i-RW-CSMC) algorithm avoids the curse of dimension: for arbitrary N, its acceptance rates and expected squared jumping distance converge to nontrivial limits as D→∞. If T=N=1, our proposed algorithm reduces to a Metropolis–Hastings or Barker’s algorithm with Gaussian random-walk moves and we recover the well-known scaling limits for such algorithms.},
  archive      = {J_AOS},
  author       = {Axel Finke and Alexandre H. Thiery},
  doi          = {10.1214/22-AOS2252},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {437-463},
  shortjournal = {Ann. Statist.},
  title        = {Conditional sequential monte carlo in high dimensions},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Debiasing convex regularized estimators and interval
estimation in linear models. <em>AOS</em>, <em>51</em>(2), 391–436. (<a
href="https://doi.org/10.1214/22-AOS2243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New upper bounds are developed for the L2 distance between ξ/Var[ξ]1/2 and linear and quadratic functions of z∼N(0,In) for random variables of the form ξ=z⊤f(z)−divf(z). The linear approximation yields a central limit theorem when the squared norm of f(z) dominates the squared Frobenius norm of ∇f(z) in expectation. Applications of this normal approximation are given for the asymptotic normality of debiased estimators in linear regression with correlated design and convex penalty in the regime p/n≤γ for constant γ∈(0,∞). For the estimation of linear functions ⟨a0,β⟩ of the unknown coefficient vector β, this analysis leads to asymptotic normality of the debiased estimate for most normalized directions a0, where “most” is quantified in a precise sense. This asymptotic normality holds for any convex penalty if γn, asymptotic normality of the debiased estimate is obtained for the Lasso and the group Lasso under additional conditions. For general convex penalties, our analysis also provides prediction and estimation error bounds of independent interest.},
  archive      = {J_AOS},
  author       = {Pierre C. Bellec and Cun-Hui Zhang},
  doi          = {10.1214/22-AOS2243},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {391-436},
  shortjournal = {Ann. Statist.},
  title        = {Debiasing convex regularized estimators and interval estimation in linear models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uniform consistency in nonparametric mixture models.
<em>AOS</em>, <em>51</em>(1), 362–390. (<a
href="https://doi.org/10.1214/22-AOS2255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study uniform consistency in nonparametric mixture models as well as closely related mixture of regression (also known as mixed regression) models, where the regression functions are allowed to be nonparametric and the error distributions are assumed to be convolutions of a Gaussian density. We construct uniformly consistent estimators under general conditions while simultaneously highlighting several pain points in extending existing pointwise consistency results to uniform results. The resulting analysis turns out to be nontrivial, and several novel technical tools are developed along the way. In the case of mixed regression, we prove L1 convergence of the regression functions while allowing for the component regression functions to intersect arbitrarily often, which presents additional technical challenges. We also consider generalizations to general (i.e., nonconvolutional) nonparametric mixtures.},
  archive      = {J_AOS},
  author       = {Bryon Aragam and Ruiyi Yang},
  doi          = {10.1214/22-AOS2255},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {362-390},
  shortjournal = {Ann. Statist.},
  title        = {Uniform consistency in nonparametric mixture models},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nested markov properties for acyclic directed mixed graphs.
<em>AOS</em>, <em>51</em>(1), 334–361. (<a
href="https://doi.org/10.1214/22-AOS2253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional independence models associated with directed acyclic graphs (DAGs) may be characterized in at least three different ways: via a factorization, the global Markov property (given by the d-separation criterion), and the local Markov property. Marginals of DAG models also imply equality constraints that are not conditional independences; the well-known “Verma constraint” is an example. Constraints of this type are used for testing edges, and in a computationally efficient marginalization scheme via variable elimination. We show that equality constraints like the “Verma constraint” can be viewed as conditional independences in kernel objects obtained from joint distributions via a fixing operation that generalizes conditioning and marginalization. We use these constraints to define, via ordered local and global Markov properties, and a factorization, a graphical model associated with acyclic directed mixed graphs (ADMGs). We prove that marginal distributions of DAG models lie in this model, and that a set of these constraints given by Tian provides an alternative definition of the model. Finally, we show that the fixing operation used to define the model leads to a particularly simple characterization of identifiable causal effects in hidden variable causal DAG models.},
  archive      = {J_AOS},
  author       = {Thomas S. Richardson and Robin J. Evans and James M. Robins and Ilya Shpitser},
  doi          = {10.1214/22-AOS2253},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {334-361},
  shortjournal = {Ann. Statist.},
  title        = {Nested markov properties for acyclic directed mixed graphs},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection, monotone likelihood ratio and group
sparsity. <em>AOS</em>, <em>51</em>(1), 312–333. (<a
href="https://doi.org/10.1214/22-AOS2251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pivotal variable selection problem, we derive the exact nonasymptotic minimax selector over the class of all s-sparse vectors, which is also the Bayes selector with respect to the uniform prior. While this optimal selector is, in general, not realizable in polynomial time, we show that its tractable counterpart (the scan selector) attains the minimax expected Hamming risk to within factor 2, and is also exact minimax with respect to the probability of wrong recovery. As a consequence, we establish explicit lower bounds under the monotone likelihood ratio property and we obtain a tight characterization of the minimax risk in terms of the best separable selector risk. We apply these general results to derive necessary and sufficient conditions of exact and almost full recovery in the location model with light tail distributions and in the problem of group variable selection under Gaussian noise and under more general anisotropic sub-Gaussian noise. Numerical results illustrate our theoretical findings.},
  archive      = {J_AOS},
  author       = {Cristina Butucea and Enno Mammen and Mohamed Ndaoud and Alexandre B. Tsybakov},
  doi          = {10.1214/22-AOS2251},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {312-333},
  shortjournal = {Ann. Statist.},
  title        = {Variable selection, monotone likelihood ratio and group sparsity},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional predictive inference for stable algorithms.
<em>AOS</em>, <em>51</em>(1), 290–311. (<a
href="https://doi.org/10.1214/22-AOS2250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate generically applicable and intuitively appealing prediction intervals based on k-fold cross-validation. We focus on the conditional coverage probability of the proposed intervals, given the observations in the training sample (hence, training conditional validity), and show that it is close to the nominal level, in an appropriate sense, provided that the underlying algorithm used for computing point predictions is sufficiently stable when feature-response pairs are omitted. Our results are based on a finite sample analysis of the empirical distribution function of k-fold cross-validation residuals and hold in nonparametric settings with only minimal assumptions on the error distribution. To illustrate our results, we also apply them to high-dimensional linear predictors, where we obtain uniform asymptotic training conditional validity as both sample size and dimension tend to infinity at the same rate and consistent parameter estimation typically fails. These results show that despite the serious problems of resampling procedures for inference on the unknown parameters (cf. in A Festschrift for Erich L. Lehmann (1983) 28–48 Wadsworth; Ann. Statist. 24 (1996) 307–335; J. Mach. Learn. Res. 19 (2018) 5), cross-validation methods can be successfully applied to obtain reliable predictive inference even in high dimensions and conditionally on the training data.},
  archive      = {J_AOS},
  author       = {Lukas Steinberger and Hannes Leeb},
  doi          = {10.1214/22-AOS2250},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {290-311},
  shortjournal = {Ann. Statist.},
  title        = {Conditional predictive inference for stable algorithms},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new approach to tests and confidence bands for
distribution functions. <em>AOS</em>, <em>51</em>(1), 260–289. (<a
href="https://doi.org/10.1214/22-AOS2249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce new goodness-of-fit tests and corresponding confidence bands for distribution functions. They are inspired by multiscale methods of testing and based on refined laws of the iterated logarithm for the normalized uniform empirical process Un(t)/ t(1−t) and its natural limiting process, the normalized Brownian bridge process U(t)/ t(1−t). The new tests and confidence bands refine the procedures of Berk and Jones (1979) and Owen (1995). Roughly speaking, the high power and accuracy of the latter methods in the tail regions of distributions are essentially preserved while gaining considerably in the central region. The goodness-of-fit tests perform well in signal detection problems involving sparsity, as in Ingster (1997), Donoho and Jin (2004) and Jager and Wellner (2007), but also under contiguous alternatives. Our analysis of the confidence bands sheds new light on the influence of the underlying ϕ-divergences.},
  archive      = {J_AOS},
  author       = {Lutz Dümbgen and Jon A. Wellner},
  doi          = {10.1214/22-AOS2249},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {260-289},
  shortjournal = {Ann. Statist.},
  title        = {A new approach to tests and confidence bands for distribution functions},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On high-dimensional poisson models with measurement error:
Hypothesis testing for nonlinear nonconvex optimization. <em>AOS</em>,
<em>51</em>(1), 233–259. (<a
href="https://doi.org/10.1214/22-AOS2248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study estimation and testing in the Poisson regression model with noisy high-dimensional covariates, which has wide applications in analyzing noisy big data. Correcting for the estimation bias due to the covariate noise leads to a nonconvex target function to minimize. Treating the high-dimensional issue further leads us to augment an amenable penalty term to the target function. We propose to estimate the regression parameter through minimizing the penalized target function. We derive the L1 and L2 convergence rates of the estimator and prove the variable selection consistency. We further establish the asymptotic normality of any subset of the parameters, where the subset can have infinitely many components as long as its cardinality grows sufficiently slow. We develop Wald and score tests based on the asymptotic normality of the estimator, which permits testing of linear functions of the members if the subset. We examine the finite sample performance of the proposed tests by extensive simulation. Finally, the proposed method is successfully applied to the Alzheimer’s Disease Neuroimaging Initiative study, which motivated this work initially.},
  archive      = {J_AOS},
  author       = {Fei Jiang and Yeqing Zhou and Jianxuan Liu and Yanyuan Ma},
  doi          = {10.1214/22-AOS2248},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {233-259},
  shortjournal = {Ann. Statist.},
  title        = {On high-dimensional poisson models with measurement error: Hypothesis testing for nonlinear nonconvex optimization},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Breaking the winner’s curse in mendelian randomization:
Rerandomized inverse variance weighted estimator. <em>AOS</em>,
<em>51</em>(1), 211–232. (<a
href="https://doi.org/10.1214/22-AOS2247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developments in genome-wide association studies and the increasing availability of summary genetic association data have made the application of two-sample Mendelian Randomization (MR) with summary data increasingly popular. Conventional two-sample MR methods often employ the same sample for selecting relevant genetic variants and for constructing final causal estimates. Such a practice often leads to biased causal effect estimates due to the well-known “winner’s curse” phenomenon. To address this fundamental challenge, we first examine its consequence on causal effect estimation both theoretically and empirically. We then propose a novel framework that systematically breaks the winner’s curse, leading to unbiased association effect estimates for the selected genetic variants. Building upon the proposed framework, we introduce a novel rerandomized inverse variance weighted estimator that is consistent when selection and parameter estimation are conducted on the same sample. Under appropriate conditions, we show that the proposed RIVW estimator for the causal effect converges to a normal distribution asymptotically and its variance can be well estimated. We illustrate the finite-sample performance of our approach through Monte Carlo experiments and two empirical examples.},
  archive      = {J_AOS},
  author       = {Xinwei Ma and Jingshen Wang and Chong Wu},
  doi          = {10.1214/22-AOS2247},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {211-232},
  shortjournal = {Ann. Statist.},
  title        = {Breaking the winner’s curse in mendelian randomization: Rerandomized inverse variance weighted estimator},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sharp global convergence guarantees for iterative nonconvex
optimization with random data. <em>AOS</em>, <em>51</em>(1), 179–210.
(<a href="https://doi.org/10.1214/22-AOS2246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a general class of regression models with normally distributed covariates, and the associated nonconvex problem of fitting these models from data. We develop a general recipe for analyzing the convergence of iterative algorithms for this task from a random initialization. In particular, provided each iteration can be written as the solution to a convex optimization problem satisfying some natural conditions, we leverage Gaussian comparison theorems to derive a deterministic sequence that provides sharp upper and lower bounds on the error of the algorithm with sample splitting. Crucially, this deterministic sequence accurately captures both the convergence rate of the algorithm and the eventual error floor in the finite-sample regime, and is distinct from the commonly used “population” sequence that results from taking the infinite-sample limit. We apply our general framework to derive several concrete consequences for parameter estimation in popular statistical models including phase retrieval and mixtures of regressions. Provided the sample size scales near linearly in the dimension, we show sharp global convergence rates for both higher-order algorithms based on alternating updates and first-order algorithms based on subgradient descent. These corollaries, in turn, reveal multiple nonstandard phenomena that are then corroborated by extensive numerical experiments.},
  archive      = {J_AOS},
  author       = {Kabir Aladin Chandrasekher and Ashwin Pananjady and Christos Thrampoulidis},
  doi          = {10.1214/22-AOS2246},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {179-210},
  shortjournal = {Ann. Statist.},
  title        = {Sharp global convergence guarantees for iterative nonconvex optimization with random data},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing for outliers with conformal p-values. <em>AOS</em>,
<em>51</em>(1), 149–178. (<a
href="https://doi.org/10.1214/22-AOS2244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the construction of p-values for nonparametric outlier detection, from a multiple-testing perspective. The goal is to test whether new independent samples belong to the same distribution as a reference data set or are outliers. We propose a solution based on conformal inference, a general framework yielding p-values that are marginally valid but mutually dependent for different test points. We prove these p-values are positively dependent and enable exact false discovery rate control, although in a relatively weak marginal sense. We then introduce a new method to compute p-values that are valid conditionally on the training data and independent of each other for different test points; this paves the way to stronger type-I error guarantees. Our results depart from classical conformal inference as we leverage concentration inequalities rather than combinatorial arguments to establish our finite-sample guarantees. Further, our techniques also yield a uniform confidence bound for the false positive rate of any outlier detection algorithm, as a function of the threshold applied to its raw statistics. Finally, the relevance of our results is demonstrated by experiments on real and simulated data.},
  archive      = {J_AOS},
  author       = {Stephen Bates and Emmanuel Candès and Lihua Lei and Yaniv Romano and Matteo Sesia},
  doi          = {10.1214/22-AOS2244},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {149-178},
  shortjournal = {Ann. Statist.},
  title        = {Testing for outliers with conformal p-values},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ScreeNOT: Exact MSE-optimal singular value thresholding in
correlated noise. <em>AOS</em>, <em>51</em>(1), 122–148. (<a
href="https://doi.org/10.1214/22-AOS2232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a formula for optimal hard thresholding of the singular value decomposition in the presence of correlated additive noise; although it nominally involves unobservables, we show how to apply it even where the noise covariance structure is not a priori known or is not independently estimable. The proposed method, which we call ScreeNOT, is a mathematically solid alternative to Cattell’s ever-popular but vague scree plot heuristic from 1966. ScreeNOT has a surprising oracle property: it typically achieves exactly, in large finite samples, the lowest possible MSE for matrix recovery, on each given problem instance, that is, the specific threshold it selects gives exactly the smallest achievable MSE loss among all possible threshold choices for that noisy data set and that unknown underlying true low rank model. The method is computationally efficient and robust against perturbations of the underlying covariance structure. Our results depend on the assumption that the singular values of the noise have a limiting empirical distribution of compact support; this property, which is standard in random matrix theory, is satisfied by many models exhibiting either cross-row correlation structure or cross-column correlation structure, and also by many situations with more general, interelement correlation structure. Simulations demonstrate the effectiveness of the method even at moderate matrix sizes. The paper is supplemented by ready-to-use software packages implementing the proposed algorithm: package ScreeNOT in Python (via PyPI) and R (via CRAN).},
  archive      = {J_AOS},
  author       = {David Donoho and Matan Gavish and Elad Romanov},
  doi          = {10.1214/22-AOS2232},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {122-148},
  shortjournal = {Ann. Statist.},
  title        = {ScreeNOT: Exact MSE-optimal singular value thresholding in correlated noise},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-dimensional latent panel quantile regression with an
application to asset pricing. <em>AOS</em>, <em>51</em>(1), 96–121. (<a
href="https://doi.org/10.1214/22-AOS2223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generalization of the linear panel quantile regression model to accommodate both sparse and dense parts: sparse means that while the number of covariates available is large, potentially only a much smaller number of them have a nonzero impact on each conditional quantile of the response variable; while the dense part is represent by a low-rank matrix that can be approximated by latent factors and their loadings. Such a structure poses problems for traditional sparse estimators, such as the ℓ1-penalized quantile regression, and for traditional latent factor estimators such as PCA. We propose a new estimation procedure, based on the ADMM algorithm, that consists of combining the quantile loss function with ℓ1 and nuclear norm regularization. We show, under general conditions, that our estimator can consistently estimate both the nonzero coefficients of the covariates and the latent low-rank matrix. This is done in a challenging setting that allows for temporal dependence, heavy-tail distributions and the presence of latent factors. Our proposed model has a “Characteristics + Latent Factors” Quantile Asset Pricing Model interpretation: we apply our model and estimator with a large-dimensional panel of financial data and find that (i) characteristics have sparser predictive power once latent factors were controlled and (ii) the factors and coefficients at upper and lower quantiles are different from the median.},
  archive      = {J_AOS},
  author       = {Alexandre Belloni and Mingli Chen and Oscar Hernan Madrid Padilla and Zixuan (Kevin) Wang},
  doi          = {10.1214/22-AOS2223},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {96-121},
  shortjournal = {Ann. Statist.},
  title        = {High-dimensional latent panel quantile regression with an application to asset pricing},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal estimation of high-dimensional gaussian location
mixtures. <em>AOS</em>, <em>51</em>(1), 62–95. (<a
href="https://doi.org/10.1214/22-AOS2207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the optimal rate of estimation in a finite Gaussian location mixture model in high dimensions without separation conditions. We assume that the number of components k is bounded and that the centers lie in a ball of bounded radius, while allowing the dimension d to be as large as the sample size n. Extending the one-dimensional result of Heinrich and Kahn (Ann. Statist. 46 (2018) 2844–2870), we show that the minimax rate of estimating the mixing distribution in Wasserstein distance is Θ((d/n)1/4+n−1/(4k−2)), achieved by an estimator computable in time O(nd2+n5/4). Furthermore, we show that the mixture density can be estimated at the optimal parametric rate Θ(d/n) in Hellinger distance and provide a computationally efficient algorithm to achieve this rate in the special case of k=2. Both the theoretical and methodological development rely on a careful application of the method of moments. Central to our results is the observation that the information geometry of finite Gaussian mixtures is characterized by the moment tensors of the mixing distribution, whose low-rank structure can be exploited to obtain a sharp local entropy bound.},
  archive      = {J_AOS},
  author       = {Natalie Doss and Yihong Wu and Pengkun Yang and Harrison H. Zhou},
  doi          = {10.1214/22-AOS2207},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {62-95},
  shortjournal = {Ann. Statist.},
  title        = {Optimal estimation of high-dimensional gaussian location mixtures},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterizing the SLOPE trade-off: A variational
perspective and the donoho–tanner limit. <em>AOS</em>, <em>51</em>(1),
33–61. (<a href="https://doi.org/10.1214/22-AOS2194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sorted ℓ1 regularization has been incorporated into many methods for solving high-dimensional statistical estimation problems, including the SLOPE estimator in linear regression. In this paper, we study how this relatively new regularization technique improves variable selection by characterizing the optimal SLOPE trade-off between the false discovery proportion (FDP) and true positive proportion (TPP) or, equivalently, between measures of type I error and power. Assuming a regime of linear sparsity and working under Gaussian random designs, we obtain an upper bound on the optimal trade-off for SLOPE, showing its capability of breaking the Donoho–Tanner power limit. To put it into perspective, this limit is the highest possible power that the Lasso, which is perhaps the most popular ℓ1-based method, can achieve even with arbitrarily strong effect sizes. Next, we derive a tight lower bound that delineates the fundamental limit of sorted ℓ1 regularization in optimally trading the FDP off for the TPP. Finally, we show that on any problem instance, SLOPE with a certain regularization sequence outperforms the Lasso, in the sense of having a smaller FDP, larger TPP and smaller ℓ2 estimation risk simultaneously. Our proofs are based on a novel technique that reduces a calculus of variations problem to a class of infinite-dimensional convex optimization problems and a very recent result from approximate message passing theory.},
  archive      = {J_AOS},
  author       = {Zhiqi Bu and Jason M. Klusowski and Cynthia Rush and Weijie J. Su},
  doi          = {10.1214/22-AOS2194},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {33-61},
  shortjournal = {Ann. Statist.},
  title        = {Characterizing the SLOPE trade-off: A variational perspective and the Donoho–Tanner limit},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grouped variable selection with discrete optimization:
Computational and statistical perspectives. <em>AOS</em>,
<em>51</em>(1), 1–32. (<a
href="https://doi.org/10.1214/21-AOS2155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithmic framework for grouped variable selection that is based on discrete mathematical optimization. While there exist several appealing approaches based on convex relaxations and nonconvex heuristics, we focus on optimal solutions for the ℓ0-regularized formulation, a problem that is relatively unexplored due to computational challenges. Our methodology covers both high-dimensional linear regression and nonparametric sparse additive modeling with smooth components. Our algorithmic framework consists of approximate and exact algorithms. The approximate algorithms are based on coordinate descent and local search, with runtimes comparable to popular sparse learning algorithms. Our exact algorithm is based on a standalone branch-and-bound (BnB) framework, which can solve the associated mixed integer programming (MIP) problem to certified optimality. By exploiting the problem structure, our custom BnB algorithm can solve to optimality problem instances with 5×106 features and 103 observations in minutes to hours—over 1000 times larger than what is currently possible using state-of-the-art commercial MIP solvers. We also explore statistical properties of the ℓ0-based estimators. We demonstrate, theoretically and empirically, that our proposed estimators have an edge over popular group-sparse estimators in terms of statistical performance in various regimes. We provide an open source implementation of our proposed framework.},
  archive      = {J_AOS},
  author       = {Hussein Hazimeh and Rahul Mazumder and Peter Radchenko},
  doi          = {10.1214/21-AOS2155},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Ann. Statist.},
  title        = {Grouped variable selection with discrete optimization: Computational and statistical perspectives},
  volume       = {51},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
