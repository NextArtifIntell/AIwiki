<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SII_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sii---44">SII - 44</h2>
<ul>
<li><details>
<summary>
(2023). Fine-tuned sensitivity analysis for non-ignorable missing
data mechanism in linear regression models. <em>SII</em>,
<em>16</em>(4), 617–627. (<a
href="https://doi.org/10.4310/22-SII748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a widespread problem in many fields, such as statistical analysis in medical research. The missing data mechanism (MDM) is overly complicated in many cases, and the most complex one is the non-ignorable missingness. In this paper, we analyse the incomplete data bias of maximum likelihood estimates on the inference of linear regression models with non-ignorable missing covariate specifically, where the working model always has a small departure from the true model. The incomplete data bias has been divided into two parts because of two types of uncertainties, one is the misspecified distribution between covariates, the other is the misspecified MDM.We identify the key sensitivity parameters in MDM and further propose generative MDM models, leading to a non-implausible set which quantify a smaller range of possible solutions comparing to the conventional sensitivity analysis and worst-case study. Our analysis focuses the sensitivity of MDM modelling in the missing covariate problems. Numerical examples are presented in both simulation studies and a real data example.},
  archive      = {J_SII},
  author       = {Zhu, Rong and Yin, Peng and Shi, Jian Qing},
  doi          = {10.4310/22-SII748},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {617-627},
  shortjournal = {Stat. Interface},
  title        = {Fine-tuned sensitivity analysis for non-ignorable missing data mechanism in linear regression models},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network vector autoregressive moving average model.
<em>SII</em>, <em>16</em>(4), 593–615. (<a
href="https://doi.org/10.4310/22-SII747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling a continuous response of a large-scale network is an important task and it has become prevailing in practice at present. This paper proposes a novel network vector autoregressive moving average (NARMA) model which considers the responses from both an ultra-high dimension vector and the network structure effects. Compared with the network vector autoregressive (NAR, [ 26 ]) model, we take into account the lagged innovations and corresponding network effect in our proposed model. With more parameters considered and a moving average term incorporated, the proposed NARMA model can fit the data more closely and accurately, thus has a better performance than the NAR model. A modified least square estimation for the NARMA model is introduced, and the consistency properties are fully investigated. Finally, we demonstrate the superiority of the proposed NARMA model by investigating the financial contagions of S&amp;P500 index constituents.},
  archive      = {J_SII},
  author       = {Chen, Xiao and Chen, Yu and Hu, Xixu},
  doi          = {10.4310/22-SII747},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {593-615},
  shortjournal = {Stat. Interface},
  title        = {Network vector autoregressive moving average model},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the optimal configuration of a square array group testing
algorithm. <em>SII</em>, <em>16</em>(4), 579–591. (<a
href="https://doi.org/10.4310/22-SII746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Up to date, only lower and upper bounds for the optimal configuration of a Square Array (A2) Group Testing (GT) algorithm are known. We establish exact analytical formulae and provide a couple of applications of our result. First, we compare the A2 GT scheme to several other classical GT schemes in terms of the gain per specimen attained at optimal configuration. Second, operating under objective Bayesian framework with the loss designed to attain minimum at optimal configuration, we suggest the preferred choice of the group size under natural minimal assumptions: the prior information regarding the prevalence suggests that grouping and application of A2 is better than individual testing. The same suggestion is provided for the Minimax strategy.},
  archive      = {J_SII},
  author       = {Čiżikovienė, Ugn and Skorniakov, Viktor},
  doi          = {10.4310/22-SII746},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {579-591},
  shortjournal = {Stat. Interface},
  title        = {On the optimal configuration of a square array group testing algorithm},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metric distributional discrepancy in metric space.
<em>SII</em>, <em>16</em>(4), 565–578. (<a
href="https://doi.org/10.4310/22-SII744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independence analysis is an indispensable step before regression analysis to find out the essential factors that influence the objects. With many applications in machine Learning, medical Learning and a variety of disciplines, statistical methods of measuring the relationship between random variables have been well studied in vector spaces. However, there are few methods developed to verify the relation between random elements in metric spaces. In this paper, we present a novel index called metric distributional discrepancy (MDD) to measure the dependence between a random element $X$ and a categorical variable $Y$ , which is applicable to the medical image and related variables. The metric distributional discrepancy statistics can be considered as the distance between the conditional distribution of $X$ given each class of $Y$ and the unconditional distribution of $X$. MDD enjoys some significant merits compared to other dependence-measures. For instance, MDD is zero if and only if $X$ and $Y$ are independent. MDD test is a distribution-free test since there is no assumption on the distribution of random elements. Furthermore, MDD test is robust to the data with heavy-tailed distribution and potential outliers. We demonstrate the validity of our theory and the property of the MDD test by several numerical experiments and real data analysis.},
  archive      = {J_SII},
  author       = {Pan, Wenliang and Li, Yujue and Liu, Jianwu and Dang, Pei and Mai, Weixiong},
  doi          = {10.4310/22-SII744},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {565-578},
  shortjournal = {Stat. Interface},
  title        = {Metric distributional discrepancy in metric space},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pairwise pseudo-likelihood approach for the additive
hazards model with left-truncated and interval-censored data.
<em>SII</em>, <em>16</em>(4), 553–563. (<a
href="https://doi.org/10.4310/22-SII743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Left-truncated and interval-censored data occur commonly and some approaches have been proposed in the literature for their analysis. However, most of the existing methods are based on the conditional likelihood given left-truncation times, which can be inefficient since the information in the marginal likelihood of the truncation times is ignored. To address this, in this paper, a pairwise pseudo-likelihood augmented estimation approach is proposed under the additive hazards model that can fully make use of all available information. The derived estimator is shown to be consistent and asymptotically normal, and simulation studies suggest that the proposed method works well and provides a substantial efficiency gain over the conditional approach. In addition, the method is applied to a set of real data arising from an AIDS cohort study.},
  archive      = {J_SII},
  author       = {Wang, Peijie and Lou, Yichen and Sun, Jianguo},
  doi          = {10.4310/22-SII743},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {553-563},
  shortjournal = {Stat. Interface},
  title        = {A pairwise pseudo-likelihood approach for the additive hazards model with left-truncated and interval-censored data},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIMEX estimation for quantile regression model with
measurement error. <em>SII</em>, <em>16</em>(4), 545–552. (<a
href="https://doi.org/10.4310/22-SII742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantile regression model with measurement error is considered. To deal with measurement error, we extend the simulation-extrapolation (SIMEX) method to the case of quantile regressions in the presence of covariate measurement error. The proposed SIMEX estimation corrects the bias caused by the measurement error, and not requires the equal distribution assumption of the regression error and measurement error. The asymptotic distribution of the proposed estimator is derived. The finite sample performance of the proposed method is investigated by a simulation study. A real dataset from the Framingham Heart Study is analyzed to illustrate the proposed method.},
  archive      = {J_SII},
  author       = {Yang, Yiping and Zhao, Peixin and Wu, Dongsheng},
  doi          = {10.4310/22-SII742},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {545-552},
  shortjournal = {Stat. Interface},
  title        = {SIMEX estimation for quantile regression model with measurement error},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Markov-switching poisson generalized autoregressive
conditional heteroscedastic models. <em>SII</em>, <em>16</em>(4),
531–544. (<a href="https://doi.org/10.4310/22-SII741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a kind of regime-switching autoregressive models for nonnegative integer-valued time series when the conditional distribution given historical information is Poisson distribution. In this type of models the link between the conditional variance (i.e. the conditional mean for Poisson distribution) and its past values as well as the observed values of the Poisson process may be different when an unobservable (hidden) variable, which is a Markovian Chain, takes different states. We study the stationarity and ergodicity of Markov-switching Poisson generalized autoregressive heteroscedastic (MS-PGARCH) models, and give a condition on parameters under which a MS-PGARCH process can be approximated by a geometrically ergodic process. Under this condition we discuss maximum likelihood estimation for MS-PGARCH models. Simulation studies and application to modelling financial count time series are presented to support our methodology.},
  archive      = {J_SII},
  author       = {Liu, Jichun and Pan, Yue and Pan, Jiazhu and Almarashi, Abdullah},
  doi          = {10.4310/22-SII741},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {531-544},
  shortjournal = {Stat. Interface},
  title        = {Markov-switching poisson generalized autoregressive conditional heteroscedastic models},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A default bayesian multiple comparison of two binomial
proportions. <em>SII</em>, <em>16</em>(4), 517–529. (<a
href="https://doi.org/10.4310/22-SII740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a default Bayesian approach to multiple testing of equality of two binomial proportions. While our approach is motivated by a scenario where one proportion corresponds to an experimental condition and the other to a control, we find it is also reasonable for comparing two proportions in general. We consider a selection of priors under the alternative(s) including the intrinsic prior and a newly proposed “mode-based” Beta prior, and investigate their properties in terms of certain desirable characteristics that we specify for default priors. We also develop priors for the hyperparameters based on the conventional hyperprior used for normal means multiple testing. We also consider a computationally more efficient empirical Bayes approach using the intrinsic prior and the proposed Beta prior. We use repeated simulation and real data sets to evaluate and illustrate the approach, and compare certain frequentist characteristics of the results based on intrinsic and mode based Beta prior using full Bayes and empirical Bayes approaches. Additionally, the results from the Bayesian approach are compared with a commonly used frequentist procedure using conventional thresholds in the respective settings. Overall, we find that the proposed mode-based Beta prior is a suitable default prior for multiple testing of equality of two proportions.},
  archive      = {J_SII},
  author       = {Gecili, Emrah and Sivaganesan, Siva},
  doi          = {10.4310/22-SII740},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {517-529},
  shortjournal = {Stat. Interface},
  title        = {A default bayesian multiple comparison of two binomial proportions},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating individualized treatment rules for multicategory
type 2 diabetes treatments using electronic health records.
<em>SII</em>, <em>16</em>(4), 505–515. (<a
href="https://doi.org/10.4310/22-SII739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a general framework to learn optimal treatment rules for type 2 diabetes (T2D) patients using electronic health records (EHRs). We first propose a joint modeling approach to characterize patient’s pretreatment conditions using longitudinal markers from EHRs. The estimation accounts for informative measurement times using inverse-intensity weighting methods. The predicted latent processes in the joint model are used to divide patients into a finite of subgroups and, within each group, patients share similar health profiles in EHRs. Within each patient group, we estimate optimal individualized treatment rules by extending a matched learning method to handle multicategory treatments using a one-versus-one approach. Each matched learning for two treatments is implemented by a weighted support vector machine with matched pairs of patients. We apply our method to estimate optimal treatment rules for T2D patients in a large sample of EHRs from the Ohio State University Wexner Medical Center. We demonstrate the utility of our method to select the optimal treatments from four classes of drugs and achieve a better control of glycated hemoglobin than any one-size-fits-all rules.},
  archive      = {J_SII},
  author       = {Lou, Jitong and Wang, Yuanjia and Li, Lang and Zeng, Donglin},
  doi          = {10.4310/22-SII739},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {505-515},
  shortjournal = {Stat. Interface},
  title        = {Estimating individualized treatment rules for multicategory type 2 diabetes treatments using electronic health records},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate frailty models using survey weights with
applications to twins infant mortality in ethiopia. <em>SII</em>,
<em>16</em>(4), 493–502. (<a
href="https://doi.org/10.4310/22-SII738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have shown that twin birth contributes substantially to infant and child mortality mainly in resource-poor countries. The excess rates among twins call for research in statistical modeling to identify the main causes behind it. In studies involving multiple individuals from the same family, the fundamental independence assumption in the classical statistical modeling is not plausible. In addition, previous studies indicated that ignoring sampling weight while dealing with a dataset collected with complex survey design can introduce serious bias. This study is then aimed to fill these methodological gaps to integrate the dependence from twin birth with an advanced statistical gamma frailty model to correctly identify the determinants of infant mortality among twins in Ethiopia. We compiled all available data from the 2016 Ethiopia Demographic and Health Survey with a total of 908 children (454 pairs of twins) with survey sampling weight incorporated in the analysis. To identify predictors and to assess the presence and significance of frailty, semiparametric univariate, bivariate shared, and correlated gamma frailty models were fitted. The likelihood ratio test was employed to test the significance of frailty term in the model. We found that sex of the child, among twins birth order, preceding birth interval, and succeeding birth interval are significantly associated with twin infant mortality. The results of this study further confirmed the significance of the shared frailty term accounting for the unobserved heterogeneity.},
  archive      = {J_SII},
  author       = {Kifle, Yehenew G. and Chen, Ding-Geng and Haileyesus, Mesfin T.},
  doi          = {10.4310/22-SII738},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {493-502},
  shortjournal = {Stat. Interface},
  title        = {Multivariate frailty models using survey weights with applications to twins infant mortality in ethiopia},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Confidence in the treatment decision for an individual
patient: Strategies for sequential assessment. <em>SII</em>,
<em>16</em>(3), 475–491. (<a
href="https://doi.org/10.4310/22-SII737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolving medical technologies have motivated the development of treatment decision rules (TDRs) that incorporate complex, costly data (e.g., imaging). In clinical practice, we aim for TDRs to be valuable by reducing unnecessary testing while still identifying the best possible treatment for a patient. Regardless of how well any TDR performs in the target population, there is an associated degree of uncertainty about its optimality for a specific patient. In this paper, we aim to quantify, via a confidence measure, the uncertainty in a TDR as patient data from sequential procedures accumulate in real-time. We first propose estimating confidence using the distance of a patient’s vector of covariates to a treatment decision boundary, with further distances corresponding to higher certainty. We further propose measuring confidence through the conditional probabilities of ultimately (with all possible information available) being assigned a particular treatment, given that the same treatment is assigned with the patient’s currently available data or given the treatment recommendation made using only the currently available patient data. As patient data accumulate, the treatment decision is updated and confidence reassessed until a sufficiently high confidence level is achieved. We present results from simulation studies and illustrate the methods using a motivating example from a depression clinical trial. Recommendations for practical use of the measures are proposed.},
  archive      = {J_SII},
  author       = {Orwitz, Nina and Tarpey, Thaddeus and Petkova, Eva},
  doi          = {10.4310/22-SII737},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {475-491},
  shortjournal = {Stat. Interface},
  title        = {Confidence in the treatment decision for an individual patient: Strategies for sequential assessment},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quadratic upper bound algorithms for estimation under cox
model in case-cohort studies. <em>SII</em>, <em>16</em>(3), 459–474. (<a
href="https://doi.org/10.4310/22-SII736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A case-cohort design is a cost-effective biased-sampling scheme in large cohort studies. Implementation of parameter estimators for case-cohort data requires numerical approaches. Using the minorization-maximization principle, which is a versatile tool for constructing optimization algorithms, we develop two quadratic-upper-bound algorithms for estimations in the Cox model under case-cohort design. The proposed algorithms are monotonic and reliably converge to the weighted estimators considered. These algorithms involve the inversion of the derived upper-bound matrix only one time in the whole process, and the upper-bound matrix is independent of both parameter and weight functions. These features make the proposed algorithms have simple update and low per-iterative cost, especially to large-dimensional problems. We conduct simulation studies and real data examples to illustrate the performance of the proposed algorithms, and compare them to Newton’s method.},
  archive      = {J_SII},
  author       = {Ding, Jieli and Zhang, Jiaqian and Feng, Yanqin and Du, Yuxuan},
  doi          = {10.4310/22-SII736},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {459-474},
  shortjournal = {Stat. Interface},
  title        = {Quadratic upper bound algorithms for estimation under cox model in case-cohort studies},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly informative priors and prior-data conflict checking
for likelihood-free inference. <em>SII</em>, <em>16</em>(3), 445–457.
(<a href="https://doi.org/10.4310/22-SII733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian likelihood-free inference, which is used to perform Bayesian inference when the likelihood is intractable, enjoys an increasing number of important scientific applications. However, many aspects of a Bayesian analysis become more challenging in the likelihood-free setting. One example of this is prior-data conflict checking, where the goal is to assess whether the information in the data and the prior are inconsistent. Conflicts of this kind are important to detect, since they may reveal problems in an investigator’s understanding of what are relevant values of the parameters, and can result in sensitivity of Bayesian inferences to the prior. Here we consider methods for prior-data conflict checking which are applicable regardless of whether the likelihood is tractable or not. In constructing our checks, we consider checking statistics based on prior-to-posterior Kullback–Leibler divergences. The checks are implemented using mixture approximations to the posterior distribution and closed-form approximations to Kullback–Leibler divergences for mixtures, which make Monte Carlo approximation of reference distributions for calibration computationally feasible. When prior-data conflicts occur, it is useful to consider weakly informative prior specifications in alternative analyses as part of a sensitivity analysis. As a main application of our methodology, we develop a technique for searching for weakly informative priors in likelihood-free inference, where the notion of a weakly informative prior is formalized using prior-data conflict checks. The methods are demonstrated in three examples.},
  archive      = {J_SII},
  author       = {Chakraborty, Atlanta and Nott, David J. and Evans, Michael},
  doi          = {10.4310/22-SII733},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {445-457},
  shortjournal = {Stat. Interface},
  title        = {Weakly informative priors and prior-data conflict checking for likelihood-free inference},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An iterative algorithm with adaptive weights and sparse
laplacian shrinkage for regression problems. <em>SII</em>,
<em>16</em>(3), 433–443. (<a
href="https://doi.org/10.4310/22-SII732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the regression problem with correlation structures among covariates. We propose an iterative algorithm, named Adaptive Sparse Laplacian Shrinkage (AdaSLS). This algorithm bases on a graph-constrained regularization. In each iteration, an adaptive weight is fitted within the feature space obtained from the previous step. Under suitable regularity conditions, AdaSLS obtains the correct feature set and accurate estimation with high probability. Its bias decay at an exponential rate. Numerical comparisons show that AdaSLS improves the accuracy of both variable selection and estimation. We also apply the proposed algorithm on a gene microarray dataset and a chimeric protein dataset, obtaining meaningful results.},
  archive      = {J_SII},
  author       = {Chen, Xingyu and Yang, Yuehan},
  doi          = {10.4310/22-SII732},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {433-443},
  shortjournal = {Stat. Interface},
  title        = {An iterative algorithm with adaptive weights and sparse laplacian shrinkage for regression problems},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage multivariate dynamic linear models to extract
environmental and climate signals in coastal ecosystem data.
<em>SII</em>, <em>16</em>(3), 419–431. (<a
href="https://doi.org/10.4310/22-SII731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In environmental time series the presence of missing data, desire for multiple modeling structures, non-simultaneous data streams and computationally costly inference in highly parameterized model structures bring major challenges. In this work, we describe how multistage dynamic linear model (DLM) structures can be used to concomitantly describe long-term patterns, infer missing data, test predictive relationships, and altogether facilitate model development where multiple objectives and data streams may exist. We demonstrate the utility of this modeling approach with long-term data from Narragansett Bay (NB), Rhode Island, USA which has undergone major ecological changes including reductions in anthropogenic nutrient pollution. In a first stage, DLMs were used both to interpolate missing data and describe changes in both seasonality and long-term trend for nitrogenous nutrients and size structure of phytoplankton communities. These models revealed a long-term decline in large phytoplankton, and intensifying seasonal blooms for smaller phytoplankton. In a second modeling stage, parameters with associated uncertainty from stage 1 were used as covariates to test how features of the nitrogen series impacted phytoplankton. Conditional on the posterior inference of predictors modeled in stage 1, the dynamic regression revealed a newly discovered seasonal dependence of large phytoplankton on nitrogen sources.},
  archive      = {J_SII},
  author       = {Strock, Jacob and Puggioni, Gavino and Menden-Deuer, Susanne},
  doi          = {10.4310/22-SII731},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {419-431},
  shortjournal = {Stat. Interface},
  title        = {Two-stage multivariate dynamic linear models to extract environmental and climate signals in coastal ecosystem data},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple hypotheses testing on dependent count data with
covariate effects. <em>SII</em>, <em>16</em>(3), 409–417. (<a
href="https://doi.org/10.4310/22-SII728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamics in the sequence of count data is usually not only affected by the underlying hidden states to be detected, but also quite likely associated with other static or dynamically changing covariates. The multiple hypotheses testing procedure developed here takes these covariates into consideration by the Poisson regression model. Also, a hidden Markov process is applied to model the switches between the null and non-null states as well as the dependence across counts. All model parameters are estimated through Bayesian computation. While a simple distribution is assumed on the null state, the observation distribution under the non-null state usually requires more flexibility. Here a mixture of parametric distributions is assumed. The number of mixture components is decided by model selection criteria, including the Bayesian Information Criterion as well as marginal likelihood methods. Simulation studies are carried out to evaluate the performance of the proposed model and that of the model selection methods. The real data example shows the application of the proposed model and its inference goal differs from the previous testing procedures with no covariate effects considered.},
  archive      = {J_SII},
  author       = {Su, Weizhe and Wang, Xia},
  doi          = {10.4310/22-SII728},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {409-417},
  shortjournal = {Stat. Interface},
  title        = {Multiple hypotheses testing on dependent count data with covariate effects},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compressing recurrent neural network models through
principal component analysis. <em>SII</em>, <em>16</em>(3), 397–407. (<a
href="https://doi.org/10.4310/22-SII727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, deep learning-based neural network models, such as recurrent neural networks (RNNs) and long shortterm memory (LSTM) architecture, are considered state-ofthe- art solutions to most of the problems associated with the effective execution of tasks in the field of natural language processing (NLP). However, a large number of parameters and significantly high memory complexity are required to ensure the effective application of such models, thereby increasing the difficulty of deploying such models in embedded systems, such as those used in mobile devices and tablets. In this study, we propose a technique for compressing RNN-based models through principal component analysis. Our proposed compression approach begins with the embedding layer, after which it progresses to the final output layer. For each target layer, we propose a principal component analysis approach for reducing the dimensions in the two-dimensional (2D) estimated weight matrix. Through this approach, we develop a reduced model structure with fewer parameters than those of the benchmark model. Additionally, our proposed approach ensures improved prediction accuracy compared to that of the benchmark model. Moreover, we propose a novel parameter-initialization method based on the score matrix of the principal component. We evaluate the effectiveness of our proposed method by conducting experiments on various NLP-related tasks, such as text classification and language translation, and datasets. The results of our experiments are significantly encouraging, as they pertain to the compression of RNN models through principal component analysis.},
  archive      = {J_SII},
  author       = {Qi, Haobo and Cao, Jingxuan and Chen, Shichong and Zhou, Jing},
  doi          = {10.4310/22-SII727},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {397-407},
  shortjournal = {Stat. Interface},
  title        = {Compressing recurrent neural network models through principal component analysis},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation in exponential family regression based on linked
data contaminated by mismatch error. <em>SII</em>, <em>16</em>(3),
379–396. (<a href="https://doi.org/10.4310/22-SII726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of matching records in multiple files can be a challenging and error-prone task. Linkage error can considerably affect subsequent statistical analysis based on the resulting linked file. Several recent papers have studied post-linkage linear regression analysis with the response variable in one file and the covariates in a second file from the perspective of the “Broken Sample Problem” and “Permuted Data”. In this paper, we present an extension of this line of research to exponential family response given the assumption of a small to moderate number of mismatches. A method based on observation-specific offsets to account for potential mismatches and $\ell_1$-penalization is proposed, and its statistical properties are discussed. We also present sufficient conditions for the recovery of the correct correspondence between covariates and responses if the regression parameter is known. The proposed approach is compared to established baselines, namely the methods by Lahiri–Larsen and Chambers, both theoretically and empirically based on synthetic and real data. The results indicate that substantial improvements over those methods can be achieved even if only limited information about the linkage process is available.},
  archive      = {J_SII},
  author       = {Wang, Zhenbang and Ben-David, Emanuel and Slawski, Martin},
  doi          = {10.4310/22-SII726},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {379-396},
  shortjournal = {Stat. Interface},
  title        = {Estimation in exponential family regression based on linked data contaminated by mismatch error},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semi-supervised density peaks clustering algorithm.
<em>SII</em>, <em>16</em>(3), 363–377. (<a
href="https://doi.org/10.4310/22-SII725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC) is a density-based unsupervised clustering algorithm with the advantages of fast clustering capacity for arbitrary shape data and easy implementation without iteration. However, in practice, a small amount of label information might be partially available but not sufficient to be used to generate supervised learning. Semi-supervised clustering is often adopted to incorporate such partial information. In this paper, a novel semisupervised density peaks clustering algorithm (SS‑DPC) is proposed to extend the classical density peaks clustering algorithm to the semi-supervised clustering. In contrast to DPC, SS‑DPC uses prior information in the form of class labels to guide the learning process for improved clustering. SS‑DPC is a semi-supervised clustering that can handle data with a small number of labels. First, SS‑DPC identifies possible cluster centers based on labeled and unlabeled data automatically. Then, to incorporate partial information, virtual labels are brought in to integrate the partial information with identified centers in a uniform framework. Moreover, labeled data are used to initialize the semi-supervised clustering process to maintain the correctness of prior information in the clustering procedure. Subsequently, the nearest-point-based method is used to detect the labels of non-center unlabeled data. Finally, a step-by-step mergence strategy is introduced to generate more reasonable results. Experiments on eight UCI datasets illustrate that the proposed semi-supervised clustering algorithm yields promising clustering results.},
  archive      = {J_SII},
  author       = {Wang, Yuanyuan and Jing, Bingyi},
  doi          = {10.4310/22-SII725},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {363-377},
  shortjournal = {Stat. Interface},
  title        = {A semi-supervised density peaks clustering algorithm},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing attributable effects hypotheses with an application
to the oregon health insurance experiment. <em>SII</em>, <em>16</em>(3),
349–361. (<a href="https://doi.org/10.4310/22-SII724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following a randomized trial, the sum of the differences in the outcomes for the treated units compared to the outcome that would have been observed if the same units had been assigned to the control condition is known as the attributable effect. Most previous methods on testing hypotheses about the attributable effect require the outcome to be binary or ordinal. In this paper, we use a simple approximation to the distribution of a carefully selected test statistic under the hypothesis that the attributable effect is zero to expand attributable effects inference for count and continuous data. The method is efficient and performs well in a variety of simulations. We demonstrate the method using a large medical insurance field experiment.},
  archive      = {J_SII},
  author       = {Fredrickson, Mark M. and Chen, Yuguo},
  doi          = {10.4310/22-SII724},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {349-361},
  shortjournal = {Stat. Interface},
  title        = {Testing attributable effects hypotheses with an application to the oregon health insurance experiment},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empirical likelihood-based portmanteau tests for
autoregressive moving average models with possible infinite variance
innovations. <em>SII</em>, <em>16</em>(2), 337–347. (<a
href="https://doi.org/10.4310/22-SII761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is an important task in the literature to check whether a fitted autoregressive moving average (ARMA) model is adequate, while the currently used tests may suffer from the size distortion problem when the underlying autoregressive models have low persistence. To fill this gap, this paper proposes two empirical likelihood-based portmanteau tests. The first one is naive but can serve as a benchmark, and the second is for the case with infinite variance innovations. The asymptotic distributions under the null hypothesis are derived under mild moment conditions, and their usefulness is demonstrated by simulation experiments and two real data examples.},
  archive      = {J_SII},
  author       = {Liu, Xiaohui and Fan, Donghui and Zhang, Xu and Liu, Catherine},
  doi          = {10.4310/22-SII761},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {337-347},
  shortjournal = {Stat. Interface},
  title        = {Empirical likelihood-based portmanteau tests for autoregressive moving average models with possible infinite variance innovations},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive clustering and feature selection for categorical
time series using interpretable frequency-domain features. <em>SII</em>,
<em>16</em>(2), 319–335. (<a
href="https://doi.org/10.4310/22-SII755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SII},
  author       = {Bruce, Scott A.},
  doi          = {10.4310/22-SII755},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {319-335},
  shortjournal = {Stat. Interface},
  title        = {Adaptive clustering and feature selection for categorical time series using interpretable frequency-domain features},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uniform consistency for local fitting of time series
non-parametric regression allowing for discrete-valued response.
<em>SII</em>, <em>16</em>(2), 305–318. (<a
href="https://doi.org/10.4310/22-SII745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local linear kernel fitting is a popular nonparametric technique for modelling nonlinear time series data. Investigations into it, although extensively made for continuousvalued case, are still rare for the time series that are discrete-valued. In this paper, we propose and develop the uniform consistency of local linear maximum likelihood (LLML) fitting for time series regression allowing response to be discrete-valued under $\beta$-mixing dependence condition. Specifically, the uniform consistency of LLML estimators is established under time series conditional exponential family distributions with aid of a beta-mixing empirical process through local estimating equations. The rate of convergence is also provided under mild conditions. Performances of the proposed method are demonstrated by a Monte-Carlo simulation study and an application to COVID-19 data. There is a huge potential for the developed theory contributing to further development of discrete-valued response semiparametric time series models.},
  archive      = {J_SII},
  author       = {Peng, Rong and Lu, Zudi},
  doi          = {10.4310/22-SII745},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {305-318},
  shortjournal = {Stat. Interface},
  title        = {Uniform consistency for local fitting of time series non-parametric regression allowing for discrete-valued response},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hankel low-rank approximation and completion in time series
analysis and forecasting: A brief review. <em>SII</em>, <em>16</em>(2),
287–303. (<a href="https://doi.org/10.4310/22-SII735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we offer a review and bibliography of work on Hankel low-rank approximation and completion, with particular emphasis on how this methodology can be used for time series analysis and forecasting.We begin by describing possible formulations of the problem and offer commentary on related topics and challenges in obtaining globally optimal solutions. Key theorems are provided, and the paper closes with some expository examples.},
  archive      = {J_SII},
  author       = {Gillard, Jonathan and Usevich, Konstantin},
  doi          = {10.4310/22-SII735},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {287-303},
  shortjournal = {Stat. Interface},
  title        = {Hankel low-rank approximation and completion in time series analysis and forecasting: A brief review},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling water table depth using singular spectrum analysis.
<em>SII</em>, <em>16</em>(2), 279–286. (<a
href="https://doi.org/10.4310/22-SII734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of countries are facing or will face a serious water crisis. As a consequence, we observe a deterioration in the water quality such as the drop in the water table and a salinity increase. Therefore, it is highly recommended to conduct a regular monitoring program on groundwater levels in order to sustain this source. Water table depth (WTD) is an index of water availability that influences many soil characteristics. Consequently, there are concerns with WTD in both time and space. This paper shows how to build a model for water table depth using Singular Spectrum Analysis (SSA). The study area is located in the Ghahavand plain in the Hamedan province, western Iran. We used water table depth records that were collected by Hamedan regional water authority as part of their monitoring system program. The data were obtained monthly by measuring WTD of about 200 wells within the study area in the period between 1988 and 2016. There were many errors, inconsistencies and missing cells in the data file. So, we started with improving data quality and filling the missing cells. The other problem with the data was related to the well samples that have changed during the study horizon. Classically, we take a simple average on the observations at each time point to build a univariate time series. However, a descriptive analysis revealed that the heterogeneity in the value of the WTD in the study area has increased over time. So, we used box plot components to build model for WTD. We used both univariate SSA and multivariate SSA to capture the information within the box plot components. The performance of the proposal was accessed by using both in sample fitting errors and out of sample forecasting errors. The results suggest that the new approach provides an attractive alternative to the classical approach.},
  archive      = {J_SII},
  author       = {Mahmoudvand, Rahim and Barati, Mehrdad and Seif, Asghar and Ranjbaran, Sahar and Canas Rodrigues, Paulo},
  doi          = {10.4310/22-SII734},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {279-286},
  shortjournal = {Stat. Interface},
  title        = {Modeling water table depth using singular spectrum analysis},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate hidden semi-markov models for dynamic
connectivity analysis in resting-state fMRI. <em>SII</em>,
<em>16</em>(2), 259–277. (<a
href="https://doi.org/10.4310/22-SII730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a study on adolescent mental health, we conduct a dynamic connectivity analysis using resting-state functional magnetic resonance imaging (fMRI) data. A dynamic connectivity analysis investigates how the interactions between different regions of the brain, represented by the different dimensions of a multivariate time series, change over time. HiddenMarkov models (HMMs) and hidden semi-Markov models (HSMMs) are common analytic approaches for conducting dynamic connectivity analyses. However, existing approaches for HSMMs are limited in their ability to incorporate covariate information. In this work, we approximate an HSMM using an HMM for modeling multivariate time series data. The approximate HSMM (aHSMM) model allows one to explicitly model dwell-time distributions that are available to HSMMs, while maintaining the theoretical and methodological advances that are available to HMMs. We conducted a simulation study to show the performance of the aHSMM relative to other approaches. Finally, we used the aHSMM to conduct a dynamic connectivity analysis, where we showed how dwell-time distributions vary across the severity of non-suicidal self-injury (NSSI) in adolescents. The aHSMM allowed us to identify states that have greater dwell-times for those with moderate or severe NSSI.},
  archive      = {J_SII},
  author       = {Fiecas, Mark B. and Coffman, Christian and Xu, Meng and Hendrickson, Timothy J. and Mueller, Bryon A. and Klimes-Dougan, Bonnie and Cullen, Kathryn R.},
  doi          = {10.4310/22-SII730},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {259-277},
  shortjournal = {Stat. Interface},
  title        = {Approximate hidden semi-markov models for dynamic connectivity analysis in resting-state fMRI},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smooth online parameter estimation for time varying VAR
models with application to rat local field potential activity data.
<em>SII</em>, <em>16</em>(2), 227–257. (<a
href="https://doi.org/10.4310/22-SII729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series data appear often as realizations of non-stationary processes where the covariance matrix or spectral matrix smoothly evolve over time. Most of the current approaches estimate the time-varying spectral properties only retrospectively – that is, after the entire data has been observed. Retrospective estimation is a major limitation in many adaptive control applications where it is important to estimate these properties and detect changes in the system as they happen in real-time. To overcome this limitation, we develop an online estimation procedure that gives a real-time update of the time-varying parameters as new observations arrive. One approach to modeling nonstationary time series is to fit time-varying vector autoregressive models (tv-VAR). However, one major obstacle in online estimation of such models is the computational cost due to the high-dimensionality of the parameters. Existing methods such as the Kalman filter or local least squares are feasible. However, they are not always suitable because they provide noisy estimates and can become prohibitively costly as the dimension of the time series increases. In our brain signal application, it is critical to develop a robust method that can estimate, in real-time, the properties of the underlying stochastic process, in particular, the spectral brain connectivity measures. For these reasons we propose a new smooth online parameter estimation approach (SOPE) that has the ability to control for the smoothness of the estimates with a reasonable computational complexity. Consequently, the models are fit in real-time even for high dimensional time series.We demonstrate that our proposed SOPE approach is as good as the Kalman filter in terms of mean-squared error for small dimensions. However, unlike the Kalman filter, the SOPE has lower computational cost and hence scalable for higher dimensions. Finally, we apply the SOPE method to local field potential activity data from the hippocampus of a rat performing an odor sequence memory task. As demonstrated in the video, the proposed SOPE method is able to capture the dynamics of the connectivity as the rat samples the different odor stimuli.},
  archive      = {J_SII},
  author       = {El Yaagoubi Bourakna, Anass and Pinto, Marco and Fortin, Norbert and Ombao, Hernando},
  doi          = {10.4310/22-SII729},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {227-257},
  shortjournal = {Stat. Interface},
  title        = {Smooth online parameter estimation for time varying VAR models with application to rat local field potential activity data},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified recurrent forecasting in singular spectrum analysis
using kalman filter and its application for bicoid signal extraction.
<em>SII</em>, <em>16</em>(2), 217–225. (<a
href="https://doi.org/10.4310/22-SII723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important topics in Drosophila melanogaster is statistical analysis of bicoid protein gradient. The bicoid protein gradient plays an important role in the segmentation stage of embryo development in the head and thorax and also has considerable noise. Therefore, it has been considered by many researchers. In this paper the state space model and Kalman filter algorithms are used for noise elimination and smoothing bicoid gene expression. The state-space allows the unobserved variables, each with a specific interpretation, to be included in the estimate with the observed model and can be analyzed using the Kalman filter algorithm. Then, the less noise bicoid gene expression are used for forecast by singular spectrum analysis (SSA) method. The results with strong evidence indicate that the proposed method can be considered as a powerful technique in the analysis and prediction of gene expression measurements.},
  archive      = {J_SII},
  author       = {Zabihi Moghadam, Reza and Yarmohammadi, Masoud and Hassani, Hossein},
  doi          = {10.4310/22-SII723},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {217-225},
  shortjournal = {Stat. Interface},
  title        = {Modified recurrent forecasting in singular spectrum analysis using kalman filter and its application for bicoid signal extraction},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Least absolute deviations estimation for nonstationary
vector autoregressive time series models with pure unit roots.
<em>SII</em>, <em>16</em>(2), 199–216. (<a
href="https://doi.org/10.4310/21-SII721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper derives the asymptotic distribution of the least absolute deviations estimator for nonstationary vector autoregressive time series models with pure unit roots under mild conditions. As this distribution has a complicated form, many commonly used bootstrap techniques cannot be directly applied. To tackle this problem, we propose a novel hybrid bootstrap method by combining the classical wild bootstrap and the method in [ 17 ]. We establish the asymptotic validity of the proposed method and further apply it to construct three bootstrapping panel unit root tests. Monte Carlo experiments support the validity of our inference procedure in finite samples. The usefulness of the proposed panel unit root tests is demonstrated via analyses of real economic and financial data sets.},
  archive      = {J_SII},
  author       = {Zheng, Yao and Wu, Jianhong and Li, Wai Keung and Li, Guodong},
  doi          = {10.4310/21-SII721},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {199-216},
  shortjournal = {Stat. Interface},
  title        = {Least absolute deviations estimation for nonstationary vector autoregressive time series models with pure unit roots},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantile recurrent forecasting in singular spectrum analysis
for stock price monitoring. <em>SII</em>, <em>16</em>(2), 189–197. (<a
href="https://doi.org/10.4310/21-SII720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring of near real-time price movement is necessary for data-driven decision making in opening and closing positions for day traders and scalpers. This can be done effectively by constructing a movement path based on forecast distribution of stock prices. High frequency trading data are generally noisy, nonlinear and nonstationary in nature. We develop a quantile recurrent forecasting algorithm via the recurrent algorithm of singular spectrum analysis that can be implemented for any type of time series data. When applied to median forecasting of deterministic and shortand long-memory processes, our quantile recurrent forecast overlaps the true signal. By estimating only the signal dimension number of parameters, this method can construct a recurrent formula by including many lag periods. We apply this method to obtain median forecasts for Facebook, Microsoft, and SNAP’s intraday and daily closing prices. Both for intraday and daily closing prices, the quantile recurrent forecasts produce lower mean absolute deviation from original prices compared to bootstrap median forecasts. We also demonstrate the tracing of price movement over forecast distribution that can be used to monitor stock prices for trading strategy development.},
  archive      = {J_SII},
  author       = {Khan, Atikur R. and Hassani, Hossein},
  doi          = {10.4310/21-SII720},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {189-197},
  shortjournal = {Stat. Interface},
  title        = {Quantile recurrent forecasting in singular spectrum analysis for stock price monitoring},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Study of impact of COVID-19 on industrial production indices
using singular spectrum analysis. <em>SII</em>, <em>16</em>(2), 181–188.
(<a href="https://doi.org/10.4310/21-SII719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the impact of the COVID-19 pandemic on 8 different indices of industrial production (IIPs) for three major European countries: France, Germany, and the UK. The analysis is based on applying a combination of Singular Spectrum Analysis (SSA) algorithms, in a way that allows for the proper separation of the trend and seasonal subcycles of the IIPs. The main purpose is to illustrate how to carry out the procedure of the correct decomposition by SSA for the specific series. The accurately extracted trends are analysed and the influence of the pandemic is calculated. The results confirm that necessary goods, such as food and utilities, have low income elasticity of demand since the effect of COVID-19 is negligible for these IIPs. However, for the IIPs of less essential products, the negative impact is much more extreme, although the severity varies depending on several factors, which also aligns with the economic theory.},
  archive      = {J_SII},
  author       = {Borodich Suarez, Sofia and Pepelyshev, Andrey},
  doi          = {10.4310/21-SII719},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {181-188},
  shortjournal = {Stat. Interface},
  title        = {Study of impact of COVID-19 on industrial production indices using singular spectrum analysis},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online change-point detection for a transient change.
<em>SII</em>, <em>16</em>(2), 163–179. (<a
href="https://doi.org/10.4310/21-SII718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a popular online change-point problem of detecting a transient change in distributions of independent random variables. For this change-point problem, several change-point procedures are formulated and some advanced results for a particular procedure are surveyed. Some new approximations for the average run length to false alarm are offered and the power of these procedures for detecting a transient change in mean of a sequence of normal random variables is compared.},
  archive      = {J_SII},
  author       = {Noonan, Jack},
  doi          = {10.4310/21-SII718},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {163-179},
  shortjournal = {Stat. Interface},
  title        = {Online change-point detection for a transient change},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of signals by monte carlo singular spectrum
analysis: Multiple testing. <em>SII</em>, <em>16</em>(1), 147–157. (<a
href="https://doi.org/10.4310/21-SII715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of a signal in a noisy time series using Monte Carlo singular spectrum analysis (MC-SSA) is studied from the statistical viewpoint. The MC-SSA test consists of simultaneous testing of several hypotheses related to the presence of different frequencies. The multiple MC-SSA test procedure is constructed to control the family-wise error rate. The technique to control both the type I and the type II errors and also to compare criteria is proposed to study several versions of MC-SSA.},
  archive      = {J_SII},
  author       = {Golyandina, Nina},
  doi          = {10.4310/21-SII715},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {147-157},
  shortjournal = {Stat. Interface},
  title        = {Detection of signals by monte carlo singular spectrum analysis: Multiple testing},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The elliptical ornstein–uhlenbeck process. <em>SII</em>,
<em>16</em>(1), 133–146. (<a
href="https://doi.org/10.4310/21-SII714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the elliptical Ornstein–Uhlenbeck (OU) process, which is a generalisation of the well-known univariate OU process to bivariate time series. This process maps out elliptical stochastic oscillations over time in the complex plane, which are observed in many applications of coupled bivariate time series. The appeal of the model is that elliptical oscillations are generated using one simple first order stochastic differential equation (SDE), whereas alternative models require more complicated vectorised or higher order SDE representations. The second useful feature is that parameter estimation can be performed semi-parametrically in the frequency domain using the Whittle Likelihood. We determine properties of the model including the conditions for stationarity, and the geometrical structure of the elliptical oscillations. We demonstrate the utility of the model by measuring periodic and elliptical properties of Earth’s polar motion.},
  archive      = {J_SII},
  author       = {Sykulski, Adam and Olhede, Sofia and Sykulska-Lawrence, Hanna},
  doi          = {10.4310/21-SII714},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {133-146},
  shortjournal = {Stat. Interface},
  title        = {The elliptical Ornstein–Uhlenbeck process},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank signal subspace: Parameterization, projection and
signal estimation. <em>SII</em>, <em>16</em>(1), 117–132. (<a
href="https://doi.org/10.4310/21-SII709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper contains several theoretical results related to the weighted nonlinear least-squares problem for low-rank signal estimation, which can be considered as a Hankel structured low-rank approximation problem. A parameterization of the subspace of low-rank time series connected with generalized linear recurrence relations (GLRRs) is described and its features are investigated. It is shown how the obtained results help to describe the tangent plane, prove optimization problem features and construct stable algorithms for solving low-rank approximation problems. For the latter, a stable algorithm for constructing the projection onto a subspace of time series that satisfy a given GLRR is proposed and justified. This algorithm is utilized for a new implementation of the known Gauss–Newton method using the variable projection approach. The comparison by stability and computational cost is performed theoretically and with the help of an example.},
  archive      = {J_SII},
  author       = {Zvonarev, Nikita and Golyandina, Nina},
  doi          = {10.4310/21-SII709},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {117-132},
  shortjournal = {Stat. Interface},
  title        = {Low-rank signal subspace: Parameterization, projection and signal estimation},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Study of automatic choice of parameters for forecasting in
singular spectrum analysis. <em>SII</em>, <em>16</em>(1), 109–116. (<a
href="https://doi.org/10.4310/21-SII707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Singular spectrum analysis (SSA) is a popular tool for analysing and forecasting time series. The SSA forecasting algorithms have two parameters which should be chosen by the researcher or using the so-called automatic choice based on the root mean squared errors (RMSE) of retrospective forecasts. We study the sensitivity of the RMSE and investigate the reliability of the automatic choice of parameters for forecasting monthly temperature and humidity recorded at three meteorological stations in Oman.},
  archive      = {J_SII},
  author       = {Al Marhoobi, Safia and Pepelyshev, Andrey},
  doi          = {10.4310/21-SII707},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {109-116},
  shortjournal = {Stat. Interface},
  title        = {Study of automatic choice of parameters for forecasting in singular spectrum analysis},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoSpec: Detection of narrowband frequency changes in time
series. <em>SII</em>, <em>16</em>(1), 97–108. (<a
href="https://doi.org/10.4310/21-SII703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most established techniques that search for structural breaks in time series have a difficult time identifying small changes in the process, especially when looking for narrowband frequency changes. The problem is that many of the techniques assume very smooth local spectra and tend to produce overly smooth estimates. The problem of oversmoothing tends to produce spectral estimates that miss slight frequency changes because frequencies that are close together will be lumped into one frequency. The goal of this work is to develop techniques that concentrate on detecting slight frequency changes by requiring a high degree of resolution in the frequency domain.},
  archive      = {J_SII},
  author       = {Stoffer, David S.},
  doi          = {10.4310/21-SII703},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {97-108},
  shortjournal = {Stat. Interface},
  title        = {AutoSpec: Detection of narrowband frequency changes in time series},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust conditional spectral analysis of replicated time
series. <em>SII</em>, <em>16</em>(1), 81–96. (<a
href="https://doi.org/10.4310/21-SII698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical second-order spectral analysis, which is based on the Fourier transform of the autocovariance functions, focuses on summarizing the oscillatory behaviors of a time series. However, this type of analysis is subject to two major limitations: first, being covariance-based, it cannot captures oscillatory information beyond the second moment, such as time-irreversibility and kurtosis, and cannot accommodate heavy-tail dependence and infinite variance; second, focusing on a single time series, it is unable to quantify the association between multiple time series and other covariates of interests. In this article, we propose a novel nonparametric approach to the spectral analysis of multiple time series and the associated covariates. The procedure is based on the copula spectral density kernel, which inherits the robustness properties of quantile regression and does not require any distributional assumptions such as the existence of finite moments. Copula spectral density kernels of different pairs are modeled jointly as a matrix to allow flexible smoothing. Through a tensor-product spline model of Cholesky components of the conditional copula spectral density matrix, the approach provides flexible nonparametric estimates of the copula spectral density matrix as nonparametric functions of frequency and covariate while preserving geometric constraints. Empirical performance is evaluated in simulation studies and illustrated through an analysis of stride interval time series.},
  archive      = {J_SII},
  author       = {Li, Zeda},
  doi          = {10.4310/21-SII698},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {81-96},
  shortjournal = {Stat. Interface},
  title        = {Robust conditional spectral analysis of replicated time series},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical dynamic PARCOR models for analysis of multiple
brain signals. <em>SII</em>, <em>16</em>(1), 69–79. (<a
href="https://doi.org/10.4310/21-SII699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an efficient hierarchical model for inferring latent structure underlying multiple non-stationary time series. The proposed model describes the time-varying behavior of multiple time series in the partial autocorrelation domain, which results in a lower dimensional representation, and consequently computationally faster inference, than those required by models in the time and/or frequency domains, such as time-varying autoregressive models, which are commonly used in practice. We illustrate the performance of the proposed hierarchical dynamic PARCOR models and corresponding Bayesian inferential procedures in the context of analyzing multiple brain signals recorded simultaneously during specific experimental settings or clinical studies. The proposed approach allows us to efficiently obtain posterior summaries of the time-frequency characteristics of the multiple time series, as well as those summarizing their common underlying structure.},
  archive      = {J_SII},
  author       = {Zhao, Wenjie and Prado, Raquel},
  doi          = {10.4310/21-SII699},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {69-79},
  shortjournal = {Stat. Interface},
  title        = {Hierarchical dynamic PARCOR models for analysis of multiple brain signals},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyses of the impact of country specific macro risk
variables on gold futures contract and its position as an asset class:
Evidence from india. <em>SII</em>, <em>16</em>(1), 57–67. (<a
href="https://doi.org/10.4310/21-SII697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the dependence of gold futures prices on macro risk factors using a multiple linear regression model. Recently introduced uncertainty indexes such as geopolitical risk index and economic policy uncertainty index are included in this study. We also examine the investment nature of gold futures contract among other assets. The results provide insights on the influence of these interrelated macro economic variables on a financial derivative contract in an emerging economy and its unique position in portfolio allocation and are aimed to help practitioners and policy makers.},
  archive      = {J_SII},
  author       = {Nargunam, Rupel and Wei, William W. S. and Anuradha, N.},
  doi          = {10.4310/21-SII697},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {57-67},
  shortjournal = {Stat. Interface},
  title        = {Analyses of the impact of country specific macro risk variables on gold futures contract and its position as an asset class: Evidence from india},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing threshold effect in single-index models.
<em>SII</em>, <em>16</em>(1), 43–56. (<a
href="https://doi.org/10.4310/21-SII694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the supremum-type score test for the single-index model against a threshold single-index model. It is shown that the test weakly converges a maxima of a Gaussian process under the null hypothesis. The bootstrap method is used to tackle the bias problem and provide the $p$-values of our test statistic. Simulations are carried out to assess the performance of our procedure and real data examples are given for its illustration.},
  archive      = {J_SII},
  author       = {Gao, Zhaoxing and Mi, Zichuan and Ling, Shiqing},
  doi          = {10.4310/21-SII694},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {43-56},
  shortjournal = {Stat. Interface},
  title        = {Testing threshold effect in single-index models},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting industrial production indices with a new
singular spectrum analysis forecasting algorithm. <em>SII</em>,
<em>16</em>(1), 31–42. (<a
href="https://doi.org/10.4310/21-SII693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing time series analysis and forecasting approaches struggle to produce accurate results in application to time series with complex trend, such as those commonly displayed by indices of industrial production (IIPs). In this study, a new version of the Singular Spectrum Analysis (SSA) technique is developed, namely the Separate Trend and Seasonality (SSA-STS) forecasting algorithm. Its performance is compared to those of benchmark, classical times series forecasting methods, including Basic SSA (the core version of SSA), ARIMA, Exponential Smoothing (ETS) and Neural Network (NN). The methods in this study are applied to both simulated and real data. The latter includes twenty four monthly series of seasonally unadjusted IIPs of various sectors for the UK, Germany and France. Using the out-of-sample forecasts, the results of this newly developed SSA-STS algorithm were compared to the other aforementioned forecasting schemes by the means of pooled Root-Mean-Square-Error (RMSE). The pooling is done based on the number of steps ahead the forecasts extend, allowing for the performance of the methods to be evaluated on short and long horizons. The Kolmogorov–Smirnov Predictive Accuracy (KSPA) statistical test is applied to certify whether the errors produced by SSA-STS are statistically significantly smaller than those of all the benchmark methods. Since this new technique is based on separate trend and seasonality forecasting, it overcomes the difficulties in forecasting series with complex trends and seasonality, thus demonstrating a clear advantage over other methods in such particular cases.},
  archive      = {J_SII},
  author       = {Borodich Suarez, Sofia and Heravi, Saeed and Pepelyshev, Andrey},
  doi          = {10.4310/21-SII693},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {31-42},
  shortjournal = {Stat. Interface},
  title        = {Forecasting industrial production indices with a new singular spectrum analysis forecasting algorithm},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized gaussian time series model for increments of EEG
data. <em>SII</em>, <em>16</em>(1), 17–29. (<a
href="https://doi.org/10.4310/21-SII692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new strictly stationary time series model with marginal generalized Gaussian distribution and exponentially decaying autocorrelation function for modeling of increments of electroencephalogram (EEG) data collected from Ugandan children during coma from cerebral malaria. The model inherits its appealing properties from the strictly stationary strong mixing Markovian diffusion with invariant generalized Gaussian distribution (GGD). The GGD parametrization used in this paper comprises some famous light-tailed distributions (e.g., Laplace and Gaussian) and some well known and widely applied heavy-tailed distributions (e.g., Student). Two versions of this model fit to the data from each EEG channel. In the first model, marginal distributions is from the light-tailed GGD sub-family, and the distribution parameters were estimated using quasi-likelihood approach. In the second model, marginal distributions is heavy-tailed (Student), and the tail index was estimated using the approach based on the empirical scaling function. The estimated parameters from models across EEG channels were explored as potential predictors of neurocognitive outcomes of these children 6 months after recovering from illness. Several of these parameters were shown to be important predictors even after controlling for neurocognitive scores immediately following cerebral malaria illness and traditional blood and cerebrospinal fluid biomarkers collected during hospitalization.},
  archive      = {J_SII},
  author       = {Leonenko, Nikolai N. and Salinger, Zeljka and Sikorskii, Alla and Suvak, Nenad and Boivin, Michael},
  doi          = {10.4310/21-SII692},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {17-29},
  shortjournal = {Stat. Interface},
  title        = {Generalized gaussian time series model for increments of EEG data},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On dual-asymmetry linear double AR models. <em>SII</em>,
<em>16</em>(1), 3–16. (<a
href="https://doi.org/10.4310/21-SII691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a dual-asymmetry linear double autoregressive (DA-LDAR) model that can allow for asymmetric effects in both the conditional location and volatility components of time series data. The strict stationarity is discussed for the new model, for which a sufficient condition is established. A self-weighted exponential quasi-maximum likelihood estimator (EQMLE) is proposed for the DALDAR model, and a mixed portmanteau test for goodness-of-fit is constructed based on the self-weighted EQMLE. It is noteworthy that all the asymptotic properties for estimation and testing are established without any moment condition on the data process, which makes the new model and its inference tools applicable for heavy-tailed data. Since all inference tools need to estimate the unknown density function of innovations, we employ a random-weighting bootstrap method to facilitate accurate inference and show its asymptotic validity. Simulation studies provide support for theoretical results, and an empirical application to NASDAQ Composite Index illustrates the usefulness of the new model.},
  archive      = {J_SII},
  author       = {Tan, Songhua and Zhu, Qianqian},
  doi          = {10.4310/21-SII691},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {3-16},
  shortjournal = {Stat. Interface},
  title        = {On dual-asymmetry linear double AR models},
  volume       = {16},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
