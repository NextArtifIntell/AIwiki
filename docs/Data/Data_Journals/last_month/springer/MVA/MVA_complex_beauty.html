<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva---23">MVA - 23</h2>
<ul>
<li><details>
<summary>
(2025). Adversarial learning for unguided single depth map
completion of indoor scenes. <em>MVA</em>, <em>36</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s00138-024-01652-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single depth map completion in the absence of any guidance from color images is a challenging, ill-posed problem in computer vision. Most of the conventional depth map completion approaches rely on information extracted from the corresponding color image and require heavy computations and optimization-based postprocessing functions, which cannot yield results in real time. Successful application of generative adversarial networks has led to significant progress in several computer vision problems including, color image inpainting. However, contrasting local and non-local features of depth maps compared to color images prevents the direct application of deep learning models designed for color image inpainting to depth map completion. Motivated by these challenges, in this work we propose to use deep adversarial learning to derive plausible estimates of missing depth information in a single degraded observation without any guidance from the corresponding RGB frame and any postprocessing. Different types of depth map degradations, such as simulated random and textual missing pixels as well as contiguous large holes found in Kinect depth maps, are effectively handled to reconstruct clean depth maps. An ablation study is also performed to investigate the contribution of our adversarial network architecture towards the recovery of missing scene depth information. We carry out an illustrative experimental analysis on the NYU-Depth V2 dataset and perform zero-shot generalization on the Middlebury and Matterport3D datasets, comparing our proposed method with several state-of-the-art algorithms. The experimental results demonstrate robustness and efficacy of the proposed approach.},
  archive      = {J_MVA},
  author       = {Medhi, Moushumi and Ranjan Sahay, Rajiv},
  doi          = {10.1007/s00138-024-01652-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adversarial learning for unguided single depth map completion of indoor scenes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A computer vision system for recognition and defect
detection for reusable containers. <em>MVA</em>, <em>36</em>(2), 1–19.
(<a href="https://doi.org/10.1007/s00138-024-01636-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small load carriers (SLCs) are standardized reusable containers used to transport and protect customer goods in many manufacturers. Throughout the life cycle of the SLCs, they will be collected, manually checked for defects (wear, cracks, and residue on the surface), and cleaned by specialized logistic companies. Human operators in small to medium-sized companies manually evaluate the defects due to the variety and degree of possible defects and varying customer needs. This manual evaluation is not scalable and prone to errors. This work aims to fill this gap by proposing a computer vision system that can recognize the SLC type for inventory management and perform defect detection automatically. First, we develop a camera portal, consisting of standard components, that capture the relevant surfaces of the SLC. A labeled dataset of 17,530 images of 34 different SLCs with their defect status was recorded using this camera portal. We trained a classification model (ConvNeXt) using our dataset to predict the different types of SLCs achieving 100% class prediction accuracy. For defect detection, we explore eight state-of-the-art (SOTA) anomaly detection models that achieved high rankings in the MVTec industrial anomaly detection benchmark. These models are trained using default hyperparameters and the two highest-scoring models were chosen and fine-tuned. The best-fine-tuned models based on “Area under the Receiver Operating Characteristic Curve (AUROC)” are PatchCore (0.811) and DRAEM (0.748). These results indicate that there is still potential for improvement in the automation of defect detection of SLCs.},
  archive      = {J_MVA},
  author       = {Wahyudi, Vincent and Ziegler, Cedric C. and Frieß, Matthias and Schramm, Stefan and Lang, Constantin and Eberhardt, Lars and Freund, Fabian and Dobhan, Alexander and Storath, Martin},
  doi          = {10.1007/s00138-024-01636-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A computer vision system for recognition and defect detection for reusable containers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-enhanced feature mapping network for
visible-infrared person re-identification. <em>MVA</em>, <em>36</em>(2),
1–17. (<a href="https://doi.org/10.1007/s00138-024-01646-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-Identification (VI-ReID) plays a pivotal role in surveillance systems, enabling the accurate identification of individuals across varying times and locations. Traditional methods struggle in low-light conditions, which motivates our research. We introduce an Attention-Enhanced Feature Mapping Network (AEFMNet) that addresses both intra-modal and inter-modal discrepancies. Our AEFMNet employs an Attention-based Feature Fusion Module (AFFM) to enhance global feature representation and a GCN-based Feature Mapping Module (GFMM) to reduce cross-modal feature gaps. The proposed network is further strengthened by a Joint Training Algorithm (JTA) that integrates multi-scale local and global features, enhancing cross-modal matching accuracy. Our approach achieves advanced performance on three large-scale data sets, demonstrating its effectiveness and robustness.},
  archive      = {J_MVA},
  author       = {Liu, Shuaiyi and Han, Ke},
  doi          = {10.1007/s00138-024-01646-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Attention-enhanced feature mapping network for visible-infrared person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Region gradient-guided diffusion model for underwater image
enhancement. <em>MVA</em>, <em>36</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s00138-024-01647-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is a critical challenge in marine visual perception and underwater robotics due to complex aquatic environments that severely degrade image quality. This paper introduces the region gradient-guided diffusion model (RGGDM), a novel framework that addresses the fundamental trade-off between local detail preservation and global consistency in UIE. RGGDM innovatively integrates a region gradient-guided mechanism with a hybrid Swin-ConvNeXt architecture, introducing a spatially adaptive denoising process governed by gradient discrepancies between input and target images. We propose a learnable parameter $$\delta $$ that dynamically modulates denoising intensity, focusing computational resources on semantically salient regions. Our approach is underpinned by rigorous mathematical analysis, demonstrating convergence properties under mild assumptions and providing theoretical guarantees for the model’s stability and effectiveness. The synergistic combination of Swin Transformer and ConvNeXt enhances feature representation, significantly improving both perceptual quality and pixel-level accuracy. Extensive experiments on benchmark datasets demonstrate RGGDM’s superior performance, consistently outperforming state-of-the-art methods across multiple evaluation metrics. Notably, RGGDM achieves a peak signal-to-noise ratio (PSNR) of 25.48 dB and an underwater image quality measure (UIQM) of 4.37 on the UIEB dataset. Furthermore, enhanced images show substantial improvements in downstream tasks such as SIFT feature matching, with an average increase of 132.19% in matching points. These results underscore RGGDM’s potential in advancing underwater visual perception and its broader implications for marine robotics and environmental monitoring applications.},
  archive      = {J_MVA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s00138-024-01647-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Region gradient-guided diffusion model for underwater image enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-core token mixer: A novel approach for underwater
image enhancement. <em>MVA</em>, <em>36</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01651-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is critical in various applications, including marine biology research, underwater archaeology, and autonomous underwater vehicle (AUV) navigation. The unpredictable nature of underwater environments frequently leads to degradation in contrast, color, and perceptual visual quality. Previous methods using the single receptive field to extract features are not capable of handling varying light conditions, which hinders detail preservation, color correction, and image quality improvement. To address these challenges, we propose Multi Core Token Mixer (MCTM) by introducing a distinctive multi-core mechanism. This mechanism is adept at extracting varied receptive fields, thereby enabling the model to capture the degradation at different scales caused by inhomogeneous underwater conditions. We performed experiments on three datasets (UIEB, EUVP, and UFO-120), and MCTM consistently outperforms existing models in image enhancement, color correction, and perceptual visual quality. Our work sets a new standard in the field and emphasizes the promise held by task-specific architectures that harness the power of Transformer models to tackle domain-specific challenges, particularly in UIE.},
  archive      = {J_MVA},
  author       = {Xu, Tianrun and Xu, Shiyuan and Chen, Xue and Chen, Feng and Li, Hongjue},
  doi          = {10.1007/s00138-024-01651-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-core token mixer: A novel approach for underwater image enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A concept-aware explainability method for convolutional
neural networks. <em>MVA</em>, <em>36</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01653-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Convolutional Neural Networks (CNN) outperform the classical models in a wide range of Machine Vision applications, their restricted interpretability and their lack of comprehensibility in reasoning, generate many problems such as security, reliability, and safety. Consequently, there is a growing need for research to improve explainability and address their limitations. In this paper, we propose a concept-based method, called Concept-Aware Explainability (CAE) to provide a verbal explanation for the predictions of pre-trained CNN models. A new measure, called detection score mean, is introduced to quantify the relationship between the filters of the model and a set of pre-defined concepts. Based on the detection score mean values, we define sorted lists of Concept-Aware Filters (CAF) and Filter-Activating Concepts (FAC). These lists are used to generate explainability reports, where we can explain, analyze, and compare models in terms of the concepts embedded in the image. The proposed explainability method is compared to the state-of-the-art methods to explain Resnet18 and VGG16 models, pre-trained on ImageNet and Places365-Standard datasets. Two popular metrics, namely, the number of unique detectors and the number of detecting filters, are used to make a quantitative comparison. Superior performances are observed for the suggested CAE, when compared to Network Dissection (NetDis) (Bau et al., in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017), Net2Vec (Fong and Vedaldi, in: Paper presented at IEEE conference on computer vision and pattern recognition (CVPR), 2018), and CLIP-Dissect (CLIP-Dis) (Oikarinen and Weng, in: The 11th international conference on learning representations (ICLR), 2023) methods.},
  archive      = {J_MVA},
  author       = {Gurkan, Mustafa Kagan and Arica, Nafiz and Yarman Vural, Fatos T.},
  doi          = {10.1007/s00138-024-01653-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A concept-aware explainability method for convolutional neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end unsupervised learning of latent-space clustering
for image segmentation via fully dense-UNet and fuzzy c-means loss.
<em>MVA</em>, <em>36</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01654-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental unsupervised approach in machine learning for grouping tasks. Image segmentation is one of the main applications of clustering and a preliminary requirement for most high-level applications in computer vision and scene understanding. However, parameter tuning requirements of conventional unsupervised image segmentation approaches limit their application. Deep learning approaches are capable of diverse and discriminate feature learning, however supervised learning paradigm and computational complexity of deep neural networks (DNNs) induces bottlenecks for real-time applications. We present unsupervised learning paradigm for fully dense-UNet (FDU-Net) model training with loss constraints: Semantic loss, Fuzzy C-means Clustering (FCM) loss, and Total Variation (TV) loss. Semantic loss works by selecting maximum activation class for each pixel spatial location and Simple Linear Iterative Clustering (SLIC)-based spatial refinement provides a coherent feature representation for model optimisation. FCM loss is based on the objective function of the conventional unsupervised Fuzzy C-means algorithm loss function. TV loss computes and minimises the spatial discontinuities in the FDU-Net activation maps. Loss constraints operate in tandem to ensure the control of false positives and false negatives. We conduct extensive experiments to compare our proposed method with unsupervised conventional and contemporary deep learning-driven (DL) methods. We experimentally demonstrate that the proposed method yields competitive quantitative and, most importantly, qualitative segmentation results, on the unseen images from the BSDS500 benchmark dataset. During inference, the segmentation quality of the proposed approach results is more significant than the contemporary DL-based and conventional clustering methods while reducing the computation cost by several folds.},
  archive      = {J_MVA},
  author       = {Khan, Zubair and Khan, Tehreem and Sattar, Mohsin and Yang, Jie},
  doi          = {10.1007/s00138-024-01654-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {End-to-end unsupervised learning of latent-space clustering for image segmentation via fully dense-UNet and fuzzy C-means loss},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WTT: Combining wavelet transform with transformer for remote
sensing image super-resolution. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01655-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, most deep learning-based super-resolution techniques primarily operate in the spatial domain, utilizing similar methods to process high- and low-frequency information in images. However, this often results in edge blurring. To address this issue, this paper introduces a novel structure that integrates wavelet transform and transformer mechanisms. The proposed method effectively segregates high- and low-frequency image information via discrete wavelet transform (DWT) and learns their correlations through a self-attention mechanism to enhance super-resolution outcomes. Specifically, the input image/feature is decomposed into four frequency domain components using DWT, which are concatenated to form a full-frequency domain feature map. A high-frequency feature map is constructed from three of these components. A new feature map is then generated using multi-head self-attention, with the full-frequency domain feature map serving as the query and value, and the high-frequency feature map as the key. The output feature map is produced by applying inverse DWT, with the new feature map serving as the low-frequency component and the original high-frequency components retained. Additionally, a parallel 1 × 1 convolution filter is employed to minimize information loss. Furthermore, a super-resolution network for remote sensing images is constructed by combining wavelet transform and transformer, incorporating hierarchical residual connections to enable the network to focus on learning high-frequency information. Experimental results on a publicly available remote sensing dataset demonstrate the superiority of the proposed method compared to existing approaches.},
  archive      = {J_MVA},
  author       = {Liu, Jingyi and Yang, Xiaomin},
  doi          = {10.1007/s00138-024-01655-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {WTT: Combining wavelet transform with transformer for remote sensing image super-resolution},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environmental factors-aware two-stream GCN for
skeleton-based behavior recognition. <em>MVA</em>, <em>36</em>(2), 1–12.
(<a href="https://doi.org/10.1007/s00138-024-01656-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of human behavior recognition, modeling human skeletons as spatio-temporal graphs using Graph Convolutional Networks (GCNs) has achieved outstanding performance. Existing GCN-based methods typically focus on the two-dimensional or three-dimensional features of the skeleton. However, the same action may represent different behaviors in different environments, making it suboptimal to consider only skeleton features for behavior recognition tasks with diverse scenarios. To simultaneously account for both skeleton features and environmental factors that influence human behaviors, this study proposed a novel two-stream Graph Convolutional Network, 2S-EGCN, which incorporates environmental factors for human behavior recognition. In this network, we designed an innovative environmental factor sampling strategy that samples fixed-scale environmental factors from variable-scale feature maps. To better integrate environmental factors with skeleton features, we further developed a Skeleton-Environment Interaction Module. This module uses a specific feature fusion method to combine environmental factors with skeleton features, allowing for the modeling of both pure skeleton information and skeleton information fused with environmental factors, thus improving behavior recognition accuracy. Extensive experiments conducted on the large Kinetics dataset demonstrate that our model outperforms the state-of-the-art, improving top-1 accuracy by 1.71–55.61% and achieving top-5 accuracy of 93.41%.},
  archive      = {J_MVA},
  author       = {Li, Zhuoran and Yan, Lianshan and Li, Hua and Wang, Yu},
  doi          = {10.1007/s00138-024-01656-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Environmental factors-aware two-stream GCN for skeleton-based behavior recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry-induced ambiguity in orientation estimation from
RGB images. <em>MVA</em>, <em>36</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01657-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of object orientation from RGB images is a core component in many modern computer vision pipelines. Traditional techniques mostly predict a single orientation per image, learning a one-to-one mapping between images and rotations. However, when objects exhibit rotational symmetries, they can appear identical from multiple viewpoints. This induces ambiguity in the estimation problem, making images map to rotations in a one-to-many fashion. In this paper, we explore several ways of addressing this problem. In doing so, we specifically consider algorithms that can map an image to a range of multiple rotation estimates, accounting for symmetry-induced ambiguity. Our contributions are threefold. Firstly, we create a data set with annotated symmetry information that covers symmetries induced through self-occlusion. Secondly, we compare and evaluate various learning strategies for multiple-hypothesis prediction models applied to orientation estimation. Finally, we propose to model orientation estimation as a binary classification problem. To this end, based on existing work from the field of shape reconstruction, we design a neural network that can be sampled to reconstruct the full range of ambiguous rotations for a given image. Quantitative evaluation on our annotated data set demonstrates its performance and motivates our design choices.},
  archive      = {J_MVA},
  author       = {Bertens, Tijn and Caasenbrood, Brandon and Saccon, Alessandro and Jalba, Andrei},
  doi          = {10.1007/s00138-024-01657-6},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Symmetry-induced ambiguity in orientation estimation from RGB images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ipdm: Identity preserving diffusion model for face sketch
and photo synthesis. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01658-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face sketch and photo synthesis is widely applied in industry and information fields, such as entertainment business and heterogeneous face retrieval. The key challenge lies in completing a face transformation with both good visual effects and face identity preservation. However, existing methods are still difficult to obtain a good synthesis due to the large model gap between the two different face domains. Recently, diffusion models have achieved great success in image synthesis, which allows us to extend its application in such a face generation task. Thus, we propose IPDM, which constructs a mapping of latent representation for domain-adaptive face features. The other proposed IDP utilizes auxiliary features to correct the latent features through their directions and supplementary identity information, so that the generation can keep face identity unchanged. The various evaluation results show that our method is superior to state-of-the-art methods in both identity preservation and visual effects.},
  archive      = {J_MVA},
  author       = {Tang, Duoxun and Jiang, Xinhang and Zhang, Ying and Dai, Yuhang and Lin, Ye},
  doi          = {10.1007/s00138-024-01658-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ipdm: Identity preserving diffusion model for face sketch and photo synthesis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personvit: Large-scale self-supervised vision transformer
for person re-identification. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01659-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (ReID) aims to retrieve relevant individuals in non-overlapping camera images and has a wide range of applications in the field of public safety. In recent years, with the development of Vision Transformer (ViT) and self-supervised learning techniques, the performance of person ReID based on self-supervised pre-training has been greatly improved. Person ReID requires extracting highly discriminative local fine-grained features of the human body, while traditional ViT is good at extracting context-related global features, making it difficult to focus on local human body features. To this end, this article introduces the recently emerged Masked Image Modeling (MIM) self-supervised learning method into person ReID, and effectively extracts high-quality global and local features through large-scale unsupervised pre-training by combining masked image modeling and discriminative contrastive learning, and then conducts supervised fine-tuning training in the person ReID task. This person feature extraction method based on ViT with masked image modeling (PersonViT) has the good characteristics of unsupervised, scalable, and strong generalization capabilities, overcoming the problem of difficult annotation in supervised person ReID, and achieves state-of-the-art results on publicly available benchmark datasets, including MSMT17, Market1501, DukeMTMC-reID, and Occluded-Duke. The code and pre-trained models of the PersonViT method are released at https://github.com/hustvl/PersonViT to promote further research in the person ReID field.},
  archive      = {J_MVA},
  author       = {Hu, Bin and Wang, Xinggang and Liu, Wenyu},
  doi          = {10.1007/s00138-025-01659-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Personvit: Large-scale self-supervised vision transformer for person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversified image style transfer—approaches, new methods and
directed variability control. <em>MVA</em>, <em>36</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s00138-025-01660-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image style transfer is to automatically redraw an input image in the style of another image, such as an artist’s painting. The disadvantage of conventional stylization algorithms is the uniqueness of result. If the user is not satisfied with the way the style was transferred, he has no option to remake the stylization. The paper provides an overview of existing style transfer methods that generate diverse results after each run and proposes two new methods. The first method enables diversity by concatenating a random vector into inner image representation inside the neural network and by reweighting image features accordingly in the loss function. The second method allows diverse stylizations by passing the stylized image through orthogonal transformations, which impact the way the target style is transferred. These blocks are trained to replicate patterns from additional pattern images, which serve as additional input and provide an interpretable way to control stylization variability for the end user. Qualitative and quantitative comparisons demonstrate that both methods are capable to generate different stylizations with higher variability achieved by the second method. The code of both methods is available on github.},
  archive      = {J_MVA},
  author       = {Ustyuzhanin, Alexander and Kitov, Victor and Kitov, Vladimir},
  doi          = {10.1007/s00138-025-01660-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Diversified image style transfer—approaches, new methods and directed variability control},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional cascaded multimodal attention for multiple
choice visual question answering. <em>MVA</em>, <em>36</em>(2), 1–16.
(<a href="https://doi.org/10.1007/s00138-025-01661-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a rapidly advancing field that aims to develop systems capable of answering questions based on image content. Performance of a VQA model largely depends on the effective integration of multimodal data. A sparsity-based Bidirectional Cascaded Multimodal Attention network has been proposed in this paper. This model leverages bidirectional attention between image and text modalities, enabling a deeper contextual understanding of one modality through the other. To encourage the focus of attention mechanism on the most relevant regions in the input, sparsity has been introduced in these interactions. In multiple choice VQA, answer options contain important context, and incorporating them with multimodal features using attention results in a comprehensive feature representation. The performance of the proposed model is assessed using the multiple-choice Visual7W dataset. To test the generalizability of the model, a modified VQAv2 dataset is prepared and evaluated. Through extensive experiments, the model demonstrates competitive performance, effectively handling diverse question types such as “what”, “where”, “who”, “why”, and “how”. A detailed analysis of attention maps for different question types highlights how the model focuses on various input regions. Visualizations of image, text, and cross-modal attention maps reveal the key areas that contributed to the model’s decision-making process.},
  archive      = {J_MVA},
  author       = {Upadhyay, Sushmita and Tripathy, Sanjaya Shankar},
  doi          = {10.1007/s00138-025-01661-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Bidirectional cascaded multimodal attention for multiple choice visual question answering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CorFormer: A hybrid transformer-CNN architecture for
corrosion segmentation on metallic surfaces. <em>MVA</em>,
<em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01663-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of periodic corrosion inspection in steel structures cannot be overstated. However, current manual inspection approaches are fraught with challenges: they are time-consuming, subjective, and pose risks. To address these limitations, extensive research has been conducted over the past decade gauging the feasibility of Convolutional Neural Networks (CNNs) for automation of corrosion inspection. Meanwhile, Transformer networks have recently emerged as powerful tools in computer vision due to their ability to model intricate global relationships. In this paper, a novel hybrid architecture, dubbed CorFormer, is proposed for effective and efficient automation of corrosion inspection. The CorFormer network fuses Transformer and CNN layers at different stages of the encoder, which captures global context through Transformer layers while leveraging the inherent inductive bias of CNNs. To bridge the semantic gap between features generated by Transformer and CNN layers, a Semantic Gap Merger (SGM) module is introduced after each feature merge operation. The encoder is complemented by a hierarchical decoder, able to decrypt complex features at large and small scales. CorFormer is compared against state-of-the-art CNN and Transformer architectures for corrosion segmentation, and is found to outperform the best alternative by 2.7% in terms of Intersection over Union (IoU) across 10 validation data splits. Furthermore, it enables real-time inspection at an impressive rate of 28 frames per second. Rigorous statistical tests provide support for the findings presented in this study, and an extensive ablation study validates all design choices.},
  archive      = {J_MVA},
  author       = {Subedi, Abhishek and Qian, Cheng and Sadeghian, Reza and Jahanshahi, Mohammad R.},
  doi          = {10.1007/s00138-025-01663-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CorFormer: A hybrid transformer-CNN architecture for corrosion segmentation on metallic surfaces},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretability of fingerprint presentation attack
detection systems: A look at the “representativeness” of samples against
never-seen-before attacks. <em>MVA</em>, <em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01666-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, fingerprint Presentation Attack Detection systems (PADs) are primarily based on deep learning architectures subjected to massive training. However, their performance decreases to never-seen-before attacks. With the goal of contributing to explaining this issue, we hypothesized that this limited ability to generalize is due to the lack of &quot;representativeness&quot; of the samples available for the PAD training. &quot;Representativeness&quot; is treated here from a geometrical perspective: the spread of samples into the feature space, especially near the decision boundaries. In particular, we explored the possibility of adopting three-dimensionality reduction methods to make the problem affordable through visual inspection. These methods enable visual inspection and interpretation by projecting data into two-dimensional spaces, facilitating the identification of weak areas in the decision regions estimated after the training phase. Our analysis delineates the benefits and drawbacks of each dimensionality reduction method and leads us to make substantial recommendations in the crucial phase of the training design.},
  archive      = {J_MVA},
  author       = {Carta, Simone and Casula, Roberto and Orrù, Giulia and Micheletto, Marco and Marcialis, Gian Luca},
  doi          = {10.1007/s00138-025-01666-z},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Interpretability of fingerprint presentation attack detection systems: A look at the “representativeness” of samples against never-seen-before attacks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Depthanything and SAM for UIE: Exploring large model
information contributes to underwater image restoration. <em>MVA</em>,
<em>36</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s00138-025-01662-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) remains a fundamental yet challenging problem in computer vision due to the complex physics of light propagation in aquatic environments. Traditional physics-based or learning-driven approaches often need more prior knowledge and representational capacity to generalize across diverse underwater conditions. This paper presents a novel theoretical framework for leveraging large-scale pre-trained models in UIE, explicitly addressing the fundamental limitations in existing methods through principled integration of depth and semantic priors. Our key contribution is twofold: First, we establish a rigorous information-theoretic foundation that quantifies how auxiliary features from foundation models enhance the representational capacity of UIE systems, providing theoretical guarantees through PAC-Bayesian bounds on generalization performance. Second, we propose a Feature Enhancement Strategy that optimally combines depth information from DepthAnything and semantic priors from the Segment Anything Model, guided by underwater optical physics. We introduce CAB-USRI, a physics-based algorithm for both baseline and theoretical validation. Our extensive experimentation on multiple benchmark datasets demonstrates that our approach consistently outperforms state-of-the-art methods by significant margins while maintaining theoretical interpretability. Our ablation studies reveal the crucial role of depth priors in underwater scenarios, establishing a clear connection between theoretical bounds and empirical performance. This work bridges the gap between foundation models and domain-specific tasks, providing theoretical insights and practical solutions for complex image restoration problems in challenging environments.},
  archive      = {J_MVA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s00138-025-01662-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Depthanything and SAM for UIE: Exploring large model information contributes to underwater image restoration},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-based power line cables and pylons detection for low
flying aircraft. <em>MVA</em>, <em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01664-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power lines are dangerous for low-flying aircraft, especially in low-visibility conditions. Thus, a vision-based system able to analyze the aircraft’s surroundings and to provide the pilots with a “second pair of eyes” can contribute to enhancing their safety. To this end, we develop a deep learning approach to jointly detect power line cables and pylons from images captured at distances of several hundred meters by aircraft-mounted cameras. In doing so, we combine a modern convolutional architecture with transfer learning and a loss function adapted to curvilinear structure delineation. We use a single network for both detection tasks and demonstrate its performance on two benchmarking datasets. We have also integrated it within an onboard system and run it inflight. We show with our experiments that it outperforms the prior distant cable detection method by Stambler et al. (in: International Conference on Robotics and Automation, 2019) on both datasets, while also successfully detecting pylons, given their annotations are available for the data.},
  archive      = {J_MVA},
  author       = {Gwizdała, Jakub and Oner, Doruk and Roy, Soumava Kumar and Shah, Mian Akbar and Eberhard, Ad and Egorov, Ivan and Krüsi, Philipp and Yakushev, Grigory and Fua, Pascal},
  doi          = {10.1007/s00138-025-01664-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Vision-based power line cables and pylons detection for low flying aircraft},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D face parsing based on 2D CPFNet: Conformal parameterized
face parsing network. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-025-01667-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face parsing is a fundamental component of many advanced face technologies, which assigns labels to each pixel on the face data. Although three-dimensional (3D) face parsing has the potential to outperform its two-dimensional (2D) counterpart, it remains challenging due to the high cost of processing 3D mesh data. Recent works have introduced various methods for 3D surface segmentation, but their performance is still limited and they consume large amounts of memory and computation. In this paper, we propose a “3D–2D–3D” strategy for 3D face parsing. First, we transform 3D face data into a topological disk-like 2D face image containing spatial and textural information via conformal parameterization. Subsequently, we use a specific 2D deep learning network called CPFNet to achieve 2D face image semantic segmentation with multiscale technology and feature aggregation. Finally, the 2D semantic result is inversely remapped to the 3D face data to achieve 3D face parsing. Experimental results show that both CPFNet and our “3D–2D–3D” strategy accomplish high-quality 3D face parsing and outperform some 2D networks and 3D methods in both qualitative and quantitative comparisons.},
  archive      = {J_MVA},
  author       = {Yang, M. and Sun, W. and Wang, Y. and Zhou, G. and Tong, J. and Zhou, P.},
  doi          = {10.1007/s00138-025-01667-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3D face parsing based on 2D CPFNet: Conformal parameterized face parsing network},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic volume measurement using nonlinear count-lines.
<em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01668-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic volume measurements are increasingly recognized as important for improving society’s infrastructure, by functions such as managing congestion and enhancing logistics. Dedicated traffic data capture devices that use sensors embedded in the road enable accurate measurements but have the problems of high cost and limited installation locations, which make it difficult to expand the coverage of traffic volume measurements. To address this issue, the approach that combines already deployed Closed-Circuit Television (CCTV) cameras with image recognition technology has attracted attention and offers practical performance in ordinary situations. One remaining problem is that accuracy is degraded by the presence of headlight flare at nighttime and occlusion by large vehicles on busy roads. In this paper, we propose a method for measuring traffic volume that automatically sets count-lines using the Kernel Support Vector Machine (Kernel SVM) at optimal positions less affected by these issues. In addition, to make the proposal robust to illumination changes and occlusion we introduce nonlinear count-lines. Extensive experiments on Japanese road video footage shows that our method improves accuracy by $$5.9\%$$ at night and $$2.1\%$$ in situations prone to occlusion compared to the most basic fixed count-line method. Additionally, experiments on a public dataset, UA-DETRAC, demonstrate the proposal’s effectiveness in countries other than Japan.},
  archive      = {J_MVA},
  author       = {Iwao, Yuwa and Yamamoto, Yota and Yaginuma, Hideki and Taniguchi, Yukinobu},
  doi          = {10.1007/s00138-025-01668-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Traffic volume measurement using nonlinear count-lines},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting few-shot learning via selective patch embedding by
comprehensive sample analysis. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-025-01669-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of limited data samples, few-shot learning continues to pose a significant challenge. A prevalent strategy in recent times has been to pre-train models on extensive datasets and subsequently transfer them to downstream few-shot tasks, which has demonstrated efficacy in enhancing performance. However, a persistent challenge lies in the inadequacy of pre-trained models to capture the essential features of the new downstream dataset. This issue is particularly acute in images containing multiple entities, where crucial features are often overlooked yet play a pivotal role in image classification. To address this challenge, we propose an innovative local information enhancement strategy that harnesses information from all samples to capture important local features in images and integrates them with global features. The objective of our strategy is to enhance class differentiation by ensuring distinct class prototypes in the embedding space through the incorporation of local information. By integrating local information, query samples exhibit closer alignment with the prototype of their respective classes, ultimately resulting in improved classification accuracy. To further bolster the performance of few-shot classification, we have refined the pre-trained model approach and augmented the dataset. Comprehensive ablation experiments demonstrate the specific impact of our approach on enhancing the accuracy of few-shot classification.},
  archive      = {J_MVA},
  author       = {Yang, Juan and Zhang, Yuliang and Wang, Ronggui and Xue, Lixia},
  doi          = {10.1007/s00138-025-01669-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Boosting few-shot learning via selective patch embedding by comprehensive sample analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced normal estimation of point clouds via fine-grained
geometric information learning. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01671-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud normal estimation is a fundamental task in 3D computer graphics, essential for downstream applications such as surface reconstruction and semantic segmentation. While recent advances in deep learning have significantly improved normal estimation accuracy, existing methods often struggle with capturing fine-grained geometric details. In this study, we propose a novel encoder that integrates a local gradient attention module and positional encoding to better capture subtle geometric variations. By introducing the gradient attention module, we effectively capture fine-grained information along the z-axis, while positional encoding using sine and cosine functions further amplifies these variations. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art methods, achieving up to a 2.53% improvement in accuracy on PCPNet dataset. Our work not only advances normal estimation but also demonstrates its potential for surface reconstruction tasks. The code is available at https://github.com/ABc90/gam-net-normal-main .},
  archive      = {J_MVA},
  author       = {Jin, Wei and Zhou, Jun and Wang, Mingjie and Li, Nannan and Wang, Weixiao and Liu, Xiuping},
  doi          = {10.1007/s00138-025-01671-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhanced normal estimation of point clouds via fine-grained geometric information learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SNFR: Salient neighbor decoding and text feature refining
for scene text recognition. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01672-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition methods are broadly categorized into serial and parallel. Serial methods achieve superior accuracy but are slower in speed. Parallel methods offer faster speed but may sacrifice accuracy. Current methods struggle to strike a balance between accuracy and inference speed, particularly facing challenges in both accuracy and speed. Therefore, we propose a new scene text recognizer called SNFR. It includes a simple yet efficient decoder, Salient Neighbor Decoder (SND), which achieves high accuracy recognition with lower computational cost for attention map calculation. SND generates a neighbor matrix by selecting salient positions, which guides the generation of all the character attention maps. We also propose a Text Feature Refining Module (TFRM) to capture the contextual relationship of text sequences, enhancing the overall feature representation of scene text. The experimental results demonstrate that our method achieves competitive performance on standard datasets and also shows superior performance on long text recognition.},
  archive      = {J_MVA},
  author       = {Lu, Tongwei and Fan, Huageng and Chen, Yuqian and Shao, Pengyan},
  doi          = {10.1007/s00138-025-01672-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SNFR: Salient neighbor decoding and text feature refining for scene text recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
