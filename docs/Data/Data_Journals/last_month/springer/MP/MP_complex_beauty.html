<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mp---20">MP - 20</h2>
<ul>
<li><details>
<summary>
(2025). On the directional asymptotic approach in optimization
theory. <em>MP</em>, <em>209</em>(1), 859–937. (<a
href="https://doi.org/10.1007/s10107-024-02089-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a starting point of our research, we show that, for a fixed order $$\gamma \ge 1$$ , each local minimizer of a rather general nonsmooth optimization problem in Euclidean spaces is either M-stationary in the classical sense (corresponding to stationarity of order 1), satisfies stationarity conditions in terms of a coderivative construction of order $$\gamma $$ , or is asymptotically stationary with respect to a critical direction as well as order $$\gamma $$ in a certain sense. By ruling out the latter case with a constraint qualification not stronger than directional metric subregularity, we end up with new necessary optimality conditions comprising a mixture of limiting variational tools of orders 1 and $$\gamma $$ . These abstract findings are carved out for the broad class of geometric constraints and $$\gamma :=2$$ , and visualized by examples from complementarity-constrained and nonlinear semidefinite optimization. As a byproduct of the particular setting $$\gamma :=1$$ , our general approach yields new so-called directional asymptotic regularity conditions which serve as constraint qualifications guaranteeing M-stationarity of local minimizers. We compare these new regularity conditions with standard constraint qualifications from nonsmooth optimization. Further, we extend directional concepts of pseudo- and quasi-normality to arbitrary set-valued mappings. It is shown that these properties provide sufficient conditions for the validity of directional asymptotic regularity. Finally, a novel coderivative-like variational tool is used to construct sufficient conditions for the presence of directional asymptotic regularity. For geometric constraints, it is illustrated that all appearing objects can be calculated in terms of initial problem data.},
  archive      = {J_MP},
  author       = {Benko, Matúš and Mehlitz, Patrick},
  doi          = {10.1007/s10107-024-02089-w},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {859-937},
  shortjournal = {Math. Program.},
  title        = {On the directional asymptotic approach in optimization theory},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An asynchronous proximal bundle method. <em>MP</em>,
<em>209</em>(1), 825–857. (<a
href="https://doi.org/10.1007/s10107-024-02088-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a fully asynchronous proximal bundle method for solving non-smooth, convex optimization problems. The algorithm can be used as a drop-in replacement for classic bundle methods, i.e., the function must be given by a first-order oracle for computing function values and subgradients. The algorithm allows for an arbitrary number of master problem processes computing new candidate points and oracle processes evaluating functions at those candidate points. These processes share information by communication with a single supervisor process that resembles the main loop of a classic bundle method. All processes run in parallel and no explicit synchronization step is required. Instead, the asynchronous and possibly outdated results of the oracle computations can be seen as an inexact function oracle. Hence, we show the convergence of our method under weak assumptions very similar to inexact and incremental bundle methods. In particular, we show how the algorithm learns important structural properties of the functions to control the inaccuracy induced by the asynchronicity automatically such that overall convergence can be guaranteed.},
  archive      = {J_MP},
  author       = {Fischer, Frank},
  doi          = {10.1007/s10107-024-02088-x},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {825-857},
  shortjournal = {Math. Program.},
  title        = {An asynchronous proximal bundle method},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stackelberg risk preference design. <em>MP</em>,
<em>209</em>(1), 785–823. (<a
href="https://doi.org/10.1007/s10107-024-02083-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk measures are commonly used to capture the risk preferences of decision-makers (DMs). The decisions of DMs can be nudged or manipulated when their risk preferences are influenced by factors such as the availability of information about the uncertainties. This work proposes a Stackelberg risk preference design (STRIPE) problem to capture a designer’s incentive to influence DMs’ risk preferences. STRIPE consists of two levels. In the lower level, individual DMs in a population, known as the followers, respond to uncertainties according to their risk preference types. In the upper level, the leader influences the distribution of the types to induce targeted decisions and steers the follower’s preferences to it. Our analysis centers around the solution concept of approximate Stackelberg equilibrium that yields suboptimal behaviors of the players. We show the existence of the approximate Stackelberg equilibrium. The primitive risk perception gap, defined as the Wasserstein distance between the original and the target type distributions, is important in estimating the optimal design cost. We connect the leader’s optimality compromise on the cost with her ambiguity tolerance on the follower’s approximate solutions leveraging Lipschitzian properties of the lower level solution mapping. To obtain the Stackelberg equilibrium, we reformulate STRIPE into a single-level optimization problem using the spectral representations of law-invariant coherent risk measures. We create a data-driven approach for computation and study its performance guarantees. We apply STRIPE to contract design problems under approximate incentive compatibility. Moreover, we connect STRIPE with meta-learning problems and derive adaptation performance estimates of the meta-parameters.},
  archive      = {J_MP},
  author       = {Liu, Shutian and Zhu, Quanyan},
  doi          = {10.1007/s10107-024-02083-2},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {785-823},
  shortjournal = {Math. Program.},
  title        = {Stackelberg risk preference design},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding global minima via kernel approximations.
<em>MP</em>, <em>209</em>(1), 703–784. (<a
href="https://doi.org/10.1007/s10107-024-02081-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the global minimization of smooth functions based solely on function evaluations. Algorithms that achieve the optimal number of function evaluations for a given precision level typically rely on explicitly constructing an approximation of the function which is then minimized with algorithms that have exponential running-time complexity. In this paper, we consider an approach that jointly models the function to approximate and finds a global minimum. This is done by using infinite sums of square smooth functions and has strong links with polynomial sum-of-squares hierarchies. Leveraging recent representation properties of reproducing kernel Hilbert spaces, the infinite-dimensional optimization problem can be solved by subsampling in time polynomial in the number of function evaluations, and with theoretical guarantees on the obtained minimum. Given n samples, the computational cost is $$O(n^{3.5})$$ in time, $$O(n^2)$$ in space, and we achieve a convergence rate to the global optimum that is $$O(n^{-m/d + 1/2 + 3/d})$$ where m is the degree of differentiability of the function and d the number of dimensions. The rate is nearly optimal in the case of Sobolev functions and more generally makes the proposed method particularly suitable for functions with many derivatives. Indeed, when m is in the order of d, the convergence rate to the global optimum does not suffer from the curse of dimensionality, which affects only the worst-case constants (that we track explicitly through the paper).},
  archive      = {J_MP},
  author       = {Rudi, Alessandro and Marteau-Ferey, Ulysse and Bach, Francis},
  doi          = {10.1007/s10107-024-02081-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {703-784},
  shortjournal = {Math. Program.},
  title        = {Finding global minima via kernel approximations},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A normal fan projection algorithm for low-rank optimization.
<em>MP</em>, <em>209</em>(1), 681–702. (<a
href="https://doi.org/10.1007/s10107-024-02079-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise a method for minimizing a low-rank quasiconcave objective function over a polytope by first projecting the polytope’s normal fan, then using the projected fan to obtain candidate solutions. When the polytope’s maximal number of nonparallel edges is bounded by a polynomial in its dimension, our method solves the problem in time that is polynomial in the number of variables and exponential in the rank of the objective function. We discuss several problems from previous literature that can be solved efficiently using this method. In all cases, our proposed algorithm matches or improves on the running time of existing problem-specific algorithms, while providing the first polynomial-time algorithm we know of for finding a spanning tree on a graph with multiple edge weight types, such that the product of the different weight types is minimized.},
  archive      = {J_MP},
  author       = {Scott, James R. and Geunes, Joseph},
  doi          = {10.1007/s10107-024-02079-y},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {681-702},
  shortjournal = {Math. Program.},
  title        = {A normal fan projection algorithm for low-rank optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample complexity analysis for adaptive optimization
algorithms with stochastic oracles. <em>MP</em>, <em>209</em>(1),
651–679. (<a href="https://doi.org/10.1007/s10107-024-02078-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several classical adaptive optimization algorithms, such as line search and trust-region methods, have been recently extended to stochastic settings where function values, gradients, and Hessians in some cases, are estimated via stochastic oracles. Unlike the majority of stochastic methods, these methods do not use a pre-specified sequence of step size parameters, but adapt the step size parameter according to the estimated progress of the algorithm and use it to dictate the accuracy required from the stochastic oracles. The requirements on the stochastic oracles are, thus, also adaptive and the oracle costs can vary from iteration to iteration. The step size parameters in these methods can increase and decrease based on the perceived progress, but unlike the deterministic case they are not bounded away from zero due to possible oracle failures, and bounds on the step size parameter have not been previously derived. This creates obstacles in the total complexity analysis of such methods, because the oracle costs are typically decreasing in the step size parameter, and could be arbitrarily large as the step size parameter goes to 0. Thus, until now only the total iteration complexity of these methods has been analyzed. In this paper, we derive a lower bound on the step size parameter that holds with high probability for a large class of adaptive stochastic methods. We then use this lower bound to derive a framework for analyzing the expected and high probability total oracle complexity of any method in this class. Finally, we apply this framework to analyze the total sample complexity of two particular algorithms, STORM (Blanchet et al. in INFORMS J Optim 1(2):92–119, 2019) and SASS (Jin et al. in High probability complexity bounds for adaptive step search based on stochastic oracles, 2021. https://doi.org/10.48550/ARXIV.2106.06454 ), in the expected risk minimization problem.},
  archive      = {J_MP},
  author       = {Jin, Billy and Scheinberg, Katya and Xie, Miaolan},
  doi          = {10.1007/s10107-024-02078-z},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {651-679},
  shortjournal = {Math. Program.},
  title        = {Sample complexity analysis for adaptive optimization algorithms with stochastic oracles},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perseus: A simple and optimal high-order method for
variational inequalities. <em>MP</em>, <em>209</em>(1), 609–650. (<a
href="https://doi.org/10.1007/s10107-024-02075-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper settles an open and challenging question pertaining to the design of simple and optimal high-order methods for solving smooth and monotone variational inequalities (VIs). A VI involves finding $$x^\star \in {\mathcal {X}}$$ such that $$\langle F(x), x - x^\star \rangle \ge 0$$ for all $$x \in {\mathcal {X}}$$ . We consider the setting in which $$F: {\mathbb {R}}^d \rightarrow {\mathbb {R}}^d$$ is smooth with up to $$(p-1)^{\text {th}}$$ -order derivatives. For $$p = 2$$ , the cubic regularization of Newton’s method has been extended to VIs with a global rate of $$O(\epsilon ^{-1})$$ (Nesterov in Cubic regularization of Newton’s method for convex problems with constraints, Tech. rep., Université catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2006). An improved rate of $$O(\epsilon ^{-2/3}\log \log (1/\epsilon ))$$ can be obtained via an alternative second-order method, but this method requires a nontrivial line-search procedure as an inner loop. Similarly, the existing high-order methods based on line-search procedures have been shown to achieve a rate of $$O(\epsilon ^{-2/(p+1)}\log \log (1/\epsilon ))$$ (Bullins and Lai in SIAM J Optim 32(3):2208–2229, 2022; Jiang and Mokhtari in Generalized optimistic methods for convex–concave saddle point problems, 2022; Lin and Jordan in Math Oper Res 48(4):2353–2382, 2023). As emphasized by Nesterov (Lectures on convex optimization, vol 137, Springer, Berlin, 2018), however, such procedures do not necessarily imply the practical applicability in large-scale applications, and it is desirable to complement these results with a simple high-order VI method that retains the optimality of the more complex methods. We propose a $$p^{\text {th}}$$ -order method that does not require any line search procedure and provably converges to a weak solution at a rate of $$O(\epsilon ^{-2/(p+1)})$$ . We prove that our $$p^{\text {th}}$$ -order method is optimal in the monotone setting by establishing a lower bound of $$\Omega (\epsilon ^{-2/(p+1)})$$ under a generalized linear span assumption. A restarted version of our $$p^{\text {th}}$$ -order method attains a linear rate for smooth and $$p^{\text {th}}$$ -order uniformly monotone VIs and another restarted version of our $$p^{\text {th}}$$ -order method attains a local superlinear rate for smooth and strongly monotone VIs. Further, the similar $$p^{\text {th}}$$ -order method achieves a global rate of $$O(\epsilon ^{-2/p})$$ for solving smooth and nonmonotone VIs satisfying the Minty condition. Two restarted versions attain a global linear rate under additional $$p^{\text {th}}$$ -order uniform Minty condition and a local superlinear rate under additional strong Minty condition.},
  archive      = {J_MP},
  author       = {Lin, Tianyi and Jordan, Michael I.},
  doi          = {10.1007/s10107-024-02075-2},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {609-650},
  shortjournal = {Math. Program.},
  title        = {Perseus: A simple and optimal high-order method for variational inequalities},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-convex scenario optimization. <em>MP</em>,
<em>209</em>(1), 557–608. (<a
href="https://doi.org/10.1007/s10107-024-02074-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scenario optimization is an approach to data-driven decision-making that has been introduced some fifteen years ago and has ever since then grown fast. Its most remarkable feature is that it blends the heuristic nature of data-driven methods with a rigorous theory that allows one to gain factual, reliable, insight in the solution. The usability of the scenario theory, however, has been restrained thus far by the obstacle that most results are standing on the assumption of convexity. With this paper, we aim to free the theory from this limitation. Specifically, we focus on the body of results that are known under the name of “wait-and-judge” and show that its fundamental achievements maintain their validity in a non-convex setup. While optimization is a major center of attention, this paper travels beyond it and into data-driven decision making. Adopting such a broad framework opens the door to building a new theory of truly vast applicability.},
  archive      = {J_MP},
  author       = {Garatti, Simone and Campi, Marco C.},
  doi          = {10.1007/s10107-024-02074-3},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {557-608},
  shortjournal = {Math. Program.},
  title        = {Non-convex scenario optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated first-order methods for a class of semidefinite
programs. <em>MP</em>, <em>209</em>(1), 503–556. (<a
href="https://doi.org/10.1007/s10107-024-02073-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new storage-optimal first-order method, CertSDP, for solving a special class of semidefinite programs (SDPs) to high accuracy. The class of SDPs that we consider, the exact QMP-like SDPs, is characterized by low-rank solutions, a priori knowledge of the restriction of the SDP solution to a small subspace, and standard regularity assumptions such as strict complementarity. Crucially, we show how to use a certificate of strict complementarity to construct a low-dimensional strongly convex minimax problem whose optimizer coincides with a factorization of the SDP optimizer. From an algorithmic standpoint, we show how to construct the necessary certificate and how to solve the minimax problem efficiently. Our algorithms for strongly convex minimax problems with inexact prox maps may be of independent interest. We accompany our theoretical results with preliminary numerical experiments suggesting that CertSDP significantly outperforms current state-of-the-art methods on large sparse exact QMP-like SDPs.},
  archive      = {J_MP},
  author       = {Wang, Alex L. and Kılınç-Karzan, Fatma},
  doi          = {10.1007/s10107-024-02073-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {503-556},
  shortjournal = {Math. Program.},
  title        = {Accelerated first-order methods for a class of semidefinite programs},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sum-of-squares relaxations for polynomial min–max problems
over simple sets. <em>MP</em>, <em>209</em>(1), 475–501. (<a
href="https://doi.org/10.1007/s10107-024-02072-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider min–max optimization problems for polynomial functions, where a multivariate polynomial is maximized with respect to a subset of variables, and the resulting maximal value is minimized with respect to the remaining variables. When the variables belong to simple sets (e.g., a hypercube, the Euclidean hypersphere, or a ball), we derive a sum-of-squares formulation based on a primal-dual approach. In the simplest setting, we provide a convergence proof when the degree of the relaxation tends to infinity and observe empirically that it can be finitely convergent in several situations. Moreover, our formulation leads to an interesting link with feasibility certificates for polynomial inequalities based on Putinar’s Positivstellensatz.},
  archive      = {J_MP},
  author       = {Bach, Francis},
  doi          = {10.1007/s10107-024-02072-5},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {475-501},
  shortjournal = {Math. Program.},
  title        = {Sum-of-squares relaxations for polynomial min–max problems over simple sets},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence rates for sums-of-squares hierarchies with
correlative sparsity. <em>MP</em>, <em>209</em>(1), 435–473. (<a
href="https://doi.org/10.1007/s10107-024-02071-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work derives upper bounds on the convergence rate of the moment-sum-of-squares hierarchy with correlative sparsity for global minimization of polynomials on compact basic semialgebraic sets. The main conclusion is that both sparse hierarchies based on the Schmüdgen and Putinar Positivstellensätze enjoy a polynomial rate of convergence that depends on the size of the largest clique in the sparsity graph but not on the ambient dimension. Interestingly, the sparse bounds outperform the best currently available bounds for the dense hierarchy when the maximum clique size is sufficiently small compared to the ambient dimension and the performance is measured by the running time of an interior point method required to obtain a bound on the global minimum of a given accuracy.},
  archive      = {J_MP},
  author       = {Korda, Milan and Magron, Victor and Ríos-Zertuche, Rodolfo},
  doi          = {10.1007/s10107-024-02071-6},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {435-473},
  shortjournal = {Math. Program.},
  title        = {Convergence rates for sums-of-squares hierarchies with correlative sparsity},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polyhedral properties of RLT relaxations of nonconvex
quadratic programs and their implications on exact relaxations.
<em>MP</em>, <em>209</em>(1), 397–433. (<a
href="https://doi.org/10.1007/s10107-024-02070-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study linear programming relaxations of nonconvex quadratic programs given by the reformulation–linearization technique (RLT), referred to as RLT relaxations. We investigate the relations between the polyhedral properties of the feasible regions of a quadratic program and its RLT relaxation. We establish various connections between recession directions, boundedness, and vertices of the two feasible regions. Using these properties, we present a complete description of the set of instances that admit an exact RLT relaxation. We then give a thorough discussion of how our results can be converted into simple algorithmic procedures to construct instances of quadratic programs with exact, inexact, or unbounded RLT relaxations.},
  archive      = {J_MP},
  author       = {Qiu, Yuzhou and Yıldırım, E. Alper},
  doi          = {10.1007/s10107-024-02070-7},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {397-433},
  shortjournal = {Math. Program.},
  title        = {Polyhedral properties of RLT relaxations of nonconvex quadratic programs and their implications on exact relaxations},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The chvátal–gomory procedure for integer SDPs with
applications in combinatorial optimization. <em>MP</em>,
<em>209</em>(1), 323–395. (<a
href="https://doi.org/10.1007/s10107-024-02069-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study the well-known Chvátal–Gomory (CG) procedure for the class of integer semidefinite programs (ISDPs). We prove several results regarding the hierarchy of relaxations obtained by iterating this procedure. We also study different formulations of the elementary closure of spectrahedra. A polyhedral description of the elementary closure for a specific type of spectrahedra is derived by exploiting total dual integrality for SDPs. Moreover, we show how to exploit (strengthened) CG cuts in a branch-and-cut framework for ISDPs. Different from existing algorithms in the literature, the separation routine in our approach exploits both the semidefinite and the integrality constraints. We provide separation routines for several common classes of binary SDPs resulting from combinatorial optimization problems. In the second part of the paper we present a comprehensive application of our approach to the quadratic traveling salesman problem (QTSP). Based on the algebraic connectivity of the directed Hamiltonian cycle, two ISDPs that model the QTSP are introduced. We show that the CG cuts resulting from these formulations contain several well-known families of cutting planes. Numerical results illustrate the practical strength of the CG cuts in our branch-and-cut algorithm, which outperforms alternative ISDP solvers and is able to solve large QTSP instances to optimality.},
  archive      = {J_MP},
  author       = {de Meijer, Frank and Sotirov, Renata},
  doi          = {10.1007/s10107-024-02069-0},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {323-395},
  shortjournal = {Math. Program.},
  title        = {The Chvátal–Gomory procedure for integer SDPs with applications in combinatorial optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized minimum 0-extension problem and discrete
convexity. <em>MP</em>, <em>209</em>(1), 279–322. (<a
href="https://doi.org/10.1007/s10107-024-02064-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a fixed finite metric space $$(V,\mu )$$ , the minimum 0-extension problem, denoted as $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ , is equivalent to the following optimization problem: minimize function of the form $$\min \nolimits _{x\in V^n} \sum _i f_i(x_i) + \sum _{ij} c_{ij}\hspace{0.5pt}\mu (x_i,x_j)$$ where $$f_i:V\rightarrow \mathbb {R}$$ are functions given by $$f_i(x_i)=\sum _{v\in V} c_{vi}\hspace{0.5pt}\mu (x_i,v)$$ and $$c_{ij},c_{vi}$$ are given nonnegative costs. The computational complexity of $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ has been recently established by Karzanov and by Hirai: if metric $$\mu $$ is orientable modular then $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ can be solved in polynomial time, otherwise $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ is NP-hard. To prove the tractability part, Hirai developed a theory of discrete convex functions on orientable modular graphs generalizing several known classes of functions in discrete convex analysis, such as $$L^\natural $$ -convex functions. We consider a more general version of the problem in which unary functions $$f_i(x_i)$$ can additionally have terms of the form $$c_{uv;i}\hspace{0.5pt}\mu (x_i,\{u,v\})$$ for $$\{u,\!\hspace{0.5pt}\hspace{0.5pt}v\}\in F$$ , where set $$F\subseteq \left( {\begin{array}{c}V\\ 2\end{array}}\right) $$ is fixed. We extend the complexity classification above by providing an explicit condition on $$(\mu ,F)$$ for the problem to be tractable. In order to prove the tractability part, we generalize Hirai’s theory and define a larger class of discrete convex functions. It covers, in particular, another well-known class of functions, namely submodular functions on an integer lattice. Finally, we improve the complexity of Hirai’s algorithm for solving $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ on orientable modular graphs.},
  archive      = {J_MP},
  author       = {Dvorak, Martin and Kolmogorov, Vladimir},
  doi          = {10.1007/s10107-024-02064-5},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {279-322},
  shortjournal = {Math. Program.},
  title        = {Generalized minimum 0-extension problem and discrete convexity},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized nash equilibrium problems with mixed-integer
variables. <em>MP</em>, <em>209</em>(1), 231–277. (<a
href="https://doi.org/10.1007/s10107-024-02063-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider generalized Nash equilibrium problems (GNEPs) with non-convex strategy spaces and non-convex cost functions. This general class of games includes the important case of games with mixed-integer variables for which only a few results are known in the literature. We present a new approach to characterize equilibria via a convexification technique using the Nikaido–Isoda function. To any given instance of the GNEP, we construct a set of convexified instances and show that a feasible strategy profile is an equilibrium for the original instance if and only if it is an equilibrium for any convexified instance and the convexified cost functions coincide with the initial ones. We develop this convexification approach along three dimensions: We first show that for quasi-linear models, where a convexified instance exists in which for fixed strategies of the opponent players, the cost function of every player is linear and the respective strategy space is polyhedral, the convexification reduces the GNEP to a standard (non-linear) optimization problem. Secondly, we derive two complete characterizations of those GNEPs for which the convexification leads to a jointly constrained or a jointly convex GNEP, respectively. These characterizations require new concepts related to the interplay of the convex hull operator applied to restricted subsets of feasible strategies and may be interesting on their own. Note that this characterization is also computationally relevant as jointly convex GNEPs have been extensively studied in the literature. Finally, we demonstrate the applicability of our results by presenting a numerical study regarding the computation of equilibria for three classes of GNEPs related to integral network flows and discrete market equilibria.},
  archive      = {J_MP},
  author       = {Harks, Tobias and Schwarz, Julian},
  doi          = {10.1007/s10107-024-02063-6},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {231-277},
  shortjournal = {Math. Program.},
  title        = {Generalized nash equilibrium problems with mixed-integer variables},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hessian barrier algorithms for non-convex conic
optimization. <em>MP</em>, <em>209</em>(1), 171–229. (<a
href="https://doi.org/10.1007/s10107-024-02062-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key problem in mathematical imaging, signal processing and computational statistics is the minimization of non-convex objective functions that may be non-differentiable at the relative boundary of the feasible set. This paper proposes a new family of first- and second-order interior-point methods for non-convex optimization problems with linear and conic constraints, combining logarithmically homogeneous barriers with quadratic and cubic regularization respectively. Our approach is based on a potential-reduction mechanism and, under the Lipschitz continuity of the corresponding derivative with respect to the local barrier-induced norm, attains a suitably defined class of approximate first- or second-order KKT points with worst-case iteration complexity $$O(\varepsilon ^{-2})$$ (first-order) and $$O(\varepsilon ^{-3/2})$$ (second-order), respectively. Based on these findings, we develop new path-following schemes attaining the same complexity, modulo adjusting constants. These complexity bounds are known to be optimal in the unconstrained case, and our work shows that they are upper bounds in the case with complicated constraints as well. To the best of our knowledge, this work is the first which achieves these worst-case complexity bounds under such weak conditions for general conic constrained non-convex optimization problems.},
  archive      = {J_MP},
  author       = {Dvurechensky, Pavel and Staudigl, Mathias},
  doi          = {10.1007/s10107-024-02062-7},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {171-229},
  shortjournal = {Math. Program.},
  title        = {Hessian barrier algorithms for non-convex conic optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated tight lyapunov analysis for first-order methods.
<em>MP</em>, <em>209</em>(1), 133–170. (<a
href="https://doi.org/10.1007/s10107-024-02061-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a methodology for establishing the existence of quadratic Lyapunov inequalities for a wide range of first-order methods used to solve convex optimization problems. In particular, we consider (i) classes of optimization problems of finite-sum form with (possibly strongly) convex and possibly smooth functional components, (ii) first-order methods that can be written as a linear system on state-space form in feedback interconnection with the subdifferentials of the functional components of the objective function, and (iii) quadratic Lyapunov inequalities that can be used to draw convergence conclusions. We present a necessary and sufficient condition for the existence of a quadratic Lyapunov inequality within a predefined class of Lyapunov inequalities, which amounts to solving a small-sized semidefinite program. We showcase our methodology on several first-order methods that fit the framework. Most notably, our methodology allows us to significantly extend the region of parameter choices that allow for duality gap convergence in the Chambolle–Pock method when the linear operator is the identity mapping.},
  archive      = {J_MP},
  author       = {Upadhyaya, Manu and Banert, Sebastian and Taylor, Adrien B. and Giselsson, Pontus},
  doi          = {10.1007/s10107-024-02061-8},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {133-170},
  shortjournal = {Math. Program.},
  title        = {Automated tight lyapunov analysis for first-order methods},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convex hulls of monomial curves, and a sparse
positivstellensatz. <em>MP</em>, <em>209</em>(1), 113–131. (<a
href="https://doi.org/10.1007/s10107-024-02060-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the closed convex hull K of a monomial curve given parametrically as $$(t^{m_1},\ldots ,t^{m_n})$$ , with the parameter t varying in an interval I. We show, using constructive arguments, that K admits a lifted semidefinite description by $$\mathcal {O}(d)$$ linear matrix inequalities (LMIs), each of size $$\left\lfloor \frac{n}{2} \right\rfloor +1$$ , where $$d= \max \{m_1,\ldots ,m_n\}$$ is the degree of the curve. On the dual side, we show that if a univariate polynomial p(t) of degree d with at most $$2k+1$$ monomials is non-negative on $${\mathbb {R}}_+$$ , then p admits a representation $$p = t^0 \sigma _0 + \cdots + t^{d-k} \sigma _{d-k}$$ , where the polynomials $$\sigma _0,\ldots ,\sigma _{d-k}$$ are sums of squares and $$\deg (\sigma _i) \le 2k$$ . The latter is a univariate positivstellensatz for sparse polynomials, with non-negativity of p being certified by sos polynomials whose degree only depends on the sparsity of p. Our results fit into the general attempt of formulating polynomial optimization problems as semidefinite problems with LMIs of small size. Such small-size descriptions are much more tractable from a computational viewpoint.},
  archive      = {J_MP},
  author       = {Averkov, Gennadiy and Scheiderer, Claus},
  doi          = {10.1007/s10107-024-02060-9},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {113-131},
  shortjournal = {Math. Program.},
  title        = {Convex hulls of monomial curves, and a sparse positivstellensatz},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of smooth parametrizations on nonconvex
optimization landscapes. <em>MP</em>, <em>209</em>(1), 63–111. (<a
href="https://doi.org/10.1007/s10107-024-02058-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop new tools to study landscapes in nonconvex optimization. Given one optimization problem, we pair it with another by smoothly parametrizing the domain. This is either for practical purposes (e.g., to use smooth optimization algorithms with good guarantees) or for theoretical purposes (e.g., to reveal that the landscape satisfies a strict saddle property). In both cases, the central question is: how do the landscapes of the two problems relate? More precisely: how do desirable points such as local minima and critical points in one problem relate to those in the other problem? A key finding in this paper is that these relations are often determined by the parametrization itself, and are almost entirely independent of the cost function. Accordingly, we introduce a general framework to study parametrizations by their effect on landscapes. The framework enables us to obtain new guarantees for an array of problems, some of which were previously treated on a case-by-case basis in the literature. Applications include: optimizing low-rank matrices and tensors through factorizations; solving semidefinite programs via the Burer–Monteiro approach; training neural networks by optimizing their weights and biases; and quotienting out symmetries.},
  archive      = {J_MP},
  author       = {Levin, Eitan and Kileel, Joe and Boumal, Nicolas},
  doi          = {10.1007/s10107-024-02058-3},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {63-111},
  shortjournal = {Math. Program.},
  title        = {The effect of smooth parametrizations on nonconvex optimization landscapes},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Level constrained first order methods for function
constrained optimization. <em>MP</em>, <em>209</em>(1), 1–61. (<a
href="https://doi.org/10.1007/s10107-024-02057-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new feasible proximal gradient method for constrained optimization where both the objective and constraint functions are given by summation of a smooth, possibly nonconvex function and a convex simple function. The algorithm converts the original problem into a sequence of convex subproblems. Formulating those subproblems requires the evaluation of at most one gradient-value of the original objective and constraint functions. Either exact or approximate subproblems solutions can be computed efficiently in many cases. An important feature of the algorithm is the constraint level parameter. By carefully increasing this level for each subproblem, we provide a simple solution to overcome the challenge of bounding the Lagrangian multipliers and show that the algorithm follows a strictly feasible solution path till convergence to the stationary point. We develop a simple, proximal gradient descent type analysis, showing that the complexity bound of this new algorithm is comparable to gradient descent for the unconstrained setting which is new in the literature. Exploiting this new design and analysis technique, we extend our algorithms to some more challenging constrained optimization problems where (1) the objective is a stochastic or finite-sum function, and (2) structured nonsmooth functions replace smooth components of both objective and constraint functions. Complexity results for these problems also seem to be new in the literature. Finally, our method can also be applied to convex function constrained problems where we show complexities similar to the proximal gradient method.},
  archive      = {J_MP},
  author       = {Boob, Digvijay and Deng, Qi and Lan, Guanghui},
  doi          = {10.1007/s10107-024-02057-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {1-61},
  shortjournal = {Math. Program.},
  title        = {Level constrained first order methods for function constrained optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
