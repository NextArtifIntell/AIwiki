<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijcv---25">IJCV - 25</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Variational rectification inference for learning
with noisy labels. <em>IJCV</em>, <em>133</em>(3), 1434. (<a
href="https://doi.org/10.1007/s11263-024-02242-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Sun, Haoliang and Wei, Qi and Feng, Lei and Hu, Yupeng and Liu, Fan and Fan, Hehe and Yin, Yilong},
  doi          = {10.1007/s11263-024-02242-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1434},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Variational rectification inference for learning with noisy labels},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on computer vision approaches
for animal tracking and modeling 2023. <em>IJCV</em>, <em>133</em>(3),
1433. (<a href="https://doi.org/10.1007/s11263-024-02241-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-024-02241-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1433},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on computer vision approaches for animal tracking and modeling 2023},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on german conference on pattern
recognition (DAGM GCPR). <em>IJCV</em>, <em>133</em>(3), 1432. (<a
href="https://doi.org/10.1007/s11263-024-02212-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Goldluecke, Bastian},
  doi          = {10.1007/s11263-024-02212-6},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1432},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on german conference on pattern recognition (DAGM GCPR)},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSKNet: A foundation lightweight backbone for remote
sensing. <em>IJCV</em>, <em>133</em>(3), 1410–1431. (<a
href="https://doi.org/10.1007/s11263-024-02247-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection, semantic segmentation and change detection, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet backbone network sets new state-of-the-art scores on standard remote sensing classification, object detection, semantic segmentation and change detection benchmarks. Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at https://github.com/zcablii/LSKNet .},
  archive      = {J_IJCV},
  author       = {Li, Yuxuan and Li, Xiang and Dai, Yimain and Hou, Qibin and Liu, Li and Liu, Yongxiang and Cheng, Ming-Ming and Yang, Jian},
  doi          = {10.1007/s11263-024-02247-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1410-1431},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LSKNet: A foundation lightweight backbone for remote sensing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified frequency-assisted transformer framework for
detecting and grounding multi-modal manipulation. <em>IJCV</em>,
<em>133</em>(3), 1392–1409. (<a
href="https://doi.org/10.1007/s11263-024-02245-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and grounding multi-modal media manipulation ( $$\hbox {DGM}^4$$ ) has become increasingly crucial due to the widespread dissemination of face forgery and text misinformation. In this paper, we present the Unified Frequency-Assisted transFormer framework, named UFAFormer, to address the $$\hbox {DGM}^4$$ problem. Unlike previous state-of-the-art methods that solely focus on the image (RGB) domain to describe visual forgery features, we additionally introduce the frequency domain as a complementary viewpoint. By leveraging the discrete wavelet transform, we decompose images into several frequency sub-bands, capturing rich face forgery artifacts. Then, our proposed frequency encoder, incorporating intra-band and inter-band self-attentions, explicitly aggregates forgery features within and across diverse sub-bands. Moreover, to address the semantic conflicts between image and frequency domains, the forgery-aware mutual module is developed to further enable the effective interaction of disparate image and frequency features, resulting in aligned and comprehensive visual forgery representations. Finally, based on visual and textual forgery features, we propose a unified decoder that comprises two symmetric cross-modal interaction modules responsible for gathering modality-specific forgery information, along with a fusing interaction module for aggregation of both modalities. The proposed unified decoder formulates our UFAFormer as a unified framework, ultimately simplifying the overall architecture and facilitating the optimization process. Experimental results on the $$\hbox {DGM}^4$$ dataset, containing several perturbations, demonstrate the superior performance of our framework compared to previous methods, setting a new benchmark in the field.},
  archive      = {J_IJCV},
  author       = {Liu, Huan and Tan, Zichang and Chen, Qiang and Wei, Yunchao and Zhao, Yao and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02245-x},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1392-1409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unified frequency-assisted transformer framework for detecting and grounding multi-modal manipulation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-VLGM: Bi-level class-severity-aware vision-language graph
matching for text guided medical image segmentation. <em>IJCV</em>,
<em>133</em>(3), 1375–1391. (<a
href="https://doi.org/10.1007/s11263-024-02246-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical reports containing specific diagnostic results and additional information not present in medical images can be effectively employed to assist image understanding tasks, and the modality gap between vision and language can be bridged by vision-language matching (VLM). However, current vision-language models distort the intra-model relation and only include class information in reports that is insufficient for segmentation task. In this paper, we introduce a novel Bi-level class-severity-aware Vision-Language Graph Matching (Bi-VLGM) for text guided medical image segmentation, composed of a word-level VLGM module and a sentence-level VLGM module, to exploit the class-severity-aware relation among visual-textual features. In word-level VLGM, to mitigate the distorted intra-modal relation during VLM, we reformulate VLM as graph matching problem and introduce a vision-language graph matching (VLGM) to exploit the high-order relation among visual-textual features. Then, we perform VLGM between the local features for each class region and class-aware prompts to bridge their gap. In sentence-level VLGM, to provide disease severity information for segmentation task, we introduce a severity-aware prompting to quantify the severity level of disease lesion, and perform VLGM between the global features and the severity-aware prompts. By exploiting the relation between the local (global) and class (severity) features, the segmentation model can include the class-aware and severity-aware information to promote segmentation performance. Extensive experiments proved the effectiveness of our method and its superiority to existing methods. The source code will be released.},
  archive      = {J_IJCV},
  author       = {Chen, Wenting and Liu, Jie and Liu, Tianming and Yuan, Yixuan},
  doi          = {10.1007/s11263-024-02246-w},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1375-1391},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bi-VLGM: Bi-level class-severity-aware vision-language graph matching for text guided medical image segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MapTRv2: An end-to-end framework for online vectorized HD
map construction. <em>IJCV</em>, <em>133</em>(3), 1352–1374. (<a
href="https://doi.org/10.1007/s11263-024-02235-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-definition (HD) map provides abundant and precise static environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. In this paper, we present Map TRansformer, an end-to-end framework for online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. To speed up convergence, we further introduce auxiliary one-to-many matching and dense supervision. The proposed method well copes with various map elements with arbitrary shapes. It runs at real-time inference speed and achieves state-of-the-art performance on both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable and robust map construction quality in complex and various driving scenes. Code and more demos are available at https://github.com/hustvl/MapTR for facilitating further studies and applications.},
  archive      = {J_IJCV},
  author       = {Liao, Bencheng and Chen, Shaoyu and Zhang, Yunchi and Jiang, Bo and Zhang, Qian and Liu, Wenyu and Huang, Chang and Wang, Xinggang},
  doi          = {10.1007/s11263-024-02235-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1352-1374},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MapTRv2: An end-to-end framework for online vectorized HD map construction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dissecting out-of-distribution detection and open-set
recognition: A critical analysis of methods and benchmarks.
<em>IJCV</em>, <em>133</em>(3), 1326–1351. (<a
href="https://doi.org/10.1007/s11263-024-02222-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting test-time distribution shift has emerged as a key capability for safely deployed machine learning models, with the question being tackled under various guises in recent years. In this paper, we aim to provide a consolidated view of the two largest sub-fields within the community: out-of-distribution (OOD) detection and open-set recognition (OSR). In particular, we aim to provide rigorous empirical analysis of different methods across settings and provide actionable takeaways for practitioners and researchers. Concretely, we make the following contributions: (i) We perform rigorous cross-evaluation between state-of-the-art methods in the OOD detection and OSR settings and identify a strong correlation between the performances of methods for them; (ii) We propose a new, large-scale benchmark setting which we suggest better disentangles the problem tackled by OOD detection and OSR, re-evaluating state-of-the-art OOD detection and OSR methods in this setting; (iii) We surprisingly find that the best performing method on standard benchmarks (Outlier Exposure) struggles when tested at scale, while scoring rules which are sensitive to the deep feature magnitude consistently show promise; and (iv) We conduct empirical analysis to explain these phenomena and highlight directions for future research. Code: https://github.com/Visual-AI/Dissect-OOD-OSR},
  archive      = {J_IJCV},
  author       = {Wang, Hongjun and Vaze, Sagar and Han, Kai},
  doi          = {10.1007/s11263-024-02222-4},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1326-1351},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dissecting out-of-distribution detection and open-set recognition: A critical analysis of methods and benchmarks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2D semantic-guided semantic scene completion. <em>IJCV</em>,
<em>133</em>(3), 1306–1325. (<a
href="https://doi.org/10.1007/s11263-024-02244-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic scene completion (SSC) aims to simultaneously perform scene completion (SC) and predict semantic categories of a 3D scene from a single depth and/or RGB image. Most existing SSC methods struggle to handle complex regions with multiple objects close to each other, especially for objects with reflective or dark surfaces. This primarily stems from two challenges: (1) the loss of geometric information due to the unreliability of depth values from sensors, and (2) the potential for semantic confusion when simultaneously predicting 3D shapes and semantic labels. To address these problems, we propose a Semantic-guided Semantic Scene Completion framework, dubbed SG-SSC, which involves Semantic-guided Fusion (SGF) and Volume-guided Semantic Predictor (VGSP). Guided by 2D semantic segmentation maps, SGF adaptively fuses RGB and depth features to compensate for the missing geometric information caused by the missing values in depth images, thus performing more robustly to unreliable depth information. VGSP exploits the mutual benefit between SC and SSC tasks, making SSC more focused on predicting the categories of voxels with high occupancy probabilities and also allowing SC to utilize semantic priors to better predict voxel occupancy. Experimental results show that SG-SSC outperforms existing state-of-the-art methods on the NYU, NYUCAD, and SemanticKITTI datasets. Models and code are available at https://github.com/aipixel/SG-SSC .},
  archive      = {J_IJCV},
  author       = {Liu, Xianzhu and Xie, Haozhe and Zhang, Shengping and Yao, Hongxun and Ji, Rongrong and Nie, Liqiang and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02244-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1306-1325},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {2D semantic-guided semantic scene completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From gaze jitter to domain adaptation: Generalizing gaze
estimation by manipulating high-frequency components. <em>IJCV</em>,
<em>133</em>(3), 1290–1305. (<a
href="https://doi.org/10.1007/s11263-024-02233-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze, as a pivotal indicator of human emotion, plays a crucial role in various computer vision tasks. However, the accuracy of gaze estimation often significantly deteriorates when applied to unseen environments, thereby limiting its practical value. Therefore, enhancing the generalizability of gaze estimators to new domains emerges as a critical challenge. A common limitation in existing domain adaptation research is the inability to identify and leverage truly influential factors during the adaptation process. This shortcoming often results in issues such as limited accuracy and unstable adaptation. To address this issue, this article discovers a truly influential factor in the cross-domain problem, i.e., high-frequency components (HFC). This discovery stems from an analysis of gaze jitter-a frequently overlooked but impactful issue where predictions can deviate drastically even for visually similar input images. Inspired by this discovery, we propose an “embed-then-suppress&quot; HFC manipulation strategy to adapt gaze estimation to new domains. Our method first embeds additive HFC to the input images, then performs domain adaptation by suppressing the impact of HFC. Specifically, the suppression is carried out in a contrasive manner. Each original image is paired with its HFC-embedded version, thereby enabling our method to suppress the HFC impact through contrasting the representations within the pairs. The proposed method is evaluated across four cross-domain gaze estimation tasks. The experimental results show that it not only enhances gaze estimation accuracy but also significantly reduces gaze jitter in the target domain. Compared with previous studies, our method offers higher accuracy, reduced gaze jitter, and improved adaptation stability, marking the potential for practical deployment.},
  archive      = {J_IJCV},
  author       = {Liu, Ruicong and Wang, Haofei and Lu, Feng},
  doi          = {10.1007/s11263-024-02233-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1290-1305},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {From gaze jitter to domain adaptation: Generalizing gaze estimation by manipulating high-frequency components},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEO: Generative latent image animator for human video
synthesis. <em>IJCV</em>, <em>133</em>(3), 1277–1289. (<a
href="https://doi.org/10.1007/s11263-024-02231-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing. Project page: https://wyhsirius.github.io/LEO-project/ .},
  archive      = {J_IJCV},
  author       = {Wang, Yaohui and Ma, Xin and Chen, Xinyuan and Chen, Cunjian and Dantcheva, Antitza and Dai, Bo and Qiao, Yu},
  doi          = {10.1007/s11263-024-02231-3},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1277-1289},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LEO: Generative latent image animator for human video synthesis},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutual prompt leaning for vision language models.
<em>IJCV</em>, <em>133</em>(3), 1258–1276. (<a
href="https://doi.org/10.1007/s11263-024-02243-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and visual sub-branches, existing prompt approaches are mainly divided into text and visual prompts. Recent text prompt methods have achieved great performance by designing input-condition prompts that encompass both text and image domain knowledge. However, roughly incorporating the same image feature into each learnable text token may be unjustifiable, as it could result in learnable text prompts being concentrated on one or a subset of characteristics. In light of this, we propose a fine-grained text prompt (FTP) that decomposes the single global image features into several finer-grained semantics and incorporates them into corresponding text prompt tokens. On the other hand, current methods neglect valuable text semantic information when building the visual prompt. Furthermore, text information contains redundant and negative category semantics. To address this, we propose a text-reorganized visual prompt (TVP) that reorganizes the text descriptions of the current image to construct the visual prompt, guiding the image branch to attend to class-related representations. By leveraging both FTP and TVP, we enable mutual prompting between the text and visual modalities, unleashing their potential to tap into the representation capabilities of VLMs. Extensive experiments on 11 classification benchmarks show that our method surpasses existing methods by a large margin. In particular, our approach improves recent state-of-the-art CoCoOp by 4.79% on new classes and 3.88% on harmonic mean over eleven classification benchmarks.},
  archive      = {J_IJCV},
  author       = {Long, Sifan and Zhao, Zhen and Yuan, Junkun and Tan, Zichang and Liu, Jiangjiang and Feng, Jingyuan and Wang, Shengsheng and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02243-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1258-1276},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mutual prompt leaning for vision language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust deep object tracking against adversarial attacks.
<em>IJCV</em>, <em>133</em>(3), 1238–1257. (<a
href="https://doi.org/10.1007/s11263-024-02226-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the vulnerability of deep neural networks (DNNs) has attracted significant attention in recent years. While recent studies on adversarial attack and defense mainly reside in a single image, few efforts have been made to perform temporal attacks against video sequences. As the temporal consistency between frames is not considered, existing adversarial attack approaches designed for static images do not perform well for deep object tracking. In this work, we generate adversarial examples on top of video sequences to improve the tracking robustness against adversarial attacks under white-box and black-box settings. To this end, we consider motion signals when generating lightweight perturbations over the estimated tracking results frame-by-frame. For the white-box attack, we generate temporal perturbations via known trackers to degrade significantly the tracking performance. We transfer the generated perturbations into unknown targeted trackers for the black-box attack to achieve transferring attacks. Furthermore, we train universal adversarial perturbations and directly add them into all frames of videos, improving the attack effectiveness with minor computational costs. On the other hand, we sequentially learn to estimate and remove the perturbations from input sequences to restore the tracking performance. We apply the proposed adversarial attack and defense approaches to state-of-the-art tracking algorithms. Extensive evaluations on large-scale benchmark datasets, including OTB, VOT, UAV123, and LaSOT, demonstrate that our attack method degrades the tracking performance significantly with favorable transferability to other backbones and trackers. Notably, the proposed defense method restores the original tracking performance to some extent and achieves additional performance gains when not under adversarial attacks.},
  archive      = {J_IJCV},
  author       = {Jia, Shuai and Ma, Chao and Song, Yibing and Yang, Xiaokang and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-024-02226-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1238-1257},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust deep object tracking against adversarial attacks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Slimmable networks for contrastive self-supervised learning.
<em>IJCV</em>, <em>133</em>(3), 1222–1237. (<a
href="https://doi.org/10.1007/s11263-024-02211-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning makes significant progress in pre-training large models, but struggles with small models. Mainstream solutions to this problem rely mainly on knowledge distillation, which involves a two-stage procedure: first training a large teacher model and then distilling it to improve the generalization ability of smaller ones. In this work, we introduce another one-stage solution to obtain pre-trained small models without the need for extra teachers, namely, slimmable networks for contrastive self-supervised learning (SlimCLR). A slimmable network consists of a full network and several weight-sharing sub-networks, which can be pre-trained once to obtain various networks, including small ones with low computation costs. However, interference between weight-sharing networks leads to severe performance degradation in self-supervised cases, as evidenced by gradient magnitude imbalance and gradient direction divergence. The former indicates that a small proportion of parameters produce dominant gradients during backpropagation, while the main parameters may not be fully optimized. The latter shows that the gradient direction is disordered, and the optimization process is unstable. To address these issues, we introduce three techniques to make the main parameters produce dominant gradients and sub-networks have consistent outputs. These techniques include slow start training of sub-networks, online distillation, and loss re-weighting according to model sizes. Furthermore, theoretical results are presented to demonstrate that a single slimmable linear layer is sub-optimal during linear evaluation. Thus a switchable linear probe layer is applied during linear evaluation. We instantiate SlimCLR with typical contrastive learning frameworks and achieve better performance than previous arts with fewer parameters and FLOPs. The code is available at https://github.com/mzhaoshuai/SlimCLR .},
  archive      = {J_IJCV},
  author       = {Zhao, Shuai and Zhu, Linchao and Wang, Xiaohan and Yang, Yi},
  doi          = {10.1007/s11263-024-02211-7},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1222-1237},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Slimmable networks for contrastive self-supervised learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking the limits of reliable prediction via generated
data. <em>IJCV</em>, <em>133</em>(3), 1195–1221. (<a
href="https://doi.org/10.1007/s11263-024-02221-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open-world recognition of safety-critical applications, providing reliable prediction for deep neural networks has become a critical requirement. Many methods have been proposed for reliable prediction related tasks such as confidence calibration, misclassification detection, and out-of-distribution detection. Recently, pre-training has been shown to be one of the most effective methods for improving reliable prediction, particularly for modern networks like ViT, which require a large amount of training data. However, collecting data manually is time-consuming. In this paper, taking advantage of the breakthrough of generative models, we investigate whether and how expanding the training set using generated data can improve reliable prediction. Our experiments reveal that training with a large quantity of generated data can eliminate overfitting in reliable prediction, leading to significantly improved performance. Surprisingly, classical networks like ResNet-18, when trained on a notably extensive volume of generated data, can sometimes exhibit performance competitive to pre-training ViT with a substantial real dataset.},
  archive      = {J_IJCV},
  author       = {Cheng, Zhen and Zhu, Fei and Zhang, Xu-Yao and Liu, Cheng-Lin},
  doi          = {10.1007/s11263-024-02221-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1195-1221},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Breaking the limits of reliable prediction via generated data},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FastComposer: Tuning-free multi-subject image generation
with localized attention. <em>IJCV</em>, <em>133</em>(3), 1175–1194. (<a
href="https://doi.org/10.1007/s11263-024-02227-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300 $$\times $$ –2500 $$\times $$ speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available here ( https://github.com/mit-han-lab/fastcomposer ).},
  archive      = {J_IJCV},
  author       = {Xiao, Guangxuan and Yin, Tianwei and Freeman, William T. and Durand, Frédo and Han, Song},
  doi          = {10.1007/s11263-024-02227-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1175-1194},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FastComposer: Tuning-free multi-subject image generation with localized attention},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lidar panoptic segmentation in an open world. <em>IJCV</em>,
<em>133</em>(3), 1153–1174. (<a
href="https://doi.org/10.1007/s11263-024-02166-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing Lidar Panoptic Segmentation (LPS) is crucial for safe deployment of autnomous vehicles. LPS aims to recognize and segment lidar points w.r.t. a pre-defined vocabulary of semantic classes, including thing classes of countable objects (e.g., pedestrians and vehicles) and stuff classes of amorphous regions (e.g., vegetation and road). Importantly, LPS requires segmenting individual thing instances (e.g., every single vehicle). Current LPS methods make an unrealistic assumption that the semantic class vocabulary is fixed in the real open world, but in fact, class ontologies usually evolve over time as robots encounter instances of novel classes that are considered to be unknowns w.r.t. thepre-defined class vocabulary. To address this unrealistic assumption, we study LPS in the Open World (LiPSOW): we train models on a dataset with a pre-defined semantic class vocabulary and study their generalization to a larger dataset where novel instances of thing and stuff classes can appear. This experimental setting leads to interesting conclusions. While prior art train class-specific instance segmentation methods and obtain state-of-the-art results on known classes, methods based on class-agnostic bottom-up grouping perform favorably on classes outside of the initial class vocabulary (i.e., unknown classes). Unfortunately, these methods do not perform on-par with fully data-driven methods on known classes. Our work suggests a middle ground: we perform class-agnostic point clustering and over-segment the input cloud in a hierarchical fashion, followed by binary point segment classification, akin to Region Proposal Network (Ren et al. NeurIPS, 2015). We obtain the final point cloud segmentation by computing a cut in the weighted hierarchical tree of point segments, independently of semantic classification. Remarkably, this unified approach leads to strong performance on both known and unknown classes.},
  archive      = {J_IJCV},
  author       = {Chakravarthy, Anirudh S. and Ganesina, Meghana Reddy and Hu, Peiyun and Leal-Taixé, Laura and Kong, Shu and Ramanan, Deva and Osep, Aljosa},
  doi          = {10.1007/s11263-024-02166-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1153-1174},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lidar panoptic segmentation in an open world},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical active learning for low-altitude drone-view
object detection. <em>IJCV</em>, <em>133</em>(3), 1140–1152. (<a
href="https://doi.org/10.1007/s11263-024-02228-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various object detection techniques are employed on drone platforms. However, the task of annotating drone-view samples is both time-consuming and laborious. This is primarily due to the presence of numerous small-sized instances to be labeled in the drone-view image. To tackle this issue, we propose HALD, a hierarchical active learning approach for low-altitude drone-view object detection. HALD extracts unlabeled image information sequentially from different levels, including point, box, image, and class, aiming to obtain a reliable indicator of image information. The point-level module is utilized to ascertain the valid count and location of instances, while the box-level module screens out reliable predictions. The image-level module selects candidate samples by calculating the consistency of valid boxes within an image, and the class-level module selects the final selected samples based on the distribution of candidate and labeled samples across different classes. Extensive experiments conducted on the VisDrone and CityPersons datasets demonstrate that HALD outperforms several other baseline methods. Additionally, we provide an in-depth analysis of each proposed module. The results show that the performance of evaluating the informativeness of samples can be effectively improved by the four hierarchical levels.},
  archive      = {J_IJCV},
  author       = {Hu, Haohao and Han, Tianyu and Wang, Yuerong and Zhong, Wanjun and Yue, Jingwei and Zan, Peng},
  doi          = {10.1007/s11263-024-02228-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1140-1152},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical active learning for low-altitude drone-view object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In search of lost online test-time adaptation: A survey.
<em>IJCV</em>, <em>133</em>(3), 1106–1139. (<a
href="https://doi.org/10.1007/s11263-024-02213-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive survey of online test-time adaptation (OTTA), focusing on effectively adapting machine learning models to distributionally different target data upon batch arrival. Despite the recent proliferation of OTTA methods, conclusions from previous studies are inconsistent due to ambiguous settings, outdated backbones, and inconsistent hyperparameter tuning, which obscure core challenges and hinder reproducibility. To enhance clarity and enable rigorous comparison, we classify OTTA techniques into three primary categories and benchmark them using a modern backbone, the Vision Transformer. Our benchmarks cover conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C, as well as real-world shifts represented by CIFAR-10.1, OfficeHome, and CIFAR-10-Warehouse. The CIFAR-10-Warehouse dataset includes a variety of variations from different search engines and synthesized data generated through diffusion models. To measure efficiency in online scenarios, we introduce novel evaluation metrics, including GFLOPs, wall clock time, and GPU memory usage, providing a clearer picture of the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, revealing that (1) transformers demonstrate heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods relies on large batch sizes, and (3) stability in optimization and resistance to perturbations are crucial during adaptation, particularly when the batch size is 1. Based on these insights, we highlight promising directions for future research. Our benchmarking toolkit and source code are available at https://github.com/Jo-wang/OTTA_ViT_survey .},
  archive      = {J_IJCV},
  author       = {Wang, Zixin and Luo, Yadan and Zheng, Liang and Chen, Zhuoxiao and Wang, Sen and Huang, Zi},
  doi          = {10.1007/s11263-024-02213-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1106-1139},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {In search of lost online test-time adaptation: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WeakCLIP: Adapting CLIP for weakly-supervised semantic
segmentation. <em>IJCV</em>, <em>133</em>(3), 1085–1105. (<a
href="https://doi.org/10.1007/s11263-024-02224-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive language and image pre-training (CLIP) achieves great success in various computer vision tasks and also presents an opportune avenue for enhancing weakly-supervised image understanding with its large-scale pre-trained knowledge. As an effective way to reduce the reliance on pixel-level human-annotated labels, weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) and produce high-quality pseudo masks. Weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) as pseudo masks, but heavily relies on inductive biases like hand-crafted priors and digital image processing methods. For the vision-language pre-trained model, i.e. CLIP, we propose a novel text-to-pixel matching paradigm for WSSS. However, directly applying CLIP to WSSS is challenging due to three critical problems: (1) the task gap between contrastive pre-training and WSSS CAM refinement, (2) lacking text-to-pixel modeling to fully utilize the pre-trained knowledge, and (3) the insufficient details owning to the $$\frac{1}{16}$$ down-sampling resolution of ViT. Thus, we propose WeakCLIP to address the problems and leverage the pre-trained knowledge from CLIP to WSSS. Specifically, we first address the task gap by proposing a pyramid adapter and learnable prompts to extract WSSS-specific representation. We then design a co-attention matching module to model text-to-pixel relationships. Finally, the pyramid adapter and text-guided decoder are introduced to gather multi-level information and integrate it with text guidance hierarchically. WeakCLIP provides an effective and parameter-efficient way to transfer CLIP knowledge to refine CAM. Extensive experiments demonstrate that WeakCLIP achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 74.0% mIoU on the val set of PASCAL VOC 2012 and 46.1% mIoU on the val set of COCO 2014. The source code and model checkpoints are released at https://github.com/hustvl/WeakCLIP .},
  archive      = {J_IJCV},
  author       = {Zhu, Lianghui and Wang, Xinggang and Feng, Jiapei and Cheng, Tianheng and Li, Yingyue and Jiang, Bo and Zhang, Dingwen and Han, Junwei},
  doi          = {10.1007/s11263-024-02224-2},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1085-1105},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {WeakCLIP: Adapting CLIP for weakly-supervised semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual face forgery detection via historical distribution
preserving. <em>IJCV</em>, <em>133</em>(3), 1067–1084. (<a
href="https://doi.org/10.1007/s11263-024-02160-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery techniques have advanced rapidly and pose serious security threats. Existing face forgery detection methods try to learn generalizable features, but they still fall short of practical application. Additionally, finetuning these methods on historical training data is resource-intensive in terms of time and storage. In this paper, we focus on a novel and challenging problem: Continual Face Forgery Detection (CFFD), which aims to efficiently learn from new forgery attacks without forgetting previous ones. Specifically, we propose a Historical Distribution Preserving (HDP) framework that reserves and preserves the distributions of historical faces. To achieve this, we use universal adversarial perturbation (UAP) to simulate historical forgery distribution, and knowledge distillation to maintain the distribution variation of real faces across different models. We also construct a new benchmark for CFFD with three evaluation protocols. Our extensive experiments on the benchmarks show that our method outperforms the state-of-the-art competitors. Our code is available at https://github.com/skJack/HDP .},
  archive      = {J_IJCV},
  author       = {Sun, Ke and Chen, Shen and Yao, Taiping and Sun, Xiaoshuai and Ding, Shouhong and Ji, Rongrong},
  doi          = {10.1007/s11263-024-02160-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1067-1084},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Continual face forgery detection via historical distribution preserving},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive fuzzy positive learning for annotation-scarce
semantic segmentation. <em>IJCV</em>, <em>133</em>(3), 1048–1066. (<a
href="https://doi.org/10.1007/s11263-024-02217-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotation-scarce semantic segmentation aims to obtain meaningful pixel-level discrimination with scarce or even no manual annotations, of which the crux is how to utilize unlabeled data by pseudo-label learning. Typical works focus on ameliorating the error-prone pseudo-labeling, e.g., only utilizing high-confidence pseudo labels and filtering low-confidence ones out. But we think differently and resort to exhausting informative semantics from multiple probably correct candidate labels. This brings our method the ability to learn more accurately even though pseudo labels are unreliable. In this paper, we propose Adaptive Fuzzy Positive Learning (A-FPL) for correctly learning unlabeled data in a plug-and-play fashion, targeting adaptively encouraging fuzzy positive predictions and suppressing highly probable negatives. Specifically, A-FPL comprises two main components: (1) Fuzzy positive assignment (FPA) that adaptively assigns fuzzy positive labels to each pixel, while ensuring their quality through a T-value adaption algorithm (2) Fuzzy positive regularization (FPR) that restricts the predictions of fuzzy positive categories to be larger than those of negative categories. Being conceptually simple yet practically effective, A-FPL remarkably alleviates interference from wrong pseudo labels, progressively refining semantic discrimination. Theoretical analysis and extensive experiments on various training settings with consistent performance gain justify the superiority of our approach. Codes are at A-FPL .},
  archive      = {J_IJCV},
  author       = {Qiao, Pengchong and Wang, Yu and Liu, Chang and Shang, Lei and Sun, Baigui and Wang, Zhennan and Zheng, Xiawu and Ji, Rongrong and Chen, Jie},
  doi          = {10.1007/s11263-024-02217-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1048-1066},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive fuzzy positive learning for annotation-scarce semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic evaluation of uncertainty calibration in
pretrained object detectors. <em>IJCV</em>, <em>133</em>(3), 1033–1047.
(<a href="https://doi.org/10.1007/s11263-024-02219-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of deep learning based computer vision, the development of deep object detection has led to unique paradigms (e.g., two-stage or set-based) and architectures (e.g., Faster-RCNN or DETR) which enable outstanding performance on challenging benchmark datasets. Despite this, the trained object detectors typically do not reliably assess uncertainty regarding their own knowledge, and the quality of their probabilistic predictions is usually poor. As these are often used to make subsequent decisions, such inaccurate probabilistic predictions must be avoided. In this work, we investigate the uncertainty calibration properties of different pretrained object detection architectures in a multi-class setting. We propose a framework to ensure a fair, unbiased, and repeatable evaluation and conduct detailed analyses assessing the calibration under distributional changes (e.g., distributional shift and application to out-of-distribution data). Furthermore, by investigating the influence of different detector paradigms, post-processing steps, and suitable choices of metrics, we deliver novel insights into why poor detector calibration emerges. Based on these insights, we are able to improve the calibration of a detector by simply finetuning its last layer.},
  archive      = {J_IJCV},
  author       = {Huseljic, Denis and Herde, Marek and Hahn, Paul and Müjde, Mehmet and Sick, Bernhard},
  doi          = {10.1007/s11263-024-02219-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1033-1047},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Systematic evaluation of uncertainty calibration in pretrained object detectors},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting class-incremental learning with pre-trained
models: Generalizability and adaptivity are all you need. <em>IJCV</em>,
<em>133</em>(3), 1012–1032. (<a
href="https://doi.org/10.1007/s11263-024-02218-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. (1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. (2) Due to the distribution gap between pre-trained and downstream datasets, PTM can be further cultivated with adaptivity via model adaptation. We propose AdaPt and mERge (Aper), which aggregates the embeddings of PTM and adapted models for classifier construction. Aper is a general framework that can be orthogonally combined with any parameter-efficient tuning method, which holds the advantages of PTM’s generalizability and adapted model’s adaptivity. (3) Additionally, considering previous ImageNet-based benchmarks are unsuitable in the era of PTM due to data overlapping, we propose four new benchmarks for assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the effectiveness of Aper with a unified and concise framework. Code is available at https://github.com/zhoudw-zdw/RevisitingCIL .},
  archive      = {J_IJCV},
  author       = {Zhou, Da-Wei and Cai, Zi-Wen and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02218-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1012-1032},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight high-speed photography built on coded exposure
and implicit neural representation of videos. <em>IJCV</em>,
<em>133</em>(3), 991–1011. (<a
href="https://doi.org/10.1007/s11263-024-02198-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for compact cameras capable of recording high-speed scenes with high resolution is steadily increasing. However, achieving such capabilities often entails high bandwidth requirements, resulting in bulky, heavy systems unsuitable for low-capacity platforms. To address this challenge, leveraging a coded exposure setup to encode a frame sequence into a blurry snapshot and subsequently retrieve the latent sharp video presents a lightweight solution. Nevertheless, restoring motion from blur remains a formidable challenge due to the inherent ill-posedness of motion blur decomposition, the intrinsic ambiguity in motion direction, and the diverse motions present in natural videos. In this study, we propose a novel approach to address these challenges by combining the classical coded exposure imaging technique with the emerging implicit neural representation for videos. We strategically embed motion direction cues into the blurry image during the imaging process. Additionally, we develop a novel implicit neural representation based blur decomposition network to sequentially extract the latent video frames from the blurry image, leveraging the embedded motion direction cues. To validate the effectiveness and efficiency of our proposed framework, we conduct extensive experiments using benchmark datasets and real-captured blurry images. The results demonstrate that our approach significantly outperforms existing methods in terms of both quality and flexibility. The code for our work is available at https://github.com/zhihongz/BDINR .},
  archive      = {J_IJCV},
  author       = {Zhang, Zhihong and Yang, Runzhao and Suo, Jinli and Cheng, Yuxiao and Dai, Qionghai},
  doi          = {10.1007/s11263-024-02198-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {991-1011},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lightweight high-speed photography built on coded exposure and implicit neural representation of videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
