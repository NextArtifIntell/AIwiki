<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MAM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mam---12">MAM - 12</h2>
<ul>
<li><details>
<summary>
(2025). Effective human oversight of AI-based systems: A signal
detection perspective on the detection of inaccurate and unfair outputs.
<em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-024-09701-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legislation and ethical guidelines around the globe call for effective human oversight of AI-based systems in high-risk contexts – that is oversight that reliably reduces the risks otherwise associated with the use of AI-based systems. Such risks may relate to the imperfect accuracy of systems (e.g., inaccurate classifications) or to ethical concerns (e.g., unfairness of outputs). Given the significant role that human oversight is expected to play in the operation of AI-based systems, it is crucial to better understand the conditions for effective human oversight. We argue that the reliable detection of errors (as an umbrella term for inaccuracies and unfairness) is crucial for effective human oversight. We then propose that Signal Detection Theory (SDT) offers a promising framework for better understanding what affects people’s sensitivity (i.e., how well they are able to detect errors) and response bias (i.e., the tendency to report errors given a perceived evidence of an error) in detecting errors. Whereas an SDT perspective on the detection of inaccuracies is straightforward, we demonstrate its broader applicability by detailing the specifics for an SDT perspective on unfairness detection, including the need to choose a standard for (un)fairness. Additionally, we illustrate that an SDT perspective helps to better understand the conditions for effective error detection by showing examples of task-, system-, and person-related factors that may affect the sensitivity and response bias of humans tasked with detecting unfairness associated with the use of AI-based systems. Finally, we discuss future research directions for an SDT perspective on error detection.},
  archive      = {J_MAM},
  author       = {Langer, Markus and Baum, Kevin and Schlicker, Nadine},
  doi          = {10.1007/s11023-024-09701-0},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Effective human oversight of AI-based systems: A signal detection perspective on the detection of inaccurate and unfair outputs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How ChatGPT changed the media’s narratives on AI: A
semi-automated narrative analysis through frame semantics. <em>MAM</em>,
<em>35</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11023-024-09705-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform a mixed-method frame semantics-based analysis on a dataset of more than 49,000 sentences collected from 5846 news articles that mention AI. The dataset covers the twelve-month period centred around the launch of OpenAI’s chatbot ChatGPT and is collected from the most visited open-access English-language news publishers. Our findings indicate that during the six months succeeding the launch, media attention rose tenfold—from already historically high levels. During this period, discourse has become increasingly centred around experts and political leaders, and AI has become more closely associated with dangers and risks. A deeper review of the data also suggests a qualitative shift in the types of threat AI is thought to represent, as well as the anthropomorphic qualities ascribed to it.},
  archive      = {J_MAM},
  author       = {Ryazanov, Igor and Öhman, Carl and Björklund, Johanna},
  doi          = {10.1007/s11023-024-09705-w},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Minds Mach.},
  title        = {How ChatGPT changed the media’s narratives on AI: A semi-automated narrative analysis through frame semantics},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical learning theory and occam’s razor: The core
argument. <em>MAM</em>, <em>35</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s11023-024-09703-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning theory is often associated with the principle of Occam’s razor, which recommends a simplicity preference in inductive inference. This paper distills the core argument for simplicity obtainable from statistical learning theory, built on the theory’s central learning guarantee for the method of empirical risk minimization. This core “means-ends” argument is that a simpler hypothesis class or inductive model is better because it has better learning guarantees; however, these guarantees are model-relative and so the theoretical push towards simplicity is checked by our prior knowledge.},
  archive      = {J_MAM},
  author       = {Sterkenburg, Tom F.},
  doi          = {10.1007/s11023-024-09703-y},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Minds Mach.},
  title        = {Statistical learning theory and occam’s razor: The core argument},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence (AI) and global justice.
<em>MAM</em>, <em>35</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s11023-024-09708-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a philosophically informed and robust account of the global justice implications of Artificial Intelligence (AI). We first discuss some of the key theories of global justice, before justifying our focus on the Capabilities Approach as a useful framework for understanding the context-specific impacts of AI on low- to middle-income countries. We then highlight some of the harms and burdens facing low- to middle-income countries within the context of both AI use and the AI supply chain, by analyzing the extraction of materials, which includes mineral extraction and the environmental harms associated with it, and the extraction of labor, which includes unethical labor practices, low wages, and the trauma experienced by some AI workers. We then outline some of the potential harms and benefits that AI poses, how these are distributed, and what global justice implications this has for low- to middle-income countries. Finally, we articulate the global justice significance of AI by utilizing the Capabilities Approach. We argue that AI must be considered from a global justice perspective given that, globally, AI puts significant downward pressure on several elements of well-being thereby making it harder for people to achieve threshold levels of the central human capabilities needed for a life of dignity.},
  archive      = {J_MAM},
  author       = {Sahebi, Siavosh and Formosa, Paul},
  doi          = {10.1007/s11023-024-09708-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Minds Mach.},
  title        = {Artificial intelligence (AI) and global justice},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Standards for belief representations in LLMs. <em>MAM</em>,
<em>35</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s11023-024-09709-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief. Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations.},
  archive      = {J_MAM},
  author       = {Herrmann, Daniel A. and Levinstein, Benjamin A.},
  doi          = {10.1007/s11023-024-09709-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Minds Mach.},
  title        = {Standards for belief representations in LLMs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cheaper spaces. <em>MAM</em>, <em>35</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11023-024-09704-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity spaces are standardly constructed by collecting pairwise similarity judgments and subjecting those to a dimension-reduction technique such as multidimensional scaling or principal component analysis. While this approach can be effective, it has some known downsides, most notably, it tends to be costly and has limited generalizability. Recently, a number of authors have attempted to mitigate these issues through machine learning techniques. For instance, neural networks have been trained on human similarity judgments to infer the spatial representation of unseen stimuli. However, these newer methods are still costly and fail to generalize widely beyond their initial training sets. This paper proposes leveraging prebuilt semantic vector spaces as a cheap alternative to collecting similarity judgments. Our results suggest that some of those spaces can be used to approximate human similarity judgments at low cost and high speed.},
  archive      = {J_MAM},
  author       = {Moullec, Matthieu and Douven, Igor},
  doi          = {10.1007/s11023-024-09704-x},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {Cheaper spaces},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness in algorithmic profiling: The AMAS case.
<em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-024-09706-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a controversial application of algorithmic profiling in the public sector, the Austrian AMAS system. AMAS was supposed to help caseworkers at the Public Employment Service (PES) Austria to allocate support measures to job seekers based on their predicted chance of (re-)integration into the labor market. Shortly after its release, AMAS was criticized for its apparent unequal treatment of job seekers based on gender and citizenship. We systematically investigate the AMAS model using a novel real-world dataset of young job seekers from Vienna, which allows us to provide the first empirical evaluation of the AMAS model with a focus on fairness measures. We further apply bias mitigation strategies to study their effectiveness in our real-world setting. Our findings indicate that the prediction performance of the AMAS model is insufficient for use in practice, as more than 30% of job seekers would be misclassified in our use case. Further, our results confirm that the original model is biased with respect to gender as it tends to (incorrectly) assign women to the group with high chances of re-employment, which is not prioritized in the PES’ allocation of support measures. However, most bias mitigation strategies were able to improve fairness without compromising performance and thus may form an important building block in revising profiling schemes in the present context.},
  archive      = {J_MAM},
  author       = {Achterhold, Eva and Mühlböck, Monika and Steiber, Nadia and Kern, Christoph},
  doi          = {10.1007/s11023-024-09706-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Fairness in algorithmic profiling: The AMAS case},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Submarine cables and the risks to digital
sovereignty. <em>MAM</em>, <em>35</em>(1), 1. (<a
href="https://doi.org/10.1007/s11023-024-09707-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Ganz, Abra and Camellini, Martina and Hine, Emmie and Novelli, Claudio and Roberts, Huw and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09707-8},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1},
  shortjournal = {Minds Mach.},
  title        = {Correction: Submarine cables and the risks to digital sovereignty},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An app a day will (probably not) keep the doctor away: An
evidence audit of health and medical apps available on the apple app
store. <em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-025-09710-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are more than 350,000 health apps available in public app stores. The extolled benefits of health apps are numerous and well documented. However, there are also concerns that poor-quality apps, marketed directly to consumers, threaten the tenets of evidence-based medicine and expose individuals to the risk of harm. This study addresses this issue by assessing the overall quality of evidence publicly available to support the effectiveness claims of health apps marketed directly to consumers. To assess the quality of evidence available to the public to support the effectiveness claims of health apps marketed directly to consumers, an audit was conducted of a purposive sample of apps available on the Apple App Store. We find the quality of evidence available to support the effectiveness claims of health apps marketed directly to consumers to be poor. Less than half of the 220 apps (44%) we audited state that they have evidence to support their claims of effectiveness and, of these allegedly evidence-based apps, more than 70% rely publicly on either very low or low-quality evidence. For the minority of app developers that do publish studies, significant methodological limitations are commonplace. Finally, there is a pronounced tendency for apps—particularly mental health and diagnostic apps—to either borrow evidence generated in other (typically offline) contexts or to rely exclusively on unsubstantiated, unpublished user metrics as evidence to support their effectiveness claims. Health apps represent a significant opportunity for individual consumers and healthcare systems. Nevertheless, this opportunity will be missed if the health apps market continues to be flooded by poor quality, poorly evidenced, and potentially unsafe apps. It must be accepted that a continuing lag in generating high-quality publicly available evidence of app effectiveness and safety is not inevitable: it is a choice. Just because it will be challenging to raise the quality of the evidence base publicly available to support the claims of health apps, this does not mean that the bar for evidence quality should be lowered. Innovation for innovation’s sake must not be prioritized over public health and safety.},
  archive      = {J_MAM},
  author       = {Morley, Jessica and Laitila, Joel and Ross, Joseph S. and Schamroth, Joel and Zhang, Joe and Floridi, Luciano},
  doi          = {10.1007/s11023-025-09710-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {An app a day will (Probably not) keep the doctor away: An evidence audit of health and medical apps available on the apple app store},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChatGPT-4 in the turing test. <em>MAM</em>, <em>35</em>(1),
1–10. (<a href="https://doi.org/10.1007/s11023-025-09711-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been considerable optimistic speculation on how well ChatGPT-4 would perform in a Turing Test. However, no minimally serious implementation of the test has been reported to have been carried out. This brief note documents the results of subjecting ChatGPT-4 to 10 Turing Tests, with different interrogators and participants. The outcome is tremendously disappointing for the optimists. Despite ChatGPT reportedly outperforming 99.9% of humans in a Verbal IQ test, it falls short of passing the Turing Test. In 9 out of the 10 tests conducted, the interrogators successfully identified ChatGPT-4 and the human participant. The probability of obtaining this result from a process in which the interrogator is really no better than chance at correct identification is calculated to be less than 1%. An additional question was posed to the interrogators at the end of each test: What led them to distinguish between the human and the machine? The interrogators, who effectively filtered out ChatGPT-4 from passing the Turing Test for intelligence, stated that they could identify the machine because it, in effect, responded more intelligently than the human. Subsequently, ChatGPT-4 was tasked with differentiating syntax from semantics and self-corrected when falling for the fallacy of equivocation. The curious situation is arrived at that passing the Turing Test for intelligence remains a challenge that ChatGPT-4 has yet to overcome, precisely because, as per the interrogators, its intellectual abilities surpass those of individual humans.},
  archive      = {J_MAM},
  author       = {Restrepo Echavarría, Ricardo},
  doi          = {10.1007/s11023-025-09711-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Minds Mach.},
  title        = {ChatGPT-4 in the turing test},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On twelve shades of green: Assessing the levels of
environmental protection in the artificial intelligence act.
<em>MAM</em>, <em>35</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11023-025-09713-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper examines twelve legal regimes related to the governance and regulation of both the environmental risks and opportunities brought forth by the use of AI systems and AI models in the Artificial Intelligence Act (‘AIA’) of EU law. The assessment of risks and opportunities of AI related to the environment includes the high-risk management procedures under Art. 9 of the AIA, the “fundamental rights impact assessment” of Art. 27, and the codes of conduct of Art. 95. These provisions are supplemented by further regulatory regimes, such as the proposal of EU directive on sustainable consumption and green claims, and Reg. (EU) 2023/588 on environmental and space sustainability, among others. The aim of the analysis is to specify which are the less or the more environmentally friendly regulatory regimes set up with the AIA. The claim is that Art. 9, 27 and 95 are among the less green pieces of the whole legislation.},
  archive      = {J_MAM},
  author       = {Pagallo, Ugo},
  doi          = {10.1007/s11023-025-09713-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Minds Mach.},
  title        = {On twelve shades of green: Assessing the levels of environmental protection in the artificial intelligence act},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The testimony gap: Machines and reasons. <em>MAM</em>,
<em>35</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11023-025-09712-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most people who have considered the matter have concluded that machines cannot be moral agents. Responsibility for acting on the outputs of machines must always rest with a human being. A key problem for the ethical use of AI, then, is to ensure that it does not block the attribution of responsibility to humans or lead to individuals being unfairly held responsible for things over which they had no control. This is the “responsibility gap”. In this paper, we argue that the claim that machines cannot be held responsible for their actions has unacknowledged implications for the conditions under which the outputs of AI can serve as reasons for belief. Following Robert Brandom, we argue that, because the assertion of a claim is an action, moral agency is a necessary condition for the giving and evaluating of reasons in discourse. Thus, the same considerations that suggest that machines cannot be held responsible for their actions suggest that they cannot be held to account for the epistemic value — or lack of value — of their outputs. If there is a responsibility gap, there is also a “testimony gap.” An under-recognised problem with the use of AI, then, is to ensure that it does not block the attribution of testimony to human beings or lead to individuals being held responsible for claims that they have not asserted. More generally, the “assertions” of machines are only capable of serving as justifications for belief or action where one or more people accept responsibility for them.},
  archive      = {J_MAM},
  author       = {Sparrow, Robert and Flenady, Gene},
  doi          = {10.1007/s11023-025-09712-5},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Minds Mach.},
  title        = {The testimony gap: Machines and reasons},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
