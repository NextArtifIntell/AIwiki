<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kis---33">KIS - 33</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Taxonomy of deep learning-based intrusion
detection system approaches in fog computing: A systematic review.
<em>KIS</em>, <em>67</em>(2), 2017. (<a
href="https://doi.org/10.1007/s10115-024-02206-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Najafli, Sepide and Toroghi Haghighat, Abolfazl and Karasfi, Babak},
  doi          = {10.1007/s10115-024-02206-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {2017},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: taxonomy of deep learning-based intrusion detection system approaches in fog computing: a systematic review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-class margin climbing with cost-sensitive learning in
neural network classification. <em>KIS</em>, <em>67</em>(2), 1993–2016.
(<a href="https://doi.org/10.1007/s10115-024-02279-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large margin stands as an intuitive indicator of reliable classifiers, reflecting classifier robustness and generalizability. However, due to the intricate nonlinear mapping, directly defining margin as the optimization objective for multi-layer neural network is always challenging. On the other hand, the cost sensitivity coefficients of individual samples hold promise for shaping decision boundaries of neural network classification. A question arises as to whether optimizing neural network classifier can be guided to achieve larger classification margin by varying instance-level cost sensitivity factor. Inspired by above question, this paper proposes a heuristic hard mining strategy designed to progressively identify challenging samples and amplify the output margin through cost-sensitive learning. The refinement process adjusts the sample distribution when optimization reaches a local minimum to ensure the sustainable optimization, ultimately leading to margin climbing. Two hard mining algorithms are designed for binary and multi-class classification problems, which utilize distinct margin definitions based on different decision-making scenarios. In the proposed method, we focus on establishing individualized margin between distinct categories to more accurately characterize the inter-class margin. Empirical results demonstrate that our proposed methodology enhances both the accuracy and robustness in neural network classification.},
  archive      = {J_KIS},
  author       = {Zhang, Siyuan and Xie, Linbo and Chen, Ying and Zhang, Shanxin},
  doi          = {10.1007/s10115-024-02279-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1993-2016},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Inter-class margin climbing with cost-sensitive learning in neural network classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evidence-based approach for open-domain question
answering. <em>KIS</em>, <em>67</em>(2), 1969–1991. (<a
href="https://doi.org/10.1007/s10115-024-02269-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain question answering (ODQA) stands at the forefront of advancing natural language understanding and information retrieval. Traditional ODQA systems, which predominantly utilize a two-step process of information retrieval followed by reading module, face significant challenges in aligning retrieved passages with the contextual nuances of user queries. This paper introduces a novel methodology that leverages a semi-structured knowledge graph to enhance both the accuracy and relevance of answers in ODQA systems. Our model employs a threefold approach: firstly, it extracts and ranks evidence from a textual knowledge graph, a semi-structured knowledge graph where the nodes are real-world entities and the edges are sentences that two entities co-occur in, based on the contextual relationships relevant to the question. Secondly, it utilizes this ranked evidence to re-rank initially retrieved passages, ensuring that they align more closely with the query’s context. Thirdly, it integrates this evidence into a generative reading component to construct precise and context-rich answers. We compare our model, termed contextual evidence-based question answering (CEQA), against traditional and state-of-the-art ODQA systems across several datasets, including TriviaQA, Natural Questions, and SQuAD Open. Our extensive experiments and ablation studies show that CEQA significantly outperforms existing methods by improving both the relevance of retrieved passages and the accuracy of the generated answers, thereby establishing a new benchmark in ODQA.},
  archive      = {J_KIS},
  author       = {Jafarzadeh, Parastoo and Ensan, Faezeh},
  doi          = {10.1007/s10115-024-02269-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1969-1991},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An evidence-based approach for open-domain question answering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised multi-hop heterogeneous hypergraph embedding
with informative pooling for graph-level classification. <em>KIS</em>,
<em>67</em>(2), 1945–1968. (<a
href="https://doi.org/10.1007/s10115-024-02259-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heterogeneous graph analysis, existing self-supervised learning (SSL) methods face several key challenges. Primarily, these approaches are tailored for node-level tasks and fail to effectively capture global graph-level features, a crucial aspect for comprehensive graph understanding. Furthermore, they predominantly rely on meta-path-based techniques to unravel graph structures, a process that can be computationally intensive and often intractable for complex networks. Another significant limitation is their inability to account for nonpairwise relationships, a common characteristic in real-world networks like protein-protein interaction and collaboration networks, limiting their effectiveness in graph-level learning where high-order connectivity is essential. To address these issues, we propose an innovative SSL framework for heterogeneous hypergraph embedding, expressly designed to enhance graph-level classification. Our framework introduces multi-hop attention in hypergraph convolution, a significant leap from existing attention mechanisms specifically for hypergraphs that primarily focus on immediate neighborhoods. This multi-hop approach allows for an expansive capture of relational structures, both near and far, uncovering intricate patterns integral to accurate graph-level classification. Complementing this, we implement an informative graph-level attentive pooling mechanism that surpasses traditional aggregation methods. It intelligently synthesizes features, taking into account their structural and semantic importance within the hypergraph, thereby preserving critical contextual information. Furthermore, we refine our contrastive learning approach and introduce targeted negative sampling strategies, creating a more robust learning environment that excels at discerning nuanced graph-level features. Rigorous evaluation against established graph kernels, graph neural networks, and graph pooling methods on real-world datasets demonstrates our model’s superior performance, validating its effectiveness in addressing the complexities inherent in heterogeneous graph-level classification.},
  archive      = {J_KIS},
  author       = {Hayat, Malik Khizar and Xue, Shan and Yang, Jian},
  doi          = {10.1007/s10115-024-02259-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1945-1968},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Self-supervised multi-hop heterogeneous hypergraph embedding with informative pooling for graph-level classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oversampling algorithm for high-dimensional imbalanced
learning with class overlapping. <em>KIS</em>, <em>67</em>(2),
1915–1943. (<a
href="https://doi.org/10.1007/s10115-024-02276-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing standard learning methods suffer from poor performance in high-dimensional imbalanced learning with class overlapping. To tackle this problem, we propose a novel oversampling algorithm that aims to generate a robust ensemble of manifold dimensionality reduction, grid clustering, and information entropy criteria. Instead of simply balancing positive and negative numbers, the algorithm considers the difference in information entropy for interclass, which first reduces the dimensionality by manifold reduction, and then group data utilize grid clustering. Subsequently, calculate the oversampling weight of each group by information entropy and find seed samples based on entropy and neighborhood. Finally, SMOTE based on Beta distribution combined with standard classifiers achieve the rapid and precise classification for high-dimensional imbalanced datasets with class overlapping. Extensive experimental results on 20 real-world imbalanced datasets and compared with eight popular oversampling algorithms show that our proposed algorithm, while achieving good performance in terms of F-measure, G-mean, and AUPRC, can lead to robust performance under high-dimensional and overlapping. It is worth noting that our algorithm substantially reduces the number of synthetic samples against the quantity-balanced oversampling algorithms, and significantly reduces the generation of class overlapping.},
  archive      = {J_KIS},
  author       = {Yang, Xu and Xue, Zhen and Zhang, Liangliang and Wu, Jianzhen},
  doi          = {10.1007/s10115-024-02276-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1915-1943},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An oversampling algorithm for high-dimensional imbalanced learning with class overlapping},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on federated learning system: A new
paradigm to machine learning. <em>KIS</em>, <em>67</em>(2), 1811–1914.
(<a href="https://doi.org/10.1007/s10115-024-02257-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a machine learning technique that permits clients to train the model at a local site in a collaborative manner. It builds a global shared model on the basis of updates of the local model without exchanging data among multiple devices. Federated learning was introduced in 2016 with the goal of enabling local training as well as distributed machine learning training at the edge node’s level. It plays a vital role in terms of preserving the privacy of data while training the machine learning model on multiple devices. However, the introduction of federated learning into real-world applications exposes certain challenges in the training process, which affect the overall efficacy and efficiency of the federated learning model in real-world scenarios. As a result, an increasing number of researchers are now focusing on tackling the issues of FL and exploring various efficient research approaches to overcome these current obstacles. This paper systematically provides a detailed overview of federated learning, covering its definition, the need behind its development, privacy concepts, characteristics, and brief knowledge regarding different system components of federated learning. Different open-source frameworks that are available and used for implementing and solving problems related to federated learning have also been addressed in this article. Beyond this, the taxonomy of federated learning systems and different architectures for the same have also been discussed. In this paper, a brief comparison of related concepts with federated learning and a comparison among existing and popular federated learning studies proposed in different articles in the area of federated learning have also been summarized. In addition to the above-stated information, this article also provides brief information and a summary of various application areas of federated learning. Lastly, this paper briefly addresses the different challenges and prospects of research that lead to progress in this field.},
  archive      = {J_KIS},
  author       = {Chaudhary, Rajesh Kumar and Kumar, Ravinder and Saxena, Nitin},
  doi          = {10.1007/s10115-024-02257-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1811-1914},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A systematic review on federated learning system: A new paradigm to machine learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMNE: Link prediction using hypergraph motifs and network
embedding in social networks. <em>KIS</em>, <em>67</em>(2), 1787–1809.
(<a href="https://doi.org/10.1007/s10115-024-02255-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embeddings, which map nodes to low-dimensional vectors, facilitate link prediction, a pivotal aspect of complex network research. However, existing methods often overlook the complexities of hypergraphs and potent structures for modeling intricate relationships among multiple entities. This paper delves into link prediction within hypergraph motifs and network embedding (HMNE), crucial for diverse fields like knowledge graphs and bioinformatics. HMNE employs motifs to perform network embedding, representing nodes as hyper-nodes. HMNE utilizes the skip-gram model to get the embedding vectors by analyzing the sequence generated using a local random walk technique. Additionally, we consider hyper-motifs as super-nodes to highlight structural similarities between nodes. To further refine our methodology, we use the depth and breadth motif random walk strategy on the embedded network with hyper-nodes. This innovative approach enriches our understanding of network dynamics and enhances the predictive power of our model. We have thoroughly experimented the proposed method on several real-world datasets, and the results consistently demonstrate its usefulness.},
  archive      = {J_KIS},
  author       = {Zhang, Yichen and Lai, Shouliang and Peng, Zelu and Rezaeipanah, Amin},
  doi          = {10.1007/s10115-024-02255-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1787-1809},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HMNE: Link prediction using hypergraph motifs and network embedding in social networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label classification with label clusters.
<em>KIS</em>, <em>67</em>(2), 1741–1785. (<a
href="https://doi.org/10.1007/s10115-024-02270-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is the task of simultaneously predicting a set of labels for an instance, with global and local being the two predominant approaches. The global approach trains a single classifier to handle all classes simultaneously, while the local approach breaks down the problem into multiple binary problems. Despite extensive research, effectively capturing label correlations remains a challenge in both methods. In this paper, we introduce an approach that clusters the label space to create hybrid partitions (disjoint correlated label clusters), striking a balance between global and local strategies while leveraging both advantages. Our approach consists of (i) clustering the label space based on correlations, (ii) generating and validating the resulting hybrid partitions, (iii) selecting the best partitions, and (iv) evaluating their performance. We also compare our approach against an oracle, exhaustive search, and random search to assess how closely our hybrid partitions approximate the best possible partitions. The oracle selects the best partition using the test set, while the exhaustive approach relies on validation data. Experiments conducted on multiple multi-label datasets demonstrate that our method, along with random partitions, achieves results that are superior or competitive compared to traditional global and local approaches, as well as the state-of-the-art Ensemble of Classifier Chains. These findings suggest that conventional methods may not fully capture label correlations, and clustering the label space offers a promising solution.},
  archive      = {J_KIS},
  author       = {Gatto, Elaine Cecília and Ferrandin, Mauri and Cerri, Ricardo},
  doi          = {10.1007/s10115-024-02270-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1741-1785},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-label classification with label clusters},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEGODE: A structure-enhanced graph neural ordinary
differential equation network model for temporal link prediction.
<em>KIS</em>, <em>67</em>(2), 1713–1740. (<a
href="https://doi.org/10.1007/s10115-024-02261-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of temporal link prediction is to forecast potential future connections in a network by analyzing its structural underpinnings and tracking its temporal dynamics. However, existing methods for temporal link prediction are overly reliant on the most recent snapshots of the network, thereby limiting their ability to uncover and utilize the fundamental evolutionary patterns for effective dynamical inference. As a result, the predictive prowess of these models tends to be heightened for proximate future scenarios, as opposed to those farther into the horizon. Furthermore, the majority of the current methodologies overlook the influence of intricate higher-order and overarching structural dynamics, which could potentially enhance predictive accuracy. To tackle these challenges, we introduce a structure-enhanced graph neural ordinary differential equation (SEGODE), a comprehensive framework that leverages neural ordinary differential equations integrated with attention mechanisms to facilitate dynamic inference. The framework enhances the ability to snatch higher-order and global structures. To substantiate the viability of our novel model, we embarked on a comprehensive set of experiments conducted on seven real datasets. The outcomes of these rigorous tests demonstrate that our SEGODE approach not just demonstrates commendable performance in the task of link prediction but additionally has good results even when data is sparse.},
  archive      = {J_KIS},
  author       = {Fu, Jiale and Guo, Xuan and Hou, Jinlin and Yu, Wei and Shi, Hongjin and Zhao, Yanxia},
  doi          = {10.1007/s10115-024-02261-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1713-1740},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SEGODE: A structure-enhanced graph neural ordinary differential equation network model for temporal link prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved affinity propagation clustering algorithms: A
PSO-based approach. <em>KIS</em>, <em>67</em>(2), 1681–1711. (<a
href="https://doi.org/10.1007/s10115-024-02260-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional clustering algorithms such as K-means need to input the number of clusters before the start of the algorithm. Affinity propagation (AP) clustering algorithm solves this problem by considering each data point as a prospective cluster head (exemplar) and finding a set of appropriate exemplars by message passing. However, the AP clustering algorithm requires two parameters: preference and damping factor. Providing the parameters in advance poses the same issue faced in the traditional clustering algorithm. Moreover, all data points are not equally relevant for becoming cluster heads. To overcome these problems, we propose two parameter-free particle swarm optimization-based algorithms, PSO-APver1 and PSO-APver2. Furthermore, we introduce a novel version of mutant PSO where two cluster validity indices are used to judge the quality of the clustering solution. In PSO-APver2, we consider the internal data distribution using the square wave function to determine the initial preference value of data points. We conducted experiments on 8 real-world datasets to show the efficacy of our proposed algorithms over classic algorithms and two AP-based algorithms. We conducted the Friedman test followed by post hoc analysis to exhibit the significance of our work.},
  archive      = {J_KIS},
  author       = {Sinha, Ankita and Jana, Prasanta K.},
  doi          = {10.1007/s10115-024-02260-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1681-1711},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improved affinity propagation clustering algorithms: A PSO-based approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Additive consistency analysis for nested probabilistic
linguistic preference relations and its application in decision making.
<em>KIS</em>, <em>67</em>(2), 1651–1680. (<a
href="https://doi.org/10.1007/s10115-024-02231-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making problems often involve complex and uncertain evaluation criteria, making linguistic variables more practical than precise numerical values for describing information. Among various linguistic terms, the nested probabilistic linguistic term excels at expressing multidimensional information through its dual-layer structure, which captures both ordinal and nominal meanings. Using NPLTs to make expressions, this paper introduces nested probabilistic linguistic preference relations (NPLPRs) for effectively conveying preference information in decision-making scenarios and provides new operational rules for quantitative computations. To ensure the consistency of the preference relations, we define a consistency index and threshold to assess the additive consistency of NPLPRs and innovatively propose an automatic iteration algorithm to improve NPLPRs with unacceptable consistency. To reflect the evaluation focus and obtain the final results, we consider the weights of nominal terms when aggregating preferences with acceptable consistency. Finally, an illustrative example of selecting the most environmentally sustainable city demonstrates the application and advantages of our approach, supported by comparative analyses.},
  archive      = {J_KIS},
  author       = {Xiao, Jinglin and Wang, Xinxin and Xu, Zeshui},
  doi          = {10.1007/s10115-024-02231-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1651-1680},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Additive consistency analysis for nested probabilistic linguistic preference relations and its application in decision making},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning and similarity measures for
requirements-driven composition of semantic web services. <em>KIS</em>,
<em>67</em>(2), 1627–1649. (<a
href="https://doi.org/10.1007/s10115-024-02244-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The composition of web service is the best chance provided by Service-Oriented Computing and Service-Oriented Architecture as it gives real competitive benefits for some industrial and technological actors via presenting them with the probability to guarantee fast and inexpensive improvement of collaborative and distributed software applications. Here, a novel technique is introduced, which contains several phases for better web services. At first, the requirements specification phase is enabled with a set of requirements such as non-functional and functional requirements. Next to the requirements specification stage, the discovery stage is enabled to choose the suitable web services that have high-matching profiles with the developer’s requirement set. Here, for a semantic matching algorithm, a new hybrid similarity measure is developed. Additionally, among the group of candidate services that the discovery phase returned, the best service is selected during the selection step. Then, hybrid Squeeze_Long Short-Term Memory (Squeeze_LSTM) is used for choosing the best service and it is designed by the formation of SqueezeNet and LSTM. The Semantic Web Services are finally implemented. The efficiency of the Squeeze_LSTM is evaluated and has achieved a superior precision of 0.909, recall of 0.890, and response time of 6.461S.},
  archive      = {J_KIS},
  author       = {Bhuvaneswari, A. and Sumathi, K. and Sarveshwaran, Velliangiri and Sivasangari, A.},
  doi          = {10.1007/s10115-024-02244-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1627-1649},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hybrid deep learning and similarity measures for requirements-driven composition of semantic web services},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A proposed real-time decision support platform for moroccan
fixed mining production systems. <em>KIS</em>, <em>67</em>(2),
1597–1626. (<a
href="https://doi.org/10.1007/s10115-024-02271-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the competition in the mining markets and the rapid evolution of customer requirements, the Moroccan mining group OCP (Office Chérifien des Phosphates) has been forced to improve the performance of its production systems. Thus, continuous performance improvement and optimization of production processes are prerequisites to remain competitive. However, in Morocco, data analytics-based mining process improvements do not fully utilize the data generated during process execution. They lack prescriptive methodologies, which is the major goal of this work, to translate analytic results into improvement actions. Indeed, we propose a new platform for optimizing the production processes of a Moroccan mine based on knowledge extraction from data, allowing mine managers to rapidly and continuously improve the performance of their production chains. The platform will be an effective and efficient tool for mining companies to generate prescriptive action recommendations during the execution of the processes.},
  archive      = {J_KIS},
  author       = {Battas, Ilham and Behja, Hicham and El Ouazguiti, Mohamed},
  doi          = {10.1007/s10115-024-02271-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1597-1626},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A proposed real-time decision support platform for moroccan fixed mining production systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving density peak clustering on multi-dimensional time
series: Rediscover and subdivide. <em>KIS</em>, <em>67</em>(2),
1573–1596. (<a
href="https://doi.org/10.1007/s10115-024-02272-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density peak clustering (DPC) algorithm identifies patterns in high-dimensional data and obtains robust outcomes across diverse data types with minimal hyperparameters. However, DPC may produce inaccurate pattern sizes in multi-dimensional datasets and exhibit poor performance in recognizing similar patterns. To solve these issues, we propose the rediscover and subdivide density peak clustering algorithm (RSDPC), which follows three key strategies. The first strategy, rediscover, iteratively uncovers prominent patterns within the existing data. The second strategy, subdivide, partitions patterns into several similar subclasses. The third strategy, re-sort, rectifies errors from the preceding steps by incorporating critical distance and nearest distance considerations. The experimental results show that RSDPC is feasible and effective in synthetic and practical datasets compared with state-of-the-art works.},
  archive      = {J_KIS},
  author       = {Wang, Huina and Liu, Bo and Zhao, Huaipu and Qu, Guangzhi},
  doi          = {10.1007/s10115-024-02272-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1573-1596},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving density peak clustering on multi-dimensional time series: Rediscover and subdivide},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A customized balanced-objective genetic algorithm for task
scheduling in reconfigurable computing systems. <em>KIS</em>,
<em>67</em>(2), 1541–1571. (<a
href="https://doi.org/10.1007/s10115-024-02268-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfiguration and hardware implementation capabilities in reconfigurable computing (RC) systems make them more appropriate to recent computationally intensive applications. However, reaching optimal resource utilization remained as one of the main challenges in these systems. In order to implement more tasks in each reconfiguration interval, several decisive factors such as execution time, data communication cost, and required hardware resources must be analyzed simultaneously. In this paper, we proposed a novel balanced-objective task selector combined with a genetic algorithm to efficiently pick up the tasks of an application and occupy the resources as more as possible. The multi-objective fitness function of this algorithm adequately partitions the input application and provides the desirable intra and inter-cluster characteristics. Moreover, a new chromosome encoding technique has been developed to prevent precedence constraint violation of invalid solutions by removing forbidden regions in the search space. We classified the input applications with topological features such as first level parallel tasks (FLPT) and critical path length (CPL) for comprehensive evaluation. Several experiments are performed on randomly generated and real-world Directed Acyclic Graphs (DAGs), and the results are more satisfying in DAGs with more FLPTs and shorter CPLs where up to 28.63% makespan and 29.3% resource utilization improvement have been achieved in comparison with previous methods.},
  archive      = {J_KIS},
  author       = {Gholamrezanejad, Milad and Shahhoseini, Hadi Shahriar and Mohtavipour, Seyed Mehdi},
  doi          = {10.1007/s10115-024-02268-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1541-1571},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A customized balanced-objective genetic algorithm for task scheduling in reconfigurable computing systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSSN: A novel cache placement method based on adapted
shannon entropy and simple additive weighting method in named data
networking. <em>KIS</em>, <em>67</em>(2), 1507–1540. (<a
href="https://doi.org/10.1007/s10115-024-02266-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new strategy called PSSN to address the Cache Placement challenge in NDN. The paper first presents a novel clustering method based on the SAW decision-making approach and dynamically adapted Shannon weighting. Criteria such as the number of neighbors, hop count, router capacity, and CPU power are simultaneously considered for clustering and determining cluster heads. A key innovation in the proposed clustering is storing a copy of each content in each cluster to reduce duplicate content, increase content diversity, and consequently improve hit rate while reducing latency. Subsequently, for content placement, popularity of content, remaining router capacity, and hop count are analyzed concurrently using the SAW method adapted with the proposed approach. This ensures that popular content is placed closer to requesters. Throughout all stages of the method, the dynamic change in the status of content requests from users leads to a dynamic adjustment of the criteria weights. Simulation results using NDNsim demonstrate improvements in key parameters, with average enhancements of 17.8% and 9% for Hit rate and Delivery Time, respectively, as well as a 30.75% improvement in Load Balancing compared to recent methods.},
  archive      = {J_KIS},
  author       = {Soltani, Mohammad and Barekatain, Behrang and Hendessi, Faramarz and Beheshti, Zahra},
  doi          = {10.1007/s10115-024-02266-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1507-1540},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PSSN: A novel cache placement method based on adapted shannon entropy and simple additive weighting method in named data networking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-aware symmetric non-negative matrix factorization
for short text clustering. <em>KIS</em>, <em>67</em>(2), 1481–1506. (<a
href="https://doi.org/10.1007/s10115-024-02226-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text clustering is a significant yet challenging task, where short texts generated from the Internet are extremely sparse, noisy, and ambiguous. The sparse nature makes traditional clustering methods, e.g., k-means family and topic modeling, much less effective. Fortunately, recent arts of document distance, e.g., word mover’s distance, and document representation, e.g., BERT, can accurately measure the similarities of short texts, especially their nearest neighbors. Inspired by those arts and observations, we induce short text clusters by directly factorizing the informative affinity matrix of nearest neighbors into the product of the cluster assignment matrix, following the intuition that neighboring short texts tend to be assigned to the same cluster. However, due to the noisy nature of short texts, many of them can be regarded as outliers or near outliers, resulting in many noisy neighboring similarities within the affinity matrix. To further alleviate this problem, we enhance the affinity matrix factorization by (1) incorporating a sparse noisy matrix to directly capture noisy neighboring similarities and (2) regularizing the cluster assignment matrix by $$\ell _{2,1}$$ norm to eliminate hard-to-clustering short texts (called pseudo-outliers), so as to indirectly neglect noisy neighboring similarities corresponding to them. After this factorization for pre-clustering, we train a classifier over the resulting clusters and adopt it to assign each pseudo-outlier to one cluster finally. We call this novel clustering method as anomaly-aware symmetric non-negative matrix factorization ( $$\hbox {A}^{2}$$ snmf). Experimental results on benchmark short text datasets demonstrate that $$\hbox {A}^{2}$$ snmf performs very competitively with the existing baseline methods. The code is available at the website https://github.com/wizardbo/A3SNMF_functions .},
  archive      = {J_KIS},
  author       = {Li, Ximing and Guan, Yuanyuan and Fu, Bo and Luo, Zhongxuan},
  doi          = {10.1007/s10115-024-02226-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1481-1506},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Anomaly-aware symmetric non-negative matrix factorization for short text clustering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CETra: Online cluster tracking for clustering of streaming
data sources. <em>KIS</em>, <em>67</em>(2), 1455–1479. (<a
href="https://doi.org/10.1007/s10115-024-02267-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream clustering tasks may be applied to cluster streaming data objects (clustering by examples) or to cluster streaming data sources based on their temporal behavior (clustering by variables). We focus on the latter problem and propose CETra (Cluster evolution tracker)—the first online cluster tracking technique designed to provide information regarding cluster evolution in a streaming scenario of clustering by variables with efficient processing suitable for real-time problems. CETra can trace different intra and inter-cluster changes by considering not only statistics of interest but also the clusters’ membership, thus allowing a deeper understanding of the clustering results. Experimental evaluation using synthetic datasets and real data from meteorological sensors shows that CETra can track abrupt and gradual cluster transitions, while the competing method misses most of the gradual changes. Furthermore, CETra performs efficiently in a clustering environment for multiple streaming data sources, twice as fast as the related method.},
  archive      = {J_KIS},
  author       = {Sousa Lima, Afonso Matheus and de Sousa, Elaine Parros Machado},
  doi          = {10.1007/s10115-024-02267-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1455-1479},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CETra: Online cluster tracking for clustering of streaming data sources},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A path-based distance computation for non-convexity with
applications in clustering. <em>KIS</em>, <em>67</em>(2), 1415–1453. (<a
href="https://doi.org/10.1007/s10115-024-02275-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms are essential in data analysis, but evaluating their performance is challenging when the true labels are not available, especially for non-convex clusters. Traditional performance evaluation metrics struggle to identify clustering quality, often assigning higher scores for linearly separated clusters than the true clusters. We propose an original approach to distance computation that accounts for the data structure, thus improving the clustering quality evaluation for non-convex clusters without affecting other shapes of clusters. We also showcase the applicability of this method through a modified version of K-Means using the proposed method that is capable of correctly separating non-convex clusters. The validation included the analysis of performance and time complexity of 3 traditional clustering quality evaluation metrics and the K-Means clustering algorithm against their augmented versions with the proposed approach. This analysis conducted on 7 benchmark synthetic datasets and 6 real datasets with various numbers of examples and features of diverse characteristics and joint complexities: simple convex clusters, overlapped and imbalanced clusters, and non-convex clusters. Through these analyses, we show the ineffectiveness of traditional methods and that the proposed approach overcomes the weaknesses of traditional methods.},
  archive      = {J_KIS},
  author       = {Ardelean, Eugen-Richard and Portase, Raluca Laura and Potolea, Rodica and Dînșoreanu, Mihaela},
  doi          = {10.1007/s10115-024-02275-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1415-1453},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A path-based distance computation for non-convexity with applications in clustering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier detection in classification based on
feature-selection-based regression. <em>KIS</em>, <em>67</em>(2),
1399–1414. (<a
href="https://doi.org/10.1007/s10115-024-02264-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An outlier is a datum that is far from other data points in which it occurs. The appearance of outliers results in a complexity to obtain an accurate classification; numerous statistical and machine learning methods have been proposed to identify the outliers. This paper devotes a regression-based algorithm to the detection and identification of outlier before selecting a suitable classifier. The problem is firstly converted to an high-dimensional regression, then a novel method, based on the combination of multiple-correlation-coefficient-based feature selection for dimensional reduction and t-test for sparsification, is proposed, and an iterated algorithm is also given. Performance on simulated numerical data, low-dimensional iris data and high-dimensional DBWorld E-mail data demonstrate the superiority of the proposed method in outlier identification for classification.},
  archive      = {J_KIS},
  author       = {Su, Jinxia and Liu, Qiwen and Cui, Jingke},
  doi          = {10.1007/s10115-024-02264-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1399-1414},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Outlier detection in classification based on feature-selection-based regression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An energy-aware migration framework using metaheuristic
algorithm in cloud computing. <em>KIS</em>, <em>67</em>(2), 1373–1398.
(<a href="https://doi.org/10.1007/s10115-024-02224-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pervasive computing requires dynamic, energy-efficient cloud architecture to deploy services in VMs to distributed computing nodes. This mapping must ensure the service level agreement (SLA) runs VMs without disruption. This paper presents an energy-efficient metaheuristic Rock Hyrax algorithm-based cloud VM migration architecture. The male Rock Hyraxes find food and ensure the colony’s safety. His behavior has been mimicked in our proposed algorithm for finding energy-efficient compute nodes. A multi-objective VM migration function considers job submission deadlines. The proposed method reduces SLA violations and energy utilization while optimizing resource utilization. When evaluating the project, makespan, energy efficiency, and SLA violations were considered. The proposed algorithm is simulated on the CloudSim simulator considering both resources and jobs dynamic in nature. The suggested strategy outperforms ant colony optimization, particle swarm optimization, cuckoo optimization, and modified gray wolf optimization. The migration technique improves resource utilization by 18%, makespan time by 5%, SLA violation by 13%, and energy usage by 15%.},
  archive      = {J_KIS},
  author       = {Singhal, Saurabh and Sharma, Ashish},
  doi          = {10.1007/s10115-024-02224-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1373-1398},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An energy-aware migration framework using metaheuristic algorithm in cloud computing},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A timely personalized comments generation assistant based on
LSTM-SNP. <em>KIS</em>, <em>67</em>(2), 1351–1372. (<a
href="https://doi.org/10.1007/s10115-024-02198-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Posting personalized comments on an upcoming hot topic in time is very meaningful, which not only likely to attract more users to participate, but also affects other users’ point of view. LSTM-SNP is a variant of long short-term memory (LSTM) inspired by the nonlinear spiking mechanism in nonlinear spiking neural systems. In order to improve the efficiency and diversity of user-edited comments, we design a novel assistant based on the LSTM-SNP model. The assistant consists of two modules, one for predicting topic hotness and the other for generating comments with personalized expression features based on blog post and user information. Experimental results show that, this novel assistant not only predicts the upcoming hot topics accurately, but also outperforms the baseline model in terms of automatic evaluation and human discernment of comment generation. More importantly, the generated comments excel in terms of timeliness and personalization.},
  archive      = {J_KIS},
  author       = {Li, Yixiao and Wu, Yue and Li, Wenjia and Chen, Hui and Chen, Qi and Li, Yuehui},
  doi          = {10.1007/s10115-024-02198-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1351-1372},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A timely personalized comments generation assistant based on LSTM-SNP},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IC-SNI: Measuring nodes’ influential capability in complex
networks through structural and neighboring information. <em>KIS</em>,
<em>67</em>(2), 1309–1350. (<a
href="https://doi.org/10.1007/s10115-024-02262-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influential nodes are the important nodes that most efficiently control the propagation process throughout the network. Among various structural-based methods, degree centrality, k-shell decomposition, or their combination identify influential nodes with relatively low computational complexity, making them suitable for large-scale network analysis. However, these methods do not necessarily explore nodes’ underlying structure and neighboring information, which poses a significant challenge for researchers in developing timely and efficient heuristics considering appropriate network characteristics. In this study, we propose a new method (IC-SNI) to measure the influential capability of the nodes. IC-SNI minimizes the loopholes of the local and global centrality and calculates the topological positional structure by considering the local and global contribution of the neighbors. Exploring the path structural information, we introduce two new measurements (connectivity strength and effective distance) to capture the structural properties among the neighboring nodes. Finally, the influential capability of a node is calculated by aggregating the structural and neighboring information of up to two-hop neighboring nodes. Evaluated on nine benchmark datasets, IC-SNI demonstrates superior performance with the highest average ranking correlation of 0.813 with the SIR simulator and a 34.1% improvement comparing state-of-the-art methods in identifying influential spreaders. The results show that IC-SNI efficiently identifies the influential spreaders in diverse real networks by accurately integrating structural and neighboring information.},
  archive      = {J_KIS},
  author       = {Nandi, Suman and Curado Malta, Mariana and Maji, Giridhar and Dutta, Animesh},
  doi          = {10.1007/s10115-024-02262-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1309-1350},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {IC-SNI: Measuring nodes’ influential capability in complex networks through structural and neighboring information},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised feature selection with minimal redundancy
based on group optimization strategy for multi-label data. <em>KIS</em>,
<em>67</em>(2), 1271–1308. (<a
href="https://doi.org/10.1007/s10115-024-02258-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of intelligence technology, high-dimensional multi-label data exist in practical applications, which makes multi-label learning a significant challenge. Feature selection can obtain more distinguishable features to enhance recognition ability to address high-dimensional problems. Nowadays, most researchers usually evaluate the relevance between labels and features and the similarity between samples. They only focus on the local characteristics of samples without considering the global characteristics. To solve the above problems, in this paper, a novel feature selection approach for semi-supervised learning with minimal redundancy and group optimization strategy (SFGR) in multi-label scenario is proposed. First, a measure based on the Laplacian score and constrain score is utilized to evaluate the relevance between each feature and label. Meanwhile, the global structure of the data is considered via the creation of graphs and a priori information to obtain the feature subset with the highest relevance. Secondly, an optimization iteration algorithm based on a regularization term combining $$\text {l}_1$$ -norm and $$\text {l}_2$$ -norm is employed to ensure the sparsity of the feature weight matrix and minimize the redundancy. Moreover, a group optimal strategy is applied as a global search approach to fusion the feature subsets to obtain an approximate globally optimal feature subset. Eventually, experimental results on various multi-labeled datasets show that SFGR can perform better than other algorithms.},
  archive      = {J_KIS},
  author       = {Qing, Depeng and Zheng, Yifeng and Zhang, Wenjie and Ren, Weishuo and Zeng, Xianlong and Li, Guohe},
  doi          = {10.1007/s10115-024-02258-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1271-1308},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semi-supervised feature selection with minimal redundancy based on group optimization strategy for multi-label data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All is attention for multi-label text classification.
<em>KIS</em>, <em>67</em>(2), 1249–1270. (<a
href="https://doi.org/10.1007/s10115-024-02253-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification(MLTC) is a key task in natural language processing. Its challenge is to extract latent semantic features from text and effectively exploit label-associated features. This work proposes an MLTC model driven solely by attention mechanisms, which includes Graph Attention(GA), Class-Specific Attention(CSA), and Multi-Head Attention(MHA) modules. The GA module examines and records label dependencies by considering label semantic features as attributes of graph nodes. It uses graph embedding to maintain structural relationships within the label graph. Meanwhile, the CSA module produces distinctive features for each category by utilizing spatial attention scores, thereby improving classification accuracy. Then, the MHA module facilitates extensive feature interactions, enhancing the expressiveness of text features and supporting the handling of long-range dependencies. Experimental evaluations conducted on two MLTC datasets show that our proposed model outperforms existing MLTC algorithms, achieving state-of-the-art performance. These results highlight the effectiveness of our attention-based approach in tackling the complexity of MLTC tasks.},
  archive      = {J_KIS},
  author       = {Liu, Zhi and Huang, Yunjie and Xia, Xincheng and Zhang, Yihao},
  doi          = {10.1007/s10115-024-02253-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1249-1270},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {All is attention for multi-label text classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoRBS: A dynamic storytelling algorithm using a novel
contextualization approach for documents utilizing BERT features.
<em>KIS</em>, <em>67</em>(2), 1213–1248. (<a
href="https://doi.org/10.1007/s10115-024-02263-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storytelling is the process of connecting documents one after another representing the evolution of an event. Existing algorithms for storytelling connect events based on content overlaps between consecutive documents ignoring the role of the same term in different documents and the contemporary contexts (e.g., dynamic embeddings) of documents and terms. Due to the lack of role and contemporary contexts in the designs of the existing storytelling methods, the resultant stories frequently jump to documents with the keywords to form a chain but not a meaningful one. In this paper, we present a novel storytelling algorithm—Contextual Role-Based Storytelling (CoRBS)—that generates a chain of documents explaining the evolution of an event, addressing role and contemporary context issues of existing methods. CoRBS starts with a given document and moves forward temporally, stitching together role and context-driven documents to represent the evolution of the events that appear in the first document. We define the role of a term in a document as a distribution of similarities of the nearest neighbors of the term based on BERT embeddings of all terms of that document. Contemporary contexts are incorporated as a mechanism to discover a coherent next document while the story progresses. Our experiments demonstrate that CoRBS generates more meaningful stories compared to other baseline storytelling techniques.},
  archive      = {J_KIS},
  author       = {Nouri, Alireza and Hossain, M. Shahriar},
  doi          = {10.1007/s10115-024-02263-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1213-1248},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CoRBS: A dynamic storytelling algorithm using a novel contextualization approach for documents utilizing BERT features},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact lossy compression of tensors via neural tensor-train
decomposition. <em>KIS</em>, <em>67</em>(2), 1169–1211. (<a
href="https://doi.org/10.1007/s10115-024-02252-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TensorCodec, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions.TensorCodec incorporates three key ideas. The first idea is neural tensor-train decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be exploited by NTTD for improved approximation. In addition, we extend TensorCodec to enable the lossy compression of tensors with missing entries, often found in real-world datasets. Our analysis and experiments on 8 real-world datasets demonstrate that TensorCodec is (a) Concise: it gives up to $$7.38 \times $$ more compact compression than the best competitor with similar reconstruction error, (b) Accurate: given the same budget for compressed size, it yields up to $$3.33\times $$ more accurate reconstruction than the best competitor, (c) Scalable: Its empirical compression time is linear in the number of tensor entries, and it reconstructs each entry in logarithmic time. Our code and datasets are available at https://github.com/kbrother/TensorCodec .},
  archive      = {J_KIS},
  author       = {Kwon, Taehyung and Ko, Jihoon and Jung, Jinhong and Jang, Jun-Gi and Shin, Kijung},
  doi          = {10.1007/s10115-024-02252-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1169-1211},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Compact lossy compression of tensors via neural tensor-train decomposition},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book recommendation using sentiment analysis and ensembling
hybrid deep learning models. <em>KIS</em>, <em>67</em>(2), 1131–1168.
(<a href="https://doi.org/10.1007/s10115-024-02250-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An immense volume of user-generated content exists online due to the exponential growth of internet usage among individuals. However, this abundance presents substantial challenges for recommendation systems and online customer services. The diverse data, encompassing consumer emails, reviews, and comments, contain a broad spectrum of information, making it challenging to decipher the underlying emotions and nuances. In social media analytics, sentiment analysis has emerged as a pivotal tool to unveil the emotional context embedded within textual data, offering deeper insights into user attitudes, opinions, and sentiments. This study introduces a novel strategy to strengthen the performance and precision of sentiment analysis-based book recommendation systems through ensemble learning on hybrid deep learning models. These book recommendation models leverage customer ratings and reviews from a vast Amazon books dataset as input. Initially, we used TextBlob to assess the polarity of customer reviews, categorizing them into neutral, negative, and positive sentiments. Subsequently, the input data underwent preprocessing, tokenization, and word embedding using bidirectional encoder representations from transformers (BERT). To effectively analyze and filter processed review comments and ratings, the proposed ensemble model integrates a diverse array of hybrid deep learning architectures, including long short-term memory (LSTM), bidirectional LSTM (BiLSTM), gated recurrent unit (GRU), and convolutional neural network (CNN). Extensive experimentation validated the superiority of the proposed ensemble model, achieving an impressive accuracy and F1-score of 98.21%. The significance of the approach lies in its ability to provide more accurate and contextually relevant book recommendations by considering the nuanced emotions expressed in customer reviews. This contributes to enhancing user satisfaction and engagement with recommendation systems, ultimately improving the overall quality of personalized book suggestions. Evaluation metrics further validate the efficacy of the proposed model, underscoring its practical utility in real-world applications of sentiment-based book recommendation systems.},
  archive      = {J_KIS},
  author       = {Devika, P. and Milton, A.},
  doi          = {10.1007/s10115-024-02250-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1131-1168},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Book recommendation using sentiment analysis and ensembling hybrid deep learning models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion pattern mining. <em>KIS</em>, <em>67</em>(2),
1101–1129. (<a
href="https://doi.org/10.1007/s10115-024-02254-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a diffusion network, some nodes exhibit similar diffusion patterns as they have analogous influence reachabilities to the other nodes. When these nodes are selected as initially infected nodes, they tend to yield similar infection results. Mining diffusion patterns of nodes is of practical significance in various applications, such as online marketing and epidemic prevention. Nonetheless, few existing work has effectively addressed this problem. In this work, we investigate how to find out which nodes in a diffusion network share similar diffusion pattern based only on historical infection results. Toward this, we first reconstruct the structure of influence relationships in the network, and then infer the infection propagation probability on each influence relationship, based on which the influence reachability of each node can be estimated. We present a diffusion pattern similarity metric to quantify the similarity of influence reachabilities, and group nodes that share similar influence reachabilities via hierarchical clustering. Extensive experimental results on both synthetic and real-world networks verify the effectiveness and efficiency of our approach.},
  archive      = {J_KIS},
  author       = {Yan, Qian and Yang, Yulan and Yin, Kai and Gan, Ting and Huang, Hao},
  doi          = {10.1007/s10115-024-02254-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1101-1129},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Diffusion pattern mining},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward deep multi-view document clustering using enhanced
semantic embedding and consistent context semantics. <em>KIS</em>,
<em>67</em>(2), 1073–1100. (<a
href="https://doi.org/10.1007/s10115-024-02249-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view document clustering (MVDC) is a sophisticated approach in natural language processing that leverages multiple representations or views of data to improve clustering performance. Existing solutions are challenging due to inconsistency of document views, high dimensions, and sparseness in text documents. On the other hand, existing MVDC-based methods often depend on the performance of bag-of-words and pretrained language models. However, these models usually do not consider contextual semantics and are suitable for single-view document clustering. This paper addresses these challenges by proposing a deep MVDC model that utilizes enhanced semantic embedding and consistent context semantics (SECS). SECS uses semantic embedding to address high-dimensional challenges by considering complementary semantic information. Meanwhile, SECS takes advantage of the potential benefits of view-consistent context semantics based on pretrained language models. The proposed model captures intricate semantic relationships between words and documents through advanced embedding techniques, ensuring a richer and more nuanced representation of textual content. Furthermore, by incorporating consistent context semantics, SECS maintains contextual integrity across multiple views, leading to more coherent and meaningful clusters. Experimental results on benchmark datasets demonstrate the superiority of our model over state-of-the-art MVDC methods, highlighting its effectiveness in improving clustering quality and interpretability.},
  archive      = {J_KIS},
  author       = {Du, Yongsheng and Sun, Hongwei and Abdollahi, MohammadJavad},
  doi          = {10.1007/s10115-024-02249-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1073-1100},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Toward deep multi-view document clustering using enhanced semantic embedding and consistent context semantics},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Twitter sentiment analysis using ensemble of multi-channel
model based on machine learning and deep learning techniques.
<em>KIS</em>, <em>67</em>(2), 1045–1071. (<a
href="https://doi.org/10.1007/s10115-024-02256-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People post a lot of comments on the websites these days, as social media and the Internet are major parts of modern life. With so much information available online related to services, products, politics, stocks, etc., thus, using artificial intelligence to understand the emotions in these comments is highly beneficial for understanding public opinion. In particular, detecting sentiment polarity in customer reviews is crucial for businesses to make informed decisions. Despite the vast amount of information available on the internet, understanding the underlying emotions in user comments remains a challenge. Our work aims to bridge this gap by proposing a sophisticated sentiment analysis model that leverages state-of-the-art deep learning techniques. In this study, we present a sentiment analysis model that combines advanced deep learning neural networks: convolutional neural network, long short-term memory networks (LSTM), Bidirectional LSTM (BiLSTM), and Bidirectional Encoder Representations from Transformers (BERT). Accurate feature extraction plays a pivotal role in sentiment analysis applications. By merging pre-trained BERT with sophisticated neural networks, the devised model achieves an impressive accuracy of 94.95%. We evaluated the proposed model on a publicly available Twitter Sentiment Analysis dataset. The proposed ensemble multi-channel model outperforms several deep learning and machine learning techniques in sentiment analysis. Hence, we suggest the use of the ensemble model to accurately determine sentiments from tweets and other textual data.},
  archive      = {J_KIS},
  author       = {Tembhurne, Jitendra V. and Lakhotia, Kirtan and Agrawal, Anant},
  doi          = {10.1007/s10115-024-02256-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1045-1071},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Twitter sentiment analysis using ensemble of multi-channel model based on machine learning and deep learning techniques},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic regex synthesis methods for english: A comparative
analysis. <em>KIS</em>, <em>67</em>(2), 1013–1043. (<a
href="https://doi.org/10.1007/s10115-024-02232-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular expressions (short form regex) find their application in program script synthesis, machine translation, information extraction and web applications, such as input validations. Their expressiveness and flexibility make them decidedly the best tool for many challenging text extraction tasks. Writing regex manually has been labeled as a laborious, time consuming and error prone task even for skilled programmers. An abundance of regex generation from text queries at online platforms mainly Stackoverflow and Quora signifies the automatic regex synthesis problem. Despite their popularity, a criminal lack of comprehensive literature study on the problem has also been observed. We intend to perform a detailed review of a variety of methods available for regex synthesis, repair, and learn beneficial lessons for appropriate datasets with one earnest goal: to synthesize resource efficient and correct regexes for given textual description.},
  archive      = {J_KIS},
  author       = {Tariq, Sadia and Rana, Toqir Ahmad},
  doi          = {10.1007/s10115-024-02232-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1013-1043},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Automatic regex synthesis methods for english: A comparative analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chromosome segmentation and classification: An updated
review. <em>KIS</em>, <em>67</em>(2), 977–1011. (<a
href="https://doi.org/10.1007/s10115-024-02243-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Karyotyping is a study of chromosomes to identify various chromosomal aberrations related to structure and number. Chromosome image analysis involves challenging issues related to overlapping and touching of chromosomes. Chromosome segmentation and classification generally focus on separating overlapping and touching chromosomes. The analysis methods start from conventional image processing methods to advanced machine learning techniques. These methods are broadly classified into low-level and high-level methods. The low-level methods are thresholding-based approaches, edge detection, feature extraction techniques like active contours and watershed approaches and machine learning for classification. The high-level methods are deep learning algorithms like convolutional neural networks (CNNs), U-Net, autoencoder architectures. These methods help in improving accuracy and automate the process of chromosome segmentation and classification. High-level approaches can handle complexity in chromosome overlaps which provides better segmentation results. The approach learns complicated patterns and structures of chromosome images, which helps in achieving better classification accuracy. The challenges are: (i) working on large and annotated dataset for training deep learning models and (ii) suffer issues with new dataset even in they perform better during training phase. The solution for all these can be a hybrid approach that combines conventional method with modern approaches. This survey gives readers a basic understanding of automated karyotyping and future direction in this domain.},
  archive      = {J_KIS},
  author       = {Somasundaram, Devaraj and Madian, Nirmala and Goh, Kam Meng and Suresh, S.},
  doi          = {10.1007/s10115-024-02243-y},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {977-1011},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chromosome segmentation and classification: An updated review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
