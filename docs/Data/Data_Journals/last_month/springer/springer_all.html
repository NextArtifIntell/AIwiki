<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>springer_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="springer">SPRINGER</h1>
<h2 id="air---32">AIR - 32</h2>
<ul>
<li><details>
<summary>
(2025). Image-based deep learning for smart digital twins: A review.
<em>AIR</em>, <em>58</em>(5), 1–36. (<a
href="https://doi.org/10.1007/s10462-024-11002-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Digital Twins (SDTs) are being increasingly used to virtually replicate and predict the behaviors of complex physical systems through continual data assimilation, enabling the optimization of the performance of these systems by controlling the actions of systems. Recently, the Deep Learning (DL) models have significantly enhanced the capabilities of SDTs, particularly for tasks such as predictive maintenance, anomaly detection, and optimization. In many domains, including medicine, engineering, and education, SDTs use image data (image-based SDTs) to observe, learn, and control system behaviors. This paper focuses on various approaches and associated challenges in developing image-based SDTs by continually assimilating image data from physical systems. The paper also discusses the challenges in designing and implementing DL models for SDTs, including data acquisition, processing, and interpretation. In addition, insights into the future directions and opportunities for developing new image-based DL approaches to develop robust SDTs are provided. This includes the potential for using generative models for data augmentation, developing multi-modal DL models, and exploring the integration of DL models with other technologies, including Fifth Generation (5 G), edge computing, and the Internet of Things (IoT). In this paper, we describe the image-based SDTs, which enable broader adoption of the Digital Twins (DTs) paradigms across a broad spectrum of areas and the development of new methods to improve the abilities of SDTs in replicating, predicting, and optimizing the behavior of complex systems.},
  archive      = {J_AIR},
  author       = {Islam, Md Ruman and Subramaniam, Mahadevan and Huang, Pei-Chi},
  doi          = {10.1007/s10462-024-11002-y},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Image-based deep learning for smart digital twins: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural combinatorial optimization with reinforcement
learning in industrial engineering: A survey. <em>AIR</em>,
<em>58</em>(5), 1–37. (<a
href="https://doi.org/10.1007/s10462-024-11045-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent trends, machine learning is widely used to support decision-making in various domains and industrial operations. Because of the increasing complexity of modern industries, industrial engineering aims not only to increase cost-effectiveness and productivity but also to consider sustainability, resilience, and human centricity, resulting in many-objective, constrained, and stochastic operations research. Based on the above stringent requirements, combinatorial optimization (CO) problems are thus developed to support the complicated decision-making process in operations research. Due to the computational complexity of exact algorithms and the uncertain solution quality of heuristic methods, there is a growing trend to leverage the power of machine learning in solving CO problems, known as neural combinatorial optimization (NCO), where reinforcement learning (RL) is the core to achieve the sequential decision support. This survey study provides a comprehensive investigation of the theories and recent advancements in applying RL to solve hard CO problems, such as vehicle routing, bin packing, assignment, scheduling, and planning problems, and, in addition, summarizes the applications of neural combinatorial optimization with reinforcement learning (NCO-RL). The detailed review found that although the research domain of NCO-RL is still under-explored, its research potential has been proven to address environmental sustainability, adaptability, and human factors. Last but not least, the technical challenges and opportunities of the NCO-RL to embrace the industry 5.0 paradigm are discussed.},
  archive      = {J_AIR},
  author       = {Chung, K. T. and Lee, C. K. M. and Tsang, Y. P.},
  doi          = {10.1007/s10462-024-11045-1},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Neural combinatorial optimization with reinforcement learning in industrial engineering: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalogram based performance comparison of deep learning
architectures for dysarthric speech detection. <em>AIR</em>,
<em>58</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10462-024-11085-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dysarthria, a speech disorder commonly associated with neurological conditions, poses challenges in early detection and accurate diagnosis. This study addresses these challenges by implementing preprocessing steps, such as noise reduction and normalization, to enhance the quality of raw speech signals and extract relevant features. Scalogram images generated through wavelet transform effectively capture the time-frequency characteristics of the speech signal, offering a visual representation of the spectral content over time and providing valuable insights into speech abnormalities related to dysarthria. Fine-tuned deep learning models, including pre-trained convolutional neural network (CNN) architectures like VGG19, DenseNet-121, Xception, and a modified InceptionV3, were optimized with specific hyperparameters using training and validation sets. Transfer learning enables these models to adapt features from general image classification tasks to classify dysarthric speech signals better. The study evaluates the models using two public datasets TORGO and UA-Speech and a third dataset collected by the authors and verified by medical practitioners. The results reveal that the CNN models achieve an accuracy (acc) range of 90% to 99%, an F1-score range of 0.95 to 0.99, and a recall range of 0.96 to 0.99, outperforming traditional methods in dysarthria detection. These findings highlight the effectiveness of the proposed approach, leveraging deep learning and scalogram images to advance early diagnosis and healthcare outcomes for individuals with dysarthria.},
  archive      = {J_AIR},
  author       = {Shabber, Shaik Mulla and Sumesh, E. P. and Ramachandran, Vidhya Lavanya},
  doi          = {10.1007/s10462-024-11085-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Scalogram based performance comparison of deep learning architectures for dysarthric speech detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Psychological and physiological computing based on
multi-dimensional foot information. <em>AIR</em>, <em>58</em>(5), 1–56.
(<a href="https://doi.org/10.1007/s10462-024-11087-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the population ages, utilizing foot information to continuously monitor the physiological and psychological health status of the elderly is emerging as a pivotal tool for meeting this crucial societal demand. However, few reviews explored how multi-dimensional foot data has been integrated into physiological and psychological computing. This review is essential as it fills a critical knowledge gap in understanding the connections between physiological and psychological disorders and various components of foot information. To identify relevant literature, a thorough search was conducted across IEEE, DBLP, Elsevier, Springer, Google Scholar, and PubMed, initially yielding 2386 publications. After multiple rounds of systematic filtering, 404 publications were selected for in-depth analysis. This review examines (1) the mechanisms linking foot information to human physiological and psychological conditions, (2) the monitoring devices that collect diverse foot-based data, (3) the datasets correlating diseases with multiple foot data, (4) the prevalent feature engineering of different foot data, and (5) the cutting-edge machine and deep learning algorithms for diseases analysis. It also provides insights into future developments in foot information health monitoring for psychological and physiological computing.},
  archive      = {J_AIR},
  author       = {Li, Shengyang and Yao, Huilin and Peng, Ruotian and Ma, Yuanjun and Zhang, Bowen and Zhao, Zhiyao and Zhang, Jincheng and Chen, Siyuan and Wu, Shibin and Shu, Lin},
  doi          = {10.1007/s10462-024-11087-5},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Psychological and physiological computing based on multi-dimensional foot information},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep crowd anomaly detection: State-of-the-art, challenges,
and future research directions. <em>AIR</em>, <em>58</em>(5), 1–111. (<a
href="https://doi.org/10.1007/s10462-024-11092-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd anomaly detection is one of the most popular topics in computer vision in the context of smart cities. A plethora of deep learning methods have been proposed that generally outperform other machine learning solutions. Our review primarily discusses algorithms that were published in mainstream conferences and journals between 2020 and 2022. We present datasets that are typically used for benchmarking, produce a taxonomy of the developed algorithms, and discuss and compare their performances. Our main findings are that the heterogeneities of pre-trained convolutional models have a negligible impact on crowd video anomaly detection performance. We conclude our discussion with fruitful directions for future research.},
  archive      = {J_AIR},
  author       = {Sharif, Md. Haidar and Jiao, Lei and Omlin, Christian W.},
  doi          = {10.1007/s10462-024-11092-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-111},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep crowd anomaly detection: State-of-the-art, challenges, and future research directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Musical heritage historical entity linking. <em>AIR</em>,
<em>58</em>(5), 1–41. (<a
href="https://doi.org/10.1007/s10462-024-11102-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linking named entities occurring in text to their corresponding entity in a Knowledge Base (KB) is challenging, especially when dealing with historical texts. In this work, we introduce Musical Heritage named Entities Recognition, Classification and Linking (mhercl), a novel benchmark consisting of manually annotated sentences extrapolated from historical periodicals of the music domain. mhercl contains named entities under-represented or absent in the most famous KBs. We experiment with several State-of-the-Art models on the Entity Linking (EL) task and show that mhercl is a challenging dataset for all of them. We propose a novel unsupervised EL model and a method to extend supervised entity linkers by using Knowledge Graphs (KGs) to tackle the main difficulties posed by historical documents. Our experiments reveal that relying on unsupervised techniques and improving models with logical constraints based on KGs and heuristics to predict NIL entities (entities not represented in the KB of reference) results in better EL performance on historical documents.},
  archive      = {J_AIR},
  author       = {Graciotti, Arianna and Lazzari, Nicolas and Presutti, Valentina and Tripodi, Rocco},
  doi          = {10.1007/s10462-024-11102-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Musical heritage historical entity linking},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of deep learning algorithms in ischemic stroke
detection, segmentation, and classification. <em>AIR</em>,
<em>58</em>(5), 1–48. (<a
href="https://doi.org/10.1007/s10462-025-11119-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ischemic, one of the fatal diseases characterized by insufficient blood supply to tissues poses a significant global health burden, necessitating the development of robust diagnostic and classification methodologies. Timely identification, intervention, and treatment are essential to reduce associated risk factors. Modern machine learning methods like deep learning and neural networks are being successfully employed on medical images to detect and segment the region of interest for various diseases where the performance of these computational methods is improving daily and for various tasks has surpassed natural intelligence. This success has convinced medical practitioners to trust computational methods and incorporate computer-based solutions into their clinical practices. It is, therefore, essential to examine the available solutions critically by considering their strengths and weaknesses to establish their trust and clinical applicability. In the context of the above-mentioned task, this work focuses on two aspects: first, a broad review has been done for Ischemic stroke prognostication using various brain-imaging biomarkers via diverse deep learning frameworks, and second, the reviewed works are categorized based on their computational approach employed for Ischemic stroke detection, segmentation, and classification. Finally, this work presents recent advances and future research directions to invent high-performance methods. It was concluded that recent advancements in ischemic stroke detection have achieved 85–98% accuracy using CNNs and transformer-based models with separate imaging, clinical, and molecular data, though combined analysis remains largely underexplored. Integrating vascular imaging, clinical signs, and proteomic data can enhance real-time monitoring. However, challenges persist in unifying diverse parameters, necessitating advanced methodologies such as transfer learning, multi-task learning, advanced transformers, federated learning, and standardized protocols. These findings pave the way for improved diagnostics, treatment, and outcomes in stroke management.},
  archive      = {J_AIR},
  author       = {Kousar, Tanzeela and Rahim, Mohd Shafry Mohd and Iqbal, Sajid and Yousaf, Fatima and Sanaullah, Muhammad},
  doi          = {10.1007/s10462-025-11119-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of deep learning algorithms in ischemic stroke detection, segmentation, and classification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on persian question answering systems: From
traditional to modern approaches. <em>AIR</em>, <em>58</em>(5), 1–27.
(<a href="https://doi.org/10.1007/s10462-025-11122-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering systems (QAS) are designed to answer questions in natural language. The objective of these types of systems is to reduce the user’s effort to manually check the retrieved documents to find the answer to the query in natural language and to create an accurate answer to the user’s query. In recent years, with the emergence of Large Language Models (LLMs), these systems have evolved significantly across different languages. However, the development of QAS in low resource languages such as Persian, while progressing, still faces unique challenges. Development of these systems has become problematic in Persian language due to the lack of comprehensive processing tools, limited question answering datasets, and specific challenges of this language. The current study provides a brief explanation of these systems’ evolution from traditional architectures to LLM-based approaches, their classification, the challenges specific to Persian language, existing question-answering datasets and language models, and studies conducted concerning Persian QAS.},
  archive      = {J_AIR},
  author       = {Jolfaei, Safoura Aghadavoud and Mohebi, Azadeh},
  doi          = {10.1007/s10462-025-11122-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on persian question answering systems: From traditional to modern approaches},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dmixnet: A dendritic multi-layered perceptron architecture
for image recognition. <em>AIR</em>, <em>58</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s10462-025-11123-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of image recognition, the all-MLP architecture (MLP-Mixer) shows superior performance. However, the current MLP-Mixer is solely based on fully connected layers. The nonlinear capability of fully connected layers is relatively weak, and their simple stacked structure has limitations under complex conditions. Therefore, inspired by the diversity of neurons in the human brain, we propose an innovative DMixNet, a dendritic multi-layered perceptron architecture. Rooted in the theory of dendritic neurons from neuroscience, we propose a dendritic neural unit (DNU) that enhances DMixNet with stronger biological interpretability and more robust nonlinear processing capabilities. The flexibility of dendritic structures allows the DNU to adjust its architecture to achieve different functionalities. Based on the DNU, we propose a novel channel fusion network $$\text {DNU}_\text {E}$$ and a dendritic classifier $$\text {DNU}_\text {C}$$ . The $$\text {DNU}_\text {E}$$ substitutes the traditional two fully connected layers as the channel mixer, constructing a dendritic mixer layer to enhance the fusion capability of channel information within the entire framework. Meanwhile, the $$\text {DNU}_\text {C}$$ replaces the traditional linear classifier, effectively improving the model’s classification performance. Experimental results demonstrate that DMixNet achieves improvements of 2.13%, 4.79%, 4.71%, 23.14% on the CIFAR-10, CIFAR-100, Tiny-ImageNet and COIL-100 benchmark image recognition datasets, respectively, as well as a 14.78% enhancement on the medical image classification dataset PathMNIST, outperforming other state-of-the-art architectures. Code is available at https://github.com/KarilynXu/DMixNet .},
  archive      = {J_AIR},
  author       = {Xu, Weixiang and Song, Yaotong and Gupta, Shubham and Jia, Dongbao and Tang, Jun and Lei, Zhenyu and Gao, Shangce},
  doi          = {10.1007/s10462-025-11123-y},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dmixnet: A dendritic multi-layered perceptron architecture for image recognition},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metaheuristic optimization algorithms for multi-area
economic dispatch of power systems: Part II—a comparative study.
<em>AIR</em>, <em>58</em>(5), 1–51. (<a
href="https://doi.org/10.1007/s10462-025-11125-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Area Economic Dispatch (MAED) plays an important role in the operation and planning of power systems. In Part I of this series, we have summarized various optimization techniques to the MAED problem comprehensively, showing clearly that metaheuristic optimization algorithms (MOAs) have become the dominant approach for solving this problem due to their ease of application and powerful search capability. Although many different types of MOAs have been proposed, there is no study on the comprehensive evaluation, comparison and recommendation of different MOAs for the MAED problem. In this part, we selected 32 algorithms including differential evolution, particle swarm optimization, teaching–learning based algorithm, JAYA algorithm, and their advanced variants to evaluate and compare their performance on the eleven reported MAED cases summarized in Part I of this series. The comparative study was comprehensively conducted based on various performance criteria including solution quality, convergence, robustness, computational efficiency, and statistical analysis. The comparisons reveal that the DE series is the most competitive overall. Nevertheless, there is no single algorithm that ranks in the top three on all cases. This study can provide a practical reference and applicability recommendation for the selection of MOAs for solving the MAED problem.},
  archive      = {J_AIR},
  author       = {Wang, Yang and Xiong, Guojiang},
  doi          = {10.1007/s10462-025-11125-w},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Metaheuristic optimization algorithms for multi-area economic dispatch of power systems: Part II—a comparative study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on EEG-based multimodal learning for emotion
recognition. <em>AIR</em>, <em>58</em>(5), 1–63. (<a
href="https://doi.org/10.1007/s10462-025-11126-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from electroencephalography (EEG) signals is crucial for human–computer interaction yet poses significant challenges. While various techniques exist for detecting emotions through EEG signals, contemporary studies have explored the combination of EEG signals with other modalities. However, the field is still rapidly evolving, and new advancements are constantly being made. Comprehensive research is essential by distilling all factors in one manuscript to stay up-to-date with the latest research findings. This review offers an overview of multimodal leaning in EEG-based emotion recognition and discusses current literature in this domain from 2017 to 2024. Three primary challenges addressed are the fusion algorithm, representation of different modalities, and classification scheme. The review thoroughly explores the challenges of fusion algorithms, representation of different modalities, and classification schemes through empirical studies, offering a detailed analysis of their effectiveness. The approach of fusion algorithms is compared and evaluated based on convention and deep learning fusion methods. The research results show that poor performance is attributed to a lack of rigor and inadequate methods to identify correlated patterns across modalities to create a unified representation for experiments. This indicates a need for more thorough analysis and integration of data in future studies. When more than two modalities are involved, it becomes increasingly important to consider different aspects of classification schemes, such as the number of features and model selection. However, designing a classification scheme without considering the number of parameters and emotional categories may compromise the accuracy of classification. To aid readers in understanding the findings better, the results of different classification schemes and their corresponding accuracies are summarized. The tables in this draft display the fusion algorithms researchers utilize and evaluate the effectiveness of selected modalities, providing valuable insights for decision-making. Key contributions include a systematic survey of EEG features, an exploration of EEG integration with behavioral modalities, an investigation of fusion methods, and an overview of key challenges and future research directions in implementing multimodal emotion recognition systems.},
  archive      = {J_AIR},
  author       = {Pillalamarri, Rajasekhar and Shanmugam, Udhayakumar},
  doi          = {10.1007/s10462-025-11126-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on EEG-based multimodal learning for emotion recognition},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A synergetic intuitionistic fuzzy model combining AHP,
entropy, and ELECTRE for data fabric solution selection. <em>AIR</em>,
<em>58</em>(5), 1–54. (<a
href="https://doi.org/10.1007/s10462-025-11128-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amidst the ongoing digital transformation, enterprises face the challenge of managing ever-expanding volumes of data from multiple sources and diverse structures. Semantic data fabric emerges as a promising solution, offering an innovative approach to integrate data resources from various channels and produce meaningful insights. The selection of an appropriate data fabric solution has become a focal point amidst burgeoning data lakes and silos, garnering international attention. This research aims to precisely evaluate potential data fabric solutions using an innovative synergetic intuitionistic fuzzy evaluation model. We propose a hybrid approach, IF-AHP-Entropy-ELECTRE, which integrates the analytic hierarchy process (AHP), entropy, and elimination et choix traduisant la réalité (ELECTRE) techniques within the framework of intuitionistic fuzzy (IF) sets. This model is utilized to a data fabric solution selection (DFSS) issue for an appliance company, identifying the optimal solution based on its superior performance in foundational technology, real-time analytics, and customizable features. The effectiveness and adaptability of this approach stem from a novel hierarchical evaluative criteria system encompassing technology, capability, cost, and security. The criteria weights, derived from IF-AHP-Entropy, reflect both subjective and objective judgments of decision-makers, while the ranking generated by IF-ELECTRE employs a piecewise scoring function and a unique distance measure, factoring in optimistic perspectives and cross-information. Through sensitivity and comparative analyses, our approach demonstrates enhanced robustness, precision, and adaptability in dynamic DFSS contexts when compared to traditional multicriteria decision-making methods, such as IF-WSM, IF-TOPSIS, and IF-ELECTRE. Specifically, our model provides a decision support system that combines extensive functionality with a user-friendly design, making it highly effective for DFSS challenges. This approach not only establishes a solid foundation for data integration in data management but also enhances business competitiveness and supports sustained growth in the digital economy.},
  archive      = {J_AIR},
  author       = {Zhou, Fang and Chen, Ting-Yu},
  doi          = {10.1007/s10462-025-11128-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A synergetic intuitionistic fuzzy model combining AHP, entropy, and ELECTRE for data fabric solution selection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel reinforcement learning-based multi-operator
differential evolution with cubic spline for the path planning problem.
<em>AIR</em>, <em>58</em>(5), 1–56. (<a
href="https://doi.org/10.1007/s10462-025-11129-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning in autonomous driving systems remains a critical challenge, requiring algorithms capable of generating safe, efficient, and reliable routes. Existing state-of-the-art methods, including graph-based and sampling-based approaches, often produce sharp, suboptimal paths and struggle in complex search spaces, while trajectory-based algorithms suffer from high computational costs. Recently, meta-heuristic optimization algorithms have shown effective performance but often lack learning ability due to their inherent randomness. This paper introduces a unified benchmarking framework, named Reda’s Path Planning Benchmark 2024 (RP2B-24), alongside two novel reinforcement learning (RL)-based path-planning algorithms: Q-Spline Multi-Operator Differential Evolution (QSMODE), utilizing Q-learning (Q-tables), and Deep Q-Spline Multi-Operator Differential Evolution (DQSMODE), based on Deep Q-networks (DQN). Both algorithms are integrated under a single framework and enhanced with cubic spline interpolation to improve path smoothness and adaptability. The proposed RP2B-24 library comprises 50 distinct benchmark problems, offering a comprehensive and generalizable testing ground for diverse path-planning algorithms. Unlike traditional approaches, RL in QSMODE/DQSMODE is not merely a parameter adjustment method but is fully utilized to generate paths based on the accumulated search experience to enhance path quality. QSMODE/DQSMODE introduces a unique self-training update mechanism for the Q-table and DQN based on candidate paths within the algorithm’s population, complemented by a secondary update method that increases population diversity through random action selection. An adaptive RL switching probability dynamically alternates between these Q-table update modes. DQSMODE and QSMODE demonstrated superior performance, outperforming 22 state-of-the-art algorithms, including the IMODEII. The algorithms ranked first and second in the Friedman test and SNE-SR ranking test, achieving scores of 99.2877 (DQSMODE) and 93.0463 (QSMODE), with statistically significant results in the Wilcoxon test. The practical applicability of the algorithm was validated on a ROS-based system using a four-wheel differential drive robot, which successfully followed the planned paths in two driving scenarios, demonstrating the algorithm’s feasibility and effectiveness for real-world scenarios. The source code for the proposed benchmark and algorithm is publicly available for further research and experimentation at: https://github.com/MohamedRedaMu/RP2B24-Benchmark and https://github.com/MohamedRedaMu/QSMODEAlgorithm .},
  archive      = {J_AIR},
  author       = {Reda, Mohamed and Onsy, Ahmed and Haikal, Amira Y. and Ghanbari, Ali},
  doi          = {10.1007/s10462-025-11129-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel reinforcement learning-based multi-operator differential evolution with cubic spline for the path planning problem},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of belief entropies: From the perspective of
evidential neural network. <em>AIR</em>, <em>58</em>(5), 1–34. (<a
href="https://doi.org/10.1007/s10462-025-11130-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Dempster-Shafer’s theory, the belief entropy for total uncertainty measure of mass function has attracted the interest of many researchers in recent years. Although various belief entropies can meet some basic requirements, how to judge the performance of belief entropies is still an open issue. This paper proposes a novel evidential neural network (ENN) classifier to evaluate different belief entropies in practical application. Driven by the least commitment principle (LCP), the maximum entropy is integrated into the traditional divergence-based loss function. The proposed loss function consists of divergence and maximum entropy parts, which considers not only the distribution difference but also the degree of approaching the maximum entropy. Some classification experiments are conducted in 7 real-world datasets to validate the effectiveness of the proposed evaluation method.},
  archive      = {J_AIR},
  author       = {Mao, Kun and Wang, Yanni and Zhou, Wen and Ye, Jiangang and Fang, Bin},
  doi          = {10.1007/s10462-025-11130-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Evaluation of belief entropies: From the perspective of evidential neural network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An actor-critic based recommender system with context-aware
user modeling. <em>AIR</em>, <em>58</em>(5), 1–40. (<a
href="https://doi.org/10.1007/s10462-025-11134-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems empower users with tailored service assistance by learning about their interactions with systems and recommending items based on their preferences and interests. Typical recommender systems view the recommendation process as a static procedure disregarding the fact that users’ preferences are changed over time. Reinforcement learning (RL) approaches are the most advanced and recent techniques used by researchers to handle challenges where the user’s interest is captured by their most recent interactions with the system. However, most of the recent research on RL-based recommender systems focuses on simply the user’s recent interactions to generate the recommendations without taking into account the context of the user in which these interactions occur. The context has a great impact on users’ interests, behaviors, and ratings e.g., user mood, time, day type, companion, social circle, and location. In this paper, we propose a context-aware deep reinforcement learning-based recommender system focusing on context-specific state modeling methods. In this approach, states are designed based on the user’s most recent context. In parallel, a list-wise version of the context-aware recommender agent is also proposed, in which a list of items is recommended to users at each step of interaction based on their context. The findings of the study indicate that modeling users’ preferences in combination with contextual variables improves the performance of RL-based recommender systems. Furthermore, we evaluate the proposed method on context-based datasets in an offline environment. The performance in terms of evaluation measures optimally indicates the worth of the proposed method in comparison with existing studies. More precisely, the highest Presicion@5, MAP@10, and NDCG@10 of the context-aware recommender agent are 77%, 76%, and 74% respectively.},
  archive      = {J_AIR},
  author       = {Bukhari, Maryam and Maqsood, Muazzam and Adil, Farhan},
  doi          = {10.1007/s10462-025-11134-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An actor-critic based recommender system with context-aware user modeling},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A critical review of artificial intelligence based
techniques for automatic prediction of cephalometric landmarks.
<em>AIR</em>, <em>58</em>(5), 1–56. (<a
href="https://doi.org/10.1007/s10462-025-11135-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic cephalometric landmark detection has emerged as a pivotal area of research that combines medical imaging, computer vision, and orthodontics. The identification of cephalometric landmarks is of utmost importance in the field of orthodontics, as it contributes significantly to the process of diagnosing and planning treatments, as well as conducting research on craniofacial aspects. This practice holds the potential to improve clinical decision-making and ultimately increase the outcomes for patients. This work explores a wide range of strategies, encompassing both traditional edge-based methods and advanced deep learning approaches. The study leveraged various academic publication databases like IEEEXplore, ScienceDirect, arXiv, Springer and PubMed to thoroughly search for articles related to automatic cephalometric landmark detection. Additionally, other pertinent publications were acquired from credible sources like Google Scholar and Wiley databases. Screening the articles relied on three selection criteria: (a) publication titles, abstracts, literature reviews, (b) cephalometric radiograph datasets suitable for 2D landmarking, and (c) studies conducted over different time periods were employed to gain a comprehensive understanding of the evolution of methodologies used in landmark prediction to identify the most relevant papers for this review. The initial electronic database search identified 268 papers on landmark detection. A total of 118 publications were selected and incorporated in the present study after a meticulous screening process. Performance analysis was conducted on studies that reported Successful Detection Rates (SDRs) within different clinically accepted precision ranges, Mean Radial Error (MRE) with Standard Deviation (SD) between manually annotated and automated landmarks as outcomes. Bar graphs and custom combination plots were utilized to analyse the correlations among different methodologies employed and their evaluation metrics outcomes. The performance comparison results indicate that Deep Learning techniques showed superior accuracy in automating 2D cephalometric landmarks compared to other conventional and Machine Learning approaches. Recently, more advanced Deep Learning algorithms have been developed to improve the accuracy of automatic landmark prediction.},
  archive      = {J_AIR},
  author       = {Neeraja, R. and Anbarasi, L. Jani},
  doi          = {10.1007/s10462-025-11135-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A critical review of artificial intelligence based techniques for automatic prediction of cephalometric landmarks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum deep learning in neuroinformatics: A systematic
review. <em>AIR</em>, <em>58</em>(5), 1–24. (<a
href="https://doi.org/10.1007/s10462-025-11136-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroinformatics involves replicating and detecting intricate brain activities through computational models, where deep learning plays a foundational role. Our systematic review explores quantum deep learning (QDL), an emerging deep learning sub-field, to assess whether quantum-based approaches outperform classical approaches in brain data learning tasks. This review is a pioneering effort to compare these deep learning domains. In addition, we survey neuroinformatics and its various subdomains to understand the current state of the field and where QDL stands relative to recent advancements. Our statistical analysis of tumor classification studies (n = 16) reveals that QDL models achieved a mean accuracy of 0.9701 (95% CI 0.9533–0.9868), slightly outperforming classical models with a mean accuracy of 0.9650 (95% CI 0.9475–0.9825). We observed similar trends across Alzheimer’s diagnosis, stroke lesion detection, cognitive state monitoring, and brain age prediction, with QDL demonstrating better performance in metrics such as F1-score, dice coefficient, and RMSE. Our findings, paired with prior documented quantum advantages, highlight QDL’s promise in healthcare applications as quantum technology evolves. Our discussion outlines existing research gaps with the intent of encouraging further investigation in this developing field.},
  archive      = {J_AIR},
  author       = {Orka, Nabil Anan and Awal, Md. Abdul and Liò, Pietro and Pogrebna, Ganna and Ross, Allen G. and Moni, Mohammad Ali},
  doi          = {10.1007/s10462-025-11136-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Quantum deep learning in neuroinformatics: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging generative AI synthetic and social media data for
content generalizability to overcome data constraints in vision deep
learning. <em>AIR</em>, <em>58</em>(5), 1–24. (<a
href="https://doi.org/10.1007/s10462-025-11137-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizing deep learning models across diverse content types is a persistent challenge in domains like facial emotion recognition (FER), where datasets often fail to reflect the wide range of emotional responses triggered by different stimuli. This study addresses the issue of content generalizability by comparing FER model performance between models trained on video data collected in a controlled laboratory environment, data extracted from a social media platform (YouTube), and synthetic data generated using Generative Adversarial Networks. The videos focus on facial reactions to advertisements, and the integration of these different data sources seeks to address underrepresented advertisement genres, emotional reactions, and individual diversity. Our FER models leverage Convolutional Neural Networks Xception architecture, which is fine-tuned using category based sampling. This ensures training and validation data represent diverse advertisement categories, while testing data includes novel content to evaluate generalizability rigorously. Precision–recall curves and ROC-AUC metrics are used to assess performance. Results indicate a 7% improvement in accuracy and a 12% increase in precision–recall AUC when combining real-world social media and synthetic data, demonstrating reduced overfitting and enhanced content generalizability. These findings highlight the effectiveness of integrating synthetic and real-world data to build FER systems that perform reliably across more diverse and representative content.},
  archive      = {J_AIR},
  author       = {Alipour, Panteha and Gallegos, Erika},
  doi          = {10.1007/s10462-025-11137-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Leveraging generative AI synthetic and social media data for content generalizability to overcome data constraints in vision deep learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued intuitionistic fuzzy generator based
low-light enhancement model for referenced image datasets. <em>AIR</em>,
<em>58</em>(5), 1–31. (<a
href="https://doi.org/10.1007/s10462-025-11138-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing is a rapidly evolving research field with diverse applications across science and technology, including biometric systems, surveillance, traffic signal control and medical imaging. Digital images taken in low-light conditions are often affected by poor contrast and pixel detail, leading to uncertainty. Although various fuzzy based techniques have been proposed for low-light image enhancement, there remains a need for a model that can manage greater uncertainty while providing better structural information. To address this, an interval-valued intuitionistic fuzzy generator is proposed to develop an advanced low-light image enhancement model for referenced image datasets. The enhancement process involves a structural similarity index measure (SSIM) based optimization approach with respect to the parameters of the generator. For experimental validation, the Low-Light (LOL), LOLv2-Real and LOLv2-Synthetic benchmark datasets are utilized. The results are compared with several existing techniques using quality metrics such as SSIM, peak signal-to-noise ratio, absolute mean brightness error, mean absolute error, root mean squared error, blind/referenceless image spatial quality evaluator and naturalness image quality evaluator, demonstrating the superiority of the proposed model. Ultimately, the model’s performance is benchmarked against state-of-the-art methods, highlighting its enhanced efficiency.},
  archive      = {J_AIR},
  author       = {Selvam, Chithra and Sundaram, Dhanasekar},
  doi          = {10.1007/s10462-025-11138-5},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Interval-valued intuitionistic fuzzy generator based low-light enhancement model for referenced image datasets},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-way decision model for multi-granular support
intuitionistic fuzzy rough sets based on overlap functions.
<em>AIR</em>, <em>58</em>(5), 1–44. (<a
href="https://doi.org/10.1007/s10462-025-11139-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision-making provides an effective framework for addressing uncertainty, aligning closely with human cognitive decision patterns. This paper proposes a novel three-way decision model based on multi-granular support intuitionistic fuzzy rough sets, integrating n-dimensional overlap and grouping functions. The model constructs optimistic and pessimistic upper and lower approximations to optimize decision rules and introduces score and precision functions for ranking. To validate the model, a consumer decision-making algorithm was developed and applied to empirical data. The results demonstrate that the proposed model effectively narrows decision boundary regions, enhances decision-making precision, and supports decision-making in complex multi-attribute scenarios. This study not only advances rough set theory but also offers practical tools for addressing real-world uncertainty in decision-making.},
  archive      = {J_AIR},
  author       = {Yu, Peng and Zhao, Xiyue},
  doi          = {10.1007/s10462-025-11139-4},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A three-way decision model for multi-granular support intuitionistic fuzzy rough sets based on overlap functions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An assessment framework for explainable AI with applications
to cybersecurity. <em>AIR</em>, <em>58</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s10462-025-11141-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several explainable AI methods are available, but there is a lack of a systematic comparison of such methods. This paper contributes in this direction, by providing a framework for comparing alternative explanations in terms of complexity and robustness. We exemplify our proposal on a real case study in the cybersecurity domain, namely, phishing website detection. In fact, in this domain explainability is a compelling issue because of its potential benefits for the detection of fraudulent attacks and for the design of efficient security defense mechanisms. For this purpose, we apply our methodology to the machine learning models obtained by analyzing a publicly available dataset containing features extracted from malicious and legitimate web pages. The experiments show that our methodology is quite effective in selecting the explainability method which is, at the same time, less complex and more robust.},
  archive      = {J_AIR},
  author       = {Calzarossa, Maria Carla and Giudici, Paolo and Zieni, Rasha},
  doi          = {10.1007/s10462-025-11141-w},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An assessment framework for explainable AI with applications to cybersecurity},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing dance movements using a mathematical model
based on optimized nature-inspired machine learning. <em>AIR</em>,
<em>58</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10462-025-11142-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recording dance movements nowadays becomes problematic due to complex recording procedures and unavoidable data loss caused by some resource elements, like bodily or clothing material composition. The task of filling in the missing data for the performed motion and retrieving the sequence as a whole becomes difficult due to the characteristics of physical motion, which include cinematographic perspectives that render the movements themselves non-linear. Previous works have indicated some level of success in loss motion recovery, but only for a short span. The first two-dimensional matrix computation paradigm lacks theoretical justification for the recovery of the non-linear motion information, which is a limitation. This issue has been addressed by developing a new enhanced model called the Machine Learning 2-Dimensional Matrix-Calculation (ML-2DMC), which is presumably designed to achieve the rehabilitation and recovery of human movement and dance. The proposed procedure takes advantage of the effectiveness of the machine learning algorithms and applies 2D matrix computation methods, permitting good results across a variety of experiments. A new method called fractal-chaotic map grey wolf optimizer (FCM-GWO) is introduced to optimize the parameters of ML-2DMC. This optimization itself increases the efficiency of the ML-2DMC model when it comes to the retrieval of complex movements of the processes involving dance. The paper gives experimental results validating the efficiency of the proposed approach against other methods, such as recurrent convolutional neural networks and other more sophisticated models and approaches incorporating multi-paradigm sensors and devices such as Kinect sensors along with low-rank matrix completion methods. The study shows that the ML-2DMC-FCM-GWO method effectively tackles the complexities of non-linear human motion and dance recovery, making a significant addition to the field of motion analysis and restoration.},
  archive      = {J_AIR},
  author       = {Song, Jing and Ding, Li},
  doi          = {10.1007/s10462-025-11142-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Reconstructing dance movements using a mathematical model based on optimized nature-inspired machine learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based bridge maintenance management: A comprehensive
review. <em>AIR</em>, <em>58</em>(5), 1–43. (<a
href="https://doi.org/10.1007/s10462-025-11144-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over recent decades, the implementation of Artificial Intelligence (AI) across various industrial fields from automation to cybersecurity has been transformative. Whilst the implementations of linking AI and data sciences remain complex and thus limited, they both aim to harness data for actionable insights and future predictions. A research focal point in the application of AI in maintenance is crucial for the sustainability and efficiency of assets. Typically, in the civil infrastructure, there are significant benefits to be gained from AI-driven applications. This study reviews the implementation of the AI in bridge maintenance decision-making by conducting a review of literature on major works undertaken by researchers and analysing 102 scientific articles published from 2010 to 2023. Our literature review revealed an emerging trend in recent studies, focusing on the exploration of defect prognosis in bridge maintenance. However, upon further analysis, it becomes evident that there is a notable gap in the existing literature, in the studies related to performance-based prognostic maintenance strategies for bridges. This gap presents an opportunity for future research, one that could yield valuable insights in the field of bridge maintenance and asset management. The review also reveals the focus of the existing literature on defect identification by using the bridge imagery processing. While the AI’s potential in damage detection using bridge imagery is evident, challenges persist including the computational processing and data availability. This review of the literature includes a comprehensive overview of the current implementation of AI in bridge maintenance, highlighting limitations, challenges, and prospective directions.},
  archive      = {J_AIR},
  author       = {Shahrivar, Farham and Sidiq, Amir and Mahmoodian, Mojtaba and Jayasinghe, Sanduni and Sun, Zhiyan and Setunge, Sujeeva},
  doi          = {10.1007/s10462-025-11144-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-based bridge maintenance management: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy fusion mayfly algorithm on task offloading
and scheduling for IoT-based fog computing multi-tasks learning.
<em>AIR</em>, <em>58</em>(5), 1–46. (<a
href="https://doi.org/10.1007/s10462-025-11145-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Internet of Things (IoT) technology has accumulated a large amount of data, which needs to be stored, processed and deeply analyzed to meet the specific goals and needs of users. As an emerging computing model, Fog computing can allocate a large number of computing resources reasonably. In order to solve the problem of insufficient population diversity and low algorithm efficiency, Aiming at the task scheduling problem of Bag-of-Tasks(BoT) application in cloud and fog environment, a multi-strategy fusion Mayfly Algorithm was proposed. The method of improving the individual learning coefficient and the global learning coefficient is used to significantly improve the convergence speed, local search ability, and global search ability, and then the method of improving the social positive attraction coefficient is used to balance the development and exploration ability of the algorithm and help the algorithm to get rid of the local optimum. The main goal of the logarithm Mayfly Algorithm (lMA) is to complete the tasks of the IoT task package in the fog system efficiently with low cost in terms of reducing execution time and operating costs. The improved algorithms were compared with Mayfly Algorithm (MA), Genetic Algorithm (GA), Grey Wolf Optimizer (GWO), Tyrannosaurus Optimization Algorithm (TROA), Harris Hawks Optimization (HHO), Reptile Search Algorithm (RSA) and Red-Tailed Hawk Algorithm (RTH), and the results showed that lMA was significantly improved in terms of reducing processing time and operating cost. The performance of lMA is verified, and the results show that the model can not only save transmission energy consumption but also have good convergence.},
  archive      = {J_AIR},
  author       = {Sui, Xiao-Fei and Wang, Jie-Sheng and Zhang, Shi-Hui and Zhang, Si-Wen and Zhang, Yun-Hao},
  doi          = {10.1007/s10462-025-11145-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-strategy fusion mayfly algorithm on task offloading and scheduling for IoT-based fog computing multi-tasks learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-attribute decision-making using q-rung orthopair fuzzy
zagreb index. <em>AIR</em>, <em>58</em>(5), 1–31. (<a
href="https://doi.org/10.1007/s10462-025-11149-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The q-rung orthopair fuzzy set (q-ROFS), an extension of intuitionistic and Pythagorean fuzzy sets, offers greater flexibility in representing vague information with two possible outcomes, yes or no. The fuzzy Zagreb index is an important graph parameter, widely used in fields such as network theory, spectral graph theory, mathematics, and molecular chemistry. In this paper, the first and second Zagreb indices for q-rung orthopair fuzzy graphs (q-ROFGs) are introduced, and bounds for these indices are established, including their behavior in regular q-ROFGs. Additionally, it is explored, how various graph operations such as union, Cartesian product, direct product, and lexicographical product affect the first Zagreb index. Furthermore, a new approach is presented that combines Multiple-Attribute Decision-Making (MADM) with graph-based models to improve decision-making, particularly in vaccine selection. The methodology constructs a bipartite graph for each attribute, where virologists assign membership and non-membership values to vaccines. The Zagreb index is used to measure the importance of each vaccine, and a weighted aggregation technique normalizes the scores. The final ranking is derived from a computed score function. The results demonstrate the effectiveness of the approach in providing a systematic and mathematically rigorous framework for multi-attribute decision-making, with rank correlation analysis confirming its robustness compared to existing methods such as q-ROF PROMETHEE, q-ROF VICOR, q-ROF TOPSIS, q-ROFWG, and q-ROFWA.},
  archive      = {J_AIR},
  author       = {Rao, Yongsheng and Kosari, Saeed and Hameed, Saira and Yousaf, Zulqarnain},
  doi          = {10.1007/s10462-025-11149-2},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-attribute decision-making using q-rung orthopair fuzzy zagreb index},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Csan: Cross-coupled semantic adversarial network for
cross-modal retrieval. <em>AIR</em>, <em>58</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10462-025-11152-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims to correlate multimedia data by bridging the heterogeneity gap. Most cross-modal retrieval approaches learn a common subspace to project the multimedia data into the subspace for directly measuring the similarity. However, the existing cross-modal retrieval frameworks cannot fully capture the semantic consistency in the limited supervision information. In this paper, we propose a Cross-coupled Semantic Adversarial Network (CSAN) for cross-modal retrieval. The main structure of this approach is mainly composed of the generative adversarial network, i.e., each modality branch is equipped with a generator and a discriminator. Besides, a cross-coupled semantic architecture is designed to fully explore the correlation of paired heterogeneous samples. To be specific, we couple a forward branch with an inverse mapping and implement a weight-sharing strategy of the inverse mapping branch to the branch of another modality. Furthermore, a cross-coupled consistency loss is introduced to minimize the semantic gap between the representations of the inverse mapping branch and the forward branch. Extensive qualitative and quantitative experiments are conducted to evaluate the performance of the proposed approach. By comparing against the previous works, the experiment results demonstrate our approach outperforms state-of-the-art works.},
  archive      = {J_AIR},
  author       = {Li, Zhuoyi and Lu, Huibin and Fu, Hao and Meng, Fanzhen and Gu, Guanghua},
  doi          = {10.1007/s10462-025-11152-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Csan: Cross-coupled semantic adversarial network for cross-modal retrieval},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of artificial intelligence - based
algorithm towards fetal facial anomalies detection (2013–2024).
<em>AIR</em>, <em>58</em>(5), 1–49. (<a
href="https://doi.org/10.1007/s10462-025-11160-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review explores the growing need for AI-based algorithms in diagnosing fetal facial anomalies, which are often difficult to detect due to limitations in current imaging techniques like ultrasound and MRI. These challenges include low resolution, motion artifacts, and insufficient annotated data, which hinder early and accurate diagnosis. AI algorithms, particularly deep learning models like Convolutional Neural Networks (CNNs) and U-Net, offers significant potential to overcome these challenges by analyzing large datasets and improving image analysis. Early diagnosis of these anomalies is crucial for enabling timely interventions, personalized treatment plans, and better prenatal care. This study adopts a systematic review approach, to assess existing research on AI-based approaches for fetal facial anomaly detection. The review includes peer-reviewed studies from key biomedical databases like PubMed, IEEE Xplore, and ScienceDirect, focusing on the last 15 years. Studies that implemented AI techniques and manual techniques for detecting anomalies in prenatal images were considered. Among all models reviewed, CNNs and U-Net architectures were found to be the most effective. CNNs excel at classifying medical images, while U-Net is particularly powerful for image segmentation. These models have demonstrated high accuracy in identifying conditions such as cleft lip, palate, and micrognathia. The use of AI in clinical settings can greatly enhance the precision and efficiency of fetal anomaly detection, addressing current limitations in medical imaging. By integrating AI, particularly deep learning models, into clinical workflows, prenatal care can be transformed, allowing for earlier and more accurate diagnosis. This can lead to more personalized care, timely interventions, and ultimately improved health outcomes for affected individuals and their families.},
  archive      = {J_AIR},
  author       = {Sriraam, Natarajan and Chinta, Babu and Seshadri, Suresh and Suresh, Sudarshan},
  doi          = {10.1007/s10462-025-11160-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of artificial intelligence - based algorithm towards fetal facial anomalies detection (2013–2024)},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic brain MRI tumors segmentation based on deep fusion
of weak edge and context features. <em>AIR</em>, <em>58</em>(5), 1–25.
(<a href="https://doi.org/10.1007/s10462-025-11151-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors pose a significant health risk to humans. The edge boundaries in brain magnetic resonance imaging (MRI) are often blurred and poorly defined, which can easily result in inaccurate segmentation of lesion areas. To address these challenges, we proposed an Automatic Brain MRI Tumor Segmentation based on deep fusion of Weak Edge and Context features (AS-WEC). First, AS-WEC introduces the Otsu Double Threshold Weak Edges Adaptive Detection (Otsu-WD), which focuses on tumor edge information and differentiates between lesion edges and normal cerebral sulci and gyri. Second, an edge branching network based on the Gated Recurrent Unit (GRU) is constructed to fully preserve the edge context information of the lesion region. Finally, a maximum index fusion mechanism has been designed to incorporate a multilayer feature map, preventing the loss of edge details during the deep feature fusion process. The experimental results demonstrate that the Otsu-WD method outperforms the Canny and TEED algorithms in detecting brain MRI tumor edges. In brain MRI tumor segmentation, AS-WEC delivers a clearer visual segmentation effect compared to the classical UNet++ network and recent models like PVT-Former. On both datasets, AS-WEC demonstrated improvements across multiple metrics. The Dice averaged 92.96%, and the mIoU reached 93.12%, effectively validating the method’s efficacy in brain MRI tumor segmentation. Code and pre-trained models are available at https://github.com/DL-Segment/AS-WEC.git .},
  archive      = {J_AIR},
  author       = {Xiao, Leyi and Zhou, Baoxian and Fan, Chaodong},
  doi          = {10.1007/s10462-025-11151-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic brain MRI tumors segmentation based on deep fusion of weak edge and context features},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised constrained clustering: An in-depth
overview, ranked taxonomy and future research directions. <em>AIR</em>,
<em>58</em>(5), 1–127. (<a
href="https://doi.org/10.1007/s10462-024-11103-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a well-known unsupervised machine learning approach capable of automatically grouping discrete sets of instances with similar characteristics. Constrained clustering is a semi-supervised extension to this process that can be used when expert knowledge is available to indicate constraints that can be exploited. Well-known examples of such constraints are must-link (indicating that two instances belong to the same group) and cannot-link (two instances definitely do not belong together). The research area of constrained clustering has grown significantly over the years with a large variety of new algorithms and more advanced types of constraints being proposed. However, no unifying overview is available to easily understand the wide variety of available methods, constraints and benchmarks. To remedy this, this study presents in-detail the background of constrained clustering and provides a novel ranked taxonomy of the types of constraints that can be used in constrained clustering. In addition, it focuses on the instance-level pairwise constraints, and gives an overview of its applications and its historical context. Finally, it presents a statistical analysis covering 315 constrained clustering methods, categorizes them according to their features, and provides a ranking score indicating which methods have the most potential based on their popularity and validation quality. Finally, based upon this analysis, potential pitfalls and future research directions are provided.},
  archive      = {J_AIR},
  author       = {González-Almagro, Germán and Peralta, Daniel and De Poorter, Eli and Cano, José-Ramón and García, Salvador},
  doi          = {10.1007/s10462-024-11103-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-127},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Semi-supervised constrained clustering: An in-depth overview, ranked taxonomy and future research directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum encoding whale optimization algorithm for global
optimization and adaptive infinite impulse response system
identification. <em>AIR</em>, <em>58</em>(5), 1–58. (<a
href="https://doi.org/10.1007/s10462-025-11120-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The whale optimization algorithm (WOA) is motivated by the predatory nature of bubble nets and mimics dwindling and encircling, bubble net persecuting, and randomized wandering and foraging actions to locate the expansive adequate value. However, the WOA has several deficiencies: inadequate resolution accuracy, sluggish convergence speed, susceptibility to search stagnation, and insufficient localized detection efficiency. A quantum encoding WOA (QWOA) is introduced for global optimization and adaptive infinite impulse response (IIR) system identification. The quantum encoding mechanism exploits the principle of a quantum bit to encode a search agent, which manipulates the state of an essential quantum bit and amends the location data. The quantum rotation gate modulates the quantum bit’s configuration, the quantum NOT gate accomplishes bit mutation and prohibits precocious convergence. The probability amplitude of the quantum bit represents the multistate superposition state of the search agent, which enriches the population diversity, advances individualized information, broadens the detection scope, inhibits premature convergence, facilitates estimation effectiveness, and promotes solution accuracy. The QWOA not only promptly locates the search scope nearest the most appropriate solution but also computes the spiral-shaped encircling route to promote predation diversification. Twenty-three benchmark functions, eight real-world engineering layouts, and adaptive IIR system identification are utilized to assess the QWOA’s feasibility and effectiveness. The experimental results reveal that QWOA successfully equalizes exploration and exploitation to accelerate convergence speed, ameliorate calculation accuracy, and strengthen stability and robustness.},
  archive      = {J_AIR},
  author       = {Zhang, Jinzhong and Liu, Wei and Zhang, Gang and Zhang, Tan},
  doi          = {10.1007/s10462-025-11120-1},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Quantum encoding whale optimization algorithm for global optimization and adaptive infinite impulse response system identification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging machine learning and peptide design for cancer
treatment: A comprehensive review. <em>AIR</em>, <em>58</em>(5), 1–59.
(<a href="https://doi.org/10.1007/s10462-025-11148-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anticancer peptides (ACPs) offer a promising alternative to traditional cancer therapies due to their specificity and reduced side effects. The development of ACPs using machine learning (ML) and deep learning (DL) follows a structured process, beginning with sequence collection from in vitro and in vivo experiments. Key features such as hydrophobicity and secondary structure are extracted, and classification models categorize peptides based on their properties. ML models predict anticancer effectiveness, followed by toxicity checks and Structure-Activity Relationship (SAR) analysis to ensure safety and efficacy, with validation tests confirming their activity. This review explores how the automated design of ACPs can be enhanced by leveraging advanced ML and DL techniques. These methods, with their ability to automate feature selection and activity prediction, have significantly improved the efficiency and accuracy of peptide discovery. This structured approach holds high potential to guide researchers in the automated design of ACPs, accelerating the discovery of effective peptides while ensuring safety. Special attention is given to new approaches such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs), which show promise in addressing key challenges like data imbalance and computational complexity. Moreover, we examine the latest published research to compare the performance of various ML models in ACP prediction. By considering these advancements and challenges, this review outlines future opportunities for improving the scalability and reliability of ACP discovery using AI-driven techniques. This structured approach underscores the transformative impact of automation in peptide design, pushing the boundaries of modern cancer therapy development.},
  archive      = {J_AIR},
  author       = {Rezaee, Khosro and Eslami, Hossein},
  doi          = {10.1007/s10462-025-11148-3},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Bridging machine learning and peptide design for cancer treatment: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of artificial intelligence technology in the
study of anthropogenic earthquakes: A review. <em>AIR</em>,
<em>58</em>(5), 1–42. (<a
href="https://doi.org/10.1007/s10462-025-11157-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has emerged as a crucial tool in the monitoring and research of anthropogenic earthquakes (AEs). Despite its utility, AEs monitoring faces significant challenges due to the intricate signal characteristics of seismic events, low signal-to-noise ratio (SNR) in data, and insufficient spatial coverage of monitoring networks, which complicate the effective deployment of AI technologies. This review systematically explores recent advancements in AI applications for identifying and classifying AEs, detecting weak signals, phase picking, event localization, and seismic risk analysis, while highlighting current issues and future developmental directions. Key challenges include accurately distinguishing specific anthropogenic seismic events due to their intricate signal patterns, limited model generalizability caused by constrained training datasets, and the lack of comprehensive models capable of handling event recognition, detection, and classification across diverse scenarios. Despite these obstacles, innovative approaches such as data-sharing platforms, transfer learning (TL), and hybrid AI models offer promising solutions to enhance AEs monitoring and improve predictive capabilities for induced seismic hazards. This review provides a scientific foundation to guide the ongoing development and application of AI technologies in AEs monitoring, forecasting, and disaster mitigation.},
  archive      = {J_AIR},
  author       = {Li, Jingwei and Zhai, Hongyu and Jiang, Changsheng and Wang, Ziang and Wang, Peng and Chang, Xu and Zhang, Yan and Wei, Yonggang and Si, Zhengya},
  doi          = {10.1007/s10462-025-11157-2},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of artificial intelligence technology in the study of anthropogenic earthquakes: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="aism---6">AISM - 6</h2>
<ul>
<li><details>
<summary>
(2025). Model free feature screening for large scale and ultrahigh
dimensional survival data. <em>AISM</em>, <em>77</em>(1), 155–190. (<a
href="https://doi.org/10.1007/s10463-024-00912-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a novel perspective on feature screening in the analysis of high-dimensional right-censored large-p-large-N survival data. The research introduces a distributed feature screening method known as Aggregated Distance Correlation Screening (ADCS). The proposed screening framework involves expressing the distance correlation measure as a function of multiple component parameters, each of which can be estimated in a distributed manner using a natural U-statistic from data segments. By aggregating the component estimates, a final correlation estimate is obtained, facilitating feature screening. Importantly, this approach does not necessitate any specific model specification for responses or predictors and is effective with heavy-tailed data. The study establishes the consistency of the proposed aggregated correlation estimator $$\widetilde{\omega }_{j}$$ under mild conditions and demonstrates the sure screening property of the ADCS. Empirical results from both simulated and real datasets confirm the efficacy and practicality of the ADCS approach proposed in this paper.},
  archive      = {J_AISM},
  author       = {Pan, Yingli and Wang, Haoyu and Liu, Zhan},
  doi          = {10.1007/s10463-024-00912-x},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {155-190},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Model free feature screening for large scale and ultrahigh dimensional survival data},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information projection approach to smoothed propensity score
weighting for handling selection bias under missing at random.
<em>AISM</em>, <em>77</em>(1), 127–153. (<a
href="https://doi.org/10.1007/s10463-024-00913-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity score weighting is widely used to correct the selection bias in the sample with missing data. The propensity score function is often developed using a model for the response probability, which completely ignores the outcome regression model. In this paper, we explore an alternative approach by developing smoothed propensity score weights that provide a more efficient estimation by removing unnecessary auxiliary variables in the propensity score model. The smoothed propensity score function is obtained by applying the information projection of the original propensity score function to the space that satisfies the moment conditions on the balancing scores obtained from the outcome regression model. By including the covariates for the outcome regression models only in the density ratio model, we can achieve an efficiency gain. Penalized regression is used to identify important covariates. Some limited simulation studies are presented to compare with the existing methods.},
  archive      = {J_AISM},
  author       = {Wang, Hengfang and Kim, Jae Kwang},
  doi          = {10.1007/s10463-024-00913-w},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {127-153},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Information projection approach to smoothed propensity score weighting for handling selection bias under missing at random},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved confidence intervals for nonlinear mixed-effects
and nonparametric regression models. <em>AISM</em>, <em>77</em>(1),
105–126. (<a href="https://doi.org/10.1007/s10463-024-00909-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inference for high-dimensional parameters (HDPs) can leverage their intrinsic correlations, as spatially or temporally close parameters tend to have similar values. This is why nonlinear mixed-effects models (NMMs) are commonly used for HDPs. Conversely, in many practical applications, the random effects (REs) in NMMs are correlated HDPs that should remain constant during repeated sampling for frequentist inference. In both scenarios, the inference should be conditional on REs, instead of marginal inference by integrating out REs. We summarize recent theory of conditional inference for NMM, and then propose a bias-corrected RE predictor and confidence interval (CI). We also extend this methodology to accommodate the case where some REs are not associated with data. Simulation studies indicate our new approach leads to substantial improvement in the conditional coverage rate of RE CIs, including CIs for smooth functions in generalized additive models, compared to the existing method based on marginal inference.},
  archive      = {J_AISM},
  author       = {Zheng, Nan and Cadigan, Noel},
  doi          = {10.1007/s10463-024-00909-6},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {105-126},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Improved confidence intervals for nonlinear mixed-effects and nonparametric regression models},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hidden AR process and adaptive kalman filter. <em>AISM</em>,
<em>77</em>(1), 61–103. (<a
href="https://doi.org/10.1007/s10463-024-00908-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work discusses a model of a partially observed linear system that depends on some unknown parameters. An approximation of the unobserved component is proposed, which involves three steps. First, a method of moment estimator of unknown parameters is constructed, and second, this estimator is used to define the one-step MLE-process. Finally, the last estimator is substituted into the equations of the Kalman filter. The solution of obtained equations provides us with the desired approximation (adaptive Kalman filter). The asymptotic properties of all the mentioned estimators and both maximum likelihood and Bayesian estimators of the unknown parameters are detailed. The asymptotic efficiency of adaptive filtering is discussed.},
  archive      = {J_AISM},
  author       = {Kutoyants, Yury A.},
  doi          = {10.1007/s10463-024-00908-7},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {61-103},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Hidden AR process and adaptive kalman filter},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of value-at-risk by <span
class="math display"><em>L</em><sup><em>p</em></sup></span> quantile
regression. <em>AISM</em>, <em>77</em>(1), 25–59. (<a
href="https://doi.org/10.1007/s10463-024-00911-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring more accurate estimates of financial value at risk (VaR) has always been an important issue in applied statistics. To this end either quantile or expectile regression methods are widely employed at present, but an accumulating body of research indicates that $$L^{p}$$ quantile regression outweighs both quantile and expectile regression in many aspects. In view of this, the paper extends $$L^{p}$$ quantile regression to a general classical nonlinear conditional autoregressive model and proposes a new model called the conditional $$L^{p}$$ quantile nonlinear autoregressive regression model (CAR- $$L^{p}$$ -quantile model for short). Limit theorems for regression estimators are proved in mild conditions, and algorithms are provided for obtaining parameter estimates and the optimal value of p. Simulation study of estimation’s quality is given. Then, a CLVaR method calculating VaR based on the CAR- $$L^{p}$$ -quantile model is elaborated. Finally, a real data analysis is conducted to illustrate virtues of our proposed methods.},
  archive      = {J_AISM},
  author       = {Sun, Peng and Lin, Fuming and Xu, Haiyang and Yu, Kaizhi},
  doi          = {10.1007/s10463-024-00911-y},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {25-59},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Estimation of value-at-risk by $$L^{p}$$ quantile regression},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplified quasi-likelihood analysis for a locally
asymptotically quadratic random field. <em>AISM</em>, <em>77</em>(1),
1–24. (<a href="https://doi.org/10.1007/s10463-024-00907-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IHK program is a general framework in asymptotic decision theory, introduced by Ibragimov and Hasminskii and extended to semimartingales by Kutoyants. The quasi-likelihood analysis (QLA) asserts that a polynomial type large deviation inequality is always valid if the quasi-likelihood random field is asymptotically quadratic and if a key index reflecting the identifiability is non-degenerate. As a result, following the IHK program, the QLA gives a way to inference for various nonlinear stochastic processes. This paper provides a reformed and simplified version of the QLA and improves accessibility to the theory. As an example of the advantages of the scheme, the user can obtain asymptotic properties of the quasi-Bayesian estimator by only verifying non-degeneracy of the key index.},
  archive      = {J_AISM},
  author       = {Yoshida, Nakahiro},
  doi          = {10.1007/s10463-024-00907-8},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Simplified quasi-likelihood analysis for a locally asymptotically quadratic random field},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="apin---250">APIN - 250</h2>
<ul>
<li><details>
<summary>
(2025). MOEA/d with adaptive weight vector adjustment and parameter
selection based on q-learning. <em>APIN</em>, <em>55</em>(6), 1–43. (<a
href="https://doi.org/10.1007/s10489-024-05906-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithms (MOEAs) are widely utilized for addressing multi-objective optimization problems (MOPs), demonstrating effectiveness in handling low-dimensional and regular Pareto fronts (PFs) MOPs. However, when the number of objectives increases (&gt;3) and the PFs become increasingly intricate, maintaining both the convergence and diversity of solutions presents a significant challenge. To address this, an adaptive weight vector adjustment and parameter selection based on Q-learning (QLMOEA/D-AWA) is proposed. In the algorithm, Q-learning is employed to select both the Tchebycheff value and the number of weight vectors, aiming to balance convergence and diversity. To enhance the convergence, an improved Tchebycheff approach is proposed. To better solve problems in high-dimensional objective spaces, the niche technique is adopted to retain elite individuals. In addition, to address MOPs with irregular PFs, a two-stage weight vector deletion strategy is proposed to remove invalid weight vectors, and a certain number of weight vectors are added based on sparsity rules. An experiment study of objective numbers ranging from 2 to 10 is conducted on DTLZ, WFG, MaF and multi-objective traveling salesman problem (MOTSP). Among 115 benchmark problems, QLMOEA/D-AWA achieves 54 and 49 best results in IGD and HV, respectively.},
  archive      = {J_APIN},
  author       = {Xue, Fei and Chen, Yuezheng and Dong, Tingting and Wang, Peiwen and Fan, Wenyu},
  doi          = {10.1007/s10489-024-05906-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-43},
  shortjournal = {Appl. Intell.},
  title        = {MOEA/D with adaptive weight vector adjustment and parameter selection based on Q-learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel drift detection method using parallel detection and
anti-noise techniques. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-024-05988-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet industry, a large amount of streaming data with significant application value will be generated on the Internet. The distribution of stream data is evolving over time compared to traditional data, posing a significant challenge in the learning process from streaming data. In order to adapt the change of data distribution, concept drift detection methods are proposed to pinpoint when the concept drift occurs. Most existing drift detection methods, however, overlook the improvement of the current classifier and the influence of noise data on drift detection. This oversight leads to a decrease in the effectiveness of drift detection. In this paper, we propose a novel adaptation drift detection method to overcome the shortcomings of previous algorithms, such as error detection and lack of anti-noise capability. Meanwhile, stream computing and parallel computing are used to enhance the efficiency of our algorithm. The results of a simulation experiment on 9 synthetic stream data and 6 real-world stream data, all exhibiting concept drift, demonstrate that our method is more effective in handling concept drift compared to other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhang, Qian and Liu, Guanjun},
  doi          = {10.1007/s10489-024-05988-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A novel drift detection method using parallel detection and anti-noise techniques},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptation for improving automatic airborne pollen
classification with expert-verified measurements. <em>APIN</em>,
<em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06021-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel approach to enhance the accuracy of automatic classification systems for airborne pollen particles by integrating domain adaptation techniques. Our method incorporates expert-verified measurements into the convolutional neural network (CNN) training process to address the discrepancy between laboratory test data and real-world environmental measurements. We systematically fine-tuned CNN models, initially developed on standard reference datasets, with these expert-verified measurements. A comprehensive exploration of hyperparameters was conducted to optimize the CNN models, ensuring their robustness and adaptability across various environmental conditions and pollen types. Empirical results indicate a significant improvement, evidenced by a 22.52% increase in correlation and a 38.05% reduction in standard deviation across 29 cases of different pollen classes over multiple study years. This research highlights the potential of domain adaptation techniques in environmental monitoring, particularly in contexts where the integrity and representativeness of reference datasets are difficult to verify.},
  archive      = {J_APIN},
  author       = {Matavulj, Predrag and Jelic, Slobodan and Severdija, Domagoj and Brdar, Sanja and Radovanovic, Milos and Tesendic, Danijela and Sikoparija, Branko},
  doi          = {10.1007/s10489-024-06021-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Domain adaptation for improving automatic airborne pollen classification with expert-verified measurements},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selections based on uncertainty measurements from
dual-quantitative improvement and double-hierarchical fusion.
<em>APIN</em>, <em>55</em>(6), 1–35. (<a
href="https://doi.org/10.1007/s10489-024-06075-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selections promote classification learning, and rough set theory offers effective mathematical methods; in practice, the performance enhancement of feature selection algorithms formulates a research target and challenge, and the corresponding problem solving usually resorts to improvement constructions of uncertainty measures. By fitting fuzzy rough sets (FFRSs), the relative dependency complement mutual information (FDCIE) motivates a recent algorithm of feature selection, called FNRDCI; however, FDCIE has improvement space of quantification view and fusion hierarchy, so the corresponding feature selection and heuristic algorithm can be advanced. In this paper, the dependency is improved by information localization, while the mutual information is enriched by information fuzzification and decision-class combination, so improved fusion measures and robuster feature selections are established by double-hierarchical fusion on decision classification and class. At first, the correctional dependency is proposed by fuzzy decision localization, and it induces a classification fusion measure (i.e. FCDCIE); based on two types of fuzzy decisions, two types of mutual information (i.e. FRCEmI and FRCFmI) are established by information fuzzification and class combination. Then, two types of dependency and two types of mutual information combinedly generate $$2\times 2=4$$ classification fusion measures (i.e. IFRDCEmI, IFRDCFmI, IFRCDCEmI, IFRCDCFmI) by pursuing class-level priority fusion; these new measures acquire semantics uncertainty, system equations, and granulation monotonicity/nonmonotonicity. Furthermore, $$1+2\times 2=5$$ fusion measures yield 5 novel feature selections with heuristic algorithms. Finally, experimental comparisons demonstrate the effectiveness and efficiency of the proposed novel methods of uncertainty measures and selection algorithms.},
  archive      = {J_APIN},
  author       = {Wang, Qian and Zhang, Xianyong and Lv, Zhiying and Mo, Zhiwen},
  doi          = {10.1007/s10489-024-06075-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Appl. Intell.},
  title        = {Feature selections based on uncertainty measurements from dual-quantitative improvement and double-hierarchical fusion},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Daily power generation forecasting for a grid-connected
solar power plant using transfer learning technique. <em>APIN</em>,
<em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-024-06090-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is efficiently used for photovoltaic power generation forecasting to handle the intermittent nature of solar energy. However, big data are required for training deep networks which are not available for newly installed plants. Therefore, in this study, a novel strategy is proposed to train a deep learning model using a transfer learning technique to cop up with the unavailability of enough training datasets. A new 400 kWp solar power plant installed in the Himalayan region is considered as a case study to evaluate the proposed model. The proposed approach utilizes solar radiation data to train a deep neural network and then fine-tune the model using the power generation data from the plant. The network architecture is optimized using grey wolf optimizer to find the best suitable model for the data. The evaluation results show that the same model can achieve higher performance in generation forecasting with percentage error improved by 2% and R-value increased by 7.7% after applying transfer learning. Moreover, SHapley Additive exPlanation and Partial Dependence Plots are used to interpret the model behavior and showed that the model is mostly dependent on the previous generation values (up to 4 days) followed by the temperature and solar radiation.},
  archive      = {J_APIN},
  author       = {Tajjour, Salwan and Chandel, Shyam Singh and Malik, Hasmat and Márquez, Fausto Pedro García and Alotaibi, Majed A.},
  doi          = {10.1007/s10489-024-06090-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Daily power generation forecasting for a grid-connected solar power plant using transfer learning technique},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Dirichlet stochastic weights averaging for
graph neural networks. <em>APIN</em>, <em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-024-06099-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Park, Minhoi and Chang, Rakwoo and Song, Kyungwoo},
  doi          = {10.1007/s10489-024-06099-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Dirichlet stochastic weights averaging for graph neural networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the imitation game: A trust-based model for
distinguishing human and machine participants. <em>APIN</em>,
<em>55</em>(6), 1–39. (<a
href="https://doi.org/10.1007/s10489-024-06133-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 1950, the imitation game has captured the interest of researchers investigating human‒machine differences. Designed to evaluate machine cognition through a game-based framework, its complexity demands refinement. The imitation game utilizes this game-based setup, but its inherent intricacy calls for further enhancements. The fundamental question of whether machines are capable of genuine thought has been a key issue in artificial intelligence (AI) studies. Recent developments challenge the ease of differentiation, as AI enables machines to display human-like characteristics. This paper seeks to address the shortcomings of the original Turing test and criticisms of the imitation game by introducing an integrated model. Although machines currently operate based on our instructions, they can learn from errors and produce novel responses through generative AI techniques, even though they do not experience emotions. In this study, a new trust-based model was introduced to improve the imitation game. This model integrated various factors to assess the reliability of participants’ responses, including grammatical accuracy, response time, thinking duration, response speed, creativity, and the use of human-like expressions. The goal was to calculate a trust factor that determines the likelihood of a participant being a human or machine. To evaluate the model’s performance, a comprehensive dataset was developed using a chat generative pretrained transformer (ChatGPT-3.5). Two other large language models (LLMs), the large language model meta AI (Llama 3) and the Claude LLM, were also taken into account. To simulate the experiment with human participants, human-generated text was also included. The simulation results revealed that GPT-3.5 Turbo, Llama 3, and Claude LLM performed differently in terms of grammatical accuracy, human-like phrasing, creativity, and trust factors. GPT-3.5 and Llama 3 had lower error rates but struggled with human-like phrases. Claude resulted in more grammatical errors but better creativity. The human participants consistently showed greater trust and human-like phrase usage. Probability assessments categorized machines with 71–78% accuracy, whereas humans were identified with only a 29–36% chance of being a machine.},
  archive      = {J_APIN},
  author       = {Gupta, Tanisha and Tripathi, Akarsh and Dubey, Ashutosh Kumar and Chahar, Ravita},
  doi          = {10.1007/s10489-024-06133-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-39},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing the imitation game: A trust-based model for distinguishing human and machine participants},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Freeway optimal control based on emission oriented
microscopic graph convolutional neural network. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-06143-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction and control in the active traffic control system is considered as one of the most critical issues in Intelligent Transportation Systems (ITS). Among the proposed AI-based approaches, Deep Learning (DL) has been largely applied while showing better performances. This research improves macroscopic traffic flow model METANET by establishing a graph convolution neural network (GCN) to explicitly and more precisely incorporate microscopic traffic flow dynamics. The microscopic emission model utilizes the feature extraction function of GCN to reduce the complexity of measuring the environmental profits for the whole traffic network. By introducing the GCN model to facilitate the aggregation of vehicle information, the proposed framework reduces the computational burden and obtains better optimization performance. The designed algorithms are tested on a microscopic simulation platform based on field data. The results demonstrate that the proposed control method produce a more robust and smooth traffic flow environment, which leads to improved traffic efficiency and overall carbon emissions of the road network.},
  archive      = {J_APIN},
  author       = {Fang, Jie and Lu, Mingwen and Fu, Lina and Wang, Juanmeizi and Xu, Mengyun},
  doi          = {10.1007/s10489-024-06143-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Freeway optimal control based on emission oriented microscopic graph convolutional neural network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving vehicle detection accuracy in complex traffic
scenes through context attention and multi-scale feature fusion module.
<em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-024-06146-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection is a fundamental task for automated driving systems. However, achieving robust performance in complex traffic scenarios remains a formidable challenge. In this paper, we propose a novel vehicle detection model that leverages contextual attention mechanisms and multi-scale feature fusion to effectively tackle the inherent challenges presented by complex scenarios, such as occlusion, truncation, and small-scale vehicle instances. The proposed model introduces a contextual attention module tailored to address vehicle occlusion, augmenting the model’s reasoning ability and overall performance through the integration of global contextual information. Additionally, we introduce a Multi-Scale Feature Fusion Module to mitigate the impact of drastic changes in vehicle scales observed in dynamic traffic scenarios. Through the deployment of a dedicated multi-scale feature fusion module, our model adeptly adapts to significant size variations of vehicles in traffic scene images, thereby enhancing its capability to handle vehicles of varying sizes. Our contributions are validated through comprehensive qualitative and quantitative experiments conducted on both the KITTI dataset and the Cityscapes dataset. The experimental results demonstrate the exceptional robustness and accuracy of our proposed model. These findings provide conclusive evidence of the superior performance and effectiveness of our model in real-world applications.},
  archive      = {J_APIN},
  author       = {Liu, Wenbo and Zhao, Binglin and Zhu, Yuxin and Deng, Tao and Yan, Fei},
  doi          = {10.1007/s10489-024-06146-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Improving vehicle detection accuracy in complex traffic scenes through context attention and multi-scale feature fusion module},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement knowledge graph reasoning based on dual agents
and attention mechanism. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-06162-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning can model knowledge graph multi-hop reasoning as Markov Decision Processes and improve the accuracy and interpretability of predicting paths between entities. Existing reasoning methods usually ignore the logic of action selection when facing one-to-many or many-to-many relationships, resulting in poor performance in knowledge graph reasoning. Furthermore, the general multi-hop reasoning only achieves effective short-path reasoning and lacks efficiency in long-distance reasoning. To address the above challenges, we propose a reinforcement learning reasoning model based on dual agents and attention mechanism, where two agents are trained at the macro and micro levels, and the macro agent guides the reasoning of the micro agent. The model employs an attention mechanism to enhance the representation of the current state of the agent, to help the policy network in making more appropriate action selections when facing one-to-many or many-to-many relationships, so as to improve the selection efficiency. Simultaneously, we propose a reward function with a penalty mechanism that penalizes the agent for prematurely reaching the correct answer without staying in place, and enhances the reward of the micro agent with the reward of the macro agent. The two agents cooperate with each other to find reasoning paths on the knowledge graph. Finally, we compare the proposed model with six well-known inference method baselines on three benchmark datasets, and the experimental results show that our proposed method achieves very competitive results.},
  archive      = {J_APIN},
  author       = {Yang, Xu-Hua and Wang, Tao and Gan, Ji-Song and Gao, Liang-Yu and Ma, Gang-Feng and Zhou, Yan-Bo},
  doi          = {10.1007/s10489-024-06162-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Reinforcement knowledge graph reasoning based on dual agents and attention mechanism},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Explainable cognitive decline detection in
free dialogues with a machine learning approach based on pre-trained
large language models. <em>APIN</em>, <em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-024-06169-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {de Arriba-Pérez, Francisco and García-Méndez, Silvia and Otero-Mosquera, Javier and González-Castaño, Francisco J.},
  doi          = {10.1007/s10489-024-06169-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Explainable cognitive decline detection in free dialogues with a machine learning approach based on pre-trained large language models},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of pre-trained CNN models and data fusion
techniques in Unity3D for connected vehicles. <em>APIN</em>,
<em>55</em>(6), 1–29. (<a
href="https://doi.org/10.1007/s10489-024-06213-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Transportation Systems (ITS) aim to enhance road safety and Internet of Things (IoT)-related solutions are crucial in achieving this objective. By leveraging Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) technologies, drivers can access valuable information about their surroundings. This research utilized the Unity 3D game engine to simulate various traffic scenarios, exploring a stochastic environment with two data sources: camera and road sign labels. We developed a full-duplex communication system to enable the communication between Python and Unity. This allows the vehicle to capture images in Unity and classify them using Convolutional Neural Network (CNN) models coded in Python. To improve road sign detection accuracy, we applied multi-sensor Data Fusion (DF) techniques to fuse the information received from the sources. We applied DF methods such as the Kalman filter, Dempster-Shafer theory, and Fuzzy Integral Operators to combine the two sources of information. Furthermore, our proposed CNN model incorporates an Ordered Weighted Averaging (OWA) layer to fuse information from three pre-trained CNN models. Our results show that the proposed model integrating the OWA layer achieved an accuracy of 98.81%, outperforming six state-of-the-art models. We compared the Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). In our work, EKF exhibited a lower execution time (0.02 seconds), yielding less accurate results. UKF, however, provided a more accurate estimate while being more computationally complex. Furthermore, the Dempster-Shafer model showed approximately 30% better accuracy compared to the Fuzzy Integral Operator. Using this methodology on autonomous vehicles in our virtual environment led to making more accurate decisions, even in a variety of weather conditions and accident scenarios. The findings of this research contribute to the development of more efficient and safer vehicles.},
  archive      = {J_APIN},
  author       = {Norouzi, Mojtaba and Hosseini, Seyed Hossein and Khoshnevisan, Mohammad and Moshiri, Behzad},
  doi          = {10.1007/s10489-024-06213-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Appl. Intell.},
  title        = {Applications of pre-trained CNN models and data fusion techniques in Unity3D for connected vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational analysis of virus-host protein-protein
interactions using gene ontology and natural language processing.
<em>APIN</em>, <em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-024-06223-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of in-silico computational methods in identifying protein-protein interactions (PPIs) between target and host proteins is crucial for developing effective infection treatments. These methods are essential for quickly determining high-quality and accurate PPIs, predicting protein pairs with the highest likelihood of physical interaction from a large pool, and reducing the need for experimental confirmation or prioritizing pairs for experiments. This study proposes using gene ontology and natural language processing (NLP) approaches to extract and quantify features from protein sequences. In the first step, proteins were represented using gene ontology terms, and a set of features was generated. In the second step, NLP techniques treated gene ontology terms as a word dictionary, creating numerical vectors using the bag of words (BoW), count vector, term frequency-inverse document frequency (TF-IDF), and information content methods. In the third step, different machine learning methods, including Decision Tree, Random Forest, Bagging-RepTree, Bagging-RF, BayesNet, Deep Neural Network (DNN), Logistic Regression, Support Vector Machine (SVM), and VotedPerceptron, were employed to predict protein interactions in the datasets. In the fourth step, the Max-Min Parents and Children (MMPC) feature selection algorithm was applied to improve predictions using fewer features. The performance of the developed method was tested on the SARS-CoV-2 protein interaction dataset. The MMPC algorithm reduced the feature count by over 99%, enhancing protein interaction prediction. After feature selection, the DNN method achieved the highest predictive performance, with an AUC of 0.878 and an F-Measure of 0.793. Sequence-based protein encoding methods AAC, APAAC, CKSAAPP, CTriad, DC, and PAAC were applied to proteins in the SARS-CoV-2 interaction dataset and their performance was compared with GO-NLP. The performance of the relevant methods was measured separately and combined. The highest performance was obtained from the combined dataset with an AUC value of 0.888. This study demonstrates that the proposed gene ontology and NLP approach can successfully predict protein-protein interactions for antiviral drug design with significantly fewer features using the MMPC-DNN model.},
  archive      = {J_APIN},
  author       = {Cihan, Pınar and Ozger, Zeynep Banu and Cakabay, Zeynep},
  doi          = {10.1007/s10489-024-06223-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Computational analysis of virus-host protein-protein interactions using gene ontology and natural language processing},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating cruise user satisfaction through online reviews:
A method based on sentiment analysis and large-scale group
decision-making. <em>APIN</em>, <em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06241-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reviews of cruise tourism, as user-generated information, contain customers’ evaluations of various aspects of the cruise tourism industry. They influence the development of this industry by affecting potential users’ purchasing decisions. To promote the sustainable development of the cruise tourism industry, it is crucial to understand the factors influencing high user satisfaction and to ensure high levels of user satisfaction. With a focus on the factors influencing the cruise travel experience, this paper proposes a method for determining user requirements (URs) and evaluating user satisfaction with cruise tourism by integrating online review analysis with large-scale group decision-making (LSGDM). First, we establish a sentiment dictionary for the cruise domain based on online reviews, selecting seed sentiment words according to word frequency and expanding them using the Word2vec and semantic orientation using pointwise mutual information algorithms. Second, we use the latent Dirichlet allocation topic model to analyze online reviews and identify the 10 URs that are of actual concern to cruise customers. Then we perform dependency syntax analysis to conduct a fine-grained sentiment analysis of each review to identify the sentiment intensity values toward different cruise URs. Third, we evaluate the final satisfaction and ranking of URs using the LSGDM method, which includes a consensus model with a personalized feedback mechanism based on the minimum adjustment cost. We conclude by providing suggestions for improving the cruise tourism experience.},
  archive      = {J_APIN},
  author       = {Shi, Jing and Chen, Jing and Wu, Jian and Liu, Yujia},
  doi          = {10.1007/s10489-025-06241-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {Evaluating cruise user satisfaction through online reviews: A method based on sentiment analysis and large-scale group decision-making},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive control approach incorporating incremental
learning. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06243-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Incremental Learning MPC (ILMPC), a novel Model Predictive Control (MPC) approach designed to enhance the adaptability of control systems in dynamic environments with unpredictable disturbances. Traditional MPC methods are often limited by their reliance on static models and fixed optimization schemes, making them less effective in handling disturbances and model inaccuracies. To overcome these limitations, ILMPC integrates incremental learning, enabling continuous refinement of the control model using real-time data. This innovation improves prediction accuracy and control performance, allowing the system to adapt to changing operational conditions and unknown disturbances. Key advances include the development of a sequence prediction model that continuously updates the state-space model through incremental learning, improved disturbance suppression for more stable control, and a reduction in computational complexity by incrementally model parameters. Experimental results show that ILMPC enhances deviation suppression significantly compared to conventional methods and significantly reduces control input volatility, demonstrating its superior performance in real-time disturbance suppression and adaptability.},
  archive      = {J_APIN},
  author       = {Chen, Jian and Pan, Haiwei and Zhang, Kejia and Lan, Haiyan and Xu, Xu and Luo, Wenhui},
  doi          = {10.1007/s10489-025-06243-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Predictive control approach incorporating incremental learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary-sensitive adaptive decoupled knowledge distillation
for acne grading. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06260-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acne grading is a critical step in the treatment and customization of personalized therapeutic plans. Although the knowledge distillation architecture exhibits outstanding performance on acne grading task, the impact of non-label classes is not considered separately, resulting in low distillation efficiency for non-label classes. Such insufficiency will cause the misclassification of the acne images located on the edge of the decision boundary. To address this issue, a novel method named Adaptive Decoupled Knowledge Distillation (ADKD) which considers the uniqueness of the acne images is proposed. In order to explore the influence of non-label classes and enhance the model’s distillation efficiency on them, ADKD splits the traditional KD loss into two parts: non-label class knowledge distillation (NCKD), and label class knowledge distillation (LCKD). Additionally, it dynamically adjusts the NCKD based on the distance between the sample and each non-label class. This allows the model to allocate different learning intensities to various non-label classes, reducing the overrecognition of classes near the sample and the underrecognition of distant classes. The proposed method enables the model to better learn the fuzzy features between acne images, and more accurately classify the samples located on the decision boundary. To verify the proposed method, extensive experiments were carried out on ACNE04 dataset, ACNEHX dataset, and DermaMnist dataset. The experimental results demonstrate the effectiveness of this method, and its performance surpasses that of current state-of-the-art (SOTA) method.},
  archive      = {J_APIN},
  author       = {Zhou, Xinyang and Liu, Wenjie and Zhang, Lei and Zhang, Xianliang},
  doi          = {10.1007/s10489-025-06260-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Boundary-sensitive adaptive decoupled knowledge distillation for acne grading},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025c). Facial StO2-based personal identification: Dataset
construction, feasibility study, and recognition framework.
<em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06267-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrics have been extensively utilized in the realm of identity recognition. However, each biometric method has its inherent limitations in specific scenarios. For example, identity recognition based on facial images is contactless but can be forged; finger vein recognition is very secure but generally requires contact collection to ensure accurate identification. In some scenarios with high security requirements, there is often a need for contactless acquisition of biometric features that cannot be forged to recognize identity. Therefore, a novel biometric, facial tissue oxygen saturation (StO2) with the advantages of robust anti-spoofing capabilities and non-contact measurement, is proposed for identity recognition. To more comprehensively verify the feasibility of facial StO2 for identity recognition, a Facial StO2 Identity Dataset (FSID148) containing 148 identities is collected and the feasibility of facial StO2 identity recognition is validated by performing verification, close-set identification, and open-set identification tasks. In order to enhance the performance of facial StO2 identity recognition, an attention-guided contrastive learning framework that enables backbones to derive discriminative identity representations from both local and global facial StO2 regions is proposed. The method proposed has achieved accuracies of 96.11%, 94.60%, and 88.51% in the aforementioned tasks, positioning facial StO2 as a promising biometric for a wide array of application scenarios.},
  archive      = {J_APIN},
  author       = {Zhang, Zheyuan and Liu, Xinyu and Jia, Yingjuan and Zhou, Ju and Wang, Hanpu and Wang, Jiaxiu and Chen, Tong},
  doi          = {10.1007/s10489-025-06267-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Facial StO2-based personal identification: Dataset construction, feasibility study, and recognition framework},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mind the naive forecast! A rigorous evaluation of
forecasting models for time series with low predictability.
<em>APIN</em>, <em>55</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s10489-025-06268-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of time series forecasting, numerous machine learning studies have assessed the performance of new methods on highly volatile data from macroeconomics and finance. Unlike in other domains, where models are also compared to simpler statistical or naive baselines, they mostly compare the performance solely relative to other complex models. This approach may lead to limited conclusions and reduce the practical significance of the results, as it overlooks the unpredictability of some highly volatile time series in the datasets used. We apply state-of-the-art methods from time-series econometrics and machine learning, including autoregressive integrated moving average (ARIMA), exponential smoothing (ETS), Bayesian vector autoregressive model (BVAR), long-short term memory neural networks (LSTM), historical consistent neural networks (HCNN), deep vector autoregressive neural networks (DeepVAR), temporal fusion transformers (TFT), and extreme gradient boosting (XGBoost). Our results demonstrate that no method consistently outperforms the naive (no-change) forecast for highly volatile time series from two popular datasets containing exchange rates and stock prices, rendering comparative analysis between complex models less meaningful. In contrast, when applied to more predictable macroeconomic price indices, many of the methods significantly outperform naive forecasts. We find that the performance of machine learning models deteriorates more than that of statistical models for high-volatility time series. This study highlights the critical importance of using appropriate benchmark models, including cost-effective, simple approaches, on datasets that permit meaningful conclusions.},
  archive      = {J_APIN},
  author       = {Beck, Nico and Dovern, Jonas and Vogl, Stefanie},
  doi          = {10.1007/s10489-025-06268-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {Mind the naive forecast! a rigorous evaluation of forecasting models for time series with low predictability},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-tailed classification by efficient contrast learning
with high quality and high relevance latent features. <em>APIN</em>,
<em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06269-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robust feature representations from long-tail distributed data is essential. Recently, contrastive learning has shown impressive progress in addressing long-tail learning challenges. While contrastive learning aims to optimize the lower bound of mutual information between feature distribution and label distribution, the previous approaches often substantially rely on less accurate and unrealistic assumptions about model distribution and overlook the long-tail nature of the instance space. Consequently, these methods fail to achieve a sufficiently tight lower bound. To address these concerns, we first propose a loss function derived from mini-Batch instance Features and Class Prototypes to construct a Conditional Gaussian mixture distribution (CGM-BF-CP), and prove its generalization ability from the perspective of generalization error upper bound. Then we create high quality and high relevance KNN graph to model relation between features. And propose a corresponding loss function, i.e., Graph based Contrast Learning Loss (GCLL). The feature information can be transferred between classes through this graph, so that the tail class features can be better learned. The experimental results on Cifar10/100-LT and ImageNet-LT show that our proposed model is competitive with the latest state-of-the-art methods. Our code is available at https://github.com/error030/CGM-BP-CP/tree/main .},
  archive      = {J_APIN},
  author       = {Yuan, Hong-li and Liu, Jian-wei},
  doi          = {10.1007/s10489-025-06269-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Long-tailed classification by efficient contrast learning with high quality and high relevance latent features},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision transformer-based generalized zero-shot learning with
data criticizing. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06271-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Zero-Shot Learning (GZSL) aims to enable accurate testing and recognition of unseen classes by utilizing training data from seen classes and leveraging attribute knowledge. However, GZSL faces a challenge wherein the model, trained solely on seen class data, tends to be biased towards recognizing visual features of seen classes, resulting in poorer recognition performance for unseen classes. To address this issue, we propose an approach called Vision Transformer-Based Generalized Zero-Shot Learning with Data Criticizing (ViT-DaCr). In order to obtain improved visual features, we thoroughly examine features extracted by Vision Transformer (ViT) with a new design. Additionally, we recognize that not all training data align with our model during the training process, leading the model to exhibit a bias towards recognizing visual features of seen classes and directly impacting visual feature recognition. Therefore, we propose a data critic mechanism that utilizes Adjusted Boxplot to filter out such data automatically during the training process. Extensive experiments demonstrate the advanced performance of our model on three challenging and popular datasets.},
  archive      = {J_APIN},
  author       = {Zhou, Quan and Liang, Yucuan and Zhang, Zhenqi and Cao, Wenming},
  doi          = {10.1007/s10489-025-06271-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Vision transformer-based generalized zero-shot learning with data criticizing},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Dynamic interactive weighted feature
selection using fuzzy interaction information. <em>APIN</em>,
<em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-025-06273-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Ma, Xi-Ao and Xu, Hao and Liu, Yi},
  doi          = {10.1007/s10489-025-06273-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Dynamic interactive weighted feature selection using fuzzy interaction information},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PTLO: A model-agnostic training strategy based on
progressive training and label optimization for fine-grained image
classification. <em>APIN</em>, <em>55</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s10489-025-06276-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to conventional image recognition, fine-grained classification exhibits increased vulnerability to labeling noise due to the presence of closely related categories, resulting in degraded performance on complex and non-representative samples. While existing approaches mitigate these issues through data cleaning, loss modification, and semi-supervised learning techniques, they often overlook the intrinsic attributes within training samples. Instead of designing any network architectures, this study introduces a model-agnostic progressive training strategy comprising of progressive training and label optimization, where the former is to decrease the affect from the noisy samples by facilitating a graduated learning approach in an easy-to-hard manner, while the latter is to denoise the label noises. Theoretical analysis also demonstrates that the proposed method uncovers valuable cues hidden in the training data, thereby enhancing the robustness of any learning-based models. Experimental evaluations on fine-grained classification benchmarks (e.g., CUB-200-2011, DTD, and Food-101) across various mainstream classification networks demonstrate the effectiveness of our training strategy. Code is available at https://github.com/cb-rep/LPPT .},
  archive      = {J_APIN},
  author       = {Chen, Yiming and Tao, Xiuting and Chen, Bo and Guo, Jian and Li, Shi},
  doi          = {10.1007/s10489-025-06276-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Appl. Intell.},
  title        = {PTLO: A model-agnostic training strategy based on progressive training and label optimization for fine-grained image classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-contextual stress prediction: Simple methodology for
comparing features and sample domain adaptation techniques in vital sign
analysis. <em>APIN</em>, <em>55</em>(6), 1–26. (<a
href="https://doi.org/10.1007/s10489-025-06277-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress significantly impacts individuals, particularly in professions like nursing and driving, leading to severe health risks and accidents. Accurate stress measurement is critical for effective interventions, yet research is hindered by incomplete datasets and inconsistent methodologies, slowing the development of reliable predictive models. This paper introduces a framework for cross-contextual stress prediction, enabling the generation of general stress prediction models adaptable to specific domain challenges. The methodology leverages two general daily life datasets and three domain-specific datasets, employing steps such as dataset selection, feature extraction, significant feature identification, feature preprocessing, fine-tuning, domain adaptation, and application to specific contexts. Through this framework, key vital signs were identified as significant predictors of stress, including electrocardiography (ECG), heart rate (HR), heart rate variability (HRV) - low frequency (LF), electrodermal activity (EDA), body temperature (TEMP), and skin conductance response (SCR). The experiments conducted include: 1) Utilizing HR and HRV-LF through domain adaptation from general to automobile driving datasets; 2) Applying EDA, HR, and TEMP from general to specific nurse activity datasets; and 3) Adapting ECG, HR, and TEMP from general to automobile driving datasets. Results demonstrate the potential of the proposed framework for cross-contextual stress prediction, with HR and HRV-LF identified as pivotal features. When applied to target datasets specific to stress scenarios, the model achieved a 62% F1 score, demonstrating the effectiveness of the feature-based Correlation Alignment (CORAL) technique combined with Random Forest models in transferring learned knowledge across domains. These findings highlight the robustness of the approach in adapting general stress prediction models to specific contexts, paving the way for real-world applications such as stress monitoring in driving and nursing during high-stress periods like COVID-19.},
  archive      = {J_APIN},
  author       = {Mihirette, Samson and De la Cal, Enrique A. and Tan, Qing and Sedano, Javier},
  doi          = {10.1007/s10489-025-06277-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {Appl. Intell.},
  title        = {Cross-contextual stress prediction: Simple methodology for comparing features and sample domain adaptation techniques in vital sign analysis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised feature learning using locality-preserved
auto-encoder with complexity-invariant distance for intelligent fault
diagnosis of machinery. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06278-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature learning (UFL) has been recognized as a promising feature extractor in machinery fault diagnosis, where the auto-encoder is a very popular UFL framework. For the auto-encoder methods, however, it is still a great challenge to learn discriminative features from complex signals in an unsupervised manner. In this paper, a new UFL method named locality-preserved auto-encoder (LPAE) is proposed by explicitly designing a locality-preserved penalty term. Concretely, the penalty term constrains local geometry of samples in the original space to be well preserved in the reconstruction space, enabling more discriminative features to be learned accordingly. To better formulate this term, the complexity-invariant distance (CID) is employed to measure similarity between two mechanical signals so as to construct a reliable neighbor graph. On a rolling bearing dataset, experimental results verify that the proposed LPAE can learn sufficiently discriminative features from complex vibration signals collected from varying operating conditions, and achieves a remarkable and superior diagnosis performance over the existing advanced UFL methods. Moreover, the effectiveness of CID has been adequately validated by comparing with several other distance measurement methods. The proposed LPAE can be applied to the feature extraction stage of machinery fault diagnosis, which provides a potential solution for engineers to realize unsupervised learning of discriminative features.},
  archive      = {J_APIN},
  author       = {Lu, Zhenghua and Chu, Zhaobi and Zhu, Min and Dong, Xueping},
  doi          = {10.1007/s10489-025-06278-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised feature learning using locality-preserved auto-encoder with complexity-invariant distance for intelligent fault diagnosis of machinery},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised symmetric non-negative matrix factorization
with graph quality improvement and constraints. <em>APIN</em>,
<em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06282-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetric non-negative matrix factorization (SNMF) decomposes a similarity matrix into the product of an indicator matrix and its transpose, allowing clustering results to be directly extracted from the indicator matrix without additional clustering methods. Furthermore, SNMF has been shown to be effective in clustering nonlinearly separable data. SNMF-based clustering methods significantly depend on the quality of the pairwise similarity matrix, yet their effectiveness is often hindered by the reliance on predefined matrices in most semi-supervised SNMF approaches. Thus, we propose a novel algorithm, named semi-supervised symmetric non-negative matrix factorization with graph quality improvement and constraints ( $$\text {S}^{3}\text {NMFGC}$$ ), addressing this limitation by employing an integrated clustering strategy that dynamically generates and adaptively updates the similarity matrices. This is accomplished by integrating a weighted graph construction based on multiple clustering results, a label propagation algorithm, and pairwise constraint terms into a unified optimization framework that enhances the semi-supervised SNMF model. Subsequently, we adopt an alternating iterative update method to solve the optimization problem and prove its convergence. Rigorous experiments highlight the superiority of our model, which outperforms seven state-of-the-art NMF methods across six datasets.},
  archive      = {J_APIN},
  author       = {Ren, Xiaowan and Yang, Youlong},
  doi          = {10.1007/s10489-025-06282-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised symmetric non-negative matrix factorization with graph quality improvement and constraints},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph denoising neural network for session-based
recommendation. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06283-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SBR) predicts the next interaction of users based on their clicked items in a session. Previous studies have shown that hypergraphs are superior in capturing complex item transitions which contribute to SBR performance. However, existing hypergraph-based methods fail to model item co-occurrence and sequential patterns simultaneously, limiting the improvement of recommendation performance. Moreover, they are more sensitive to noisy items than conventional graph models due to the item association mechanism. In this paper, we propose a novel hypergraph-based method named Hypergraph Denoising Neural Network (HDNN) for SBR to tackle the abovementioned problems. The proposed method involves two newly-designed modules: a sequential pattern learning module (SPLM) and an adaptive attention selection module (AASM). In particular, SPLM models item sequential patterns to complement the hypergraph-based models which only focus on co-occurrence patterns. Meanwhile, AASM employs learnable attention score thresholds to exclude items with low attention scores, mitigating the impact of noisy items in hypergraphs. Furthermore, the sequential denoising unit (SDU) designed in SPLM is employed to eliminate noise in item sequential patterns, thus realizing the dual denoising purpose. Extensive experiments are conducted on three real-world datasets. The results of the experiments show that our HDNN framework shows better performance than the state-of-the-art models. In particular, all evaluation metrics in Tmall and RetailRocket showed improvements of over 15% and 5%, respectively.},
  archive      = {J_APIN},
  author       = {Ding, Jiawei and Tan, Zhiyi and Lu, Guanming and Wei, Jinsheng},
  doi          = {10.1007/s10489-025-06283-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Hypergraph denoising neural network for session-based recommendation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking probability volume for multi-view stereo: A
probability analysis method. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06284-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing learning-based multi-view stereo (MVS) models primarily focus on predicting depth maps through a cascaded structure to achieve more robust reconstruction results. However, they often emphasize improving the quality of stereo matching while overlooking the importance of depth hypotheses. In this paper, we propose a novel MVS model from the perspective of probability volume analysis. First, the guiding effect of the probability volume is considered for depth refinement. Ideally, the probability distribution along the depth dimension of the probability volume follows an unimodal pattern. We design an unimodal curve to fit this pattern. Then, a reasonable depth refinement range is adaptively selected for each pixel position based on a predefined probability threshold. Additionally, considering that matching noise may cause the probability volume to appear as a blurred unimodal peak, we design the probability volume split-merge module (PVS-PVM). This module performs a peak search based on conditional constraints, splitting the probability volume into main and sub probability volumes, then computes the two sets of depth hypotheses from them. Finally, the new main and sub probability volumes are computed based on these depth hypotheses and merged to predict the depth. This approach allows for a more comprehensive consideration of the regions with higher probability, improving the robustness of depth hypotheses. Experimental results demonstrate that our method effectively utilizes probability volume information to guide depth map refinement and yields enhanced reconstruction results on the DTU and Tanks &amp; Temples datasets. Our code will be released at https://github.com/zongh5a/ProbMVSNet .},
  archive      = {J_APIN},
  author       = {Yu, Zonghua and Wang, Huaijun and Li, Junhuai and Jin, Haiyan and Cao, Ting and Cheng, Kuanhong},
  doi          = {10.1007/s10489-025-06284-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Rethinking probability volume for multi-view stereo: A probability analysis method},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). You are what your feeds make you: A study of user aggressive
behavior on twitter. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06286-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of aggressive language on Twitter raises concerns about potential negative influences on user behavior. Despite previous research exploring aggression and negativity on the platform, the relationship between consuming aggressive content and users’ aggressive behavior remains underexplored. This study investigates whether exposure to aggressive content on Twitter can lead users to behave more aggressively. Our methodological approach contains four stages: data collection and annotation, aggressive post detection, user aggression intensity metric, and user profiling. We proposed the English Twitter Aggression dataset (TAG-EN) with substantial inter-annotator agreement (Krippendorff’s alpha=0.78). Subsequently, we benchmark the aggression detection performance on TAG-EN dataset (macro F1=0.92) by fine-tuning a pre-trained RoBERTa-large. We quantified user aggression with a proposed “user aggression intensity” metric based on their overall aggressive activity. Our analysis of 14M posts from 63K users revealed that aggressive Twitter feeds can influence users to behave more aggressively online. Furthermore, the study found that users tend to support and encourage aggressive content on social media, which can contribute to the proliferation of aggressive behavior.},
  archive      = {J_APIN},
  author       = {Mane, Swapnil and Kundu, Suman and Sharma, Rajesh},
  doi          = {10.1007/s10489-025-06286-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {You are what your feeds make you: A study of user aggressive behavior on twitter},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view learning based on product and process metrics for
software defect prediction. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06288-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction plays a crucial role as a quality assurance technology in software development. The software metrics are associated with the software quality and are vital for prediction models. Most existing defect prediction methods build the prediction model ignoring the complementary information between these two kinds of metrics. In this work, we intend to jointly leverage these two kinds of metrics. For a software instance, we regard the product metrics and the process metrics as its two views. We model the problem of discriminative feature learning from these two kinds of metrics as the problem of multi-view learning. However, it is a challenging task to construct an effective prediction model based on both product and process metrics due to the heterogeneity in data of product and process metrics, and the defect data often has class imbalance characteristic. How to explore the discriminant both inter-view and intra-view effectively has not been well studied. These characteristics make it challenging to construct an effective prediction model. In this paper, we propose a Deep Multi-view Defect Prediction (DMDP) approach, which can predict software defect based on both product and process metrics. We design a neural network with two sub-network branches, which are enforced to share the weights in the last output layer, to map the data from different views to a common space. To guide the training of networks, we design the loss function including the discrepancy loss, discrimination loss and classification loss, which further promotes the distribution consistency across views, makes full use of label information to obtain the discriminative representations, and utilizes the complementarity information for prediction. To alleviate the class imbalance problem, we design a dynamic sampling strategy for dealing with class-imbalanced data. Comprehensive experiments are conducted on 15 projects from three widely used defect datasets. The experimental results demonstrate that multi-view learning based on product and process metrics is helpful for software defect prediction and DMDP outperforms the state-of-the-art baselines.},
  archive      = {J_APIN},
  author       = {Sun, Ying and Wu, Fei and Wu, Di and Jing, Xiao-Yuan and Sun, Yanfei},
  doi          = {10.1007/s10489-025-06288-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view learning based on product and process metrics for software defect prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A user preference knowledge graph incorporating
spatio-temporal transfer features for next POI recommendation.
<em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06290-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs can improve the performance of recommendation systems and provide explanations for recommendation results, which have been widely applied in the next Point-of-Interest (POI) recommendation. However, the current knowledge graph method for the next POI recommendation focuses on the static attributes of POIs, and only describes the spatio-temporal characteristics when the user transfers between POIs. To fully tap into user preferences for different POIs, we have done the following innovative work. (1) We construct a user preference knowledge graph with spatio-temporal characteristics, named UPSTKG, which expresses preference information from both individual user and global user perspectives. (2) We use local preference triplets in preference knowledge graphs to construct user preference graphs. And use GCN to obtain user preference vectors to replace common user vectors in the sequence, thereby strengthening the potential connection between users and different POIs. (3) We combine UPSTKG and user preference graph to propose the UPSTKGRec method for the next POI recommendation. To evaluate the effectiveness of UPSTKGRec, it is compared to six highly regarded techniques on three distinct benchmark datasets. Compared with the baseline, the average performance of indicators recell@5 and NDCG@5 has increased by 13.8% and 13.1%.},
  archive      = {J_APIN},
  author       = {Sang, Chun-Yan and Yang, Yang and Zhang, Yi-Bo and Liao, Shi-Gen},
  doi          = {10.1007/s10489-025-06290-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {A user preference knowledge graph incorporating spatio-temporal transfer features for next POI recommendation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive sparsity detection-based evolutionary algorithm for
large-scale sparse multi-objective optimization problems. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06291-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale sparse multi-objective optimization problems (LSSMOPs) widely exist in practical applications, which have large-scale decision variables and sparse Pareto optimal solutions. Existing algorithms have some shortcomings in dealing with LSSMOPs: (1) failing to make full use of the knowledge of the sparsity of the Pareto optimal solutions, leading to insufficient sparsity detection; (2) ignoring the connection between binary and real variables, leading to insufficient variables optimization. This paper proposes an adaptive sparsity detection-based evolutionary algorithm (ASD-MOEA) to address these issues, which is a two-stage algorithm. The first stage performs an adaptive sparsity detection strategy, which dynamically adjusts the probability of binary variables flipping and the fitness of decision variables according to the iteration ratio. Then, non-zero variables are mined based on fitness. The second stage performs a variable grouping-based optimization strategy, grouping decision variables according to their sparsity in the set of non-dominated solutions to reduce the search space, then performs genetic operations in the subspace. Finally, we compare ASD-MOEA with six mainstream algorithms. The results show that the proposed algorithm significantly outperforms the existing algorithms in dealing with LSSMOPs, and achieves a balance between sparsity maintenance and variable optimization.},
  archive      = {J_APIN},
  author       = {Qiu, Feiyue and Long, Donghui and Chen, Qi and Hu, Huizhen and Qiu, Qicang},
  doi          = {10.1007/s10489-025-06291-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive sparsity detection-based evolutionary algorithm for large-scale sparse multi-objective optimization problems},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TITD: Enhancing optimized temporal position encoding with
time intervals and temporal decay in irregular time series forecasting.
<em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06293-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate Time Series (MTS) acquisition processes often exhibit irregularities, making accurate MTS forecasting challenging. Previous researches focused on interpolation approaches to address data completeness in irregular MTS, but these approaches may introduce noise, thereby altering the feature distributions of irregular MTS. Recent researches trend advocate embedding the missing temporal information through position encoding for forecasting irregular MTS. However, these position encodings were typically designed for text sequences and assumed fixed time intervals, which lead to the loss or distortion of temporal information when applied to irregular MTS. Moreover, they struggled to capture the temporal dynamic information in irregular MTS. To address these challenges, we propose a novel approach called TITD (Time Interval and Temporal Decay), which utilizes time interval and temporal decay information to enhance irregular MTS forecasting. TITD optimizes position encoding to effectively capture both local time interval features and long-term temporal decay patterns, breaking the limitations of static and fixed interval position encoding on time dynamic representation. Simultaneously, TITD integrates multi-view input information from irregular MTS to enhance the representation learning of the relationships across different views, thereby achieving superior forecasting performance without interpolation. Extensive experiments on three real-world time series datasets have demonstrated that TITD provides significant improvements over state-of-the-art methods in irregular MTS forecasting.},
  archive      = {J_APIN},
  author       = {Ji, Jinquan and Cao, Yu and Ma, Yukun and Yan, Jianzhuo},
  doi          = {10.1007/s10489-025-06293-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {TITD: Enhancing optimized temporal position encoding with time intervals and temporal decay in irregular time series forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rail-PatchCore: Unsupervised learning-based detection of
visual anomalies in the railway-turnout environment. <em>APIN</em>,
<em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06294-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and openness of railway turnout environments pose great challenges to anomaly detection, and supervised methods are highly dependent on labels, making it difficult to address the diverse types of anomalies and the scarcity of samples in turnout environments. To solve these problems, this paper proposes a new method, Rail-PatchCore, which is based on unsupervised learning and effectively reduces the interference of background noise and enhances the ability to capture anomalous features by adding a Dual-Dimensional Channel Attention (DDCA) module and a projection anomaly scoring module to the PatchCore model. The experiments on our railway-turnout anomaly detection dataset(RTAD) and other datasets (RSDDs, MVTec-AD, BTAD, AEBAD-S) show that the detection performance of Rail-PatchCore is better than that of the existing methods, and the image-level and pixel-level AUCROC indices of Rail-PatchCore on the railway turnout anomaly detection dataset reach 72.2% and 95.3%, respectively. This approach provides an efficient and reliable solution for anomaly detection in railway turnout environments.},
  archive      = {J_APIN},
  author       = {Zhang, YuanHao and Yu, Zujun and Zhu, Liqiang and Guo, Baoqing and Wang, Yao},
  doi          = {10.1007/s10489-025-06294-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Rail-PatchCore: Unsupervised learning-based detection of visual anomalies in the railway-turnout environment},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view human point cloud registration method with
overlapping regions semantic constraints and feature weighting.
<em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06296-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view human point cloud registration is a crucial step in 3D human reconstruction tasks. The symmetric structures and similar geometric features in human point clouds often lead to feature mismatches in point cloud registration. Therefore, we propose a pipeline for game tree registration based on semantic constraints and feature weighting (GTR-SCFW) that enhances the stability and accuracy of feature matching, thereby improving the registration precision of multi-view point clouds. First, we calculate and compare the feature similarity between multi-view point clouds and use a generalized best-first search (BFS) method to construct a multi-layered registration game tree. At each game node, overlapping regions are divided into multiple sub-regions based on semantic information, and global fast registration is used to determine the matching relationships of features within each sub-region. Then, the best matching points in each sub-region are selected based on the confidence of feature pairs, and the weights of all the best point pairs are calculated. Finally, the initial rigid transformation matrix is computed using weighted least squares (WLS), and ICP is employed to achieve fast fine registration. GTR-SCFW effectively avoids incorrect matching relationships caused by geometric feature similarity during the initial transformation estimation, providing a good initial pose for iterative closest point (ICP) fine registration. For point clouds with different initial poses, the registration’s rotational error approaches 0 $$^\circ $$ , while the translational error is as low as 1.203e-4 mm. Comparative experimental results show that this method outperforms existing feature-based registration methods regarding robustness, reliability, and computational efficiency.},
  archive      = {J_APIN},
  author       = {Li, Ming and Li, Guiqin and Li, Xihang and Li, Tiancai},
  doi          = {10.1007/s10489-025-06296-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view human point cloud registration method with overlapping regions semantic constraints and feature weighting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A complex history browsing text categorization method with
improved BERT embedding layer. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06298-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For long texts composed of multiple short fragments, the importance of each fragment to the classification task varies. Some fragments have higher discriminative power and positively contribute to the classification, while others lack discriminative power or even mislead it. Existing methods struggle to converge when handling texts with negative examples. This study analyzes user behavior and assigns interest scores to text fragments based on their classification relevance, allowing the model to focus more on important fragments. Building on bidirectional encoder representations from transformers (BERT), we propose an interest encoding layer model for historical browsing texts. By analyzing user behavior and incorporating an improved term frequency-inverse document frequency (TF-IDF) method, the model adds indicators to fragments with higher discriminative power for user behavior analysis, enabling the model to focus more on these during training. Finally, comparative experiments on the BERT model series validate the advantages of the proposed approach.},
  archive      = {J_APIN},
  author       = {Wang, Yuanhang and Zhou, Yonghua and Qi, Huiyu and Wang, Dingyi and Huang, Annan},
  doi          = {10.1007/s10489-025-06298-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {A complex history browsing text categorization method with improved BERT embedding layer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end audio classification framework with diverse
features for obstructive sleep apnea-hypopnea syndrome diagnosis.
<em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06299-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obstructive sleep apnea-hypopnea syndrome (OSAHS) is a prevalent chronic disorder that affects sleep quality and general health. The current diagnostic methods, primarily polysomnography (PSG), are laborious. Furthermore, audio-based methods for diagnosing OSAHS face limited sample sizes and neglect patients’ physiological signs and medical histories. To address these challenges, we introduce a data-driven framework called DFNet, which also considers patients’ medical histories and health indicators. DFNet incorporates an automated audio segmentation- and labeling-based preprocessing procedure to reduce expert annotation costs and subjective errors. We employed random convolutional kernels based on receptive fields for audio feature extraction purposes. These kernels captured both local and global features within the input audio. Additionally, for the first time, we introduced a medical language model that utilizes patients’ medical histories and physiological information as covariates to enhance features. We extensively validated DFNet on an OSAHS dataset obtained from a collaborative university hospital. Our framework classified patients into four categories according to their OSAHS severity: normal, mild, moderate, and severe. DFNet achieved state-of-the-art performance, with a four-class accuracy of 84.12%. DFNet offers a large-scale and cost-effective screening approach for diagnosing OSAHS, reducing the labor requirements of diagnosis. Our code is available at https://github.com/testlbin/DFNet .},
  archive      = {J_APIN},
  author       = {Li, Bin and Qiu, Xihe and Tan, Xiaoyu and Yang, Long and Tao, Jing and Fang, Zhijun and Huang, Jingjing},
  doi          = {10.1007/s10489-025-06299-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {An end-to-end audio classification framework with diverse features for obstructive sleep apnea-hypopnea syndrome diagnosis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple instance learning with hierarchical discrimination
and smoothing attention for histopathological diagnosis. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06300-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The microscopic structure of human tissue can be observed by pathological slides, which provides a strong basis for cancer diagnosis. However, the serious lack of experienced pathologists and the complexity of the diagnostic process have facilitated the development of computer-aided pathological image analysis. Pathological slides generally have high resolution, and multiple instance learning (MIL) has been widely used in histopathological whole slide image (WSI) analysis, where each WSI has a large number of unlabelled patches and only a WSI-level label is given. The bag-based MIL methods often learn the decision boundary at the bag level, and thus hard to learn the discriminative features at the instance level. Furthermore, the difficulty of identification varies between positive instances in a bag, and the existing attention-based aggregation methods always assign higher attention scores for the easy-to-identify positive instances, but assign lower attention scores for the difficult-to-identify positive instances and thus cannot learn these difficult instances sufficiently. In this paper, we propose a novel MIL method with hierarchical discrimination learning and a smoothing attention strategy for cancer subtype diagnosis. Particularly, to learn hierarchical discriminative features, the proposed MIL method simultaneously trains a bag classifier and multiple instance classifiers, where the multi-way attention scores of each instance for different categories are used to guide the selection of training samples for the instance classifimer. The smoothing strategy is designed to trade off the attention weights between the easily and hardly identifiable positive instances. We conducted experiments on histopathological diagnosis datasets and achieved state-of-the-art performance. Codes are available at https://github.com/bravePinocchio/HDSA-MIL.},
  archive      = {J_APIN},
  author       = {Zhao, Jing and Zhao, Zhikang and Song, Xueru and Sun, Shiliang},
  doi          = {10.1007/s10489-025-06300-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Multiple instance learning with hierarchical discrimination and smoothing attention for histopathological diagnosis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex layout generation for large-scale floor plans via
deep edge-aware GNNs. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06311-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In architectural layout generation, deep learning techniques have advanced the residential generation in multiple scenarios. However, current approaches fail to extract complex graph features from large-scale layouts, neglecting large-scale global context. Additionally, the lack of robust, quantitative evaluation metrics for layouts hampers the objective comparison of different generative approaches. To address these issues, we propose a multi-scale applicable layout generation method based on deep edge-aware GNNs, stressing edge-specific and non-local spatial information. Next, we introduce quantitative metrics to assess layout quality, including room accessibility index and space property proportion, whose purpose is to establish layout standards in the computer-aided design field. Lastly, we create the Public Space Floor Plan Dataset (P-PLAN), a collection of 4,535 annotated layout samples designed to serve as a robust evaluation platform for large-scale layout models. We conducted extensive qualitative and quantitative experiments on the Residential Floor Plan Dataset (R-PLAN) and P-PLAN dataset to demonstrate the effectiveness of the proposed method. Notably, with the proposed evaluation metrics, our method significantly outperforms existing models in accessibility and diversity.},
  archive      = {J_APIN},
  author       = {Lu, Zhengyang and Li, Yifan and Wang, Feng},
  doi          = {10.1007/s10489-025-06311-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Complex layout generation for large-scale floor plans via deep edge-aware GNNs},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-attention fusion and edge-guided fully supervised
contrastive learning network for rail surface defect detection.
<em>APIN</em>, <em>55</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06314-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been significant research focus on efficiently and accurately detecting defects on rail surfaces using computer vision. Utilizing depth information from the rail surface has emerged as an effective approach for detecting visually insignificant types of defects that are unique in nature. However, previous methods have typically overlooked the long-distance dependency between the two modalities when fusing them using conventional convolutional network methods. Additionally, these methods have often relied on traditional cross-entropy loss for edge supervision without considering the intra and inter-pixel relationships associated with edge features. To address these limitations, we propose a novel approach called CECLNet (cross-attention fusion and edge-guided fully supervised contrastive learning network) for rail surface defect detection (RSDD). The proposed CECLNet incorporates a module for inter-modal cross-attention fusion, which effectively explores the complementary information by considering the long-range relationship. Furthermore, we introduce a progressive aggregation-based multiscale feature interactions decoder to promote sufficient information interaction between multiscale features, thus facilitating the generation of final predictions. Finally, we propose a pixel-level fully supervised contrastive learning approach to enhance the efficiency of utilizing edge-assisted information. Extensive experiments conducted on the industrial NEU RGB-D RSDDS-AUG dataset demonstrate the superiority of our proposed CECLNet over 17 state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yang, Jinxin and Zhou, Wujie},
  doi          = {10.1007/s10489-025-06314-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Cross-attention fusion and edge-guided fully supervised contrastive learning network for rail surface defect detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamically modulated robot compliance via online fuzzy
neural networks for individualized ankle rehabilitation. <em>APIN</em>,
<em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06317-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Admittance control, benefiting from human-robot interaction compliance, is widely used in rehabilitation robot studies. However, using inappropriate parameters for the admittance control model can cause harm like overuse syndrome. Therefore, it is necessary to dynamically adjust these parameters to assist patients under varying recovery periods, enabling active rehabilitation training across a wider range of recovery stages. Integrating multiple intelligent approaches presents a promising solution to this challenge. This paper proposes a variable admittance control strategy that employs a variable operator fuzzy neural network (VAC-VOFNN). The VOFNN facilitates the fuzzification of the inference process, leading to superior non-linear fitting capability. Additionally, the network’s parameters are updated online to match the rehabilitation stages of different subjects. Compared to the admittance control strategy with fixed parameters (ACS-FP) and the variable admittance control strategy with fuzzy neural networks (VAC-FNN), the proposed strategy reduces the root mean square (RMS) of surface electromyography (sEMG) from the medial gastrocnemius by 29.14% and 29.04%, respectively, while also decreasing the average interaction torque by 28.63% and 12.24%, respectively. These results suggest that the proposed strategy leads to reduced effort from subjects and increased training cycles before muscle fatigue during the same rehabilitation activities. This makes it beneficial for ankle rehabilitation of patients in different recovery periods.},
  archive      = {J_APIN},
  author       = {Li, Jianfeng and Zhou, Yu and Zuo, Shiping and Dong, Mingjie},
  doi          = {10.1007/s10489-025-06317-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Dynamically modulated robot compliance via online fuzzy neural networks for individualized ankle rehabilitation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust decision-making for autonomous vehicles via deep
reinforcement learning and expert guidance. <em>APIN</em>,
<em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06319-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate decision-making within highly interactive driving environments is vital for the safety of self-driving vehicles. Despite the significant progress achieved by the existing models for autonomous vehicle decision-making tasks, there remains untapped potential for further exploration in this field. Previous models have focused primarily on specific scenarios or single tasks, with inefficient sample utilization and weak robustness problems, making them challenging to apply in practice. Motivated by this, a robust decision-making method named DRL-EPKG is proposed, which enables the simultaneous determination of vertical and horizontal behaviors of driverless vehicles without being limited to specific driving scenarios. Specifically, the DRL-EPKG integrates human driving knowledge into a framework of soft actor-critic (SAC), where we derive expert policy by a generative model: variational autoencoders (VAE), train agent policy by employing the SAC algorithm and further guide the behaviors of the agent by regulating the Wasserstein distance between the two policies. Moreover, a multidimensional reward function is designed to comprehensively consider safety, driving velocity, energy efficiency, and passenger comfort. Finally, several baseline models are employed for comparative evaluation in three highly dynamic driving scenarios. The findings demonstrate that the proposed model outperforms the baselines regarding the success rate, highlighting the practical applicability and robustness of DRL-EPKG in addressing complex, real-world problems in autonomous driving.},
  archive      = {J_APIN},
  author       = {Li, Feng-Jie and Zhang, Chun-Yang and Chen, C. L. Philip},
  doi          = {10.1007/s10489-025-06319-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Robust decision-making for autonomous vehicles via deep reinforcement learning and expert guidance},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chaotic opposition-based plant propagation algorithm for
engineering problem. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06320-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Plant Propagation Algorithm (PPA), often exemplified by the Strawberry Algorithm, has demonstrated its effectiveness in solving lower-dimensional optimization problems as a neighborhood search algorithm. While multiple enhancements have been introduced to boost its performance, PPA remains a population-based metaheuristic algorithm. A key element of PPA involves balancing exploration and exploitation, akin to a strawberry plant seeking the best survival strategy. This paper delves into the integration of chaotic numbers and opposition theory in PPA, focusing on how these additions impact its efficiency. The primary research questions revolve around enhancing PPA’s performance and reducing its search space to expedite the algorithm, ultimately leading to faster overall results. Experiments were carried out on three challenging engineering problems: the Pressure Vessel Optimization, the Spring Design Optimization, and the Welded Beam Problem, to fully assess the effectiveness of the improved PPA. The effectiveness of the original PPA, the Chaotic Opposition-Based PPA (COPPA), and several other metaheuristic algorithms were examined in each of these problems. In terms of efficiency and solution quality, the findings consistently demonstrate that COPPA performs better than the traditional PPA and other algorithms. The results indicate that using chaotic-based oppositional processes decreases the search space and enhances performance, resulting in faster and more resource-efficient optimization. The investigation reveals that incorporating chaotic-based oppositional PPA yields improved results while conserving resources and accelerating execution.},
  archive      = {J_APIN},
  author       = {Suny, Alfe and Liza, Maimuna Akter and Fahim, Md. and Reza, Ahmed Wasif and Siddique, Nazmul},
  doi          = {10.1007/s10489-025-06320-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Chaotic opposition-based plant propagation algorithm for engineering problem},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep random walk inspired multi-view graph convolutional
networks for semi-supervised classification. <em>APIN</em>,
<em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06322-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies highlight the growing appeal of multi-view learning due to its enhanced generalization. Semi-supervised classification, using few labeled samples to classify the unlabeled majority, is gaining popularity for its time and cost efficiency, particularly with high-dimensional and large-scale multi-view data. Existing graph-based methods for multi-view semi-supervised classification still have potential for improvement in further enhancing classification accuracy. Since deep random walk has demonstrated promising performance across diverse fields and shows potential for semi-supervised classification. This paper proposes a deep random walk inspired multi-view graph convolutional network model for semi-supervised classification tasks that builds signal propagation between connected vertices of the graph based on transfer probabilities. The learned representation matrices from different views are fused by an aggregator to learn appropriate weights, which are then normalized for label prediction. The proposed method partially reduces overfitting, and comprehensive experiments show it delivers impressive performance compared to other state-of-the-art algorithms, with classification accuracy improving by more than 5% on certain test datasets.},
  archive      = {J_APIN},
  author       = {Chen, Zexi and Chen, Weibin and Yao, Jie and Li, Jinbo and Wang, Shiping},
  doi          = {10.1007/s10489-025-06322-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Deep random walk inspired multi-view graph convolutional networks for semi-supervised classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overview of the application of intelligent optimization
algorithms in multi-attribute group decision making. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06324-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Optimization Algorithms (IOAs) have great potential in solving multi-attribute group decision-making (MAGDM) problems. These problems have gradually become a research hotspot in the field of intelligent decision-making due to their advantages of high decision-making accuracy, versatility, and objective evaluation. This study provides a detailed analysis of the challenges in the MAGDM process and evaluates the feasibility of applying IOAs in this context. Specifically, we study the application of IOAs in the MAGDM process and discuss the advantages and limitations across various application scenarios, including the applications of granulating linguistic information, adjusting decision information, optimizing weights, and aggregating decision information. In addition, the development prospects and challenges of IOAs integration with MAGDM are summarized.},
  archive      = {J_APIN},
  author       = {Kang, Kaiying and Xie, Jialiang and Liu, Xiaohui and Wang, Honghui},
  doi          = {10.1007/s10489-025-06324-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Overview of the application of intelligent optimization algorithms in multi-attribute group decision making},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). MVSRF: Point cloud semantic segmentation and optimization
method for granular construction objects. <em>APIN</em>, <em>55</em>(6),
1–16. (<a href="https://doi.org/10.1007/s10489-025-06326-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying shapeless granular materials in complex construction scenarios is critical for achieving automation in engineering equipment such as wheel loaders. The challenges of segmenting point clouds for granular materials involve dealing with sparsity, real-time processing requirements, the lack of distinct shape representation, and the issue of different materials sharing similar shapes. This paper proposes MVSRF, a real-time multi-view based point cloud semantic segmentation method incorporating a single-frame re-segmentation component and a multi-frame semantic filter to enhance accuracy and robustness. First, the segmentation system generates a sparse pixel-depth grid map via semantic projection to encapsulate global points and their behaviors, while employing an edge detector to label boundary points around objects. Second, a zero-shot re-segmentation algorithm involving seed extension, novel one-dimensional DBSCAN, Delaunay triangulation, and semantic reassignment corrects mis-segmented points caused by mapping bias. Finally, a lightweight semantic filter is designed to suppress semantic noise during multiple observations. We have built a multi-sensor platform on a wheel loader and collected experimental data to verify the effectiveness of our method. Two optimization components illustrated exceptional performance on the annotated dataset. The MVSRF method possesses strong robustness against external calibration errors, camera pose estimation errors, and inaccurate image segmentation, providing a practical solution for real-time perception of granular materials.},
  archive      = {J_APIN},
  author       = {Zhang, Lunhui and Liu, Guangjun and Lu, Jiaqi and Wang, Changxin},
  doi          = {10.1007/s10489-025-06326-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {MVSRF: Point cloud semantic segmentation and optimization method for granular construction objects},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A 3D-CNN and multi-loss video prediction architecture.
<em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06328-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The achievements of deep learning in the sphere of computer vision have elevated video prediction to a prominent research focus. The prevailing trend in current deep learning endeavors is to pursue advanced optimization of model architectures and enhancement of their performance metrics. The task of video prediction is inherently complex, and most of the algorithm models proposed in the past are also. In this paper, we propose a novel simple video prediction network structure based on three-Dimensional Convolutional Neural Network (3D-CNN) and multi-loss, abbreviated as ML3DVP. Our network model is completely based on 3D-CNN. Compared with Convolutional Long Short-Term Memory (ConvLSTM), Recurrent Neural Network (RNN), Generative Adversarial Network (GAN) and its variants, we start from the most basic network structure to reduce complexity, thereby improving the speed of model prediction. In addition, most models today will encounter quality problems such as insufficient clarity. To solve this problem, we introduced multiple losses for back propagation. Using multiple quality evaluation indicators, Structural Similarity (SSIM) and Peak Signal-to-Noise Ratio (PSNR), as optimization objectives, continuously improves the prediction quality during the training process. The evaluation of model complexity, parameter count, and predictive outcomes across four datasets substantiates that our proposed model has successfully attained the objectives of structural refinement and enhanced performance.},
  archive      = {J_APIN},
  author       = {Qin, Ziru and Dai, Qun},
  doi          = {10.1007/s10489-025-06328-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {A 3D-CNN and multi-loss video prediction architecture},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concept agent network for zero-base generalized few-shot
learning. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06331-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Few-Shot Learning (GFSL) aims to recognize novel classes with limited training samples without forgetting knowledge of auxiliary data (base classes). Most current approaches re-engage the base classes after initial training to balance the predictive bias between the base and novel classes. However, re-using the auxiliary data might not always be possible due to privacy or ethical constraints. Consequently, the zero-base GFSL paradigm emerges, where models trained on the base classes are directly fine-tuned on the novel classes without revisiting the auxiliary data, avoiding the re-balancing of prediction biases. We believe that solving this paradigm relies on a critical yet often overlooked issue: feature overlap between the base and novel classes in the embedding space. To tackle this issue, we propose the Concept Agent Network, a novel framework that interprets visual features as affinity features, thereby effectively diminishing feature overlap by aggregating feature embeddings of the novel classes according to their similarity with the base classes. Additionally, we present the Concept Catena Generator, which creates multiple concepts per base class, improving understanding of the feature distribution of the base classes and clarifying the relationships between the base and novel concepts. To prevent the catastrophic forgetting of the base classes when adapting to the novel ones, we propose an Active Training Regularization strategy, promoting the preservation of base class knowledge. Extensive experimental results on two benchmarks, mini-ImageNet and tiered-ImageNet, have demonstrated the effectiveness of our framework. The potential utility of our framework spans several real-world applications, including autonomous driving, medical image analysis, and real-time surveillance, where the ability to rapidly learn from a few examples without forgetting previously acquired knowledge is critical.},
  archive      = {J_APIN},
  author       = {Wang, Xuan and Ji, Zhong and Liu, Xiyao and Pang, Yanwei and Li, Xuelong},
  doi          = {10.1007/s10489-025-06331-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Concept agent network for zero-base generalized few-shot learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAAR: Dual attention cooperative adaptive pruning rate by
data-driven for filter pruning. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06332-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model compression can address the limitations of deep learning in resource-constrained situations by reducing the computational and storage requirements of the model. Structured pruning has emerged as an important compression technique because of its operational flexibility and effectiveness. However, the existing structural pruning methods have two limitations: 1) They use a single measurement to identify the importance of the filters in all the layers, resulting in a loss of spatial information in the shallow layers. 2) The pruning rate is highly dependent on manual interference, which is highly subjective. In this paper, a filter pruning method called dual attention cooperative adaptive pruning rate (DAAR) is proposed. Specifically, a dual attention module that combines spatial attention and channel attention is proposed to measure the effectiveness of the filters. Spatial attention is used in the shallow layers, and channel attention is used in the deep layers. This allows the filter measurements to consider spatial information effectively. An adaptive pruning rate adjustment strategy is also used to eliminate manual subjectivity, achieving precision pruning of each convolutional layer. The experimental results on various datasets and networks demonstrate that the DAAR method achieves improved model performance after pruning. For example, in the CIFAR10 dataset, the precision increases from 93.5% to 93.75% after removing the floating point operations (FLOPs) of 84.1%, outperforming the state-of-the-art pruning methods.},
  archive      = {J_APIN},
  author       = {Lian, Suyun and Zhao, Yang and Pei, Jihong},
  doi          = {10.1007/s10489-025-06332-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {DAAR: Dual attention cooperative adaptive pruning rate by data-driven for filter pruning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ED-END: Robust watermarking technology based on deep
coupling of feature extractors. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06333-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based watermarking methods have been developed to address the shortcomings of traditional watermarking algorithms. Some methods adopt an end-to-end framework to train the watermarking model, enabling excellent watermark embedding and extraction. However, the visual quality and robustness of these approaches remain insufficient, especially ignoring the adequacy of image feature extraction and the correlation between network modules. We propose a feature extractor and decoder deep coupled watermark network, which can help generate high-robust watermarked images. Specifically, a down-sampling feature extractor is employed to supplement image features post-decoder, the extracted features are synchronously provided to the encoder for watermark embedding. Additionally, skip-connection is introduced to share each layer feature information of the decoder with the encoder, thereby improving the correlation between network modules. Comprehensive experimental results show that the proposed scheme can achieve high robustness against screen-shooting and paper printing processes while maintaining the visual quality of the watermarked image.},
  archive      = {J_APIN},
  author       = {Li, Jun and Fang, Yixiang and Zhao, Yi and Xu, Kangkang and Wang, Junxiang},
  doi          = {10.1007/s10489-025-06333-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {ED-END: Robust watermarking technology based on deep coupling of feature extractors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control of traffic network signals based on deep
deterministic policy gradients. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06208-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The centralized control of traffic signals is a challenging problem due to the high randomness and complexity of traffic flow on urban road networks and the interaction between intersections. Centralized control leads to high spatial dimensionality of joint actions for traffic road network signal control. However, the decisive action output can solve the problem of “dimensional explosion” caused by joint actions. In this paper, we propose a deep deterministic policy gradient-based algorithm for centralized control of urban traffic road network signals. We simplify the traffic signal control to a four-phase green signal ratio, and the deep deterministic policy gradient-based algorithm deterministically outputs the control signal for each intersection based on the information of the whole traffic network, thus avoiding the problem of “dimensional explosion”. In particular, a new normalization function is proposed to generate the green rate of traffic signals and constrain it to a range of maximum and minimum sustained green time by linear transformation, which makes the generated traffic signals more realistic. Our proposed algorithm is shown to be optimal and robust compared to Deep Q-Network(DQN) based and fixed time control for 7-hour SUMO simulation of a single-peak traffic network with three intersections.},
  archive      = {J_APIN},
  author       = {Hu, Huifeng and Lin, Shu and Wang, Ping and Xu, Jungang},
  doi          = {10.1007/s10489-024-06208-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Control of traffic network signals based on deep deterministic policy gradients},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loop closure detection based on image feature matching and
motion trajectory similarity for mobile robot. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-05874-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual simultaneous localization and mapping (SLAM), loop closure detection plays an irreplaceable role in eliminating cumulative errors, optimizing robot poses, and ensuring map consistency. Most loop closure detection algorithms adopt single feature or feature fusion to detect loop closures, which makes it difficult to ensure accuracy in environments with changing lighting or high-similarity scenes. In this work, image features and motion trajectories are combined to improve the effectiveness of loop closure detection via a staged detection method. First, histogram equalization is used to reduce the algorithm’s sensitivity to lighting. Then, LBP features are used to divide keyframes into multiple sequences, and the sequence where the loop closure candidate frame is located is determined according to the image feature matching results. Then, the most matched keyframe is searched in the sequence as a candidate loop closure. Finally, the true loop closure is confirmed by comparing the motion trajectory similarity to improve the algorithm’s adaptability to high-similarity scenes. The experimental results show that in different application scenarios, the proposed method can achieve good results in terms of precision, recall, area under the curve (AUC), and recall when the precision is 100%.},
  archive      = {J_APIN},
  author       = {Hao, Weilong and Wang, Peng and Ni, Cui and Huangfu, Wenjun and Liu, Zhu and Qi, Kaiyuan},
  doi          = {10.1007/s10489-024-05874-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Loop closure detection based on image feature matching and motion trajectory similarity for mobile robot},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous multi-modal graph network for arterial travel
time prediction. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-024-05895-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Travel time prediction has important influence on the overall control of urban Intelligent Transportation Systems (ITS). Urban arterial networks are typically composed of links and intersections, where each link or intersection can be regarded as a spatial node within the network. However, existing researches predominantly focus on modeling spatial nodes in the link modality to predict travel times in urban arterial networks, neglecting the potential correlations among heterogeneous modal nodes. To overcome these limitations, we propose a Heterogeneous Multi-Modal Graph Neural Network (HMGNN) specifically tailored for travel time prediction in arterial networks. Specifically, we innovatively construct spatial correlation graphs that capture the unique traffic characteristics of intersection modal nodes. Furthermore, we design a cross-modal graph generator that captures the latent spatiotemporal features between spatial nodes of distinct modalities, resulting in the generation of heterogeneous modal graphs. Finally, our proposed HMGNN model incorporates tailored network structures for graphs of varying complexities, enabling targeted mining of their inherent information to derive the final prediction results. Extensive experiments conducted using real-world traffic data from Zhangzhou, China, demonstrate that our HMGNN model achieves significant improvements in prediction accuracy.},
  archive      = {J_APIN},
  author       = {Fang, Jie and He, Hangyu and Xu, Mengyun and Wu, Xiongwei},
  doi          = {10.1007/s10489-024-05895-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Heterogeneous multi-modal graph network for arterial travel time prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel consensus reaching approach for large-scale
multi-attribute emergency group decision-making under social network
clustering based on graph attention mechanism. <em>APIN</em>,
<em>55</em>(6), 1–28. (<a
href="https://doi.org/10.1007/s10489-024-05992-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency decision-making problem is common in our daily life. To solve this kind of problem, a group of decision-makers (DMs) are usually invited to make a decision in a limited time. Since multiple attributes are usually considered, it’s called large-scale multi-attribute emergency group decision-making (LS-MA-EGDM). There are two issues in the general research of LS-MA-EGDM. First, clustering and consensus-reaching process (CRP) should consider the influence of DMs’ intrinsic features. Second, consensus adjustment within and among sub-clusters ought to be fast to prevent multi-round iteration. Accordingly, (1) we introduce graph attention mechanism to calculate the attention coefficients between DM pair’s intrinsic features. The multi-head graph attention coefficient based on social network analysis (SNA) is proposed, which is then combined with opinion similarity to construct a social network clustering method. (2) The Einstein product operator is introduced to propagate the attention coefficients and yield DMs’ weights, which is then incorporated in the subsequent adjustment allocation. (3) Identification rules are provided based on four consensus types in the CRP. The one-iteration personalized adjustment strategies corresponding to different consensus types are then proposed. (4) Evidential reasoning (ER) algorithm is finally utilized to aggregate the preferences of clusters after consensus is reaching. The proposed method is further applied to a chemical plant explosion in Texas to illustrate its effectiveness and validity in dealing with emergencies.},
  archive      = {J_APIN},
  author       = {Zhou, Mi and Zhang, Ying and Fan, Xin-Yu and Wu, Ting and Cheng, Ba-Yi and Wu, Jian},
  doi          = {10.1007/s10489-024-05992-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Appl. Intell.},
  title        = {A novel consensus reaching approach for large-scale multi-attribute emergency group decision-making under social network clustering based on graph attention mechanism},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for intelligent disease diagnosis
using audio signals: Beyond copd to a spectrum of diseases.
<em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-06028-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the widespread prevalence and significant patient base of COPD (Chronic Obstructive Pulmonary Disease), the development of simple and rapid diagnostic methods has emerged as a key research focus. Through pathological studies, the medical community has identified the potential of cough sounds for diagnosing COPD, sparking interest in leveraging deep learning to analyze various disease-related sounds, including those associated with COVID-19 and cardiac conditions, etc. Yet, research specifically targeting COPD remains scarce, primarily due to two challenges: traditional models trained on small medical datasets often fall short of expectations due to stringent data privacy and collection requirements in healthcare; and the scarcity of publicly accessible COPD datasets, particularly those that could obviate the need for medical equipment. Addressing these challenges, our paper introduces a novel dataset of smartphone-recorded cough sounds, termed the CC (COPD-Cough) dataset. It comprises 221 recordings from COPD patients and 632 from healthy individuals, marking the first dataset explicitly curated for COPD cough sound analysis. The dataset, endorsed by clinical professionals and collected independently of medical devices, promises to propel advancements in straightforward COPD diagnostics. Furthermore, we propose a self-supervised learning model enhanced by unique data augmentation techniques and an efficient sound feature extractor, demonstrating superior performance across three distinct disease datasets and achieving state-of-the-art results. Comprehensive ablation studies affirm our model’s efficacy, while sensitivity analyses optimize its applicability to various tasks. For further engagement, the framework’s source code and dataset are available at https://github.com/auto-chao/COPD_Diagnosis and https://zenodo.org/records/10209837 , respectively.},
  archive      = {J_APIN},
  author       = {Sun, Wenchao and Wu, Gang and Ming, Ming and Zhang, Jiameng and Shi, Chun and Qin, Linlin},
  doi          = {10.1007/s10489-024-06028-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised learning for intelligent disease diagnosis using audio signals: Beyond copd to a spectrum of diseases},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A BERT-based review helpfulness prediction model utilizing
consistency of ratings and texts. <em>APIN</em>, <em>55</em>(6), 1–14.
(<a href="https://doi.org/10.1007/s10489-024-06100-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting review helpfulness (RH) to ensure that consumers make effective purchasing decisions is a significant area of study. Many scholars have attempted to develop accurate review helpfulness prediction (RHP) methodologies. However, most previous studies have mainly focused on predictions using product review texts, and few studies have used product satisfaction as indicated by star ratings, particularly the consistency between review texts and star ratings. This study proposes a novel model called BHelP-CoRT (Bidirectional Encoder Representations from Transformers based RHP model utilizing consistency of ratings and texts) to predict RH. The proposed model consists of a review text encoder, star rating encoder, and text-rating interaction. The review text encoder was developed by applying the BERT model to extract contextual semantic features embedded in review texts. The star rating encoder was designed to embed star ratings into feature vectors. The text-rating interaction was constructed by applying an attention mechanism to extract the text-rating interaction and introduce consistency into the RHP tasks. This study conducted extensive experiments to demonstrate the effectiveness of the proposed model from multiple perspectives using real-world online reviews collected from Amazon. The experimental results show that the proposed model outperforms the state-of-the-art models, indicating that it can improve the RHP performance. Specifically, this effectiveness is reflected in the processing of reviews containing inconsistent information. This study supports the marketing efforts of the e-commerce industry by providing an RHP service to address consumer information overload.},
  archive      = {J_APIN},
  author       = {Li, Xinzhe and Li, Qinglong and Ryu, Dongyeop and Kim, Jaekyeong},
  doi          = {10.1007/s10489-024-06100-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {A BERT-based review helpfulness prediction model utilizing consistency of ratings and texts},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCSNet: A novel transformer-CNN fusion architecture for
enhanced segmentation and classification on high-resolution
semiconductor micro-scale defects. <em>APIN</em>, <em>55</em>(6), 1–18.
(<a href="https://doi.org/10.1007/s10489-024-06122-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of semiconductor integrated circuit manufacturing, accurately identifying the root causes of defects is critical for enhancing yield rates. Traditionally, this analytical process has been both time-intensive and challenged by inaccuracies, primarily due to the intricate and varied morphology of wafer defects. While convolutional neural networks (CNNs) with encoder-decoder architectures have made significant strides in the segmentation of defects, they inherently struggle to capture distant interactions and achieve high performance in classification tasks. Conversely, recent advancements in transformers have showcased their proficiency in learning global image dependencies. However, transformers often lack the specific graphical priors and the adaptability typically associated with CNNs. Addressing these limitations, we introduce SCSNet, an innovative architecture that merges the strengths of transformers and CNNs. This fusion network is designed to enhance both segmentation and classification of scanning electron microscopy (SEM) images of wafer defects. SCSNet incorporates a conventional encoder-decoder framework, supplemented by shape flow branches and multi-cross-attention (MCF) modules within a skip connection architecture. Rigorous experimentation on a dataset of 4425 high-resolution wafer defects, sourced from our operational wafer fabrication facility, demonstrates SCSNet’s superior performance. Notably, SCSNet surpasses existing advanced CNNs, transformers, and their hybrid counterparts, achieving a classification accuracy of 97.62% and a segmentation Intersection over Union (IoU) of 84.09%. Currently implemented on our local server for engineering use, SCSNet represents a major advancement in semiconductor manufacturing, offering a more precise and efficient tool for wafer defect analysis.},
  archive      = {J_APIN},
  author       = {Luo, Yuening and Mei, Zhouzhouzhou and Qiao, Yibo and Chen, Yining},
  doi          = {10.1007/s10489-024-06122-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {SCSNet: A novel transformer-CNN fusion architecture for enhanced segmentation and classification on high-resolution semiconductor micro-scale defects},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreqFaceNet: An enhanced transformer architecture with
dual-order frequency attention for deepfake detection. <em>APIN</em>,
<em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06168-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of AI-based image synthesis tools and techniques, Deepfakes have become a serious problem as they pose a massive threat to one’s information security and personal privacy. Several architectures have been proposed to achieve robust Deep Fake detection. However, these methods suffer a drastic drop in performance if the images are visually degraded or have low resolution. To resolve these two issues, a novel FreqFaceNet model has been proposed that employs two novel attentions namely, Wavelet Attention and Fourier Attention, for extracting important frequency-based features from low-resolution images. The extraction of frequency-based features ensures minimal interference of noise due to image compression or low resolution. The proposed model excels on two public benchmark datasets—the DFDC and CelebDF. On the DFDC dataset, FreqFaceNet achieves 98.041% accuracy, an AUC value of 99.748, and a Mathews Correlation Coefficient (MCC) value of 93.857, while on the CelebDF dataset, it obtains an accuracy of 98.325%, an AUC value of 99.81, and an MCC value of 92.819. Qualitative analysis of the proposed model indicates strong classification capabilities. An ablation study has also been conducted to verify the complementary contributions of both Wavelet and Fourier Attention mechanisms.},
  archive      = {J_APIN},
  author       = {Gupta, Varun and Srivastava, Vaibhav and Yadav, Ankit and Vishwakarma, Dinesh Kumar and Kumar, Narendra},
  doi          = {10.1007/s10489-024-06168-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {FreqFaceNet: An enhanced transformer architecture with dual-order frequency attention for deepfake detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VAEneu: A new avenue for VAE application on probabilistic
forecasting. <em>APIN</em>, <em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-024-06203-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces VAEneu, a novel autoregressive method for multistep ahead univariate probabilistic time series forecasting, designed to address the challenges of generating sharp and well-calibrated probabilistic forecasts without assuming a specific parametric form for the predictive distribution. VAEneu leverages the Conditional VAE framework and optimizes the likelihood of the predictive distribution using the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule, as the loss function. This approach enables the model to learn flexible, sharp, and well-calibrated predictive distributions without the need for a tractable likelihood function. In a comprehensive empirical study, VAEneu is rigorously benchmarked against 12 baseline models across 12 datasets, demonstrating superior performance in both forecasting accuracy and uncertainty quantification. VAEneu provides a valuable tool for quantifying future uncertainties, and our extensive empirical study lays the foundation for future comparative studies for univariate multistep ahead probabilistic forecasting.},
  archive      = {J_APIN},
  author       = {Koochali, Alireza and Tahaei, Ensiye and Dengel, Andreas and Ahmed, Sheraz},
  doi          = {10.1007/s10489-024-06203-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {VAEneu: A new avenue for VAE application on probabilistic forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: A diverse/converged individual competition
algorithm for computationally expensive many-objective optimization.
<em>APIN</em>, <em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-024-06225-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Lin, Jie and Zhang, Sheng Xin and Zheng, Shao Yong},
  doi          = {10.1007/s10489-024-06225-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: A diverse/converged individual competition algorithm for computationally expensive many-objective optimization},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IPAttack: Imperceptible adversarial patch to attack object
detectors. <em>APIN</em>, <em>55</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s10489-025-06246-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of deep learning, general object detectors have become increasingly popular in our daily lives. Extensive research, however, has shown that existing detectors are vulnerable to patch-based adversarial attacks, which fool such detectors by crafting adversarial patches. Although existing methods have made significant progress in terms of attack success rate, they still suffer from a highly perceptible problem, making it easy for humans to distinguish these evil examples. To address this issue, in this paper, we propose a novel spatial transform-based end-to-end patch attack method, called IPAttack, to synthesize imperceptible adversarial patches. Our approach estimates a flow field $$\varvec{f}$$ to formulate adversarial examples rather than introduce small $$L_p$$ -norm constrained external perturbations. Besides, to improve the imperceptibility and maintain a high attack performance, we propose the Object Detector Class Activation Map (OD-CAM) for objectors to extract the most interesting region, which will be applied to spatial transform to generate the final adversarial examples. Extensive experiments demonstrate that the proposed IPAttack can generate patch-wised adversarial examples with high imperceptibility while achieving the best attack performance compared to existing methods.},
  archive      = {J_APIN},
  author       = {Wen, Yongming and Si, Peiyuan and Zhou, Wei and Zhao, Zongheng and Yi, Chao and Liu, Renyang},
  doi          = {10.1007/s10489-025-06246-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Appl. Intell.},
  title        = {IPAttack: Imperceptible adversarial patch to attack object detectors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FinCaKG-onto: The financial expertise depiction via
causality knowledge graph and domain ontology. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06247-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality stands as an essential relation for elucidating the reasoning behind given contents. However, current causality knowledge graphs fall short in effectively illustrating the inner logic in a specific domain, i.e. finance. To generate such a functional knowledge graph, we propose the multi-faceted approach encompassing causality detection module, entity linking module, and causality alignment module to automatically construct FinCaKG-Onto with the guidance of expert financial ontology - FIBO. In this paper, we outline the resources and methodology employed for FinCaKG-Onto construction, present the schema of FinCaKG-Onto, and share the final knowledge graph FinCaKG-Onto. Through various user scenarios, we demonstrate that FinCaKG-Onto not only captures nuanced domain expertise but also explicitly unveils the causal logic for any anchor terms. To facilitate your convenience of future use, a check table is conducted as well to showcase the quality of FinCaKG-Onto. The related resources are available in the webpage&lt; https://www.ai.iee.e.titech.ac.jp/FinCaKG-Onto/ &gt;.},
  archive      = {J_APIN},
  author       = {Xu, Ziwei and Ichise, Ryutaro},
  doi          = {10.1007/s10489-025-06247-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {FinCaKG-onto: The financial expertise depiction via causality knowledge graph and domain ontology},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-guided classification and regression surrogates
co-assisted multi-objective soft subspace clustering algorithm.
<em>APIN</em>, <em>55</em>(6), 1–29. (<a
href="https://doi.org/10.1007/s10489-025-06266-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of multi-objective soft subspace clustering algorithms (MSSCAs) can be low when applied to large-scale datasets. This inefficiency arises because the multi-objective evolutionary algorithms (MOEAs) utilized in MSSCAs often require a large number of soft subspace clustering objective function evaluations due to their population-based nature. Moreover, relying solely on negative Shannon entropy to constrain feature weights is inadequate for soft subspace clustering algorithms. To address these issues, a knowledge-guided classification and regression surrogates co-assisted multi-objective soft subspace clustering (KCRS-MOSSC) algorithm is presented. First, an inter-cluster feature weight dissimilarity function is designed to further constrain the feature weights. Furthermore, a novel surrogate-based optimization framework called the knowledge-guided classification and regression surrogates co-assisted multi-objective evolutionary framework (KCRS-MOEF) is proposed to efficiently optimize the proposed inter-cluster feature weight dissimilarity function, intra-cluster compactness function, inter-cluster separation function, and negative Shannon entropy function. In KCRS-MOEF, a classification decision tree is utilized as the classification surrogate model to help generate a set of promising offspring, while a radial basis function (RBF) model is employed as the regression surrogate model to assist in the infill criterion by predicting the objective function values of the offspring. Furthermore, to fully leverage the knowledge of the evolutionary process, an infill criterion guided by dynamic process knowledge of elite individuals is designed to enhance the convergence and diversity of the population. Finally, a clustering ensemble strategy based on knee point guidance is proposed to generate a final solution from a set of non-dominated individuals. KCRS-MOEF outperforms state-of-the-art counterparts in terms of convergence, diversity, and time efficiency, as demonstrated in four experiments conducted on the DTLZ benchmark. Furthermore, experiments on various datasets show that the clustering performance and time efficiency of KCRS-MOSSC exceed those of comparison algorithms.},
  archive      = {J_APIN},
  author       = {Zhao, Feng and Li, Lu and Liu, Hanqiang},
  doi          = {10.1007/s10489-025-06266-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge-guided classification and regression surrogates co-assisted multi-objective soft subspace clustering algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reusability of bayesian networks case studies: A survey.
<em>APIN</em>, <em>55</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s10489-025-06289-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Networks (BNs) are probabilistic graphical models used to represent variables and their conditional dependencies, making them highly valuable in a wide range of fields, such as radiology, agriculture, neuroscience, construction management, medicine, and engineering systems, among many others. Despite their widespread application, the reusability of BNs presented in papers that describe their application to real-world tasks has not been thoroughly examined. In this paper, we perform a structured survey on the reusability of BNs using the PRISMA methodology, analyzing 147 papers from various domains. Our results indicate that only 18% of the papers provide sufficient information to enable the reusability of the described BNs. This creates significant challenges for other researchers attempting to reuse these models, especially since many BNs are developed using expert knowledge elicitation. Additionally, direct requests to authors for reusable BNs yielded positive results in only 12% of cases. These findings underscore the importance of improving reusability and reproducibility practices within the BN research community, a need that is equally relevant across the broader field of Artificial Intelligence.},
  archive      = {J_APIN},
  author       = {Babakov, Nikolay and Sivaprasad, Adarsa and Reiter, Ehud and Bugarín-Diz, Alberto},
  doi          = {10.1007/s10489-025-06289-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {Reusability of bayesian networks case studies: A survey},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NPGCL: Neighbor enhancement and embedding perturbation with
graph contrastive learning for recommendation. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06301-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have significantly advanced recommendation systems by modeling user-item interactions through bipartite graphs. However, real-world user-item interaction data are often sparse and noisy. Traditional bipartite graph modeling fails to capture higher-order relationships between users and items, limiting the ability of GNNs to learn high-quality node embeddings. While existing graph contrastive learning methods address data sparsity by partitioning nodes into positive and negative pairs, they also neglect these higher-order relationships, thus limiting the effectiveness of contrastive learning in recommendation systems. Furthermore, due to the inherent limitations of graph convolution, noise can propagate and amplify with increasing layers in deep graph convolutional networks. To address these challenges, Neighbor Enhancement and Embedding Perturbation for Graph Contrastive Learning (NPGCL) is proposed, which introduces two key modules - Relational Neighbor Enhancement Module and Collaborative Neighbor Enhancement Module - to capture higher-order relationships between homogeneous nodes and calculate interaction importance for noise suppression. Moreover, NPGCL employs an Embedding Perturbation Strategy and applies inter-layer contrastive learning to mitigate the noise impact caused by multi-layer graph convolutions. Experimental results demonstrate that NPGCL significantly improves performance across four publicly available datasets, with a notable enhancement in robustness, especially in noisy environments. Specifically, NPGCL achieves performance improvements of 1.77%-3.34% and 3.87%-9.07% on the Gowalla and Amazon-books datasets, respectively. In noisy datasets, NPGCL improves Recall@20 by 4.98% and 10.92%, respectively.},
  archive      = {J_APIN},
  author       = {Wu, Xing and Wang, Haodong and Yao, Junfeng and Qian, Quan and Song, Jun},
  doi          = {10.1007/s10489-025-06301-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {NPGCL: Neighbor enhancement and embedding perturbation with graph contrastive learning for recommendation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composite gaussian processes flows for learning
discontinuous multimodal policies. <em>APIN</em>, <em>55</em>(6), 1–20.
(<a href="https://doi.org/10.1007/s10489-025-06302-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning control policies for real-world robotic tasks often involve challenges such as multimodality, local discontinuities, and the need for computational efficiency. These challenges arise from the complexity of robotic environments, where multiple solutions may coexist. To address these issues, we propose Composite Gaussian Processes Flows (CGP-Flows), a novel semi-parametric model for robotic policy. CGP-Flows integrate Overlapping Mixtures of Gaussian Processes (OMGPs) with the Continuous Normalizing Flows (CNFs), enabling them to model complex policies addressing multimodality and local discontinuities. This hybrid approach retains the computational efficiency of OMGPs while incorporating the flexibility of CNFs. Experiments conducted in both simulated and real-world robotic tasks demonstrate that CGP-flows significantly improve performance in modeling control policies. In a simulation task, we confirmed that CGP-Flows had a higher success rate compared to the baseline method, and the success rate of GCP-Flow was significantly different from the success rate of other baselines in chi-square tests.},
  archive      = {J_APIN},
  author       = {Wang, Shu-yuan and Sasaki, Hikaru and Matsubara, Takamitsu},
  doi          = {10.1007/s10489-025-06302-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Composite gaussian processes flows for learning discontinuous multimodal policies},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and analysis of a variable-parameter noise-tolerant
ZNN for solving time-variant nonlinear equations and applications.
<em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06304-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solvers considering time-varying parameters are more suitable for addressing a variety of time-varying problems, whereas traditional fixed-parameter neural networks are somewhat insufficient for efficiently and quickly solving these problems. Many existing zeroing neural networks ensure rapid convergence using the infinite-valued AFs. For solving time-varying nonlinear equations, this paper proposes a finitely-activated variable parameter noise tolerant zeroing neural network (VPNTZNN), applied to trajectory tracking of redundant robotic arms. The designed variable parameters are error-dependent, enabling adaptive adjustment to optimal values as errors fluctuate, thereby ensuring faster convergence of the proposed VPNTZNN. And the constructed variable parameters and activation functions (AFs) do not escalate infinitely over time. Affected by the above variable parameters, the proposed finitely-activated VPNTZNN achieves rapid finite-time convergence with strong noise suppression. Simulation results validate the effectiveness of our method in solving time-variant nonlinear equations and in trajectory tracking of redundant manipulators. Moreover, this approach employs a finite-valued activation function to design a variable-parameter neural network, thereby avoiding the difficulties of practical implementation.},
  archive      = {J_APIN},
  author       = {Zhang, Yu and Wang, Liming and Zhong, Guomin},
  doi          = {10.1007/s10489-025-06304-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Design and analysis of a variable-parameter noise-tolerant ZNN for solving time-variant nonlinear equations and applications},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-GCDT: Advanced reinforcement learning with GAN-enhanced
data for continuous excavation system. <em>APIN</em>, <em>55</em>(6),
1–22. (<a href="https://doi.org/10.1007/s10489-025-06308-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automation of excavator operations entails the development and implementation of systems that allow excavators to execute tasks autonomously, thereby significantly reducing the need for human intervention. By integrating advanced sensors and artificial intelligence algorithms, these systems aim to increase operational efficiency, safety, and precision in construction and mining. However, previously developed methods have two weaknesses. First, existing automated excavator systems struggle with adapting to diverse and complex environmental conditions and with precision in control mechanisms. Second, operating an excavator involves multiple, repeated decisions that need to be modeled, planned, and executed in real time. However, there is a significant lack of comprehensive datasets that reflect real-world excavation operations to support this process. In this paper, we present an innovative system named E-GCDT. This system integrates the DoppelGANger module, which generates action time series by emulating human-mined trajectories through a generative adversarial mechanism and replays them in a simulation environment, ultimately expanding the dataset to 155 continuous mining trajectories. Furthermore, E-GCDT integrates terrain features into the decision-making process with the contrastive language-image pre-training model (CLIP), in which the decision transformer optimizes trajectory planning for efficient and accurate continuous excavation tasks. E-GCDT uniquely integrates advanced data augmentation and terrain awareness, developing an advanced Markov decision framework (DT) for continuous excavation tasks. The experimental results of a bulldozer verify that the efficiency of E-GCDT surpasses human efficiency. This system sets a new standard for continuous autonomous mining, and this study provides a new perspective on the application of reinforcement learning in industrial environments.},
  archive      = {J_APIN},
  author       = {Zhao, Qianyou and Gao, Le and Wu, Duidi and Lei, Yihao and Wang, Lingyu and Qi, Jin and Hu, Jie},
  doi          = {10.1007/s10489-025-06308-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {E-GCDT: Advanced reinforcement learning with GAN-enhanced data for continuous excavation system},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WLKA-RVS: A retinal vessel segmentation method using
weighted large kernel attention. <em>APIN</em>, <em>55</em>(6), 1–12.
(<a href="https://doi.org/10.1007/s10489-025-06309-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is an important task in medical image analysis and has a wide range of applications in the diagnosis and treatment of retinal diseases. However, existing segmentation methods still have some shortcomings in accurately segmenting thin vessels. Based on this observation, we propose a Retinal Vessel Segmentation method based on Weighted Large Kernel Attention (WLKA-RVS), which aims to improve the accuracy of retinal vessel segmentation to better assist physicians in clinical diagnosis and treatment. Our method consists of an encoder and a decoder. In the encoder, a convolution stem first reduces the dimension of the input image. Then, feature extraction is performed by four stages of Swin Transformer modules, each stage with a downsampling layer. In the decoder, there are four different stages of Weighted Large Kernel Attention Block (WLKAB) corresponding to the Swin Transformer modules in the encoder. Then WLKA-RVS applies the Patch Expanding module to achieve upsampling. Finally, a linear layer outputs the final results. We have performed extensive experiments comparing several recent advanced models on three public datasets. WLKA-RVS led by 0.32%, 1.24%, and 0.71% in the mAcc metric, respectively. At the same time, the inference speed of WLKA-RVS met the real-time requirements for medical diagnosis. A series of experiments demonstrated the efficiency, robustness, and applicability of WLKA-RVS.},
  archive      = {J_APIN},
  author       = {Li, Jiayao and Zeng, Min and Wu, Chenxi and Cheng, Qianxiang and Guo, Qiuyan and Li, Song},
  doi          = {10.1007/s10489-025-06309-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Appl. Intell.},
  title        = {WLKA-RVS: A retinal vessel segmentation method using weighted large kernel attention},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TADST: Reconstruction with spatio-temporal feature fusion
for deviation-based time series anomaly detection. <em>APIN</em>,
<em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06310-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is crucial in time series analysis for identifying abnormal events. To address the limitations of traditional methods in integrating spatiotemporal correlations and modeling normal patterns, we propose a Time Series Anomaly Detection Model Based on Spatio-Temporal Feature Fusion (TADST). First, the Spatio-Temporal Feature Fusion Network (STF) combines temporal convolutional networks and graph attention influence networks to capture temporal dynamic dependencies and attribute correlations respectively, facilitating joint spatiotemporal feature modeling. Then, the Time Series Reconstruction Network (TSR) employs a multi-layer encoder-decoder architecture to learn the normal sample distribution and amplify discrepancies between reconstructed and anomalous data. Finally, the Anomaly Detection Mechanism (ADM) identifies anomalies by fitting the tail distribution of reconstruction deviations. When the anomaly score exceeds a predefined threshold, the mechanism updates the parameters of the Generalized Pareto Distribution, keeping the detection criteria adaptive. Experiments demonstrate that the proposed TADST achieves state-of-the-art results on five publicly available datasets.},
  archive      = {J_APIN},
  author       = {Yang, Bin and Ma, Tinghuai and Rong, Huan and Huang, Xuejian and Wang, Yubo and Zhao, Bowen and Wang, Chaoming},
  doi          = {10.1007/s10489-025-06310-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {TADST: Reconstruction with spatio-temporal feature fusion for deviation-based time series anomaly detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph regularized independent latent low-rank representation
for image clustering. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06312-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representation (LRR) has been proved to be effective in exploring low-dimensional subspace structure embedded in the observations. However, existing LRR algorithms often pay no attention to data redundancy, easily leading to performance decay. In addition, the LRR characterizes data global inter-connections, from which some latent similarity features should be further learned and exploited to improve the performance of clustering. Therefore, a novel method termed Graph Regularized Independent Latent Low-Rank Representation (GRI-LLRR) is presented to address the above issues. As we know, Hilbert–Schmidt Independence Criterion (HSIC) measures the independence between two distributions. In the proposed method, it is introduced and developed to another novel graph regularization independent term to remove the uncorrelation between vectors and to preserve the data local geometry. With other constraints, including the sparse, nonnegative and symmetric, the LRR is obtained from the observations. Then, the proposed method further learns the cosine features as latent representation of the LRR for final clustering. Massive experiments have been conducted on eight benchmark data sets. Experimental results show that the proposed GRI-LLRR outperforms some state-of-the-art (SOTA) approaches with improvements of 2.24%, 2.73%, and 2.65% on average for CCA, NMI, and Purity, respectively.},
  archive      = {J_APIN},
  author       = {Li, Bo and Pan, Lin-Feng},
  doi          = {10.1007/s10489-025-06312-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Graph regularized independent latent low-rank representation for image clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). A code completion approach combining pointer network and
transformer-XL network. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06315-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code completion is a crucial aspect of contemporary integrated development environments (IDEs), as it not only streamlines the software development process but also bolsters the quality of software products. By leveraging large-scale codes to learn the probability distribution among code token units, deep learning methods have demonstrated significant improvements in the accuracy of token unit recommendations. However, the efficacy of code completion employing deep learning is often compromised by information loss. To mitigate this issue, we introduce a novel code language model that incorporates both the pointer network and the Transformer-XL architecture to surpass the constraints of current approaches in code completion. Our proposed model accepts as input the original code snippet and its corresponding abstract syntax tree (AST), utilizing the Transformer-XL model as the foundational architecture for capturing long-term dependencies. Additionally, we incorporate a pointer network as an adjunct component to forecast Out-of-Vocabulary (OoV) words. Our approach has been rigorously evaluated on the authentic PY150 and JS150 datasets. The comparative experimental results demonstrate the effectiveness of our model in improving the accuracy of the code completion task at the token unit level.},
  archive      = {J_APIN},
  author       = {Zhang, Xiangping and Liu, Jianxun and Long, Teng and Hu, Haize},
  doi          = {10.1007/s10489-025-06315-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {A code completion approach combining pointer network and transformer-XL network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GenKP: Generative knowledge prompts for enhancing large
language models. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06318-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have demonstrated extensive capabilities across various natural language processing (NLP) tasks. Knowledge graphs (KGs) harbor vast amounts of facts, furnishing external knowledge for language models. The structured knowledge extracted from KGs must undergo conversion into sentences to align with the input format required by LLMs. Previous research has commonly utilized methods such as triple conversion and template-based conversion. However, sentences converted using existing methods frequently encounter issues such as semantic incoherence, ambiguity, and unnaturalness, which distort the original intent, and deviate the sentences from the facts. Meanwhile, despite the improvement that knowledge-enhanced pre-training and prompt-tuning methods have achieved in small-scale models, they are difficult to implement for LLMs in the absence of computational resources. The advanced comprehension of LLMs facilitates in-context learning (ICL), thereby enhancing their performance without the need for additional training. In this paper, we propose a knowledge prompts generation method, GenKP, which injects knowledge into LLMs by ICL. Compared to inserting triple-conversion or templated-conversion knowledge without selection, GenKP entails generating knowledge samples using LLMs in conjunction with KGs and makes a trade-off of knowledge samples through weighted verification and BM25 ranking, reducing knowledge noise. Experimental results illustrate that incorporating knowledge prompts enhances the performance of LLMs. Furthermore, LLMs augmented with GenKP exhibit superior improvements compared to the methods utilizing triple and template-based knowledge injection.},
  archive      = {J_APIN},
  author       = {Li, Xinbai and Peng, Shaowen and Yada, Shuntaro and Wakamiya, Shoko and Aramaki, Eiji},
  doi          = {10.1007/s10489-025-06318-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {GenKP: Generative knowledge prompts for enhancing large language models},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent forecasting algorithm of power industry
expansion based on time series and entropy weight method. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06321-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accurately predict the electricity consumption trend of individual users and even the entire industry, this paper studies an intelligent prediction algorithm for the power industry based on time series and entropy weight method. Using ARIMA model and X12 model to establish a monthly electricity consumption prediction model, the study obtains the monthly electricity consumption prediction value for the expansion of the power industry. The entropy weight method is employed to calculate the weights of two power industry expansion month electricity consumption forecasting models, thereby achieving intelligent forecasting. The experimental results demonstrate that the maximum error of the proposed method is only 1.78%, and the average time complexity and average space complexity of the proposed algorithm are both below the set threshold.},
  archive      = {J_APIN},
  author       = {Wu, Guoyao and Lan, Zhiqiang and Wu, Xiaofang and Huang, Xiaoying and Mao, Linling},
  doi          = {10.1007/s10489-025-06321-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent forecasting algorithm of power industry expansion based on time series and entropy weight method},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel data-driven model for explainable hog price
forecasting. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06323-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting hog prices is an important and challenging task for pig producers and managers as it plays a crucial role in decision-making processes. Given the significant impact of raw pork supply, public concern, animal diseases, and international markets on hog prices, this study proposes a comprehensive and explainable hybrid model for hog price forecasting by combining principal component analysis (PCA), variational mode decomposition (VMD), weighted average algorithm (WAA) algorithm, and temporal fusion transformers (TFT). To improve the quality of input variables, search engine data reflecting public concern about live pig prices are dimensionally reduced using PCA. This reduction process helps in eliminating unnecessary information and enhancing the input’s relevance. Additionally, VMD is applied to decompose raw pig futures prices, enabling the capture of their underlying trends over time. Subsequently, all the input variables, including the processed search engine data and the decomposed pig futures prices, are fed into the WAA-TFT model. WAA algorithm optimizes the parameters of the TFT model, resulting in accurate predicted values. The interpretable nature of the TFT model provides valuable decision-making insights for practitioners in the agricultural products market. The experimental results show that the proposed model achieves a mean absolute percentage error (MAPE) of only 1.76% on the Chinese hog price prediction dataset, demonstrating the excellent predictive performance of the proposed model.},
  archive      = {J_APIN},
  author       = {Wu, Binrong and Zeng, Huanze and Hu, Huanling and Wang, Lin},
  doi          = {10.1007/s10489-025-06323-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {A novel data-driven model for explainable hog price forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning discriminative features for multi-hop knowledge
graph reasoning. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06327-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL)-based multi-hop knowledge graph reasoning has achieved remarkable success in real-world applications and can effectively handle knowledge graph completion tasks. This approach involves a policy-based agent navigating the graph environment to extend reasoning paths and identify the target entity. However, most existing multi-hop reasoning models are typically constrained to stepwise inference, which inherently disrupts the global information of multi-hop paths. To overcome this limitation, we introduce discriminative features between valid and invalid paths as global information. Here, we propose a multi-hop path encoder specifically designed to extract these discriminative features. Firstly, a multi-hop path encoding module is employed to derive each path’s hidden features, using cross-attention mechanisms to strengthen the interaction between triple context and path features. Secondly, a discriminative feature extraction module is used to capture the differences between valid and invalid paths. Thirdly, an attention-enhanced gated fusion mechanism is implemented to integrate these discriminative features into the multi-hop inference decoder. We further evaluate our method on five standard datasets. Our method outperforms the baseline models, demonstrating the effectiveness of discriminative features in improving prediction performance, learning speed, and path interpretability.},
  archive      = {J_APIN},
  author       = {Liu, Hao and Li, Dong and Zeng, Bing and Xu, Yang},
  doi          = {10.1007/s10489-025-06327-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Learning discriminative features for multi-hop knowledge graph reasoning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal clustering enhanced multi-graph
convolutional network for traffic flow prediction. <em>APIN</em>,
<em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06329-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamics and uncertainty are the fundamental reasons for the difficulty in accurately predicting traffic flow. In recent years, graph convolutional networks have been widely used in traffic flow prediction because of their excellent dynamic feature mapping ability. However, the existing models usually overlook the correlations among the nodes and the complex impact of external factors on traffic flow, which make it challenging to explore the complex spatial-temporal features. To overcome these shortcomings, we propose a novel Spatial-temporal Clustering enhanced Multi-Graph Convolutional Network (SCM-GCN) for traffic flow prediction. First, a Spatial-Temporal Clustering (STS) module based on the improved adjacency matrix DBSCAN clustering algorithm is constructed, this module divides traffic nodes into multiple highly correlated clusters, each of which consists of multi-graph features and time-varying features. Then, a Multi-Graph Spatial Feature Extraction (MGSFE) module that integrates the graph convolution operation and attention mechanism is designed to extract dynamic spatial features of multi-graph and time-varying features. Next, the Time-Varying Feature Extraction (TVFE) module based on the dilated convolution and gated attention mechanism is constructed. It integrates the output of the MGSFE module to extract dynamic temporal features of time-varying features and output the predicted values. Finally, the comparison and ablation experiments on four datasets show that the proposed model performs better than state-of-the-art models. The key source code and data are available at https://github.com/Bounger2/SCMGCN .},
  archive      = {J_APIN},
  author       = {Bao, Yinxin and Shen, Qinqin and Cao, Yang and Shi, Quan},
  doi          = {10.1007/s10489-025-06329-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Spatial-temporal clustering enhanced multi-graph convolutional network for traffic flow prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic fusion of multi-source heterogeneous data using MOE
mechanism for stock prediction. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06330-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock prices are influenced by numerous factors, including social media, news, and financial reports, serving as indicators of financial market dynamics. However, harnessing diverse information from different sources and structures to predict price trends remains challenging. In this paper, we propose a dual-stage deep learning model based on the Mixture-of-Expert (MoE) mechanism. In stage one, three distinct expert networks encode information about price movements, financial news, and investor sentiments through multi-source interaction attention. In stage two, a gated network dynamically fuses outputs, capturing temporal relationships in windowed data. Experimental results on the Chinese stock market demonstrate our model outperforms existing ones in forecasting tasks.},
  archive      = {J_APIN},
  author       = {Dong, Yuxin and Wu, Zirui and Hao, Yongtao},
  doi          = {10.1007/s10489-025-06330-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic fusion of multi-source heterogeneous data using MOE mechanism for stock prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection based on multi-perspective dynamic
neighbourhood entropy measures in a dynamic neighbourhood rough set.
<em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06336-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighbourhood rough set (NRS)-based feature selection has been extensively applied in data mining. However, the effectiveness of the NRS model is limited by its reliance on the grid search method to determine the optimal neighbourhood parameter, insensitivity to data distribution under different features, and consideration of uncertainty measures from only one single perspective. To address the aforementioned issues, this study first defines a spatial function that can obtain information about the distribution of samples in space according to the change in the feature subset. On this basis, three perspectives of dynamic neighbourhoods are proposed: pessimistic, neutral, and optimistic. Next, the concept of the dynamic neighbourhood rough set (DNRS) model is developed. The most significant feature of this model is its adaptive ability to dynamically update the neighbourhood radius of samples on the basis of the information of their distribution in space, without the necessity of setting neighbourhood parameters artificially. Then, algebraic and information-theoretic views are introduced to propose multi-perspective dynamic neighbourhood entropy measures, which effectively measure the uncertainty of the data. In addition, a nonmonotonic feature selection algorithm based on mutual information is designed to overcome the limitations of feature selection algorithms that rely on monotonic evaluation functions. This algorithm utilizes multi-perspective dynamic neighbourhood entropy measures from a neutral perspective. Finally, to mitigate the high time complexity in feature selection for high-dimensional datasets, the Fisher score is introduced in an initial dimensionality reduction method. The results of the experiment show that the algorithm effectively eliminates redundant features and improves accuracy.},
  archive      = {J_APIN},
  author       = {Xu, Jiucheng and Ma, Miaoxian and Zhang, Shan and Niu, Wulin},
  doi          = {10.1007/s10489-025-06336-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection based on multi-perspective dynamic neighbourhood entropy measures in a dynamic neighbourhood rough set},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GANet: Geometry-aware network for RGB-d semantic
segmentation. <em>APIN</em>, <em>55</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06337-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of RGB-D semantic segmentation has attracted considerable interest in recent times. The challenge is to develop an effective method for combining RGB images, which capture colour variations, with depth images, which provide robust information about object geometry regardless of lighting conditions. Treating both image types equally through the same convolution operator fails to take into account their inherent differences. Thus, in this paper, we propose a novel approach that combines a geometry-aware convolution (GAConv) module and a multiscale fusion module (MFM) with the aim of enhancing the performance of RGB-D image segmentation. The GAConv module effectively captures fine-grained geometric details from depth images, while the MFM module enables efficient integration of multi-scale features, allowing the network to utilise both spatial and semantic information. Extensive experimentation was conducted on the NYUv2 and SUN RGB-D datasets, wherein our model demonstrated consistent superiority over existing state-of-the-art methods in terms of pixel accuracy and mean intersection over union (mIoU).},
  archive      = {J_APIN},
  author       = {Tian, Chunqi and Xu, Weirong and Bai, Lizhi and Yang, Jun and Xu, Yanjun},
  doi          = {10.1007/s10489-025-06337-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {GANet: Geometry-aware network for RGB-D semantic segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature optimization based on multi-order fusion and
adaptive recursive elimination for motion classification in doppler
radar. <em>APIN</em>, <em>55</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06342-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radar-based human motion recognition (HMR) technology has gained substantial importance across diverse domains such as security surveillance, post-disaster search and rescue operations, and the development of smart home environments. The intricate nature of human movements generates radar echo signals with pronounced non-stationary attributes, which encapsulate a wealth of target feature data. However, striking a balance between the precision of motion recognition and the requirement for real-time processing, especially in the context of extracting meaningful features from radar signals, remains a formidable challenge. This research paper introduces a novel approach to tackle this challenge. Firstly,we apply the multi-order fractional Fourier transform (m-FRFT) to radar echo signals, facilitating the extraction of micro-Doppler (m-D) frequency information. Secondly, we have developed an optimized feature selection model named MPG, which stands for m-D parameter screening based on genetic algorithm (GA) and adaptive weight particle swarm optimization (AWPSO). Thirdly, we apply the MPG model to the recursive feature elimination (RFE) algorithm to refine the representation of m-D frequency information, allowing for adaptive parameter adjustment and effective feature dimensionality reduction. The proposed method has been tested using human motion echo data collected from a Doppler radar prototype. The experimental outcomes demonstrate that our approach outperforms traditional feature extraction methods in terms of reducing feature dimensionality, computational efficiency, and classification accuracy.},
  archive      = {J_APIN},
  author       = {Sun, Tong and Ding, Yipeng and Chen, Yuxin and Ping, Lv},
  doi          = {10.1007/s10489-025-06342-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Feature optimization based on multi-order fusion and adaptive recursive elimination for motion classification in doppler radar},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced decision framework for two-player zero-sum markov
games with diverse opponent policies. <em>APIN</em>, <em>55</em>(6),
1–21. (<a href="https://doi.org/10.1007/s10489-025-06344-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper takes into account a general two-player zero-sum Markov game scenario in which our agent faces multi-type opponents with multiple policies. To enhance our agent’s return against opponent’s diverse policies, a novel Decision-making Framework based on Opponent Distinguishing and Policy Judgment (DF-ODPJ) is proposed. On the basis of the pre-trained Nash equilibrium strategies, DF-ODPJ can distinguish the opponent’s type by sampling from the interaction trajectory. Then a fast criterion is proposed to judge the opponent’s policy which is proven to minimize the misjudgment probability with optimal threshold calculated. According to the identification results, appropriate policies are generated to enhance the return. The proposed DF-ODPJ is more flexible since it is orthogonal to existing Nash equilibrium algorithms and single-agent reinforcement learning algorithms. The experimental results on grid world, video games, and UAV aerial combat environments illustrate the effectiveness of DF-ODPJ. The code is available at https://github.com/ChenXJ295/DF-ODPJ .},
  archive      = {J_APIN},
  author       = {Zhu, Jin and Wang, Xuan and Geir E., Dullerud},
  doi          = {10.1007/s10489-025-06344-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced decision framework for two-player zero-sum markov games with diverse opponent policies},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large scale group decision making with expert guidance via
discrete conditional variational autoencoder. <em>APIN</em>,
<em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06345-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Large Scale Group Decision Making (LSGDM), the differences in decision-makers’ professional backgrounds and attitudes often lead to high-quality decisions being overshadowed by numerous low-quality decisions, thus affecting the accuracy of the final decision. This study proposes a new decision-making method to address this challenge. First, a few experts are invited to make decisions as cluster centers, followed by obtaining decisions from a large number of ordinary decision-makers. The ordinary decisions are then generated and modified using a Discrete Conditional Variational Autoencoder (DCVAE) to enhance decision quality while maintaining consistency with expert decisions. Finally, the normalized prediction selection rate (NPSR) and the Borda Count consensus method are integrated to obtain the final result. Experimental results demonstrate the effectiveness of this method in improving the quality of LSGDM, providing a new solution to the coexistence of high- and low-quality decisions.},
  archive      = {J_APIN},
  author       = {Zhang, Hengshan and He, Adong and Sun, Jiaze and Chen, Yanping},
  doi          = {10.1007/s10489-025-06345-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {A large scale group decision making with expert guidance via discrete conditional variational autoencoder},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional time-dependent dynamic graph neural
network for metro passenger flow prediction. <em>APIN</em>,
<em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06346-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate metro passenger flow prediction can provide data support for vehicle scheduling and personnel allocation by metro operation departments, ensuring the efficient utilization of related resources. In recent years, Graph Convolutional Networks (GCNs) have demonstrated excellent performance in spatial processing, making them an effective method for extracting spatiotemporal dependencies in metro passenger flow prediction. However, traditional GCN models focus solely on static relationships between stations, overlooking the dynamic changes in station relationships and typically concentrating on short-term temporal dependencies while neglecting longer-term temporal features. To fully consider the spatiotemporal relationships within the metro network, a Multi-Dimensional Temporal Dependency Graph Neural Network (MTDGNN) is proposed for metro passenger flow prediction. Specifically, 1D dilated convolutions are employed to initially extract multi-dimensional temporal dependencies, generating multiple spatiotemporal dependency extraction channels. Two correlation matrices combined with GCN are then proposed to extract spatial relationships between stations within the metro network. The extracted spatiotemporal features are further captured by a Gated Recurrent Unit (GRU) to enhance temporal feature extraction. Subsequently, a multi-head attention mechanism is utilized to integrate the extraction results from multiple channels to obtain the final prediction. Finally, the model is evaluated using metro ridership data from two cities in southwestern and central China. The results indicate that the proposed model exhibits superior predictive performance compared to other methods. The MAE values on the two datasets are 1.5% to 59.3% lower than those of other methods, and the RMSE values are 3.4% to 60.0% lower than those of other methods.},
  archive      = {J_APIN},
  author       = {Li, Ruisen and Zhao, Liqiang and Tang, Jinjin and Tang, Shuixiong and Hao, Zhenxing},
  doi          = {10.1007/s10489-025-06346-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Multi-dimensional time-dependent dynamic graph neural network for metro passenger flow prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An echo state network with adaptive improved pigeon-inspired
optimization for time series prediction. <em>APIN</em>, <em>55</em>(6),
1–32. (<a href="https://doi.org/10.1007/s10489-025-06347-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective alternative model to recurrent neural network (RNN), echo state network (ESN) has garnered more attention due to its efficiency in handling time series data. Despite the simple training process and rapid convergence speed of ESN, appropriate parameter settings and a concise network structure are crucial for optimal model performance. Therefore, many optimization algorithms have been proposed to obtain the optimal parameters of ESN. Among these methods, the Pigeon-Inspired Optimization (PIO) has gained attention due to its fast search speed, strong evolution capability, and excellent optimization ability. However, the main drawbacks of PIO are that it may easily get trapped in local optima and achieve lower precision results. To address these issues, this paper proposes a hybrid algorithm combining adaptive improved pigeon-inspired optimization with tabu search (TS-APIO) algorithm. By combining the improved PIO and the tabu search (TS), it not only enhances the global search capability but also strengthens its robustness. Additionally, the adaptive adjustment mechanism can improve the generalization ability. Through theoretical analysis and simulation examples, the TS-APIO algorithm can adaptively select the optimal ESN parameters and structure based on different scenarios. It can effectively enhance the ability to capture the dynamic features and reduce the prediction error.},
  archive      = {J_APIN},
  author       = {Yang, Xu and Wang, Lei and Chen, Qili},
  doi          = {10.1007/s10489-025-06347-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Appl. Intell.},
  title        = {An echo state network with adaptive improved pigeon-inspired optimization for time series prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised heterogeneous graph neural network based on
deep and broad neighborhood encoding. <em>APIN</em>, <em>55</em>(6),
1–23. (<a href="https://doi.org/10.1007/s10489-025-06348-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised heterogeneous graph neural networks have shown remarkable effectiveness in addressing the challenge of limited labeled data. However, current contrastive learning methods face limitations in leveraging neighborhood information for each node. Some approaches utilize the local information of the target node, ignoring useful signals from deeper neighborhoods. On the other hand, simply stacking convolutional layers to expand the neighborhood inevitably leads to over-smoothing. To address the problems, we propose HGNN-DB, a Self-supervised Heterogeneous Graph Neural Network Based on Deep and Broad Neighborhood Encoding to tackle the over-smoothing problem within heterogeneous graphs. Specifically, HGNN-DB aims to learn informative node representations by incorporating both deep and broad neighborhoods. We introduce a deep neighborhood encoder with a distance-weighted strategy to capture deep features of target nodes. Additionally, a single-layer graph convolutional network is employed for the broad neighborhood encoder to aggregate broad features of target nodes. Furthermore, we introduce a collaborative contrastive mechanism to learn the complementarity and potential invariance between the two views of neighborhood information. Experimental results on four real-world datasets and seven baselines demonstrate that our model significantly outperforms the current state-of-the-art techniques on multiple downstream tasks. The codes and datasets for this work are available at https://github.com/SSQiana/HGNN-DB.},
  archive      = {J_APIN},
  author       = {Song, Qianyu and Li, Chao and Fu, Jinhu and Zeng, Qingtian and Xie, Nengfu},
  doi          = {10.1007/s10489-025-06348-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised heterogeneous graph neural network based on deep and broad neighborhood encoding},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NNBSVR: Neural network-based semantic vector representations
of ICD-10 codes. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06349-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically predicting ICD-10 codes from clinical notes using machine learning models can reduce the burden of manual coding. However, existing methods often overlook the semantic relationships between ICD-10 codes, resulting in inaccurate evaluations when clinically similar codes are considered completely different. Traditional evaluation metrics, which rely on equality-based matching, fail to capture the clinical relevance of predicted codes. This study introduces NNBSVR (Neural Network-Based Semantic Vector Representations), a novel approach for generating semantic-based vector representations of ICD-10 codes. Unlike traditional approaches that rely on exact code matching, NNBSVR incorporates contextual and hierarchical information to enhance both prediction accuracy and evaluation methods. We validate NNBSVR using intrinsic and extrinsic evaluation methods. Intrinsic evaluation assesses the vectors’ ability to reconstruct the ICD-10 hierarchy and identify clinically meaningful clusters. Extrinsic evaluation compares our relevancy-based approach, which includes customized evaluation metrics, to traditional equality-based metrics on an ICD-10 code prediction task using a 9.57 million clinical notes corpus. NNBSVR demonstrates significant improvements over equality-based metrics, achieving a 9.81% gain in micro-F1 score on the training set and a 12.73% gain on the test set. A manual review by medical experts on a sample of 10,000 predictions confirms an accuracy of 92.58%, further validating our approach. This study makes two significant contributions: first, the development of semantic-based vector representations that encapsulate ICD-10 code relationships and context; second, the customization of evaluation metrics to incorporate clinical relevance. By addressing the limitations of traditional equality-based evaluation metrics, NNBSVR enhances the automated assignment of ICD-10 codes in clinical settings, demonstrating superior performance over existing methods.},
  archive      = {J_APIN},
  author       = {Hatoum, Monah Bou and Charr, Jean Claude and Ghaddar, Alia and Guyeux, Christophe and Laiymani, David},
  doi          = {10.1007/s10489-025-06349-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {NNBSVR: Neural network-based semantic vector representations of ICD-10 codes},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning sparse filters-based convolutional networks without
offline training for robust visual tracking. <em>APIN</em>,
<em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06350-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the scarcity of training samples in the visual tracking task, almost all existing Convolutional Neural Networks (CNNs) based deep tracking algorithms rely heavily on large auxiliary datasets to train the tracking model offline. However, such offline training has two inevitable disadvantages: (1) the learned generic features may be less discriminative for tracking specific objects; (2) the training process demands huge computational power provided by high-performance graphics processing units (GPUs), which is not always available in many practical applications. Therefore, learning effective generic features without offline training for robust visual tracking is a necessary and challenging task. This paper tackles this task by proposing the Sparse Filters-based Convolutional Network (SFCN), which is a fully feed-forward convolutional network with a lightweight structure including two convolutional layers. Its convolutional kernels are a set of sparse filters learned and updated online from local patches using sparse dictionary learning. Benefiting from the learned sparse filters, SFCN learns effective generic features by exploiting both the discriminative information between the foreground and background of the target region and the hierarchical layout information among the local patches inside each target candidate region. Furthermore, a dynamic model updating strategy is adopted to alleviate the drift problem. Extensive experiments on five large-scale benchmark datasets show that the proposed method performs favorably against several state-of-the-art tracking algorithms.},
  archive      = {J_APIN},
  author       = {Xu, Qi and Xu, Zhuoming and Chen, Zhe and Chen, Yun and Wang, Huabin and Tao, Liang},
  doi          = {10.1007/s10489-025-06350-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {Learning sparse filters-based convolutional networks without offline training for robust visual tracking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LGCGNet: A local-global context guided network for real-time
water surface semantic segmentation. <em>APIN</em>, <em>55</em>(6),
1–20. (<a href="https://doi.org/10.1007/s10489-025-06351-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned boats will encounter many static and dynamic obstacles during navigation, and only real-time obstacle sensing can ensure safe navigation and long endurance of unmanned boats. In this paper, LGCGNet is proposed to perform real-time water surface semantic segmentation on the images captured by the on-board camera. In order to ensure that the model adapted to obstacles with extremely variable scales, a local-global module is proposed in this paper. The local-global module consisted of residual dense dilated module and context-enhanced separable self-attention. Residual dense dilated module enabled the enhancement of local detail information and context-enhanced separable self-attention enabled model receptive field expansion. In addition, the sub-pixel downsampling module is used to avoid the loss of feature information to improve segmentation accuracy. Experiments on the MaSTr1325 dataset showed that LGCGNet apprpached the segmentation accuracy of state-of-the-art semantic segmentation models with only 689,000 parameters and 9.068G floating point operations per second, with an mIoU of 84.14%. In addition, the processing speed of LGCGNet is 34.86FPS, which meets the frame rate conditions of commercially available photovoltaic equipment. The experiments demonstrated that the LGCGNet proposed in this paper strike a good balance between achieving high accuracy, reducing model size and improving real-time performance.},
  archive      = {J_APIN},
  author       = {Liu, Ting and Luo, Peiqi and Wang, Guofeng and Zhang, Yuxin and Lu, Xiangyi and Dong, Mengyu},
  doi          = {10.1007/s10489-025-06351-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {LGCGNet: A local-global context guided network for real-time water surface semantic segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scnet: Spectral convolutional networks for multivariate time
series classification. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06352-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of time series data, the study of classification techniques has become an important topic. Although existing multivariate time series classification (MTSC) methods have made progress, they often rely on one-dimensional (1D) time series, which limits their ability to capture complex temporal dynamics and multiscale features. To address these challenges, a Spectral Convolutional Network (SCNet) is introduced in this work. SCNet effectively transforms 1D time series data into the frequency domain using an enhanced Discrete Fourier Transform (enhanced_DFT), revealing periodicity and key frequency components while reshaping the data into a two-dimensional (2D) time series for better representation. Furthermore, it uses a Spectral Energy Prioritization method to optimize frequency domain energy distribution and a multiscale convolutional module to capture features at different scales, improving the model’s ability to analyze short-term and long-term trends. To validate the effectiveness and superiority, we conducted extensive experiments on 10 sub-datasets from the well-known UEA dataset. The results show that our proposed SCNet achieved the highest average accuracy of 74.3%, which is 2.2% higher than the current state-of-the-art models, demonstrating its potential for practical application and efficiency in MTSC task.},
  archive      = {J_APIN},
  author       = {Wu, Xing and Xing, Xinyu and Yao, Junfeng and Qian, Quan and Song, Jun},
  doi          = {10.1007/s10489-025-06352-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Scnet: Spectral convolutional networks for multivariate time series classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precise spiking neurons for fitting any activation function
in ANN-to-SNN conversion. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06354-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) are recognized for their energy efficiency due to spike-based communication. In this regard, the shift towards SNNs is driven by their ability to significantly reduce energy consumption while maintaining the performance of ANNs. Converting Artificial Neural Networks (ANNs) to SNNs is a key research focus, but existing methods often struggle with balancing conversion accuracy and latency, and are typically restricted to ReLU activations. We introduce Precision Spiking (PS) neurons, a novel dynamic spiking neuron model that can precisely fit any activation function by jointly regulating spike timing, reset voltage, and membrane potential threshold. This capability enables exact parameter optimization via iterative methods, achieving low-latency, high-accuracy ANN-to-SNN conversion. Experiments on image classification and natural language processing benchmarks confirm state-of-the-art results, with a maximum conversion loss of 0.55% and up to 0.38% accuracy improvement over the original ANN. To the best of our knowledge, this method offers a significant advancement over existing approaches by achieving high-precision fitting of arbitrary activation functions with low latency and minimal conversion loss, thus considerably expanding the range of feasible ANN-to-SNN conversions.},
  archive      = {J_APIN},
  author       = {Wang, Tianqi and Shen, Qianzi and Li, Xuhang and Zhang, Yanting and Wang, Zijian and Yan, Cairong},
  doi          = {10.1007/s10489-025-06354-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Precise spiking neurons for fitting any activation function in ANN-to-SNN conversion},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NoRD: A framework for noise-resilient self-distillation
through relative supervision. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06355-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has become a pivotal technique in deep learning, facilitating model compression and regularization by transferring knowledge from one neural network to another, enhancing its capabilities for downstream tasks such as classification. However, real-world datasets often suffer from noisy label problems, significantly hindering neural network learning in supervised tasks. Recent advancements in KD aim to improve noise-robustness and regularization in deep neural networks through different learning paradigms. Yet, prevalent approaches often exhibit noise-prone behaviors as the student network heavily relies on the teacher’s learning. To address this challenge, we propose a robust knowledge transfer method, NoRD: a Noise-Resilient Self-Distillation framework. This approach leverages relative self-supervision combined with decision matching to minimize noise susceptibility during the knowledge transfer process. Our study evaluates this technique on CIFAR-10, CIFAR-100, and MNIST datasets with synthetic label noise. Results showcase that our method achieves 8-10% higher test accuracy compared to state-of-the-art noise-robust loss functions at noise rates exceeding 50%, surpassing well-known KD methods by 4-5% in top-1 test accuracy. The code is available at https://github.com/philsaurabh/NoRD_Applied-Intelligence .},
  archive      = {J_APIN},
  author       = {Sharma, Saurabh and Lodhi, Shikhar Singh and Srivastava, Vanshika and Chandra, Joydeep},
  doi          = {10.1007/s10489-025-06355-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {NoRD: A framework for noise-resilient self-distillation through relative supervision},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lbgcn: Lightweight bilinear graph convolutional network with
attention mechanism for recommendation. <em>APIN</em>, <em>55</em>(6),
1–17. (<a href="https://doi.org/10.1007/s10489-025-06357-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graph Convolutional Neural Network (GCN) is a powerful technique for learning and representing graph data, commonly utilized in model-based collaborative filtering recommendation algorithms. However, despite its effectiveness, the issues are data sparsity and interpretability. Most existing GCN-based models simply update the central node’s features by aggregating the features of its neighbors, typically via a weighted sum. Unfortunately, this approach fails to capture the cooperative information hidden in the neighbor interactions. To address this limitation, we propose a recommendation algorithm based on a convolution network of lightweight neighborhood interactive graphs, named the Lightweight Bilinear Graph Convolutional Network (LBGCN). Our approach employs a lightweight graph convolutional neural network as a multi-level feature aggregator, leveraging higher-order connectivity to aggregate neighborhood information into a multi-level feature of the node through the aggregator. Meanwhile, we introduce a local feature aggregator to capture the collaborative filtering signals in the interaction features of neighbors. Finally, we combine the results using an attention mechanism to obtain the embedded representation of final users and items. In addition, we demonstrate the rationality and effectiveness of our proposed model through experiments on three public datasets. The results show that our method could gain 2.52% NDCG improvement at most.},
  archive      = {J_APIN},
  author       = {Su, Yu and Wei, Pingzhu and Zhu, Linbo and Xu, Lixiang and Wang, Xianquan and Tong, He and Han, Ze},
  doi          = {10.1007/s10489-025-06357-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Lbgcn: Lightweight bilinear graph convolutional network with attention mechanism for recommendation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel ensemble bagging-logistic regression algorithm for
NoSQL database security. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06358-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present era, the use of the Internet has drastically increased in the sharing of digital information. In this case, the digital information is stored using cloud technology or NoSQL databases. However, there is a significant challenge in protecting and managing the cloud and NoSQL-based data and extracting required information from these sources while maintaining the actual information. The network traffic has also increased significantly, which requires more memory and sufficient systems to manage and monitor the influx of Big Data. Traditional relational databases face issues in managing and securing the cloud-based dynamic data generated from various sources. NoSQL databases have recently been used to store and manage dynamic data effectively. However, there are security and privacy issues with the NoSQL databases, which remain challenging to provide. Consequently, in the present study, we propose a novel algorithm that enhances the security of the NoSQL databases and predicts its success rate. Initially, we implemented the Fernet data masking algorithm to secure the NoSQL database. Then, the secured data is classified and predicted using an innovative proposed method called the Ensemble Bagging Classifier-Logistic Regression (EBC-LR) to validate the accuracy of the secured NoSQL database. The experimental outcomes depict that our proposed algorithm achieves 85 percent accuracy, better than traditional methods in enhancing the security of NoSQL databases. Our proposed algorithm can effectively predict secure standard databases with the highest success rate.},
  archive      = {J_APIN},
  author       = {Kanade, Anuradha and Vibhute, Amol D. and Kanade, Shantanu},
  doi          = {10.1007/s10489-025-06358-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Novel ensemble bagging-logistic regression algorithm for NoSQL database security},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixmamba-fewshot: Mamba and attention mixer-based method
with few-shot learning for bearing fault diagnosis. <em>APIN</em>,
<em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06361-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, artificial intelligence, particularly machine learning and deep learning has ushered in a new era of technological advancements leading to significant progress across various domains. In the field of computer vision, deep learning has made substantial contributions, impacting everything from daily life to production and industry. When machines, rotating devices, and engines operate, bearing failures are inevitable. Our task is to accurately detect or diagnose these failures. However, a key challenge lies in the lack of sufficient data on bearing faults to train a model capable of delivering highly accurate diagnostic results. To address this issue, in this paper, we propose a new approach named MixMamba-Fewshot, leveraging few-shot learning and using a feature extraction module that integrates an attention mechanism called the Priority Attention Mixer and Mamba - a novel theory that has recently gained considerable attention within the research community. Using Mamba for vision-based feature extraction in classification tasks, particularly in few-shot learning is an innovative approach, and it has shown promising results in improving the accuracy of bearing fault diagnosis. When we tested our model on the datasets provided by Case Western Reserve University (CWRU) and the Paderborn University (PU) Bearing Dataset, we compared it with previously published models. Our proposed approach demonstrated a significant improvement in diagnostic accuracy and clearly outperformed existing approaches. Our code will be available at: https://github.com/linhthan216/MixMamba-Fewshot .},
  archive      = {J_APIN},
  author       = {Than, Nhu-Linh and Nguyen, Van Quang and Truong, Gia-Bao and Pham, Van-Truong and Tran, Thi-Thao},
  doi          = {10.1007/s10489-025-06361-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Mixmamba-fewshot: Mamba and attention mixer-based method with few-shot learning for bearing fault diagnosis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HiProIBM: Unsupervised continual learning through
hierarchical prototypical cross-level discrimination along with
information bottleneck subnetwork masking. <em>APIN</em>,
<em>55</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s10489-025-06362-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic Forgetting (CF) occurs when a machine learning model forgets the experience of previous tasks while learning new tasks due to inadequate retention mechanisms. Unsupervised continual learning (UCL) addresses this by enabling the model to adapt to new tasks using unlabeled data while retaining past knowledge. To mitigate CF in UCL, we use a parameter isolation technique to mask sub-networks dedicated to each task, thus preventing interference with previous tasks. However, relying solely on weight magnitude for constructing these sub-networks can result in the retention of irrelevant weights and the creation of redundant sub-networks. This approach also risks capacity saturation and information suppression for tasks encountered later in the sequence. To overcome this, we use masked sub-networks, inspired by the information bottleneck (IB) concept. It accumulates valuable information into essential weights to construct redundancy-free sub-networks which effectively mitigates CF and enables the new task training. The IB subnetwork masking faces challenges in balancing input compression with meaningful pattern preservation without labels. It risks overcompression and loss of crucial latent structures, which degrades model performance. We address this by learning multiple semantic hierarchies present in the data using unsupervised contrastive learning. However traditional contrastive learning techniques learn meaningful representations by contrasting similar and dissimilar data points. These approaches lack adequate representational power for modeling datasets with multiple semantic hierarchies. The inherent hierarchical semantic structures in datasets are necessary to integrate semantically related clusters into larger, coarser-grained clusters, but existing contrastive learning methods often overlook this and limit semantic understanding. We address this by constructing and updating hierarchical prototypes with cross-level group discrimination to represent semantic structures in the latent space. Our experiments on four standard datasets show performance improvements over SOTA baselines for varying task-sequences from 5 to 100, with nearly-zero forgetting.},
  archive      = {J_APIN},
  author       = {Malviya, Ankit and Kumar Maurya, Chandresh},
  doi          = {10.1007/s10489-025-06362-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {HiProIBM: Unsupervised continual learning through hierarchical prototypical cross-level discrimination along with information bottleneck subnetwork masking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting gas flow rates of wellhead chokes based on a
cascade forwards neural network with a historically limited penetrable
visibility graph. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06365-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel hybrid model that combines the cascade forward neural network (CFNN) with a historical limited penetrable visibility graph (HLPVG) for accurate prediction of gas flow rates through wellhead chokes in shale gas production. The model addresses the challenges of complex, nonlinear relationships between multiple variables affecting gas flow, including liquid–gas ratio (LGR), upstream pressure, temperature, and choke bean size. Using 11,572 field production samples from shale gas fields in the southern Sichuan Basin, the CFNN-HLPVG model demonstrates superior predictive performance compared to the conventional methods. The HLPVG algorithm transforms time series data into a graph structure, enabling the extraction of rich temporal and topological features, whereas the CFNN captures the complex interactions between variables. The model achieves a mean absolute relative error (MARE) of 0.014, significantly outperforming traditional approaches, including the Gilbert-type correlation, support vector machine, and other neural network architectures. Sobol sensitivity analysis revealed that choke bean size has the greatest impact on gas flow prediction (37.7% first-order sensitivity), followed by upstream pressure (19.3%) and temperature (11.6%), whereas LGR has a minimal influence (0.6%). The model performs particularly well under normal operating conditions but shows decreased accuracy in extreme environments with high temperature and pressure. This research provides a novel approach to gas flow prediction in wellhead chokes, offering valuable insights for optimizing shale gas production operations while highlighting areas for future improvement in handling extreme conditions and multisource data integration.},
  archive      = {J_APIN},
  author       = {Jiang, Youshi and Hu, Jingkai and Chen, Xiyu and Mo, Weiren},
  doi          = {10.1007/s10489-025-06365-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Predicting gas flow rates of wellhead chokes based on a cascade forwards neural network with a historically limited penetrable visibility graph},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-supervised anomalous machine sound detection model
based on spectrogram decomposition and parallel sub-network.
<em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06366-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalous Sound Detection (ASD) has research significance and application prospect industrial automation. Most existing models of ASD have limited ability to effectively utilize machine sound features, leading to reduced stability against sound anomalies and domain shift variations. To address the above issues, we propose a self-supervised ASD model based on spectrogram decomposition and parallel sub-network in this paper. Firstly, we decompose the spectrogram along the time and frequency dimensions to balance feature size and information integrity. This approach emphasizes the temporal and frequency variations in the feature map, facilitating a better understanding of the factors that affect machine sounds under domain shift conditions. Secondly, we design a pair of parallel training sub-networks. The parallel sub-networks employ self-attention mechanisms and shared gradients to effectively capture changes in features across both time and frequency dimensions. This approach improves model stability against anomalies and domain shifts. Finally, the anomaly scores of sub-network branches are fused as anomalous detection results. The performance of the proposed model is validated on DCASE2022 Task2 dataset. The Area under the Receiver Operating Characteristic Curve (AUC) and partial AUC (pAUC) of our model reached 72.89% and 64.83%. The results confirm the effectiveness of the proposed model, achieving better performance.},
  archive      = {J_APIN},
  author       = {Zhang, Tao and Kong, Lingguo and Zhao, Xin and Li, Donglei and Geng, Yanzhang and Ding, Biyun and Wang, Chao},
  doi          = {10.1007/s10489-025-06366-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A self-supervised anomalous machine sound detection model based on spectrogram decomposition and parallel sub-network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of deep non-smooth symmetric nonnegative matrix
factorization on hierarchical clustering. <em>APIN</em>, <em>55</em>(6),
1–16. (<a href="https://doi.org/10.1007/s10489-025-06367-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep matrix factorization (deep MF) is an increasingly popular unsupervised data-mining technique that operates as a deep decomposition rooted in traditional nonnegative matrix factorization (NMF). Compared with standard NMF, deep MF has shown excellent performance in the extraction of hierarchical information from complex datasets. For cases in which the data matrices corresponding to the dataset are symmetric—such as the adjacency matrix of an undirected graph in network analysis—this paper proposes a deep MF variant called deep non-smooth nonnegative symmetric matrix factorization (DNSSNMF). The aim of this work is to enhance the extraction of complex hierarchical structures in high-dimensional datasets and achieve the clustering of structures inherent in graphical representations by improving the goodness-of-fit of the factor matrix product. Accordingly, we successfully applied DNSSNMF to post-traumatic-stress-disorder (PTSD) datasets and synthetic datasets to extract several hierarchical communities. In particular, we extracted non-disjoint communities in the partial correlation network of psychiatric symptoms in PTSD, revealing correlations between different symptoms and leading to meaningful clinical interpretations. The results of our numerical experiments indicated promising applications of DNSSNMF in fields including network analysis and medicine.},
  archive      = {J_APIN},
  author       = {Li, Shunli and Lu, Linzhang and Liu, Qilong and Chen, Zhen},
  doi          = {10.1007/s10489-025-06367-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Analysis of deep non-smooth symmetric nonnegative matrix factorization on hierarchical clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composed image retrieval: A survey on recent research and
development. <em>APIN</em>, <em>55</em>(6), 1–35. (<a
href="https://doi.org/10.1007/s10489-025-06372-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, composed image retrieval (CIR) has gained significant attention within the research community due to its excellent research value and extensive real-world applications. CIR allows modifying query images based on user-provided text descriptions, producing search results that better match users’ intent. This paper conducts a comprehensive and up-to-date survey of CIR research and its applications. We summarise recent advancements in CIR methodologies from these perspectives by breaking down a CIR system into four key processes-feature extraction, feature alignment, feature fusion, and image retrieval. We examine feature extraction, emphasizing deep learning techniques for images and text. As deep learning evolves, feature alignment increasingly integrates with other processes, encouraging us to categorize related methods into explicit and implicit approaches. From the perspective of feature fusion, we investigate advancements in image-text feature fusion techniques, categorizing them into 6 broad categories and 17 subcategories. We also summarize different architecture types and training loss functions for image retrieval. Additionally, we review standard benchmark datasets and evaluation metrics in CIR, presenting a comparative analysis of the accuracy of crucial CIR approaches. Finally, we put forward several critical yet underexplored issues in the field.},
  archive      = {J_APIN},
  author       = {Wan, Yongquan and Zou, Guobing and Zhang, Bofeng},
  doi          = {10.1007/s10489-025-06372-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Appl. Intell.},
  title        = {Composed image retrieval: A survey on recent research and development},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anchoring actions using conditional behavior trees and
genetic programming. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06373-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic Kitting means the creation of parts assortment to be used later. These parts are selected from one or more containers in which there are different types of them randomly distributed. The Anchoring Problem should be considered if we want to provide a general solution to robotic kitting, since users want that it works with different types of parts that are not known ’a priori’. Therefore, we are working on a human supervised approach in which Behavior Trees, robot learning and human-robot interaction are used to anchor percepts and operations to symbols during commissioning or reconfiguration phases. In this paper we explain: (1) the anchoring mechanisms in our system and how behavior trees can be used to represent an anchor, and (2) how Genetic Programming is used to generate Conditional Behavior Trees that anchor symbolic actions to robot operations.},
  archive      = {J_APIN},
  author       = {Escudero-Rodrigo, Diego and Alquezar, René and Aranda, Joan},
  doi          = {10.1007/s10489-025-06373-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Anchoring actions using conditional behavior trees and genetic programming},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADIMPL: A dynamic, real-time and robustness attack detection
model for industrial cyber-physical systems based on improved meta
pseudo labels. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06374-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the introduction of networking has increased the efficiency of Industrial Cyber-Physical Systems (ICPS), it has also lowered the cost for attackers, significantly increasing security risks. Current research on ICPS attack detection focuses on deep learning methods. However, the dependence on large labeled datasets often hinders these systems from adapting quickly to the dynamic changes and real-time demands of the ICPS environment. To address these issues, we present an attack detection method based on improved meta pseudo label (ADIMPL). ADIMPL innovatively combines two-layer network traffic feature extraction with the compact SqueezeNet deep neural network, achieving high performance with a minimal number of labeled samples. Additionally, the method dynamically adapts to changing attack patterns, significantly increasing detection accuracy while enhancing the robustness and real-time processing capabilities of the detection system. Extensive experiments on real-world industrial CPS datasets (CIC-IDS2017, CIC-IDS2018, and the CIC-Attack Dataset 2023) demonstrate that ADIMPL can effectively, robustly, and in real-time detect network attacks against industrial CPS. Notably, ADIMPL achieves a detection accuracy of 99.13% with an average latency of 0.098 s and maintains a minimum attack detection accuracy of 91.99% even under our proposed GAN+OPSO malicious attacks.},
  archive      = {J_APIN},
  author       = {Zhang, Bohan and Zhang, Pan and Wang, Zhiwen and Lv, Jiaqi and Miao, Wei},
  doi          = {10.1007/s10489-025-06374-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {ADIMPL: A dynamic, real-time and robustness attack detection model for industrial cyber-physical systems based on improved meta pseudo labels},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STGFP: Information enhanced spatio-temporal graph neural
network for traffic flow prediction. <em>APIN</em>, <em>55</em>(6),
1–21. (<a href="https://doi.org/10.1007/s10489-025-06377-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is crucial for the development of intelligent transportation systems aimed at preventing and mitigating traffic issues. We present an information-enhanced spatio-temporal graph neural network model to predict traffic flow, addressing the inefficient utilization of non-Euclidean structured traffic data. Firstly, we employ a multivariate temporal attention mechanism to capture dynamic temporal correlations across different time intervals, while a second-order graph attention network identifies spatial correlations within the network. Secondly, we construct two types of traffic topology graphs that comprehensively describe traffic flow features by integrating non-Euclidean traffic flow data, regional traffic status information, and node features. Finally, a multi-graph convolution neural network is designed to extract long-range spatial features from these traffic topology graphs. The spatio-temporal feature extraction module then combines these long-range spatial features with spatio-temporal features to fuse multiple features and improve prediction accuracy. Experimental results demonstrate that the proposed approach outperforms state-of-the-art baseline methods in predicting traffic flow performance.},
  archive      = {J_APIN},
  author       = {Li, Qi and Wang, Fan and Wang, Chen},
  doi          = {10.1007/s10489-025-06377-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {STGFP: Information enhanced spatio-temporal graph neural network for traffic flow prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-local modeling of enhancer-promoter interactions, a
correspondence on “LOCO-EPI: Leave-one-chromosome-out (LOCO) as a
benchmarking paradigm for deep learning based prediction of
enhancer-promoter interactions.” <em>APIN</em>, <em>55</em>(6), 1–5. (<a
href="https://doi.org/10.1007/s10489-025-06378-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent paper by Tahir et al. (Appl Intell 55:71, 2024) in Applied Intelligence reported a computational model of enhancer promoter interactions without realizing that many of their conclusions were previously published in 2018. In addition to correcting this record, the authors appear to be unaware of an additional body of previous work on enhancer-promoter interactions, which can explain why their computational model performs poorly. We describe how the weak predictive power of their model is consistent with new insights gained from substantial recent progress in the area of detecting and modeling enhancer promoter interactions constrained by DNA looping, extrusion by cohesin, and CTCF.},
  archive      = {J_APIN},
  author       = {Beer, Michael A.},
  doi          = {10.1007/s10489-025-06378-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-5},
  shortjournal = {Appl. Intell.},
  title        = {Non-local modeling of enhancer-promoter interactions, a correspondence on “LOCO-EPI: Leave-one-chromosome-out (LOCO) as a benchmarking paradigm for deep learning based prediction of enhancer-promoter interactions”},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). A local generation-mix cascade network for image
translation with limited data. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06379-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image translation based on deep generative models often overfits with limited data. Current methods overcome this problem through mix-based data augmentation. However, if latent features are mixed without considering semantic correspondences, augmented samples may exhibit visible artifacts and mislead model training. In this paper, we propose a Local Generation-Mix Cascade Network (LogMix), a data augmentation strategy for image translation tasks with limited data. Through cascading a local feature generation module and mixing module, LogMix enables the generation of a reference feature bank, which is mixed with the most similar local representation to form a new intermediate sample. Furthermore, we design a semantic relationship loss based on the mixed distance of latent features ensures consistency in the distribution of features between the generated and source domains. LogMix effectively mitigates the overfitting problem by learning to translate intermediate samples instead of memorizing the training data Experimental results across various tasks demonstrate that, even with limited data, LogMix data augmentation reduces image ambiguity and offers significant advantages in establishing realistic cross-domain mappings.},
  archive      = {J_APIN},
  author       = {Zhang, Yusen and Li, Min and Gou, Yao and Zhang, Xianjie and He, Yujie},
  doi          = {10.1007/s10489-025-06379-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {A local generation-mix cascade network for image translation with limited data},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel fuzzy knowledge graph structure for decision making
of multimodal big data. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06381-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making in the era of big data is always a challenge. Recently, various methods especially graph sampling have been presented to assist the decision more effectively. As real-world graphs are large, constantly evolving, and distributed in nature, it becomes necessary to sample their structures for many different goals. Therefore, acquiring a comprehensive and in-depth understanding of graph sampling is essential to strengthen this field. In addition, graph sampling techniques often rely on edge or vertex sampling without effective methods for rule or path sampling. In this paper, we propose a novel framework for the rule-based sampling method on fuzzy knowledge graphs. In this framework, fuzzy knowledge graphs are built on integrated databases from multiple sources. We design a purposive random sampling method based on fuzzy rules on graphs to prioritize important rules for output inference. The remaining important rules form the core structure of the fuzzy knowledge graph, known as the Fuzzy Knowledge Graph Structure (FKGS). This structure is considered as a compression mechanism to reduce computational complexity when representing and performing calculations for large-scale data problems. Experimental results based on benchmark datasets on diabetes mellitus show that the sampling method greatly reduces the calculation time while maintaining high accuracy. Moreover, the purposive random sampling method results in significantly higher accuracy than the random sampling method. Besides, the ANOVA method is also conducted to statistically validate the model. The results are significant for decision-making in the context of big data.},
  archive      = {J_APIN},
  author       = {Tan, Nguyen Hong and Long, Cu Kim and Tuan, Tran Manh and Chuan, Pham Minh and Hai, Pham Van and Khanh, Phan Hung and Son, Le Hoang},
  doi          = {10.1007/s10489-025-06381-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {A novel fuzzy knowledge graph structure for decision making of multimodal big data},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instructed fine-tuning based on semantic consistency
constraint for deep multi-view stereo. <em>APIN</em>, <em>55</em>(6),
1–25. (<a href="https://doi.org/10.1007/s10489-025-06382-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing depth map-based multi-view stereo (MVS) methods typically assume that texture features remain consistent across different viewpoints. However, factors such as lighting changes, occlusions, and weakly textured regions can lead to inconsistent texture features, posing challenges for feature extraction. As a result, relying solely on texture consistency does not always yield high-quality reconstruction results in certain scenarios. In contrast, high-level semantic concepts corresponding to the same objects remain consistent across different viewpoints, which we define as semantic consistency. Since designing and training new MVS networks from scratch is both costly and labor-intensive, we propose fine-tuning existing depth map-based MVS networks during testing phase by incorporating semantic consistency constraints to improve the reconstruction quality in regions with poor results. Considering the robust open-set detection and zero-shot segmentation capabilities of Grounded-SAM, we first use Grounded-SAM to generate semantic segmentation masks for arbitrary objects in multi-view images based on text instructions. These masks are then used to fine-tune pre-trained MVS networks via aligning them from different viewpoints to the reference viewpoint and optimizing the depth maps based on the proposed semantic consistency loss function. Our method is designed as a test-time approach that is adaptable to a wide range of depth map-based MVS networks, requiring only adjustments to a small number of depth-related parameters. Comprehensive experimental evaluation across different MVS networks and large-scale scenarios demonstrates that our method effectively enhances reconstruction quality at a lower computational cost.},
  archive      = {J_APIN},
  author       = {Zhang, Yan and Yan, Hongping and Ding, Kun and Cai, Tingting and Zhou, Yueyue},
  doi          = {10.1007/s10489-025-06382-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {Instructed fine-tuning based on semantic consistency constraint for deep multi-view stereo},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DropMismatch: Removing mismatched UI elements for better
pixel to code generation. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06384-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating the generation of user interface (UI) code from design images has gained significant attention due to its potential to streamline application development. However, the effectiveness of deep learning models in this domain is often hindered by mismatches between UI images and their corresponding layout code, a common issue in image-text datasets. In this paper, we introduce a framework that locates and removes these mismatches, thereby improving the accuracy of UI code generation models. Our approach leverages a convolutional neural network to predict the alignment between UI components and layout code nodes, coupled with a tree-based heuristic algorithm to localize mismatches. Through extensive evaluation, we demonstrate that our method enhances the accuracy of UI code generation by approximately 15%, while significantly reducing the need for costly manual annotations. The proposed framework not only advances the state of automated UI code generation but also lays the foundation for creating high-quality, large-scale UI datasets, essential for future research and development in this field.},
  archive      = {J_APIN},
  author       = {Li, Ming and Lin, Tao},
  doi          = {10.1007/s10489-025-06384-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {DropMismatch: Removing mismatched UI elements for better pixel to code generation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NVS-former: A more efficient medical image segmentation
model. <em>APIN</em>, <em>55</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s10489-025-06387-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current field of medical image segmentation research, numerous Transformer-based segmentation models have emerged. However, these models often suffer from limitations in multi-scale feature extraction and struggle to capture local detail features and contextual information, thereby constraining their segmentation performance. This paper introduces a novel model for medical image segmentation, called NVS-Former, which comprises both an encoder and a decoder. The key innovation of NVS-Former lies in its redesigned core module during the encoding phase, which not only enhances feature extraction capabilities but also improves the capture of local detail information. Additionally, the decoder structure has been reengineered to further optimize the model’s class prediction abilities. NVS-Former has demonstrated superior performance in tasks involving multi-organ, pulmonary detail, and cell segmentation. In various comparative experiments, it consistently outperformed state-of-the-art methods, highlighting its efficiency and stability in medical image segmentation.},
  archive      = {J_APIN},
  author       = {Huang, Xiangdong and Huang, Junxia and Ibrahim, Noor Farizah},
  doi          = {10.1007/s10489-025-06387-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Appl. Intell.},
  title        = {NVS-former: A more efficient medical image segmentation model},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for solving bias in graph-based recommender
systems with a causal perspective. <em>APIN</em>, <em>55</em>(6), 1–21.
(<a href="https://doi.org/10.1007/s10489-025-06388-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems founded on graph neural networks (GNN) have been extensively employed because of their exceptional recommendation efficiency. Nevertheless, numerous recommendation biases also crop up, We have observed that delicate details such as gender and age are frequently implicitly apprehended by recommendation systems, culminating in unfair recommendations, and the associated algorithms of GNN will magnify this bias. To tackle these difficulties, this paper puts forth a method of introducing the notion of causal fairness into the issue of fairness in GNN-based recommendation systems, to accomplish counterfactual fairness of user-sensitive information and thereby attain unbiased recommendations. Specifically, given a GNN-based recommendation system model, which is implemented in our devised fairness framework, chiefly obtaining equitable effects through two facets: (1) attaining user embedding fairness through the counterfactual fairness technique; (2) mitigating the prejudiced impact caused by the GNN algorithm using the proposed central association subgraph method. The amalgamation of these two facets ultimately delivers unbiased recommendations. The effectiveness and sophistication of our proposed method for mitigating partiality problems in GNN recommendation systems from a causal perspective (MGRC) have been proven via experiments on four real-world datasets.},
  archive      = {J_APIN},
  author       = {Yang, Kewu and Li, Guogang and Wang, Linjia and Xie, Jianrong},
  doi          = {10.1007/s10489-025-06388-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {A framework for solving bias in graph-based recommender systems with a causal perspective},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAG-unet: Multi-task dense scene understanding with
pixel-attention-guided unet. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06389-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task dense scene understanding is a fundamental research area in computer vision (CV). By predicting pixels, perceiving, and reasoning about multiple related tasks, it improves both accuracy and data efficiency. However, it faces the challenge that some tasks may require more independent feature representations, and excessive sharing can lead to interference between tasks. To address this issue, we propose a novel Pixel-Attention-Guided Unet (PAG-Unet). PAG-Unet incorporates a Pixel-Attention-Guided Fusion module (PAG Fusion) and a Multi-Task Self-Attention module (MTSA) to enhance task-specific feature extraction and reduce task interference. PAG Fusion leverages the relationship between shallow and deep features by using task-specific deep features to calibrate the distribution of shared shallow features. This suppresses background noise and enhances semantic features, thereby fully extracting task-specific features for different tasks and achieving feature enhancement. MTSA considers both global and local spatial interactions for each task during task interactions, capturing task-specific information and compensating for the loss of crucial details, thus improving prediction accuracy for each task. Our method achieves superior multi-task performance on the New York University Depth v2(NYUD-v2) and PASCAL Visual Object Classes Context(PASCAL-Context) datasets, with most metrics significantly outperforming previous state-of-the-art methods. The code is available at https://github.com/UPLI-123/Pag-Unet .},
  archive      = {J_APIN},
  author       = {Xu, Yi and Li, Changhao},
  doi          = {10.1007/s10489-025-06389-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {PAG-unet: Multi-task dense scene understanding with pixel-attention-guided unet},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting hemodynamic parameters based on arterial blood
pressure waveform using self-supervised learning and fine-tuning.
<em>APIN</em>, <em>55</em>(6), 1–26. (<a
href="https://doi.org/10.1007/s10489-025-06391-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arterial blood pressure waveform (ABPW) serves as a less invasive technique for evaluating hemodynamic parameters, offering a lower risk compared to the more invasive pulmonary artery catheter (PAC) thermodilution method. Various studies suggest that deep learning models can potentially predict the hemodynamic parameters of ABPW. However, the scarcity of ground truth data restricts the accuracy of these models, preventing them from gaining clinical acceptance. To mitigate this data and domain challenge, this work proposed a self-supervised generative learning model for hemodynamic parameter prediction, called SSHemo (Self-Supervised Hemodynamic model). Specifically, SSHemo suggests first to leverage large amounts of unlabeled ABPW data to learn the representative embedding and then to fine-tune for the downstream task with a small amount of hemodynamic parameters’ ground truth. To verify the effectiveness of SSHemo, we utilize the public available VitalDB data set to train the model, and evaluation was conducted on two public datasets: VitalDB and MIMIC. The experimental results reveal that SSHemo’s regression mean absolute error (MAE) improved significantly from 1.63 L/min to 1.25 L/min when predicting cardiac output (CO). The trending tracking ability for CO changes meets clinical acceptance (radial limit of agreement (LOA) is $$\pm 25.56$$ °, less than $$\pm 30$$ °). In addition, SSHemo demonstrates robust stability in various conditions and cohorts, as evidenced by subgroup analysis, varying systemic vascular resistance (SVR) range analysis, and rapid CO analysis, compared to the most widely used commercial devices, the EV1000. Computational analysis further underscores the value and potential of practical application of the model in various settings.},
  archive      = {J_APIN},
  author       = {Liao, Ke and Elibol, Armagan and Gao, Ziyan and Meng, Lingzhong and Chong, Nak Young},
  doi          = {10.1007/s10489-025-06391-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {Appl. Intell.},
  title        = {Predicting hemodynamic parameters based on arterial blood pressure waveform using self-supervised learning and fine-tuning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale contrastive learning via aggregated subgraph for
link prediction. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06394-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction seeks to uncover potential or future connections within a network using structural or attribute information. Recently, Graph Neural Network (GNN)-based methods have attracted considerable attention for their effectiveness in link prediction. However, most GNN-based approaches focus solely on single-scale input graphs, which limits their ability to comprehensively capture network structure information. In this paper, multi-scale subgraphs are introduced as input graphs to obtain complementary network structures from different perspectives. Simultaneously, to obtain embedding vectors with better representational capacity, contrastive loss from self-supervised learning is incorporated for link prediction. Specifically, Multi-scale Contrastive learning framework based on Aggregated Subgraph (MCAS) is proposed for predicting missing links. Firstly, we construct enclosing subgraph by extracting neighbors of target nodes. By applying aggregation operation to these subgraphs, different granularities of multi-scale subgraphs are obtained. Secondly, encoders are used to learn information from multiple scales of subgraphs separately. Next, contrastive learning is employed to achieve information balance among the multi-scale subgraphs. Finally, the minimization of the loss allows us to improve the model’s robustness. Empirical evidence indicates that our approach excels state-of-the-art methods on nine datasets, including biological and citation networks. All source code is publicly available at: https://github.com/yabingyao/MCAS4LinkPrediction .},
  archive      = {J_APIN},
  author       = {Yao, Yabing and Guo, Pingxia and Mao, Zhiheng and Ti, Ziyu and He, Yangyang and Nian, Fuzhong and Zhang, Ruisheng and Ma, Ning},
  doi          = {10.1007/s10489-025-06394-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale contrastive learning via aggregated subgraph for link prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSFL: A blockchain-based data sharing and federated learning
framework. <em>APIN</em>, <em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06400-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive amount of data generated by the proliferation of Internet of Things (IoT) devices has become one of the key factors driving the advancement of artificial intelligence (AI) technology. However, the lack of storage space and limited computational power of edge devices make it difficult to directly process large data volumes or run complex machine learning algorithms on these devices. At the same time, existing Federated Learning (FL) schemes still face a number of shortcomings, including a single point of failure, vulnerability to poisoning attacks, and a lack of incentives. To address the above issues, we propose DSFL, a blockchain-based framework for fair data sharing and FL. Specifically, we combine digital envelope technology and one-way accumulator with smart contracts to design fair, secure, and trustworthy data sharing protocols that facilitate edge devices to share data proactively, realize the value of data and reduce storage pressure. In addition, we propose blockchain extension schemes suitable for coupling with FL to improve training efficiency. Importantly, the node management mechanism and incentive algorithms are designed to effectively monitor and trace the behavior of nodes, and promote the virtuous cycle of model training and the motivation of participants. Experimental results show that DSFL is able to ensure fair data sharing and efficient model training without the involvement of trusted third parties. In particular, it is able to achieve model accuracy close to that of existing popular schemes even when 40% of the nodes are lazy, providing an excellent defense against malicious nodes.},
  archive      = {J_APIN},
  author       = {Niu, Haiqian and Zhang, Xing and Chu, Zhiguang and Shi, Wei},
  doi          = {10.1007/s10489-025-06400-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {DSFL: A blockchain-based data sharing and federated learning framework},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROCIP: Robust continuous inertial position tracking for
complex actions emerging from the interaction of human actors and
environment. <em>APIN</em>, <em>55</em>(6), 1–10. (<a
href="https://doi.org/10.1007/s10489-025-06409-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inertial navigation is advancing rapidly due to improvements in sensor technology and tracking algorithms, with consumer-grade inertial measurement units (IMUs) becoming increasingly compact and affordable. Despite progress in pedestrian dead reckoning (PDR), IMU-based positional tracking still faces significant noise and bias issues. While traditional model-based methods and recent machine learning approaches have been employed to reduce signal drift, error accumulation remains a barrier to long-term system performance. Inertial tracking’s self-contained nature offers broad applicability but limits integration with a global reference frame. To solve this problem, a system that could “introspect its error” and “learn from the past” is proposed. It consists of a neural statistical motion model that regresses both poses and uncertainties with DenseNet, which are then fed into Rao-Blackwellised particle filter (RBPF) for calibration with a probabilistic transition map. An inertial tracking dataset with head-mounted IMUs was collected, including walking and running with different speeds while allowing participants to rotate their heads in a self-selected manner. The dataset consisted of 19 volunteers that generated 151 sequences in 4 scenarios with a total time of 929.8 min. It was shown that our proposed method (ROCIP) outperformed the leading methods in the field, with a relative trajectory error (RTE) of 4.94m and absolute trajectory error (ATE) of 4.36m. ROCIP could also solve the problem of error accumulation in dead reckoning and maintain a small and consistent error during long-term tracking.},
  archive      = {J_APIN},
  author       = {Hou, Xinyu and Bergmann, Jeroen},
  doi          = {10.1007/s10489-025-06409-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-10},
  shortjournal = {Appl. Intell.},
  title        = {ROCIP: Robust continuous inertial position tracking for complex actions emerging from the interaction of human actors and environment},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A statistical categorization-based curriculum learning
approach for multi-task classification of images. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06270-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification and the detection of features within images remain significant challenges in computer vision. Several approaches, including serial task models and multi-output models, have been explored to address these challenges. This study focuses on multitasking attention mechanisms, which enable simultaneous categorization of data and tasks. By applying a statistical framework, the proposed method enhances the efficiency and accuracy of image classification and feature detection, with a focus on handling multiple tasks concurrently. To enhance the robustness of the model, a data-driven approach based on curriculum learning was proposed. The experiments were conducted using two distinct datasets. The first dataset involves forensic examinations, specifically identifying firearms and their calibers from firing pin marks. The proposed model achieved an accuracy of 95% in brand detection and 98% in caliber detection on this dataset. In the second part of the experiments, the animals with attributes 2 (AwA2) dataset, where state-of-the-art models have previously been applied, was used. The proposed model reduced classification errors by 1 to 10% compared to traditional convolutional neural network (CNN) architectures. The experimental results from both the forensic and public datasets demonstrate that the proposed model effectively handles multitask classification tasks, validating its applicability across diverse domains.},
  archive      = {J_APIN},
  author       = {Veranyurt, Ozan and Sakar, C. Okan},
  doi          = {10.1007/s10489-025-06270-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A statistical categorization-based curriculum learning approach for multi-task classification of images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial network embedding with bootstrapped
representations for sparse networks. <em>APIN</em>, <em>55</em>(6),
1–22. (<a href="https://doi.org/10.1007/s10489-025-06343-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent sparsity of real-world networks presents challenges in learning-rich embeddings and accurately reconstructing networks. To address these challenges, a novel method termed Adversarial Network Embedding with Bootstrapped Representations (ANEBR) is proposed. Firstly, a novel network augmentation method is employed for positive sampling. ANEBR utilizes the Katz Index to extract higher-order latent information and refines it with $$\alpha $$ -entmax. The crucial information is extracted while minimizing noise generation. Secondly, ANEBR circumvents negative sampling by learning bootstrapped representations. Building on bootstrapped representations from the BYOL algorithm, ANEBR incorporates the GAN techniques to align the learned embeddings nonlinearly. Finally, ANEBR attains accurate network reconstruction by imposing a low-rank constraint on the reconstruction error through the nuclear norm. Extensive experiments with statistical and sensitivity analyses demonstrate that ANEBR outperforms state-of-the-art methods in various tasks. Specifically, ANEBR reconstructs the PPI network with a precision of 0.9992, marking a relative improvement of 6.65%. Code is available at https://github.com/wuzelong/ANEBR .},
  archive      = {J_APIN},
  author       = {Wu, Zelong and Wang, Yidan and Lin, Guoliang and Liu, Junlong},
  doi          = {10.1007/s10489-025-06343-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Adversarial network embedding with bootstrapped representations for sparse networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust hierarchical clustering algorithm for automatic
identification of clusters. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06376-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggregation-based hierarchical clustering algorithms are widely used in data analysis due to their robust clustering performance. Although some existing hierarchical clustering methods can identify the number of clusters in a dataset, most are only effective for well-separated clusters and struggle to identify the number of clusters in complex datasets, particularly non-convex noisy datasets. To address these shortcomings, this paper proposes a robust hierarchical clustering algorithm for automatic identification of clusters(RHCAIC), which can identify the optimal number of clusters while providing reliable clustering results. To reduce the impact of noise in clustering, the method first calculates reverse density and designs a dynamic noise discriminator to denoise the dataset. Based on the fact that more similar points have a higher probability of being clustered into the same cluster among multiple results of hierarchical clustering, a robust solution was designed. After constructing a directed graph using the kNN algorithm, the graph merging process is performed by iteratively traversing the directed edges. During this process, the number of clusters is identified, and the clustering results of the denoised dataset are obtained. Finally, by incorporating density information into the noise clustering, the final clustering results are obtained. A series of experiments conducted on 12 synthetic datasets and 8 real datasets demonstrate that, compared to seven other benchmark algorithms, the RHCAIC algorithm not only accurately identifies the number of clusters in the dataset but also produces better clustering results.},
  archive      = {J_APIN},
  author       = {Long, Jianwu and Wang, Qiang and Liu, Luping},
  doi          = {10.1007/s10489-025-06376-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A robust hierarchical clustering algorithm for automatic identification of clusters},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating potential causes of sepsis with bayesian
network structure learning. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06405-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is a life-threatening and serious global health issue. This study combines knowledge with available hospital data to investigate the potential causes of Sepsis that can be affected by policy decisions. We investigate the underlying causal structure of this problem by combining clinical expertise with score-based, constraint-based, and hybrid structure learning algorithms. A novel approach to model averaging and knowledge-based constraints was implemented to arrive at a consensus structure for causal inference. The structure learning process highlighted the importance of exploring data-driven approaches alongside clinical expertise. This includes discovering unexpected, although reasonable, relationships from a clinical perspective. Hypothetical interventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and Diabetes suggest that the presence of any of these risk factors in patients increases the likelihood of Sepsis. This finding, alongside measuring the effect of these risk factors on Sepsis, has potential policy implications. Recognising the importance of prediction in improving health outcomes related to Sepsis, the model is also assessed in its ability to predict Sepsis by evaluating accuracy, sensitivity, and specificity. These three indicators all had results around 70%, and the AUC was 80%, which means the causal structure of the model is reasonably accurate given that the models were trained on data available for commissioning purposes only.},
  archive      = {J_APIN},
  author       = {Petrungaro, Bruno and Kitson, Neville K. and Constantinou, Anthony C.},
  doi          = {10.1007/s10489-025-06405-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Investigating potential causes of sepsis with bayesian network structure learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-channel graph-level anomaly detection method based on
multi-graph representation learning. <em>APIN</em>, <em>55</em>(6),
1–16. (<a href="https://doi.org/10.1007/s10489-024-05852-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-level anomaly detection plays a crucial role in anomaly identification by comparing and classifying the graph-level features of normal and anomalous graphs. Despite advancements, existing methods often suffer from low detection rates and high false-positive rates when dealing with sparse anomalous data. To address this limitation, we propose a dual-channel graph-level anomaly detection model that utilizes two graph isomorphic networks to separately learn from labeled anomalous data and unlabeled normal data. This model enhances the identification of unlabeled anomalies by learning from both types of data through separate channels. Furthermore, to enable the model to be applicable to complex graph types in graph-level anomaly detection applications, we introduce a novel multi-graph representation learning method that can transform multi-graphs into a simplified graph representation. We have rigorously evaluated the proposed model on 6 public datasets, and the experimental results demonstrate the effectiveness of the model, with significant performance improvements over 9 baseline models.},
  archive      = {J_APIN},
  author       = {Jing, Yongjun and Wang, Hao and Chen, Jiale and Chen, Xu},
  doi          = {10.1007/s10489-024-05852-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Dual-channel graph-level anomaly detection method based on multi-graph representation learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D-FaIR: 3D facial imperfection regeneration with defects by
fully convolutional mesh autoencoder. <em>APIN</em>, <em>55</em>(6),
1–15. (<a href="https://doi.org/10.1007/s10489-024-05880-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an effective approach using a fully convolutional mesh autoencoder model to reconstruct 3D facial features in the presence of imperfections. The method accurately simulates facial scars in a virtual environment, adapting to unique situations. This article presents the “Cir3D-FaIR” dataset, which is specifically tailored to address issues related to facial scars. Additionally, we propose a new technique called 3D facial imperfection regeneration (3D-FaIR), which focusses on reconstructing a complete face based on the remaining features of the patient’s face. To further enhance the applicability of this research, the article has developed an advanced outlier detection technique that isolates affected areas and provides appropriate models for wound coverage. The Cir3D-FaIR dataset, consisting of imperfect facial models and open-source package, is available at https://github.com/SIMOGroup/3DFaIR . Our findings demonstrate that the proposed approach can potentially aid in faster and safer patient recovery through convenient methods. We hope that this work inspires the development of new products and innovative solutions for facial scar regeneration.},
  archive      = {J_APIN},
  author       = {Nguyen, Phuong D. and Le, Thinh D. and Nguyen, Duong Q. and Nguyen, Thanh Q. and Chou, Li-Wei and Nguyen-Xuan, H.},
  doi          = {10.1007/s10489-024-05880-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {3D-FaIR: 3D facial imperfection regeneration with defects by fully convolutional mesh autoencoder},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An online self-organizing radial basis function neural
network based on gaussian membership. <em>APIN</em>, <em>55</em>(6),
1–17. (<a href="https://doi.org/10.1007/s10489-024-05989-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radial basis function neural network (RBFNN) is one of the most popular neural networks, and an appropriate selection of its structure and learning algorithms is crucial for its performance. Aiming to alleviate the sensitivity of the RBFNN to its parameters and improve the overall performance of the network, this study proposes a Gaussian Membership-based online self-organizing RBF neural network (GM-OSRBFNN). First, the Gaussian Membership is introduced to enhance network insensitivity to network parameters and used as a similarity metric to indicate the similarity between the sample to a hidden neuron and that between hidden neurons. Second, the similarity metric is used to design the neuron addition and merging rules to achieve a self-organizing network structure, and error constraints are introduced to the neuron addition rule; also, the noisy neuron deletion rule is defined to make the network structure more compact. In addition, an online fixed mini-batch gradient algorithm is used for online learning of network parameters, which can guarantee fast and stable convergence of the network. Finally, the proposed GM-OSRBFNN is tested on common nonlinear system modeling problems to verify its effectiveness. The experimental results show that compared to the existing models, the GM-OSRBFNN can achieve competitive prediction performance with a more compact network structure, faster convergence speed, and, more importantly, better insensitivity to network parameters.},
  archive      = {J_APIN},
  author       = {Jia, Lijie and Li, Wenjing and Qiao, Junfei and Zhang, Xinliang},
  doi          = {10.1007/s10489-024-05989-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {An online self-organizing radial basis function neural network based on gaussian membership},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel recurrent neural network with transformer for
anomalous trajectory detection. <em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-024-06069-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalous trajectory detection within urban road traffic networks is crucial for identifying operational vehicle fraud in intelligent transportation systems. However, most existing approaches are limited to detecting anomalous trajectories solely based on the same original point, neglecting the extraction of spatiotemporal features and contextual information embedded in trajectory data. To address these limitations, a Parallel Recurrent Neural Network with Transformer (PRNNT) model is proposed for anomalous trajectory detection. Specifically, the position embedding and a transformer encoder module are utilized to train trajectory embeddings, allowing the model to learn sequential features and contextual information of trajectories. Moreover, a parallel recurrent neural network is employed to extract hidden trajectory features, capturing the differences between normal and anomalous trajectories. Finally, a linear layer is applied to fuse the spatiotemporal features and output the probability of an anomalous trajectory, enhancing the detection of vehicle trajectory anomalies. Experimental results on Beijing and Porto datasets demonstrate that the proposed PRNNT model significantly outperforms the iBAT (Isolation-Based Anomalous Trajectory), ATDC (Anomalous Trajectory Detection and Classification), ATD-RNN (Anomalous Trajectory Detection using Recurrent Neural Network), XGBoost (Extreme Gradient Boosting), GM-VSAE (Gaussian Mixture Variational Sequence AutoEncoder), and UA-OATD (Deep Unified Attention-based Sequence Modeling for Online Anomalous Trajectory Detection) models, achieving at least a 3.8%, 22.7%, 3.8%, 22.7%, 15%, and 16.7% improvement in F1-score, respectively.},
  archive      = {J_APIN},
  author       = {Xia, Dawen and Li, Yunsong and Ao, Yuce and Wei, Xiaoduo and Chen, Yan and Hu, Yang and Li, Yantao and Li, Huaqing},
  doi          = {10.1007/s10489-024-06069-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Parallel recurrent neural network with transformer for anomalous trajectory detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Implicit regularization of a deep augmented
neural network model for human motion prediction. <em>APIN</em>,
<em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-024-06148-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Yadav, Gaurav Kumar and Abdel-Nasser, Mohamed and Rashwan, Hatem A. and Puig, Domenec and Nandi, G. C.},
  doi          = {10.1007/s10489-024-06148-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Implicit regularization of a deep augmented neural network model for human motion prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based spatial-temporal synchronous graph
convolution networks for traffic flow forecasting. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06341-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow forecasting is crucial for urban traffic control, planning, and detection. Most existing spatial-temporal modeling methods overlook the hidden dynamic correlations between road network nodes and the time series nonstationarity while synchronously capturing complex long- and short-term spatial-temporal dependencies. To this end, this paper proposes an Attention-based Spatial-Temporal Synchronous Graph Convolutional Network (AST-SGCN) to capture complex spatial-temporal correlations over long and short terms. Specifically, we design a self-attention mechanism that utilizes spatial-temporal synchronous computation to efficiently mine dynamic spatial-temporal correlations with changes in traffic and enhance computational efficiency. Then, we construct a residual adaptive adjacency matrix, which includes historical data and node vectors, to stimulate the information transfer of spatial-temporal graph nodes and mine the hidden spatial-temporal dependencies through the graph convolution layer. Next, we establish a Fourier transform layer (FTL) to handle the nonstationary data. Finally, we develop a spatial-temporal hybrid stacking module for capturing complex long-term spatial-temporal correlations, within which two layers of graph convolution and one layer of self-attention are deployed. Extensive experimental results on three real-world traffic flow datasets demonstrate that our AST-SGCN model outperforms the comparable models.},
  archive      = {J_APIN},
  author       = {Wei, Xiaoduo and Xia, Dawen and Li, Yunsong and Ao, Yuce and Chen, Yan and Hu, Yang and Li, Yantao and Li, Huaqing},
  doi          = {10.1007/s10489-025-06341-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Attention-based spatial-temporal synchronous graph convolution networks for traffic flow forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARTS-EAST: An edge-adaptive selection with topology first
differentiable architecture selection method. <em>APIN</em>,
<em>55</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06353-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DARTS+PT is a well-known differentiable neural architecture search (NAS) method that evaluates the contribution of operations to the performance of the super-network, ultimately deriving the final architecture. However, DARTS+PT introduces randomness into the edge discretization process by selecting edges randomly, which leads to performance instability. Moreover, the method assesses the impact of each candidate operation by iteratively removing them and measuring the resulting drop in super-network performance, leading to a high search cost. To address these issues, this paper identifies the root cause of instability and proposes a novel edge selection criterion to establish an adaptive edge discretization order, improving stability. Additionally, we introduce a topology-first discretization scheme that prioritizes topology selection over operation selection, significantly reducing the search cost. We name this approach DARTS-EAST (Edge-Adaptive Selection with Topology-First Differentiable Architecture Selection). Extensive experiments on widely used benchmarks demonstrate that DARTS-EAST not only achieves competitive performance but also offers significant improvements in both stability and search efficiency.},
  archive      = {J_APIN},
  author       = {Fang, Xuwei and Xie, Weisheng and Li, Hui and Zhou, Wenbin and Hang, Chen and Gao, Xiangxiang},
  doi          = {10.1007/s10489-025-06353-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {DARTS-EAST: An edge-adaptive selection with topology first differentiable architecture selection method},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TANGAN: Solving tangram puzzles using generative adversarial
network. <em>APIN</em>, <em>55</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s10489-025-06364-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While humans show remarkable proficiency in solving visual puzzles, machines often fall short due to the complex combinatorial nature of such tasks. Consequently, there is a growing interest in developing computational methods for the automatic solution of different puzzles, especially through deep learning approaches. The Tangram, an ancient Chinese puzzle, challenges players to arrange seven polygonal pieces to construct different patterns. Despite its apparent simplicity, solving the Tangram is considered an NP-complete problem, being a challenge even for the most sophisticated algorithms. Moreover, ensuring the generality and adaptability of machine learning models across different Tangram arrangements and complexities is an ongoing research problem. In this paper, we introduce a generative model specifically designed to solve the Tangram. Our model competes favorably with previous methods regarding accuracy while delivering fast inferences. It incorporates a novel loss function that integrates pixel-based information with geometric features, promoting a deeper understanding of the spatial relationships between pieces. Unlike previous approaches, our model takes advantage of the geometric properties of the Tangram to formulate a solving strategy, exploiting its inherent properties only through exposure to training data rather than through direct instruction. Extending the proposed loss function, we present a novel evaluation metric as a better fitting measure for assessing Tangram solutions than previous metrics. We further provide a new dataset containing more samples than others reported in the literature. Our findings highlight the potential of deep learning approaches in geometric problem domains.},
  archive      = {J_APIN},
  author       = {Yamada, Fernanda Miyuki and Batagelo, Harlen Costa and Gois, João Paulo and Takahashi, Hiroki},
  doi          = {10.1007/s10489-025-06364-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {TANGAN: Solving tangram puzzles using generative adversarial network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Batch process quality prediction based on denoising
autoencoder-spatial temporal convolutional attention mechanism fusion
network. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06368-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In batch processes, the accurate prediction of quality variables plays a crucial role in smooth production and quality control. However, various sources of noise in the production environment cause abnormal data fluctuations that deviate from the real value. Coupled with the dynamic nonlinearity of batch processing and the complex spatiotemporal relationship of variables, which greatly increase the difficulty of prediction and pose a severe challenge to prediction performance. Therefore, a denoising autoencoder-Spatial Temporal Convolution Attention Fusion Network (DAE-STCAFN) prediction method is proposed. Firstly, combining DAE and maximum information coefficient (MIC), multi-level data features are extracted to prepare high-quality input data for the quality prediction model. DAE is used to denoise the original data, and relevant variables are selected through MIC. Then, an augmented matrix is constructed to eliminate the autocorrelation of the selected variables in the time series. Secondly, a spatial temporal convolutional attention fusion mechanism is created to extract the spatial temporal fusion features between the input and output variable sequences. Thirdly, to further enhance the learning ability of the model, a batch attention module is constructed to automatically learn the relationship among sample in small batch. Finally, experiments were carried out on the simulation platform of penicillin fermentation and hot tandem rolling process. In the prediction process of penicillin concentration, RMSE and MAE of the proposed method were 0.0099 and 0.0077, respectively. In the prediction of strip thickness, the RMSE and MAE are 0.0008 and 0.0003 respectively. The results show that the proposed method is effective both in simulation experiment and in actual industrial production in terms of prediction accuracy, stability and generalization ability.},
  archive      = {J_APIN},
  author       = {Zhang, Yan and Cao, Jie and Zhao, Xiaoqiang and Hui, Yongyong},
  doi          = {10.1007/s10489-025-06368-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Batch process quality prediction based on denoising autoencoder-spatial temporal convolutional attention mechanism fusion network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-order consensus graph learning for incomplete
multi-view clustering. <em>APIN</em>, <em>55</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s10489-025-06375-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-View Clustering (IMVC) aims to partition data with missing samples into distinct groups. However, most IMVC methods rarely consider the high-order neighborhood information of samples, which represents complex underlying interactions, and often neglect the weights of different views. To address these issues, we propose a High-order Consensus Graph Learning (HoCGL) model. Specifically, we integrate a reconstruction term to recover the incomplete multi-view data. High-order proximity matrices are constructed, and the self-representation similarity matrices and multiple high-order proximity matrices are learned mutually, allowing the similarity matrices to incorporate complex high-order information. Finally, the consensus graph representation is derived from the similarity matrices through a self-weighted strategy. An efficient algorithm is designed to solve the proposed model. The excellent clustering performance of the proposed model is validated by comparing it with eight state-of-the-art models across nine datasets.},
  archive      = {J_APIN},
  author       = {Guo, Wei and Che, Hangjun and Leung, Man-Fai},
  doi          = {10.1007/s10489-025-06375-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {High-order consensus graph learning for incomplete multi-view clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StackMFF: End-to-end multi-focus image stack fusion network.
<em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06383-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing end-to-end multi-focus image fusion (MFF) networks demonstrate excellent performance when fusing image pairs. However, when image stacks are processed, the necessity for iterative fusion leads to error accumulation, resulting in various types and degrees of image degradation, which ultimately limits the algorithms’ practical applications. To address this challenge and expand the application scenarios of multi-focus fusion algorithms, we propose a relatively simple yet effective approach: utilizing 3D convolutional neural networks to directly model and fuse entire multi-focus image stacks in an end-to-end manner. To obtain large-scale training data, we developed a refocusing pipeline based on monocular depth estimation technology that can synthesize a multi-focus image stack from any all-in-focus image. Furthermore, we extended the attention mechanisms commonly used in image pair fusion networks from two dimensions to three dimensions and proposed a comprehensive loss function group, effectively enhancing the fusion quality. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance in both fusion quality and processing speed while avoiding image degradation issues, establishing a simple yet powerful baseline for the multi-focus image stack fusion task. The codes are available at https://github.com/Xinzhe99/StackMFF .},
  archive      = {J_APIN},
  author       = {Xie, Xinzhe and Qingyan, Jiang and Chen, Dong and Guo, Buyu and Li, Peiliang and Zhou, Sangjun},
  doi          = {10.1007/s10489-025-06383-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {StackMFF: End-to-end multi-focus image stack fusion network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructive sample partition-based parameter-free sampling
for class-overlapped imbalanced data classification. <em>APIN</em>,
<em>55</em>(6), 1–29. (<a
href="https://doi.org/10.1007/s10489-025-06385-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data widely exists in real applications ranging from medical diagnosis to economic fraud detection, etc. Data level method is one of the prevalent methods to deal with imbalanced data by re-balancing the distribution between different classes. Recent researches reveal that handling the class-overlapping of imbalanced data when designing data-level approach can effectively improve the performance of imbalanced learning. However, most existing data-level methods rely on specific parameters to obtain desired performance, making them hard to generalize to other scenarios. And the intractable data difficulty factors, i.e., the most frequent class-overlapping problem, makes them confront additional challenges. Designing efficient, flexible method that considers the parameter-free designing and the class-overlapping handling simultaneously remains a challenge. This paper proposes to deal with the class-overlapped imbalanced data with parameter-free adaptive method. To be specific, we first propose a parameter-free constructive sample partition (CSP) method, and then design an adaptive parameter-free CSP-based undersampling method (CSPUS) and an adaptive parameter-free CSP-based hybrid sampling method (CSPHS) to balance the class distribution by handling the class-overlap of the original data. Numerical experiments on 18 representative high-overlap imbalanced datasets from KEEL repository and 23 state-of-the-art comparison methods demonstrate the effectiveness of CSPUS and CSPHS. The source code of our proposed methods is available at https://github.com/ytyancp/CSPS.},
  archive      = {J_APIN},
  author       = {Wang, Weiqing and Yan, Yuanting and Zhou, Peng and Zhao, Shu and Zhang, Yiwen},
  doi          = {10.1007/s10489-025-06385-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Appl. Intell.},
  title        = {Constructive sample partition-based parameter-free sampling for class-overlapped imbalanced data classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of traditional machine learning algorithms for
featuring educational exercises. <em>APIN</em>, <em>55</em>(6), 1–25.
(<a href="https://doi.org/10.1007/s10489-025-06386-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) algorithms are important in educational environments, and the use of machine learning algorithms to evaluate and improve the quality of education. Previous studies have individually analyzed algorithms to estimate item characteristics, such as grade, number of attempts, and time from student interactions. By contrast, this study integrated all three characteristics to discern the relationships between attempts, time, and performance in educational exercises. We analyzed 15 educational assessments using different machine learning algorithms, specifically 12 for regression and eight for classification, with different hyperparameters. This study used real student interaction data from Zenodo.org, encompassing over 150 interactions per exercise, to predict grades and to improve our understanding of student performance. The results show that, in regression, the Bayesian ridge regression and random forest regression algorithms obtained the best results, and for the classification algorithms, Random Forest and Nearest Neighbors stood out. Most exercises in both scenarios involved more than 150 student interactions. Furthermore, the absence of a pattern in the variables contributes to suboptimal outcomes in some exercises. The information provided makes it more efficient to enhance the design of educational exercises.},
  archive      = {J_APIN},
  author       = {Jiménez-Macías, Alberto and Muñoz-Merino, Pedro J. and Moreno-Marcos, Pedro Manuel and Kloos, Carlos Delgado},
  doi          = {10.1007/s10489-025-06386-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {Evaluation of traditional machine learning algorithms for featuring educational exercises},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of reservoir water levels via an improved
attention mechanism based on CNN − LSTM. <em>APIN</em>, <em>55</em>(6),
1–20. (<a href="https://doi.org/10.1007/s10489-025-06393-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water level prediction is crucial for flood control scheduling and water resource management. The application of various deep learning methods to water level prediction in reservoirs is limited. Accurate water level prediction aids in optimizing reservoir operation strategies, ensuring flood safety downstream and meeting water supply demands. To achieve accurate predictions, a new structure based on a convolutional neural network − long short-term memory (CNN − LSTM) model is proposed, which incorporates a self-attention mechanism and a local attention mechanism in an SL − CNN − LSTM coupled model. Using the Three Gorges Reservoir head area in China as a case study, hydrometeorological data from three points in the reservoir&#39;s head area and upstream water level characteristics are used as input variables. Data collected every six hours from 2008 to 2021 were used, with the model trained and tested at an 8:2 ratio. The study revealed that a two-layer CNN configuration performed best in most models. The SL − CNN − LSTM-2 model achieved the best performance across all the metrics, with an R2 of 0.9988, an MAE of 0.2767, an RMSE of 0.3404, and a MAPE of 0.1717, particularly for extreme water level predictions with minimal residuals, validating its strong ability to balance long- and short-term dependencies. Additionally, the model effectively extracts features and captures critical information in time series data, balancing learning capacity and computational efficiency. The research results are highly important for water resource management in large reservoirs, providing reliable technical support for flood control scheduling and water resource optimization.},
  archive      = {J_APIN},
  author       = {Li, Haoran and Zhang, Lili and Yao, Yunsheng and Zhang, Yaowen},
  doi          = {10.1007/s10489-025-06393-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Prediction of reservoir water levels via an improved attention mechanism based on CNN − LSTM},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A personalized consensus-reaching method for large-group
decision-making in social networks combining self-confidence and trust
relationships. <em>APIN</em>, <em>55</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10489-025-06395-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large-scale group decision-making (LSGDM) in social network environments considering experts’ psychological behaviors has received increasing attention. Moreover, existing studies have shown that whether it is internal self-confidence or external trust relationships of experts, they play a crucial role in reaching consensus. Therefore, this paper integrates self-confidence and trust relationships, and proposes a personalized consensus-reaching method for LSGDM from the perspective of adjustment willingness. Firstly, we explored the promoting effect of opinion similarity on the efficiency of trust propagation and proposed a method to evaluate unknown trust relationships among experts, integrating the objectivity of trust relationships and the subjectivity of self-confidence to determine the experts’ weights. Secondly, a hierarchical fuzzy clustering algorithm based on the chi-square test is proposed for effective subgroup division, which avoids the impact of setting initial clustering parameters on the clustering results. Afterwards, the adjustment willingness of the subgroups is determined by combining the experts’ self-confidence and the trust relationships between them. In addition to this, a personalized consensus feedback adjustment mechanism that synthesizes the adjustment willingness and trust relationship is constructed to reach consensus, which can better preserve the original information. Finally, the effectiveness of the proposed method is verified through a numerical example. In addition, the advantages of the proposed method are demonstrated by comparing with other methods.},
  archive      = {J_APIN},
  author       = {Liu, Zhengmin and Ding, Ruxue and Wang, Wenxin and Liu, Peide and Gao, Shanshan},
  doi          = {10.1007/s10489-025-06395-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {A personalized consensus-reaching method for large-group decision-making in social networks combining self-confidence and trust relationships},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pursuit-evasion game with online planning using deep
reinforcement learning. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06396-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pursuit-evasion with round-up is a problem where multiple pursuers aim to capture a moving target within a specific encirclement to prevent its escape. In this paper, multi-UAV pursuit-evasion algorithms in obstacle environments are investigated and deployed on real-world micro quadcopters. In order to guide pursuers in quickly avoiding obstacles and swiftly approaching the evader, an end-to-end distributed reinforcement learning framework is proposed, and a two-stage reward function is designed. Building upon this, a MADDPG framework based on a trajectory prediction network is constructed to assist pursuers in completing round-up more quickly. Furthermore, unlike most reinforcement learning algorithms, the proposed algorithm is deployed onto a micro quadcopter controller, and a pursuit-evasion game is conducted in a real-world scenario. The results of simulations and physical experiments show that the proposed algorithm can complete round-up more quickly and can be successfully transferred to the real world.},
  archive      = {J_APIN},
  author       = {Chen, Yong and Shi, Yu and Dai, Xunhua and Meng, Qing and Yu, Tao},
  doi          = {10.1007/s10489-025-06396-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Pursuit-evasion game with online planning using deep reinforcement learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The improved mountain gazelle optimizer for spatiotemporal
support vector regression: A novel method for railway subgrade
settlement prediction integrating multi-source information.
<em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06397-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uneven settlement of railway subgrades not only affects the comfort of train operations but, in extreme cases, may compromise operational safety. As a result, accurately predicting subgrade settlement is crucial for maintaining both safety and operational efficiency. This study introduces an Improved Mountain Gazelle Optimizer for the Spatiotemporal Support Vector Regression (IMGO-STSVR) model, which effectively predicts railway subgrade settlement. Data are collected using Permanent Scatterer Interferometric Synthetic Aperture Radar (PS-InSAR) technology in combination with a multi-source environmental monitoring system. The proposed improvement to the Mountain Gazelle Optimizer (IMGO) enhances the model’s optimization capabilities, while the Support Vector Regression model is improved by the constructed spatiotemporal kernel function (STSVR). Experimental results demonstrate that the IMGO-STSVR model achieves high accuracy and stability across various experimental sites. This method provides valuable insights for predicting subgrade settlement in the railway industry, aiding in the early identification of potential risks, optimizing maintenance strategies, and ensuring the safe and efficient operation of rail transport.},
  archive      = {J_APIN},
  author       = {Chen, Guangwu and Zhao, Shilin and Li, Peng and Wang, Shilin and Zhou, Xin and Potekhin, Vyacheslav},
  doi          = {10.1007/s10489-025-06397-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {The improved mountain gazelle optimizer for spatiotemporal support vector regression: A novel method for railway subgrade settlement prediction integrating multi-source information},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention ensemble mixture: A novel offline reinforcement
learning algorithm for autonomous vehicles. <em>APIN</em>,
<em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06403-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline Reinforcement Learning (RL), which optimizes policies from previously collected datasets, is a promising approach for tackling tasks where direct interaction with the environment is infeasible due to high risk or cost of errors, such as autonomous vehicle (AV) applications. However, offline RL faces a critical challenge: extrapolation errors arising from out-of-distribution (OOD) data. In this paper, we propose Attention Ensemble Mixture (AEM), a novel offline RL algorithm that leverages ensemble learning and an attention mechanism. Ensemble learning enhances the confidence of Q-function predictions, while the attention mechanism evaluates the uncertainty of selected actions. By assigning appropriate attention weights to each Q-head, AEM effectively down-weights OOD actions and up-weights in-distribution actions. We further introduce three key improvements to enhance the robustness and generality of AEM: attention-weighted Bellman backups, KL divergence regularization, and delayed attention updates. Extensive comparative experiments demonstrate that AEM outperforms several state-of-the-art ensemble offline RL algorithms, while ablation studies underscore the significance of the proposed enhancements. In AV tasks, AEM exhibits superior performance compared to other methods, excelling in both offline and online evaluations.},
  archive      = {J_APIN},
  author       = {Han, Xinchen and Afifi, Hossam and Moungla, Hassine and Marot, Michel},
  doi          = {10.1007/s10489-025-06403-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Attention ensemble mixture: A novel offline reinforcement learning algorithm for autonomous vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning missing instances in intact and projection spaces
for incomplete multi-view unsupervised feature selection. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06406-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view unsupervised feature selection has achieved great success in identifying a subset of prominent features from multi-view data to produce compact and meaningful representations. However, most existing methods assume that all data views are complete, which is often not the case in real-world scenarios. Multi-view data is frequently incomplete, with some instances missing in certain views. To address this issue, we propose an incomplete multi-view unsupervised feature selection model based on multiple space learning, termed Learning Missing Instances in Intact and Projection Spaces for Incomplete Multi-view Unsupervised Feature Selection (LIPS). This model integrates intact latent space learning, projection space learning, missing instance imputation, and correlation structure learning into a joint framework. Specifically, LIPS employs intact latent space learning to generate intact representations that capture the full information of multi-view data. Using these representations, LIPS calculates correlations between data through a constrained self-expression strategy, generating a sparse correlation matrix where each row contains few non-zero entries, signifying that each data point can be linearly reconstructed using only a small subset of related neighbors. Subsequently, LIPS projects data into low-dimensional spaces to retain the neighborhood correlations. Finally, it leverages complementary information to impute the missing instances from a cross-view perspective based on intact representations and utilizes neighborhood information to generate neighborhood-smooth imputations for missing instances from view-specific perspectives. Additionally, an effective algorithm is developed to resolve the optimization problem. Extensive experiments conducted on six public datasets of different types, including image datasets (MSRC-v1, Caltech101-7, and CIFAR-10), text datasets (BBCSport and WebKB), and a face dataset (Yale), measured by Acc and NMI, demonstrate that the proposed LIPS outperforms state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wu, Jian-Sheng and Yu, Hong-Wei and Li, Yanlan and Min, Weidong},
  doi          = {10.1007/s10489-025-06406-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Learning missing instances in intact and projection spaces for incomplete multi-view unsupervised feature selection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-like synthetic sperm video generation from learned
behaviors. <em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06407-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-assisted sperm analysis is an open research problem, and a main challenge is how to test its performance. Deep learning techniques have boosted computer vision tasks to human-level accuracy, when sufficiently large labeled datasets were provided. However, when it comes to sperm (either human or not) there is lack of sufficient large datasets for training and testing deep learning systems. In this paper we propose a solution that provides access to countless fully annotated and realistic synthetic video sequences of sperm. Specifically, we introduce a parametric model of a spermatozoon, which is animated along a video sequence using a denoising diffusion probabilistic model. The resulting videos are then rendered with a photo-realistic appearance via a style transfer procedure using a CycleGAN. We validate our synthetic dataset by training a deep object detection model on it, achieving state-of-the-art performance once validated on real data. Additionally, an evaluation of the generated sequences revealed that the behavior of the synthetically generated spermatozoa closely resembles that of real ones.},
  archive      = {J_APIN},
  author       = {Hernández-García, Sergio and Cuesta-Infante, Alfredo and Makris, Dimitrios and S. Montemayor, Antonio},
  doi          = {10.1007/s10489-025-06407-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Real-like synthetic sperm video generation from learned behaviors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dense image-mask attention-guided transformer network for
jaw lesions classification and segmentation in dental cone-beam computed
tomography images. <em>APIN</em>, <em>55</em>(6), 1–26. (<a
href="https://doi.org/10.1007/s10489-025-06408-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation and classification of jaw lesions from cone-beam computed tomography (CBCT) images are crucial in computer-assisted diagnosis and treatment planning for oral and maxillofacial (OMF) surgery. However, the evolutionary nature of jaw lesions and their morphological diversity pose significant challenges to both segmentation and classification tasks. Although existing deep learning-based works have achieved promising results on segmentation and classification of other types of lesions, they often consider the two tasks separately, thereby overlooking the strong guidance that lesion masks can provide in determining lesion categories. In this manuscript, we propose a dense image-mask attention-guided transformer network for end-to-end jaw lesions classification and segmentation in 3D CBCT images based on a multi-task learning (MTL) architecture. Specifically, we design multi-dimension attention (MDA) and multi-scale attention (MSA) modules to incorporate dense features from different dimensions and scales, explicitly enhancing the guidance of lesion segmentation for classification decisions. Furthermore, to effectively encode long-term contextual information, we employ a transformer as the classification decoder and design a 3D positional embedding method to preserve the 3D positional information of sequential feature inputs for the transformer. Finally, we design a task merge module that employs a per-lesion inference strategy to assign a category to each lesion instance. A large in-house dataset consisting of 358 CBCT scans with five types of jaw lesions is constructed to evaluate the proposed method. The experimental results show a binary segmentation DICE score of 90%, a mean classification accuracy of 89.23%, and a multi-class segmentation DICE score of 79.06%, surpassing many state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Li, Xiang and Liu, Wei and Tang, Wei and Guo, Jixiang},
  doi          = {10.1007/s10489-025-06408-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {Appl. Intell.},
  title        = {Dense image-mask attention-guided transformer network for jaw lesions classification and segmentation in dental cone-beam computed tomography images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NTFNet: Narrowing-then-fusing network for RGB-TIR semantic
segmentation. <em>APIN</em>, <em>55</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10489-025-06411-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the task of understanding scenes in visible (RGB) and thermal-infrared (TIR) images has garnered increasing interest in the field of computer vision. However, most existing methods employ simplistic fusion strategies to merge features from different modalities. These strategies often overlook the differences in shallow-level features between modalities, thereby reducing the discriminability of the fused features and resulting in suboptimal segmentation performance. To address this issue, we present a novel RGB-TIR semantic segmentation framework, named NTFNet. This framework exploits the potential consistency of semantic-level features to rectify shallow-level features and reduce discrepancies between modalities prior to integration. Specifically, auxiliary encoders are employed at each layer to capture semantically consistent information. To obtain rich multi-modal semantic features, we designed a High-Level Feature Fusion Module (HFFM) that enhances feature representation in both channel and spatial dimensions. Subsequently, the Shallow Feature Difference Rectification Module (SFDRM) is introduced to rectify the difference in shallow-level features. To address the loss of detailed information during the rectification process, the SFDRM incorporates a Detail Attention Mechanism (DAM) to preserve the original detail information, thereby further optimizing the final segmentation results. In the end, a Multi-Scale Feature Fusion module (Multi-Scale FFM) is designed to combine the rectified features. Comprehensive experiments on two public RGB-TIR datasets show that our method significantly outperforms other state-of-the-art approaches in terms of performance.},
  archive      = {J_APIN},
  author       = {Liu, Yichen and Ye, Junjie and He, Wangpeng and Qu, Zhiqiang and Xu, Ruoxuan},
  doi          = {10.1007/s10489-025-06411-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {NTFNet: Narrowing-then-fusing network for RGB-TIR semantic segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic graph-based graph attention network for anomaly
detection in industrial multivariate time series data. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06412-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For industrial big data, anomaly detection for multivariate time series data is of critical strategic significance. However, the complexity of industrial equipment and production processes, combined with the high dimensionality of production data, makes it challenging for traditional anomaly detection methods to effectively capture the complex interdependencies and dynamic evolutionary relationships among multiple variables. Additionally, issues such as unstable data distributions, variability, and data drift exacerbate the challenges faced by anomaly detection methods in industrial data analysis and decision-making. This study presents a dynamic graph-based graph attention network for anomaly detection in a multivariate time series data (D-GATAD) model, introducing an innovative approach to dynamic graph construction. The proposed method seamlessly integrates node content features with graph topological structure information, enabling adaptive construction of dynamic graphs based on the current sensor network structure. This design allows for precise modeling of the complex temporal dependencies between variables. Furthermore, the method incorporates an optimized prediction-based model design that organically combines embedding vectors with node data, thereby significantly enhancing the interpretability of the analytical results. Experimental evaluations demonstrate that the proposed method outperforms existing state-of-the-art models across multiple public benchmark datasets. Notably, on the highly complex WADI dataset, it achieves a 5.12% improvement in the AUC score, underscoring its robustness and effectiveness in industrial anomaly detection. This research offers an innovative and widely applicable solution for industrial data analysis and anomaly detection, with significant implications for practical deployment.},
  archive      = {J_APIN},
  author       = {Gao, Cong and Ma, Hongye and Pei, Qingqi and Chen, Yanping},
  doi          = {10.1007/s10489-025-06412-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic graph-based graph attention network for anomaly detection in industrial multivariate time series data},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved data-driven model-free adaptive control method for
an upper extremity power-assist exoskeleton. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06415-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of power-assist exoskeletons in physical labor and daily activities has increased the demand for robust control strategies to address challenges in human-exoskeleton interaction. Factors such as collisions and friction introduce uncertain disturbances, making it difficult to establish an accurate human-exoskeleton interaction model, thereby limiting the applicability of current model-based control methods. To overcome these problems, this study proposes an improved data-driven model-free adaptive control method (IMFAC) for the upper extremity power-assist exoskeleton. The stability and convergence of the closed-loop system are rigorously proven. To optimize the initial conditions of IMFAC, we propose an improved snake optimizer (ISO) algorithm incorporating opposition-based learning. The proposed ISO-IMFAC method is evaluated in two scenarios: a nonlinear Hammerstein model benchmark and a physical exoskeleton platform. Experimental results demonstrate that ISO-IMFAC outperforms other popular data-driven control methods across six metrics: integrated absolute error (4.756), mean integral of time-weighted absolute error (0.457), maximum error (1.167), minimum error (0), mean error (0.032), and error standard deviation (0.169). Additionally, the ISO-IMFAC method effectively drives the exoskeleton without relying on its dynamic model. In two load-bearing experiments conducted with five subjects wearing the exoskeleton, the proposed method reduces average muscle exertion per unit time by over 50% and extended working time by more than 180%. These findings highlight the significant potential of the proposed method to enhance user endurance and reduce physical strain, paving the way for practical applications in diverse real-world scenarios. The code is released at https://github.com/Shurun-Wang/ISO-IMFAC .},
  archive      = {J_APIN},
  author       = {Wang, Shurun and Tang, Hao and Ping, Zhaowu and Tan, Qi and Wang, Bin},
  doi          = {10.1007/s10489-025-06415-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Improved data-driven model-free adaptive control method for an upper extremity power-assist exoskeleton},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated echocardiogram image quality assessment with YOLO
and resnet in the left ventricular myocardium of A4C views.
<em>APIN</em>, <em>55</em>(6), 1–31. (<a
href="https://doi.org/10.1007/s10489-025-06419-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image quality of echocardiography is an important factor to affect cardiovascular disease diagnosis. Currently, the deep learning (DL) used in cardiac echocardiogram image quality assess model focus more on evaluating the whole dynamic video, but the outputs revealed less local anatomical details in judging the image quality in heart chambers. This study was aimed to achieve the local part image quality assess, specifically for the five locals in the left ventricle of A4C section for myocardium. The object detection model, YOLOv8 (You Only Look Once), were used to crop five local parts in the left ventricle myocardium of A4C section. Then, the ResNet-18 model was used to evaluate the image quality of each cropped part, that output from score 0 to 3, four quality levels. The YOLOv8 model demonstrated exceptional performance metrics with Precision of 98.77%, Recall of 98.84%, mAP50 of 98.95%, and mAP50-90 of 81.33%. Additionally, the model exhibited an average Inference Time of 215ms per frame. Comparatively, the ResNet-18 model achieved Accuracy scores of 79.34%, 82.41%, 77.82%, 82.33%, and 78.13%, which correspond to the assessment of the left ventricular myocardium in all five local A4C views. The aggregate performance of the ResNet-18 model was characterized by average Macro Precision of 66.77%, Recall of 59.89%, and F1 Score of 59.49%. Furthermore, the model displayed average Micro Precision of 67.42%, Recall of 70.00%, and F1 Score of 69.98%. This study determined the effectiveness of YOLOv8 to find the bounding box of local myocardium and ResNet-18 for real-time automatic quality assessment, and had the potential to improve the efficiency of diagnosis for the doctor using echocardiogram.},
  archive      = {J_APIN},
  author       = {Liu, Weiyang and Wang, Qiushuang and Zhang, Peifang and Deng, Yujiao and Zhao, Yawei and Zhang, Yongming and Xu, Hongli and Qiu, Xiaowan and Chen, Xu and Xu, Jiayu and He, Kunlun},
  doi          = {10.1007/s10489-025-06419-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Appl. Intell.},
  title        = {Automated echocardiogram image quality assessment with YOLO and resnet in the left ventricular myocardium of A4C views},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time AI-driven quality control for laboratory
automation: A novel computer vision solution for the opentrons OT-2
liquid handling robot. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06334-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of robotics and automated solutions in life sciences R&amp;D has accelerated in recent years, driven by the need to process increasing sample volumes, protect laboratory staff from hazardous substances, and manage financial pressures. Various automation systems, each with distinct levels of sample processing, transportation tasks, and data management, are available to meet specific application requirements, with liquid handling robots taking pivotal positions in these systems. However, current liquid handling robots, such as the Opentrons OT-2, lack integrated vision-based quality control, which limits their accuracy and reliability. This study presents an AI-driven computer vision model designed to enhance quality control in laboratory automation. By integrating the YOLOv8 object detection model with the OT-2, our model enables precise detection of pipette tips and liquid volumes, providing real-time feedback on errors, such as missing tips and incorrect liquid levels. Our results demonstrate the model&#39;s effectiveness and accessibility, presenting an affordable solution for improving automation in academic and research laboratories. This closed-loop system transforms the OT-2 into a robust tool for automated laboratory tasks, making it an accessible and cost-effective approach for enhancing quality control in laboratory automation and addressing a critical gap in available tools for resource-limited settings.},
  archive      = {J_APIN},
  author       = {Khan, Sana Ullah and Møller, Vilhelm Krarup and Frandsen, Rasmus John Normand and Mansourvar, Marjan},
  doi          = {10.1007/s10489-025-06334-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Real-time AI-driven quality control for laboratory automation: A novel computer vision solution for the opentrons OT-2 liquid handling robot},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection in crowd scenes via cross trajectories.
<em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06338-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel finite-time braid entropy (FTBE) theorem to extract feature vectors to detect abnormal events occurring globally and locally in crowds. Detecting abnormal events or behavior in crowd movements is a key research topic regarding community security and management. A trajectory- based method depending on the FTBE theorem and the distribution of motion vectors is presented to determine abnormal events. The FTBE theory determines the complexity of the pattern occurring during the movement of the trajectories describing the behavior. In most studies in the literature, the image is divided into equal regions and the solution is produced by separating every behavior into more than one zone. However, this may result in incorrect results. Our study separated the behavior within a certain time interval into location-independent motion clusters. Each cluster indicated a behavior, which was represented by a feature vector derived from the distribution of FTBE and motion vectors. The learning model and fully connected deep neural network were used to detect which cluster was behaving abnormally in the local area. In addition, abnormal events were determined globally by the step braid entropy score (SBES) value calculated for the current scene. The method was tested using the UMN, UCSD and UCF-Crime databases. The experimental results of the method showed an alternative approach to the detection of abnormal behavior.},
  archive      = {J_APIN},
  author       = {Akpulat, Murat and Ekinci, Murat},
  doi          = {10.1007/s10489-025-06338-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Anomaly detection in crowd scenes via cross trajectories},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional neural networks for advanced adhesive joints
application patterns. <em>APIN</em>, <em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06340-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adhesive bonding is a widely used joining technique across various industries. Achieving uniform adhesive coverage over the entire surface without the formation of air pockets is crucial for creating strong and durable joints. Simultaneously, it is essential to minimise waste caused by material leakage at the edges. However, generating an optimal adhesive pattern to achieve the desired adhesive distribution after compression remains a challenge, as fluids tend to spread in a circular manner, while industry-relevant target geometries are typically non-circular. This paper investigates the application of Convolutional Neural Networks (CNNs) to optimise adhesive application patterns by utilising a simplified simulation model known as the Partially Filled Gaps Model (PFGM) to generate extensive training data. The CNN is trained to predict fluid distribution outcomes based on initial adhesive application patterns and addresses the inverse problem of determining an optimal application pattern to achieve a desired target distribution after compression. Two training approaches are introduced: a basic inverse model that utilizes a straightforward input–output data exchange, and a more advanced strategy that incorporates a forward model to improve accuracy. The forward model predicts the final distribution, enabling better refinement of the initial application patterns. The results demonstrate that the CNN-based approach is highly effective in generating optimal application patterns for adhesive bonds. Its primary advantage, compared to alternative methods, lies in its ability to achieve precise results within a short computation time. However, a significant drawback is the limited flexibility in accommodating variations in parameters.},
  archive      = {J_APIN},
  author       = {Scholtes, Kiro and Flaig, Florian and Kaufmann, Marvin and Lehne, Frank Guido and Vallée, Till and Fricke, Holger and Müller, Michael},
  doi          = {10.1007/s10489-025-06340-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Convolutional neural networks for advanced adhesive joints application patterns},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffusionLight: A multi-agent reinforcement learning
approach for traffic signal control based on shortcut-diffusion model.
<em>APIN</em>, <em>55</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s10489-025-06359-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous researches have shown that the Reinforcement Learning(RL) is an effective solution to solve large-scale traffic signal control(TSC) problems. However, facing multi-scenario and emergencies, cooperative control of traffic signals at multi-intersection becomes a challenging multi-agent reinforcement learning(MARL) process. In order to solve real-world problems, this paper proposes a MARL algorithm called DiffusionLight, which combines Shortcut-Diffusion Model(SDM) and Soft Actor-Critic(SAC), and a fast Diffusion model to solve the traffic signal cooperative control problem of multi-scenario and multi-intersection. DiffusionLight has the stable characteristics and powerful expression ability of the SDM as the strategy network to solve the action space, while SAC is used as the value network to better explore the solution space. Experimental results show that DiffusionLight exhibits better stability compared to the baseline algorithm in the face of multi-scenario TSC and burst data anomalies, and as well as excellent performance on multiple public datasets of grid and arterial traffic networks.},
  archive      = {J_APIN},
  author       = {Yu, JiLin and Wang, Zhiwen and Zhang, Ruonan},
  doi          = {10.1007/s10489-025-06359-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {DiffusionLight: A multi-agent reinforcement learning approach for traffic signal control based on shortcut-diffusion model},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TsDa-ASAM: Balancing efficiency and accuracy in coke image
particle size segmentation via two-stage distillation-aware adaptive
segment anything model. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06427-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coke image segmentation is a crucial step in coke particle size control of the sintering process. However, due to the complexity of model architecture and the dense distribution of coke particles in the images, existing segmentation methods fail to satisfy the efficiency and accuracy requirements for coke image segmentation in industrial scenarios. To address these challenges, this paper proposes a two-stage distillation-aware adaptive segment anything model to balance efficiency and accuracy in coke image particle size segmentation, referred to as TsDa-ASAM. In the first stage, knowledge distillation methods are employed to distill the Segment Anything Model (SAM) into a lightweight model, explicitly focusing on enhancing segmentation efficiency. In the second stage, a domain knowledge injection strategy is formulated, which incorporates domain knowledge into the distillation model to effectively enhance the accuracy. Moreover, an adaptive prompt point selection algorithm is introduced to address the redundancy issue of prompt points in SAM, improving the efficiency of TsDa-ASAM. The effectiveness of TsDa-ASAM is validated through extensive experiments on the publicly available dataset SA-1B and the coke image dataset from industrial sites. After distillation and fine-tuning, the segmentation accuracy of the proposed model improved by 10%, and the segmentation efficiency of TsDa-ASAM was enhanced by 2 to 3 times with the integration of the adaptive prompt point selection algorithm. The experimental results have effectively demonstrated the potential of the proposed model in balancing accuracy and efficiency.},
  archive      = {J_APIN},
  author       = {Wang, Yalin and Peng, Yubin and Tan, Xujie and Pan, Yuqing and Liu, Chenliang},
  doi          = {10.1007/s10489-025-06427-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {TsDa-ASAM: Balancing efficiency and accuracy in coke image particle size segmentation via two-stage distillation-aware adaptive segment anything model},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Entropy guidance hierarchical rich-scale feature network
for remote sensing image semantic segmentation of high resolution.
<em>APIN</em>, <em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06433-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of high-resolution remote sensing images (HRRSIs) is crucial for a wide range of applications, such as urban planning and disaster management. However, in HRRSIs, existing multiscale feature extraction and fusion methods often fail to achieve the desired accuracy because of the challenges posed by densely distributed small objects and large-scale variations. Therefore, we propose a hierarchical rich-sale feature network with entropy guidance (HRFNet), which introduces an entropy-based weighting and feature mining strategy to enhance feature extraction and fusion. Specifically, image entropy is employed as a quantifiable index to characterize the object distribution within remote sensing images, enabling an adaptive image division strategy. The image entropy is further used as weights during network training to emphasize regions with high entropy, which often correspond to edges and densely populated small objects. Additionally, the proposed feature mining strategy effectively integrates both global and local contextual information across multilayer feature maps. Extensive experiments show that HRFNet achieves mIoU scores of 81.31%, 86.47%, and 51.5% on the Vaihingen, Potsdam, and LoveDA datasets, respectively, outperforming existing methods by 1.0–3.0% mIoU.},
  archive      = {J_APIN},
  author       = {Zhang, Haoxue and Li, Linjuan and Xie, Xinlin and He, Yun and Ren, Jinchang and Xie, Gang},
  doi          = {10.1007/s10489-025-06433-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Entropy guidance hierarchical rich-scale feature network for remote sensing image semantic segmentation of high resolution},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty weighted policy optimization based on bayesian
approximation. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06303-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient exploration remains a major challenge in the field of reinforcement learning (RL). Bayesian methods have been widely investigated within the RL paradigm and are used to implement intelligent exploration strategies. However, most of these methods inevitably introduce some complexity within the Bayesian neural networks (BNNs) or are difficult to optimize elegantly. In this work, a novel algorithm called uncertainty weighted policy optimization (UWPO) based on Bayesian approximation, is introduced. UWPO theoretically analyzes the uncertainty of the policy space using the Dirichlet distribution and Monte Carlo (MC) dropout for both discrete and continuous spaces, eliminating the need for an explicit distribution representation in BNNs. The algorithm also proposes an implicit distributional training method for the value function, which is compatible with Bayesian inference. Moreover, an uncertainty-weighted update principle is adopted to adaptively adjust the contribution of each training instance to the objective. Finally, comparing UWPO with other prevailing deep reinforcement learning (DRL) algorithms on the Atari, MuJoCo, and Box2D platforms. The experimental results demonstrate that the algorithm improves the average reward score by nearly 15% while reducing computational costs by 20% compared to current state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Li, Tianyi and Yang, Genke and Chu, Jian},
  doi          = {10.1007/s10489-025-06303-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Uncertainty weighted policy optimization based on bayesian approximation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFP: Temporal knowledge graph completion based on
sequence-focus patterns representation learning. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06306-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extrapolation task in the temporal knowledge graph has received increasing attention from scholars due to its wide range of practical application scenarios. At present, recurrent neural networks are currently widely used in temporal knowledge graph completion techniques. These networks are employed to depict the sequential pattern of entities and relations. However, as the sequence lengthens, some critical early information may become diluted. Prediction errors ensue in the completion task as a result. Furthermore, it is observed that existing temporal knowledge graph completion methods fail to account for the topological structure of relations, which leads to relation representations with essentially little distinction across different timestamps. In order to tackle the previously mentioned concern, our research introduces a Temporal Knowledge Graph Completion Method utilizing Sequence-Focus Patterns Representation Learning (SFP). This method contains two patterns: the Focus pattern and the Sequential pattern. In the SFP model, we developed a novel graph attention network called ConvGAT. This network efficiently distinguishes and extracts complex relation information, thereby enhancing the accuracy of entity representations that are aggregated in the Focus pattern and Sequential pattern. Furthermore we proposed RelGAT, a graph attention network that simulates the topological structure of relations. This enhances the precision of relation representations and facilitates the differentiation between relation embeddings generated at various timestamps in the Focus pattern. Utilizing a time-aware attention mechanism, the Focus pattern extracts vital information at particular timestamps in order to amplify the data that the Sequential pattern dilutes. On five distinct benchmark datasets, SFP significantly outperforms the baseline, according to a comprehensive series of experiments.},
  archive      = {J_APIN},
  author       = {Wang, Jingbin and Ke, XiFan and Zhang, FuYuan and Wu, YuWei and Zhang, SiRui and Guo, Kun},
  doi          = {10.1007/s10489-025-06306-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {SFP: Temporal knowledge graph completion based on sequence-focus patterns representation learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-computing, deep reinforcement learning-based predictive
human-robot neuromechanical simulation for wearable robots.
<em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06360-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-robot interaction (HRI) is widely used in robotics to assist humans, with wearable robots enhancing mobility for both able-bodied individuals and those with impairments. Traditionally, characterizing human biomechanical responses to these robots requires extensive human testing, which is time-consuming, costly, and potentially risky. Developing computational HRI simulations for wearable robots offers a promising solution. However, modeling the high-fidelity human-exoskeleton interaction in simulations presents significant challenges that remain underexplored. These include creating a high-fidelity autonomous human motion control agent, accounting for the non-passive nature of human responses, and incorporating closed-loop control within the robotic system. In this paper, we propose an AI-computing, deep reinforcement learning-based HRI simulation to predict complex and realistic human biomechanical responses to exoskeleton assistance. The multi-neural network training process develops an end-to-end, autonomous control policy that reduces human muscle effort by utilizing current human kinematic states. This approach processes state information from both the human musculoskeletal and exoskeleton control neural network, generating control policies for robust human walking movement and reducing muscle effort. Numerical experiments demonstrated the framework’s ability to simulate human motion control, showing reductions in hip joint torque (13.04 $$\%$$ ), rectus femoris (RF) muscle activation (7.31 $$\%$$ ), and biceps femoris (BF) muscle activation (12.21 $$\%$$ ) with exoskeleton use. Validation through real-world experiments further confirmed a decrease in RF and BF muscle activations by 22.12 $$\%$$ and 11.45 $$\%$$ , respectively. These results highlight the effectiveness of our proposed AI computing-based simulation method in replicating and optimizing human biomechanics during exoskeleton-assisted movement. This AI computing-based human-exoskeleton predictive simulation may offer a general, high-fidelity platform for studying human biomechanical responses and enabling autonomous control for assistive devices without requiring intensive human testing in the rehabilitation field.},
  archive      = {J_APIN},
  author       = {Wang, Mingyi and Luo, Shuzhen},
  doi          = {10.1007/s10489-025-06360-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {AI-computing, deep reinforcement learning-based predictive human-robot neuromechanical simulation for wearable robots},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance of machine learning methods for cattle
identification and recognition from retinal images. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06398-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animal identification is a critical issue in terms of security, traceability, and animal health, especially in large-scale livestock enterprises. Traditional methods (such as ear tags and branding) both negatively affect animal welfare and may lead to security vulnerabilities. This study aims to develop a biometric system based on retinal vascular patterns for the identification and recognition of cattle. This system aims to provide a safer and animal welfare-friendly alternative by using image processing techniques instead of traditional device-based methods. In the study, preprocessing, segmentation, feature extraction, and performance evaluation steps were applied for the biometric identification and recognition process using retinal images taken from both eyes. Techniques such as green channel extraction, contrast-limited adaptive histogram equalization, morphological operations, noise filtering, and threshold determination were used in the preprocessing stage. Fuzzy C-means, K-means, and Level-set methods were applied for segmentation, and feature extraction was performed using SIFT, SURF, BRISK, FAST, and HARRIS methods. At the end of the study, the highest accuracy rate was obtained as 95.6% for identification and 87.9% for recognition. In addition, the obtained dataset was shared publicly, thus creating a reusable resource that researchers from different disciplines can use. It was concluded that this study made a significant contribution to the field of biometric-based animal identification and recognition and offered a practically usable solution in terms of animal welfare and safety.},
  archive      = {J_APIN},
  author       = {Cihan, Pınar and Saygılı, Ahmet and Akyüzlü, Muhammed and Özmen, Nihat Eren and Ermutlu, Celal Şahin and Aydın, Uğur and Yılmaz, Alican and Aksoy, Özgür},
  doi          = {10.1007/s10489-025-06398-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Performance of machine learning methods for cattle identification and recognition from retinal images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect of continuous s-shaped rectified linear function on
deep convolutional neural network. <em>APIN</em>, <em>55</em>(6), 1–24.
(<a href="https://doi.org/10.1007/s10489-025-06399-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vanishing gradient issue in convolutional neural networks (CNNs) is often addressed by improving activation functions, such as the S-shaped rectified linear activation unit (SReLU). However, SReLU can pose challenges in updating training parameters effectively. To mitigate this, we propose applying the Aggregation Fischer–Burmeister (AFB) function to SReLU, which smooths the secant line slope of the function from both sides. However, direct application of AFB to SReLU can intensify the vanishing gradient issue due to irregular function behavior. To address this concern, we introduce a regulated version of AFB (ReAFB) that ensures proper gradient and mean activation output conditions when applied to SReLU (ReAFBSReLU). We evaluate the performance of CNNs using ReAFBSReLU on three benchmark datasets: MNIST, CIFAR-10 (with and without data augmentation), and CIFAR-100. Specifically, we implement Network in Network (NIN) for MNIST and CIFAR-10, and LeNet for CIFAR-100 dataset. Additionally, we utilize SqueezeNet exclusively to compare the performance of CNNs using the proposed ReAFBSReLU activation function against state-of-the-art activation functions. Our results demonstrate that ReAFBSReLU outperforms other activation functions tested in this study, indicating its efficacy in enhancing training parameter updates and subsequently improving accuracy.},
  archive      = {J_APIN},
  author       = {Ghazvini, Anahita and Abdullah, Siti Norul Huda Sheikh and Ayob, Masri},
  doi          = {10.1007/s10489-025-06399-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Effect of continuous S-shaped rectified linear function on deep convolutional neural network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Use of artificial intelligence techniques in
characterization of vibration signals for application in agri-food
engineering. <em>APIN</em>, <em>55</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10489-025-06424-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bottling machinery is a critical component in agri-food industries, where maintaining operational efficiency is key to ensuring productivity and minimizing economic losses. Early detection of faulty conditions in this equipment can significantly improve maintenance procedures and overall system performance. This research focuses on health monitoring of gripping pliers in bottling plants, a crucial task that has traditionally relied on analyzing raw vibration signals or using narrowly defined, application-specific features. However, these methods often face challenges related to limited robustness, high computational costs, and sensitivity to noise. To address these limitations, we propose a novel approach based on generic features extracted through basic signal processing techniques applied to vibration signals. These features are then classified using a random forest algorithm, enabling an effective analysis of health states. The proposed method is evaluated against traditional approaches and demonstrates clear advantages, including higher accuracy in detecting and classifying faulty conditions, greater robustness against random perturbations, and a reduced computational cost. Additionally, the method requires fewer training instances to achieve reliable performance. This study highlights the potential of artificial intelligence and signal processing techniques in predictive maintenance, offering a scalable and efficient solution for fault detection in manufacturing processes, particularly within the agri-food sector.},
  archive      = {J_APIN},
  author       = {Luque, Amalia and Campos Olivares, Daniel and Mazzoleni, Mirko and Ferramosca, Antonio and Previdi, Fabio and Carrasco, Alejandro},
  doi          = {10.1007/s10489-025-06424-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Use of artificial intelligence techniques in characterization of vibration signals for application in agri-food engineering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison study of several strategies in multivariate
time series clustering based on graph community detection.
<em>APIN</em>, <em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06444-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data analysis, especially forecasting, classification, imputation, and anomaly detection, has gained a lot of research attention in recent years due to its prevalence and wide application. Compared to classification, clustering is an unsupervised task and thus more applicable for analyzing massive time series without labels. One latest way is based on the idea of graph community detection: first transforming a time series set into a graph (or a network), in which a node represents a time series instance and an edge denotes that the two connected nodes (thus the represented time series) are more similar to each other; then, running a community detection algorithm on the graph to discover a community structure, that gives out a clustering result. Recently, there are several works based on the graph community detection idea to cluster multivariate time series. However, such works focus only on specific methods in each step, and a performance comparison of combinations of methods in different steps is lacking. This paper outlines the process of graph-based multivariate time clustering as four phases (referred to as framework), namely representation learning, similarity computing, relation network construction, and clustering, lists typical methods in each phase, and makes a comparison study of combinations of each phase methods (called strategies in this paper). Recent time series deep neural network models are introduced to the framework as time series representation learning methods as well. In addition, $$\varvec{\varepsilon } \varvec{k}$$ NN, an improvement of $$\varvec{k}$$ NN by filtering out unnecessary low similarity connections during network construction, is proposed. A great number of experiments are conducted on eight real-world multivariate time series with various properties to verify the performance of different strategy combinations. The results suggest that proper deep neural network is a promising way for learning time series vector representations to compute similarities, and strategies including $$\varvec{\varepsilon } \varvec{k}$$ NN for network construction, average for multi-layer network merging and Louvain for clustering are more effective from a statistical perspective.},
  archive      = {J_APIN},
  author       = {Sun, Hanlin and Jie, Wei and Chen, Yanping and Wang, Zhongmin},
  doi          = {10.1007/s10489-025-06444-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {A comparison study of several strategies in multivariate time series clustering based on graph community detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QuinNet: Quintuple u-shape networks for scale- and
shape-variant lesion segmentation. <em>APIN</em>, <em>55</em>(6), 1–15.
(<a href="https://doi.org/10.1007/s10489-025-06448-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches have demonstrated remarkable efficacy in medical image segmentation. However, they continue to struggle with challenges such as the loss of global context information, inadequate aggregation of multi-scale context, and insufficient attention to lesion regions characterized by diverse shapes and sizes. To address these challenges, we propose a new medical image segmentation network, which consists of one main U-shape network (MU) and four auxiliary U-shape sub-networks (AU), leading to Quintuple U-shape networks in total, thus abbreviated as QuinNet hereafter. MU devises special attention-based blocks to prioritize important regions in the feature map. It also contains a multi-scale interactive aggregation module to aggregate multi-scale contextual information. To maintain global contextual information, AU encoders extract multi-scale features from the input images, then fuse them into feature maps of the same level in MU, while the decoders of AU refine features for the segmentation task and co-supervise the learning process with MU. Overall, the dual supervision of MU and AU is very beneficial for improving the segmentation performance on lesion regions of diverse shapes and sizes. We validate our method on four benchmark datasets, showing that it achieves significantly better segmentation performance than the competitors. Source codes of QuinNet are available at https://github.com/Truman0o0/QuinNet .},
  archive      = {J_APIN},
  author       = {Fan, Gaojuan and Wang, Jie and Xia, Ruixue and Zhou, Funa and Zhang, Chongsheng},
  doi          = {10.1007/s10489-025-06448-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {QuinNet: Quintuple u-shape networks for scale- and shape-variant lesion segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reinforcement learning malware detection model based on
heterogeneous information network path representation. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06417-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant increase of Android malware, the APP privacy data leakage incidents occur frequently, which poses a great threat to user property and information security. Specifically, the new malware has the characteristics of high evolution rate and diverse variants, leading to the fact that the current malware detection methods still have three key problems: (1) Difficulty in acquiring Android sample structural features; (2) Weakly in representing malware behavior structure; (3) Poor robustness of the detection model. To address the above limitations, we propose a new malware detection framework MPRLDroid with reinforcement learning. First of all, the MPRLDroid model extracts the Android APP structural features and constructs the heterogeneous information network data based on the semantic call structure between APP, API and permission. Subsequently, the model utilizes reinforcement learning to adaptively generate a meta-path for each sample and combines it with a graph attention network to effectively represent the graph of nodes. Finally, the low-dimensional graph node vector data is brought into the downstream detection task for classification, where the performance change of the classification result is used as a reward function for reinforcement learning. The experimental results demonstrate that the MPRLDroid model, when integrated with reinforcement learning, outperforms the baseline models in terms of performance, and its detection model exhibits greater robustness compared to other models.},
  archive      = {J_APIN},
  author       = {Yang, Kang and Cai, Lizhi and Wu, Jianhua and Liu, Zhenyu and Zhang, Meng},
  doi          = {10.1007/s10489-025-06417-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A reinforcement learning malware detection model based on heterogeneous information network path representation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSF-SegFormer: A feature fusion algorithm for magnetic
leakage image segmentation. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06453-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional segmentation networks have low segmentation accuracy for flux leakage images, often leading to missed or false detections of small defects, which significantly affect the evaluation of defect severity. Based on the SegFormer network, a high-accuracy decoder based on multi-scale feature fusion is proposed, which is more suitable for the segmentation of small defects in flux leakage and replaces the multi-layer perceptron (MLP) decoder of the original network. The new network model is called MSF-SegFormer. MSF-SegFormer introduces a feature fusion network MSF that integrates high-resolution and low-resolution features and introduces feature pyramid fusion, which can merge output features at different levels across different scales. A cascaded attention module is proposed, combining two local attention mechanisms in a cascade and using a residual network to enhance the local feature representation of flux leakage images, improving the accuracy and stability of the task. In the application of flux leakage defect data, compared with benchmark models such as CNN and SegFormer, this model can accurately segment target edges with fewer parameters, maintain high accuracy, reduce false detection probability, and improve the Miou value of the traditional MLP decoder from 88.21% to 90.44%.},
  archive      = {J_APIN},
  author       = {Wang, Zhujun and Ni, Rongtai and Sun, Tianhe and Jiang, Yulong and Liu, Bin},
  doi          = {10.1007/s10489-025-06453-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {MSF-SegFormer: A feature fusion algorithm for magnetic leakage image segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FA3-net: Feature aggregation and augmentation with attention
network for sound event localization and detection. <em>APIN</em>,
<em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06437-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound event localization and detection (SELD) aims to identify the category and duration of sound events (SED) while also estimating their respective direction of arrival (DOA). This multi-task problem presents unique challenges, as the features required for SED and DOA tasks are not entirely aligned. Consequently, incomplete feature extraction and suboptimal feature fusion often hinder performance. To address these issues, we propose a feature aggregation and augmentation with attention network (FA3-Net). FA3-Net consists of two main components: the feature aggregation and augmentation with attention (FA3) module and the Conformer module. The FA3 module plays a critical role in fusing and enhancing high-level features, which is specifically designed to efficiently handle the distinct requirements of SED and DOA tasks. It ensures that task-specific features are extracted effectively, while also improving feature discriminability and reducing confusion. The feature aggregation residual block (FAResBlock), a component of the FA3 module, handles task-specific feature aggregation, while the feature augmentation with attention block (FAA block) enhances feature representation across multiple dimensions. The Conformer module is employed to model the temporal sequence, as it excels in capturing both local and global dependencies, making it ideal for comprehensive time sequence analysis. Finally, to overcome data limitations, audio channel swapping (ACS) is employed. Experiments on the STARSS23 dataset, DCASE2021 dataset and L3DAS22 dataset show that FA3-Net significantly outperforms other models in both feature aggregation and augmentation, while also being more efficient and lightweight. The code is available in: https://github.com/wangchuan11111111/FA3-NET},
  archive      = {J_APIN},
  author       = {Wang, Chuan and Huang, Qinghua},
  doi          = {10.1007/s10489-025-06437-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {FA3-net: Feature aggregation and augmentation with attention network for sound event localization and detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved DAB-DETR model for irregular traffic obstacles
detection in vision based driving environment perception scenario.
<em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06440-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine vision based irregular traffic obstacles recognition plays a pivotal role in the autonomous driving and Advanced Driver Assistance Systems (ADAS) by providing the necessary environment perception capabilities. Traditional models for recognizing irregular traffic obstacles suffer from challenges with small target detection, poor performance in diverse environmental conditions and computational complexity. This work addresses the critical issue of recognizing irregular traffic obstacles in roadway environments. We present an enhanced target detection model based on the Dynamic Anchor Boxes-recognition Transformer (DAB-DETR). The original model’s structure was limited in expressing relative positional information between features due to the reliance on absolute position encoding. To overcome this limitation, the improved DAB-DETR incorporates relative position encoding within the multi-headed self-attention mechanism of the Transformer encoder. Additionally, we propose a novel Average Precision (AP) loss function that unifies classification and localization losses into a single parameterized formula, addressing performance degradation observed in the original model. Experimental results demonstrate significant improvements in detection accuracy for irregular traffic objects, showcasing the effectiveness of the proposed enhancements. According to the testing results, the improved DAB-DETR model’s detection accuracy is 82.00% with Intersection over Union (IoU) equals to 0.5, which is 3.3% better than the original model and 6.20% and 7.71% better than the conventional models, YOLOv5 and Faster R-CNN, respectively.},
  archive      = {J_APIN},
  author       = {Yang, Junchao and Zhang, Hui and Zhou, Yuting and Guo, Zhiwei and Lin, Feng},
  doi          = {10.1007/s10489-025-06440-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Improved DAB-DETR model for irregular traffic obstacles detection in vision based driving environment perception scenario},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ship pipeline defect detection method based on deep learning
and transfer fusion of ultrasonic guided wave signals. <em>APIN</em>,
<em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06390-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasonic guided waves (UGW) hold great promise for structural health monitoring (SHM) of pipeline structures. However, the inherent complexity of pipeline defect features within the UGW makes the intuitive and accurate identification of defects based only on UGW signals challenging. In addition, the existing neural network-based UGW signal recognition methods require a large number of defect waveform samples, which limits their applicability. This study proposes a signal recognition method based on deep learning and sample transfer fusion for the identification of UGW signals in ship pipelines, allowing to accurately detect their potential defects. A time–frequency imaging algorithm for ship pipeline UGW signals is first introduced using the continuous wavelet transform (CWT) to capture their time–frequency characteristics. Leveraging transfer learning, UGW signal samples from various operational scenarios onshore oil pipelines are then fused to pre-train the GoogLeNet convolutional neural network (CNN) model. Finally, the pre-trained GoogLeNet model is fine-tuned with ship pipeline UGW signal samples, which allows to accurately detect the underlying defects. The experimental results demonstrate that the proposed method significantly increases the classification accuracy of ship pipeline defects compared with non-transfer learning methods and time-domain imaging. More precisely, the accuracy increases from 63.3% to 97.3%. Furthermore, the obtained results show that the proposed method has high robustness.},
  archive      = {J_APIN},
  author       = {Tang, Ruoli and Li, Yongzhe and Zhang, Shangyu},
  doi          = {10.1007/s10489-025-06390-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Ship pipeline defect detection method based on deep learning and transfer fusion of ultrasonic guided wave signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning-based road surveillance system in
distributed CCTV environment: Pedestrian fall recognition using
spatio-temporal attention networks. <em>APIN</em>, <em>55</em>(6), 1–16.
(<a href="https://doi.org/10.1007/s10489-025-06451-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent CCTV systems are highly effective in monitoring pedestrian and vehicular traffic and identifying anomalies in the roadside environment. In particular, it is necessary to develop an effective recognition system to address the problem of pedestrian falls, which is a major cause of injury in road traffic environments. However, the existing systems have challenges such as communication constraints and performance instability. In this paper, we propose a novel fall recognition system based on Federated Learning (FL) to solve these challenges. The proposed system utilizes a GAT combined with LSTM and attention layers to extract spatio-temporal features, which can more accurately identify pedestrian falls. Each road CCTV works as an independent client to generate local data, and the server aggregates these models to learn a global model. This ensures robust operation in different views and environments, and solves the bottleneck of data communication and security challenges. We validated the feasibility and applicability of the FL-based fall recognition method by implementing the prototype and applying it to the UP-FALL benchmark dataset, which is widely used for fall recognition. Code has been made available at: https://github.com/Kim-Byeong-Hun/Fed-PFR .},
  archive      = {J_APIN},
  author       = {Kim, Byeonghun and Im, Jaegyun and Noh, Byeongjoon},
  doi          = {10.1007/s10489-025-06451-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Federated learning-based road surveillance system in distributed CCTV environment: Pedestrian fall recognition using spatio-temporal attention networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional requirements for reinforcement
recommendation reasoning. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-024-05854-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized recommendation systems not only need to improve the accuracy of recommendations, but also need to focus on the variety and novelty of recommendations to improve user satisfaction. Currently, most of the existing recommendation systems focus on improving the accuracy and diversity of recommendation items, however, they usually do not consider the original user needs, and the potential relationship between diversity and novelty is not deeply explored. In addition to accuracy and diversity, we also consider novelty, and analyze the relationship between diversity and novelty (same place and different place), and propose an explainable recommendation system that integrates multiple (multidimensional) requirements such as accuracy, diversity, and novelty. The model combines semantic relations of knowledge graphs and multi-hop inference so as to analyze and consider the diversity and novelty requirements of users. Meanwhile, a recurrent neural network is used to construct a temporal multi-label classification network to predict users’ multidimensional demands and capture the dependencies between diversity and novelty demands. Finally, a composite reward function, including accuracy reward, diversity reward and novelty reward, is designed to implement a multi-demand, multi-decision recommendation method. Experiments are conducted on three real-world datasets, and the experimental results show that the model can guarantee the accuracy while improving the diversity and novelty of recommended items.},
  archive      = {J_APIN},
  author       = {Li, Yinggang and Tong, Xiangrong and Lv, Zhongming},
  doi          = {10.1007/s10489-024-05854-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Multi-dimensional requirements for reinforcement recommendation reasoning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISL-net: Dual-stream interaction network with task-optimized
modules for more accurate, complete iris segmentation and localization.
<em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-024-05862-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris images captured in uncooperative and unconstrained environments pose significant challenges for iris segmentation and localization owing to factors including high occlusions, specular reflections, motion blur, iris rotation, and off-angle images. To address this challenge, this paper proposes ISL-Net, a multitask segmentation network with a task-optimization module based on deep learning for joint iris segmentation and localization. We developed a dual-stream interactive module (DSIM) that combines dual-stream decoders to facilitate information exchange between tasks without interference. To optimize the iris-segmentation and iris-localization performance, we incorporated a balanced attention module (BAM) and a boundary-enhancement module (BEM) in the skip connections of the respective task stream decoders. The BEM recovers missing boundaries in iris localization, while the BAM focuses on uncertain areas in iris segmentation, enhancing the model’s ability to handle these regions. These modules complement each other, improving overall system performance without interference. The proposed model was evaluated on three challenging iris datasets, outperforming most existing models by achieving e1 index scores of 0.34, 0.79, and 0.61% and average normalized Hausdorff distances (HDs) of 0.7221, 1.1914, and 1.0396%. The results indicate that ISL-Net can generate normalized iris images with simple post-processing, making it suitable for direct application in existing iris-recognition systems.},
  archive      = {J_APIN},
  author       = {He, Lei and Yang, Xiaokai and Zheng, Jian and Liu, Zhaobang and Yang, Xiaoguo},
  doi          = {10.1007/s10489-024-05862-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {ISL-net: Dual-stream interaction network with task-optimized modules for more accurate, complete iris segmentation and localization},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDDP: Sensitive data detection method for user-controlled
data pricing. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06229-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, there is an urgent need for data sharing, in which data pricing is a crucial issue, because a reasonable price can not only enhance the willingness of users to share data but also promote the progress of data sharing. However, current research is mostly approached from the perspective of data sharing platforms, treating all data equally without sufficient evaluation of sensitive data within shared datasets and personalized perception of privacy from the users themselves. To address this problem, we detected sensitive data in each piece of data and then defined the pricing function based on information entropy and the user’s perception of sensitive information. To enhance the accuracy of sensitive data detection, we integrated an attention mechanism into a pre-trained model to comprehensively represent the samples. Subsequently, on the basis of automatically generating label correlation vectors to calculate the correlation matrix, a graph convolutional neural network was employed to mine the correlation between labels. Furthermore, based on the detection results, information entropy and user ratings are reasonably mapped to prices. Pricing based on user ratings is more suitable for pricing personal data rather than government or institutional data. The experimental results on the dataset of Twitter text sent by users have demonstrated that the average precision of our sensitive data detection model has improved by up to 9.26% compared to comparison models, and SDDP can provide reasonable pricing for samples containing sensitive data and fair compensation for users.},
  archive      = {J_APIN},
  author       = {Hu, Yuchuan and Hu, Bitao and Guo, Bing and Dai, Cheng and Shen, Yan},
  doi          = {10.1007/s10489-025-06229-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {SDDP: Sensitive data detection method for user-controlled data pricing},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale dual-stream visual feature extraction and graph
reasoning for visual question answering. <em>APIN</em>, <em>55</em>(6),
1–18. (<a href="https://doi.org/10.1007/s10489-025-06325-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning algorithms have significantly expanded the capabilities of systems to handle vision-to-language (V2L) tasks. Visual question answering (VQA) presents challenges that require a deep understanding of visual and language content to perform complex reasoning tasks. The existing VQA models often rely on grid-based or region-based visual features, which capture global context and object-specific details, respectively. However, balancing the complementary strengths of each feature type while minimizing fusion noise remains a significant challenge. This study propose a multi-scale dual-stream visual feature extraction method that combines grid and region features to enhance both global and local visual feature representations. Also, a visual graph relational reasoning (VGRR) approach is proposed to further improve reasoning by constructing a graph that models spatial and semantic relationships between visual objects, using Graph Attention Networks (GATs) for relational reasoning. To enhance the interaction between visual and textual modalities, we further propose a cross-modal self-attention fusion strategy, which enables the model to focus selectively on the most relevant parts of both the image and the question. The proposed model is evaluated on the VQA 2.0 and GQA benchmark datasets, demonstrating competitive performance with significant accuracy improvements compared to state-of-the-art methods. Ablation studies confirm the effectiveness of each module in enhancing visual-textual understanding and answer prediction.},
  archive      = {J_APIN},
  author       = {Yusuf, Abdulganiyu Abdu and Feng, Chong and Mao, Xianling and Li, Xinyan and Haruna, Yunusa and Duma, Ramadhani Ally},
  doi          = {10.1007/s10489-025-06325-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale dual-stream visual feature extraction and graph reasoning for visual question answering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Locomotion mode prediction in real-life walking with and
without ankle–foot exoskeleton assistance. <em>APIN</em>,
<em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06416-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exoskeletons can assist human locomotion in real-life scenarios, but existing tools for decoding locomotion modes (LMs) focus on recognition rather than prediction, which can lead to delayed assistance. This study proposes a long short-term memory (LSTM) neural network to predict five LMs (level-walking, ramp ascent/descent, stair ascent/descent) with greater lead time compared to state-of-the-art methods. We examined the optimal sequence length (SL) for LSTM-based LM prediction, using data from inertial sensors placed on the lower limbs and the lower back, along with a waist-mounted infrared laser. Ten subjects walked in real-life scenarios, both with and without an ankle–foot exoskeleton. Results show that a 1-s SL provides the most advanced and accurate LM prediction, outperforming SLs of 0.6, 0.8, and 1.2 s. The proposed LSTM model achieved an accuracy of 98 ± 0.31%, predicting LMs 0.66 s in advance (for an average stride time of 1.98 ± 0.83 s). Level-walking presented more misclassifications, and the model primarily relied on inertial data over laser input. Overall, these findings demonstrate the LSTM’s strong predictive capability for both assisted and non-assisted walking and independent of which limb executes the transition, supporting its applicability for exoskeleton-assisted locomotion.},
  archive      = {J_APIN},
  author       = {Carvalho, Simão P. and Figueiredo, Joana and Cerqueira, João J. and Santos, Cristina P.},
  doi          = {10.1007/s10489-025-06416-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Locomotion mode prediction in real-life walking with and without ankle–foot exoskeleton assistance},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scale-cross non-local network with higher-level semantics
guidance for smoke segmentation. <em>APIN</em>, <em>55</em>(6), 1–17.
(<a href="https://doi.org/10.1007/s10489-025-06420-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoke semantic segmentation (SSS) is particularly challenging task due to the various patterns of the target itself, which are caused by the characteristics of smoke, like, non-rigid, translucent, fuzzy, environment-sensitive, and so forth. This paper tailor-makes the Scale-Cross Non-Local Network (SCNN) for Smoke Segmentation, aiming to accurately locate the position of smoke in complex scenes. While non-local enjoys the bonus of the excellent competence in modeling long-range contextual dependencies acquired by self-attention, the constraint on single-scale input and the suitability for low-resolution feature erode its capability in information representation. To address these issues, we bespoke a Scale-Cross Non-Local (SCNL) module to better integrate local features with global dependencies. In practical scenes, diverse non-smoke objects sharing similarity with smoke pose great obstacles to accurate location of smoke. As a solution, we design a Pyramid Irregular Convolution (PIC) module containing rich high-level semantic to further refine the feature representation of segmentation task. By supervising classification task, the high-level semantics obtained can guide the segmentation feature to correct semantic errors at the image level and alleviate the issue of between-class similarity. To assess its generalization ability, we empirically evaluate our SCNN on extensive synthetic and real data. Experimental results demonstrate that SCNN achieves state-of-the-art performance, exhibiting enhanced smoke localization, accuracy in boundary detection, and a significant reduction in the false segmentation rate for smoke-like objects.},
  archive      = {J_APIN},
  author       = {Zhang, Lin and Wu, Jing and Zhao, Yun and Yuan, Feiniu},
  doi          = {10.1007/s10489-025-06420-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {A scale-cross non-local network with higher-level semantics guidance for smoke segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HKGAT: Heterogeneous knowledge graph attention network for
explainable recommendation system. <em>APIN</em>, <em>55</em>(6), 1–19.
(<a href="https://doi.org/10.1007/s10489-025-06446-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the Heterogeneous Knowledge Graph Attention Network (HKGAT) for recommendation systems. As recommendation technology evolves, systems now emphasize diversity, fairness, and explainability alongside accuracy. Traditional methods encounter issues integrating knowledge graphs and lack explainability. HKGAT addresses these by leveraging heterogeneous knowledge graphs. It consists of a heterogeneous information aggregation layer, an attention-aware heterogeneous relation fusion layer, and a prediction layer. First, recommendation data forms a user-item knowledge graph. Then, the aggregation layer collects relation information, followed by the fusion layer integrating it for higher-order feature representations. The prediction layer combines link prediction and recommendation score prediction. Additionally, paths of top-ten results are analyzed and quantified for explainability to optimize ranking. Experiments on self-constructed and Amazon-book datasets show HKGAT outperforms baselines like HetGCN, with significant improvements in Precision, Recall, F1 score, and NDCG@10, and a notable 1.9% gain in NDCG@10 from explainable ranking optimization.},
  archive      = {J_APIN},
  author       = {Zhang, Yongchuan and Tian, Jiahong and Sun, Jing and Chan, Huirong and Qiu, Agen and Liu, Cailin},
  doi          = {10.1007/s10489-025-06446-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {HKGAT: Heterogeneous knowledge graph attention network for explainable recommendation system},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging CQT-VMD and pre-trained AlexNet architecture for
accurate pulmonary disease classification from lung sound signals.
<em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06452-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel algorithm for classifying pulmonary diseases using lung sound signals by integrating Variational Mode Decomposition (VMD) and the Constant-Q Transform (CQT) within a pre-trained AlexNet convolutional neural network. Breathing sounds from the ICBHI and KAUHS databases are analyzed, where three key intrinsic mode functions (IMFs) are extracted using VMD and subsequently converted into CQT-based time-frequency representations. These images are then processed by the AlexNet model, achieving an impressive classification accuracy of 93.30%. This approach not only demonstrates the innovative synergy of CQT-VMD for lung sound analysis but also underscores its potential to enhance computerized decision support systems (CDSS) for pulmonary disease diagnosis. The results, showing high accuracy, a sensitivity of 91.21%, and a specificity of 94.9%, highlight the robustness and effectiveness of the proposed method, paving the way for its clinical adoption and the development of lightweight deep-learning algorithms for portable diagnostic tools. Overview of the proposed methodology for pulmonary disease classification using CQT-VMD and pre-trained AlexNet architecture applied to lung sound signals},
  archive      = {J_APIN},
  author       = {Neili, Zakaria and Sundaraj, Kenneth},
  doi          = {10.1007/s10489-025-06452-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Leveraging CQT-VMD and pre-trained AlexNet architecture for accurate pulmonary disease classification from lung sound signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OTMKGRL: A universal multimodal knowledge graph
representation learning framework using optimal transport and
cross-modal relation. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06459-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for integrating multimodal information, such as text and images, has grown significantly as it enables richer and more comprehensive knowledge representations. Most existing multimodal knowledge graph representation learning (KGRL) methods focus primarily on fusing multimodal entity information, directly applying multimodal entities and single-modal relations to downstream tasks. However, these methods face challenges related to the heterogeneity of multi-source entity data, which amplifies the differences in feature distributions between entity and relation representations. To address these challenges, we propose a universal multimodal KGRL framework, OTMKGRL, which seamlessly incorporates multimodal information into three types of single-modal KGRL methods. First, OTMKGRL employs Tucker decomposition to project entity text and image data into a shared space, thereby generating multimodal entity representations. It then uses optimal transport to integrate multimodal entity information into the original single-modal entity representations. Second, OTMKGRL introduces a cross-modal relation attention mechanism that fuses effective multimodal entity features into the original single-modal relations, yielding cross-modal relation representations. Extensive experiments across three multimodal datasets demonstrate the effectiveness and versatility of our approach. The OTMKGRL framework significantly enhances the performance of existing single-modal KGRL models in multimodal settings.},
  archive      = {J_APIN},
  author       = {Wang, Tao and Shen, Bo},
  doi          = {10.1007/s10489-025-06459-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {OTMKGRL: A universal multimodal knowledge graph representation learning framework using optimal transport and cross-modal relation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-based attention deep q-network with prior-based
knowledge. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-024-05850-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based reinforcement learning (RL) is a potent algorithm for addressing tasks related to visual behavioural decision-making; nevertheless, it operates as a black-box, directly training models with images as input in the end-to-end fashion. Therefore, to elucidate the underlying mechanisms of the model and the agent’s focus on different features during the decision-making process, a vision-based attention (VA) mechanism is introduced into vision-based RL in this paper. A prior-based mechanism is introduced to address the issue of instability in the attention maps observed by the agent when attention mechanisms are directly integrated into network updates that results in an increase in single-step errors and larger cumulative errors. Thus, a vision-based attention deep Q-network (VADQN) method with a prior-based mechanism is proposed. Specifically, prior attention maps are obtained using a learnable Gaussian filtering and a spectral residual method. Next, the attention maps are fine-tuned using a self-attention (SA) mechanism to enhance their performance. During training, both the attention maps and the parameters of the policy network are concurrently trained to ensure explanations of the regions of interest during online training. Finally, a series of ablation experiments are conducted on Atari games to compare the proposed method with humans, convolutional neural networks, and other approaches. The results demonstrate that the proposed method not only reveals the regions of interest attended to by DRL during the decision-making process but also enhances DRL performance in certain scenarios. This approach provides valuable insights for understanding and improving the performance of DRL in visual decision-making tasks.},
  archive      = {J_APIN},
  author       = {Ma, Jialin and Li, Ce and Hong, Liang and Wei, Kailun and Zhao, Shutian and Jiang, Hangfei and Qu, Yanyun},
  doi          = {10.1007/s10489-024-05850-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Vision-based attention deep q-network with prior-based knowledge},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised text classification method based on
three-way decision with evidence theory. <em>APIN</em>, <em>55</em>(6),
1–15. (<a href="https://doi.org/10.1007/s10489-024-06129-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning methods play a crucial role in text classification tasks. However, due to limitation of scarce labeled training data, the uncertainty of pseudo labels is still an unavoidable problem in semi-supervised text classification. To address this issue, this paper introduces three-way decision theory into semi-supervised text classification model, which divides the model output pseudo-labeled samples into different regions and adopts different processing strategies. The accurate and effective pseudo-labeled samples are selected as much as possible to expand the original training set. For the pseudo-labeled outputs by the model, we use evidence theory to fuse the probability outputs of the samples to improve the stability and credibility of pseudo labels. Experimental results demonstrate that the method introduced in this paper effectively enhances the accuracy of semi-supervised text classification while exhibiting high stability.},
  archive      = {J_APIN},
  author       = {Yang, Ziping and Jiang, Chunmao and Huang, Chunmei},
  doi          = {10.1007/s10489-024-06129-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised text classification method based on three-way decision with evidence theory},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse pinball universum nonparallel support vector machine
and its safe screening rule. <em>APIN</em>, <em>55</em>(6), 1–33. (<a
href="https://doi.org/10.1007/s10489-025-06356-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparallel support vector machine (NPSVM) is an effective and popular classification technique, which introduces the $$\epsilon $$ -insensitive loss function instead of the quadratic loss function in twin support vector machine (TSVM), making the model have the same sparsity and kernel strategy as support vector machine (SVM). However, NPSVM is sensitive to noise points and does not utilize the prior knowledge embedded in the unlabeled samples. Therefore, to improve its generalization ability and robustness, a sparse pinball Universum nonparallel support vector machine (SPUNPSVM) is first proposed in this paper. On the one hand, the sparse pinball loss is employed to enhance the robustness. On the other hand, it exploits the Universum data, which do not belong to any class, to embed prior knowledge into the model. Numerical experiments have verified its effectiveness. Furthermore, to further speed up SPUNPSVM, we propose a safe screening rule (SSR-SPUNPSVM) based on its sparsity, which achieves acceleration without sacrificing accuracy. Numerical experiments and statistical tests demonstrate the superiority of our SSR-SPUNPSVM.},
  archive      = {J_APIN},
  author       = {Wang, Hongmei and Li, Ping and Zheng, Yuyan and Jiang, Kun and Xu, Yitian},
  doi          = {10.1007/s10489-025-06356-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-33},
  shortjournal = {Appl. Intell.},
  title        = {Sparse pinball universum nonparallel support vector machine and its safe screening rule},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DGCformer: 3-dimensional graph convolutional transformer
for multi-step origin–destination matrix forecasting. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06371-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting Human mobility is of great significance in the simulation and control of infectious diseases like COVID-19. To get a clear picture of potential future outbreaks, it is necessary to forecast multi-step Origin–Destination (OD) matrices for a relatively long period in the future. However, multi-step Origin–Destination Matrix Forecasting (ODMF) is a non-trivial problem. First, previous ODMF models only forecast the OD matrix for the next time-step, and they cannot perform well on long-term multi-step forecasts due to error accumulation. Second, many ODMF methods capture spatial and temporal dependencies with separate modules, which is insufficient to model spatio-temporal correlations in the time-varying OD matrix sequence. To address the challenges in multi-step ODMF, we propose 3-Dimensional Graph Convolutional Transformer (3DGCformer). As an enhancement of the original 3DGCN, we propose a novel Origin–Destination Feature Propagation (ODFP) rule between 3DGCN layers and integrate 2 3DGCNs with different spatio-temporal graphs and corresponding feature propagation rules to model the formation of OD flows in a more comprehensive way. For multi-step forecasts, 3DGCformer uses Transformer to capture long-term global temporal dependency, and adapt its decoder using labeled tokens to avoid error accumulation and improve time efficiency. To avoid information loss as the number of regions increases, we propose a patch embedding approach to convert data from 3DGCNs to the Transformer module. We perform extensive experiments on 4 real-world human mobility datasets, and the results show that our proposed model outperforms the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Huang, Yiou and Deng, Hao and Zhao, Shengjie},
  doi          = {10.1007/s10489-025-06371-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {3DGCformer: 3-dimensional graph convolutional transformer for multi-step origin–destination matrix forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HRMG-EA: Heterogeneous graph neural network recommendation
with multi-level guidance based on enhanced-attributes. <em>APIN</em>,
<em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06428-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Graph Neural Networks are an efficient and powerful tool for modeling graph structure data in recommendation systems. However, existing heterogeneous graph neural networks often fail to model the dependencies between user and item attribute preferences, limiting graph structure optimization and consequently reducing the accuracy of recommendations. To overcome these issues, we propose a Heterogeneous graph neural network Recommendation with Multi-level Guidance based on Enhanced-Attributes (HRMG-EA). First, we design an attribute enhanced gated network to model user-item interaction attribute scenarios and obtain enhanced-attributes by capturing complex attribute dependencies. It effectively avoids the expansion of the graph scale in attribute graph scenarios and further covers personalized attribute relationship distribution characteristics of users and items. Then, we propose a novel multi-level graph structure guidance strategy based on enhanced-attributes. It guides graph structure learning from three optimization levels, optimizing from two perspectives: explicit (heterogeneity and homogeneity) and implicit (contrast enhancement). The former can screen higher-quality heterogeneous neighbor nodes in a direct interaction environment, and filter out redundant or erroneous edges under different similar semantic interest paths to improve the quality of the neighborhood environment. The latter aligns representation embeddings of enhanced-attributes and graph structure in a latent space, explores their potential commonalities, and obtains more comprehensive, fine-grained semantic and beneficial structural information. Finally, on two real-world datasets, HRMG-EA significantly outperforms the state-of-the-art baseline algorithms in both recall and normalized discounted cumulative gain. A large number of ablation experiments and analytical verifications also verify its effectiveness.},
  archive      = {J_APIN},
  author       = {Wang, Longtao and Yuan, Guiyuan and Li, Chao and Zhao, Yufei and Duan, Hua and Zeng, Qingtian},
  doi          = {10.1007/s10489-025-06428-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {HRMG-EA: Heterogeneous graph neural network recommendation with multi-level guidance based on enhanced-attributes},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separable n-soft sets: A tool for multinary descriptions
with large-scale parameter sets. <em>APIN</em>, <em>55</em>(6), 1–37.
(<a href="https://doi.org/10.1007/s10489-025-06435-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft set theory builds on the idea of a parameterized family of subsets of a universal set, where for each pertinent characteristic, any specific member of the universe either satisfies it or not. The concept of an N-soft set sharpens this model with the aid of multinary parameterized descriptions; that is, N-soft sets categorize the options in terms of multiple classifications of the characteristics. The aim of this research is fourfold. First, this research focuses on daily-life decision-making problems that involve both positive and negative attributes that can be naturally distributed among classes. Each comparable group of attributes produces an N-soft set, and we can represent all these N-soft sets using separable N-soft sets. We show that this structure facilitates decision-making in the presence of large numbers of attributes. Second, to develop tools that provide a mechanism for the selection of an alternative in this new model, we first develop a complement operator for N-soft sets to uniformize the data, and then, we propose strategies for taking advantage of the qualities of the attributes. Aggregation operators are employed to aggregate the data into a resultant N-soft set, a fuzzy N-soft set, or a hesitant N-soft set. Several algorithmic procedures are proposed to define these methods. Third, we define the novel notion of a multihesitant N-soft set. This loosely defined concept is helpful for representing data with multiple and repetitive entries while avoiding information loss. Finally, we provide solutions to several real-life decision-making problems to illustrate the versatility of our approaches. We apply this theory to construct a new method for ranking countries participating in the Olympic Games. Our motivation is that the existing lexicographic procedure is unable to distinguish among gold, silver, and bronze medals won at sports with very different characteristics.},
  archive      = {J_APIN},
  author       = {Khan, Muhammad Jabir and Alcantud, Jose Carlos R. and Akram, Muhammad and Ding, Weiping},
  doi          = {10.1007/s10489-025-06435-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Appl. Intell.},
  title        = {Separable N-soft sets: A tool for multinary descriptions with large-scale parameter sets},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dark-ControlNet: An enhanced dehazing universal plug-in
based on the dark channel prior. <em>APIN</em>, <em>55</em>(6), 1–16.
(<a href="https://doi.org/10.1007/s10489-025-06439-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing dehazing models have excellent performance in synthetic scenes but still face the challenge of low robustness in real scenes. In this paper, we propose Dark-ControlNet, a generalized and enhanced dehazing plug-in that uses the dark channel prior as a control condition, which can be deployed on existing dehazing models and can be simply fine-tuned to enhance their robustness in real scenes while improving their dehazing performance. We first freeze the backbone network to preserve its encoding and decoding capabilities and input the dark channel prior with high robustness as conditional information to the plug-in network to obtain prior knowledge. Then, we fuse the dark channel prior features into the backbone network in the form of mean-variance alignment via the Haze&amp;Dark(HD) module and guide the backbone network to decode clear images by fine-tuning the plug-in network. The experimental results show that the existing dehazing model enhanced by Dark-ControlNet performs well on synthetic datasets and real datasets.},
  archive      = {J_APIN},
  author       = {Yang, Yu and Yin, Xuesong and Wang, Yigang},
  doi          = {10.1007/s10489-025-06439-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Dark-ControlNet: An enhanced dehazing universal plug-in based on the dark channel prior},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-enhanced and decomposed transformer for
multivariate time series anomaly detection. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06441-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of the Internet of Things (IoT), vast amounts of multivariate time series data are generated, which reflect the operational status of systems. Accurate and efficient anomaly detection in these data is crucial for maintaining system stability. However, data from unstable environments often exhibit high volatility, data drift, and complex patterns of anomalies. Unsupervised anomaly detection models are typically designed for stable data and lack generalizability, leading to a high rate of false positives when applied to unstable data. This paper introduces the frequency-enhanced and decomposed transformer for anomaly detection (FDTAD), which is a novel anomaly detection model based on a transformer that is enhanced with frequency and time series decomposition. FDTAD addresses data drift by decomposing time series and leverages both time-domain and frequency-domain information to improve the generalization ability of the model. The model preserves major amplitudes in the frequency domain to extract primary periodic patterns, uses spectral residuals to capture detailed variations, and incorporates a frequency-domain correlation attention mechanism to extract dependencies in frequency-domain data in a sparse representation. Additionally, a spatiotemporal module is designed to extract the temporal correlations in the data and spatial correlations among the data with different attributes. FDTAD combines a data periodic pattern reconstructor and a data detailed pattern reconstructor through an adversarial mechanism to achieve maximum accuracy in reconstructing normal data. Extensive experiments on 10 public datasets demonstrate that FDTAD outperforms state-of-the-art baseline methods, with a 4.1% improvement in the F1 score and a 4.7% improvement in precision.},
  archive      = {J_APIN},
  author       = {Li, Shijiang and Wang, Zhihai and Wang, Xiaokang and Yin, Zihao and Yao, Muyun},
  doi          = {10.1007/s10489-025-06441-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Frequency-enhanced and decomposed transformer for multivariate time series anomaly detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing graph representation learning via type-aware
decoupling and node influence allocation. <em>APIN</em>, <em>55</em>(6),
1–14. (<a href="https://doi.org/10.1007/s10489-025-06443-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional graph representation methods can fit the information of graph with low-dimensional vectors, but they cannot interpret their composition, resulting in insufficient security. Graph decoupling, as a method of graph representation, can analyze the latent factors composing the graph representation vectors. However, in current graph decoupling methods, the number of factors is a hyperparameter, and enforce uniform decoupling vector dimensions which leads to information loss or redundancy. To address these issues, we propose a type-aware graph decoupling based on influence called Variational Graph Decoupling Auto-Encoder (VGDAE). It uses node labels as interpretable and objectively existing natural semantics for decoupling and allocates embedding space based on node influence, addressing the issues of manually setting the number of factors in traditional graph decoupling and the mismatch between node information size and embedding space. On the Cora, Citeseer, and fb-CMU datasets, VGDAE shows the impact of different node classes as decoupling targets on classification tasks. Furthermore, we perform visualization of the representations, VGDAE exhibits performance improvements of 2% in classification tasks and 12% in clustering tasks when compared with baseline models.},
  archive      = {J_APIN},
  author       = {Zhu, Guochang and Hu, Jun and Liu, Li and Zhang, Qinghua and Wang, Guoyin},
  doi          = {10.1007/s10489-025-06443-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing graph representation learning via type-aware decoupling and node influence allocation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REFD: Recurrent encoder and fusion decoder for temporal
knowledge graph reasoning. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06445-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning over Temporal Knowledge Graphs (TKGs) presents challenges in modeling the dynamic relationships and evolving behaviors of entities and relations over time. Traditional approaches often treat entities and relations separately, which limits their ability to capture their joint temporal evolution and interactions. To overcome these limitations, REFD (Recurrent Encoder and Fusion Decoder) is proposed, a novel framework designed to improve TKG reasoning. The REFD framework consists of two primary components: a recurrent encoder and a fusion decoder. The recurrent encoder incorporates three key modules: (1) the full-domain multi-scale temporal recurrent encoder, which effectively captures temporal dependencies across varying time scales, (2) the entity-relation symbiotic temporal feature deep fusion engine, which integrates temporal features of both entities and relations, and (3) the intelligent temporal feature priority dynamic adjustment mechanism, which adaptively adjusts the importance of different features over time. The fusion decoder, particularly the entity-relation feature Fusion Decoder, combines the temporal features of entities and relations to model their joint evolution, overcoming the limitations of previous methods that model them separately. By jointly capturing the evolving dynamics of entities and relations over time, REFD significantly enhances the accuracy of temporal reasoning tasks. Experimental results show that REFD outperforms existing approaches, offering superior prediction accuracy and better handling of the complexities in TKGs.},
  archive      = {J_APIN},
  author       = {Liu, Qian and Feng, Siling and Huang, MengXing and Bhatti, Uzair Aslam},
  doi          = {10.1007/s10489-025-06445-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {REFD: Recurrent encoder and fusion decoder for temporal knowledge graph reasoning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mastering table tennis with hierarchy: A reinforcement
learning approach with progressive self-play training. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06450-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical Reinforcement Learning (HRL) is widely applied in various complex task scenarios. In complex tasks where simple model-free reinforcement learning struggles, hierarchical design allows for more efficient utilization of interactive data, significantly reducing training costs and improving training success rates. This study delves into the use of HRL based on the model-free policy layer to learn complex strategies for a robotic arm playing table tennis. Through processes such as pre-training, self-play training, and self-play training with top-level winning strategies, the robustness of the lower-level hitting strategies has been enhanced. Furthermore, a novel decay reward mechanism has been employed in the training of the higher-level agent to improve the win rate in adversarial matches against other methods. After pre-training and adversarial training, we achieved an average of 52 rally cycles for the forehand strategy and 48 rally cycles for the backhand strategy in testing. The high-level strategy training based on the decay reward mechanism resulted in an advantageous score when competing against other strategies.},
  archive      = {J_APIN},
  author       = {Ma, Hongxu and Fan, Jianyin and Xu, Haoran and Wang, Qiang},
  doi          = {10.1007/s10489-025-06450-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Mastering table tennis with hierarchy: A reinforcement learning approach with progressive self-play training},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new deep learning-based approach for predicting the
geothermal heat pump’s thermal power of a real bioclimatic house.
<em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06457-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, growing concern about climate change and the need to reduce greenhouse gas emissions have highlighted the role of energy efficiency and sustainability on the global agenda. Energy policies are decisive in establishing regulatory frameworks and incentives to address these challenges, leading to an inclusive and more resilient energy transition. In this context, geothermal energy is an essential source of renewable, low-emission energy, capable of providing heat and electricity sustainably. The present research focuses on a bioclimatic house’s geothermal energy system based on a heating pump and a horizontal heat exchanger. The main aim is to predict the generated thermal power of the heat pump using historical data from several sensors. In particular, two approaches were proposed with both uni-variate and multi-variate scenarios. Several deep learning techniques were applied: LSTM, GRU, 1D-CNN, CNN-LSTM, and CNN-GRU, obtaining satisfactory results over the whole dataset, which comprised one year of data acquisition. Specifically, promising results have been achieved using hybrid methods combining recurrent-based and convolutional neural networks.},
  archive      = {J_APIN},
  author       = {Zayas-Gato, Francisco and Díaz-Longueira, Antonio and Arcano-Bea, Paula and Michelena, Álvaro and Calvo-Rolle, Jose Luis and Jove, Esteban},
  doi          = {10.1007/s10489-025-06457-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A new deep learning-based approach for predicting the geothermal heat pump’s thermal power of a real bioclimatic house},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An approach to software defect prediction for small-sized
datasets. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06458-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction (SDP) is an active research subject in the software engineering domain. The earlier works on SDP use the same project’s data for prediction in future releases, called within-project defect prediction (WPDP). WPDP may not perform well when the data available for training is small in size. In this work, to address the issue of small-size data, we suggest enhancing the data by borrowing data from other software projects. For better prediction accuracy of learning models, both train and test data must follow the same distribution. However, this may not be true in the case of data being transferred from the other project. Data from different projects may follow different distributions. So, to handle this issue, we have proposed a data preprocessing method, namely data transfer-based WPDP (DT-WPDP). Next, we have shown the use of the deep neural network (DNN) for WPDP and compared it with other classical machine learning (ML) models such as k nearest neighbor, decision tree, logistic regression, and Naive Bayes classifiers. Further, we have performed experimental analysis to assess the effect of the proposed DT-WPDP data preprocessing method with DNN and other ML models. Experimental results show that the proposed approach significantly improves the accuracies of different models. Among different models, the DNN model performed best for all datasets. In the case of very small-sized datasets, which is our main concern in this work, the accuracy of the DNN model is improved by 7% after using the proposed approach.},
  archive      = {J_APIN},
  author       = {Bal, Pravas Ranjan and Shukla, Suyash and Kumar, Sandeep},
  doi          = {10.1007/s10489-025-06458-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {An approach to software defect prediction for small-sized datasets},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TableGPT: A novel table understanding method based on table
recognition and large language model collaborative enhancement.
<em>APIN</em>, <em>55</em>(5), 1–25. (<a
href="https://doi.org/10.1007/s10489-024-05937-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s information age, table images play a crucial role in storing structured information, making table image recognition technology an essential component in many fields. However, accurately recognizing the structure and text content of various complex table images has remained a challenge. Recently, large language models (LLMs) have demonstrated exceptional capabilities in various natural language processing tasks. Therefore, applying LLMs to the correction tasks of structure and text content after table image recognition presents a novel solution. This paper introduces a new method, TableGPT, which combines table recognition with LLMs and develops a specialized multimodal agent to enhance the effectiveness of table image recognition. Our approach is divided into four stages. In the first stage, TableGPT_agent initially evaluates whether the input is a table image and, upon confirmation, uses algorithms such as the transformer for preliminary recognition. In the second stage, the agent converts the recognition results into HTML format and autonomously assesses whether corrections are needed. If corrections are needed, the data are input into a trained LLM to achieve more accurate table recognition and optimization. In the third stage, the agent evaluates user satisfaction through feedback and applies superresolution algorithms to low-quality images, as this is often the main reason for user dissatisfaction. Finally, the agent inputs both the enhanced and original images into the trained model, integrating the information to obtain the optimal table text representation. Our research shows that trained LLMs can effectively interpret table images, improving the Tree Edit Distance Similarity (TEDS) score by an average of 4% even when based on the best current table recognition methods, across both public and private datasets. They also demonstrate better performance in correcting structural and textual errors. We also explore the impact of image superresolution technology on low-quality table images. Combined with the LLMs, our TEDS score significantly increased by 54%, greatly enhancing the recognition performance. Finally, by leveraging agent technology, our multimodal model improved table recognition performance, with the TEDS score of TableGPT_agent surpassing that of GPT-4 by 34%.},
  archive      = {J_APIN},
  author       = {Ren, Yi and Yu, Chenglong and Li, Weibin and Li, Wei and Zhu, Zixuan and Zhang, TianYi and Qin, ChenHao and Ji, WenBo and Zhang, Jianjun},
  doi          = {10.1007/s10489-024-05937-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {TableGPT: A novel table understanding method based on table recognition and large language model collaborative enhancement},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSTM-SVM-weibull modeling for decommissioning amount
prediction of power batteries based on attention mechanism and ISPBO
algorithm. <em>APIN</em>, <em>55</em>(5), 1–29. (<a
href="https://doi.org/10.1007/s10489-024-05941-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking Shanghai as the research area, fully considering the randomness and timeliness of power battery recycling, a combined prediction model of Long Short-term Memory network—Support Vector Machine—Weibull (LSTM-SVM-Weibull) that integrates attention mechanism and hyperparameters optimized by improved student psychology based optimization (ISPBO) algorithm is proposed to predict the retired amount of power batteries, to further improve the prediction accuracy. In the first stage, the grey relational analysis (GRA) method is used to screen out the strong related influence factors of power battery installed amount. In the second stage, a two-stage predictive model of power battery installed amount based on LSTM network with attention mechanism (Attention-LSTM) and SVM optimized by ISPBO algorithm (ISPBO-SVM) is constructed. Firstly, the attention mechanism is fused in the hidden layer of the LSTM network to accurately predict the various indicators selected by GRA, reduce the impact of indicator value errors on the target value prediction, and highlight the contribution degree of the input sequence at different time prediction points; Then, the Lévy flight strategy is introduced and combined with the Metropolis criterion of the simulated annealing algorithm to accept inferior solutions, the ISPBO algorithm is designed to optimize the hyperparameters of SVM model, so as to predict the target value (power battery installed amount) on the basis of future indicator values by training the ISPBO-SVM target prediction layer. By collecting the historical data of power battery installed amount in Shanghai for simulation experiments, the comparison results show that the designed Attention-LSTM-ISPBO-SVM two-stage power battery installed amount prediction model is significantly better than other comparison models in terms of error and accuracy on different data sets, and it has high accuracy and generalization ability for the prediction of power battery retirement amount. In the third stage, based on the predictive model of power battery installed amount, the Weibull life distribution model is combined to predict the retired amount of power batteries in Shanghai. Through the goodness-of-fit test and evaluating the retirement amount prediction results of different models, it is proved that the predictive model for power battery retired amount based on Weibull life distribution has high stability and practical application value, which effectively reflects the overall change trend of the future power battery retirement amount in Shanghai, and can provide data reference for improving the recovery rate of waste batteries.},
  archive      = {J_APIN},
  author       = {Zhao, Mengna and Chen, Shiping},
  doi          = {10.1007/s10489-024-05941-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-29},
  shortjournal = {Appl. Intell.},
  title        = {LSTM-SVM-weibull modeling for decommissioning amount prediction of power batteries based on attention mechanism and ISPBO algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy constrained fairness estimation for decision trees.
<em>APIN</em>, <em>55</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10489-024-05953-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protection of sensitive data becomes more vital, as data increases in value and potency. Furthermore, the pressure increases from regulators and society on model developers to make their Artificial Intelligence (AI) models non-discriminatory. To boot, there is a need for interpretable, transparent AI models for high-stakes tasks. In general, measuring the fairness of any AI model requires the sensitive attributes of the individuals in the dataset, thus raising privacy concerns. In this work, the trade-offs between fairness (in terms of Statistical Parity (SP)), privacy (quantified with a budget), and interpretability are further explored in the context of Decision Trees (DTs) as intrinsically interpretable models. We propose a novel method, dubbed Privacy-Aware Fairness Estimation of Rules (PAFER), that can estimate SP in a Differential Privacy (DP)-aware manner for DTs. Our method is the first to assess algorithmic fairness on a rule-level, providing insight into sources of discrimination for policy makers. DP, making use of a third-party legal entity that securely holds this sensitive data, guarantees privacy by adding noise to the sensitive data. We experimentally compare several DP mechanisms. We show that using the Laplacian mechanism, the method is able to estimate SP with low error while guaranteeing the privacy of the individuals in the dataset with high certainty. We further show experimentally and theoretically that the method performs better for those DTs that humans generally find easier to interpret.},
  archive      = {J_APIN},
  author       = {van der Steen, Florian and Vink, Fré and Kaya, Heysem},
  doi          = {10.1007/s10489-024-05953-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {Privacy constrained fairness estimation for decision trees},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised motion forecasting with local information
interaction in autonomous driving. <em>APIN</em>, <em>55</em>(5), 1–13.
(<a href="https://doi.org/10.1007/s10489-024-06030-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion forecasting presents significant challenges critical for ensuring the safety of autonomous driving systems. The accuracy of these forecasts relies heavily on factors such as map topology and the behaviors of vehicles and pedestrians. However, within vast datasets, certain features with unique properties, capable of enhancing representation generalization often remain hidden and overlooked. While self-supervised learning (SSL) has shown promise in uncovering such hidden features through pretext tasks, its application to motion forecasting remains underexplored. In this paper, we propose a novel self-supervised motion forecasting method that exploits the interaction of map topology and actors’ maneuvers within localized focal points to generate more informative and generalizable representations for forecasting task. Since intersections, characterized by intricate structures and frequent motion state changes among actors, serve as pivotal locations where the topology of the intersection map profoundly influences actors’ intentions to change course, we leverage this interplay by calculating map structure-based actors’ attributes, and actors’ maneuver-based map attributes. These attributes yield significant advantages for motion forecasting tasks. Experimentally, our proposed method outperforms the baseline on both the challenging large-scale Argoverse benchmark (Chang et al. 2019) and local test, which demonstrates the effectiveness of the fusion of cross-domain information in a local neighborhood.},
  archive      = {J_APIN},
  author       = {Lei, Xinyu and Liu, Longjun and Li, Haoteng and Zhang, Haonan},
  doi          = {10.1007/s10489-024-06030-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised motion forecasting with local information interaction in autonomous driving},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous navigation of UAV in complex environment: A deep
reinforcement learning method based on temporal attention.
<em>APIN</em>, <em>55</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s10489-024-06036-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for Unmanned Aerial Vehicles (UAVs) in both military and civil applications, the ability for UAVs to automatically avoid obstacles and navigate to specific destinations has been receiving growing attention. However, most current methods focus on environments where global information is available or both destination and obstacles are static, which are not suitable for dense, dynamic, complex real-time tasks. Therefore, we propose a novel autonomous navigation method based on Deep Reinforcement Learning (DRL), which is suitable for more complex environments. Based on the Soft Actor-Critic (SAC) algorithm, this method incorporates changes of the state space into network input with a temporal attention mechanism, which allows UAVs to adaptively extract key information from historical environments while maintaining sensitivity to the current environment. We establish a visualized two-dimensional navigation task environment and design different simulation tests to evaluate its the performance and generalization. Results show that compared to baselines, our algorithm can achieve higher average rewards and more stable convergence after training in a static multi-obstacle environment, and can demonstrate better performance in environments featuring multiple obstacles of varying numbers, sizes, and speeds, thereby achieving a balance between task completion efficiency and security.},
  archive      = {J_APIN},
  author       = {Liu, Shuyuan and Zou, Shufan and Chang, Xinghua and Liu, Huayong and Zhang, Laiping and Deng, Xiaogang},
  doi          = {10.1007/s10489-024-06036-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Autonomous navigation of UAV in complex environment: A deep reinforcement learning method based on temporal attention},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SSSA: Low data sentiment analysis using boosting
semi-supervised approach and deep feature learning network.
<em>APIN</em>, <em>55</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s10489-024-06071-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is the process of determining the expressive direction of the user reviews. Recently, sentiment analysis gets more attention. However, low data sentiment analysis receives less attention. The existing works try to augment the samples to consider this issue. In this study, we have utilized a semi-supervised approach to propose a new approach for low-data sentiment analysis. To do so, we have utilized pre-trained XLNet as a feature extractor network to initialize the feature vector for each tweet. Next, these initial representations are fed into the embedding update module to map features into the new space by optimizing the contrastive loss. Then, we utilized a semi-supervised boosting method to assign pseudo labels to unlabeled data. The iteration between the semi-supervised module and the embedding update module is done until convergence is happened. During these iterations, the embedding update module propagates the error-correcting signals to a semi-supervised module. To evaluate the proposed approach, we have applied it to the SemEval2017dataset (task 4), Sentiment 140, and IMDB Movie Reviews. We have designed many different experiment settings to validate the proposed approach’s different modules. On SemEval2017dataset (task 4), we have got 75.9% and 77.1% in AvgRec and $${F}_{1}^{PN}$$ respectively. Also, when only 10% of the training samples as labeled samples are used, we get the 71.8% and 73.6% in AvgRec and $${F}_{1}^{PN}$$ respectively. The results show that our approach significantly improves with respect to the comparable methods. Also, on IMDB Movie Reviews and Sentiment 140, the proposed approach demonstrates improved performance compared to comparable methods.},
  archive      = {J_APIN},
  author       = {Rashidi, Shima and Tanha, Jafar and Sharifi, Arash and Hosseinzadeh, Mehdi},
  doi          = {10.1007/s10489-024-06071-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {SSSA: Low data sentiment analysis using boosting semi-supervised approach and deep feature learning network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skeleton-based human action recognition using LSTM and
depthwise separable convolutional neural network. <em>APIN</em>,
<em>55</em>(5), 1–21. (<a
href="https://doi.org/10.1007/s10489-024-06082-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of computer vision, the task of human action recognition (HAR) represents a challenge, due to the complexity of capturing nuanced human movements from video data. To address this issue, researchers have developed various algorithms. In this study, a novel two-stream architecture is developed that combines LSTM with a depthwise separable convolutional neural network (DSConV) and skeleton information, with the aim of enhancing the accuracy of HAR. The 3D coordinates of each joint in the skeleton are extracted using the Mediapipe library, and the 2D coordinates are obtained using MoveNet. The proposed method comprises two streams, called the temporal LSTM module and the joint-motion module, and was developed to overcome the limitations of prior two-stream RNN models, such as the vanishing gradient problem and the difficulty of effectively extracting temporal-spatial information. A performance evaluation on the benchmark datasets of JHMDB (73.31%), Florence-3D Action (97.67%), SBU Interaction (95.2%), and Penn Action (94.0%) showcases the effectiveness of the proposed model. A comparison with state-of-the-art methods demonstrates the superior performance of the approach on these datasets. This study contributes to advancing the field of HAR, with potential applications in surveillance and robotics.},
  archive      = {J_APIN},
  author       = {Le, Hoangcong and Lu, Cheng-Kai and Hsu, Chen-Chien and Huang, Shao-Kang},
  doi          = {10.1007/s10489-024-06082-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Skeleton-based human action recognition using LSTM and depthwise separable convolutional neural network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CaVIT: An integrated method for image style transfer using
parallel CNN and vision transformer. <em>APIN</em>, <em>55</em>(5),
1–15. (<a href="https://doi.org/10.1007/s10489-024-06114-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on image style transfer, aiming to generate images with the desired style while preserving the underlying content structure. Existing models face challenges in accurately representing both content and style features. To address this, an integrated method for image style transfer is proposed, utilizing a parallel CNN and Vision Transformer (CaVIT). It combines a Convolutional Neural Network (CNN) with a Vision Transformer (VIT) to achieve enhanced performance. Our method utilizes VGG-19 with residual blocks to encode style features for enhanced refinement. Additionally, the PA-Trans Encoder Layer is introduced, inspired by the Transformer Encoder Layer, to efficiently encode content features while preserving the complete content structure. The fused features are then decoded into stylized images using a CNN decoder. Qualitative and quantitative evaluations demonstrate that our proposed method outperforms existing models, delivering high-quality results.},
  archive      = {J_APIN},
  author       = {Zhang, ZaiFang and Lu, ShunLu and Guo, Qing and Gao, Nan and Yang, YuXiao},
  doi          = {10.1007/s10489-024-06114-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {CaVIT: An integrated method for image style transfer using parallel CNN and vision transformer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A meta-heuristic approach to estimate and explain classifier
uncertainty. <em>APIN</em>, <em>55</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s10489-024-06127-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust is a crucial factor affecting the adoption of machine learning (ML) models. Qualitative studies have revealed that end-users, particularly in the medical domain, need models that can express their uncertainty in decision-making allowing users to know when to ignore the model’s recommendations. However, existing approaches for quantifying decision-making uncertainty are not model-agnostic, or they rely on complex mathematical derivations that are not easily understood by laypersons or end-users, making them less useful for explaining the model’s decision-making process. This work proposes a set of class-independent meta-heuristics that can characterise the complexity of an instance in terms of factors that are mutually relevant to both human and ML decision-making. The measures are integrated into a meta-learning framework that estimates the risk of misclassification. The proposed framework outperformed predicted probabilities and entropy-based methods of identifying instances at risk of being misclassified. Furthermore, the proposed approach resulted in uncertainty estimates that proves more independent of model accuracy and calibration than existing approaches. The proposed measures and framework demonstrate promise for improving model development for more complex instances and provides a new means of model abstention and explanation.},
  archive      = {J_APIN},
  author       = {Houston, Andrew and Cosma, Georgina},
  doi          = {10.1007/s10489-024-06127-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A meta-heuristic approach to estimate and explain classifier uncertainty},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent gear shifting strategy of mining truck based on
deep learning and real-time vehicle condition. <em>APIN</em>,
<em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06142-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The driving conditions in mining areas are complex, and developing a suitable automatic shifting strategy for mining trucks is crucial. However, the development of automatic shifting strategies faces challenges, as it relies on experience and historical experimental data, which are the highest commercial secrets of manufacturers. In recent years, some shifting strategies based on artificial intelligence technologies have been implemented. However, many people shift gears based on the current state of the vehicle, ignoring the influence of historical data. There is a potential risk of mis-shift when unexpected sensor data is received, and continuously shifting gears in a short period of time can increase the likelihood of transmission damage, affecting the driving experience. To this end, this study proposes a novel gear shifting prediction method based on a multi-parameter Bi-directional Long Short-Term Memory(Bi-LSTM) network operating over continuous time periods. Real-time vehicle state data is collected via the CAN bus and 9 parameters that are positively correlated with gear shifting are selected through R/S analysis. By inputting values of those 9 parameters within continuous time periods into the machine learning model, gear shifting prediction is conducted. The experimental results show that our model predicts gear shifting with 96.85% accuracy while its average time cost is around 3.86 ms, meeting the real-time processing requirement. The model balances prediction accuracy and time consumption, and it overcomes the impact of transient abnormal sensor data. Hence, it has the potential for wide application in predictive models based on data with temporal characteristics.},
  archive      = {J_APIN},
  author       = {Su, Qinghua and Xu, Xiaoyu and Wang, Liyong and Zhang, Dingge and Xie, Min and Zhang, Pengbo},
  doi          = {10.1007/s10489-024-06142-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent gear shifting strategy of mining truck based on deep learning and real-time vehicle condition},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPBE: Multi-perspective boundary enhancement network for
aspect sentiment triplet extraction. <em>APIN</em>, <em>55</em>(5),
1–17. (<a href="https://doi.org/10.1007/s10489-024-06144-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect Sentiment Triple Extraction (ASTE) is an emerging task in sentiment analysis that aims to extract triplets consisting of aspect terms, opinion terms, and sentiment polarity from review texts. Previous span-based methods often struggle with accurately identifying the boundaries of aspect and opinion terms, especially when multiple word spans appear in a sentence. This limitation arises from their reliance on a single, simplistic approach to constructing contextual features. To address these challenges, we propose Multi-Perspective Boundary Enhancement Network (MPBE). The network captures rich contextual features by adopting a dual-encoder mechanism and constructs multiple channels to further enhance these features. Specifically, we introduce enhanced semantic and syntactic information in two channels, while the third channel transforms the features using discrete fourier transform. In addition, we design a dual-graph cross fusion module to fuse features from different channels for more efficient information interaction and integration. Finally, by statistically analyzing the length distribution of aspect and opinion terms, a candidate length-based decoding strategy is proposed to achieve more accurate decoding. In experiments, the proposed MPBE model achieved excellent results on four benchmark datasets (14Lap, 14Res, 15Res, 16Res), with F1 scores of 62.32%, 73.78%, 65.32%, and 73.36%, respectively, demonstrating the superiority of the method.},
  archive      = {J_APIN},
  author       = {Yang, Kun and Zong, Liansong and Tang, Mingwei and Zheng, Yanxi and Chen, Yujun and Zhao, Mingfeng and Jiang, Zhongyuan},
  doi          = {10.1007/s10489-024-06144-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {MPBE: Multi-perspective boundary enhancement network for aspect sentiment triplet extraction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From spatial to semantic: Attribute-aware fashion similarity
learning via iterative positioning and attribute diverging.
<em>APIN</em>, <em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06173-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fashion image retrieval emphasizes accurately perceiving the fine-grained features to meet users’ precise needs. However, the existing global image-based retrieval methods encounter challenges such as imprecise positioning of attributes, difficulty in distinguishing visually similar but semantically different attribute values, and struggles in the learning of attribute features within specific regions and viewpoints. This paper proposes a two-stage hybrid framework called IPAD (Iterative Positioning and Attribute Diverging) for attribute-aware fashion similarity learning. In the initial stage, we present an iterative positioning strategy to precisely identify local attribute regions through an iterative attention mechanism with adaptive suppression. IPAD leverages the strengths of Convolutional Neural Networks and Vision Transformers. Subsequently, we design an attribute diverging strategy to optimize attribute value aggregation via online clustering using a momentum encoder, thereby enhancing model stability and representation. During inference, we further present a feature reasoning mechanism to refine retrieval results through subgraph similarity matrix generation and re-ranking to enhance accuracy and robustness. Extensive evaluations on three public datasets demonstrate IPAD’s superior performance over state-of-the-art methods in retrieval accuracy, achieving an average improvement in MAP by +4.22%. The source code is available at https://github.com/h8e9r7/IPAD .},
  archive      = {J_APIN},
  author       = {Wan, Yongquan and Zheng, Jianfei and Yan, Cairong and Zou, Guobing},
  doi          = {10.1007/s10489-024-06173-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {From spatial to semantic: Attribute-aware fashion similarity learning via iterative positioning and attribute diverging},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGT: Depth-guided RGB-d occluded target detection with
transformers. <em>APIN</em>, <em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06192-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In occluded urban environments, the traditional object detection algorithm relies solely on RGB as input, making it challenging to discern the spatial relationship of occluded objects and consequently affecting the target detection accuracy. Previous studies primarily focused on fusing depth and RGB information at the feature level, resulting in the loss of detailed features from the original data, such as occlusion boundaries. This leads to blurred fusion features and degraded model detection performance. Therefore, this paper proposes a depth-guided RGB-D occluded target detection framework based on transformers (DGT) to effectively extract occlusion boundary information and guide the occlusion discrimination via data-level fusion of depth and RGB information. In particular, a multimodal data-level fusion model is proposed for a two-part task. One is to generate dense depth images with strengthened occlusion edge features by extracting the depth difference of object edges in the point cloud data. The other is to dilute the influence of useless information using RGB-D data-level fusion. A depth-guided occlusion layered detection network with transformers was designed to obtain the cross-module guided feature vector by exchanging the weights of the residual and interaction vectors. Extensive experiments showed that DGT achieves state-of-the-art performance in occluded environments.},
  archive      = {J_APIN},
  author       = {Xu, Kelei and Wang, Chunyan and Zhao, Wanzhong and Liu, Jinqiang},
  doi          = {10.1007/s10489-024-06192-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {DGT: Depth-guided RGB-D occluded target detection with transformers},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counterfactual regret minimization for the safety
verification of autonomous driving. <em>APIN</em>, <em>55</em>(5), 1–13.
(<a href="https://doi.org/10.1007/s10489-024-06194-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rare safety-critical events remain a major challenge in autonomous vehicle testing. This paper proposes to use game theory to build a novel testing environment for autonomous vehicles. In this environment, a virtual agent based on counterfactual minimization (CFR) is used to accelerate testing and validate the safety performance of autonomous vehicles. The virtual agent updates the adversarial policies to be enforced by continuously accumulating regret values, thus increasing the probability of security-critical events occurring during the testing process. Finally, recognized metrics such as Time-to-Collision (TTC) and Minimum Safe Distance Factor (MSDF) are introduced to assess the quality of the scenario. Experimental results show that the virtual agent based on counterfactual minimization explicitly generates more safety-critical scenarios and accelerates the evaluation process by multiple orders of magnitude ( $$10^{3}$$ times faster).},
  archive      = {J_APIN},
  author       = {Wang, Yong and Sun, Pengchao and Zhang, Daifeng and Li, Yanqiang},
  doi          = {10.1007/s10489-024-06194-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Counterfactual regret minimization for the safety verification of autonomous driving},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated knowledge graph embedding with
client-wise relation graph. <em>APIN</em>, <em>55</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s10489-024-06211-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Knowledge Graph Embedding (FKGE) has recently garnered considerable interest due to its capacity to extract expressive representations from distributed knowledge graphs, while concurrently safeguarding the privacy of individual clients. Existing FKGE methods typically harness the arithmetic mean of entity embeddings from all clients as the global supplementary knowledge, and learn a replica of global consensus entities embeddings for each client. However, these methods usually neglect the inherent semantic disparities among distinct clients. This oversight not only results in the globally shared complementary knowledge being inundated with too much noise when tailored to a specific client, but also instigates a discrepancy between local and global optimization objectives. Consequently, the quality of the learned embeddings is compromised. To address this, we propose Personalized Federated knowledge graph Embedding with client-wise relation Graph (PFedEG), a novel approach that employs a client-wise relation graph to learn personalized embeddings by discerning the semantic relevance of embeddings from other clients. Specifically, PFedEG learns personalized supplementary knowledge for each client by amalgamating entity embedding from its neighboring clients based on their “affinity” on the client-wise relation graph. Each client then conducts personalized embedding learning based on its local triples and personalized supplementary knowledge. We conduct extensive experiments on four benchmark datasets to evaluate our method against state-of-the-art models and results demonstrate the superiority of our method.},
  archive      = {J_APIN},
  author       = {Zhang, Xiaoxiong and Zeng, Zhiwei and Zhou, Xin and Niyato, Dusit and Shen, ZhiQi},
  doi          = {10.1007/s10489-024-06211-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Personalized federated knowledge graph embedding with client-wise relation graph},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FineDiffusion: Scaling up diffusion models for fine-grained
image generation with 10,000 classes. <em>APIN</em>, <em>55</em>(5),
1–16. (<a href="https://doi.org/10.1007/s10489-024-06215-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers’ parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56 $$\times $$ training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: https://finediffusion.github.io/ .},
  archive      = {J_APIN},
  author       = {Pan, Ziying and Wang, Kun and Li, Gang and He, Feihong and Lai, Yongxuan},
  doi          = {10.1007/s10489-024-06215-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {FineDiffusion: Scaling up diffusion models for fine-grained image generation with 10,000 classes},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSHDNet: Temporal-spatial heterogeneity decoupling network
for multi-mode traffic flow prediction. <em>APIN</em>, <em>55</em>(5),
1–14. (<a href="https://doi.org/10.1007/s10489-024-06218-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the intricate spatial dependencies and dynamic trends among diverse road segments, the prediction of spatio-temporal traffic flow data presents a formidable challenge. To address this challenge within the complexity of urban multi-mode transportation systems, this paper introduces an innovative solution. Anchored by the TSHDNet framework, the proposed methodology presents a novel spatio-temporal heterogeneous decoupling network that adeptly captures the inherent relationships between traffic patterns and temporal-spatial fluctuations. By seamlessly integrating temporal and nodal embeddings, dynamic graph learning, and multi-scale representation modules, TSHDNet demonstrates remarkable efficacy in unraveling the subtle dynamics of traffic flow. Empirical evaluations and ablation experiments conducted on four real-world datasets affirm the framework’s capability and the effectiveness of the decoupling approach.The source codes are available at: https://github.com/MeiWu2/TSHDNet.git},
  archive      = {J_APIN},
  author       = {Wu, Mei and Weng, Wenchao and Wang, Xinran and Seng, Dewen},
  doi          = {10.1007/s10489-024-06218-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {TSHDNet: Temporal-spatial heterogeneity decoupling network for multi-mode traffic flow prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The relationship among some special concepts from the
perspective of formal context restoration. <em>APIN</em>,
<em>55</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06227-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal context restoration is a recently developing topic in the field of formal concept analysis (FCA). Its goal is to restore a formal context from some known formal concepts. Each type of formal concept uses its unique perspective to restore formal contexts or concept lattices. This paper investigates the relationship among five types of basic concepts: the object concepts, the attribute concepts, the join-irreducible concepts, the meet-irreducible concepts, and the formal concepts in concept reducts. This paper first studies the elementary relationship among the basic sets of formal concepts and presents these relationship in a Venn diagram. Then the relationship among the basic concept sets are explored from a restoration perspective, specifically including the relationship among basic concepts from the perspectives of formal context restoration and concept lattice restoration. These relationship are then used to study the transformation between the basic concept sets, and are summarised in a formal context and its concept lattice. Finally, a practical case is given to illustrate the relationship among the basic concept sets explored in this paper, including the elementary relationship, and the relationship from the perspectives of formal context restoration and concept lattice restoration.},
  archive      = {J_APIN},
  author       = {Zhao, Siyu and Qi, Jianjun and Wei, Ling and Wan, Qing},
  doi          = {10.1007/s10489-025-06227-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {The relationship among some special concepts from the perspective of formal context restoration},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on small-scale face detection methods in dense
scenes. <em>APIN</em>, <em>55</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06231-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face detection serves as the core foundation for applications such as face analysis, recognition and reconstruction. In dense scenarios, the target scale difference is significant, and the instance pixels are too small as well as the mutual occlusion is serious leading to inconspicuous feature representation. However, existing detection methods rely on convolutional and pooling layers for feature extraction, with insufficient deep feature extraction and limited inference capability, leading to inaccurate recognition and high leakage rate. Therefore, we propose a small-scale face detection model YOLO-SXS based on the extended Transformer structure, which makes full use of contextual information and feature fusion networks to significantly improve the detection performance for small-scale and occluded faces. Specifically, the fusion of Swin Transformer and Convolutional Neural Networks (CNN) for feature extraction enhances the network’s ability to perceive global features; the Space to Depth (SPD-Conv) mapping is used to improve the network’s feature extraction in low-resolution and small-target detection tasks; furthermore, by adding fine-grained features, YOLO-SXS can significantly improve its performance for small-scale and occluded face detection capability; in addition, by adding a fine-grained feature fusion layer, feature information is retained to the maximum extent, which effectively reduces the loss of target information. The performance evaluation was performed on WIDER FACE, SCUT-HEAD and FDDB datasets, and the experimental results show that our proposed method significantly improves the performance of recognizing small-sized faces and achieves high detection rate and low error rate.},
  archive      = {J_APIN},
  author       = {Cao, Yuan and Zhang, Bei and Wang, Changqing and Wang, Meng},
  doi          = {10.1007/s10489-025-06231-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Research on small-scale face detection methods in dense scenes},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGMI: A diffusion-based generative adversarial framework for
multivariate air quality imputation. <em>APIN</em>, <em>55</em>(5),
1–21. (<a href="https://doi.org/10.1007/s10489-025-06240-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of monitoring spatiotemporal air quality data, data sample missingness is prevalent, thus rectifying missing values in spatiotemporal data holds paramount significance. In recent years, diffusion probability models have played a prominent role in image, video, and text generation, and have also begun to be applied in the field of spatiotemporal data imputation. However, such models face challenges in extracting fine-grained features for stable model operation and accurate modeling of data probability distributions. To address the aforementioned issues, we propose a Diffusion-based Generative adversarial framework for Multivariate air quality data Imputation, termed DGMI. Recognizing the similar temporal, sensor, and indicator change characteristics inherent in air quality data, our framework is designed to cater to the spatiotemporal characteristics of air quality data by incorporating a multi-cycle temporal feature extraction module and a sensor indicator feature extraction module, facilitating multidimensional refinement and integration of temporal, sensor, and indicator information. Moreover, the initial missing value is encoded with linear interpolation and sine-cosine functions. Following the generation of imputed values by the model, we introduce a discriminator module to discern the consistency between imputed values and observed values to provide feedback for optimizing the model from a data distribution perspective. DGMI outperforms most current data imputation methods under various missing ratios in two real air quality datasets by 4.1% (root mean square error) and 3.0% (mean absolute error), exhibiting efficacy in scenarios characterized by multidimensional spatiotemporal and high missing rates data.},
  archive      = {J_APIN},
  author       = {Cheng, Nuo and Ni, Qingjian},
  doi          = {10.1007/s10489-025-06240-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {DGMI: A diffusion-based generative adversarial framework for multivariate air quality imputation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced interpretation of novel datasets by summarizing
clustering results using deep-learning based linguistic models.
<em>APIN</em>, <em>55</em>(5), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06250-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s technology-driven era, the proliferation of data is inevitable across various domains. Within engineering, sciences, and business domains, particularly in the context of big data, it can extract actionable insights that can revolutionize the field. Amid data management and analysis, patterns or groups of interconnected data points, commonly referred to as clusters, frequently emerge. These clusters represent distinct subsets containing closely related data points, showcasing unique characteristics compared to other clusters within the same dataset. Spanning across disciplines such as physics, biology, business, and sales, clustering is important in understanding these novel datasets’ essential characteristics, developing complex statistical models, and testing various hypotheses. However, interpreting the characteristics and physical implications of generated clusters by different clustering algorithms is challenging for researchers unfamiliar with these algorithms’ inner workings. This research addresses the intricacies of comprehending data clustering, cluster attributes, and evaluation metrics, especially for individuals lacking proficiency in clustering or related disciplines like statistics. The primary objective of this study is to simplify cluster analysis by furnishing users or analysts from diverse domains with succinct linguistic synopses of clustering results, circumventing the necessity for intricate numerical or mathematical terms. Deep learning techniques based on large language models, such as encoder-decoders (for example, the T5 model) and generative pre-trained transformers (GPTs), are employed to achieve this. This study aims to construct a summarization model capable of ingesting data clusters, producing a condensed overview of the contained insights in a simplified, easily understandable linguistic format. The evaluation process revealed a clear preference among evaluators for the summaries generated by GPT, with T5 summaries following closely behind. GPT and T5 summaries were good at fluency, demonstrating their ability to capture the original content in a human-like manner. In contrast, while providing a structured framework for summarization, the linguistic protoform-based approach is needed to match the quality and coherence of the GPT and T5 summaries.},
  archive      = {J_APIN},
  author       = {K, Natarajan and Verma, Srikar and Kumar, Dheeraj},
  doi          = {10.1007/s10489-025-06250-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced interpretation of novel datasets by summarizing clustering results using deep-learning based linguistic models},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing suicidal ideation detection through advanced
feature selection and stacked deep learning models. <em>APIN</em>,
<em>55</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06256-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting suicidal ideation on communication platforms such as social media is critical for suicide prevention, as these platforms are frequently used for emotional expression and can reflect significant behavior changes. Many machine learning and deep learning techniques have been employed to address this issue, utilizing embedding methods such as Count Vector, Term Frequency-Inverse Document Frequency, Bidirectional Encoder Representations from Transformers, Multilingual Universal Sentence Encoder etc generate high-dimensional vectors. Directly inputting word embeddings into models can introduce noise and outliers, which may negatively impact predictive accuracy. Therefore, feature selection to optimize the dimensionality of word embedding vectors has emerged as a promising direction for future research. This study proposes a feature selection method called Propose Best Feature Selection, which combines Grey Wolf Optimization, Recursive Feature Elimination, and Stepwise Feature Selection. It uses a Voting Classifier to identify and filter the most significant features, reducing dimensionality. These optimized features are then fed into a stacked ensemble hybrid model, with Bi-Directional Gated Recurrent Unit with Attention and Convolutional Neural Network, acting like base and Extreme Gradient Boostis working like the meta-classifier, achieving an accuracy of 98% in Reddit and 97% in Twitter(X) dataset, outperforming similar methods in the field. This work is focused on textual data, and future efforts may expand to include multimodal analysis, incorporating image-based emotional cues. Scalability challenges for large datasets and real-time applications remain a key limitation.},
  archive      = {J_APIN},
  author       = {Shukla, Shiv Shankar Prasad and Singh, Maheshwari Prasad},
  doi          = {10.1007/s10489-025-06256-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing suicidal ideation detection through advanced feature selection and stacked deep learning models},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A learning artificial visual system and its application to
orientation detection. <em>APIN</em>, <em>55</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s10489-024-05991-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a learning artificial visual system, the Learning Dendritic Model Artificial Visual System (DModel-AVS), for orientation detection inspired by biological visual mechanisms. The DModel-AVS consists of two layers: local orientation detection neurons layer and global orientation detection neurons layer. The local neurons detect local features of an image, utilizing dendrite model neurons. The global neurons are designed to implement global features of the image by summing the outputs of the local dendritic neurons. The backpropagation-based learning is performed only to the dendritic neurons. The effectiveness of the DModel-AVS is evaluated through several experiments comparing it with various convolutional neural network (CNN)-based orientation detection systems. Results show that the DModel-AVS is a more biologically plausible and effective solution to orientation detection, with higher accuracy, and lower learning costs. The proposed system has practical applications in various fields such as computer vision and robotics.},
  archive      = {J_APIN},
  author       = {Chen, Tianqi and Kobayashi, Yuki and Yan, Chenyang and Qiu, Zhiyu and Hua, Yuxiao and Todo, Yuki and Tang, Zheng},
  doi          = {10.1007/s10489-024-05991-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A learning artificial visual system and its application to orientation detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STNeRF: Symmetric triplane neural radiance fields for novel
view synthesis from single-view vehicle images. <em>APIN</em>,
<em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06005-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents STNeRF, a method for synthesizing novel views of vehicles from single-view 2D images without the need for 3D ground truth data, such as point clouds, depth maps, CAD models, etc., as prior knowledge. A significant challenge in this task arises from the characteristics of CNNs and the utilization of local features can lead to a flattened representation of the synthesized image when training and validation with images from a single viewpoint. Many current methodologies tend to overlook local features and rely on global features throughout the entire reconstruction process, potentially resulting in the loss of fine-grained details in the synthesized image. To tackle this issue, we introduce Symmetric Triplane Neural Radiance Fields (STNeRF). STNeRF employs a triplane feature extractor with spatially aware convolution to extend 2D image features into 3D. This decouples the appearance component, which includes local features, and the shape component, which consists of global features, and utilizes them to construct a neural radiance field. These neural priors are then employed for rendering novel views. Furthermore, STNeRF leverages the symmetric properties of vehicles to liberate the appearance component from reliance on the original viewpoint and to align it with the symmetry of the target space, thereby enhancing the neural radiance field network’s ability to represent the invisible regions. The qualitative and quantitative evaluations demonstrate that STNeRF outperforms existing solutions in terms of both geometry and appearance reconstruction. More supplementary materials and the implementation code are available for access at the following link: https://github.com/ll594282475/STNeRF .},
  archive      = {J_APIN},
  author       = {Liu, Zhao and Fu, Zhongliang and Li, Gang and Hu, Jie and Yang, Yang},
  doi          = {10.1007/s10489-024-06005-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {STNeRF: Symmetric triplane neural radiance fields for novel view synthesis from single-view vehicle images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way reductions of conflict analysis based on relation
matrices and integration measures. <em>APIN</em>, <em>55</em>(5), 1–26.
(<a href="https://doi.org/10.1007/s10489-024-06020-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conflicts serve as an important focus of uncertainty analysis, and their reductions facilitate the issue identification and conflict solving to become valuable but rare. At present, conflict analysis reductions mainly embrace relation matrices, and they never concern uncertainty measures with highly concentrated information. In this paper, three-way reductions of conflict analysis are transferred from relation matrices to integration measures, and corresponding heuristic reduction algorithms are constructed for information systems. At first, three-way membership degrees and three-way similarity degrees are proposed for conflict analysis, and their measurement boundedness, issue monotonicity, calculation algorithm, and transformation interrelationship are researched. Then, alliance, conflict, and neutrality reductions are proposed based on similarity degrees to acquire heuristic reduction algorithms, and they can be equivalently characterized by both membership degrees and relation matrices. Finally by table examples and data experiments, similarity degrees and relevant measurement properties are validated, and two groups of three-way reduction algorithms related to relation matrices and similarity degrees are comparatively analyzed; as a result, three-way reduction algorithms based on similarity degrees become novel and effective for conflict analysis. This study provides an in-depth insight into three-way reductions of conflict analysis from algebraic measurement.},
  archive      = {J_APIN},
  author       = {Chen, Jiang and Zhang, Xianyong},
  doi          = {10.1007/s10489-024-06020-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Appl. Intell.},
  title        = {Three-way reductions of conflict analysis based on relation matrices and integration measures},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetric perception and ordinal regression for detecting
scoliosis natural image. <em>APIN</em>, <em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-05849-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scoliosis is one of the most common diseases in adolescents. Traditional screening methods for the scoliosis usually use radiographic examination, which requires certified experts with medical instruments and brings the radiation risk. Considering such requirement and inconvenience, we propose to use natural images of the human back for wide-range scoliosis screening, which is a challenging problem. In this paper, we notice that the human back has a certain degree of symmetry, and asymmetrical human backs are usually caused by spinal lesions. Besides, scoliosis severity levels have ordinal relationships. Taking inspiration from this, we propose a dual-path scoliosis detection network with two main modules: symmetric feature matching module (SFMM) and ordinal regression head (ORH). Specifically, we first adopt a backbone to extract features from both the input image and its horizontally flipped image. Then, we feed the two extracted features into the SFMM to capture symmetric relationships. Finally, we use the ORH to transform the ordinal regression problem into a series of binary classification sub-problems. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods as well as human performance, which provides a promising and economic solution to wide-range scoliosis screening. In particular, our method achieves accuracies of 95.11% and 81.46% in estimation of general severity level and fine-grained severity level of the scoliosis, respectively.},
  archive      = {J_APIN},
  author       = {Zhu, Xiaojia and Chen, Rui and Guo, Xiaoqi and Shao, Zhiwen and Dai, Yuhu and Zhang, Ming and Lang, Chuandong},
  doi          = {10.1007/s10489-024-05849-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Symmetric perception and ordinal regression for detecting scoliosis natural image},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph reconstruction and attraction method for community
detection. <em>APIN</em>, <em>55</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-05858-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection as one of the hot issues in complex networks has attracted a large amount of attention in the past several decades. Although many methods perform well on this problem, they become incapable if the networks exhibit more complicated characteristics, e.g. strongly overlapping communities. This paper explores a graph reconstruction and attraction method (GRAM) for community detection. In GRAM, we extract network structure information of a graph by introducing a new passing probability matrix based on Markov Chains by which a new graph is further reconstructed, and modularity optimization is adopted on the reconstructed one instead of the original one for non-overlapping community detection. For identifying overlapping communities, we first initialize a cluster with a vital node as an origin of attraction, then the cluster is extended by graph attraction based on the passing probability. This procedure is repeated for the remaining nodes, and each isolated node if exists is finally classified into its most attractable cluster. Experiments on artificial and real-world datasets have shown the superiority of the proposed method for community detection particularly on the datasets with even more complex, sparse and ambiguous network structures.},
  archive      = {J_APIN},
  author       = {Wu, Xunlian and Teng, Da and Zhang, Han and Hu, Jingqi and Quan, Yining and Miao, Qiguang and Sun, Peng Gang},
  doi          = {10.1007/s10489-024-05858-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Graph reconstruction and attraction method for community detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fooling machine learning models: A novel out-of-distribution
attack through generative adversarial networks. <em>APIN</em>,
<em>55</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s10489-024-05974-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in machine learning (ML) have facilitated the deployment of ML models across various real-world applications. However, these ML models might suffer from various potential security threats. In this paper, we propose a novel out-of-distribution attack: Leveraging pre-trained generative adversarial networks (GANs), an adversary aims to fool an ML model and make the model misclassify a sample from GANs as a pre-specified target class. Our attack is based on the insight that ML models do not know when they do not know, and ML models can unexpectedly recognize a completely different sample (e.g. cartoon face) as a certain class (e.g. airplane) with high confidence. Specifically, we introduce a targeted attack framework through GANs for white-box and black-box scenarios. Our framework casts this problem as an optimization problem and a family of attack methods are developed. Extensive experimental results show that our methods can achieve competitive performance, even compared with several state-of-the-art adversarial example attacks. Furthermore, our methods can evade several widely-used and the latest defenses. We also elaborately analyze various factors that affect the attack performance. Our work will provide a supplementary test to comprehensively evaluate the robustness of ML systems.},
  archive      = {J_APIN},
  author       = {Hu, Hailong and Pang, Jun},
  doi          = {10.1007/s10489-024-05974-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Fooling machine learning models: A novel out-of-distribution attack through generative adversarial networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based neural marked spatio temporal point
process model for analyzing football match events. <em>APIN</em>,
<em>55</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-05996-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive modeling plays a crucial role in machine learning, data analysis, and statistics. In sports, predictive modeling methods have emerged to provide insights and evaluate performances based on key performance metrics. However, most existing models tend to focus on predicting only partial aspects of an event, such as the outcome, action type, or location, while neglecting the temporal factors involved. To address this gap, this study introduces the Transformer-Based Neural Marked Spatio-Temporal Point Process (NMSTPP) model, specifically designed for football event data. The NMSTPP model predicts a comprehensive set of future event components, including inter-event time, zone, and action. Additionally, it features a dependent prediction layers architecture to enhance model performance. The Holistic Possession Utilization Score (HPUS) metric is also proposed to evaluate the effectiveness and efficiency of possession periods in football based on the NMSTPP model. With open-source football event data, the NMSTPP model successfully predicted the aforementioned three components of future events, with an improvement of up to 4% overall and 9% for individual components compared to baseline models. The HPUS demonstrated a 0.9 correlation with existing performance metrics, highlighting its utility in performance evaluation. The NMSTPP and HPUS were applied to the Premier League to demonstrate their practical feasibility.},
  archive      = {J_APIN},
  author       = {Yeung, Calvin and Sit, Tony and Fujii, Keisuke},
  doi          = {10.1007/s10489-024-05996-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Transformer-based neural marked spatio temporal point process model for analyzing football match events},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double-level discriminative domain adaptation network for
cross-domain fault diagnosis. <em>APIN</em>, <em>55</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-06016-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately determining the health of critical components contributes to the efficient operation of industrial equipment and systems. Domain adaptation has emerged as a potent tool for cross-domain diagnosis, particularly in real-world scenarios that involve variations in the distributions of the utilized training and test data. However, the current domain adaptation methods are overly concerned with feature alignment while neglecting feature discriminability and the distinguishability of the intrinsic structure within the target domain. This results in the misclassification of target samples according to decision boundaries. In response to this issue, a double-level discriminative domain adaptation network (DL-DDAN) for cross-domain fault diagnosis is proposed. The DL-DDAN aligns domain-level features via adversarial training and designs a class-level discriminative module and a sample-level discriminative module. On the one hand, the class-level discriminative module not only achieves class-level alignment, but also promotes intra-class compactness and inter-class separation by pushing features belonging to the same class closer together and maintaining sufficient separation between the features of different classes. On the other hand, the sample-level discriminative module is applied to the target samples to mine their potential distinguishable information. The experimental results obtained on bearing and gearbox datasets, under various working conditions and measurement points, demonstrated the effectiveness and superiority of the DL-DDAN. The diagnosis framework of DL-DDAN, including Health classifier loss, Domain discriminator loss, Class-level discriminative loss, and Sample-level discriminative loss.},
  archive      = {J_APIN},
  author       = {Li, Yufeng and Xu, Xinghan and Hu, Lei and Sun, Kai and Han, Min},
  doi          = {10.1007/s10489-024-06016-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Double-level discriminative domain adaptation network for cross-domain fault diagnosis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BAM-SORT: Border-guided activated matching for online
multi-object tracking. <em>APIN</em>, <em>55</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s10489-024-06037-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking aims at estimating object bounding boxes and identity IDs in videos. Most tracking methods combine a detector and a Kalman filter using the IoU distance as a similarity metric for association matching of the previous trajectories with the current detection box. These methods usually suffer from ID switches and fragmented trajectories in response to congested and frequently occluded scenarios. To solve this problem, in this study, a simple and effective association method is proposed. First, a bottom edge cost matrix is introduced for the utilization of depth information to improve the data association and increase the robustness in the case of occlusion. Second, an asymmetric trajectory classification mechanism is proposed to distinguish the false-postive trajectories, and an activated trajectory matching strategy is introduced to reduce the interference of noise and transient objects in tracking. Finally, the trajectory deletion strategy is improved by introducing the number of trajectory state switches to delete the trajectories caused by spurious high-scoring detection boxes in real time, as a result, the number of fragmented trajectories is also reduced. These innovations achieve excellent performance on various benchmarks, including MOT17, MOT20, and especially DanceTrack where interactions and occlusions are frequent and severe. The code and models are available at https://github.com/djdodsjsjx/BAM-SORT/ .},
  archive      = {J_APIN},
  author       = {Chao, Yuan and Zhu, Huaiyang and Lu, Hengyu},
  doi          = {10.1007/s10489-024-06037-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {BAM-SORT: Border-guided activated matching for online multi-object tracking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybridization of differential evolution and particle swarm
optimization with distributed acceleration constants to solve economic
load dispatch problem. <em>APIN</em>, <em>55</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s10489-024-06051-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The economic load dispatch problem owns features like multimodality, non-convexity, and hence, the traditional programming models fail to offer a solution to it. We have shown our research interest in PSO to solve the problem due to the umpteen numbers of applications it supports along with their improved swarming intelligence. Our previous paper has been dealt with an improved form of PSO, wherein adaptive acceleration constants were exploited. The adaptive nature comes from the dynamic changing of the acceleration constants with regard to the particle’s position as well as the number of function assessments. In that algorithm, the particles were enabled to search in a systematic distributed environment and the algorithm was known as PSO with Distributed Acceleration Constant (PSODAC). This paper makes an extension of PSODAC through hybridizing it with the notion behind Differential Evolution (DE). The hybridization has been performed in a sequential manner. Hence, it achieves the name as Sequentially Hybridized DE and PSODAC (SH-DEPSODAC), irrespective of the fact that the PSODAC has been employed. A total of three test systems have been utilized to test and examine the superiority of PSODAC for exhibiting higher particle dynamics.},
  archive      = {J_APIN},
  author       = {Yadav, Naresh Kumar},
  doi          = {10.1007/s10489-024-06051-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Hybridization of differential evolution and particle swarm optimization with distributed acceleration constants to solve economic load dispatch problem},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-project defect prediction based on autoencoder with
dynamic adversarial adaptation. <em>APIN</em>, <em>55</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s10489-024-06087-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-project defect prediction enables a target software project with limited defect data to build a defect prediction model by leveraging abundant data in the source project. However, existing methods of cross-project defect prediction ignore the relative importance of global and local distributions in learning project-invariant feature spaces. This paper proposes a novel approach for cross-project defect prediction called Adan (autoencoder with dynamic adversarial adaptation) to dynamically adjust a project-invariant feature space by aligning global and local distributions simultaneously with adversarial learning. Specifically, the au-encoder was adopted to produce a latent space used as a project-invariant feature space for source and target artifacts. Global and local discriminators were used to adjust the latent space to ensure that representations of source and target artifacts in the project-invariant feature space have approximate global distribution and local distribution, respectively. The prediction model for the target artifacts was then trained using representations of the source artifacts in the project-invariant feature space. Experiments on four open-source projects with 12 pairs of tasks on cross-project defect prediction demonstrated that the proposed Adan approach outperformed state-of-the-art techniques, with an average improvement of 8.42% in terms of AUC.},
  archive      = {J_APIN},
  author       = {Zhang, Wen and Zhao, Jiangpeng and Qin, Guangjie and Wang, Song},
  doi          = {10.1007/s10489-024-06087-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Cross-project defect prediction based on autoencoder with dynamic adversarial adaptation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended topic classification utilizing LDA and BERTopic: A
call center case study on robot agents and human agents. <em>APIN</em>,
<em>55</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s10489-024-06106-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two ways to know why customers call the center: from the predetermined calling reason said by the customer to a Robot Agent (RA) before service with a Human Agent (HA) or directly from the customer’s conversation with an HA during the service. Obtaining tags by telling the call reason is easy, but customers can choose the wrong service operation at a non-negligible rate. So, this study used the data from 20,000 Turkish phone conversations with a HA at an inbound call center in the electronic products sector, which are handled for topic extraction with Latent Dirichlet Allocation (LDA) and Bidirectional Encoder Representations from Transformers Topic (BERTopic) topic modeling. First, the customer speeches converted to text received from the system were passed through cleaning and editing typos. Then, the models were created, and the topic extraction process was performed. LDA and BERTopic algorithms were evaluated by comparing the machine learning technology results of the call center with HA and RA. The topics covered were used for classification with Light Gradient Boosting Machine (LGBM) linear Support Vector Machines (SVM), Long Short Term Memory (LSTM), and Logistic Regression (LR). The classification and statistical test results showed that LDA is more successful than the guided BERTopic algorithm. In addition, LDA-based classification was also more successful than RA-based classification. Although LDA-based LSTM and LR algorithms were superior to others, the best performance according to accuracy score belongs to LDA-based LSTM.},
  archive      = {J_APIN},
  author       = {Kazanci, Nevra},
  doi          = {10.1007/s10489-024-06106-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Extended topic classification utilizing LDA and BERTopic: A call center case study on robot agents and human agents},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time optimal trajectory planning of robotic arm based on
improved sand cat swarm optimization algorithm. <em>APIN</em>,
<em>55</em>(5), 1–54. (<a
href="https://doi.org/10.1007/s10489-024-06124-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to address the issue of automatic charging for electric vehicles, a hanging automatic charging system was proposed, with a particular focus on the time-optimal trajectory planning of the robotic arm within the system. Additionally, a multi-strategy improved Sand Cat Swarm Optimization Algorithm (YSCSO) was put forth as a potential solution. The 0805A six-axis manipulator was selected as the research object, and a kinematic model was constructed using the D-H parameter method. The 5-7-5 polynomial interpolation function was proposed and solved to construct the motion trajectory of the robotic arm joint. The cubic chaos-refraction inverse learning, introduced to initialize the population based on the sand cat swarm algorithm SCSO, balances the relationship between the elite pool weighted guided search behavior and the spiral Lévy flight predation behavior through the use of a dynamic nonlinear sensitivity range. Furthermore, the vigilance behavior mechanism of the sand cat was increased to improve the overall optimization performance of the algorithm. The proposed method was applied to 36 benchmark functions of global optimization, and the improvement strategy, convergence behavior, population diversity, exploration, and development of the algorithm were experimentally analyzed. The results demonstrated that the proposed method exhibited superior performance, with 80.86% of the test results significantly different from those of the comparison algorithm. Three constrained mechanical design optimization problems were employed to assess the algorithm’s practicality in engineering applications. Subsequently, the algorithm was applied to the optimal trajectory planning of a robotic arm, resulting in a significant reduction in the optimized joint motion time, a smooth and continuous kinematic curve devoid of abrupt changes, and a 42.72% reduction in motion time. These findings further substantiate the theoretical feasibility and superiority of the algorithm in addressing engineering challenges.},
  archive      = {J_APIN},
  author       = {Lu, Zhenkun and You, Zhichao and Xia, Binghan},
  doi          = {10.1007/s10489-024-06124-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-54},
  shortjournal = {Appl. Intell.},
  title        = {Time optimal trajectory planning of robotic arm based on improved sand cat swarm optimization algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A temporal-spatial encoder convolutional network model for
multitasking prediction. <em>APIN</em>, <em>55</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s10489-024-06145-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs), as a specialized neural network architecture for processing time series data, are increasingly vital in predicting the remaining useful life (RUL) and tool wear. However, RNNs have inherent sequence dependency, which makes it difficult to effectively parallelize when processing input data, significantly reducing training efficiency. To address these limitations, this paper proposes a temporal-spatial encoder convolutional network (TSECN) for RUL and tool wear prediction. This model uses the temporal feature extraction (TFE) module is adopted to excavate temporal features parallelly and dynamically weigh the features of different timesteps to improve its feature representation capability. Meanwhile, the spatial feature extraction (SFE) module is employed to excavate both local and global spatial features, which are then fused by a new feature fusion layer to enhance its prediction accuracy. The feature compression module is utilized to reduce the computational complexity and mitigate over-fitting. Finally, the regression prediction module is used to realize an accurate prediction of the target variable. Based on the C-MAPSS and PHM2010 datasets, experiments were conducted to assess the performance of the TSECN model, which shows that the TSECN model surpasses the state-of-the-arts in both the RUL and wear prediction tasks in terms of prediction accuracy.},
  archive      = {J_APIN},
  author       = {Zhao, Chengying and Shi, Huaitao and Huang, Xianzhen and Zhang, Yongchao and He, Fengxia},
  doi          = {10.1007/s10489-024-06145-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {A temporal-spatial encoder convolutional network model for multitasking prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel adaptive predefined-time complete tracking control of
nonlinear systems via ELM. <em>APIN</em>, <em>55</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s10489-024-06153-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A predefined-time sliding mode adaptive control method (PDTSMAC)for nonlinear system is proposed in the presence of parameters unknown, external disturbances and arbitrary initial values. Firstly, the expected trajectory of the system is extended to the arrival process with characters of predefined-time convergence and the accurate tracking process of completely tracking the desired trajectory, the design principle of extended trajectory is given; Then, an extreme learning machine (ELM) with exponential convergence of external weights is designed to compensate the uncertainties of the system, and a sliding mode adaptive controller with predefined-time convergence is constructed based on a predefined-time convergent sliding mode surface. The stability of the closed-loop system is proved theoretically. The simulation results show that the control strategy can ensure that the construction robot in arbitrary initial state converges to the extended desired trajectory within the predefined-time, and realizes the complete and accurate tracking of the preset desired trajectory, and the trajectory tracking error is less than 0.008.},
  archive      = {J_APIN},
  author       = {Yin, Chun-Wu and Riaz, Saleem},
  doi          = {10.1007/s10489-024-06153-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Novel adaptive predefined-time complete tracking control of nonlinear systems via ELM},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: A framework to evaluate the barriers for
adopting the internet of medical things using the extended generalized
TODIM method under the hesitant fuzzy environment. <em>APIN</em>,
<em>55</em>(5), 1. (<a
href="https://doi.org/10.1007/s10489-024-06187-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Alattas, Khalid and Wu, Qun},
  doi          = {10.1007/s10489-024-06187-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Retraction note: A framework to evaluate the barriers for adopting the internet of medical things using the extended generalized TODIM method under the hesitant fuzzy environment},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STViT+: Improving self-supervised multi-camera depth
estimation with spatial-temporal context and adversarial geometry
regularization. <em>APIN</em>, <em>55</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s10489-024-06191-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-camera depth estimation has gained significant attention in autonomous driving due to its importance in perceiving complex environments. However, extending monocular self-supervised methods to multi-camera setups introduces unique challenges that existing techniques often fail to address. In this paper, we propose STViT+, a novel Transformer-based framework for self-supervised multi-camera depth estimation. Our key contributions include: 1) the Spatial-Temporal Transformer (STTrans), which integrates local spatial connectivity and global context to capture enriched spatial-temporal cross-view correlations, resulting in more accurate 3D geometry reconstruction; 2) the Spatial-Temporal Photometric Consistency Correction (STPCC) strategy that mitigates the impact of varying illumination, ensuring brightness consistency across frames during photometric loss calculation; 3) the Adversarial Geometry Regularization (AGR) module, which employs Generative Adversarial Networks to impose spatial constraints by using unpaired depth maps, enhancing performance under adverse conditions such as rain and nighttime driving. Extensive evaluations on large-scale autonomous driving datasets, including Nuscenes and DDAD, confirm that STViT+ sets a new benchmark for multi-camera depth estimation.},
  archive      = {J_APIN},
  author       = {Chen, Zhuo and Zhao, Haimei and Hao, Xiaoshuai and Yuan, Bo and Li, Xiu},
  doi          = {10.1007/s10489-024-06191-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {STViT+: Improving self-supervised multi-camera depth estimation with spatial-temporal context and adversarial geometry regularization},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time trajectory prediction of a ping-pong ball using a
GRU-TAE. <em>APIN</em>, <em>55</em>(5), 1–21. (<a
href="https://doi.org/10.1007/s10489-024-06204-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table tennis with collaborative robots has been a challenge in robotics for decades, due to its unique challenges, primarily requiring precise real-time ball trajectory predictions to enable responsive, accurate gameplay. Traditional physical models, while widely studied, struggle with factors like spin, limiting accuracy. With advancements in computational power, data-driven approaches as sequence-to-sequence (seq2seq) models as trajectory autoencoders (TAE) offer potential for improved long-term prediction. However, they are not fully optimized for time-series prediction. More advanced seq2seq models with recurrent layers are better suited and improve prediction accuracy, but remain underexplored for trajectory prediction. Additionally, recurrent layers integrated with TAE, as gated-recurrent unit TAE (GRU-TAE), have not been applied for real-world trajectories. This study introduces a novel GRU-TAE model designed for high-accuracy, low-latency trajectory predictions in table tennis. Our approach was evaluated on both real and simulated data, demonstrating that TAE-based architectures outperform traditional recurrent models (e.g., LSTM, GRU, RNN) by 44% in long-term prediction accuracy, achieving an average computation time of 8.2 ms. Additionally, GRU-TAE improves accuracy by 14% over traditional TAE models and by 41% compared to baseline model-based methods in predicting real ball trajectories. These results are important because they enhance robotic arm performance in returning balls to human players, allowing earlier positioning for ball return, and reducing late position adjustments. This work sets the foundation for integrating GRU-TAE with advanced model-based approaches, aiming toward real-world deployment in collaborative table tennis robots.},
  archive      = {J_APIN},
  author       = {Toussaint, Baptiste and Raison, Maxime},
  doi          = {10.1007/s10489-024-06204-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Real-time trajectory prediction of a ping-pong ball using a GRU-TAE},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A masked autoencoder network for spatiotemporal predictive
learning. <em>APIN</em>, <em>55</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s10489-024-06214-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is about predictive learning, which is generating future frames given previous images. Suffering from the vanishing gradient problem, existing methods based on RNN and CNN can’t capture the long-term dependencies effectively. To overcome the above dilemma, we present MastNet a spatiotemporal framework for long-term predictive learning. In this paper, we design a Transformer-based encoder-decoder with hierarchical structure. As for the transformer block, we adopt the spatiotemporal window based self-attention to reduce computational complexity and the spatiotemporal shifted window partitioning approach. More importantly, we build a spatiotemporal autoencoder by the random clip mask strategy, which leads to better feature mining for temporal dependencies and spatial correlations. Furthermore, we insert an auxiliary prediction head, which can help our model generate higher-quality frames. Experimental results show that the proposed MastNet achieves the best results in accuracy and long-term prediction on two spatiotemporal datasets compared with the state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Sun, Fengzhen and Jin, Weidong},
  doi          = {10.1007/s10489-024-06214-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Appl. Intell.},
  title        = {A masked autoencoder network for spatiotemporal predictive learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view prototype balance and temporary proxy constraint
for exemplar-free class-incremental learning. <em>APIN</em>,
<em>55</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06233-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exemplar-free class-incremental learning recognizes both old and new classes without saving old class exemplars because of storage limitations and privacy constraints. To address the forgetting of knowledge caused by the absence of old training data, we present a novel method that consists of two modules, multi-view prototype balance and temporary proxy constraints, which are based on feature retention and representation optimization. Specifically, multi-view prototype balance first extends the prototypes to maintain the general state of the class and then balances these prototypes combining knowledge distillation and prototype compensation to ensure the stability and plasticity of the model. To alleviate the feature overlap, the proposed temporary proxy constraint sets the temporary proxies to lightly compress the feature distribution during each mini-batch of training. Extensive experiments on five datasets with different settings demonstrate the superiority of our method against the state-of-the-art exemplar-free class-incremental learning methods.},
  archive      = {J_APIN},
  author       = {Tian, Heng and Zhang, Qian and Wang, Zhe and Zhang, Yu and Xu, Xinlei and Fu, Zhiling},
  doi          = {10.1007/s10489-025-06233-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view prototype balance and temporary proxy constraint for exemplar-free class-incremental learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive prototype loss based discriminative feature
network for few-shot learning. <em>APIN</em>, <em>55</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06234-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric-based few-shot image classification methods generally perform classification by comparing the distances between the query sample features and the prototypes of each class. These methods often focus on constructing prototype representations for each class or learning a metric, while neglecting the significance of the feature space itself. In this paper, we redirect the focus to feature space construction, with the goal of constructing a discriminative feature space for few-shot image classification tasks. To this end, we designed a contrastive prototype loss that incorporates the distribution of query samples with respect to class prototypes in the feature space, emphasizing intra-class compactness and inter-class separability, thereby guiding the model to learn a more discriminative feature space. Based on this loss, we propose a contrastive prototype loss based discriminative feature network (CPL-DFNet) to address few-shot image classification tasks. CPL-DFNet enhances sample utilization by fully leveraging the distance relationships between query samples and class prototypes in the feature space, creating more favorable conditions for few-shot image classification tasks and significantly improving classification performance. We conducted extensive experiments on both general and fine-grained few-shot image classification benchmark datasets to validate the effectiveness of the proposed CPL-DFNet method. The experimental results show that CPL-DFNet can effectively perform few-shot image classification tasks and outperforms many existing methods across various task scenarios, demonstrating significant performance advantages.},
  archive      = {J_APIN},
  author       = {Yan, Leilei and He, Feihong and Zheng, Xiaohan and Zhang, Li and Zhang, Yiqi and He, Jiangzhen and Du, Weidong and Wang, Yansong and Li, Fanzhang},
  doi          = {10.1007/s10489-025-06234-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Contrastive prototype loss based discriminative feature network for few-shot learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sine and cosine based learning rate for gradient descent
method. <em>APIN</em>, <em>55</em>(5), 1–28. (<a
href="https://doi.org/10.1007/s10489-025-06235-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning networks have been trained using first-order-based methods. These methods often converge more quickly when combined with an adaptive step size, but they tend to settle at suboptimal points, especially when learning occurs in a large output space. When first-order-based methods are used with a constant step size, they oscillate near the zero-gradient region, which leads to slow convergence. However, these issues are exacerbated under nonconvexity, which can significantly diminish the performance of first-order methods. In this work, we propose a novel Boltzmann Probability Weighted Sine with a Cosine distance-based Adaptive Gradient (BSCAGrad) method. The step size in this method is carefully designed to mitigate the issue of slow convergence. Furthermore, it facilitates escape from suboptimal points, enabling the optimization process to progress more efficiently toward local minima. This is achieved by combining a Boltzmann probability-weighted sine function and cosine distance to calculate the step size. The Boltzmann probability-weighted sine function acts when the gradient vanishes and the cooling parameter remains moderate, a condition typically observed near suboptimal points. Moreover, using the sine function on the exponential moving average of the weight parameters leverages geometric information from the data. The cosine distance prevents zero in the step size. Together, these components accelerate convergence, improve stability, and guide the algorithm toward a better optimal solution. A theoretical analysis of the convergence rate under both convexity and nonconvexity is provided to substantiate the findings. The experimental results from language modeling, object detection, machine translation, and image classification tasks on a real-world benchmark dataset, including CIFAR10, CIFAR100, PennTreeBank, PASCALVOC and WMT2014, demonstrate that the proposed step size outperforms traditional baseline methods.},
  archive      = {J_APIN},
  author       = {Verma, Krutika and Maiti, Abyayananda},
  doi          = {10.1007/s10489-025-06235-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Appl. Intell.},
  title        = {Sine and cosine based learning rate for gradient descent method},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive archive exploitation for gaussian estimation of
distribution algorithm. <em>APIN</em>, <em>55</em>(5), 1–31. (<a
href="https://doi.org/10.1007/s10489-025-06237-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gaussian Estimation of Distribution Algorithm (GEDA) is a fundamental evolutionary algorithm widely applied to continuous optimization problems but often encounters premature convergence. While external archives have been introduced to mitigate this issue, they frequently misuse historical information, leading to suboptimal results. To address this, we propose an Adaptive Archive Exploitation for GEDA (AAE-GEDA). AAE-GEDA incorporates two key mechanisms: adaptive selection of archive quantities (ASAQ) and angle skewness-landscape (ASL) eigenvalue adaptation. ASAQ selectively utilizes a subset of solutions from the archive to improve the accuracy of covariance estimation, preventing the algorithm from being misled by outdated or irrelevant information. ASL dynamically adjusts the search range, ensuring a balanced trade-off between exploration and exploitation. Experimental results on the IEEE CEC2014 and CEC2017 test suites demonstrate that AAE-GEDA consistently outperforms state-of-the-art evolutionary algorithms.},
  archive      = {J_APIN},
  author       = {Zhao, Dongmin and Tian, Yi and Zeng, Lingshun and Liang, Chunquan},
  doi          = {10.1007/s10489-025-06237-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive archive exploitation for gaussian estimation of distribution algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A convex kullback-leibler divergence and critical-descriptor
prototypes for semi-supervised few-shot learning. <em>APIN</em>,
<em>55</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06239-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning has achieved great success in recent years, thanks to its requirement of limited number of labeled data. However, most of the state-of-the-art techniques of few-shot learning employ transfer learning, which still requires massive labeled data to train. To simulate the human learning mechanism, a deep model of few-shot learning is proposed to learn from one, or a few examples. First of all in this paper, we analyze and note that the problem with representative semi-supervised few-shot learning methods is getting stuck in local optimization and prototype bias problems. To address these challenges, we propose a new semi-supervised few-shot learning method with Convex Kullback-Leibler and critical descriptor prototypes, hereafter referred to as CKL. Specifically, CKL optimizes joint probability density via KL divergence, subsequently deriving a strictly convex function to facilitate global optimization in semi-supervised clustering. In addition, by incorporating dictionary learning, the critical descriptor facilitates the extraction of more prototypical features, thereby capturing more distinct feature information and avoiding the problem of prototype bias caused by limited labeled samples. Intensive experiments have been conducted on three popular benchmark datasets, and the experimental results show that this method significantly improves the classification ability of few-shot learning and obtains the most advanced performance. In the future, we will explore additional methods that can be integrated with deep learning to further uncover essential features within samples.},
  archive      = {J_APIN},
  author       = {Liu, Yukun and Shi, Daming},
  doi          = {10.1007/s10489-025-06239-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {A convex kullback-leibler divergence and critical-descriptor prototypes for semi-supervised few-shot learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning portfolio model based on mixture
of experts. <em>APIN</em>, <em>55</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06242-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of artificial intelligence, the portfolio management problem has received widespread attention. Portfolio models based on deep reinforcement learning enable intelligent investment decision-making. However, most models only consider modeling the temporal information of stocks, neglecting the correlation between stocks and the impact of overall market risk. Moreover, their trading strategies are often singular and fail to adapt to dynamic changes in the trading market. To address these issues, this paper proposes a Deep Reinforcement Learning Portfolio Model based on Mixture of Experts (MoEDRLPM). Firstly, a spatio-temporal adaptive embedding matrix is designed, temporal and spatial self-attention mechanisms are employed to extract the temporal information and correlations of stocks. Secondly, dynamically select the current optimal expert from the mixed expert pool through router. The expert makes decisions and aggregates to derive the portfolio weights. Next, market index data is utilized to model the current market risk and determine investment capital ratios. Finally, deep reinforcement learning is employed to optimize the portfolio strategy. This approach generates diverse trading strategies according to dynamic changes in the market environment. The proposed model is tested on the SSE50 and CSI300 datasets. Results show that the total returns of this model increase by 12% and 8%, respectively, while the Sharpe Ratios improve by 64% and 51%.},
  archive      = {J_APIN},
  author       = {Wei, Ziqiang and Chen, Deng and Zhang, Yanduo and Wen, Dawei and Nie, Xin and Xie, Liang},
  doi          = {10.1007/s10489-025-06242-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Deep reinforcement learning portfolio model based on mixture of experts},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffGen: A data-driven framework for generating truncated
differentials. <em>APIN</em>, <em>55</em>(5), 1–24. (<a
href="https://doi.org/10.1007/s10489-025-06248-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential cryptanalysis involves searching for high-probability differential trails. Traditionally, this search requires the use of constraint solvers or dedicated algorithms. Data-driven methods that rely on machine learning are typically limited to constructing statistical distinguishers for specific ciphers. In this paper, we develop a data-driven approach to the differential search problem by introducing DiffGen, a fully data-driven truncated differential search framework. DiffGen employs a metaheuristic algorithm with an active S-box prediction machine learning model as its fitness function to identify potentially valid truncated differentials within a given range of active S-boxes. A second machine learning model then validates the identified truncated differentials. We demonstrate the effectiveness of the DiffGen framework on generalized Feistel ciphers as a case study. Our results show that DiffGen can effectively generate valid truncated differentials, particularly when using particle swarm optimization as a metaheuristic and a differential validation model based on a fully connected artificial neural network. We verified that 84% of the truncated differentials generated by DiffGen in this setting correspond to actual differential trails. Our findings highlight, for the first time, the feasibility of applying a data-driven approach to the differential search problem.},
  archive      = {J_APIN},
  author       = {Idris, Mohamed Fadl and Teh, Je Sen and Yusoff, Mohd Najwadi},
  doi          = {10.1007/s10489-025-06248-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {DiffGen: A data-driven framework for generating truncated differentials},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage graph attention networks and q-learning based
maintenance tasks scheduling. <em>APIN</em>, <em>55</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06249-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maintenance tasks scheduling optimization is important and challenging for improving the oil and gas exploitation efficiency. Traditionally, this problem is addressed using exact algorithms, metaheuristic algorithms or solvers. However, due to the large-scale nature of this problem, these trials often fail in practical use. To address this, a compositional message passing neural network (CMPNN) is introduced for graph embedding, and the messages of the whole graph are obtained by aggregating the messages of neighboring nodes, which is used as the input of the subsequent framework. Based on CMPNN, a framework combining two-stage Graph Attention Networks and Q-learning (TSGAT+Q-learning) is proposed in this paper. In the first stage, the agent embedding is completed, i.e., each service technician’s messages are represented by a constructed graph; In the second phase, each maintenance task selects an agent based on probability. In this way, the task assignment scheme is obtained, and finally Q-learning is used for further optimization. In addition, a key contribution is the proposal of a novel exponential reward, designed to speed up model training using REINFORCE in reinforcement learning. To validate the effectiveness of proposed method, scenarios with different scales are provided. In most cases, TSGAT+Q-learning outperforms CPLEX, OR-Tools and other learning-based algorithms. Moreover, the trained networks can also solve the problem with varying numbers of maintenance tasks, which implies that TSGAT+Q-learning has good generalization ability. Finally, the proposed method is also proved to be effective in solving on-site maintenance tasks scheduling problem with multiple constraints. A Two-stage Graph Attention Networks and Q-learning Framework Based Maintenance Tasks Scheduling},
  archive      = {J_APIN},
  author       = {Gao, Xiaoyong and Peng, Diao and Yang, Yixu and Huang, Fuyu and Yuan, Yu and Tan, Chaodong and Li, Feifei},
  doi          = {10.1007/s10489-025-06249-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Two-stage graph attention networks and Q-learning based maintenance tasks scheduling},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive pre-training and instruction tuning for
cross-lingual aspect-based sentiment analysis. <em>APIN</em>,
<em>55</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06251-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Natural Language Processing (NLP), aspect-based sentiment analysis (ABSA) has always been one of the critical research areas. However, due to the lack of sufficient sentiment corpora in most languages, existing research mainly focuses on English texts, resulting in limited studies on multilingual ABSA tasks. In this paper, we propose a new pre-training strategy using contrastive learning to improve the performance of cross-lingual ABSA tasks, and we construct a semantic contrastive loss to align parallel sentence representations with the same semantics in different languages. Secondly, we introduce instruction prompt template tuning, which enables the language model to fully understand the task content and learn to generate the required targets through manually constructed instruction prompt templates. During the generation process, we create a more generic placeholder template-based structured output target to capture the relationship between aspect term and sentiment polarity, facilitating cross-lingual transfer. In addition, we have introduced a copy mechanism to improve task performance further. We conduct detailed experiments and ablation analyzes on eight languages to demonstrate the importance of each of our proposed components.},
  archive      = {J_APIN},
  author       = {Zhao, Wenwen and Yang, Zhisheng and Yu, Song and Zhu, Shiyu and Li, Li},
  doi          = {10.1007/s10489-025-06251-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Contrastive pre-training and instruction tuning for cross-lingual aspect-based sentiment analysis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matrix-based incremental local feature selection with
dynamic covering granularity. <em>APIN</em>, <em>55</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06253-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multigranulation rough set is composed of a set of granularities, providing a theoretical framework for solving problems from a multigranulation perspective. Feature selection aims to find the minimal set of attributes that does not compromise the overall classification capability. It has significant applications in the field of information processing. However, in practical application environments, the granularities in information systems often evolve dynamically over time. To address this scenario, an incremental feature selection algorithm for data with changing granularities in local multigranulation neighborhood covering rough sets is proposed. Firstly, the method of local related family is introduced, relationships between matrix operations of local related sets and those of approximate sets are discussed, and feature selection is studied using matrix methods. Subsequently, two matrix-based incremental feature selection algorithms are proposed for the cases where granularity structures in the data are added or deleted due to feature changes. Experiments on six datasets from UCI are then conducted to evaluate the performance of the proposed algorithms. The experimental results demonstrate that the two proposed incremental feature selection algorithms are highly effective.},
  archive      = {J_APIN},
  author       = {Shi, Qi and Zhang, Yan-Lan},
  doi          = {10.1007/s10489-025-06253-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Matrix-based incremental local feature selection with dynamic covering granularity},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User similarity-based graph convolutional neural network for
shilling attack detection. <em>APIN</em>, <em>55</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06254-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative recommendation systems have been widely used in various fields, such as movies, music and e-commerce. However, due to the natural openness of its ratings, it is vulnerable to shilling attacks. Shilling attacks greatly affect the accuracy and trustworthiness of recommendation systems, so we urgently need effective methods to counter shilling attacks. Some detection methods have been proposed previously. However, they mostly use manual feature extraction-based methods. These methods require specialized statistical knowledge to summarize user-specific rating patterns in user rating databases, which is very difficult. Thus, we propose a method called User Similarity-based Graph convolutional neural network for Shilling Attack Detection (USGSAD). This method achieves the purpose of detecting shilling attacks without using manual features. First, our method calculates user similarity by jointing both correlation and deviation of user rating behaviors. Second, we build a user relationship graph based on user similarity matrix and use graph embedding method to obtain user low-dimensional embedding vectors. Finally, we design a User Similarity Graph Convolutional Network (USGCN) to assign weights to aggregate user embeddings and predict the attackers in the recommender system. Adequate experiments on Amazon and MovieLens datasets show that our proposed method outperforms the baseline methods in detection performance.},
  archive      = {J_APIN},
  author       = {Zhang, Yan and Hao, Qingbo and Zheng, Wenguang and Xiao, Yingyuan},
  doi          = {10.1007/s10489-025-06254-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {User similarity-based graph convolutional neural network for shilling attack detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep subspace clustering via latent representation learning.
<em>APIN</em>, <em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06255-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep subspace clustering networks (DSC-Nets), which combine deep autoencoders and self-expressive modules, have garnered widespread attention due to their outstanding performance. Within these networks, the autoencoder captures the latent representations of data by reconstructing the input data, while the self-expressive layer learns an affinity matrix based on these latent representations. This matrix guides spectral clustering, ultimately completing the clustering task. However, the latent representations learned solely through self-reconstruction by the autoencoder lack discriminative power. The quality of these latent representations directly affects the performance of the affinity matrix, which inevitably limits the clustering performance. To address this issue, we propose learning dissimilar relationships between samples using a classification module, and similar relationships using the self-expressive module. We integrate the information from both modules to construct a graph based on learned similarities, which is then embedded into the autoencoder network. Furthermore, we introduce a pseudo-label supervision module to guide the learning of higher-level similarities in the latent representations, thus achieving more discriminative latent features. Additionally, to enhance the quality of the affinity matrix, we employ an entropy norm constraint to improve connectivity within the subspaces. Experimental results on four public datasets demonstrate that our method achieves superior performance compared to other popular subspace clustering approaches.},
  archive      = {J_APIN},
  author       = {Pei, Shenglei and Han, Qinghao and Hao, Zepu and Zhao, Hong},
  doi          = {10.1007/s10489-025-06255-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Deep subspace clustering via latent representation learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-aware recommendation based on hypergraph
representation learning and transformer model optimization.
<em>APIN</em>, <em>55</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06257-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation algorithms based on knowledge graphs (KGs) have been a research focus and hotspot in the field of recommendation systems in recent years. This is mainly because the introduction of a KG can yield auxiliary information about the item of interest and achieve more accurate recommendation effects for users. However, such a model faces two major challenges in predictive tasks. First, these methods find it difficult to capture the interaction information between users and items from a global perspective. Second, in most cases, noisy data, which result from users mistakenly clicking on items they are not interested in, exist in KGs, and these noisy data have a negative impact on the resulting recommendation effect. To address these challenges, this paper proposes a knowledge-aware recommendation approach based on hypergraph representation learning and transformer model optimization (KHRT), which employs a hypergraph that can be used to directly model higher-order relations, thereby enriching the interaction information between users and items. Owing to the lack of global interaction information between users and items in local graphs, a global hypergraph is constructed within the given local graph. Conversely, nonglobal graphs contain redundant information; thus, a nonglobal hypergraph is constructed, enabling the capture of more comprehensive interaction information between users and items. Moreover, the multihead attention mechanism of the transformer model is used to enhance the cooperative relationships between user nodes and item nodes, and more valuable preference information is mined from noisy user interaction data, such as items that users are not interested in. The embeddings of user and item nodes are optimized to alleviate noise interference and achieve improved recommendation performance based on user preferences. Experiments conducted on three real recommendation datasets indicate that the proposed approach outperforms the state-of-the-art traditional recommendation methods and KG-based methods in almost all comparisons.},
  archive      = {J_APIN},
  author       = {Zuo, Yuqi and Zhang, Yunfeng and Zhang, Qiuyue and Zhang, Wenbo},
  doi          = {10.1007/s10489-025-06257-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge-aware recommendation based on hypergraph representation learning and transformer model optimization},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discriminative identification of redundant features for
multi-label feature selection. <em>APIN</em>, <em>55</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06258-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection is a hotspot in multi-label learning, aiming to tackle the curse of dimensionality. Recently, several embedded models based on sparsity regularization have emerged. Most of them focus on learning an optimal feature selection matrix by means of regression, in which the correlation of instances and labels is concerned. However, the redundancy between features and the discriminative structure of labels have not been involved in. To argue these issues, a novel approach named discriminative identification of redundant features for multi-label feature selection (DIRF) is explored. In the proposed model, a feature affinity graph is constructed to find potentially redundant features with the idea that high similarity between features implies redundancy. Meanwhile, discriminative label correlation is revealed in terms of both label similarity and dissimilarity. Two regularizers are thereby designed to penalize the weights of redundant features. The structural consistency between original labels and predicted labels is therefore maintained. Extensive experiments and analysis show that the proposed DIRF outperforms the state-of-the-art multi-label feature selection methods.},
  archive      = {J_APIN},
  author       = {Jia, Qingwei and Deng, Tingquan and Zhang, Ziang and Wang, Yan and Wang, Changzhong},
  doi          = {10.1007/s10489-025-06258-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Discriminative identification of redundant features for multi-label feature selection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An LSTM approach to predict emergency events using spatial
features. <em>APIN</em>, <em>55</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06261-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the global population on the rise, the frequency and severity of emergency events like fires and traffic accidents are becoming more frequent and severe. Attending to these emergencies demands valuable and limited resources, such as professionals and vehicles, so it is important to efficiently allocate them to regions that are more likely to require their services. However, the fact that emergencies can be related to spatial and temporal contexts makes resource allocation a highly complex task requiring specialized tools and techniques to exploit these relationships efficiently. This paper proposes an emergency event prediction solution using spatial segmentation and Long Short-Term Memory (LSTM) neural networks to model associations in space and time domains. We used data from real emergency occurrences in Florianópolis, Brazil, collected over five and a half years. Clustering algorithms combined with the silhouette metric were used to segment the time series in four different city regions. A comparison with traditional forecasting techniques and machine learning models showed that the LSTM network is consistent in its predictions and outperforms other approaches. Compared with a state-of-the-art reference employing LSTM, our solution leads to a 17.8% reduction in mean absolute error. Two methodologies for multi-step lookahead prediction are also presented and compared, showing that reusing the output of LSTM to predict future time steps is better than a full model retraining. To assess the generalizability of the model and proposed methodology, we applied the entire pipeline to new data from a different city. Our results demonstrate that models tailored to specific cities significantly outperform those trained on generalized datasets, highlighting the importance of localized training data.},
  archive      = {J_APIN},
  author       = {Vieira Roque, Felipe and Fröhlich, Antônio Augusto and Grellert, Mateus},
  doi          = {10.1007/s10489-025-06261-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {An LSTM approach to predict emergency events using spatial features},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrowdFPN: Crowd counting via scale-enhanced and
location-aware feature pyramid network. <em>APIN</em>, <em>55</em>(5),
1–13. (<a href="https://doi.org/10.1007/s10489-025-06263-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting has emerged as a prevalent research direction within computer vision, focusing on estimating the number of pedestrians in images or videos. However, existing methods tend to ignore crowd location information and model efficiency, leading to reduced accuracy due to challenges such as multi-scale variations and intricate background interferences. To address these issues, we propose the scale-enhanced and location-aware feature pyramid network for crowd counting (CrowdFPN). First, it can fine-tune each feature layer to focus more on crowd objects within a specific scale through the Scale Enhancement Module. Then, feature information from different layers is effectively fused using the lightweight Adaptive Bi-directional Feature Pyramid Network. Recognizing the importance of crowd location information for accurate counting, we introduce the Location Awareness Module, which embeds crowd location data into the channel attention mechanism while mitigating the effects of complex background interference. Finally, extensive experiments on four popular crowd counting datasets demonstrate the effectiveness of the proposed model. The code is available at https://github.com/zf990312/CrowdFPN.},
  archive      = {J_APIN},
  author       = {Yu, Ying and Zhu, Feng and Qian, Jin and Fujita, Hamido and Yu, Jiamao and Zeng, Kangli and Chen, Enhong},
  doi          = {10.1007/s10489-025-06263-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {CrowdFPN: Crowd counting via scale-enhanced and location-aware feature pyramid network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-spatial guided context propagation network for
camouflaged object detection. <em>APIN</em>, <em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06264-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to detect objects that blend in with their surroundings and is a challenging task in computer vision. High-level semantic information and low-level spatial information play important roles in localizing camouflaged objects and reinforcing spatial cues. However, current COD methods directly connect high-level features with low-level features, ignoring the importance of the respective features. In this paper, we design a Semantic-spatial guided Context Propagation Network (SCPNet) to efficiently mine semantic and spatial features while enhancing their feature representations. Firstly, we design a twin positioning module (TPM) to explore semantic cues to accurately locate camouflaged objects. Afterward, we introduce a spatial awareness module (SAM) to mine spatial cues in shallow features deeply. Finally, we develop a context propagation module (CPM) to assign semantic and spatial cues to multi-level features and enhance their feature representations. Experimental results show that our SCPNet outperforms state-of-the-art methods on three challenging datasets. Codes will be made available at https://github.com/RJC0608/SCPNet .},
  archive      = {J_APIN},
  author       = {Ren, Junchao and Zhang, Qiao and Kang, Bingbing and Zhong, Yuxi and He, Min and Ge, Yanliang and Bi, Hongbo},
  doi          = {10.1007/s10489-025-06264-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Semantic-spatial guided context propagation network for camouflaged object detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic pruning rate adjustment for dynamic token
reduction in vision transformer. <em>APIN</em>, <em>55</em>(5), 1–15.
(<a href="https://doi.org/10.1007/s10489-025-06265-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformer (ViT) has demonstrated excellent accuracy in image recognition and has been actively studied in various fields. However, ViT requires a large matrix multiplication called Attention, which is computationally expensive. Since the computational cost of Self-Attention used in ViT increases quadratically with the number of tokens, research to reduce the computational cost by pruning the number of tokens has been active in recent years. To prune tokens, it is necessary to set the pruning rate, and in many studies, the pruning rate is set manually. However, it is difficult to manually determine the optimal pruning rate because the appropriate pruning rate varies from task to task. In this study, we propose a method to solve this problem. The proposed pruning rate adjustment adjusts the pruning rate so that the training loss is converged by Gradient-Aware Scaling (GAS). In addition, we propose Variable Proportional Attention (VPA) for Top-K, a general-purpose token pruning method, to mitigate the performance loss due to pruning. For the CIFAR-10 dataset, several competitive pruning methods improve recognition accuracy over manually setting the pruning rate; eTPS+Adjust on Hybrid ViT-S achieves 99.01% Accuracy with -31.68% FLOPs. Furthermore, Top-K+VPA outperforms token merging when the pruning rate is large for trained ViT-L inference on ImageNet-1k and has superior scalability in the Accuracy-Latency relation. In particular, when Top-K+VPA is applied to ViT-L on a GPU environment with a pruning rate of 6%, it achieves 80.62% Accuracy on the ImageNet-1k dataset with -50.44% FLOPs and -46.8% Latency.},
  archive      = {J_APIN},
  author       = {Ishibashi, Ryuto and Meng, Lin},
  doi          = {10.1007/s10489-025-06265-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Automatic pruning rate adjustment for dynamic token reduction in vision transformer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting PPI components using a hybrid hierarchical
prediction framework with parameter adaptive transfer algorithm.
<em>APIN</em>, <em>55</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06275-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of economic forecasting directly influences the formulation of economic policies and profoundly impacts the stable operation of the economy. As a pivotal indicator of economic activity, predicting the producer price index (PPI) is crucial. Although most existing research is focused on the overall PPI, economic and financial institutions are increasingly interested in its partially disaggregated components. Therefore, this paper proposes a hybrid hierarchical deep network prediction framework called Attention-HRNN-GRU (AHG) with parameter adaptive transfer, which integrates algorithms such as the attention mechanism, the Hierarchical Recurrent Neural Network (HRNN) and the Gated Recurrent Unit (GRU). First, an independent GRU network is designed and trained separately for each PPI level to perform preliminary predictions. The internal parameters of each level’s network are preserved to facilitate interlevel information transfer, forming an initial HRNN framework. An attention mechanism is then introduced to adaptively adjust the parameters of the upper-level prediction model that are used as the prediction parameters for the lower-level model. This process enables effective information transfer across multiple levels, producing high-accuracy prediction outcomes. This method effectively addresses a common issue in traditional hierarchical data prediction, where the direct application of upper-level parameters to lower-level data often overlooks variations between sequences. Experimental results show that the proposed AHG model markedly reduces prediction errors compared with those of the current advanced HRNN model. For example, the root mean square error (RMSE) of the producing materials index improved by 13.11% over that of the advanced model.},
  archive      = {J_APIN},
  author       = {Zhu, Jiaming and Dai, Wan and Wu, Jie and Zhang, Xiang and Chen, Huayou},
  doi          = {10.1007/s10489-025-06275-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Forecasting PPI components using a hybrid hierarchical prediction framework with parameter adaptive transfer algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The synchronisation control of fractional 4-d quantum game
chaotic map with its application in image encryption. <em>APIN</em>,
<em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06281-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to reflect the nonlinear dynamics of quantum game map more precisely and improve the performance of current cryptosystem, in this paper, we first obtain the fractional four-dimensional quantum game chaotic map according to the fractional calculus. We then proceed to obtain the trajectory, the maximum Lyapunov index, the bifurcation diagram of the aforementioned map, and compare its dynamical behaviours with those of the four-dimensional quantum game chaotic map. Subsequently, we design a synchronisation control system for the proposed fractional system and apply the two systems to the field of information security. Finally, we undertake a comprehensive analysis of the encryption system, examining it from five distinct perspectives. As a result, the key space of proposed algorithm equals to $$6.7 \times 10^{165}$$ and the encrypted image’s information entropy is 7.9997. The NPCR and UACI for the difference attack analysis is $$99.61\%$$ and $$33.44\%$$ respectively accompanied with that the correlation coefficients is near to 0. The result indicates that the proposed algorithm can defense both known plaintext and chosen plaintext attacks which means that it is superior to other algorithms in most of the above aspects.},
  archive      = {J_APIN},
  author       = {Liu, Zeyu and Feng, Binshuai and Yao, Yuxin and Wang, Xujing},
  doi          = {10.1007/s10489-025-06281-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {The synchronisation control of fractional 4-D quantum game chaotic map with its application in image encryption},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel dropout approach for mitigating over-smoothing in
graph neural networks. <em>APIN</em>, <em>55</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06285-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have emerged as a powerful tool for analyzing structured data represented as graphs. They offer significant contributions across various domains due to their ability to effectively capture and process complex relational information. However, most existing GNNs still suffer from undesirable phenomena such as non-robustness, overfitting, and over-smoothing. These challenges have raised significant interest among researchers. In this context, this work aims to address these issues by proposing a new vision of Dropout named A-DropEdge. First, it applies a message-passing layer to ensure the connection between nodes and avoid dropping in the input. Then, the information propagates through many branches with different random configurations to enhance the aggregation process. Moreover, consistency regularization is adopted to perform self-supervised learning. The experimental results on three graph data sets including Cora, Citeseer, and PubMed show the robustness and performance of the proposed approach in mitigating the over-smoothing problem.},
  archive      = {J_APIN},
  author       = {Hssayni, El houssaine and Boufssasse, Ali and Joudar, Nour-Eddine and Ettaouil, Mohamed},
  doi          = {10.1007/s10489-025-06285-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Novel dropout approach for mitigating over-smoothing in graph neural networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network adaptive terminal sliding mode trajectory
tracking control for mechanical leg systems with uncertainty.
<em>APIN</em>, <em>55</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06228-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an adaptive terminal sliding mode control based on neural block approximation for mechanical leg systems characterized by uncertainty and external disturbances. This control is based on a dynamic model of the mechanical leg and introduces an ideal system trajectory as a constraint. The structure of the paper is as follows. First, the RBF neural network is used to approximate the parameters of the dynamic model in blocks. This process is supplemented with a nonsingular terminal sliding mode surface to accelerate the convergence of tracking errors, and an adaptive law is used to adjust weights online to reconstruct the mechanical leg model. Next, an integral sliding mode control robust component is provided to mitigate external disturbances and correct model inaccuracies. Within this step, the Lyapunov method is used to prove the finite-time stability and uniform boundedness of the control system. Finally, the algorithm is validated and tested using the CAPACE rapid control system on a three-degree-of-freedom mechanical leg platform. The experimental results show that the proposed RBFTSM algorithm performs well in the performance evaluation of the MASE and RMSE values, with high trajectory tracking accuracy, anti-interference ability and strong robustness. Further evidence is presented to demonstrate the effectiveness and practicality of the proposed method.},
  archive      = {J_APIN},
  author       = {Chen, Minbo and Hu, Likun and Liao, Zifeng},
  doi          = {10.1007/s10489-025-06228-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Neural network adaptive terminal sliding mode trajectory tracking control for mechanical leg systems with uncertainty},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A knowledge graph completion model based on weighted fusion
description information and transform of the dimension and the scale.
<em>APIN</em>, <em>55</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06230-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing knowledge graph completion model represents entity and description information by uniform fusion. The convolutional kernel has fewer sliding steps on a triplet matrix composed of entities and relations and does not obtain different-scale characteristics for entities and relations. In this study, a knowledge graph completion model based on weighted fusion description information and the transformation of the dimension and scale, EDMSConvKE, is proposed. First, the entity description information is obtained using the SimCSE model of comparative learning and then combined with the entity according to a certain weight to obtain an entity vector with a stronger expression ability. Second, the head entity, relation, and tail entity vectors are combined into a three-column matrix, and a new matrix is generated using a dimensional transformation strategy to increase the number of sliding steps of the convolution kernel and enhance the information interaction ability of the entity and relation in more dimensions. Third, the multi-scale semantic features of the triples were extracted using convolution kernels of different sizes. Finally, the model in this study was evaluated using a link-prediction experiment, and the model was significantly improved in the Hits@10 and mean rank (MR) indices.},
  archive      = {J_APIN},
  author       = {Yin, Panfei and Zhao, Erping and BianBaDroMa and Ngodrup},
  doi          = {10.1007/s10489-025-06230-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {A knowledge graph completion model based on weighted fusion description information and transform of the dimension and the scale},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-parameter oversampling approach for imbalanced data
classification based on hybrid natural neighbors. <em>APIN</em>,
<em>55</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10489-025-06236-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, researchers have developed numerous interpolation-based oversampling techniques to tackle class imbalance in classification tasks. However, most existing techniques encounter the challenge of k parameter due to the involvement of k nearest neighbor (kNN). Furthermore, they only adopt one sole neighborhood rule, disregarding the positional characteristics of minority samples. This often leads to the generation of synthetic noise or overlapping samples. This paper proposes a non-parameter oversampling framework called the hybrid natural neighbor synthetic minority oversampling technique (HNaNSMOTE). HNaNSMOTE effectively determines an appropriate k value through iterative search and adopts a hybrid neighborhood rule for each minority sample to generate more representative and diverse synthetic samples. Specifically, 1) a hybrid natural neighbor search procedure is conducted on the entire dataset to obtain a data-related k value, which eliminates the need for manually preset parameters. Different natural neighbors are formed for each sample to better identify the positional characteristics of minority samples during the procedure. 2) To improve the quality of the generated samples, the hybrid natural neighbor (HNaN) concept has been proposed. HNaN utilizes kNN and reverse kNN to find neighbors adaptively based on the distribution of minority samples. It is beneficial for mitigating the generation of synthetic noise or overlapping samples since it takes into account the existence of majority samples. Experimental results on 32 benchmark binary datasets with three classifiers demonstrate that HNaNSMOTE outperforms numerous state-of-the-art oversampling techniques for imbalanced classification in terms of Sensitivity and G-mean.},
  archive      = {J_APIN},
  author       = {Lin, Junyue and Liang, Lu},
  doi          = {10.1007/s10489-025-06236-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {A non-parameter oversampling approach for imbalanced data classification based on hybrid natural neighbors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="alg---5">Alg - 5</h2>
<ul>
<li><details>
<summary>
(2025). Complexity framework for forbidden subgraphs i: The
framework. <em>Alg</em>, <em>87</em>(3), 429–464. (<a
href="https://doi.org/10.1007/s00453-024-01289-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a set of graphs $${\mathcal {H}}$$ , a graph G is $${\mathcal {H}}$$ -subgraph-free if G does not contain any graph from $${{{\mathcal {H}}}}$$ as a subgraph. We propose general and easy-to-state conditions on graph problems that explain a large set of results for $${\mathcal {H}}$$ -subgraph-free graphs. Namely, a graph problem must be efficiently solvable on graphs of bounded treewidth, computationally hard on subcubic graphs, and computational hardness must be preserved under edge subdivision of subcubic graphs. Our meta-classification says that if a graph problem $$\Pi $$ satisfies all three conditions, then for every finite set $${{{\mathcal {H}}}}$$ , it is “efficiently solvable” on $${{{\mathcal {H}}}}$$ -subgraph-free graphs if $${\mathcal {H}}$$ contains a disjoint union of one or more paths and subdivided claws, and $$\Pi $$ is “computationally hard” otherwise. We apply our meta-classification on many well-known partitioning, covering and packing problems, network design problems and width parameter problems to obtain a dichotomy between polynomial-time solvability and NP-completeness. For distance-metric problems, we obtain a dichotomy between almost-linear-time solvability and having no subquadratic-time algorithm (conditioned on some hardness hypotheses). Apart from capturing a large number of explicitly and implicitly known results in the literature, we also prove a number of new results. Moreover, we perform an extensive comparison between the subgraph framework and the existing frameworks for the minor and topological minor relations, and pose several new open problems and research directions.},
  archive      = {J_Alg},
  author       = {Johnson, Matthew and Martin, Barnaby and Oostveen, Jelle J. and Pandey, Sukanya and Paulusma, Daniël and Smith, Siani and van Leeuwen, Erik Jan},
  doi          = {10.1007/s00453-024-01289-2},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {429-464},
  shortjournal = {Algorithmica},
  title        = {Complexity framework for forbidden subgraphs i: The framework},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FREIGHT: Fast streaming hypergraph partitioning.
<em>Alg</em>, <em>87</em>(3), 405–428. (<a
href="https://doi.org/10.1007/s00453-024-01291-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitioning the vertices of a (hyper)graph into k roughly balanced blocks such that few (hyper)edges run between blocks is a key problem for large-scale distributed processing. A current trend for partitioning huge (hyper)graphs using low computational resources are streaming algorithms. In this work, we propose FREIGHT: a Fast stREamInG Hypergraph parTitioning algorithm which is an adaptation of the widely-known graph-based algorithm Fennel. By using an efficient data structure, we make the overall running of FREIGHT linearly dependent on the pin-count of the hypergraph and the memory consumption linearly dependent on the numbers of nets and blocks. The results of our extensive experimentation showcase the promising performance of FREIGHT as a highly efficient and effective solution for streaming hypergraph partitioning. Our algorithm demonstrates competitive running time with the Hashing algorithm, with a geometric mean runtime within a factor of four compared to the Hashing algorithm. Significantly, our findings highlight the superiority of FREIGHT over all existing (buffered) streaming algorithms and even the in-memory algorithm HYPE, with respect to both cut-net and connectivity measures. This indicates that our proposed algorithm is a promising hypergraph partitioning tool to tackle the challenge posed by large-scale and dynamic data processing.},
  archive      = {J_Alg},
  author       = {Eyubov, Kamal and Fonseca Faraj, Marcelo and Schulz, Christian},
  doi          = {10.1007/s00453-024-01291-8},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {405-428},
  shortjournal = {Algorithmica},
  title        = {FREIGHT: Fast streaming hypergraph partitioning},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shared versus private randomness in distributed interactive
proofs. <em>Alg</em>, <em>87</em>(3), 377–404. (<a
href="https://doi.org/10.1007/s00453-024-01288-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed interactive proofs, the nodes of a graph G interact with a powerful but untrustable prover who tries to convince them, in a small number of rounds and through short messages, that G satisfies some property. This series of rounds is followed by a phase of distributed verification, which may be either deterministic or randomized, where nodes exchange messages with their neighbors. The nature of this last verification round defines the two types of interactive protocols. We say that the protocol is of Arthur–Merlin type if the verification round is deterministic. We say that the protocol is of Merlin–Arthur type if, in the verification round, the nodes are allowed to use a fresh set of random bits. In the original model introduced by Kol, Oshman, and Saxena [PODC 2018], the randomness was private in the sense that each node had only access to an individual source of random coins. Crescenzi, Fraigniaud, and Paz [DISC 2019] initiated the study of the impact of shared randomness (the situation where the coin tosses are visible to all nodes) in the distributed interactive model. In this work, we continue that research line by showing that the impact of the two forms of randomness is very different depending on whether we are considering Arthur–Merlin protocols or Merlin–Arthur protocols. While private randomness gives more power to the first type of protocols, shared randomness provides more power to the second. We also show that there exists at most an exponential gap between the certificate size in distributed interactive proofs with respect to distributed verification protocols without any randomness.},
  archive      = {J_Alg},
  author       = {Montealegre, Pedro and Ramírez-Romero, Diego and Rapaport, Ivan},
  doi          = {10.1007/s00453-024-01288-3},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {377-404},
  shortjournal = {Algorithmica},
  title        = {Shared versus private randomness in distributed interactive proofs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient algorithm for power dominating set.
<em>Alg</em>, <em>87</em>(3), 344–376. (<a
href="https://doi.org/10.1007/s00453-024-01283-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem Power Dominating Set (PDS) is motivated by the placement of phasor measurement units to monitor electrical networks. It asks for a minimum set of vertices in a graph that observes all remaining vertices by exhaustively applying two observation rules. Our contribution is twofold. First, we determine the parameterized complexity of PDS by proving it is W[P]-complete when parameterized with respect to the solution size. We note that it was only known to be W[2]-hard before. Our second and main contribution is a new algorithm for PDS that efficiently solves practical instances. Our algorithm consists of two complementary parts. The first is a set of reduction rules for PDS that can also be used in conjunction with previously existing algorithms. The second is an algorithm for solving the remaining kernel based on the implicit hitting set approach. Our evaluation on a set of power grid instances from the literature shows that our solver outperforms previous state-of-the-art solvers for PDS by more than one order of magnitude on average. Furthermore, our algorithm can solve previously unsolved instances of continental scale within a few minutes.},
  archive      = {J_Alg},
  author       = {Bläsius, Thomas and Göttlicher, Max},
  doi          = {10.1007/s00453-024-01283-8},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {344-376},
  shortjournal = {Algorithmica},
  title        = {An efficient algorithm for power dominating set},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry breaking in the plane. <em>Alg</em>,
<em>87</em>(3), 321–343. (<a
href="https://doi.org/10.1007/s00453-024-01286-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a fundamental question related to the feasibility of deterministic symmetry breaking in the infinite Euclidean plane for two robots that have minimal or no knowledge of the respective capabilities and “measuring instruments” of themselves and each other. Assume that two anonymous mobile robots are placed at different locations at unknown distance d from each other on the infinite Euclidean plane. Each robot knows neither the location of itself nor of the other robot. The robots cannot communicate wirelessly, but have a certain nonzero visibility radius r (with range r unknown to the robots). By rendezvous we mean that they are brought at distance at most r of each other by executing symmetric (identical) mobility algorithms. The robots are moving with unknown and constant but not necessarily identical speeds, their clocks and pedometers may be asymmetric, and their chirality inconsistent. We demonstrate that rendezvous for two robots is feasible under the studied model iff the robots have either: different speeds; or different clocks; or different orientations but equal chiralities. When the rendezvous is feasible, we provide a universal algorithm which always solves rendezvous despite the fact that the robots have no knowledge of which among their respective parameters may be different.},
  archive      = {J_Alg},
  author       = {Czyzowicz, Jurek and Gąsieniec, Leszek and Killick, Ryan and Kranakis, Evangelos},
  doi          = {10.1007/s00453-024-01286-5},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {321-343},
  shortjournal = {Algorithmica},
  title        = {Symmetry breaking in the plane},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="bcyb---2">BCYB - 2</h2>
<ul>
<li><details>
<summary>
(2025). Antifragile control systems in neuronal processing: A
sensorimotor perspective. <em>BCYB</em>, <em>119</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s00422-025-01003-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability–robustness–resilience–adaptiveness continuum in neuronal processing follows a hierarchical structure that explains interactions and information processing among the different time scales. Interestingly, using “canonical” neuronal computational circuits, such as Homeostatic Activity Regulation, Winner-Take-All, and Hebbian Temporal Correlation Learning, one can extend the behavior spectrum towards antifragility. Cast already in both probability theory and dynamical systems, antifragility can explain and define the interesting interplay among neural circuits, found, for instance, in sensorimotor control in the face of uncertainty and volatility. This perspective proposes a new framework to analyze and describe closed-loop neuronal processing using principles of antifragility, targeting sensorimotor control. Our objective is two-fold. First, we introduce antifragile control as a conceptual framework to quantify closed-loop neuronal network behaviors that gain from uncertainty and volatility. Second, we introduce neuronal network design principles, opening the path to neuromorphic implementations and transfer to technical systems.},
  archive      = {J_BCYB},
  author       = {Axenie, Cristian},
  doi          = {10.1007/s00422-025-01003-7},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Biol. Cybern.},
  title        = {Antifragile control systems in neuronal processing: A sensorimotor perspective},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counteracting uncertainty: Exploring the impact of anxiety
on updating predictions about environmental states. <em>BCYB</em>,
<em>119</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00422-025-01006-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anxious emotional states disrupt decision-making and control of dexterous motor actions. Computational work has shown that anxiety-induced uncertainty alters the rate at which we learn about the environment, but the subsequent impact on the predictive beliefs that drive action control remains to be understood. In the present work we tested whether anxiety alters predictive (oculo)motor control mechanisms. Thirty participants completed an experimental task that consisted of manual interception of a projectile performed in virtual reality. Participants were subjected to conditions designed to induce states of high or low anxiety using performance incentives and social-evaluative pressure. We measured subsequent effects on physiological arousal, self-reported state anxiety, and eye movements. Under high pressure conditions we observed visual sampling of the task environment characterised by higher variability and entropy of position prior to release of the projectile, consistent with an active attempt to reduce uncertainty. Computational modelling of predictive beliefs, using gaze data as inputs to a partially observable Markov decision process model, indicated that trial-to-trial updating of predictive beliefs was reduced during anxiety, suggesting that updates to priors were constrained. Additionally, state anxiety was related to a less deterministic mapping of beliefs to actions. These results support the idea that organisms may attempt to counter anxiety-related uncertainty by moving towards more familiar and certain sensorimotor patterns.},
  archive      = {J_BCYB},
  author       = {Harris, David and Arthur, Tom and Wilson, Mark and Le Gallais, Ben and Parsons, Thomas and Dill, Ally and Vine, Sam},
  doi          = {10.1007/s00422-025-01006-4},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Biol. Cybern.},
  title        = {Counteracting uncertainty: Exploring the impact of anxiety on updating predictions about environmental states},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="cc---66">CC - 66</h2>
<ul>
<li><details>
<summary>
(2025). Cognitive-inspired spectral spatiotemporal analysis for
emotion recognition utilizing electroencephalography signals.
<em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10361-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of computer technologies, along with the significant role of emotions in daily life, has driven interest in intelligent emotion recognition systems. Electroencephalography (EEG) serves as a prominent objective tool in affective computing. However, effectively integrating multichannel EEG spatial and temporal information remains a critical challenge. This study introduces a novel emotion recognition model grounded in cognitive and biological principles, emphasizing the importance of spatiotemporal dynamics in emotional processing. In this research, brain frequency bands were extracted through wavelet analysis, and the signals within predefined time windows were quantified. These features were then concatenated across distinct brain channels to create a comprehensive matrix representing spatiotemporal brain information. The matrix was characterized using both the summation of matrix cells and the highest singular value to optimize computational costs during classification. The resulting attributes were input into a classification module for emotion detection. Experimental results on the Database for Emotion Analysis using Physiological Signals (DEAP) achieved a maximum accuracy of 89.55%. This work introduces a novel approach to analyzing and classifying EEG signals elicited by various emotional stimuli, demonstrating that the proposed model is competitive with the state-of-the-art classification schemes, thereby paving the way for future development of a robust spatiotemporal-based EEG emotion recognition system.},
  archive      = {J_CC},
  author       = {Goshvarpour, Atefeh and Goshvarpour, Ateke},
  doi          = {10.1007/s12559-024-10361-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Cognitive-inspired spectral spatiotemporal analysis for emotion recognition utilizing electroencephalography signals},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A levitated controlled attention for named entity
recognition. <em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10381-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlled attention is a mechanism developed in cognitive neuroscience. It has been successfully applied to support named entity recognition, where the start and end boundaries of a possible named entity are marked by two specific tokens to indicate its position in a sentence. Then, it is fed into a deep network for classification. The entity boundary markers enable a deep neural network to be aware of entity boundaries and build the contextual dependency of a sentence relevant to entity boundaries. The problem with this strategy is that every possible named entity must be evaluated independently. This leads to very high computational complexity and cannot construct the semantic dependency between different named entities. In this paper, a levitated controlled attention mechanism is presented for named entity recognition. In this method, all possible named entities are fed together into a deep network for one-pass classification, which can establish the semantic dependency between contextual features and possible named entities. In the experiments, the levitated controlled attention is evaluated on four public datasets. The results show that it not only considerably reduces the computational complexity but also improves the performance of named entity recognition.},
  archive      = {J_CC},
  author       = {Huang, Rong and Chen, Yanping and Huang, Ruizhang},
  doi          = {10.1007/s12559-024-10381-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A levitated controlled attention for named entity recognition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph neural network with spatial attention for emotion
analysis. <em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10358-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition plays a crucial role in the diagnosis and treatment of various mental disorders. Research studies revealed the close relationship between brain regions and their functional roles in emotions. Propose a learning method that extends graph neural networks and takes into account the spatial relationship between EEG channels and their contributions of different regions of the brain to human emotions. Our method uses the adjacency matrix to model the spatial topological relationships in multi-channel EEG signals and learns weights to adjust their contributions to the classification. Extensive evaluation is conducted using public data sets, including comparison studies with state-of-the-art methods and performance analysis. In our comparison studies, our method demonstrates superior performance in terms of average accuracy. It is demonstrated that the proposed method improves the accuracy of emotion recognition and analyzes the brain at a fine granularity to decide the part that is most related to the triggering of the emotion.},
  archive      = {J_CC},
  author       = {Chen, Tian and Li, Lubao and Yuan, Xiaohui},
  doi          = {10.1007/s12559-024-10358-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A graph neural network with spatial attention for emotion analysis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining interpretable embedded multicriteria feature
cross-selection engineering and machine learning to mimic the brain for
stock trading signal prediction. <em>CC</em>, <em>17</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s12559-024-10365-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock trading signal prediction is very important for investors’ trading decisions. However, since the stock market is a complex and nonlinear system, stock trading is frequent and complex. Human beings cannot integrate all the relevant information in time and make the right decisions by their brains alone. Machine learning can mimic the brain, learn from experience, and discover the connection between different things, thus realizing correct prediction and decision-making. Therefore, this study innovatively proposes a fusion of interpretable embedded multicriteria feature cross-selection engineering to capture effective features. Meanwhile, an optimized neural network prediction model is proposed where the Bayesian (BO) algorithm assumes the task of searching for hyperparameter combinations. The methods are as follow: (1) Daily stock prices are categorized into four types of key points for stock trading signals based on the time series extreme point algorithm. (2) A more comprehensive range of impact factors is constructed. Starting from the stock’s historical trading data, based on the stock’s trend, volatility, and turnover flow, five categories of technical indicators are constructed: Overlap Study, Momentum Indicator, Momentum Indicator, Volatility Indicator, and Price Conversion. (3) To construct a feature cross-selection method with multiple feature screening criteria to find the optimal feature influencing factors from different evaluation dimensions. (4) The hyper-parameters of the Artificial Neural Network (ANN) are optimized using Bayesian optimization algorithm. The optimized ANN is then used to model the data and obtain predictions. Twenty stocks were randomly selected from Shanghai Stock Exchange and Shenzhen Stock Exchange as experimental data to verify the validity of the model. The accuracy of the model proposed in this paper is 54.83%, 55.46%, and 54.70% for stocks with upward, steady, and downward trends respectively. The accuracy is on average 7.93%, 8.09%, and 8.09% higher than the comparison model. The return on investment through the predicted results of the model is 21.87%, 7.76%, and −3.51% respectively, which is better than the other comparative models. It can be seen from the experiments that the feature cross-selection method with multi-feature screening criterion can help the model to better find the optimal feature influencing factor, which helps to improve the accuracy of prediction. The Bayesian optimization algorithm contributes to the performance improvement of the ANN. After modeling the features using the Bayesian optimized ANN, the stock trading signal prediction model proposed in this paper is significantly better than other prediction models.},
  archive      = {J_CC},
  author       = {Wang, Jujie and Dong, Ying},
  doi          = {10.1007/s12559-024-10365-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Combining interpretable embedded multicriteria feature cross-selection engineering and machine learning to mimic the brain for stock trading signal prediction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density estimation-based stein variational gradient descent.
<em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10370-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximating a target distribution, such as a Bayesian posterior, is important in many areas, including cognitive computation. We introduce a variant of Stein variational gradient descent (SVGD) (Liu and Wang Adv Neural Inf Process Syst 29, 2016), called the density estimation-based Stein variational gradient descent (DESVGD). SVGD has proven to be promising as a sampling method for approximating target distributions. SVGD, however, suffers from discontinuity inherent in the empirical measure, making it difficult to closely monitor the convergence of the sampling-based approximation to the target. DESVGD utilizes kernel density estimation to replace the empirical measure in SVGD with its continuous counterpart. This allows direct computation of the KL divergence between the current approximation and the target distribution, thereby helping to monitor the numerical convergence of the iterative optimization process. DESVGD also offers derivatives of the KL divergence, which can be used to better design learning rates and thus to achieve faster convergence. By simply replacing the kernel used in SVGD with its weighted average, one can easily implement DESVGD based on existing SVGD algorithms. Our numerical experiments demonstrate that DESVGD approximates the target distribution well and outperforms the original SVGD in terms of approximation quality.},
  archive      = {J_CC},
  author       = {Kim, Jeongho and Lee, Byungjoon and Min, Chohong and Park, Jaewoo and Ryu, Keunkwan},
  doi          = {10.1007/s12559-024-10370-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Density estimation-based stein variational gradient descent},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HC3: A three-way clustering method based on hierarchical
clustering. <em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10379-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision is a field of research pertaining to human-inspired computation. Guided by the principle of three-way decision, three-way clustering addresses the information uncertainty problem by using the core region and the fringe region to characterize a cluster. The universe is split into three parts by these two regions, which capture three kinds of relationships between objects and a cluster, namely, belonging to, partially belonging to, and not belonging to. In recent years, there have been considerable three-way clustering algorithms. However, the generalization and scalability of current three-way cluster algorithms remain relatively weak, with most algorithms adhering to a fixed allocation strategy or fixed threshold parameters. In order to overcome this problem, this paper proposes a multilevel three-way clustering algorithm based on a hierarchical strategy (HC3 for short). The proposed algorithm uses kernel density estimation information of data to adaptively construct a multilevel structure of data, where the higher levels (or the internal layers) with the high-density objects are closer to core regions of clusters, and the lower levels (or the external layers) with the low-density objects are closer to fringe regions of clusters. Under the multilevel structure, we establish a three-way allocation strategy based on the stability of subclass clusters, obtaining the correct attribution of data after fully considering neighboring information. The experiments are conducted on 13 data sets with different dimensions. By comparing to other 8 clustering algorithms, the effectiveness of the proposed HC3 is verified through accuracy (ACC), adjusted Rand index (ARI), and adjusted mutual information (AMI).},
  archive      = {J_CC},
  author       = {Guan, Wenrui and Wang, Pingxin and Jiang, Wengang and Zhang, Ying},
  doi          = {10.1007/s12559-024-10379-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {HC3: A three-way clustering method based on hierarchical clustering},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-head attention and long short-term network for
enhanced inpainting of occluded handwriting. <em>CC</em>,
<em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10382-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of handwritten character recognition, inpainting occluded offline characters is essential. Relying on the remarkable achievements of transformers in various tasks, we present a novel framework called “Enhanced Inpainting with Multi-head Attention and stacked long short-term memory (LSTM) Network” (E-Inpaint). This framework aims to restore occluded offline handwriting while capturing its online signal counterpart, enriched with dynamic characteristics. The proposed approach employs Convolutional Neural Network (CNN) and Multi-Layer Perceptron (MLP) in order to extract essential hidden features from the handwriting image. These features are then decoded by stacked LSTM with Multi-head Attention, achieving the inpainting process and generating the online signal corresponding to the uncorrupted version. To validate our work, we utilize the recognition system Beta-GRU on Latin, Indian, and Arabic On/Off dual datasets. The obtained results show the efficiency of using stacked-LSTM network with multi-head attention, enhancing the quality of the restored image and significantly improving the recognition rate using the innovative Beta-GRU system. Our research mainly highlights the potential of E-Inpaint in enhancing handwritten character recognition systems.},
  archive      = {J_CC},
  author       = {Rabhi, Besma and Elbaati, Abdelkarim and Hamdi, Yahia and Dhahri, Habib and Pal, Umapada and Chabchoub, Habib and Ouahada, Khmaies and Alimi, Adel M.},
  doi          = {10.1007/s12559-024-10382-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-head attention and long short-term network for enhanced inpainting of occluded handwriting},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of artificial neural network computing systems.
<em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-024-10383-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An artificial neural network (ANN) is currently used in multiple different applications such as bio-medicine, finance, Internet, and mobile networks. Since their inception, many advances have taken place introducing new models and features. Such progress resulted in different ANN models and most importantly different types of implementation, which vary from software (SW) to hardware (HW) following specific development principles. Researchers have been working significantly the last decade in this area tackling with different aspects of ANN’s implementations. In this survey, we present the progress of ANN in terms of implementation as part of computing platforms. Thus, we present the ANN-enabled computing platforms in terms of algorithmic models, computing architectures, and SW/HW implementations. This work concludes with open challenges and lessons learned in order to summarize what is potentially useful for further research in the area of ANN computing platforms with a wide spectrum of applications. An artificial neural network (ANN) is considered the key element of future computing systems applied to different domains. While the algorithmic design of an ANN is one of the major engineering elements, the implementation of ANN is equally important with many difficulties that should be overcome by future engineers. This survey aims to provide a comprehensive tutorial about the ANN-enabled computing systems, i.e., computing architectures with embedded artificial intelligence (AI). Starting with the ANN models and their applications, the survey provides a taxonomy of the types of ANN computing systems. Both SW and HW implementations are provided for each of those types, which highlight the key architectural elements as well as the performance of the ANN-enabled computing systems. Open challenges and lessons learned follow to provide a discussion for future research in the area of AI computing systems.},
  archive      = {J_CC},
  author       = {Foukalas, Fotis},
  doi          = {10.1007/s12559-024-10383-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {A survey of artificial neural network computing systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-type image coding method acting on supervised
hierarchical deep spiking convolutional neural networks for image
classification. <em>CC</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-024-10355-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have gained significant momentum in recent times as they transmit information via discrete spikes, similar to neuromorphic low-power systems. However, existing spike coding methods are often limited to a single scale of time or rate, and typically suffer from drawbacks such as reduced accuracy or long classification latency. In this paper, we propose a pixel-based multi-type image coding (PMIC) method inspired by the functional organization of primate visual systems to address the issues at hand. The encoded information comprises both spatial and temporal details, represented by spiking firing time and intensity, respectively. Subsequently, we combine the spiking firing time and intensity as inputs of a hierarchical spiking convolutional neural network (SCNN) including several convolutional and pooling layers. During the training phase, we use error backpropagation to optimize parameters. Comparison of experimental results with some state-of-the-art approaches on MNIST dataset, Fashion-MNIST dataset as well as ETH-80 dataset of image classification demonstrates that SCNN using PMIC can achieve the best test accuracy, which is 99.13%, 90.31%, and 94.29%, respectively. The proposed PMIC utilizes multiple filters and coding strategies to extract multi-type information and is more beneficial to the performance of SNNs compared to methods that extract single-scale or single-type information.},
  archive      = {J_CC},
  author       = {Liu, Fang and Xu, Jialin and Yang, Jie and Wu, Wei},
  doi          = {10.1007/s12559-024-10355-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-type image coding method acting on supervised hierarchical deep spiking convolutional neural networks for image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRCFusionAICADx: Integrative CNN-LSTM approach for accurate
colorectal cancer diagnosis in colonoscopy images. <em>CC</em>,
<em>17</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s12559-024-10357-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal cancer (CRC) is a critical health issue worldwide and is very treatable if diagnosed on time. This paper proposes an innovative CADx system, namely CRCFusionAICADx, that enhances the efficiency of diagnosis by fusing CNNs with LSTM networks and feature integration techniques. Using data from the CKHK-22 colonoscopy image dataset, we preprocess the images into grayscale first and then apply LBP analysis for emphasizing textural features. These are further analyzed using three different pre-trained CNN models: VGG16, DenseNet-201, and ResNet50. These were chosen because of their complementary feature extraction capabilities. The resultant features from grayscale, LBP, and raw images will be fused to create an integrated dataset. To increase variability in the dataset and reduce overfitting for the network, we decided to apply a series of data augmentation techniques, which included zooming in, rotation, and horizontal flipping. By doing so, we expanded the dataset into 57,148 images. This augmented dataset is then used to train a model, RDV-22, which includes an integration of the architectures of VGG16, DenseNet-201, and ResNet50, with CNN and CNN + LSTM layers. The LSTM network learns the temporal dependencies of frames in a sequence and hence allows for more sensitive and specific detection of CRC. CRCFusionAICADx produces very impressive results, where the RDV-22 model produces a testing accuracy of 90.81%, precision of 91.00%, recall of 90.00%, and an F1 score of 90.49% in its results. This gives the model an ROC AUC of 0.98, reflecting very strong discriminatory power. This integrative approach thus shows tremendous promise for early CRC detection by offering a strong diagnostic tool that integrates both spatial and temporal features into a new standard in clinical diagnostics.},
  archive      = {J_CC},
  author       = {Raju, Akella S. Narasimha and Jayavel, Kayalvizhi and Rajalakshmi, Thulasi and Rajababu, M.},
  doi          = {10.1007/s12559-024-10357-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Cogn. Comput.},
  title        = {CRCFusionAICADx: Integrative CNN-LSTM approach for accurate colorectal cancer diagnosis in colonoscopy images},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-aided diagnosis of graphomotor difficulties
utilizing direction-based fractional order derivatives. <em>CC</em>,
<em>17</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12559-024-10360-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children who do not sufficiently develop graphomotor skills essential for handwriting often develop graphomotor disabilities (GD), impacting the self-esteem and academic performance of the individual. Current examination methods of GD consist of scales and questionaries, which lack objectivity, rely on the perceptual abilities of the examiner, and may lead to inadequately targeted remediation. Nowadays, one way to address the factor of subjectivity is to incorporate supportive machine learning (ML) based assessment. However, even with the increasing popularity of decision-support systems facilitating the diagnosis and assessment of GD, this field still lacks an understanding of deficient kinematics concerning the direction of pen movement. This study aims to explore the impact of movement direction on the manifestations of graphomotor difficulties in school-aged. We introduced a new fractional-order derivative-based approach enabling quantification of kinematic aspects of handwriting concerning the direction of movement using polar plot representation. We validated the novel features in a barrage of machine learning scenarios, testing various training methods based on extreme gradient boosting trees (XGBboost), Bayesian, and random search hyperparameter tuning methods. Results show that our novel features outperformed the baseline and provided a balanced accuracy of 87 % (sensitivity = 82 %, specificity = 92 %), performing binary classification (children with/without graphomotor difficulties). The final model peaked when using only 43 out of 250 novel features, showing that XGBoost can benefit from feature selection methods. Proposed features provide additional information to an automated classifier with the potential of human interpretability thanks to the possibility of easy visualization using polar plots.},
  archive      = {J_CC},
  author       = {Gavenciak, Michal and Mucha, Jan and Mekyska, Jiri and Galaz, Zoltan and Zvoncakova, Katarina and Faundez-Zanuy, Marcos},
  doi          = {10.1007/s12559-024-10360-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Computer-aided diagnosis of graphomotor difficulties utilizing direction-based fractional order derivatives},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fixed-time control algorithm for multiagent systems with
input delay via event-triggered strategy. <em>CC</em>, <em>17</em>(1),
1–12. (<a href="https://doi.org/10.1007/s12559-024-10366-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent systems have become increasingly relevant in numerous fields due to their potential for cooperative control and distributed networks. Developing effective adaptive algorithms for these systems is crucial, as adaptive techniques can enhance the ability of multiple agents to collaboratively complete specific tasks. In this paper, we propose an event-triggered adaptive fixed-time containment algorithm for multiagent systems with input delay. Firstly, our methodology employs Fuzzy Logic Systems to approximate uncertain terms in system dynamics and addresses the algebraic loop problem inherent in non-strict feedback functions. Additionally, an integral term is introduced to mitigate the adverse effects of time delay on system performance. Under the fixed-time stability framework, the proposed algorithm achieves containment within an exact time constant, independent of initial conditions. Furthermore, a dynamic event-triggered mechanism is incorporated to optimize the number of triggers, thereby conserving communication resources. As a result, the designed control algorithm guarantees that the closed-loop systems signals meet the criteria for semi-global practical fixed-time stability, allowing the outputs of followers to converge to the convex hull of leaders within a finite time. Finally, the feasibility and effectiveness of the algorithm are demonstrated through simulations involving underwater vehicle systems.},
  archive      = {J_CC},
  author       = {Liu, Guijiang and Wang, Xin and Guang, Weiwei and Chen, Hongyu},
  doi          = {10.1007/s12559-024-10366-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A fixed-time control algorithm for multiagent systems with input delay via event-triggered strategy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid semantics and syntax-based graph convolutional
network for aspect-level sentiment classification. <em>CC</em>,
<em>17</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12559-024-10367-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-level sentiment classification seeks to ascertain the sentiment polarities of individual aspects within a sentence. Most existing research in this field focuses on individually assessing the importance of contexts on individual aspects, disregarding the negative impact of imbalanced relations between aspects due to their mutual influence. This paper presents a hybrid semantics and syntax-based graph convolutional network (SS-GCN) for aspect-level sentiment classification. This model addresses the imbalanced limitation by creating aspects-based balance relations between the strengths and weaknesses of different aspects through an auxiliary task. Furthermore, the multi-head self-attention mechanism utilizes position-enhanced encoding to identify the most relevant aspects of the current word. Extensive experiments demonstrate that SS-GCN outperforms other baselines in terms of classification performance. Compared to state-of-the-art methods, SS-GCN significantly improves 0.39–1.66% in accuracy and 0.43–1.92% in Macro-F1 on the SemEval 14-15 and MAMS datasets.},
  archive      = {J_CC},
  author       = {Huang, Chen and Li, Xianyong and Du, Yajun and Dong, Zhicheng and Huang, Dong and Kumar Jain, Deepak and Hussain, Amir},
  doi          = {10.1007/s12559-024-10367-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {A hybrid semantics and syntax-based graph convolutional network for aspect-level sentiment classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DmrNet: Dual-stream mutual information contraction and
re-discrimination network for semi-supervised temporal action detection.
<em>CC</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-024-10374-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised temporal action detection only requires a small number of labeled samples from the dataset and utilizes the remaining unlabeled samples for model training, effectively alleviating the significant time and manpower costs associated with annotating large-scale temporal action detection datasets. However, previous semi-supervised temporal action detection methods relied on sequential action localization and classification, which leads to erroneous localization predictions that can easily affect subsequent classification predictions, resulting in error propagation problem. To overcome error propagation, we propose a dual-stream mutual information contraction and re-discrimination network (DmrNet). Specifically, the traditional two-step strategy of temporal action detection has been changed to a four-step parallel strategy by us. Firstly, this paper designs the first-step classification prediction and the second-step localization prediction as a parallel structure to prevent error propagation from localization to classification. Then, in the third step, the dual-stream mutual information contraction part maps the dual-stream features to a new vector space to ensure the cross-correlation between classification and action localization. Finally, the fourth step of classification re-discrimination part captures the consistency information of the dual-stream structure to enhance internal representation. Compared with existing methods, DmrNet achieved an average accuracy improvement of 10.7% on ActivityNet v1.3 and 5.2% on THUMOS14 using only 10% annotation data. The experimental results show that the proposed DmrNet not only achieves good detection performance in semi-supervised learning but also achieves performance comparable to state-of-the-art methods in fully supervised learning.},
  archive      = {J_CC},
  author       = {Zhang, Qiming and Hu, Zhengping and Wang, Yulu and Bi, Shuai and Zhang, Hehao and Di, Jirui},
  doi          = {10.1007/s12559-024-10374-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {DmrNet: Dual-stream mutual information contraction and re-discrimination network for semi-supervised temporal action detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event classification on subsea pipeline inspection data
using an ensemble of deep learning classifiers. <em>CC</em>,
<em>17</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12559-024-10377-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsea pipelines are the backbone of the modern oil and gas industry, transporting a total of 28% of global oil production. Due to several factors, such as corrosion or deformations, the pipelines might degrade over time, which might lead to serious economic and environmental damages if not addressed promptly. Therefore, it is crucial to detect any serious damage to subsea pipelines before they cause dangerous catastrophes. Inspections of subsea pipelines are usually made using a Remote Operating Vehicle and the inspection data is usually processed manually, which is subject to human errors, and requires experienced Remote Operating Vehicle operators. It is thus necessary to automate the inspection process to enable more efficiency as well as reduce costs. Besides, it is recognised that specific challenges of noisy and low-quality inspection data arising from the underwater environment prevent the industry from taking full advantage of the recent development in the Artificial Intelligence field to the problem of subsea pipeline inspection. In this paper, we developed an ensemble of deep learning classifiers to further improve the performance of single deep learning models in classifying anomalous events on the subsea pipeline inspection data. The output of the proposed ensemble was combined based on a weighted combining method. The weights of base classifiers were found by minimising the difference between the weighted combining result and the given associated ground truth annotation information. Three inspection datasets, gathered from different oil and gas companies in the United Kingdom, were analysed. These datasets were recorded under varying conditions and include a range of anomalies. The results showed that the proposed ensemble achieves around 78% accuracy on two datasets and more than 99% accuracy on one dataset, which is better compared to base classifiers and two popular ensembles.},
  archive      = {J_CC},
  author       = {Dang, Truong and Nguyen, Tien Thanh and Liew, Alan Wee-Chung and Elyan, Eyad},
  doi          = {10.1007/s12559-024-10377-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Event classification on subsea pipeline inspection data using an ensemble of deep learning classifiers},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DisTrack: A new tool for semi-automatic misinformation
tracking in online social networks. <em>CC</em>, <em>17</em>(1), 1–18.
(<a href="https://doi.org/10.1007/s12559-024-10378-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces DisTrack, a methodology and a tool developed for tracking and analyzing misinformation within online social networks (OSNs). DisTrack is designed to combat the spread of misinformation through a combination of natural language processing (NLP) social network analysis (SNA) and graph visualization. The primary goal is to detect misinformation, track its propagation, identify its sources, and assess the influence of various actors within the network. DisTrack’s architecture incorporates a variety of methodologies including keyword search, semantic similarity assessments, and graph generation techniques. These methods collectively facilitate the monitoring of misinformation, the categorization of content based on alignment with known false claims, and the visualization of dissemination cascades through detailed graphs. The tool is tailored to capture and analyze the dynamic nature of misinformation spread in digital environments. The effectiveness of DisTrack is demonstrated through three case studies focused on different themes: discredit/hate speech, anti-vaccine misinformation, and false narratives about the Russia-Ukraine conflict. These studies show DisTrack’s capabilities in distinguishing posts that propagate falsehoods from those that counteract them, and tracing the evolution of misinformation from its inception. The research confirms that DisTrack is a valuable tool in the field of misinformation analysis. It effectively distinguishes between different types of misinformation and traces their development over time. By providing a comprehensive approach to understanding and combating misinformation in digital spaces, DisTrack proves to be an essential asset for researchers and practitioners working to mitigate the impact of false information in online social environments.},
  archive      = {J_CC},
  author       = {Villar-Rodríguez, Guillermo and Huertas-García, Álvaro and Martín, Alejandro and Huertas-Tato, Javier and Camacho, David},
  doi          = {10.1007/s12559-024-10378-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {DisTrack: A new tool for semi-automatic misinformation tracking in online social networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust synchronization of stochastic markovian jumping CVNs
with randomly occurring nonlinearities and generally uncertain
transition rates. <em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-024-10362-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article intents to the robust synchronization issue for Markovian jumping complex-valued networks (CVNs) subject to the stochastic noises and randomly occurring nonlinearities, where the considered transition rates are generally uncertain, which expands the existed relevant results. Meanwhile, two random variables with pre-given statistical characteristics are proposed to explain the involved randomly occurring nonlinearities phenomenon, and random variables are mutually independent. By designing the appropriate mode-dependent controller, combined with generalized complex It $$\hat{o}$$ ’s formula, Lyapunov stability theory, and the properties of the transition rate matrix, some sufficient conditions are achieved to ensure error system realizes stochastically asymptotically mean-square stable. Furthermore, sufficient mode/delay-dependent criteria of globally exponential synchronization for considered CVNs are also investigated. In the end, two illustrative examples with simulations are proposed to verify the effectiveness and feasibility of the designed control schemes.},
  archive      = {J_CC},
  author       = {Li, Qiang and Wei, Hanqing and Hua, Dingli and Wang, Jinling and Zheng, Yuanshi},
  doi          = {10.1007/s12559-024-10362-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Robust synchronization of stochastic markovian jumping CVNs with randomly occurring nonlinearities and generally uncertain transition rates},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extensions and detailed analysis of synergy between
traditional classification and classification based on negative features
in deep convolutional neural networks. <em>CC</em>, <em>17</em>(1),
1–16. (<a href="https://doi.org/10.1007/s12559-024-10369-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, deep convolutional neural networks became an irreplaceable tool for pattern recognition in many different machine learning applications, especially in image classification. On the other hand, these models are often used in critical systems which are the reason for new and recent research regarding their robustness and reliability. One of the most important issues for these models is their susceptibility to different adversarial attacks. In our previous work Milošević and Racković (Neural Network World. 2019;29(4):221–34), and Milošević and Racković (Neural Comput Applic. 2021;33:7593–602), the new type of learning applicable to all the convolutional neural networks was introduced: the classification based on the negative features and the synergy of traditional and those newly introduced network models. In the case of partial inputs/image occlusion, it was shown that our new method creates models that are more robust and perform better when compared to traditional models of the same architecture. In this paper, some extensions of the earlier proposed synergy are given by introducing negatively trained features and additional synergy between four independent neural network models. A detailed analysis of the robustness of the newly proposed model is performed on EMNIST and CIFAR-10 image classification data sets in the case of the selected input occlusions and adversarial attacks. The newly proposed neural network architecture improves the robustness of the neural network and increases its resistance to various types of input damage and adversarial attacks.},
  archive      = {J_CC},
  author       = {Racković, Miloš and Vidaković, Jovana and Milošević, Nemanja},
  doi          = {10.1007/s12559-024-10369-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Extensions and detailed analysis of synergy between traditional classification and classification based on negative features in deep convolutional neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EXplainable AI for word embeddings: A survey. <em>CC</em>,
<em>17</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s12559-024-10373-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, word embeddings have become integral to natural language processing (NLP), offering sophisticated machine understanding and manipulation of human language. Yet, the complexity of these models often obscures their inner workings, posing significant challenges in scenarios requiring transparency and explainability. This survey conducts a comprehensive review of eXplainable artificial intelligence (XAI) strategies focused on enhancing the interpretability of word embeddings. By classifying the existing body of work into six broad categories based on their methodological approaches—a classification that, to our knowledge, does not exist in the literature—we provide a structured overview of current techniques and their characteristics. Additionally, we uncover a noteworthy oversight: a predominant emphasis on interpreting model outputs at the expense of exploring the models’ internal mechanics. This finding underscores the necessity of shifting research efforts toward not only clarifying the results these models produce but also demystifying the models themselves. Such a shift is crucial for uncovering and addressing biases inherent in word embeddings, thus ensuring the development of fair and trustworthy AI systems. Through this analysis, we identify key research questions for future studies and advocate for a holistic approach to transparency in word embeddings, encouraging the research community to explore both the outcomes and the underlying algorithms of these models.},
  archive      = {J_CC},
  author       = {Boselli, Roberto and D’Amico, Simone and Nobani, Navid},
  doi          = {10.1007/s12559-024-10373-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {EXplainable AI for word embeddings: A survey},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence inspired task offloading and
resource orchestration in intelligent transportation systems.
<em>CC</em>, <em>17</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s12559-024-10380-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Vehicles (IoV) applications require the support of communication, caching, and computation (3C) resources to offload the computation-intensive tasks and for uplifting the traffic conditions in the development of sustainable smart cities. Intelligent Transportation Systems (ITS) lack the integrated ecosystems of addressing the low-latency task handovers, resource management issues, and centralized incentivization strategies. Digital Twin (DT) aids in capturing the real-time varying resource needs of the vehicles and the communication infrastructure that will regulate the task offloading process and facilitates in incentivizing the vehicular instances. In this manuscript, we establish a digital twin counterpart ( $$DT_{PIoV}$$ ) of the physical IoV (PIoV) to meet the QoS requirements during dynamic offloading and the time-varying resource supply–demand of computationally intensive applications. We formulate a response delay minimization function which is solved by the proposed DT-driven context-aware dynamic offloading method (CADOM). Furthermore, we use M/M/1/N/FCFS queueing method that combats the drawbacks of handling the simultaneous deadline-based tasks in a volatile environment of PIoV. In addition, we also maximize the utilities of vehicle and RSU service satisfaction by employing a reward-based mechanism for on-demand allocation of resources based on the Stackelberg game, where the DT of vehicle is deemed as a leader and service provider RSUs as a follower. The simulation results establish that the proposed system outpaces the conventional traffic management system by emphasizing the role of $$DT_{PIoV}$$ in jointly optimizing the overall response latency for different task sizes and also ensure a better utility satisfaction by catering on-demand resource allocation.},
  archive      = {J_CC},
  author       = {Rawlley, Oshin and Gupta, Shashank and Chandrakar, Jyotsana and Johnson, Manisha K. and Kalra, Chahat},
  doi          = {10.1007/s12559-024-10380-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Cogn. Comput.},
  title        = {Artificial intelligence inspired task offloading and resource orchestration in intelligent transportation systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuned BERT algorithm-based automatic query expansion
for enhancing document retrieval system. <em>CC</em>, <em>17</em>(1),
1–16. (<a href="https://doi.org/10.1007/s12559-024-10354-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online retrieval systems are mostly web-based, which makes document collecting more dynamic or fluid than in traditional information retrieval systems. With the web growing in size every day, finding meaningful information on it using a search query consisting of only a few keywords which has become increasingly difficult. One important factor in making Internet searches better is query expansion, or QE. Manual query expansion method involves the user adding terms to the query, which takes a long time but produces good results. However, the automatic query expansion (AQE) method determines the best statements with minimal time consumption. Therefore, to improve document retrieval system, a fine-tuned BERT algorithm is developed for automatic query expansion. Initially, the input text was augmented using embedding augmentation (EA) approach. The augmented text was pre-processed using tokenization, normalization, splitting, stemming, stop word removal, as well as lemmatization. Then extracting the technical keywords from the pre-processed text using co-occurrence statistical information. After extracting the keywords, a fine-tuned BERT model is utilized for expanding the query to improve document retrieval system. The hyper parameters present in the BERT was tuned using frilled lizard optimization to enhance the performance of the BERT model. Proposed model provides 92% accuracy, 95% precision, and 95.6% recall. Thus, a fine-tuned BERT model minimizing query-document mismatch and thereby improving retrieval performance.},
  archive      = {J_CC},
  author       = {Vishwakarma, Deepak and Kumar, Suresh},
  doi          = {10.1007/s12559-024-10354-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Fine-tuned BERT algorithm-based automatic query expansion for enhancing document retrieval system},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic neighborhood selection for context aware temporal
evolution using graph neural networks. <em>CC</em>, <em>17</em>(1),
1–19. (<a href="https://doi.org/10.1007/s12559-024-10359-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNN) have seen significant growth recently for modeling temporal evolution in dynamic networks. Representation of complex networks in the form of graph data structures has enabled researchers to study how entities within these networks interact with each other. These interactions evolve over time. Developing a generic methodology for modeling this temporal evolution in complex networks for tracking evolving relationships has been a significant challenge. Most of the existing methods fail to extract contextual representations of historical neighborhood interactions for future link prediction. To address these challenges, this paper presents a novel method for modeling temporal evolution in complex networks using GNNs. A Context-Aware Graph Temporal Neural Network (CATGNN) method that uses dynamic neighborhood selection based on common neighbors for a given node is presented. The method uses dynamic neighborhood selection using contextual embeddings extracted from the historical interactions of the down-sampled set of neighbors of a central node based on a common neighborhood. Fixed-sized contextual memory modules are constructed for each node that store the historical interactions of its neighbors and are updated based on the recency and significance of interactions. The proposed method has been evaluated using six real-world datasets and has comparable performance against state-of-the-art methods, both in terms of accuracy and efficiency. It shows an improvement of 7.52 to 0.05% over the baselines in terms of average precision. The results demonstrate that the proposed CATGNN model can capture complex patterns of change that are difficult to identify using traditional techniques by propagating information over the graph structure. The model can be applied in various fields involving complex systems.},
  archive      = {J_CC},
  author       = {Zeb, Muhammad Ali and Uddin, M. Irfan and Alarood, Ala Abdulsalam and Shafiq, Muhammad and Habibullah, Safa and Alsulami, Abdulkream A.},
  doi          = {10.1007/s12559-024-10359-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Dynamic neighborhood selection for context aware temporal evolution using graph neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeuroInteract: An innovative deep learning strategy for
effective drug repositioning in schizophrenia therapy. <em>CC</em>,
<em>17</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s12559-024-10384-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia (SCZ) is a serious physiological and neurological disorder that affects an individual’s perception of factuality. It expresses different symptoms such as thinking, aberrant behavior, delusions, and hallucinations. An efficient approach for inferring potential indications for drugs is through drug repositioning. In this context, drug repositioning imparts a valuable strategy to gain safer, faster, and potentially efficient treatment options to improve schizophrenia therapy. Current treatments are insufficient and existing drug repositioning methods are unsuccessful in solving the drug-disease interactions’ difficulties, including long-term efficacy, drug synergy, and capturing genetic variations. Also, existing methods are restrained because of the incapacity to efficiently integrate heterogeneous biomedical data, which results in suboptimal predictions. This research introduces a NeuroInteract model using deep learning in order to predict candidate drugs for SCZ therapy. The proposed model enhances the accuracy of drug repositioning through the collection of various data sources such as genetic information and drug-disease associations. The novelty of the proposed model is the utilization of the heterogeneous data network that is integrated with the progressive optimization model for the purpose of improving prediction accuracy. The developed method imparts effective learning from various data characteristics through the integration of various types of neural network layers such as fully connected layers, convolutional layers, recurrent layers, and graph convolutional layers. The collected data from DrugBank 5.0 and repoDB undergoes a process of data integration, which aids in generating precise predictions for candidate drugs for repositioning. A data pre-processing technique is employed to improve the data quality. After data pre-processing, the proposed method effectively extracts the meaningful features and finds the spatial dependencies to predict the potential candidate drugs for SCZ treatment. Also, it efficiently handles sequential dependencies and genetic information. The oppositional crossover boosted meerkat optimization (OCMO) algorithm is deployed to optimize the performance of the model. The OCMO optimizes the learning process and enhances the model accuracy by dynamically adjusting its search strategy. Ultimately, comprehensive experimental analyses are conducted using several estimation parameters. The proposed method gains greater effectiveness and outperforms existing methods in drug repositioning. The developed method reaches an accuracy of 98.84% and a hit rate of 98.76%. These experimental findings ascertain the ability of NeuroInteract to find promising drugs for repurposing, furnishing a robust and more cost-effective model for SCZ treatment.},
  archive      = {J_CC},
  author       = {J., Sherine Glory and P., Durgadevi and P., Ezhumalai},
  doi          = {10.1007/s12559-024-10384-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {NeuroInteract: An innovative deep learning strategy for effective drug repositioning in schizophrenia therapy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel interpretable graph convolutional neural network for
multimodal brain tumor segmentation. <em>CC</em>, <em>17</em>(1), 1–25.
(<a href="https://doi.org/10.1007/s12559-024-10387-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have revolutionized computer vision, demonstrating remarkable performance in various tasks. However, their end-to-end learning strategy poses challenges to explainability. In this work, we explore the application of explainability techniques in brain tumor segmentation using magnetic resonance imaging (MRI) data. Our adaptive learning class activation map (AL-CAM) employs a unique multiple-pop-out training strategy and contrastive learning to enhance internal outputs, improving interpretability. Additionally, we introduce a novel approach to explainability in graph convolutional neural networks (GCNNs). The usage of traditional CNN interpretability tools such as saliency maps, CAM, and EB are often unable to handle the complexity of graph-structured data. Our work brim this gap by adapting and improving these techniques for graph convolutional neural networks (GCNN). We present two innovative tools: adaptive CAM for differentiated interpretability and contrastive EB for deeper insights into functions. Using a novel feature fusion approach, we further push the boundaries and combine the feature strengths of GNN and CNN for a holistic understanding of GCNN decision-making. Our proposed framework enables interpretability in various areas, not just medical imaging. Our work demonstrates the versatility of explainability methods and demonstrates their power in unlocking the secrets of GCNNs and ultimately solving real-world challenges, particularly in the field of medical image analysis.},
  archive      = {J_CC},
  author       = {Arshad Choudhry, Imran and Iqbal, Saeed and Alhussein, Musaed and Aurangzeb, Khursheed and Qureshi, Adnan N. and Hussain, Amir},
  doi          = {10.1007/s12559-024-10387-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {A novel interpretable graph convolutional neural network for multimodal brain tumor segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive decision-making system for behavior analysis
among young adults. <em>CC</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-024-10372-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global spread of the pandemic, secure isolation regulations, logistical limitations, and delays in reopening educational institutions such as colleges and universities have all had a severe psychological impact. Students, in particular, are regarded as a vulnerable population, experiencing higher levels of fear, stress, depression, and unhealthy eating compared to the general population. To reduce these psychological consequences, the study provides a multi-dimensional evaluation approach to bridge the gap between governments and health institutions in preventing and controlling biological hazards, such as mental illness among students. In the aftermath of the pandemic, this study presents a comprehensive evaluation approach designed to mitigate the psychological impact on students by connecting governments and health institutions in preventing and controlling biological hazards, particularly mental illness. To establish complex and vague data concerning the discussed communities, psychological details were obtained in the complex spherical fuzzy $$ \mathscr {N}$$ -soft context. An enhanced group decision-making methodology was then established in two phases. Initially, weight analytics were defined using the Reyni entropy technique. In the subsequent phase, the Combined Compromise Solution (CoCoSo) approach was applied to examine the possibilities. Students attending schools and colleges experience significant psychological impacts. To evaluate these effects, an analytical study was conducted, suggesting that improved educational amenities are necessary to mitigate these psychological consequences. Furthermore, the study validates the significance of the proposed decision system through its analysis. A unique score function is suggested for analyzing the psychological consequences among adults because it effectively addresses the ambiguity in periodic data, resulting in accurate and consistent judgments within a two-dimensional framework. Experts thoroughly analyzed the data using the complex spherical fuzzy $$ \mathscr {N}$$ -soft set-integrated CoCoSo method, and its limitations were also addressed.},
  archive      = {J_CC},
  author       = {Pragathi, Subramaniam and Narayanamoorthy, Samayan and Pamucar, Dragan and Kang, Daekook},
  doi          = {10.1007/s12559-024-10372-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {An adaptive decision-making system for behavior analysis among young adults},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced self-attention-based rapid CNN for detecting dense
objects in varying illumination. <em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-024-10376-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of efficient detection of densely arranged unordered items under varying illumination. Specifically, a novel convolutional neural network-based method is proposed for item vector detection, recognition, and classification, termed Self-Attention and Concatenation-Based Detector (ACDet). In a benchmark pharmaceutical case study, rapid and accurate detection of pharmaceutical package contours is achieved, enabling the automatic and fast verification of both the quantity and types of pharmaceuticals during distribution. At the input stage, a combined image augmentation method is applied to improve the detection model’s ability to learn the appearance features of items from multiple angles. Based on YOLOv8 model, integrating computational module C2F with Attention (C2F-A), multidimensional self-attention reinforcement is applied to the outputs of multiple gradient streams. The designed Weighted Concatenation (WConcat) module self-learns to weight and concatenate multi-level feature maps, enhancing the model’s cognitive capability. Finally, simulation experiments are conducted to determine the optimal timing for utilizing each module. Simulation experiments compared the proposed ACDet with several state-of-the-art YOLO architecture models utilizing the benchmark Comprehensive Pharmaceutical Package Dataset (CPPD). ACDet achieved 81.0% mAP and 79.5% Smooth mAP on the CPPD dataset, outperforming other models by an average of 5.5% to 16.6%. On public datasets, the results were 52.2% and 51.0%, respectively. The impact of utilizing C2F-A at different stages on performance was also tested, concluding that the WConcat module does not necessitate spatial attention. Finally, in zero-shot testing, the verification success rate reached 99.91%. Our work shows that the proposed ACDet can overcome many challenges in complex object detection scenarios, enhancing robustness while maintaining a lightweight design. The proposed model can serve as a new benchmark.},
  archive      = {J_CC},
  author       = {Chen, Lu and Yang, Li and Jie, Tan and Haoyuan, Ma and Yu, Liu and Shenbing, Fu and Wang, Junkang and Wu, Hao and Li, Gun},
  doi          = {10.1007/s12559-024-10376-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Enhanced self-attention-based rapid CNN for detecting dense objects in varying illumination},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary multitasking with adaptive tradeoff selection
strategy. <em>CC</em>, <em>17</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s12559-024-10386-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new emerging evolutionary framework, evolutionary multitasking aims to optimize multiple tasks simultaneously. Knowledge transfer is an important component of evolutionary multitasking. How to extract and transfer knowledge significantly affects the performance of the algorithm. A serious challenge for evolutionary multitasking is the inappropriate knowledge transfer or insufficient exploration and exploitation. To address this challenge, an evolutionary multitasking with adaptive tradeoff selection strategy (EMT-ATS) is proposed. To enhance global exploration and local exploitation during the evolution, an adaptive tradeoff selection mechanism is developed to select promising offspring during different stages to guide the population toward more promising solution regions. In addition, a Cohen’s d indicator-based is used to adjust knowledge transfer. To verify the effectiveness of the proposed EMT-ATS, a series of experiments are conducted with several popular evolutionary multitasking algorithms on multitasking benchmark problems. In addition, a multitask optimization problem involving two real-world problems is used to validate the practicability of the proposed EMT-ATS. Experimental results demonstrate the effectiveness of the proposed EMT-ATS.},
  archive      = {J_CC},
  author       = {Li, Wei and Zhou, Yinhui and Wang, Lei},
  doi          = {10.1007/s12559-024-10386-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Evolutionary multitasking with adaptive tradeoff selection strategy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive unsupervised graph convolution network for data
clustering with graph reconstruction. <em>CC</em>, <em>17</em>(1), 1–14.
(<a href="https://doi.org/10.1007/s12559-024-10364-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph clustering has emerged as one of the most challenging problems in the field of deep learning. With the increasing complexity of real-world networks, such as social and biological networks, more and more effective methods are needed to organize and understand these structures. Various cognition-based techniques have been explored for classifying nodes within these graphs, and graph convolution networks (GCNs) have attracted great interest. GCNs, a deep semi-supervised learning approach, provide a powerful framework for learning node representations by utilizing both local and global graph information. By iteratively aggregating information from neighboring nodes, GCNs effectively capture the intricate relationships and dependencies within complex networks. We introduce a novel deep unsupervised learning scheme built upon the foundation of GCN architecture. The key contributions are outlined below. First, the entire architecture is trained with three unsupervised learning losses. The first loss focuses on kernelized features that use node attributes to reflect the information extracted from the data. The second loss leverages spectral smoothness that uses connections between nodes to capture global cluster structure. The third loss is based on graph reconstruction that introduces additional regularization of the representation of nodes by the output of the model. Second, the spectral smoothing loss involves an adaptive approach using an additional graph matrix associated with the node representations. This adaptive integration of additional structural information increases the learning efficiency during the training phase. The adaptive fused graph used for spectral smoothing loss incorporates structural insights derived from both data features and deep node representations. To assess the performance of our approach, we conducted extensive experimental evaluations on four benchmark datasets widely used in the field of graph clustering. These datasets were carefully selected to cover diverse domains and varying degrees of complexity, ensuring a comprehensive evaluation of our method’s efficacy. Our results showcase the remarkable performance of our unsupervised GCN across multiple metrics, surpassing other state-of-the-art graph neural network techniques in terms of clustering accuracy, purity, and other relevant measures. Notably, our method consistently outperforms competing approaches across the used datasets, demonstrating its versatility and effectiveness in handling various real-world scenarios.},
  archive      = {J_CC},
  author       = {Jreidy, M. Al and Constantin, J. and Dornaika, F. and Hamad, D.},
  doi          = {10.1007/s12559-024-10364-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Adaptive unsupervised graph convolution network for data clustering with graph reconstruction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disturbance observer-based control: Weighted aggregated
aczel-alsina sum product assessment based on power operators for
managing fuzzy 2-tuple linguistic neural networks. <em>CC</em>,
<em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-024-10371-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disturbance observer–based control (DOBC) is a valuable strategy for enhancing control system performance by compensating for disturbances. The core idea is to design an observer that accurately estimates external disturbances affecting the system, and then use this estimation to adjust the control input accordingly. This article introduces a novel approach involving fuzzy 2-tuple linguistic (F2-TL) sets, incorporating algebraic and Aczel-Alsina operational laws. Additionally, we propose several operators: the F2-TL Aczel-Alsina power averaging (F2-TLAAPA) operator, the F2-TL Aczel-Alsina power weighted averaging (F2-TLAAPWA) operator, the F2-TL Aczel-Alsina power geometric (F2-TLAAPG) operator, and the F2-TL Aczel-Alsina power weighted geometric (F2-TLAAPWG) operator. These operators are used to aggregate information into a singleton set, and we discuss their fundamental properties, including idempotency, monotonicity, and boundedness. Moreover, we explain the WASPAS (weighted aggregated sum product assessment) technique, applying the proposed methods and illustrating them with relevant examples. The article also explores the application of these techniques to multi-attribute decision-making (MADM) problems, demonstrating their value. Finally, to validate the introduced methods and highlight the superiority of the proposed approach, we compare the ranking results obtained using our methods with those from existing techniques.},
  archive      = {J_CC},
  author       = {Ali, Zeeshan and Hila, Kostaq},
  doi          = {10.1007/s12559-024-10371-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Disturbance observer-based control: Weighted aggregated aczel-alsina sum product assessment based on power operators for managing fuzzy 2-tuple linguistic neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view hierarchical graph neural network for
argumentation mining. <em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10391-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation mining (AM) aims to detect the arguments and their relations from argumentative texts. Generally, AM contains three key challenging subtasks, including argument component type classification (ACTC), argumentative relation identification (ARI), and argumentative relation type classification (ARTC). Most previous studies solve these three subtasks separately, neglecting the rich interrelation information among the three tasks. In this paper, we propose a multi-view hierarchical graph neural network (MHGNN) for AM, which resolves the three interacted subtasks in a unified multi-task learning framework. Concretely, MHGNN learns graph embeddings from multiple views (i.e., word view and semantic view) that often provide more comprehensive information. Each graph view is equipped with a two-level graph structure: (i) the first level is the argumentation graph with each argumentation component (AC) as a graph node, which learns the inter-AC knowledge from the input text; (ii) the second level is the AC graph with each word or semantic role as graph node respectively, which learn the fine-grained intra-AC knowledge within each AC from the word level or semantic level. The multi-view hierarchical GNN makes our model more effective to utilize the rich information among and within the ACs in the input text. Then, we transform ACTC, ARI, and ARTC into node classification, edge prediction, and edge type classification on the argumentation graph by devising novel graph attention mechanisms to learn comprehensive and relation-aware graph embeddings. These three subtasks are integrated into a unified model through multi-task learning and partial parameters sharing. Extensive experiments on two benchmark datasets demonstrate that the proposed MHGNN framework outperforms the strong baseline methods for all three subtasks.},
  archive      = {J_CC},
  author       = {Sun, Yang and Bao, Jianzhu and Tu, Geng and Liang, Bin and Yang, Min and Xu, Ruifeng},
  doi          = {10.1007/s12559-024-10391-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Multi-view hierarchical graph neural network for argumentation mining},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the integration of large-scale time series distance
matrices into deep visual analytic tools. <em>CC</em>, <em>17</em>(1),
1–18. (<a href="https://doi.org/10.1007/s12559-024-10394-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series are essential for modeling a lot of activities such as software behavior, heart rate, and business processes. The analysis of the series data can prevent errors, boost profits, and improve the understanding of behaviors. Among the many techniques available, we can find deep learning techniques and data mining techniques. In data mining, distance matrices between subsequences (similarity matrices, recurrence plots) have already shown their potential in fast large-scale time series behavior analysis. In deep learning, there exist different tools for analyzing the models’ embedding space to get insights into the data behavior. DeepVATS is a tool for large time series analysis that allows the visual interaction within the embedding space (latent space) of deep learning models and the original data. The training and analysis of the model may result in a large use of computational resources, resulting in a lack of interactivity. To solve this issue, we integrate distance matrix plots within the tool. The incorporation of these plots with the associated downsampling techniques makes DeepVATS a more efficient and user-friendly tool for a first quick analysis of the data, achieving runtimes reductions of up to $$10^4$$ seconds, allowing fast preliminary analysis of datasets of up to 7 M elements. Also, this incorporation allows us to detect trends, extending its capabilities. The new functionality is tested in three use cases: the M-Toy synthetic dataset for anomaly detection, the S3 synthetic dataset for trend detection, and the real-world dataset pulsus paradoxus for anomaly checking.},
  archive      = {J_CC},
  author       = {Santamaria-Valenzuela, Inmaculada and Rodriguez-Fernandez, Victor and Camacho, David},
  doi          = {10.1007/s12559-024-10394-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {On the integration of large-scale time series distance matrices into deep visual analytic tools},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy soft topological numbers with an operation of vertex
deletion: A comparative study with TOPSIS method and its application in
car import decision-making. <em>CC</em>, <em>17</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s12559-024-10396-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two distinct soft computing models for representing ambiguity and uncertainty: fuzzy and soft sets. A novel mathematical technique for handling uncertainties is the soft set. This set offers a parameterized viewpoint for soft computing and uncertainty modeling. Soft sets have been shown to have potential uses in a number of disciplines, including probability theory, operations research, game theory, measurement theory, and the smoothness of functions. Topological numbers possess significant importance in graph theory. On topological numbers in fuzzy graph theory, there is also a wealth of literature. However, there has not been much research done on fuzzy soft topological numbers up until now. Fuzzy soft graphs are very versatile instruments available for decision-making. Therefore, in fuzzy soft graph theory, it would be very beneficial to introduce and apply topological numbers. Multi-criteria decision-making approaches give decision-makers the required instruments, but their underlying theories and assumptions may differ. Therefore, choosing the best way to make decisions is just as crucial as actually making the decision. The technique for order preference by similarity to ideal solution (TOPSIS) is the most widely used multi-criteria decision-making method. TOPSIS can solve the real-world problems. TOPSIS is a technique for ranking, based on the weights and impacts of the given elements. However, prior to this study, no work has been done on the application of fuzzy soft topological numbers in decision-making systems. The novelty of this research manuscript is to calculate three fuzzy soft topological numbers before and after the deletion of the vertex for a generalized graph and in a fuzzy soft framework and make a comparison in the results obtained before and after deleting the vertex. Subsequently, we demonstrated an application of international automobile importation into the United States by several nations using various graphical networks, employing a parameterized fuzzy soft graph point of view and three different modes of transportation. It is established that if a vertex is removed from a network, the profit made will decrease significantly. Additionally, the optimal network of nations for all purposes is evaluated. By using the TOPSIS approach, the ranking of networks is also generated from a set of alternatives.},
  archive      = {J_CC},
  author       = {Anwar, Shabana and Kamran Jamil, Muhammad and Azeem, Muhammad and Deveci, Muhammet and Antucheviciene, Jurgita},
  doi          = {10.1007/s12559-024-10396-9},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Fuzzy soft topological numbers with an operation of vertex deletion: A comparative study with TOPSIS method and its application in car import decision-making},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural adaptive dynamic event-triggered containment control
for uncertain multi-agent systems under markovian switching dynamics.
<em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10388-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the containment control problem for multi-agent systems with Markovian switching dynamics by proposing a novel adaptive dynamic event-triggered sliding mode control scheme based on radial basis function neural networks. First, the unknown nonlinear dynamics of the system were approximated by using radial basis function neural networks. The dynamic event-triggered control scheme designed in the framework of sliding mode control operated at specific event sampling moments, thereby reducing computational and communication burdens. The containment control was achieved through a synergistic approach integrating dynamic event-triggered control with neural network-based adaptive control in a stochastic switching system. Moreover, we proved that Zeno behavior was effectively avoided. The proposed distributed containment control technique was validated through simulations, demonstrating its effectiveness and superiority.},
  archive      = {J_CC},
  author       = {Cai, Jiayi and Wu, Wenjun and Yi, Chengbo and Chen, Yanxian},
  doi          = {10.1007/s12559-024-10388-9},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Neural adaptive dynamic event-triggered containment control for uncertain multi-agent systems under markovian switching dynamics},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional cross-modal autoencoder-based few-shot
learning for data augmentation with application to alzheimer dementia
diagnosis. <em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10390-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel deep few-shot learning method for magnetic resonance images-based Alzheimer’s dementia (AD) diagnosis. The proposed method consists of two main phases namely data augmentation and data classification. With regard to data augmentation and, to deal with data scarcity issues, we designed a convolutional cross-modal autoencoder (CCMAE) model for data generation. This model, which consists of two encoders and one decoder, receives two image modalities namely longitudinal and cross-section MRI, and generates a new cross-section image. We opt for a convolutional version of the autoencoder to capture the spatial information more effectively and reduce the number of trainable parameters. Moreover, to make the model able to perform deep analysis of input image, we establish a skip connection strategy between the first encoder and the decoder similar to the UNet mechanism. With regard to classification, we design a convolutional neural network-based model in which both textual and visual features are fused to strengthen the network performance and produce more reliable decisions. A comprehensive experiment on a publicly available dataset has been conducted to demonstrate the effectiveness of the proposed method compared to some related works. The code is publicly available at: https://github.com/Bazine-Othmane/scientific-paper-code .},
  archive      = {J_CC},
  author       = {Bazine, Othmane and Rai, Omar and Aiadi, Oussama and Hedjam, Rachid and Khaldi, Belal and Zhong, Guoqiang},
  doi          = {10.1007/s12559-024-10390-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Convolutional cross-modal autoencoder-based few-shot learning for data augmentation with application to alzheimer dementia diagnosis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative analysis of metaphorical cognition in ChatGPT
and human minds. <em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10393-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT represents a significant advancement in the field of Artificial Intelligence (AI), showcasing the development of a robust AI system capable of multitasking and generating human-like language. At present, many scholars have done evaluations on ChatGPT in terms of language, reasoning, and scientific knowledge abilities, based on benchmarks or well-crafted questions. However, to the best of our knowledge, there is currently no existing comparative analysis from a cognitive perspective that directly assesses ChatGPT alongside humans. Metaphor, serving as a manifestation of linguistic creativity, provides a valuable avenue for examining cognition. This is due to the mapping relationship it establishes between the target and source conceptual domains, reflecting distinct cognitive patterns. In this paper, we use a metaphor processing tool, MetaPro, to analyze the cognitive differences between ChatGPT and humans through the metaphorical expressions in ChatGPT- and human-generated text. We illustrate the preferences in metaphor usage, concept mapping, and cognitive pattern variances across different domains. The methodology utilized in this study makes a valuable contribution to the task-agnostic evaluation of AI systems and cognitive research. The insights garnered from this research prove instrumental in comprehending the cognitive distinctions between ChatGPT and humans, facilitating the identification of potential cognitive biases within ChatGPT.},
  archive      = {J_CC},
  author       = {Mao, Rui and Chen, Guanyi and Li, Xiao and Ge, Mengshi and Cambria, Erik},
  doi          = {10.1007/s12559-024-10393-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A comparative analysis of metaphorical cognition in ChatGPT and human minds},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel depth-connected region-based convolutional neural
network for small defect detection in additive manufacturing.
<em>CC</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-024-10397-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection on the computed tomography (CT) images plays an important role in the development of metallic additive manufacturing (AM). Although some deep learning techniques have been adopted in the CT image-based defect detection problem, it is still a challenging task to accurately detect small-size defects in the presence of undesirable noises. In this paper, a novel defect detection method, namely, the depth-connected region-based convolutional neural network (DC-RCNN), is proposed to detect small defects and reduce the influence of noises. In particular, a saliency-guided region proposal method is first developed to generate small-size region proposals with the aim to accommodate the small defects. Then, the main architecture of DC-RCNN is proposed to extract and connect the consistent features across multiple frames, thereby reducing the influence of randomly distributed noises. Moreover, the transfer learning technique is utilized to improve the generalization ability of the proposed DC-RCNN. In order to verify the effectiveness and superiority, the proposed method is applied to the real-world AM data for defect detection. The experimental validations show that the proposed DC-RCNN is able to detect the small-size defects under noises and outperforms the original RCNN method in terms of detection accuracy and running time.},
  archive      = {J_CC},
  author       = {Wang, Yiming and Wang, Zidong and Liu, Weibo and Zeng, Nianyin and Lauria, Stanislao and Prieto, Camilo and Sikström, Fredrik and Yu, Hui and Liu, Xiaohui},
  doi          = {10.1007/s12559-024-10397-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {A novel depth-connected region-based convolutional neural network for small defect detection in additive manufacturing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued intuitionistic fuzzy yager power operators
and possibility degree-based group decision-making model. <em>CC</em>,
<em>17</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s12559-024-10368-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extended form of intuitionistic fuzzy set, the theory of interval-valued intuitionistic fuzzy set (IVIFS) can describe fuzziness more flexibly. This study aims to develop a group decision-making model based on the distance measure, Yager power aggregation operators and the possibility measure in the context of IVIFSs. For this purpose, new distance measure is proposed to quantify the dissimilarity between two IVIFSs. In addition, comparison with existing distance measures is performed to illustrate the efficiency of introduced measure. Combining the Yager’s triangular norms with the proposed distance-based power operators, a series of interval-valued intuitionistic fuzzy (IVIF) Yager power aggregation operators are introduced with their desirable properties. Moreover, a possibility measure is developed for pairwise comparisons of IVIFSs, which overcomes the shortcomings of existing IVIF-score function, IVIF-accuracy function, and IVIF-possibility measures. The developed possibility measure is further utilized to compute the weights of criteria. To prove the practicality and effectiveness of introduced model, it is applied to a case study of manufacturing plant location selection problem with IVIF information. Finally, sensitivity and comparative analyses are carried out to test the stability and robustness of the proposed method under the setting of IVIFSs.},
  archive      = {J_CC},
  author       = {Rani, Pratibha and Mishra, Arunodaya Raj and Deveci, Muhammet and Alrasheedi, Adel Fahad and Alshamrani, Ahmad M. and Pedrycz, Witold},
  doi          = {10.1007/s12559-024-10368-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Interval-valued intuitionistic fuzzy yager power operators and possibility degree-based group decision-making model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified multi-view data clustering: Simultaneous learning of
consensus coefficient matrix and similarity graph. <em>CC</em>,
<em>17</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12559-024-10392-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating data from multiple sources or views has become increasingly common in data analysis, particularly in fields like healthcare, finance, and social sciences. However, clustering such multi-view data poses unique challenges due to the heterogeneity and complexity of the data sources. Traditional clustering methods are often unable to effectively leverage the information from different views, leading to suboptimal clustering results. To address this challenge, multi-view clustering techniques have been developed, aiming to integrate information from multiple views to improve clustering performance. These techniques typically involve learning a similarity matrix for each view and then combining these matrices to form a consensus similarity matrix, which is subsequently used for clustering. However, existing approaches often suffer from limitations such as the need for manual tuning of parameters and the inability to effectively capture the underlying structure of the data. In this paper, we propose a novel approach for multi-view clustering that addresses these limitations by jointly learning the consensus coefficient matrix and similarity graph. Unlike existing methods that follow a sequential approach of first learning the coefficient matrix and then constructing the similarity graph, our approach simultaneously learns both matrices, ensuring a more regularized consensus graph. Additionally, our method automatically adjusts the weight of each view, eliminating the need for manual parameter tuning. Our approach involves several key steps. First, we formulate an optimization problem that jointly optimizes the consensus coefficient matrix, unified spectral projection matrix, coefficient matrix, and soft cluster assignment matrix. We then propose an efficient algorithm to solve this optimization problem, which involves iteratively updating the matrices until convergence. To learn the consensus coefficient matrix and similarity graph, we leverage techniques from matrix factorization and graph-based learning. Specifically, we use a self-representation technique to learn the coefficient matrix (regularization graPh) and a graph regularization technique to learn the similarity graph. By jointly optimizing these matrices, we ensure that the resulting consensus graph is more regularized and better captures the underlying structure of the data. We evaluate our approach on several public image datasets, comparing it against state-of-the-art multi-view clustering methods. Our experimental results demonstrate that our approach consistently outperforms existing methods in terms of clustering accuracy and robustness. Additionally, we conduct sensitivity analysis to evaluate the impact of different hyperparameters on the clustering performance. We present a novel approach for multi-view data clustering that jointly learns the consensus coefficient matrix and similarity graph. By simultaneously optimizing these matrices, our approach achieves better clustering performance compared to existing methods. Our results demonstrate the effectiveness and robustness of our approach across different datasets, highlighting its potential for real-world applications in various domains.},
  archive      = {J_CC},
  author       = {Dornaika, F. and El Hajjar, S. and Charafeddine, J. and Barrena, N.},
  doi          = {10.1007/s12559-024-10392-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Unified multi-view data clustering: Simultaneous learning of consensus coefficient matrix and similarity graph},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing social issues strategies by using bipolar complex
fuzzy muirhead mean decision-making approach. <em>CC</em>,
<em>17</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12559-024-10353-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive processes that affect people perceptions, comprehension, and interactions with others, such as attribution mistakes and heuristics, are responsible for social issues. These social issues includes polarization, prejudice, and inequality. To address these issues, we must comprehend cognitive mechanisms, and this can be made by using some appropriate multi-attribute decision-making (MADM) approach, that can handle people perceptions of complex and bipolar nature. Thus, in this manuscript, we concentrate on a MADM technique that relies on certain novel aggregation operators in the framework of bipolar complex fuzzy sets. These aggregation operators include Muirhead mean (MM) operator and dual Muirhead mean (DMM) operator of several types. To authenticate the validity of these defined aggregation operators, certain properties of these operators are proved. Furthermore, we consider the interpreted operators to produce a decision-making (DM) technique to deal with bipolar complex fuzzy MADM issues. We then consider a real life example to show the application and need of the interpreted work in daily life. To confirm the viability and potential of the offered technique, we compare our established technique with some other prevailing techniques.},
  archive      = {J_CC},
  author       = {Rehman, Ubaid ur and Mahmood, Tahir and García, Gustavo Santos},
  doi          = {10.1007/s12559-024-10353-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Optimizing social issues strategies by using bipolar complex fuzzy muirhead mean decision-making approach},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A weakly supervised data labeling framework for machine
lexical normalization in vietnamese social media. <em>CC</em>,
<em>17</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s12559-024-10356-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an innovative automatic labeling framework to address the challenges of lexical normalization in social media texts for low-resource languages like Vietnamese. Social media data is rich and diverse, but the evolving and varied language used in these contexts makes manual labeling labor-intensive and expensive. To tackle these issues, we propose a framework that integrates semi-supervised learning with weak supervision techniques. This approach enhances the quality of the training dataset and expands its size while minimizing manual labeling efforts. Our framework automatically labels raw data, converting non-standard vocabulary into standardized forms, thereby improving the accuracy and consistency of the training data. Experimental results demonstrate the effectiveness of our weak supervision framework in normalizing Vietnamese text, especially when utilizing pre-trained language models. The proposed framework achieves an impressive F1-score of 82.72% and maintains vocabulary integrity with an accuracy of up to 99.22%. Additionally, it effectively handles undiacritized text under various conditions. This framework significantly enhances natural language normalization quality and improves the accuracy of various NLP tasks, leading to an average accuracy increase of 1–3%.},
  archive      = {J_CC},
  author       = {Nguyen, Dung Ha and Nguyen, Anh Thi Hoang and Van Nguyen, Kiet},
  doi          = {10.1007/s12559-024-10356-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Cogn. Comput.},
  title        = {A weakly supervised data labeling framework for machine lexical normalization in vietnamese social media},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BrainEnsemble: A brain-inspired effective ensemble pruning
algorithm for pattern classification. <em>CC</em>, <em>17</em>(1), 1–21.
(<a href="https://doi.org/10.1007/s12559-024-10363-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain comprises distinct regions, each with specific functions. Interconnected through neural pathways, the brain regions collaborate to process complex information. Similarly, ensemble learning enhances pattern classification by leveraging the collaboration and complementarity between classifiers. The similarity between the two suggests that simulating the brain’s functional network holds the potential for groundbreaking advancements in the design of ensemble learning algorithms. Motivated by this, our paper proposes a brain-inspired ensemble pruning method called BrainEnsemble. This method provides an example of using classifier combinations to emulate the functions of brain regions. Guided by the principles of curriculum learning and the divide-and-conquer strategy, each artificial brain region can specialize in specific functions and tasks. Additionally, BrainEnsemble simulates the brain regions’ responses and connectivity mechanisms through graph connections. In this model, different artificial brain regions can dynamically reorganize and adjust their interactions to adapt to continuously changing environments or data distributions, enabling the model to maintain high performance when confronted with new data. Extensive experimental results demonstrate the superior performance of BrainEnsemble. In summary, drawing inspiration from the information processing mechanism of the human brain can provide new ideas for the design of ensemble learning algorithms, and more research can be conducted in this direction in the future.},
  archive      = {J_CC},
  author       = {Li, Danyang and Huang, Shisong and Wen, Guihua and Zhang, Zhuhong},
  doi          = {10.1007/s12559-024-10363-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {BrainEnsemble: A brain-inspired effective ensemble pruning algorithm for pattern classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting deep contrast feature for image retrieval.
<em>CC</em>, <em>17</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s12559-024-10375-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of content-based image retrieval (CBIR), fused feature-based methods have demonstrated their advanced performance on the popular benchmark datasets. However, it is inevitable increase the vector dimensionality because the fused features have diversity. Therefore, achieving both a low-dimensional representation and high retrieval performance remains challenging. To address this problem, an image retrieval method based on the deep contrast-based layer is proposed, namely the deep contrast feature histogram (DCFH), to image retrieval. There are three highlights as follows: (1) texture features based on the edge orientation are calculated to build contrast-based layer; it can enhance the discriminative power of deep features; (2) a generalized mean aggregation method is introduced to effectively aggregate the representative information in the deep feature maps of convolutional neural network (CNN); (3) a multi-orientational PCA whitening method is proposed to provide a compact yet discriminative representation. Comparative experiments demonstrated that our method can provide outstandingly competitive retrieval performance on popular benchmark datasets. This work captures visual information from both global and local perspectives, presenting an approach in line with human visual cognitive. Experiments demonstrated that our method can efficiently combine the strengths of various features to provide the robust representation, thereby improving the retrieval performance. Moreover, our method is easily to be implemented without requiring to retrain the CNN models and not the use of additional supervision.},
  archive      = {J_CC},
  author       = {Lu, Zhou and Liu, Guang-Hai and Li, Zuoyong and Yang, Lu},
  doi          = {10.1007/s12559-024-10375-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Exploiting deep contrast feature for image retrieval},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a decision support system for performance
measurement of social movements. <em>CC</em>, <em>17</em>(1), 1–27. (<a
href="https://doi.org/10.1007/s12559-024-10385-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social movements encompass the collective actions of groups gathered under the same goal, operating within a specific organizational structure to reflect their thoughts and views through a series of actions. Social movements are expected to exhibit characteristics of violence, rationality, continuity, and public benefit. The successful performance of social movements is contingent upon their ability to embody these characteristics. The primary motivation of this research is to render the performance of social movements measurable and comparable, aiming to determine their effectiveness. The underlying motivation for this approach is to move beyond subjective evaluations of social movements and instead treat them as structured decision-making processes. The core objective is to develop a decision support system for assessing the performance of social movements and facilitating insights into the performance of movements within a country or region. In this way, the performance evaluation processes of social movements are enhanced, fostering the development of more informed and deliberate social movements. To determine the performance of social movements, the type-2 neutrosophic number (T2NN)–Schweizer Sklar (SS)–symmetry point of criterion (SPC)–evaluation based on relative utility and nonlinear standardization (ERUNS)–(T2NN-SS-SPC-ERUNS) hybrid model is developed and proposed in this research. The T2NN-SS-weighted arithmetic mean aggregation operator is used to combine expert evaluations. The weights of criteria are calculated using the T2NN-SPC method. Performance rankings of social movements are determined using the T2NN-ERUNS method. An algorithm for the three-stage T2NN-SS-SPC-ERUNS hybrid model is developed to evaluate the performance of social movements in Türkiye through a case study. The robustness and consistency of the proposed hybrid method are supported by scenarios. As a result of the research, the “Early Retirement Scheme Victims” social movement is identified as having the highest performance among social movements in Türkiye.},
  archive      = {J_CC},
  author       = {Yalçın, Galip Cihan and Kara, Karahan and Işık, Gülcan and Simic, Vladimir and Pamucar, Dragan},
  doi          = {10.1007/s12559-024-10385-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Cogn. Comput.},
  title        = {Development of a decision support system for performance measurement of social movements},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I2V-CMGAN: Generative adversarial cross-modal network-based
image-to-video person re-identification. <em>CC</em>, <em>17</em>(1),
1–22. (<a href="https://doi.org/10.1007/s12559-024-10389-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information asymmetry situation amongst image and video features in image-to-video (I2V) person re-identification (Re-ID) refers to the difficulty in extracting consistent and dependable features from both image and video data in order to accurately match and identify a person in both modalities. The problem arises because images and videos have different characteristics and represent different aspects of a person. This difference in representation can result in inconsistent and unreliable features being extracted from image and video data, leading to difficulty in accurately matching and re-identifying a person between modalities. The temporal information provided by videos can also boost the accuracy of person re-identification, especially in crowded and cluttered environments. To address the information asymmetry problem, a generative adversarial cross-modal network–based I2V Person Re-ID (I2V-CMGAN) is proposed, which works by using a generator to transform the features learned from the video network into an image network with an additional loss function to improve the consistency and reliability of features extracted from both image and video data and also preserve identity information. Extensive studies show the efficacy of the proposed approach, and the aggregate results on the MARS dataset outperform the state-of-the-art methods by a substantial margin and achieved rank-1 accuracy of 88.9% (+ 2.9), rank-5 accuracy of 95.5% (+ 2.3), rank-10 accuracy of 97.1% (+ 2.9), and mean average precision of 81.2% (+ 1.1) for I2V Re-ID. On iLIDS-VID and PRID2011 datasets, the proposed method attains outstanding margins with rank-1, rank-5, rank-10, mAP of 64.7%, 89.3%, 92.7%, 74.2%, and 80.9%, 93.3%, 98.9%, and 86.8% respectively.},
  archive      = {J_CC},
  author       = {Joshi, Aditya and Diwakar, Manoj},
  doi          = {10.1007/s12559-024-10389-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {I2V-CMGAN: Generative adversarial cross-modal network-based image-to-video person re-identification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic review and thematic analysis of digital games for
cognitive enhancement in children with autism spectrum disorder: Toward
a conceptual framework. <em>CC</em>, <em>17</em>(1), 1–38. (<a
href="https://doi.org/10.1007/s12559-024-10395-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to typically developing people, children with autism spectrum disorder (ASD) have distinct cognitive and intelligence profiles. Some of these children require cognitive rehabilitation. Through the use of cutting-edge therapy and cognitive empowerment methods, some cognitive skills in children with ASD can be strengthened by digital game-based tools. This study’s main purpose is to provide a systematic review and qualitative study about designing digital games for cognitive enhancement in autistic children and to determine the main design components of such digital games. The primary focus of this study is to explore the citations in which the technical and functional elements are provided thoroughly. Furthermore, a conceptual framework is elaborated for designing a digital game for autism. A thorough review of the literature was conducted in the databases of Medline (via PubMed), Web of Science (WOS), Scopus, and IEEE Xplore for English publications published before January 23, 2023. Of 976 papers, 34 studies were found to be eligible in this systematic review. The bulk of the studies were carried out in Asia and Europe. Three (8.8%) studies used games that were built to be multilingual, while 22 (64.7%) studies used games that were only created in English. Creating motivation through narratives, providing incentive systems, raising the complexity level, targeting main skills, and adjusting the choices are the principal components of digital game design. (1) Main cognitive rehabilitation domains in ASD; (2) game designing details: platforms and game genres, motivations, evaluations, game graphics designs, aesthetic mechanisms, incentive systems, and famous game development engines; and (3) mutual interaction between child, therapist, and parents are the crucial categories that are described to devise a conceptual framework in this qualitative study. Of the total number of included studies, 25 studies reported positive effects on autism cases, and in nine, there has not been any evaluation of real cases; however, only usability tests have been conducted. Children with autism may benefit from using appropriate digital game-based interventions to improve mental indices. According to a review, it can be stated that the suitable computerized and digital game-based solutions could enhance cognitive outcomes in children with autism spectrum disorder. However, more research is required to ascertain the true efficacy of these new technologies.},
  archive      = {J_CC},
  author       = {Rezayi, Sorayya and Shahmoradi, Leila and Tehrani-Doost, Mehdi},
  doi          = {10.1007/s12559-024-10395-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Cogn. Comput.},
  title        = {Systematic review and thematic analysis of digital games for cognitive enhancement in children with autism spectrum disorder: Toward a conceptual framework},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-invasive approach for early alzheimer’s detection
through spontaneous speech analysis using deep visibility graphs.
<em>CC</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-024-10398-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying Alzheimer’s disease (AD) in its early stages is a challenging task for physicians and clinicians. This paper proposes a new algorithm for diagnosing AD, which is based on analyzing spontaneous speech signals. The proposed method uses two visibility graph methods, Natural Visibility Graph (NVG) and Horizontal Visibility Graph (HVG), to derive features from speech windows. These features are then given to a deep BiLSTM-based classifier to decide about segments of the signal. The proposed approach could obtain a sensitivity of 98.33%, specificity of 99.44%, and accuracy of 99.17%. The advantage of converting speech signals into graphs using NVG and HVG is that it allows for the extraction of complex structural features that are not easily captured by traditional methods. This method is highly beneficial due to its non-invasive nature, low cost, and lack of side effects. Patients can undergo the procedure without experiencing any discomfort, while also benefiting from its affordability and accessibility. The method’s safety and practicality make it an ideal choice for those seeking a reliable and effective solution. Moreover, the proposed algorithm has a high accuracy in detecting the early stage of AD, which makes it a promising tool to evaluate Alzheimer’s disease diagnosis in its pre-clinical stage.},
  archive      = {J_CC},
  author       = {Mohammadpoory, Zeynab and Nasrolahzadeh, Mahda and Amiri, Sekineh Asadi and Haddadnia, Javad},
  doi          = {10.1007/s12559-024-10398-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {A non-invasive approach for early alzheimer’s detection through spontaneous speech analysis using deep visibility graphs},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-term power load forecasting in city based on
ISSA-BiTCN-LSTM. <em>CC</em>, <em>17</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s12559-024-10401-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term power load forecasting is crucial for the stable operation of power systems. In this paper, we propose an advanced forecasting model that combines the Salp Swarm Algorithm (SSA), Bidirectional Temporal Convolutional Network (BiTCN), and Long Short-Term Memory (LSTM). The model first exploits the parallel fusion of BiTCN and LSTM (BiTCN-LSTM), taking full advantage of BiTCN’s strength in parallel processing of local features and the LSTM’s ability to capture long-term dependencies through its gating mechanisms. Subsequently, the Improved Salp Swarm Algorithm (ISSA) is enhanced through adaptive leader ratio adjustment, dual-food design, and food lure follower strategy. Finally, the hyperparameters of the BiTCN-LSTM model are optimized using ISSA to improve the model performance. In the short-term load forecasting experiments, electric load data and weather data from Los Angeles, Tetouan, and Johor were used to compare the proposed model with eight existing models. The evaluation metrics included root mean square error (RMSE), mean absolute error (MAE), normalized root mean square error (NRMSE), and mean absolute percentage error (MAPE). The experimental results showed that the model achieved lower error values than the comparison model in most cases in different seasons, working days, and rest days in different cities. In particular, the error values of RMSE, MAE, NRMSE, and MAPE were 925.11 kW, 732.63 kW, 0.019, and 1.034% for the rest days in the city of Tetouan, respectively. Compared with other algorithms, ISSA demonstrates stronger optimization capability and shorter optimization time. Additionally, model structure analysis was conducted through optimization comparison and ablation experiments, further demonstrating the proposed model’s strong predictive performance.},
  archive      = {J_CC},
  author       = {Fan, Chaodong and Li, Gongrong and Xiao, Leyi and Yi, Lingzhi and Nie, Shanghao},
  doi          = {10.1007/s12559-024-10401-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {Short-term power load forecasting in city based on ISSA-BiTCN-LSTM},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of metaheuristic algorithms with supervised
machine learning for accurate power consumption prediction. <em>CC</em>,
<em>17</em>(1), 1–35. (<a
href="https://doi.org/10.1007/s12559-025-10402-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate power consumption prediction is a crucial part of energy management. Some of the machine learning models that are the focus of this study for the prediction of power use include Support Vector Regression, Adaptive Boosting, and Decision Tree Regression. These models have been improved with the use of some novel optimizers-namely, the Trochoid Search Optimization, Red-Tailed Hawk, and Giant Armadillo Optimization methods-for hyper-parameter tuning to enhance prediction accuracy. When tested against real data, DTGA outperformed with R2 values of 0.9918, 0.9924, and 0.9934 for three zones. This work extends the study on the forecast of power consumption by integrating machine learning and optimization techniques that provide effective energy management strategies.},
  archive      = {J_CC},
  author       = {Wang, Mengxia and Zhu, Chaoyang and Zhang, Yunxiang and Deng, Jinxin and Cai, Yiwei and Wei, Wei and Guo, Mengxing},
  doi          = {10.1007/s12559-025-10402-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Cogn. Comput.},
  title        = {Application of metaheuristic algorithms with supervised machine learning for accurate power consumption prediction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-aware prediction with secure and lightweight
cognitive decision model in smart cities. <em>CC</em>, <em>17</em>(1),
1–12. (<a href="https://doi.org/10.1007/s12559-025-10403-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive networks with the integration of smart and physical devices are rapidly utilized for the development of smart cities. They are explored by many real-time applications such as smart homes, healthcare, safety systems, and other unpredictable environments to gather data and process network requests. However, due to the external conditions and inherent uncertainty of wireless systems, most of the existing approaches cannot cope with routing disturbances and timely delivery performance. Further, due to limited resources, the demand for a secure communication system raises another potential research challenge to protect sensitive data and maintain the integrity of the urban environment. This paper presents a secured decision-making model using reinforcement learning with the combination of blockchain to enhance the degree of trust and data protection. The proposed model increases the network efficiency for resource utilization and the management of communication devices with the alliance of security. It provides a reliable and more adaptive paradigm by exploring learning techniques for dealing with the intrinsic uncertainty and imprecision of cognitive systems. Also, the incorporation of blockchain technology reduces the risk of a single point of failure, malicious vulnerabilities, and data leakage, ultimately fostering trust for urban sensor applications. It validates the incoming routing links and identifies any communication fault incurred due to malicious interference. The proposed model is rigorously tested and verified using simulations and its significance has been proven for network metrics in comparison to existing solutions.},
  archive      = {J_CC},
  author       = {Al-Quayed, Fatima and Humayun, Mamoona and Alnusairi, Thanaa S. and Ullah, Inam and Bashir, Ali Kashif and Hussain, Tariq},
  doi          = {10.1007/s12559-025-10403-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Context-aware prediction with secure and lightweight cognitive decision model in smart cities},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Engaging preference optimization alignment in large language
model for continual radiology report generation: A hybrid approach.
<em>CC</em>, <em>17</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s12559-025-10404-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) remain relatively underutilized in medical imaging, particularly in radiology, which is essential for disease diagnosis and management. Nonetheless, radiology report generation (RRG) is a time-consuming task that can result in delays and inconsistencies. To address these challenges, we present a novel hybrid approach that integrates multi-modal radiology information and preference optimization alignment in LLM for continual RRG. Our method integrates a pre-trained small multi-modal model to analyze radiology images and generate an initial report, which is subsequently refined and aligned by an LLM using odds ratio preference optimization (ORPO) and with historical patient data and assessments to mimic radiologist-like responses, bypassing reinforcement learning from human feedback-based (RLHF) alignment. This two-stage fusion—supervised fine-tuning followed by preference optimization—ensures high accuracy while minimizing hallucinations and errors. We also propose a data field curation strategy extendable to various other RRG modality datasets, focusing on selecting relevant responses for preference alignment. We evaluate our approach on two public datasets, achieving state-of-the-art performance with average Bleu scores of 0.375 and 0.647, Meteor scores of 0.495 and 0.714, Rouge-L scores of 0.483 and 0.732, and average F1-RadGraph scores of 0.488 and 0.487, for chest X-rays and lung CT scan datasets, respectively. We further provide in-depth qualitative analyses and ablation studies to explain the workings of our model and grasp the clinical relevance for RRG. This work presents the first application of preference optimization in continual RRG, representing a significant advancement in automating clinically reliable report generation. By reducing cognitive burdens on radiologists through AI-powered reasoning and alignment in LLMs, the proposed model improves decision-making, perception, and diagnostic precision, streamlining workflows and enhancing patient care. Our code is available at https://github.com/AI-14/r2gpoallm .},
  archive      = {J_CC},
  author       = {Izhar, Amaan and Idris, Norisma and Japar, Nurul},
  doi          = {10.1007/s12559-025-10404-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Engaging preference optimization alignment in large language model for continual radiology report generation: A hybrid approach},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hyperparameter optimization approach for supervised
classification: Phase prediction of multi-principal element alloys.
<em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-025-10405-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a hyperparameter optimization approach is proposed for the phase prediction of multi-principal element alloys (MPEAs) through the introduction of two novel hyperparameters: outlier detection and feature subset selection. To gain a deeper understanding of the connection between alloy phases and their elemental properties, an artificial neural network is employed, with hyperparameter optimization performed using a genetic algorithm to select the optimum hyperparameters. The two novel hyperparameters, outlier detection and feature subset selection, are introduced within the optimization framework, along with new crossover and mutation operators for handling single and multi-valued genes simultaneously. Ablation studies are conducted, illustrating an improvement in prediction accuracy with the inclusion of these new hyperparameters. A comparison with five existing algorithms in multi-class classification is made, demonstrating an improvement in the performance of phase prediction, thereby providing a better perception of the alloy phase space for high-throughput MPEA design.},
  archive      = {J_CC},
  author       = {Fatimi, Syed Hassan and Wang, Zidong and Chang, Isaac T. H. and Liu, Weibo and Liu, Xiaohui},
  doi          = {10.1007/s12559-025-10405-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {A novel hyperparameter optimization approach for supervised classification: Phase prediction of multi-principal element alloys},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmenting cardiovascular disease prediction through CWCF
integration leveraging harris hawks search in deep belief networks.
<em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-025-10406-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease (CVD) is a major global health concern, demanding accurate predictive models to aid preventive healthcare strategies. Heart failure, stroke, and coronary artery disease are among the disorders that fall under the umbrella term of cardiovascular disease (CVD). Leveraging the Harris Hawks Optimization (HHO) algorithm in conjunction with deep belief networks (DBNs) aims to improve CVD risk prediction accuracy. Harris Hawks Optimization (HHO) draws inspiration from the cooperative behavior of Harris’s hawks in nature, providing an efficient metaheuristic search algorithm for optimization problems. Integrating HHO into machine learning frameworks enhances the exploration and exploitation of search spaces, leading to improved model performance and convergence rates, particularly in deep learning tasks like feature selection and hyperparameter tuning. Powerful generative models called deep belief networks (DBNs) are made up of several layers of latent, stochastic variables. Restricted Boltzmann machines (RBMs) serve as building blocks in the training process of deep belief networks (DBNs), facilitating the unsupervised pre-training of hidden layers. Leveraging RBMs within DBNs enables the extraction of hierarchical representations, enhancing the network’s ability to learn intricate patterns and improve predictive performance in complex data settings. They leverage unsupervised learning techniques to extract intricate patterns and hierarchical representations from complex data, making them ideal for tasks such as feature learning and classification in machine learning research. This study introduces innovative algorithms, including the correlation-based weighted compound feature generation (CWCFG) technique, to enhance the optimization process of HHO. Comparative analysis against traditional machine learning models and rule-based firefly optimizer (RBFO) and Grey Wolf Optimizer (GWO) with the state-of-the-art deep learning techniques demonstrates the efficacy of the CWCFG-HHO-DBN model. Additionally, an in-depth feature importance analysis identifies key predictors, enriching the model’s interpretability. The research conducts a comprehensive evaluation of the proposed model, employing various performance metrics such as accuracy, precision, recall, and F-measure. With a remarkable accuracy of 97.19%, the HHO-DBN model shows promise in enhancing CVD risk prediction. The findings underscore its potential in personalized medicine, facilitating tailored interventions for high-risk individuals. Future directions include refining the algorithm and expanding its application in healthcare settings.},
  archive      = {J_CC},
  author       = {Savitha, S. and Kannan, A. Rajiv and Logeswaran, K.},
  doi          = {10.1007/s12559-025-10406-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Augmenting cardiovascular disease prediction through CWCF integration leveraging harris hawks search in deep belief networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An asymmetric semantic segmentation model via lightweight
attention-guided feature enhancement and fusion. <em>CC</em>,
<em>17</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12559-025-10407-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is widely used in fields such as autonomous driving and unmanned aerial vehicle navigation. However, the huge computational burden and redundant parameters limit its application in edge devices such as mobile phones. In this study, we propose an asymmetric lightweight semantic segmentation model via lightweight attention-guided feature enhancement and fusion. Specifically, the proposed model adopts an encoder-decoder structure. In the encoder, we design an asymmetric feature extraction module to extract image information and use the locally sensitive Hash self-attention to enhance the global information. In the decoder, we first use channel attention to filter out the useless information in shallow layers and adopt the spatial attention to refine local features in deep layers. We then fuse the multi-scale features by the gating mechanism. Additionally, we also design an auxiliary loss to supervise the segmentation of small objects. The results on Cityscapes and CamVid show that the proposed model achieves a good balance between accuracy and the number of parameters. It obtains 70.68% and 72.19% mIoU on the two test datasets with 0.86M parameters, respectively. Code is available on https://github.com/year410/LAANET},
  archive      = {J_CC},
  author       = {Tang, Qingsong and Zhao, Minghui and Ren, Yalei and Shi, Xiaomeng and Jiang, Wuming},
  doi          = {10.1007/s12559-025-10407-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {An asymmetric semantic segmentation model via lightweight attention-guided feature enhancement and fusion},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic behavior of three-layer fractional-order neural
networks with multiple delays. <em>CC</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-025-10411-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the complex network in the real world are not single-layer networks, and networks will be connected with each other. Networks with multi-layer is important because it means cognitive and artificial intelligence. Most current studies of networks consider the case that with n-nodes including ring network, small-word network, scale-free network, etc. This type of network is not enough to describe the complex structure of actual neural networks. However, it is more actual to study the dynamic behavior of multi-layer networks than single-layer networks. In this paper, the stability and bifurcation of a class of three-layer fractional-order neural networks with multiple delays was studied for the first time. By selecting the appropriate bifurcation parameter, the internal dynamic behavior of the given model was discussed by using the theory of Hopf bifurcation, and the critical value and criterion for Hopf bifurcation are derived. The influence of delay, fractional order, and the number of hidden neurons on the bifurcation point were discussed in detail. And the critical value of Hopf bifurcation is accurately calculated. The results show that the stability of the system can be destroyed by increasing the fractional order and the number of hidden neurons. The correctness of the theoretical results is verified by numerical simulation.},
  archive      = {J_CC},
  author       = {Li, Xinyu and Cheng, Zunshui and Xin, Youming and Shang, Yun},
  doi          = {10.1007/s12559-025-10411-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Dynamic behavior of three-layer fractional-order neural networks with multiple delays},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to calibrate prototypes for few-shot image
classification. <em>CC</em>, <em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s12559-025-10412-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to generalise the model to novel classes by using a limited amount of discriminative samples (a.k.a., prototypes). With few labelled samples, there is much uncertainty and randomness in the data, which makes it more difficult for the model to learn the complete underlying patterns. This paper proposes a Discriminative Property Calibration Network (DPCNet) to enable model building in linear space with robust separability. Concretely, the property features of samples are extracted to facilitate filtering out low-informative key points at the instance level, and then the key points are further refined from the perspective of property features to retain those dimensions that contain the most relevant properties. Furthermore, the discriminative key properties are re-weighted by accounting for the correlation between images, thus forcing the model to focus more on the key property information. Moreover, a new margin algorithm is proposed to optimise the data distribution of features by dynamically adjusting the distance between classes. We conduct extensive experiments on four datasets, i.e., miniImageNet, tiredImagenet, CUB-200-2011 and CIFAR-FS, achieving the accuracies of 67.96%, 72.57%, 79.6% and 74.56%, respectively, on the 5-way 1-shot setting, and the same very competitive performance on the 5-way 5-shot setting. The proposed method can well extract the most relevant and discriminative properties, the re-weighted features further emphasise the discrimination and the dynamic margin algorithm enhances the stability and generalisation ability. The proposed method achieves the state-of-the-art performance, and it will have meaningful inspiration for future works.},
  archive      = {J_CC},
  author       = {Liang, Chenchen and Jiang, Chenyi and Wang, Shidong and Zhang, Haofeng},
  doi          = {10.1007/s12559-025-10412-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Cogn. Comput.},
  title        = {Learning to calibrate prototypes for few-shot image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tweet credibility ranker: A credibility features’ fusion
model. <em>CC</em>, <em>17</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s12559-025-10413-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misinformation on social media has emerged as a modern weapon of warfare, disrupting societal peace, trust, justice, and democracy. It is quite challenging to address the issue of information credibility for microblogs. It becomes more challenging when the authenticity of the poster is hidden. The concept of information credibility has multi-perspectives. There are many necessary aspects of information credibility which must be considered for effective credibility assessment. It is observed that some important aspects of credibility are not considered in existing studies. The complete credibility assessment solution needs a comprehensive and diverse set of features for such complex identification. Therefore, these features are identified and proposed by exploring the related research studies consisting of the necessary credibility aspects. These features consist of diverse levels provided by microblogs. These levels include the post, poster, poster’s social network, and actual information propagation network. An exploratory study is also conducted to propose the best credibility features that are used in the proposed solution. The attempt is made for a hybrid features fusion model which combines feature-based or machine learning and graph-based approaches. It is a lightweight, high-performing, non-latent features model to avoid their drawbacks. It assesses the levels of credibility of the concerned post. It is designed for high-impact applications to combat low-credibility content during elections, crises, and other critical scenarios. The model is executed over a publicly available dataset extended for credibility assessment. The model provides good results with 95.6% accuracy by XGBoost using platinum features. The performance of the proposed model is compared with state-of-the-art that produced much-appreciating results.},
  archive      = {J_CC},
  author       = {Qureshi, Khubaib Ahmed and Malick, Rauf Ahmed Shams},
  doi          = {10.1007/s12559-025-10413-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Cogn. Comput.},
  title        = {Tweet credibility ranker: A credibility features’ fusion model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive neural network algorithm with quasi
opposition-based learning for numerical optimization problems.
<em>CC</em>, <em>17</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s12559-025-10415-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structure of artificial neural networks and the biological nervous systems serve as the foundation for the creation of the neural network algorithm (NNA). The robust global search capability of NNAs makes it an effective tool for solving a wide range of complex optimization problems. Unfortunately, its limited relevance to many optimization problems is due to its poor exploitation, weak convergence, and tendency to fall into local optima. The paper’s goal is to introduce an enhanced version of the NNA known as the adaptive quasi-opposition-based neural network algorithm (AQOBNNA) in order to overcome these issues. The quasi-opposition-based learning (QOBL) and an adaptive strategy are combined in this suggested algorithm, where the adaptive technique is added to determine whether or not to use QOBL. The QOBL technique replaces a random search individual with the best one throughout the position update phase in order to enhance exploitation and increase exploration capabilities. The performance of the suggested AQOBNNA is assessed using a set of 23 traditional benchmark functions and compared with a number of current methods. It is evident from the experimental data that AQOBNNA performs better overall and outperforms all the algorithms that were examined.},
  archive      = {J_CC},
  author       = {Kundu, Tanmay and Garg, Harish},
  doi          = {10.1007/s12559-025-10415-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Cogn. Comput.},
  title        = {An adaptive neural network algorithm with quasi opposition-based learning for numerical optimization problems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring influence of different emotions on decision-making
by analyzing the temporal, spatial, and spectral domains of EEG.
<em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-025-10416-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making is a complex cognitive process, in which emotion is one of the most important factors. But insights into the influence of emotion on decision-making are scarce, especially the underlying mechanism of the brain. To reveal the brain’s underlying mechanisms of the influence of emotion on decision-making, an experiment involving emotion elicitation and decision-making tasks was designed. Electroencephalography (EEG), behavioral, and subjective data were collected and conducted. We constructed time-varying weighted directed networks by phase slope index (PSI) in four frequency bands and calculated graph theory metrics. Firstly, the period that the brain processes information most efficiently is 100–300 ms after the appearance of the decision-making task. Secondly, by analyzing the temporal-spatial domains of EEG, the significant differences in global efficiency (GE) and local efficiency (LE) were found among three different emotion groups in the alpha band in the low-difficulty task during 100–300 ms. Thirdly, most activation regions of different emotions were similar and concentrated in the parietal, and occipital lobes but there were still slight differences that were more likely to be found in the prefrontal and left temporal lobes. Graph theory metrics in the decision-making process changed dynamically in the temporal domain and graph theory metrics of different emotions were different.},
  archive      = {J_CC},
  author       = {Wang, Xinyuan and Wang, Danli and Zhao, Yanyan},
  doi          = {10.1007/s12559-025-10416-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Exploring influence of different emotions on decision-making by analyzing the temporal, spatial, and spectral domains of EEG},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HLAE: Hierarchical local attention encoder for MRI brain
tumor image classification. <em>CC</em>, <em>17</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12559-025-10419-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MRI-based brain tumor classification is a challenging neuroimaging task, where the key lies in leveraging ensemble information from brain images. However, current algorithms primarily encode global appearance features of brain images and fail to account for local dependencies inherent in brain tissue adequately. In addition, most existing approaches do not thoroughly investigate the importance coefficients among different regions of brain images. To address these issues, in this work, we propose a novel cognitively-inspired hierarchical local attention encoder (HLAE) framework for capturing local dependency information in MRI images. Based on the characteristics of MRI images, we focus on local information from two perspectives: long-range visual feature dependencies and high-order structural context correlations to fully describe the content association and location relations of brain MRI images. For this purpose, a Swin Transformer is first utilized for encoding the patch-wise content dependencies of a brain MRI image by shifting windows and skipping connections. Meanwhile, a graph structure is also extracted from the MRI image, and a graph attention network is employed to capture the image’s contextual correlations. Finally, the two local information are integrated, and a softmax layer is used to obtain the final brain tumor classification result. This framework naturally contains the attention mechanism, which can effectively quantify the importance among different brain image regions, so as to locate the most discriminative regions in brain tumor classification accurately. Extensive experiments are conducted on two publicly available brain tumor MRI datasets. The results demonstrate its ability to automatically detect brain tumors with superior performance compared to state-of-the-art algorithms.},
  archive      = {J_CC},
  author       = {Dong, Changxu and Sun, Dengdi and Luo, Bin},
  doi          = {10.1007/s12559-025-10419-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {HLAE: Hierarchical local attention encoder for MRI brain tumor image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the power of generative AI in agriculture 4.0 for
smart and sustainable farming. <em>CC</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-025-10420-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative artificial intelligence (GAI) represents a pioneering class of artificial intelligence systems renowned for producing diverse media, such as text and images. Agriculture 4.0 (AG-4.0) is a concept that integrates advanced technologies such as the Internet of Things (IoT), data analytics, artificial intelligence, and precision agriculture into the agricultural sector. The integration of GAI and AG-4.0 can generate new and valuable agricultural insights and solutions through pattern recognition and data analysis. This integration enhances farming practices by generating predictive models, simulating optimal growth conditions, diagnosing plant diseases, and optimizing genetic traits. In spite of the tremendous scope of GAI in agriculture, there has been no detailed study concerning the applications and scope of GAI in AG-4.0. Addressing this research gap, we explore various applications, real-world products, and limitations of GAI in agriculture. We explore how GAI models such as ChatGPT and Dall-E can be personalized advisors for farmers, help increase awareness about farmer relief programs, design farm layouts, and many other such applications. Additionally, we cover four real-world GAI products deployed to assist farmers. Since GAI is a growing technology, it poses challenges such as scarcity of data, data privacy, and interpretability. We elaborately discuss these limitations and suggest multiple directions for future research in GAI for agriculture.},
  archive      = {J_CC},
  author       = {Sai, Siva and Kumar, Sanjeev and Gaur, Aanchal and Goyal, Shivam and Chamola, Vinay and Hussain, Amir},
  doi          = {10.1007/s12559-025-10420-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Unleashing the power of generative AI in agriculture 4.0 for smart and sustainable farming},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative deep learning framework for accurate plant
disease detection and crop productivity enhancement. <em>CC</em>,
<em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-025-10421-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern agriculture, the detection of plant diseases is crucial for enhancing crop productivity. Predicting disease onset and providing advice to farmers are essential steps to achieve increased yields on a large scale. The research addresses the critical need for timely and accurate plant leaf disease diagnosis to prevent growth issues. Leveraging deep learning advancements, the work confronts challenges like small lesion characteristics, distorted backgrounds, data imbalances, and limited generalization in agricultural datasets. After preprocessing the leaf images with tasks like data augmentation and resizing, a sheaf attention U-net with K-means clustering (SAUKC) is employed for segmentation to identify the region of interest. The segmented features are then input into the Orientation-guided Crystal Edge Deep Network (OCEDN) for infection detection. Fine-tuning with the improved kookaburra optimization algorithm (IKOA) addresses training challenges. The proposed method accurately identifies plant leaf diseases, achieving a remarkable accuracy rate of 98%. The validity of the statistical analysis is confirmed to substantiate the outcomes regarding accuracy, specificity, and recall.},
  archive      = {J_CC},
  author       = {M., Mohan and Anandamurugan, S.},
  doi          = {10.1007/s12559-025-10421-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Innovative deep learning framework for accurate plant disease detection and crop productivity enhancement},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Verifying technical indicator effectiveness in
cryptocurrency price forecasting: A deep-learning time series model
based on sparrow search algorithm. <em>CC</em>, <em>17</em>(1), 1–21.
(<a href="https://doi.org/10.1007/s12559-025-10422-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting cryptocurrency prices is challenging due to market volatility and dynamic behavior. This study aims to enhance prediction accuracy for Bitcoin (BTC), Ethereum (ETH), and Litecoin (LTC) by proposing a novel deep learning framework. The framework integrates the Sparrow Search Algorithm (SSA) for selecting optimal technical indicators with Bidirectional Long Short-Term Memory (Bi-LSTM) networks. Technical indicators derived from historical market data, including prices and trading volume, are analyzed to improve forecasting. The results demonstrate that the proposed framework effectively enhances prediction accuracy for BTC and LTC. For ETH, the best performance is achieved using all 34 indicators with the Bi-LSTM model. These findings highlight the importance of selecting relevant indicators and demonstrate the potential of advanced deep learning models in addressing the complexities of cryptocurrency markets. This research provides valuable insights and a reliable framework for improving cryptocurrency price predictions.},
  archive      = {J_CC},
  author       = {Cheng, Ching-Hsue and Yang, Jun-He and Dai, Jia-Pei},
  doi          = {10.1007/s12559-025-10422-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Verifying technical indicator effectiveness in cryptocurrency price forecasting: A deep-learning time series model based on sparrow search algorithm},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional connectivity imbalance between positive and
negative networks in mild cognitive impairment via feature selection.
<em>CC</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-024-10399-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CC},
  author       = {Wu, Haifeng and Pu, Changlin},
  doi          = {10.1007/s12559-024-10399-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Functional connectivity imbalance between positive and negative networks in mild cognitive impairment via feature selection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute reduction in a hybrid decision information system
based on fuzzy conditional information entropy using iterative model and
matrix operation. <em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-024-10400-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction of hybrid decision information systems (HDISs) is a significant research area within the field of machine learning. Due to the presence of nominal attributes, it is difficult to accurately measure the distance between objects in HDISs, which often results in poor attribute reduction for these systems. Rough set theory (RST) is a crucial tool for attribute reduction, but it requires computation of upper and lower approximations, which often leads to computational difficulties. In response to the aforementioned issues, this paper proposes a fast attribute reduction algorithm for HDISs based on fuzzy conditional information entropy that utilizes an iterative model and matrix operations. Firstly, a novel measurement of the distance between nominal attribute values is defined using decision attributes. Subsequently, fuzzy conditional information entropy is calculated from the perspective of “the attribute values is fed back to the attribute set” and its properties are provided. Additionally, an iterative attribute reduction model and difference matrix are established, and two new matrix operations are introduced. Finally, an iterative attribute reduction algorithm is provided. The results of experiments and statistical tests on fifteen UCI datasets, including three large datasets, demonstrate that the proposed algorithm is more effective and efficient than nine state-of-the-art algorithms. This paper not only addresses the issue of difficulty in measuring the distance between nominal attribute values but also significantly improves the computational efficiency of attribute reduction algorithms based on RST, making it possible for them to be applied to large datasets.},
  archive      = {J_CC},
  author       = {Ma, Xiaoqin and Peng, Yichun and Yu, Wenchang and Xu, Yi and Zhang, Qinli and Li, Zhaowen},
  doi          = {10.1007/s12559-024-10400-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Attribute reduction in a hybrid decision information system based on fuzzy conditional information entropy using iterative model and matrix operation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-attribute group decision-making method under
linguistic q-rung orthopair fuzzy environment based on archimedean
copula and extended power average operator. <em>CC</em>, <em>17</em>(1),
1–28. (<a href="https://doi.org/10.1007/s12559-025-10409-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute group decision-making (MAGDM) refers to a series of decision-making problems that rank all possible alternatives based on decision makers’ cognition and evaluations over alternatives from multiple attributes. Hence, the precondition of MAGDM is felicitously describing decision makers’ fuzzy and uncertain cognitive information in complicated decision-making issues. The recently proposed linguistic q-rung orthopair fuzzy set (Lq-ROFS), which uses two linguistic terms to denote membership and non-membership degrees, has been proved to be an effective and promising tool to depict decision makers’ complex cognition in real MAGDM problems. Considering the drawbacks of existing Lq-ROFS-based decision-making methods, this paper focuses on MAGDM approaches where decision makers’ cognitive information is denoted by Lq-ROFSs. The main contribution of this paper is to propose a novel MAGDM method based on Lq-ROFSs. This paper introduces a new MAGDM method under Lq-ROFSs. In order to do this, this study first puts forward some new operational rules for linguistic q-rung orthopair fuzzy numbers (Lq-ROFNs) based on Archimedean copula. These new operational rules are more flexible than existing ones and some other operations can be derived by using different generators. Second, to effectively aggregate Lq-ROFNs, the extended power average operator is applied in linguistic q-rung orthopair fuzzy environment and based on the new operational rules, some novel aggregation operators are generated. Afterward, the developed aggregation operators are used in decision-making problems and a novel MAGDM method which concentrates on linguistic q-rung orthopair fuzzy decision environment is introduced. Specific steps of the new method are illustrated in detail and it is then applied in some illustrative examples to verify its effectiveness. Our proposed method is effective for handling MAGDM problems under Lq-ROFSs. Numerical examples have shown the effectiveness in handling realistic MAGDM problems. In addition, comparison with some existing methods illustrates the advantages and superiorities of our method. This paper introduces a new MAGDM method under Lq-ROFSs. This method is based on Archimedean copula, extended power average operator, and Lq-ROFSs, and is powerful and flexible to cope with MAGDM problems in reality.},
  archive      = {J_CC},
  author       = {Tang, Fangcheng and Zhang, Yushu and Wang, Jun},
  doi          = {10.1007/s12559-025-10409-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-attribute group decision-making method under linguistic q-rung orthopair fuzzy environment based on archimedean copula and extended power average operator},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovering the cognitive bias of toxic language through
metaphorical concept mappings. <em>CC</em>, <em>17</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s12559-025-10423-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prosperity of social media, toxic language spreading over social media has become an unignorable challenge for individual mental health and social harmony. Many researchers have studied toxic language identification to control or mitigate it. However, it still leaves a blank in the cognitive patterns of toxic language. Metaphors as a common feature in natural language connect literal and metaphorical meanings, which could be a useful tool to study the underlying cognitive patterns of the text. In this paper, we utilize a metaphor processing tool, MetaPro, to process a public toxic language dataset and analyze the cognitive biases between toxic and non-toxic language, multiple levels and subtypes of toxic language as well as toxic language mentioning different genders, sexual orientations, and races. Our study demonstrates that significant differences exist in cognitive patterns of the above-mentioned categories and analyzes the differences with machine learning methods.},
  archive      = {J_CC},
  author       = {Ge, Mengshi and Mao, Rui and Cambria, Erik},
  doi          = {10.1007/s12559-025-10423-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Discovering the cognitive bias of toxic language through metaphorical concept mappings},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="cis---31">CIS - 31</h2>
<ul>
<li><details>
<summary>
(2025). ConvNeXt embedded u-net for semantic segmentation in urban
scenes of multi-scale targets. <em>CIS</em>, <em>11</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s40747-024-01735-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of urban scenes is essential in urban traffic analysis and road condition information acquisition. The semantic segmentation model with good performance is the key to applying high-resolution urban locations. However, the types of these images are diverse, and the spatial relationships are complex. It is greatly affected by weather and light. Objects of different scales pose significant challenges to image segmentation of urban scenes. The existing semantic segmentation is mostly solved from the target scale and superpixel methods. Our research mainly fills the gap in image segmentation field of ConvNeXt fusion U-Net pyramid network model in specific urban scenes. These methods could be more accurate. Therefore, we propose the multi-scale fusion deformation residual pyramid network model method in this paper. This method captures features of different scales and effectively solves the problem of urban scene image segmentation of memory scenes by objects of different scales. We construct a spatial information interaction module to reduce the semantic ambiguity caused by complex spatial relations. By combining spatial and channel characteristics, a series of problems caused by weather and light can be alleviated. We verify the improved semantic segmentation model on the Cityscape dataset. The experimental results show that the method achieves 84.25% MPA and 75.61% MIoU. Our improved algorithm, ConvNeXt embedding in the U-Net algorithm architecture, is named Conv-UNet. The improved method proposed in this paper is superior to other methods in the semantic segmentation of urban scenes. The main advantage of this algorithm is to explore the specific loss function and segmentation strategy suitable for urban scene in the face of the complexity and diversity of urban scene images.},
  archive      = {J_CIS},
  author       = {Wu, Yanyan and Li, Qian},
  doi          = {10.1007/s40747-024-01735-2},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Complex Intell. Syst.},
  title        = {ConvNeXt embedded U-net for semantic segmentation in urban scenes of multi-scale targets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model of feature extraction for well logging data based on
graph regularized non-negative matrix factorization with optimal
estimation. <em>CIS</em>, <em>11</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s40747-025-01783-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir oil-bearing recognition is the process of predicting reservoir types based on well logging data, which determines the accuracy of recognition. However, the original well logging data is multidimensional and contains potential noise, which can influence the performance of sequent processing, such as clustering and classification. It is crucial to obtain key low-dimensional features and study an accurate automatic recognition algorithm under unsupervised condition. To solve this problem, we propose a feature extraction method named graph regularized non-negative matrix factorization with optimal estimation (GNMF-OE) according to the characteristics of well logging data in this paper. Firstly, the low dimensional embedding dimension of high-dimensional well logging data is modeled and estimated, which enables the method to obtain the appropriate number of features that reflect the data structure. Secondly, local features are optimized by structured initial vectors in the framework of GNMF, which encourages the basis matrix to have clear reservoir category characteristics. These two approaches are meaningful and beneficial to construct an appropriate basis matrix that discovers the intrinsic structure of well logging data. The visualized experimental results on real datasets from Jianghan oilfield in China show that the proposed method has significant clustering performance for reservoir oil-bearing recognition.},
  archive      = {J_CIS},
  author       = {Yuan, Kehong and Shang, Youlin and Guo, Haixiang and Dong, Yongsheng and Liu, Zhonghua},
  doi          = {10.1007/s40747-025-01783-2},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {A model of feature extraction for well logging data based on graph regularized non-negative matrix factorization with optimal estimation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new representation in genetic programming with hybrid
feature ranking criterion for high-dimensional feature selection.
<em>CIS</em>, <em>11</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s40747-025-01784-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a common method for improving classification performance. Selecting features for high-dimensional data is challenging due to the large search space. Traditional feature ranking methods that search for top-ranked features cannot remove redundant and irrelevant features and may also ignore interrelated features. Evolutionary computation (EC) techniques are widely used in feature selection due to their global search capability. However, EC can easily fall into local optima when dealing with feature selection for high-dimensional applications. The top-ranked features are more likely to construct effective feature subsets and help EC reduce the search space. This paper proposes a feature selection method based on Genetic Programming (GP) with hybrid feature ranking criterion called GPHC, which combines multiple feature ranking methods into the GP structure using a novel GP representation to search for effective feature subsets. Experiments on eight high-dimensional datasets show that GPHC achieves significantly better classification performance compared to five feature ranking methods. Further comparisons between GPHC and other evolutionary algorithms demonstrate that GPHC has advantages in terms of classification performance, the number of features, and convergence speed.},
  archive      = {J_CIS},
  author       = {Li, Jiayi and Zhang, Fan and Ma, Jianbin},
  doi          = {10.1007/s40747-025-01784-1},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Complex Intell. Syst.},
  title        = {A new representation in genetic programming with hybrid feature ranking criterion for high-dimensional feature selection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demonstration and offset augmented meta reinforcement
learning with sparse rewards. <em>CIS</em>, <em>11</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s40747-025-01785-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces DOAMRL, a novel meta-reinforcement learning (meta-RL) method that extends the Model-Agnostic Meta-Learning (MAML) framework. The method addresses a key limitation of existing meta-RL approaches, which struggle to effectively use suboptimal demonstrations to guide training in sparse reward environments. DOAMRL effectively combines reinforcement learning (RL) and imitation learning (IL) within the inner loop of the MAML framework, with dynamically adjusted weights applied to the IL component. This enables the method to leverage the exploration strengths of RL and the efficiency benefits of IL at different stages of training. Additionally, DOAMRL introduces a meta-learned parameter offset, which enhances targeted exploration in sparse reward settings, helping to guide the meta-policy toward regions with non-zero rewards. To further mitigate the impact of suboptimal demonstration data on meta-training, we propose a novel demonstration data enhancement module that iteratively improves the quality of the demonstrations. We provide a comprehensive analysis of the proposed method, justifying its design choices. A comprehensive comparison with existing methods in various stages (including training and adaptation), using both optimal and suboptimal demonstrations, along with results from ablation and sensitivity analysis, demonstrates that DOAMRL outperforms existing approaches in performance, applicability, and robustness.},
  archive      = {J_CIS},
  author       = {Li, Haorui and Liang, Jiaqi and Wang, Xiaoxuan and Jiang, Chengzhi and Li, Linjing and Zeng, Daniel},
  doi          = {10.1007/s40747-025-01785-0},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Complex Intell. Syst.},
  title        = {Demonstration and offset augmented meta reinforcement learning with sparse rewards},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDGANets: A semantically enhanced dual graph-aware network
for affine and registration of remote sensing images. <em>CIS</em>,
<em>11</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s40747-025-01792-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image pairs of different time phases have complex and changeable semantic contents, and traditional convolutional registration methods are challenging in modeling subtle local changes and global large-scale deformation differences in detail. This results in poor registration performance and poor feature representation. To address these problems, a semantically enhanced dual-graph perception framework is proposed. This framework aims to gradually achieve semantic alignment and precise registration of remote sensing image pairs of different time phases via coarse to fine stages. On the one hand, a newly designed large-selection kernel convolution attention module is used to learn affine transformation parameters. Attention to global semantics perceives the large pixel displacement deviation caused by large-scale deformation, and the association relationship is established between remote sensing image pairs of different time phases. At the same time, dual-graph perception modules are embedded in multiple subspace structures, and the subtle local changes of remote sensing image pairs are modeled through the dynamic aggregation ability of graph perception nodes to achieve coarse registration of remote sensing images. On the other hand, a U-shaped module guided by global attention with deformable convolution is used to refine the local spatial structural features and global contextual semantic information of the rough registration, establish dependencies between channels, and correct the pixel displacement deviation of remote sensing image pairs of different phases through position encoding. It is worth noting that the newly designed weighted loss function supervises the learning of each module and the entire network structure from the perspective of inverse consistency, promoting the network’s optimal performance. Finally, the experimental results on the AerialData and GFRS datasets show that the proposed framework has good registration performance, with mean absolute error (MAE) of 3.64 and 3.81, respectively.},
  archive      = {J_CIS},
  author       = {Zhuli, Xie and Gang, Wan and Jia, Liu and Dongdong, Bu},
  doi          = {10.1007/s40747-025-01792-1},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {SDGANets: A semantically enhanced dual graph-aware network for affine and registration of remote sensing images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BSAformer: Bidirectional sequence splitting aggregation
attention mechanism for long term series forecasting. <em>CIS</em>,
<em>11</em>(4), 1–22. (<a
href="https://doi.org/10.1007/s40747-025-01794-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting plays a crucial role across various sectors, including energy, transportation, meteorology, and epidemiology. However, existing models often struggle with capturing long-term dependencies and managing computational efficiency when handling complex and extensive time series data. To address these challenges, this paper introduces the BSAformer model, which leverages a unique combination of frequency-domain Sequence Progressive Split-Aggregation (SPSA) and Bidirectional Splitting-Agg Attention (BSAA) mechanisms. The SPSA module decomposes sequences into seasonal and trend components, enhancing the model’s ability to identify cyclical patterns, while the BSAA mechanism captures forward and backward dependencies, providing a comprehensive understanding of temporal dynamics. Extensive experiments conducted on seven benchmark datasets demonstrate the BSAformer model&#39;s superior performance, with notable improvements in accuracy and efficiency over state-of-the-art models. Specifically, the BSAformer achieves significant Mean Squared Error (MSE) reductions of 63.7% on the ECL dataset, 28.1% on the Traffic dataset, and 49.8% on the ILI dataset. These results validate the model’s robustness and its adaptability across diverse time series forecasting scenarios. The insights gained from this study contribute to the advancement of time series forecasting by providing a model that improves both accuracy and computational efficiency, especially in handling long-term dependencies and complex temporal patterns.},
  archive      = {J_CIS},
  author       = {Zhu, QingBo and Han, JiaLin and Yang, Sheng and Xie, ZhiQiang and Tian, Bo and Wan, HaiBo and Chai, Kai},
  doi          = {10.1007/s40747-025-01794-z},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Complex Intell. Syst.},
  title        = {BSAformer: Bidirectional sequence splitting aggregation attention mechanism for long term series forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of weighted based divided-search enhanced
karnik–mendel algorithms for type reduction of general type-2 fuzzy
logic systems. <em>CIS</em>, <em>11</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s40747-025-01798-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General type-2 fuzzy logic systems (GT2 FLSs) based on the $$\alpha$$ -planes representation of general T2 fuzzy sets (FSs) have become more accessible to FL investigators in recent years. Type reduction (TR) is the most important block for GT2 FLSs. Here the weighted type-reduction algorithms based on the Newton and Cotes quadrature formulas of numerical methods of integration technique are first given, and the searching spaces are divided. Then a type of weighted divided search enhanced Karnik–Mendel (WDEKM) algorithms is shown to complete the centroid TR. In contrast to the WEKM algorithms, four simulation instances show that the WDEKM algorithms get lesser absolute errors and faster calculational speeds, which may offer the potentially application values for applying T2 FLSs.},
  archive      = {J_CIS},
  author       = {Chen, Yang},
  doi          = {10.1007/s40747-025-01798-9},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Design of weighted based divided-search enhanced Karnik–Mendel algorithms for type reduction of general type-2 fuzzy logic systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADWTune: An adaptive dynamic workload tuning system with
deep reinforcement learning. <em>CIS</em>, <em>11</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s40747-025-01801-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to reduce the burden of DBA, the knob tuning method based on reinforcement learning has been proposed and achieved good results in some cases. However, the performance of these solutions is not ideal as the workload features are not considered enough. To address these issues, we propose a database tuning system called ADWTune. In this model, ADWTune employs the idea of multiple sampling to gather workload data at different time points during the observation period. ADWTune uses these continuous data slices to characterize the dynamic changes in the workload. The key of ADWTune is its adaptive workload handling approach, which combines the dynamic features of workloads and the internal metrics of database as the state of the environment. At the same time, ADWTune includes a data repository, which reuses historical data to improve the adaptability of model to workload shifts. We conduct extensive experiments on various workloads. The experimental results demonstrate that ADWTune is better suited for dynamic environments than other methods based on reinforcement learning.},
  archive      = {J_CIS},
  author       = {Li, Cuixia and Wang, Junhai and Shi, Jiahao and Liu, Liqiang and Zhang, Shuyan},
  doi          = {10.1007/s40747-025-01801-3},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Complex Intell. Syst.},
  title        = {ADWTune: An adaptive dynamic workload tuning system with deep reinforcement learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). <span class="math display">H<sup>2</sup>CAN</span>:
Heterogeneous hypergraph attention network with counterfactual learning
for multimodal sentiment analysis. <em>CIS</em>, <em>11</em>(4), 1–16.
(<a href="https://doi.org/10.1007/s40747-025-01806-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) has garnered significant attention for its immense potential in human-computer interaction. While cross-modality attention mechanisms are widely used in MSA to capture inter-modality interactions, existing methods are limited to pairwise interactions between two modalities. Additionally, these methods can not utilize the causal relationship to guide attention learning, making them susceptible to bias information. To address these limitations, we introduce a novel method called Heterogeneous Hypergraph Attention Network with Counterfactual Learning $$(\text {H}^2\text {CAN}).$$ The method constructs a heterogeneous hypergraph based on sentiment expression characteristics and employs Heterogeneous Hypergraph Attention Networks (HHGAT) to capture interactions beyond pairwise constraints. Furthermore, it mitigates the effects of bias through a Counterfactual Intervention Task (CIT). Our model comprises two main branches: hypergraph fusion and counterfactual fusion. The former uses HHGAT to capture inter-modality interactions, while the latter constructs a counterfactual world using Gaussian distribution and additional weighting for the biased modality. The CIT leverages causal inference to maximize the prediction discrepancy between the two branches, guiding attention learning in the hypergraph fusion branch. We utilize unimodal labels to help the model adaptively identify the biased modality, thereby enhancing the handling of bias information. Experiments on three mainstream datasets demonstrate that $$\text {H}^2\text {CAN}$$ sets a new benchmark.},
  archive      = {J_CIS},
  author       = {Huang, Changqin and Lin, Zhenheng and Huang, Qionghao and Huang, Xiaodi and Jiang, Fan and Chen, Jili},
  doi          = {10.1007/s40747-025-01806-y},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {$$\text {H}^2\text {CAN}$$: Heterogeneous hypergraph attention network with counterfactual learning for multimodal sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal multilevel attention for semi-supervised
skeleton-based gesture recognition. <em>CIS</em>, <em>11</em>(4), 1–16.
(<a href="https://doi.org/10.1007/s40747-025-01807-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although skeleton-based gesture recognition using supervised learning has achieved promising results, the reliance on extensive annotated data poses significant costs. This paper addresses the challenge of semi-supervised skeleton-based gesture recognition, to effectively learn feature representations from labeled and unlabeled data. To resolve this problem, we propose a novel multimodal multilevel attention network designed for semi-supervised learning. This model utilizes the self-attention mechanism to polymerize multimodal and multilevel complementary semantic information of the hand skeleton, designing a multimodal multilevel contrastive loss to measure feature similarity. Specifically, our method explores the relationships between joint, bone, and motion to learn more discriminative feature representations. Considering the hierarchy of the hand skeleton, the skeleton data is divided into multilevel to capture complementary semantic information. Furthermore, the multimodal contrastive loss measures similarity among these multilevel representations. The proposed method demonstrates improved performance in semi-supervised skeleton-based gesture recognition tasks, as evidenced by experiments on the SHREC-17 and DHG 14/28 datasets.},
  archive      = {J_CIS},
  author       = {Liu, Jinting and Gan, Minggang and He, Yuxuan and Guo, Jia and Hu, Kang},
  doi          = {10.1007/s40747-025-01807-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {Multimodal multilevel attention for semi-supervised skeleton-based gesture recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metalinguist: Enhancing hate speech detection with
cross-lingual meta-learning. <em>CIS</em>, <em>11</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s40747-025-01808-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of social media has led to an increase in hate speech. Hate speech is generally described as a deliberate act of aggression aimed at a particular group, intended to harm or marginalize them based on specific attributes of their identity. While positive interactions in diverse communities can greatly enhance confidence, it is important to acknowledge that negative remarks such as hate speech can weaken community unity and present a significant impact on people’s well-being. This highlights the need for improved monitoring and guidelines on social media platforms to protect individuals from discriminatory and harmful actions. Despite extensive research on resource-rich languages, such as English and German, the detection and analysis of hate speech in less-resourced languages, such as Norwegian, remains underexplored. Addressing this gap, our study leverages a metalinguistic approach that uses advanced meta-learning techniques to enhance the detection capabilities across bilingual texts, effectively linking technical advancements directly to the pressing social issue of hate speech. In this study, we introduce techniques that adapt models that deal with hate speech detection within the same languages (intra-lingual), across different languages (cross-lingual), and techniques that adapt models to new languages with minimal extra training, independent of the model type (cross-lingual model-agnostic meta-learning-based approaches) for bilingual text analysis in Norwegian and English. Our methodology incorporates attention mechanisms (components that help the model focus on relevant parts of the text) and adaptive learning rate schedulers (tools that adjust the learning speed based on performance). Our methodology incorporates components that help the model focus on relevant parts of the text (attention mechanisms) and tools that adjust the learning speed based on performance (adaptive learning rate schedulers). We conducted various experiments using language-specific and multilingual transformers. Among these, the combination of Nor-BERT and LSTM with zero-shot and few-shot model-agnostic meta-learning achieved remarkable F1 scores of 79% and 90%, highlighting the effectiveness of our proposed framework.},
  archive      = {J_CIS},
  author       = {Hashmi, Ehtesham and Yayilgan, Sule Yildirim and Abomhara, Mohamed},
  doi          = {10.1007/s40747-025-01808-w},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Complex Intell. Syst.},
  title        = {Metalinguist: Enhancing hate speech detection with cross-lingual meta-learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pattern mining-based evolutionary multi-objective algorithm
for beam angle optimization in intensity-modulated radiotherapy.
<em>CIS</em>, <em>11</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s40747-025-01809-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multi-objective algorithms have been applied to beam angle optimization (called BAO) for generating diverse trade-off radiotherapy treatment plans. However, their performance is not so effective due to the ignorance of using the specific clinical knowledge that can be obtain intuitively by clinical physicist. To address this issue, we suggest a pattern mining based evolutionary multi-objective algorithm called PM-EMA, in which two strategies for using the knowledge are proposed to accelerate the speed of population convergence. Firstly, to discover the potential beam angle distribution and discard the worse angles, the pattern mining strategy is used to detect the maximum and minimum sets of beam angles in non-dominated solutions of the population and utilize them to generate offspring to enhance the convergence. Moreover, to improve the quality of initial solutions, a tailored population initialization strategy is proposed by using the score of beam angles defined by this study. The experimental results on six clinical cancer cases demonstrate the superior performance of the proposed algorithm over six representative algorithms.},
  archive      = {J_CIS},
  author       = {Cao, Ruifen and Chen, Wei and Zhang, Tielu and Si, Langchun and Pei, Xi and Zhang, Xingyi},
  doi          = {10.1007/s40747-025-01809-9},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Complex Intell. Syst.},
  title        = {Pattern mining-based evolutionary multi-objective algorithm for beam angle optimization in intensity-modulated radiotherapy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact particle flow daum-huang filters for mobile robot
localization in occupancy grid maps. <em>CIS</em>, <em>11</em>(4), 1–15.
(<a href="https://doi.org/10.1007/s40747-025-01810-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel localization algorithm for mobile robots navigating in complex planar environments, a critical capability for various real-world applications such as autonomous driving, robotic assistance, and industrial automation. Although traditional methods such as particle filters and extended Kalman filters have been widely used, there is still room for assessing the capabilities of modern filtering techniques for this task. Building on a recent localization method that employs a chamfer distance-based observation model, derived from an implicit measurement equation, we explore its potential further by incorporating exact particle flow Daum–Huang filters to achieve superior accuracy. Recent advancements have spotlighted Daum–Huang filters as formidable contenders, outshining both the extended Kalman filters and traditional particle filters in various scenarios. We introduce two new Daum–Huang-based localization algorithms and assess their tracking performance through comprehensive simulations and real-world trials. Our algorithms are benchmarked against various methods, including the widely acclaimed Adaptive Monte–Carlo Localization algorithm. Overall, our algorithm demonstrates superior performance compared to the baseline models in simulations and exhibits competitive performance in the evaluated real-world application.},
  archive      = {J_CIS},
  author       = {Csuzdi, Domonkos and Bécsi, Tamás and Gáspár, Péter and Törő, Olivér},
  doi          = {10.1007/s40747-025-01810-2},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Complex Intell. Syst.},
  title        = {Exact particle flow daum-huang filters for mobile robot localization in occupancy grid maps},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal decoupling attention transformer for 3D
skeleton-based driver action recognition. <em>CIS</em>, <em>11</em>(4),
1–12. (<a href="https://doi.org/10.1007/s40747-025-01811-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driver action recognition is crucial for in-vehicle safety. We argue that the following factors limit the related research. First, spatial constraints and obstructions in the vehicle restrict the range of motion, resulting in similar action patterns and difficulty collecting the full body posture. Second, in skeleton-based action recognition, establishing the joint dependencies by the self-attention computation is always limited to a single frame, ignoring the effect of body spatial structure on dependence weights and inter-frame. Common convolution in temporal flow only focuses on frame-level temporal features, ignoring motion pattern features at a higher semantic level. Our work proposed a novel spatiotemporal decoupling attention transformer (SDA-TR). The SDA module uses a spatiotemporal decoupling strategy to decouple the weight computation according to body structure and directly establish joint dependencies between multiple frames. The TFA module aggregates sub-action-level and frame-level temporal features to improve similar recognition accuracy. On the Driver Action Recognition dataset Drive&amp;Act using driver upper body skeletons, SDA-TR achieves state-of-the-art performance. SDA-TR also achieved 92.2%/95.8% accuracy under the CS/CV benchmarks of NTU RGB+D 60, 88.6%/89.8% accuracy under the CS/CSet benchmarks of NTU RGB+D 120, on par with other state-of-the-art methods. Our method demonstrates great scalability and generalization for action recognition.},
  archive      = {J_CIS},
  author       = {Xu, Zhuoyan and Xu, Jingke},
  doi          = {10.1007/s40747-025-01811-1},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Complex Intell. Syst.},
  title        = {Spatiotemporal decoupling attention transformer for 3D skeleton-based driver action recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised random walk manifold contrastive hashing for
multimedia retrieval. <em>CIS</em>, <em>11</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s40747-025-01814-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth in both the variety and volume of data on networks, especially within social networks containing vast multimedia data such as text, images, and video, there is an urgent need for efficient methods to retrieve helpful information quickly. Due to their high computational efficiency and low storage costs, unsupervised deep cross-modal hashing methods have become the primary method for managing large-scale multimedia data. However, existing unsupervised deep cross-modal hashing methods still need help with issues such as inaccurate measurement of semantic similarity information, complex network architectures, and incomplete constraints among multimedia data. To address these issues, we propose an Unsupervised Random Walk Manifold Contrastive Hashing (URWMCH) method, designing a simple deep learning architecture. First, we build a random walk-based manifold similarity matrix based on the random walk strategy and modal-individual similarity structure. Second, we construct intra- and inter-modal similarity preservation and coexistent similarity preservation loss based on contrastive learning to constrain the training of hash functions, ensuring that the hash codes contain complete semantic association information. Finally, we designed comprehensive experiments on the MIRFlickr-25K, NUS-WIDE, and MS COCO datasets to demonstrate the effectiveness and superiority of the proposed URWMCH method.},
  archive      = {J_CIS},
  author       = {Chen, Yunfei and Long, Yitian and Yang, Zhan and Long, Jun},
  doi          = {10.1007/s40747-025-01814-y},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Unsupervised random walk manifold contrastive hashing for multimedia retrieval},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KeyBoxGAN: Enhancing 2D object detection through annotated
and editable image synthesis. <em>CIS</em>, <em>11</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s40747-025-01817-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample augmentation, especially sample generation is conducive for addressing the challenge of training robust image and video object detection models based on the deep learning. Still, the existing methods lack sample editing capability and suffer from annotation work. This paper proposes an image sample generation method based on key box points detection and Generative adversarial network (GAN), named as KeyBoxGAN, to make image sample generation labeled and editable. KeyBoxGAN firstly predefines key box points positions, embeddings which control the objects’ positions and then the corresponding masks are generated according to Mahalanobis–Gaussuan heatmaps and Swin Transformer-SPADE generator to control objects’ generation regions, as well as the background generation. This adaptive and precisely supervised image generation method disentangles object position and appearance, enables image editable and self-labeled abilities. The experiments show KeyBoxGAN surpasses DCGAN, StyleGAN2 and DDPM in objective assessments, including Inception Distance (FID), Inception Score (IS), and Multi-Scale Structural Similarity Index (MS-SSIM), as well as in subjective evaluations by showing better visual quality. Moreover, the editable and self-labeled image generation capabilities make it a valuable tool in addressing challenges like occlusion, deformation, and varying environmental conditions in the 2D object detection.},
  archive      = {J_CIS},
  author       = {Bai, Yashuo and Song, Yong and Dong, Fei and Li, Xu and Zhou, Ya and Liao, Yizhao and Huang, Jinxiang and Yang, Xin},
  doi          = {10.1007/s40747-025-01817-9},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Complex Intell. Syst.},
  title        = {KeyBoxGAN: Enhancing 2D object detection through annotated and editable image synthesis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel robust multi-objective evolutionary optimization
algorithm based on surviving rate. <em>CIS</em>, <em>11</em>(4), 1–25.
(<a href="https://doi.org/10.1007/s40747-025-01822-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary optimization is widely utilized in industrial design. Despite the success of multi-objective evolutionary optimization algorithms in addressing complex optimization problems, research focusing on input disturbances remains limited. In many manufacturing processes, design parameters are vulnerable to random input disturbances, resulting in products that often perform less effectively than anticipated. To address this issue, we propose a novel robust multi-objective evolutionary optimization algorithm based on the concept of survival rate. The algorithm comprises two stages: the evolutionary optimization stage and the construction stage of the robust optimal front. In the former stage, we introduce the survival rate as a new optimization objective. Subsequently, we seek a robust optimal front that concurrently addresses convergence and robustness by employing a non-dominated sorting approach. Furthermore, we propose a precise sampling method and a random grouping mechanism to accurately recover solutions resilient to real noise while ensuring population’s diversity. In the latter stage, we introduce a performance measure that integrates both robustness and convergence to guide the construction of the robust optimal front. Experimental results demonstrate the superiority of the proposed algorithm in terms of both convergence and robustness compared to existing approaches under noisy conditions.},
  archive      = {J_CIS},
  author       = {Jiang, Wenxiang and Gao, Kai and Zhu, Shuwei and Xu, Lihong},
  doi          = {10.1007/s40747-025-01822-y},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Complex Intell. Syst.},
  title        = {A novel robust multi-objective evolutionary optimization algorithm based on surviving rate},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust-aware privacy-preserving QoS prediction with graph
neural collaborative filtering for internet of things services.
<em>CIS</em>, <em>11</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s40747-025-01824-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The booming development of the Internet of Things (IoT) has led to an explosion of web services, making it more inconvenient for users to choose satisfactory services among numerous options. Therefore, ensuring quality of service (QoS) in a service-oriented IoT environment is crucial, highlighting QoS prediction as a prominent research focus. However, issues related to information credibility, user data privacy, and prediction accuracy in QoS prediction for IoT services have become significant challenges in current research. To tackle these issues, we propose TPP-GNCF, a trust-aware privacy-preserving QoS prediction framework that integrates graph neural networks with collaborative filtering methods. In TPP-GNCF, we filter out untrustworthy QoS values provided by users for certain services to select credible QoS values. Then, a message-passing graph neural network (MP-GNN) is utilized to effectively capture information transmission and relationships in the graph structure, while differential privacy is used to protect user node information. In addition, we use a similarity calculation method based on weight function in collaborative filtering to mine implicit embedded features that graph neural networks cannot directly utilize. Finally, the final missing QoS values are achieved by fusing graph neural predicted QoS and feature collaborative filtering predicted QoS. We conducted extensive experiments on the well-known WS-DREAM dataset. The results demonstrate that the TPP-GNCF framework not only surpasses existing schemes in performance but also effectively addresses issues of information credibility and user privacy.},
  archive      = {J_CIS},
  author       = {Wang, Weiwei and Ma, Wenping and Yan, Kun},
  doi          = {10.1007/s40747-025-01824-w},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {Trust-aware privacy-preserving QoS prediction with graph neural collaborative filtering for internet of things services},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small sample smart contract vulnerability detection method
based on multi-layer feature fusion. <em>CIS</em>, <em>11</em>(4), 1–26.
(<a href="https://doi.org/10.1007/s40747-025-01782-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of vulnerabilities in smart contracts is necessary for ensuring their security. As a pre-trained language model, BERT has been employed in the detection of smart contract vulnerabilities, exhibiting high accuracy in tasks. However, it has certain limitations. Existing methods solely depend on features extracted from the final layer, thereby disregarding the potential contribution of features from other layers. To address these issues, this paper proposes a novel method, which is named multi-layer feature fusion (MULF). Experiments investigate the impact of utilizing features from other layers on performance improvement. To the best of our knowledge, this is the first instance of multi-layer feature sequence fusion in the field of smart contract vulnerability detection. Furthermore, there is a special type of patched contract code that contains vulnerability features which need to be studied. Therefore, to overcome the challenges posed by limited smart contract vulnerability datasets and high false positive rates, we introduce a data augmentation technique that incorporates function feature screening with those special smart contracts into the training set. To date, this method has not been reported in the literature. The experimental results demonstrate that the MULF model significantly enhances the performance of smart contract vulnerability identification compared to other models. The MULF model achieved accuracies of 98.95% for reentrancy vulnerabilities, 96.27% for timestamp dependency vulnerabilities, and 87.40% for overflow vulnerabilities, which are significantly higher than those achieved by existing methods.},
  archive      = {J_CIS},
  author       = {Fan, Jinlin and He, Yaqiong and Wu, Huaiguang},
  doi          = {10.1007/s40747-025-01782-3},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Complex Intell. Syst.},
  title        = {Small sample smart contract vulnerability detection method based on multi-layer feature fusion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CycleGuardian: A framework for automatic respiratory sound
classification based on improved deep clustering and contrastive
learning. <em>CIS</em>, <em>11</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s40747-025-01800-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auscultation plays a pivotal role in early respiratory and pulmonary disease diagnosis. Despite the emergence of deep learning-based methods for automatic respiratory sound classification post-Covid-19, limited datasets impede performance enhancement. Distinguishing between normal and abnormal respiratory sounds poses challenges due to the coexistence of normal respiratory components and noise components in both types. Moreover, different abnormal respiratory sounds exhibit similar anomalous features, hindering their differentiation. Besides, existing state-of-the-art models suffer from excessive parameter size, impeding deployment on resource-constrained mobile platforms. To address these issues, we design a lightweight network CycleGuardian and propose a framework based on an improved deep clustering and contrastive learning. We first generate a hybrid spectrogram for feature diversity and grouping spectrograms to facilitate intermittent abnormal sound capture. Then, CycleGuardian integrates a deep clustering module with a similarity-constrained clustering component to improve the ability to capture abnormal features and a contrastive learning module with group mixing for enhanced abnormal feature discernment. Multi-objective optimization enhances overall performance during training. In experiments, we use the ICBHI2017 dataset, following the official split method and without any pre-trained weights, our method achieves Sp: 82.06 $$\%$$ , Se: 44.47 $$\%$$ , and Score: 63.26 $$\%$$ with a network model size of 38 M. Compared to the current model, our method leads by nearly 7 $$\%$$ , achieving the current best performances. Additionally, we deploy the network on Android devices, showcasing a comprehensive intelligent respiratory sound auscultation system.},
  archive      = {J_CIS},
  author       = {Chu, Yun and Wang, Qiuhao and Zhou, Enze and Fu, Ling and Liu, Qian and Zheng, Gang},
  doi          = {10.1007/s40747-025-01800-4},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Complex Intell. Syst.},
  title        = {CycleGuardian: A framework for automatic respiratory sound classification based on improved deep clustering and contrastive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized non-convex online optimization with adaptive
momentum estimation and quantized communication. <em>CIS</em>,
<em>11</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s40747-025-01818-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider the decentralized non-convex online optimization problem over an undirected network. To solve the problem over a communication-efficient manner, we propose a novel quantized decentralized adaptive momentum gradient descent algorithm based on the adaptive momentum estimation methods, where quantified information is exchanged between agents. The proposed algorithm not only can effectively reduce the data transmission volume but also contribute to improved convergence. Theoretical analysis proves that the proposed algorithm can achieve sublinear dynamic regret under appropriate step-size and quantization level, which matches the convergence of the decentralized online algorithm with exact-communication. Extensive simulations are given to demonstrate the efficacy of the algorithm.},
  archive      = {J_CIS},
  author       = {Lv, Yunshan and Xiong, Hailing and Zhang, Fuqing and Dong, Shengying and Dai, Xiangguang},
  doi          = {10.1007/s40747-025-01818-8},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {Decentralized non-convex online optimization with adaptive momentum estimation and quantized communication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A local search with chain search path strategy for
real-world many-objective vehicle routing problem. <em>CIS</em>,
<em>11</em>(4), 1–32. (<a
href="https://doi.org/10.1007/s40747-025-01825-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on a new application-oriented variant of vehicle routing problem. This problem comes from the daily distribution scenarios of a real-world logistics company. It is a large-scale (with customer sizes up to 2000), many-objective (with six objective functions) NP-hard problem with six constraints. Then, a local search with chain search path strategy (LS-CSP) is proposed to effectively solve the problem. It is a decomposition-based algorithm. First, the considered problem is decomposed into multiple single-objective subproblems. Then, local search is applied to solve these subproblems one by one. The advantage of the LS-CSP lies in a chain search path strategy, which is designed for determining the order of solving the subproblems. This strategy can help the algorithm find a high-quality solution set quickly. Finally, to assess the performance of the proposed LS-CSP, three instance sets containing 132 instances are provided, and four state-of-the-art decomposition-based approaches are adopted as the competitors. Experimental results show the effectiveness of the proposed algorithm for the considered problem.},
  archive      = {J_CIS},
  author       = {Zhou, Ying and Kong, Lingjing and Wang, Hui and Cai, Yiqiao and Liu, Shaopeng},
  doi          = {10.1007/s40747-025-01825-9},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Complex Intell. Syst.},
  title        = {A local search with chain search path strategy for real-world many-objective vehicle routing problem},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled pixel-wise correction for abdominal multi-organ
segmentation. <em>CIS</em>, <em>11</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s40747-025-01796-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism has emerged as a crucial component in medical image segmentation. Attention-based deep neural networks (ADNNs) fundamentally engage in the iterative computation of gradients for both input layers and weight parameters. Our research reveals a remarkable similarity between the optimization trajectory of ADNN and non-negative matrix factorization (NMF), where the latter involves the alternate adjustment of the base and coefficient matrices. This similarity implies that the alternating optimization strategy—characterized by the adjustment of input features by the attention mechanism and the adjustment of network weights—is central to the efficacy of attention mechanisms in ADNNs. Drawing an analogy to the NMF approach, we advocate for a pixel-wise adjustment of the input layer within ADNNs. Furthermore, to reduce the computational burden, we have developed a decoupled pixel-wise attention module (DPAM) and a self-attention module (DPSM). These modules are designed to counteract the challenges posed by the high inter-class similarity among different organs when performing multi-organ segmentation. The integration of our DPAM and DPSM into traditional network architectures facilitates the creation of an NMF-inspired ADNN framework, known as the DPC-Net, which comes in two variants: DPCA-Net for attention and DPCS-Net for self-attention. Our extensive experiments on the Synapse and FLARE22 datasets demonstrate that the DPC-Net achieves satisfactory performance and visualization results with lower computational cost. Specifically, the DPC-Net achieved a Dice score of 77.98% on the Synapse dataset and 87.04% on the FLARE22 dataset, while possessing merely 14.991 million parameters. Notably, our findings indicate that DPC-Net, when equipped with convolutional attention, surpasses those networks utilizing Transformer attention mechanisms on multi-organ segmentation tasks. Our code is available at https://github.com/605671435/DPC-Net .},
  archive      = {J_CIS},
  author       = {Yu, Xiangchun and Ding, Longjun and Zhang, Dingwen and Wu, Jianqing and Liang, Miaomiao and Zheng, Jian and Pang, Wei},
  doi          = {10.1007/s40747-025-01796-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {Decoupled pixel-wise correction for abdominal multi-organ segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing geometric modeling in convolutional neural
networks: Limit deformable convolution. <em>CIS</em>, <em>11</em>(4),
1–14. (<a href="https://doi.org/10.1007/s40747-025-01799-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are constrained in their capacity to model geometric transformations due to their fixed geometric structure. To overcome this problem, researchers introduce deformable convolution, which allows the convolution kernel to be deformable on the feature map. However, deformable convolution may introduce irrelevant contextual information during the learning process and thus affect the model performance. DCNv2 introduces a modulation mechanism to control the diffusion of the sampling points to control the degree of contribution of offsets through weights, but we find that such problems still exist in practical use. Therefore, we propose a new limit deformable convolution to address this problem, which enhances the model ability by adding adaptive limiting units to constrain the offsets and adjusts the weight constraints on the offsets to enhance the image-focusing ability. In the subsequent work, we perform lightweight work on the limit deformable convolution and design three kinds of LDBottleneck to adapt to different scenarios. The limit deformable network, equipped with the optimal LDBottleneck, demonstrated an improvement in mAP75 of 1.4% compared to DCNv1 and 1.1% compared to DCNv2 on the VOC2012+2007 dataset. Furthermore, on the CoCo2017 dataset, different backbones equipped with our limit deformable module achieved satisfactory results. The source code for this work is publicly available at https://github.com/1977245719/LDCN.},
  archive      = {J_CIS},
  author       = {Wang, Wei and Meng, Yuanze and Li, Han and Chang, Guiyong and Li, Shun and Zhang, Chenghong},
  doi          = {10.1007/s40747-025-01799-8},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Enhancing geometric modeling in convolutional neural networks: Limit deformable convolution},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive transplanting of black-box adversarial
attacks from multi-class to multi-label models. <em>CIS</em>,
<em>11</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s40747-025-01805-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples generated by perturbing raw data with carefully designed, imperceptible noise have emerged as a primary security threat to artificial intelligence systems. In particular, black-box adversarial attack algorithms, which only rely on model input and output to generate adversarial examples, are easy to implement in real scenarios. However, previous research on black-box attacks has primarily focused on multi-class classification models, with relatively few studies on black-box attack algorithms for multi-label classification models. Multi-label classification models exhibit significant differences from multi-class classification models in terms of structure and output. The former can assign multiple labels to a single sample, with these labels often exhibiting correlations, while the latter classifies a sample as the class with the highest confidence. Therefore, existing multi-class attack algorithms cannot directly attack multi-label classification models. In this paper, we study the transplantation methods of multi-class black-box attack algorithms to multi-label classification models and propose the multi-label versions for eight classic black-box attack algorithms, which include three score-based attacks and five decision-based (label-only) attacks, for the first time. Experimental results indicate that the transplanted black-box attack algorithms demonstrate effective attack performance across various attack types, except for extreme attacks. Especially, most transplanted attack algorithms achieve more than 60% success rate on the ML-GCN model and more than 30% on the ML-LIW model under the hiding all attack type. However, the performance of these transplanted attack algorithms shows variation among different attack types due to the correlations between labels.},
  archive      = {J_CIS},
  author       = {Chen, Zhijian and Zhou, Qi and Liu, Yujiang and Luo, Wenjian},
  doi          = {10.1007/s40747-025-01805-z},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Complex Intell. Syst.},
  title        = {A comprehensive transplanting of black-box adversarial attacks from multi-class to multi-label models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AILDP: A research on ship number recognition technology for
complex scenarios. <em>CIS</em>, <em>11</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s40747-025-01820-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of global maritime trade and the increasingly urgent need for maritime surveillance and security management, fast and accurate identification of vessels has become a crucial aspect. The task of ship number recognition mainly faces two challenges: first, the ship number is usually located in different parts of the hull, and due to the shooting distance, the size of the ship number can vary greatly on different vessels, making automated recognition complex. Second, adverse weather conditions and complex sea surface environments may affect the accuracy of visual recognition. To address the above issues, we produce a private dataset containing 2436 images of ships in a variety of scenarios and propose an algorithm (AILDP) for interactive feature learning and adaptive enhancement to tackle multiple challenges in ship number recognition. Firstly, in the detection phase, for the problem of varying size and position in the ship number recognition task, the detection effect is optimized by a module (AIFI_LPE) that combines feature interaction and learned position encoding. Secondly, to deal with the issues of blurring and occlusion of ship numbers due to ship movement or bad weather, a module (C2f_IRMB_DRB) is proposed that can capture high-quality features while weighing the computational effort when processing low-quality images. After detection, the results are divided into two categories: clear ship number and low-quality ship number. In order to save computational resources, only the low-quality images are first subjected to preliminary image enhancement processing, and then the Thin Plate Spline (TPS) is introduced in the recognition part based on the framework of PaddleOCRv4 and combined with the feature extraction and enhancement module to adjust the spatial features of the images to ensure that both types of ship number images can be accurately processed in the feature extraction and recognition process. Experimental results show that the AILDP can improve the accuracy of ship number recognition, with the precision, recall, and mAP0.5 for ship number detection increased to 95.7%, 94.5%, and 94.8%. The Character_accuracy of the recognition task can reach 95.23%.},
  archive      = {J_CIS},
  author       = {Wei, Tianjiao and Hu, Zhuhua and Zhao, Yaochi and Fan, Xiyu},
  doi          = {10.1007/s40747-025-01820-0},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Complex Intell. Syst.},
  title        = {AILDP: A research on ship number recognition technology for complex scenarios},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mamba meets tracker: Exploiting token aggregation and
diffusion for robust unmanned aerial vehicles tracking. <em>CIS</em>,
<em>11</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s40747-025-01821-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Transformer-based tracking approach achieves excellent results in unmanned aerial vehicles (UAV) tracking tasks. However, the existing tracking framework usually deals with this problem by visual grounding and visual tracking separately. This independent framework does not consider the correlation between the two steps mentioned above, that is, natural language description can provide global semantic information. Meanwhile, a separate framework is unable to conduct end-to-end training. As a remedy, We propose a joint natural language Mamba based tracking framework (named TADMT). Specifically, we propose a token aggregator that condenses rich features into a small number of visual tokens through a coarse to fine strategy to improve subsequent tracking speed. Then, we designed a mamba module based on the serpentine scanning strategy to effectively establish the relationship between natural language and visual images. In addition, we have designed a novel shift add multilayer perceptron in the prediction head, with the aim of achieving final classification and localization with less computation. Numerous experimental results have shown that TADMT achieves good tracking performance on six UAV tracking datasets and three general tracking datasets, with an average speed of 120FPS. The experimental results on the embedded platform also demonstrate the applicability of TADMT on UAV platforms.},
  archive      = {J_CIS},
  author       = {Du, Guocai and Zhou, Peiyong and Yadikar, Nurbiya and Aysa, Alimjan and Ubul, Kurban},
  doi          = {10.1007/s40747-025-01821-z},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {Mamba meets tracker: Exploiting token aggregation and diffusion for robust unmanned aerial vehicles tracking},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling object mask interaction for compositional action
recognition. <em>CIS</em>, <em>11</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s40747-025-01823-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human actions can be abstracted as interactions between humans and objects. The recently proposed task of compositional action recognition emphasizes the independence and combinability of verbs (actions) and nouns (humans or objects) constituting human actions. Nonetheless, most traditional appearance-based action recognition methods usually extract spatial-temporal features from input videos concurrently to understand actions. This approach tends to excessively rely on overall appearance features and lacks precise modelling of interactions between objects, often leading to the neglect of the actions themselves. Consequently, the biases introduced by the appearance prevent the model from effectively generalizing to unseen combinations of actions and objects. To address this issue, we propose a method that explicitly models the object interaction path, aiming to capture interactions between humans and objects. The advantage of this approach is that these interactions are not affected by the object or environmental appearance bias, providing additional clues for appearance-based action recognition methods. Our method can easily be combined with any appearance-based visual encoder, significantly improving the compositional generalization ability of action recognition algorithms. Extensive experimental results on the Something-Else dataset and the IKEA-Assembly dataset demonstrate the effectiveness of our approach.},
  archive      = {J_CIS},
  author       = {Li, Xinya and Shen, Zhongwei and Xu, Benlian and Li, Rongchang and Lu, Mingli and Cong, Jinliang and Zhang, Longxin},
  doi          = {10.1007/s40747-025-01823-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Modelling object mask interaction for compositional action recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Target pursuit for multi-AUV system: Zero-sum stochastic
game with WoLF-PHC assisted. <em>CIS</em>, <em>11</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s40747-025-01788-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of the underwater environment and the difficulty of the underwater energy recharging, utilizing multiple autonomous underwater vehicles (AUVs) to pursue the invading vehicle is a challenging project. This paper focuses on devising the rational and energy-efficient pursuit motion for a multi-AUV system in an unknown three-dimensional environment. Firstly, the pursuit system model is constructed on the two-player zero-sum stochastic game (ZSSG) framework. This framework enables the fictitious play on the behaviors of the invading AUV. Fictitious play involves players updating their strategies by observing and inferring the actions of others under incomplete information. Under this framework, a relay-pursuit mechanism is adopted by the pursuit system to form the action set in an energy-efficient way. Then, to reflect the pursuit goals of capturing the invading vehicle as soon as possible and avoid it from reaching its point of attack, two corresponding pursuit factors are considered in the designed reward function. To enable the pursuit AUVs to navigate in an unknown environment, WoLF-PHC algorithm is introduced and applied to the proposed ZSSG-based model. Finally, simulations demonstrate the effectiveness, the advantages, and the robustness of the proposed approach.},
  archive      = {J_CIS},
  author       = {Hong, Le and Cui, Weicheng},
  doi          = {10.1007/s40747-025-01788-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {Target pursuit for multi-AUV system: Zero-sum stochastic game with WoLF-PHC assisted},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bi-objective optimization approach for scheduling electric
ground-handling vehicles in an airport. <em>CIS</em>, <em>11</em>(4),
1–27. (<a href="https://doi.org/10.1007/s40747-025-01815-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce airport operating costs and minimize environmental pollution, converting ground-handling vehicles from fuel-powered to electric ones is inevitable. However, this transformation introduces complexity in scheduling due to additional factors, such as battery capacities and charging requirements. This study models the electric ground-handling vehicle scheduling problem as a bi-objective integer programming model to address these challenges. The objectives of this model are to minimize the total travel distance of vehicles serving flights and the standard deviation of the total occupancy time for each vehicle. In order to solve this model and generate optimal scheduling solutions, this study combines the non-dominated sorting genetic algorithm 2 (NSGA2) with the large neighborhood search (LNS) algorithm, proposing a novel NSGA2-LNS algorithm. A dynamic priority method is used by the NSGA2-LNS to construct the initial population, thereby speeding up the convergence. The NSGA2-LNS employs the LNS algorithm to overcome the problem that metaheuristic algorithms often lack clear directions in the process of finding solutions. In addition, this study designs the correlation-based destruction operator and the priority-based repair operator in the NSGA2-LNS algorithm, thereby significantly enhancing its ability to find optimal solutions for the electric ground-handling vehicle scheduling problem. The algorithm is verified using flight data from Chengdu Shuangliu International Airport and is compared with manual scheduling methods and traditional multi-objective optimization algorithms. Experimental results demonstrate that the NSGA2-LNS can rapidly solve the scheduling problem of allocating electric ground-handling vehicles for hundreds of flights and produce high-quality scheduling solutions.},
  archive      = {J_CIS},
  author       = {Fu, Weigang and Li, Jiawei and Liao, Zhe and Fu, Yaoming},
  doi          = {10.1007/s40747-025-01815-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Complex Intell. Syst.},
  title        = {A bi-objective optimization approach for scheduling electric ground-handling vehicles in an airport},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discriminator guided visible-to-infrared image translation.
<em>CIS</em>, <em>11</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s40747-025-01827-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a discriminator-guided visible-to-infrared image translation algorithm based on a generative adversarial network and designs a multi-scale fusion generative network. The generative network enhances the perception of the image’s fine-grained features by fusing features of different scales in the channel direction. Meanwhile, the discriminator performs the infrared image reconstruction task, which provides additional infrared information to train the generator. This enhances the convergence efficiency of generator training through soft label guidance generated through knowledge distillation. The experimental results show that compared to the existing typical infrared image generation algorithms, the proposed method can generate higher-quality infrared images and achieve better performance in both subjective visual description and objective metric evaluation, and that it has better performance in the downstream tasks of the template matching and image fusion tasks.},
  archive      = {J_CIS},
  author       = {Ma, Decao and Su, Juan and Xian, Yong and Li, Shaopeng},
  doi          = {10.1007/s40747-025-01827-7},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Discriminator guided visible-to-infrared image translation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="cms---8">CMS - 8</h2>
<ul>
<li><details>
<summary>
(2025). High-order self-excited threshold integer-valued
autoregressive model: Estimation and testing. <em>CMS</em>,
<em>13</em>(1), 233–260. (<a
href="https://doi.org/10.1007/s40304-022-00325-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the need of modeling and inference for high-order integer-valued threshold time series models, this paper introduces a pth-order two-regime self-excited threshold integer-valued autoregressive (SETINAR(2, p)) model. Basic probabilistic and statistical properties of the model are discussed. The parameter estimation problem is addressed by means of conditional least squares and conditional maximum likelihood methods. The asymptotic properties of the estimators, including the threshold parameter, are obtained. A method to test the nonlinearity of the underlying process is provided. Some simulation studies are conducted to show the performances of the proposed methods. Finally, an application to the number of people suffering from meningococcal disease in Germany is provided.},
  archive      = {J_CMS},
  author       = {Yang, Kai and Li, Ang and Li, Han and Dong, Xiaogang},
  doi          = {10.1007/s40304-022-00325-3},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {233-260},
  shortjournal = {Commun. Math. Stat.},
  title        = {High-order self-excited threshold integer-valued autoregressive model: Estimation and testing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic geometric iterative method for loop subdivision
surface fitting. <em>CMS</em>, <em>13</em>(1), 217–231. (<a
href="https://doi.org/10.1007/s40304-022-00323-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a stochastic geometric iterative method (S-GIM) to approximate the high-resolution 3D models by finite loop subdivision surfaces. Given an input mesh as the fitting target, the initial control mesh is generated using the mesh simplification algorithm. Then, our method adjusts the control mesh iteratively to make its finite loop subdivision surface approximate the input mesh. In each geometric iteration, we randomly select part of points on the subdivision surface to calculate the difference vectors and distribute the vectors to the control points. Finally, the control points are updated by adding the weighted average of these difference vectors. We prove the convergence of S-GIM and verify it by demonstrating error curves in the experiment. In addition, compared with existing geometric iterative methods, S-GIM has a shorter running time under the same number of iteration steps.},
  archive      = {J_CMS},
  author       = {Xu, Chenkai and He, Yaqi and Hu, Hui and Lin, Hongwei},
  doi          = {10.1007/s40304-022-00323-5},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {217-231},
  shortjournal = {Commun. Math. Stat.},
  title        = {Stochastic geometric iterative method for loop subdivision surface fitting},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of informatively interval-censored case–cohort
studies with the application to HIV vaccine trials. <em>CMS</em>,
<em>13</em>(1), 195–215. (<a
href="https://doi.org/10.1007/s40304-022-00322-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case–cohort studies are commonly used in various investigations, and many methods have been proposed for their analyses. However, most of the available methods are for right-censored data or assume that the censoring is independent of the underlying failure time of interest. In addition, they usually apply only to a specific model such as the Cox model that may often be restrictive or violated in practice. To relax these assumptions, we discuss regression analysis of interval-censored data, which arise more naturally in case–cohort studies than and include right-censored data as a special case, and propose a two-step inverse probability weighting estimation procedure under a general class of semiparametric transformation models. Among other features, the approach allows for informative censoring. In addition, an EM algorithm is developed for the determination of the proposed estimators and the asymptotic properties of the proposed estimators are established. Simulation results indicate that the approach works well for practical situations and it is applied to a HIV vaccine trial that motivated this investigation.},
  archive      = {J_CMS},
  author       = {Du, Mingyue and Zhou, Qingning},
  doi          = {10.1007/s40304-022-00322-6},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {195-215},
  shortjournal = {Commun. Math. Stat.},
  title        = {Analysis of informatively interval-censored Case–Cohort studies with the application to HIV vaccine trials},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical bayesian approach for finite mixture of mode
regression model using skew-normal distribution. <em>CMS</em>,
<em>13</em>(1), 173–194. (<a
href="https://doi.org/10.1007/s40304-022-00320-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data that exhibit asymmetrical behavior can be well modeled with skew-normal random errors. Moreover, data that arise from a heterogeneous population can be efficiently analyzed by a finite mixture of regression models. These observations motivate us to propose a novel finite mixture of mode regression model based on a mixture of the skew-normal distributions to explore asymmetrical data from several subpopulations. Thanks to the stochastic representation of the skew-normal distribution, we construct a Bayesian hierarchical modeling framework and then develop an efficient Markov chain Monte Carlo sampling algorithm to generate posterior samples for obtaining the Bayesian estimates of the unknown parameters and their corresponding standard errors. Simulation studies and a real-data example are presented to illustrate the performance of the proposed Bayesian methodology.},
  archive      = {J_CMS},
  author       = {Zeng, Xin and Wang, Min and Ju, Yuanyuan and Wu, Liucang},
  doi          = {10.1007/s40304-022-00320-8},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {173-194},
  shortjournal = {Commun. Math. Stat.},
  title        = {A hierarchical bayesian approach for finite mixture of mode regression model using skew-normal distribution},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature screening and error variance estimation for
ultrahigh-dimensional linear model with measurement errors.
<em>CMS</em>, <em>13</em>(1), 139–171. (<a
href="https://doi.org/10.1007/s40304-022-00317-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we mainly study the feature screening and error variance estimation in ultrahigh-dimensional linear model with errors-in-variables (EV). Given that sure independence screening (SIS) method by marginal Pearson’s correlation learning may omit some important observation variables due to measurement errors, a corrected SIS called EVSIS is proposed to rank the importance of features according to their corrected marginal correlation with the response variable. Also, a corrected error variance procedure is proposed to accurately estimate the error variance, which could greatly attenuate the influence of measurement errors and spurious correlations, simultaneously. Under some regularization conditions, the proposed EVSIS possesses sure screening property and consistency in ranking and the corrected error variance estimator is also proved to be asymptotically normal. The two methodologies are illustrated by some simulations and a real data example, which suggests that the proposed methods perform well.},
  archive      = {J_CMS},
  author       = {Cui, Hengjian and Zou, Feng and Ling, Li},
  doi          = {10.1007/s40304-022-00317-3},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {139-171},
  shortjournal = {Commun. Math. Stat.},
  title        = {Feature screening and error variance estimation for ultrahigh-dimensional linear model with measurement errors},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum n-toroidal algebras and extended quantized GIM
algebras of n-fold affinization. <em>CMS</em>, <em>13</em>(1), 99–137.
(<a href="https://doi.org/10.1007/s40304-022-00316-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the notion of quantum N-toroidal algebras as natural generalization of the quantum toroidal algebras as well as extended quantized GIM algebras of N-fold affinization. We show that the quantum N-toroidal algebras are quotients of the extended quantized GIM algebras of N-fold affinization, which generalizes a well-known result of Berman and Moody for Lie algebras.},
  archive      = {J_CMS},
  author       = {Gao, Yun and Jing, Naihuan and Xia, Limeng and Zhang, Honglian},
  doi          = {10.1007/s40304-022-00316-4},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {99-137},
  shortjournal = {Commun. Math. Stat.},
  title        = {Quantum N-toroidal algebras and extended quantized GIM algebras of N-fold affinization},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the lag window estimators of the spectrum and
memory for long-memory stationary gaussian processes. <em>CMS</em>,
<em>13</em>(1), 59–98. (<a
href="https://doi.org/10.1007/s40304-022-00304-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) is a stochastic process that has been successfully applied in finance, black-box modeling of biosystems, machine learning, geostatistics, multitask learning or robotics and reinforcement learning. Effectively estimating the spectral density function (SDF) and degree of memory (DOM) of a long-memory stationary GP (LMSGP) is needed to get accurate information about the process. The practice demonstrated that the periodogram estimator (PE) and lag window estimator (LWE) that are the extremely used estimators of the SDF and DOM have some drawbacks, especially for LMSGPs. The behaviors of the PEs and LWEs are soundly investigated numerically; however, the theoretical justifications are limited and thus the challenge to improve them is still daunting. This paper gives a closer look at the theoretical justifications of the efficiency of the LWEs that provides new sufficient conditions (NSCs) for improving the LWEs of the SDF and DOM for LMSGPs. The precision, the convergence rate of the bias and variance, and the asymptotic distributions of the LWEs under the NSCs are studied. A comparison study among the LWEs under the NSCs, the LWEs without the NSCs and the PEs is given to investigate the significance of the NSCs. The main theoretical and simulation results show that: the LWEs under the NSCs are asymptotically unbiased and consistent and better than the LWEs without the NSCs and the PEs, and the asymptotic distributions of the LWEs under the NSCs are chi-square for SDF and normal for DOM.},
  archive      = {J_CMS},
  author       = {Laala, Barkahoum and Belaloui, Soheir and Fang, Kai-Tai and Elsawah, A. M.},
  doi          = {10.1007/s40304-022-00304-8},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {59-98},
  shortjournal = {Commun. Math. Stat.},
  title        = {Improving the lag window estimators of the spectrum and memory for long-memory stationary gaussian processes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dimensions of tri-quadratic <span
class="math display"><em>c</em><sup>1</sup></span> spline spaces over
hierarchical 3D t-meshes. <em>CMS</em>, <em>13</em>(1), 1–57. (<a
href="https://doi.org/10.1007/s40304-022-00296-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3D T-mesh is generally a cuboid grid which allows hanging vertices. Here, a hanging vertex is an interior vertex, but it is not a corner point of eight cells. Spline function spaces with high order smoothness over 3D T-meshes have great application prospect due to their local refinement and relatively low degrees of freedom, for example, 3D isogeometric analysis and implicit representation of surfaces. However, there are still no available dimension formulae of those kinds of spline spaces for application. In this paper, we explore the dimensions of trivariate quadratic spline spaces with $$ C^1 $$ continuity over hierarchical 3D T-meshes. By using space embedding method, the problem is converted into a system of linear constraints, and then a lower bound on the dimension of the spline space over a hierarchical 3D T-mesh is provided. For a special type of hierarchical 3D T-meshes, the explicit dimension formula is obtained. In addition, a topological explanation of the dimension is given, which presents a way to construct basis functions.},
  archive      = {J_CMS},
  author       = {Liu, Min and Deng, Fang and Deng, Jiansong},
  doi          = {10.1007/s40304-022-00296-5},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {1-57},
  shortjournal = {Commun. Math. Stat.},
  title        = {Dimensions of tri-quadratic $$ c^1 $$ spline spaces over hierarchical 3D T-meshes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="coap---12">COAP - 12</h2>
<ul>
<li><details>
<summary>
(2025). An arc-search interior-point algorithm for nonlinear
constrained optimization. <em>COAP</em>, <em>90</em>(3), 969–995. (<a
href="https://doi.org/10.1007/s10589-025-00648-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new arc-search interior-point algorithm for the nonlinear constrained optimization problem. The proposed algorithm uses the second-order derivatives to construct a search arc that approaches the optimizer. Because the arc stays in the interior set longer than any straight line, it is expected that the scheme will generate a better new iterate than a line search method. The computation of the second-order derivatives requires to solve the second linear system of equations, but the coefficient matrix of the second linear system of equations is the same as the first linear system of equations. Therefore, the matrix decomposition obtained while solving the first linear system of equations can be reused. In addition, most elements of the right-hand side vector of the second linear system of equations are already computed when the coefficient matrix is assembled. Therefore, the computation cost for solving the second linear system of equations is insignificant and the benefit of having a better search scheme is well justified. The convergence of the proposed algorithm is established. Some preliminary test results are reported to demonstrate the merit of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Yang, Yaguang},
  doi          = {10.1007/s10589-025-00648-1},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {969-995},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An arc-search interior-point algorithm for nonlinear constrained optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the stochastically controlled stochastic gradient
method by the bandwidth-based stepsize. <em>COAP</em>, <em>90</em>(3),
941–968. (<a href="https://doi.org/10.1007/s10589-025-00651-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepsize plays an important role in the stochastic gradient method. The bandwidth-based stepsize allows us to adjust the stepsize within a banded region determined by some boundary functions. Based on the bandwidth-based stepsize, we propose a new method, namely SCSG-BD, for smooth non-convex finite-sum optimization problems. For the boundary functions 1/t, $$1/(t\log (t + 1))$$ and $$1/t^p$$ ( $$p\in (0,1)$$ ), SCSG-BD converges sublinearly to a stationary point at a faster rate than the stochastically controlled stochastic gradient (SCSG) method under certain conditions. Moreover, SCSG-BD is able to converge linearly to the solution if the objective function satisfies the Polyak–Łojasiewicz condition. We also introduce the 1/t-Barzilai–Borwein stepsize for practical computation. Numerical experiments demonstrate that SCSG-BD performs better than SCSG and its variants.},
  archive      = {J_COAP},
  author       = {Liu, Chenchen and Huang, Yakui and Wang, Dan},
  doi          = {10.1007/s10589-025-00651-6},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {941-968},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Improving the stochastically controlled stochastic gradient method by the bandwidth-based stepsize},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A second-order sequential optimality condition for nonlinear
second-order cone programming problems. <em>COAP</em>, <em>90</em>(3),
911–939. (<a href="https://doi.org/10.1007/s10589-025-00649-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last two decades, the sequential optimality conditions, which do not require constraint qualifications and allow improvement on the convergence assumptions of algorithms, had been considered in the literature. It includes the work by Andreani et al. (IMA J Numer Anal 37:1902–1929, 2017), with a sequential optimality condition for nonlinear programming, that uses the second-order information of the problem. More recently, Fukuda et al. (Set-Valued Var Anal 31:15, 2023) analyzed the conditions that use second-order information, in particular for nonlinear second-order cone programming problems (SOCP). However, such optimality conditions were not defined explicitly. In this paper, we propose an explicit definition of approximate-Karush-Kuhn-Tucker 2 (AKKT2) and complementary-AKKT2 (CAKKT2) conditions for SOCPs. We prove that the proposed AKKT2/CAKKT2 conditions are satisfied at local optimal points of the SOCP without any constraint qualification. We also present two algorithms that are based on augmented Lagrangian and sequential quadratic programming methods and show their global convergence to points satisfying the proposed conditions.},
  archive      = {J_COAP},
  author       = {Fukuda, Ellen H. and Okabe, Kosuke},
  doi          = {10.1007/s10589-025-00649-0},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {911-939},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A second-order sequential optimality condition for nonlinear second-order cone programming problems},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the douglas–rachford splitting method through
the lenses of moreau-type envelopes. <em>COAP</em>, <em>90</em>(3),
881–910. (<a href="https://doi.org/10.1007/s10589-024-00646-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the Douglas–Rachford splitting method for weakly convex optimization problems, by the token of the Douglas–Rachford envelope, a merit function akin to the Moreau envelope. First, we use epi-convergence techniques to show that this artifact approximates the original objective function via epigraphs. Secondly, we present how global convergence and local linear convergence rates for Douglas–Rachford splitting can be obtained using such envelope, under mild regularity assumptions. The keystone of the convergence analysis is the fact that the Douglas–Rachford envelope satisfies a sufficient descent inequality alongside the generated sequence, a feature that allows us to use arguments usually employed for descent methods. We report numerical experiments that use weakly convex penalty functions, which are comparable with the known behavior of the method in the convex case.},
  archive      = {J_COAP},
  author       = {Atenas, Felipe},
  doi          = {10.1007/s10589-024-00646-9},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {881-910},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Understanding the Douglas–Rachford splitting method through the lenses of moreau-type envelopes},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A convex combined symmetric alternating direction method of
multipliers for separable optimization. <em>COAP</em>, <em>90</em>(3),
839–880. (<a href="https://doi.org/10.1007/s10589-025-00647-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Alternating Direction Method of Multipliers (ADMM) is a powerful first-order method used in many practical separable optimization problems. In this paper, we propose a new variant of the symmetric ADMM, called the Convex Combined Symmetric ADMM (CcS-ADMM), by integrating a convex combination technique. CcS-ADMM retains all the favorable features of ADMM, including the ability to take full advantage of problem structures and global convergence under relaxed parameter conditions. Furthermore, using the moderate assumptions and primal-dual gap, we analyze the convergence and the O(1/N) ergodic convergence rate of the algorithm with convex setting. Additionally, we propose the convergence of the CcS-ADMM with nonconvex setting in Euclidean space under so called Kurdyka–Lojasiewicz property and some widely used assumptions, and we establish the pointwise iteration-complexity of CcS-ADMM with respect to the augmented Lagrangian function and the primal-dual residuals. Finally, we present the results from preliminary numerical experiments to demonstrate the performance of the proposed algorithms.},
  archive      = {J_COAP},
  author       = {Wang, Xiaoquan and Shao, Hu and Wu, Ting},
  doi          = {10.1007/s10589-025-00647-2},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {839-880},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A convex combined symmetric alternating direction method of multipliers for separable optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized methods via cubic model subspace minimization
for nonconvex optimization. <em>COAP</em>, <em>90</em>(3), 801–837. (<a
href="https://doi.org/10.1007/s10589-025-00655-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive cubic regularization methods for solving nonconvex problems need the efficient computation of the trial step, involving the minimization of a cubic model. We propose a new approach in which this model is minimized in a low dimensional subspace that, in contrast to classic approaches, is reused for a number of iterations. Whenever the trial step produced by the low-dimensional minimization process is unsatisfactory, we employ a regularized Newton step whose regularization parameter is a by-product of the model minimization over the low-dimensional subspace. We show that the worst-case complexity of classic cubic regularized methods is preserved, despite the possible regularized Newton steps. We focus on the large class of problems for which (sparse) direct linear system solvers are available and provide several experimental results showing the very large gains of our new approach when compared to standard implementations of adaptive cubic regularization methods based on direct linear solvers. Our first choice as projection space for the low-dimensional model minimization is the polynomial Krylov subspace; nonetheless, we also explore the use of rational Krylov subspaces in case where the polynomial ones lead to less competitive numerical results.},
  archive      = {J_COAP},
  author       = {Bellavia, Stefania and Palitta, Davide and Porcelli, Margherita and Simoncini, Valeria},
  doi          = {10.1007/s10589-025-00655-2},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {801-837},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Regularized methods via cubic model subspace minimization for nonconvex optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence of the gradient descent method with
stochastic fixed-point rounding errors under the polyak–łojasiewicz
inequality. <em>COAP</em>, <em>90</em>(3), 753–799. (<a
href="https://doi.org/10.1007/s10589-025-00656-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the training of neural networks with low-precision computation and fixed-point arithmetic, rounding errors often cause stagnation or are detrimental to the convergence of the optimizers. This study provides insights into the choice of appropriate stochastic rounding strategies to mitigate the adverse impact of roundoff errors on the convergence of the gradient descent method, for problems satisfying the Polyak–Łojasiewicz inequality. Within this context, we show that a biased stochastic rounding strategy may be even beneficial in so far as it eliminates the vanishing gradient problem and forces the expected roundoff error in a descent direction. Furthermore, we obtain a bound on the convergence rate that is stricter than the one achieved by unbiased stochastic rounding. The theoretical analysis is validated by comparing the performances of various rounding strategies when optimizing several examples using low-precision fixed-point arithmetic.},
  archive      = {J_COAP},
  author       = {Xia, Lu and Massei, Stefano and Hochstenbach, Michiel E.},
  doi          = {10.1007/s10589-025-00656-1},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {753-799},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the convergence of the gradient descent method with stochastic fixed-point rounding errors under the polyak–Łojasiewicz inequality},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All saddle points for polynomial optimization.
<em>COAP</em>, <em>90</em>(3), 721–752. (<a
href="https://doi.org/10.1007/s10589-025-00657-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study how to compute all saddle points for the constrained and unconstrained polynomial optimization, respectively. For the constrained polynomial optimization, a scalar-type semidefinite relaxation algorithm is proposed based on the Karush-Kuhn-Tucker conditions. While for the unconstrained polynomial optimization, a matrix-type semidefinite relaxation algorithm is proposed based on the second-order optimality conditions. Both algorithms can detect the nonexistence of saddle points or find all of them if there are finitely many ones. The finite convergence of the algorithms can also be obtained under some genericity conditions.},
  archive      = {J_COAP},
  author       = {Zhou, Anwa and Yin, Shiqian and Fan, Jinyan},
  doi          = {10.1007/s10589-025-00657-0},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {721-752},
  shortjournal = {Comput. Optim. Appl.},
  title        = {All saddle points for polynomial optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the use of restriction of the right-hand side in spatial
branch-and-bound algorithms to ensure termination. <em>COAP</em>,
<em>90</em>(3), 691–720. (<a
href="https://doi.org/10.1007/s10589-025-00652-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial branch-and-bound algorithms for global minimization of non-convex problems require both lower and upper bounding procedures that finally converge to a globally optimal value in order to ensure termination of these methods. Whereas convergence of lower bounds is commonly guaranteed for standard approaches in the literature, this does not always hold for upper bounds. For this reason, different so-called convergent upper bounding procedures are proposed. These methods are not always used in practice, possibly due to their additional complexity or possibly due to increasing runtimes on average problems. For that reason, in this article we propose a refinement of classical branch-and-bound methods that is simple to implement and comes with marginal overhead. We prove that this small improvement already leads to convergent upper bounds, and thus show that termination of spatial branch-and-bound methods is ensured under mild assumptions.},
  archive      = {J_COAP},
  author       = {Kirst, Peter and Füllner, Christian},
  doi          = {10.1007/s10589-025-00652-5},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {691-720},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the use of restriction of the right-hand side in spatial branch-and-bound algorithms to ensure termination},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parabolic optimal control problems with combinatorial
switching constraints, part III: Branch-and-bound algorithm.
<em>COAP</em>, <em>90</em>(3), 649–689. (<a
href="https://doi.org/10.1007/s10589-025-00654-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a branch-and-bound algorithm for globally solving parabolic optimal control problems with binary switches that have bounded variation and possibly need to satisfy further combinatorial constraints. More precisely, for a given tolerance $$\varepsilon &gt;0$$ , we show how to compute in finite time an $$\varepsilon $$ -optimal solution in function space, independently of any prior discretization. The main ingredients in our approach are an appropriate branching strategy in infinite dimension, an a posteriori error estimation in order to obtain safe dual bounds, and an adaptive refinement strategy in order to allow arbitrary switching points in the limit. The performance of our approach is demonstrated by extensive experimental results.},
  archive      = {J_COAP},
  author       = {Buchheim, Christoph and Grütering, Alexandra and Meyer, Christian},
  doi          = {10.1007/s10589-025-00654-3},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {649-689},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Parabolic optimal control problems with combinatorial switching constraints, part III: Branch-and-bound algorithm},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cardinality constrained mean-variance portfolios: A penalty
decomposition algorithm. <em>COAP</em>, <em>90</em>(3), 631–648. (<a
href="https://doi.org/10.1007/s10589-025-00653-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cardinality-constrained mean-variance portfolio problem has garnered significant attention within contemporary finance due to its potential for achieving low risk while effectively managing transaction costs. Instead of solving this problem directly, many existing methods rely on regularization and approximation techniques, which hinder investors’ ability to precisely specify a portfolio’s desired cardinality level. Moreover, these approaches typically include more hyper-parameters and increase the problem’s dimensionality. To address these challenges, we propose a customized penalty decomposition algorithm. We demonstrate that this algorithm not only does it converge to a local minimizer of the cardinality-constrained mean-variance portfolio problem, but is also computationally efficient. Our approach leverages a sequence of penalty subproblems, each tackled using Block Coordinate Descent (BCD). We show that the steps within BCD yield closed-form solutions, allowing us to identify a saddle point of the penalty subproblems. Finally, by applying our penalty decomposition algorithm to real-world datasets, we highlight its efficiency and its superiority over state-of-the-art methods across several performance metrics.},
  archive      = {J_COAP},
  author       = {Mousavi, Ahmad and Michailidis, George},
  doi          = {10.1007/s10589-025-00653-4},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {631-648},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Cardinality constrained mean-variance portfolios: A penalty decomposition algorithm},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-DS: A cost saving algorithm for expensive constrained
multi-fidelity blackbox optimization. <em>COAP</em>, <em>90</em>(3),
607–629. (<a href="https://doi.org/10.1007/s10589-024-00645-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces Inter-DS, a blackbox optimization algorithmic framework for computationally expensive constrained multi-fidelity problems. When applying a direct search method to such problems, the scarcity of feasible points may lead to numerous costly evaluations spent on infeasible points. Our proposed algorithm addresses this issue by leveraging multi-fidelity information, allowing for premature interruption of an evaluation when a point is estimated to be infeasible. These estimations are controlled by a biadjacency matrix, for which we propose a construction. The proposed method acts as an intermediary component bridging any non multi-fidelity direct search solver and a multi-fidelity blackbox problem, giving the user freedom of choice for the solver. A series of computational tests are conducted to validate the approach. The results show a significant improvement in solution quality when an initial feasible starting point is provided. When this condition is not met, the outcomes are contingent upon specific properties of the blackbox.},
  archive      = {J_COAP},
  author       = {Alarie, Stéphane and Audet, Charles and Diago, Miguel and Digabel, Sébastien Le and Lebeuf, Xavier},
  doi          = {10.1007/s10589-024-00645-w},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {607-629},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inter-DS: A cost saving algorithm for expensive constrained multi-fidelity blackbox optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="dmkd---6">DMKD - 6</h2>
<ul>
<li><details>
<summary>
(2025). Missing value replacement in strings and applications.
<em>DMKD</em>, <em>39</em>(2), 1–50. (<a
href="https://doi.org/10.1007/s10618-024-01074-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values arise routinely in real-world sequential (string) datasets due to: (1) imprecise data measurements; (2) flexible sequence modeling, such as binding profiles of molecular sequences; or (3) the existence of confidential information in a dataset which has been deleted deliberately for privacy protection. In order to analyze such datasets, it is often important to replace each missing value, with one or more valid letters, in an efficient and effective way. Here we formalize this task as a combinatorial optimization problem: the set of constraints includes the context of the missing value (i.e., its vicinity) as well as a finite set of user-defined forbidden patterns, modeling, for instance, implausible or confidential patterns; and the objective function seeks to minimize the number of new letters we introduce. Algorithmically, our problem translates to finding shortest paths in special graphs that contain forbidden edges representing the forbidden patterns. Our work makes the following contributions: (1) we design a linear-time algorithm to solve this problem for strings over constant-sized alphabets; (2) we show how our algorithm can be effortlessly applied to fully sanitize a private string in the presence of a set of fixed-length forbidden patterns [Bernardini et al. 2021a]; (3) we propose a methodology for sanitizing and clustering a collection of private strings that utilizes our algorithm and an effective and efficiently computable distance measure; and (4) we present extensive experimental results showing that our methodology can efficiently sanitize a collection of private strings while preserving clustering quality, outperforming the state of the art and baselines. To arrive at our theoretical results, we employ techniques from formal languages and combinatorial pattern matching.},
  archive      = {J_DMKD},
  author       = {Bernardini, Giulia and Liu, Chang and Loukides, Grigorios and Marchetti-Spaccamela, Alberto and Pissis, Solon P. and Stougie, Leen and Sweering, Michelle},
  doi          = {10.1007/s10618-024-01074-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-50},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Missing value replacement in strings and applications},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative evaluation of clustering-based outlier
detection. <em>DMKD</em>, <em>39</em>(2), 1–55. (<a
href="https://doi.org/10.1007/s10618-024-01086-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform an extensive experimental evaluation of clustering-based outlier detection methods. These methods offer benefits such as efficiency, the possibility to capitalize on more mature evaluation measures, more developed subspace analysis for high-dimensional data and better explainability, and yet they have so-far been neglected in literature. To our knowledge, our work is the first effort to analytically and empirically study their advantages and disadvantages. Our main goal is to evaluate whether or not clustering-based techniques can compete in efficiency and effectiveness against the most studied state-of-the-art algorithms in the literature. We consider the quality of the results, the resilience against different types of data and variations in parameter configuration, the scalability, and the ability to filter out inappropriate parameter values automatically based on internal measures of clustering quality. It has been recently shown that several classic, simple, unsupervised methods surpass many deep learning approaches and, hence, remain at the state-of-the-art of outlier detection. We therefore study 14 of the best classic unsupervised methods, in particular 11 clustering-based methods and 3 non-clustering-based ones, using a consistent parameterization heuristic to identify the pros and cons of each approach. We consider 46 real and synthetic datasets with up to 125k points and 1.5k dimensions aiming to achieve plausibility with the broadest possible diversity of real-world use cases. Our results indicate that the clustering-based methods are on par with (if not surpass) the non-clustering-based ones, and we argue that clustering-based methods like KMeans−− should be included as baselines in future benchmarking studies, as they often offer a competitive quality at a relatively low run time, besides several other benefits.},
  archive      = {J_DMKD},
  author       = {Sánchez Vinces, Braulio V. and Schubert, Erich and Zimek, Arthur and Cordeiro, Robson L. F.},
  doi          = {10.1007/s10618-024-01086-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-55},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A comparative evaluation of clustering-based outlier detection},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-level relation extraction for informative taxonomy
learning. <em>DMKD</em>, <em>39</em>(2), 1–31. (<a
href="https://doi.org/10.1007/s10618-024-01088-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the ever-shifting domain of technological advancement, the quest for automatic taxonomy construction is intensifying. This paper confronts the nuanced challenges of distilling synonym and hyponym relationships from diverse, domain-specific scientific literature, treating it as a domain-level relation extraction problem and resulting in the creation of a hierarchical taxonomy through arborescence generation. The proposed Multi-Scale Identity Connection (MSIC) model excels in capturing inter-entity relationships across various scales, demonstrating superior empirical performance compared to existing relation extraction models. To enhance practicality, a two-stage optimization is introduced to improve efficiency without compromising performance. Additionally, the Depth-prioritized Maximum Spanning Arborescence (DMSA) algorithm has been proposed as a highly efficient strategy for generating an informative and well-structured taxonomy tree. We annotated a concise dataset to train and validate the MSIC model, subsequently applying it to a substantial domain-specific dataset for taxonomy induction. The experimental findings indicate that the DMSA efficiently constructs an information-rich taxonomy tree structure by leveraging extensive domain-specific scientific literature. These results not only affirm the efficacy of the approach but also highlight its effectiveness in supporting industrial-grade applications.},
  archive      = {J_DMKD},
  author       = {Hu, Maodi and Song, Donghuan and Qian, Li and Zhang, Zhixiong},
  doi          = {10.1007/s10618-024-01088-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Domain-level relation extraction for informative taxonomy learning},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximity forest 2.0: A new effective and scalable
similarity-based classifier for time series. <em>DMKD</em>,
<em>39</em>(2), 1–29. (<a
href="https://doi.org/10.1007/s10618-024-01085-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is a challenging task due to the diversity of types of features that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms other state-of-the-art methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series similarity measures — (1) computationally efficient early abandoning and pruning to speedup elastic similarity computations; (2) a new elastic similarity measure, Amerced Dynamic Time Warping ( $${{\,\textrm{ADTW}\,}}$$ ); and (3) cost function tuning. It rationalizes the set of similarity measures employed, reducing the eight base measures of the original PF to four and using the first derivative transform with all similarity measures, rather than a limited subset. It also incorporates HYDRA, a dictionary-based transform. We have re-implemented PF 1.0 and implemented PF 2.0 framework in Java, making the PF framework more efficient.},
  archive      = {J_DMKD},
  author       = {Tan, Chang Wei and Herrmann, Matthieu and Salehi, Mahsa and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-024-01085-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Proximity forest 2.0: A new effective and scalable similarity-based classifier for time series},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TED: Related party transaction guided tax evasion detection
on heterogeneous graph. <em>DMKD</em>, <em>39</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s10618-025-01091-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tax evasion causes severe losses of government revenues and disturbs the economic order of fair competition. To help alleviate this problem, the latest tax evasion detection solutions utilize expert knowledge to extract features and then train classifiers to determine whether a company is suspected of tax evasion. However, existing solutions mainly focus on the statistical features of the company, but fail to exploit the rich interactive information in tax scenarios, which affect the detection performance. In this paper, we first model the tax scenario as a heterogeneous graph and study the tax evasion detection problem under the heterogeneous graph model. To improve the performance of tax evasion detection, a novel graph neural network model is proposed to extract the comprehensive information of heterogeneous graphs. Specifically, we use heterogeneous and complex related party transaction groups to filter low-level noise information. Moreover, a hierarchical attention mechanism is designed to capture the deeper structure and semantic information hidden in the related party transaction group. We apply our method to the real risk management system of the tax bureau, and evaluate it on two human-labeled real-world tax datasets. The results demonstrate that our method significantly outperforms the state-of-the-art in the tax evasion detection task. The code and data are available at: https://github.com/yimingxu24/TED .},
  archive      = {J_DMKD},
  author       = {Xu, Yiming and Shi, Bin and Dong, Bo and Wang, Jiaxiang and Wei, Hua and Zheng, Qinghua},
  doi          = {10.1007/s10618-025-01091-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TED: Related party transaction guided tax evasion detection on heterogeneous graph},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring tie strength in temporal networks. <em>DMKD</em>,
<em>39</em>(2), 1–31. (<a
href="https://doi.org/10.1007/s10618-025-01093-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring tie strengths in social networks is an essential task in social network analysis. Common approaches classify the ties as weak and strong ties based on the strong triadic closure (STC). The STC states that if for three nodes, A, B, and C, there are strong ties between A and B, as well as A and C, there has to be a (weak or strong) tie between B and C. A variant of the STC called STC+ allows adding a few new weak edges to obtain improved solutions. So far, most works discuss the STC or STC+ in static networks. However, modern large-scale social networks are usually highly dynamic, providing user contacts and communications as streams of edge updates. Temporal networks capture these dynamics. To apply the STC to temporal networks, we first generalize the STC and introduce a weighted version such that empirical a priori knowledge given in the form of edge weights is respected by the STC. Similarly, we introduce a generalized weighted version of the STC+. The weighted STC is hard to compute, and our main contribution is an efficient 2-approximation (resp. 3-approximation) streaming algorithm for the weighted STC (resp. STC+) in temporal networks. As a technical contribution, we introduce a fully dynamic k-approximation for the minimum weighted vertex cover problem in hypergraphs with edges of size k, which is a crucial component of our streaming algorithms. An empirical evaluation shows that the weighted STC leads to solutions that better capture the a priori knowledge given by the edge weights than the non-weighted STC. Moreover, we show that our streaming algorithm efficiently approximates the weighted STC in real-world large-scale social networks.},
  archive      = {J_DMKD},
  author       = {Oettershagen, Lutz and Konstantinidis, Athanasios L. and Italiano, Giuseppe F.},
  doi          = {10.1007/s10618-025-01093-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Inferring tie strength in temporal networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ei---31">EI - 31</h2>
<ul>
<li><details>
<summary>
(2025). Data-driven weight initialization strategy for convolutional
neural networks. <em>EI</em>, <em>18</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s12065-024-00985-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep Convolutional Neural Networks consists of multiple forward and backward passes over several epochs until the loss converges, making it time-consuming. Various factors affect the training times of networks, weight initialization being one of them. The idea of a proper weight initialization technique is to set the initial weights such that the network converges faster by extracting meaningful features from the data. This paper proposes a data-driven weight initialization to accelerate the training process. This technique is based on obtaining initial weights by appropriately analysing the training data. The proposed weight initialization strategy initializes filters from a pre-defined filter bank that is created before training, and it contains standard edge and texture feature extracting filters and data-driven filters obtained using principal component analysis, linear discriminant analysis and partial least squares. The proposed technique has been validated on AlexNet, VGG-16 and ResNet-50 for Intel Image Classification, CIFAR10 and CIFAR100 datasets. The results show that the proposed technique gives better validation results than other state-of-the-art techniques in fewer epochs and works well with state-of-the-art activation functions.},
  archive      = {J_EI},
  author       = {Narkhede, Meenal and Mahajan, Shrinivas and Bartakke, Prashant},
  doi          = {10.1007/s12065-024-00985-w},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Evol. Intell.},
  title        = {Data-driven weight initialization strategy for convolutional neural networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards load frequency management in thermal power systems
using an improved open-source development model algorithm. <em>EI</em>,
<em>18</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12065-024-00986-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining a balance between generated and consumed power is crucial for the efficient operation of interconnected power grids. This paper focuses on the issue of load frequency management in thermal power systems. We present a metaheuristic approach to address this problem, leveraging the advantages of metaheuristic algorithms in solving complex optimization problems. Our approach is based on an improved Open-source Development model algorithm (ODMA). The proposed method, referred to as ODMA-GA, integrates ODMA with a Genetic algorithm (GA) to configure a hybrid optimization technique. This combination allows for the effective exploitation and exploration of the search space, leading to high-quality solutions. We evaluate the performance of ODMA-GA using several benchmark functions with varying scenarios. The results demonstrate that ODMA-GA outperforms its peer metaheuristic algorithms, showcasing superior optimization capabilities. This promising performance suggests that ODMA-GA offers an effective solution for the challenging task of load frequency management in thermal power systems, ensuring stable and efficient operation. Specifically, the finding shows that our method improves the frequency change area and tie-line power change by 2.3% compared to the best existing method.},
  archive      = {J_EI},
  author       = {Khezri, Edris and Rezaeipanah, Amin and Hassanzadeh, Hiwa and Majidpour, Jafar},
  doi          = {10.1007/s12065-024-00986-9},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Evol. Intell.},
  title        = {Towards load frequency management in thermal power systems using an improved open-source development model algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integration of metaheuristic operators through unstructured
evolutive game theory approach: A novel hybrid methodology. <em>EI</em>,
<em>18</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s12065-024-00988-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach to addressing the complexity of current optimization challenges by developing hybrid metaheuristic algorithms. These algorithms combine the strengths of different strategies to enhance solution identification and refinement. In this work, the integration of metaheuristic operators using the Unstructured Evolutive Game Theory to regulate and merge the advantageous features of the Cheetah Optimizer and Particle Swarm Optimization to solve continuous optimization problems is proposed. The Cheetah Optimizer and Particle Swarm Optimization are chosen for their superior exploitation and exploration capabilities, respectively. Our methodology unfolds in two primary phases: combination and modulation. In the combination phase, Cheetah Optimizer and Particle Swarm Optimization search strategies are merged, creating a unified population that generates new candidate solution positions based on the best solution and a modulation factor. During the modulation phase, pairwise competitions based on Unstructured Evolutive Game Theory assess candidate solutions against the objective function, adjusting their modulation factor and position accordingly. We conducted various experiments to evaluate our approach against the original Cheetah Optimizer and Particle Swarm Optimization, as well as seven other well-known metaheuristic algorithms and hybrid schemes, across 30 benchmark functions in dimensions of 30, 50, and 100. The results reveal that our hybrid scheme outperforms the comparative algorithms, demonstrating enhanced performance, effectiveness, and robustness through a detailed convergence and significance analysis.},
  archive      = {J_EI},
  author       = {Escobar-Cuevas, Hector and Cuevas, Erik and Lopez, Jesus and Perez-Cisneros, Marco},
  doi          = {10.1007/s12065-024-00988-7},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Evol. Intell.},
  title        = {Integration of metaheuristic operators through unstructured evolutive game theory approach: A novel hybrid methodology},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning-based spatio-temporal graph
neural network for solving job shop scheduling problem. <em>EI</em>,
<em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12065-024-00989-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The job shop scheduling problem (JSSP) is a well-known NP-hard combinatorial optimization problem that focuses on assigning tasks to limited resources while adhering to certain constraints. Currently, deep reinforcement learning (DRL)-based solutions are being widely used to solve the JSSP by defining the problem structure on disjunctive graphs. Some of the proposed approaches attempt to leverage the structural information of the JSSP to capture the dynamics of the environment without considering the time dependency within the JSSP. However, learning graph representations only from the structural relationship of nodes results in a weak and incomplete representation of these graphs which does not provide an expressive representation of the dynamics in the environment. In this study, unlike existing frameworks, we defined the JSSP as a dynamic graph to explicitly consider the time-varying aspect of the JSSP environment. To this end, we propose a novel DRL framework that captures both the spatial and temporal attributes of the JSSP to construct rich and complete graph representations. Our DRL framework introduces a novel attentive graph isomorphism network (Attentive-GIN)-based spatial block to learn the structural relationship and a temporal block to capture the time dependency. Additionally, we designed a gated fusion block that selectively combines the learned representations from the two blocks. We trained the model using the proximal policy optimization algorithm of reinforcement learning. Experimental results show that our trained model exhibits significant performance enhancement compared to heuristic dispatching rules and learning-based solutions for both randomly generated datasets and public benchmarks.},
  archive      = {J_EI},
  author       = {Gebreyesus, Goytom and Fellek, Getu and Farid, Ahmed and Hou, Sicheng and Fujimura, Shigeru and Yoshie, Osamu},
  doi          = {10.1007/s12065-024-00989-6},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Evol. Intell.},
  title        = {Deep reinforcement learning-based spatio-temporal graph neural network for solving job shop scheduling problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep metric learning with in-batch feature vector
constraints and unsupervised label integration. <em>EI</em>,
<em>18</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12065-024-00990-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning has increasingly captured the interest of the research community in recent years, mainly due to its effectiveness in integrating distance metric learning with deep neural networks. Despite the existence of various approaches, such as pair-based angular loss functions or spherical embedding constraints, these methods often necessitate extra trainable parameters and overlook class similarities. This paper presents two approaches: ‘In-batch feature vector constraint’ and ‘Unsupervised label integration.’ These methods are notable for considering the similarities between different classes. Because of their high compatibility, these techniques can be seamlessly integrated with various loss functions. Comprehensive experimental evaluations encompassing four image classification datasets and seven network architectures have demonstrated the effectiveness of these proposed methods in enhancing network performance.},
  archive      = {J_EI},
  author       = {Kim, Wonjik},
  doi          = {10.1007/s12065-024-00990-z},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Evol. Intell.},
  title        = {Deep metric learning with in-batch feature vector constraints and unsupervised label integration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary algorithm framework for optimizing truck
scheduling in multi-dock truck cross-docking centers. <em>EI</em>,
<em>18</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s12065-024-00992-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-docking optimizes logistics by reducing storage and handling times in warehouses, where cargos are unloaded from inbound trucks and loaded directly onto outbound vehicles. This paper addresses the critical challenge of minimizing makespan in a multi-dock, parallel machine setting within cross-docking centers. We propose a novel framework that integrates an evolutionary algorithm (EA) with a machine learning (ML) based hyperparameter tuning mechanism to optimize truck sequencing. This study fills the research gap by offering a quantifiable improvement over traditional heuristic methods, delivering up to a 37% improvement in the GAP metric compared to state-of-the-art techniques. It also achieves a 30% enhancement over the PCH constructive heuristic used for generating initial solutions. Additionally, our ML-based tuning strategy provides up to a 21% performance increase over static tuning methods. Notably, these improvements are attained while maintaining a competitive computational time as reported in the literature.},
  archive      = {J_EI},
  author       = {Nogueira, Thiago Henrique and Coutinho, Felipe Provezano and Peixoto, Maria Gabriela Mendonça and Carrano, Eduardo Gontijo and Ravetti, Martín Gómez},
  doi          = {10.1007/s12065-024-00992-x},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Evol. Intell.},
  title        = {Evolutionary algorithm framework for optimizing truck scheduling in multi-dock truck cross-docking centers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian mixture model and bayesian convolutional neural
network for abnormal musculoskeletal radiographs classification.
<em>EI</em>, <em>18</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12065-024-00993-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Musculoskeletal radiograph classification has been gaining great attention due to its important application in automated diagnosis of musculoskeletal disorders. Generally, musculoskeletal radiographs capture images of various bones in the whole body, which can be considered as a combination of multiple single datasets. Existing works have tried to build either a traditional neural network (NN), convolution neural network (CNN), or Bayesian convolutional neural network (BCNN) to achieve efficient classification. However, none of them explore the mixed-dataset property. In this context, we propose a novel Gaussian mixture model-based BCNN (MixBCNN) for the abnormal musculoskeletal radiographs classification problem. In the proposed method, the distribution of NN weights is represented as a Gaussian mixture distribution, expecting a new neural network model with better representation. In addition, we propose an ensemble learning approach to efficiently combine potential NN backbones. Performance evaluation is examined for the popular MURA dataset and shows the superiority of the proposed model with respect to state-of-the-art BCNN methods, notably by 0.86 in F1 score and 0.72 in Cohen’s kappa metrics while asking for negligible complexity overhead.},
  archive      = {J_EI},
  author       = {HoangVan, Xiem and Tran Van, Khoa},
  doi          = {10.1007/s12065-024-00993-w},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Evol. Intell.},
  title        = {Gaussian mixture model and bayesian convolutional neural network for abnormal musculoskeletal radiographs classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brhamo: Metaheuristic optimization algorithm for speech
emotion recognition using spectral and hybrid features. <em>EI</em>,
<em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12065-024-00994-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient speech emotion recognition (SER) is a crucial component in the development of natural and intuitive human-computer interaction systems. The problem addressed in this research is the challenge of achieving high accuracy in SER through optimized feature extraction, selection, and model tuning. The objective is to achieve more accurate speech emotion recognition through optimized feature extraction and selection, and the generation of an optimized random forest model. This paper presents a machine learning framework for speech emotion recognition based on Metaheuristic principles. The framework comprises two primary components: feature extraction and selection. Additionally, it generates an optimised random forest model by utilising the hybrid bat algorithm (HBA) to fine-tune the hyperparameters of the RF model, known as the Bat Random Forest Hybrid Meta Heuristic Algorithm (BRHAMO). Three speech corpora, including RAVDESS, SAVEE, and novel ANAKE Hindi speech corpus, have been utilized. The speech emotion recognition was subjected to a series of experiments and tests utilizing two unique features of speech characteristics, namely spectral features and the amalgamation of hybrid features. The experimental findings demonstrated BRHAMO based model achieved an accuracy of 81%, 79.6%, and 77.6% for the RAVDESS, SAVEE, and ANAKE datasets, respectively, in the spectral feature category. Furthermore, for the hybrid feature category, the RAVDESS, SAVEE, and ANAKE datasets achieved accuracy rates of 93.8%, 85.4%, and 89.8%, respectively. The performance of the BRHAMO, has been compared to several benchmark machine learning models, namely vanilla Random Forest, gradient boost, adaptive boost, and support vector machines. It is observed that the Meta Heuristic Algorithm (MHA) based approach can deliver better performance in terms of accuracy, precision, F1 score, and recall compared to all the individual classifiers in both categories.},
  archive      = {J_EI},
  author       = {Agrawal, Akshat and Jain, Anurag},
  doi          = {10.1007/s12065-024-00994-9},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Evol. Intell.},
  title        = {Brhamo: Metaheuristic optimization algorithm for speech emotion recognition using spectral and hybrid features},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intellectual model of pest detection and classification
using enhanced optimization-assisted single shot detector and graph
attention network. <em>EI</em>, <em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12065-024-00995-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying pests in the plants is considered to be an efficient, accurate, and sustainable way to improve the food quality and economic values in the agricultural industry. It equips farmers with cutting-edge instruments to safeguard their harvests, lessen their effect on the environment, and satisfy the growing need for sustenance on a worldwide scale. Deep learning can completely transform the identification of pests as it develops, protecting agriculture&#39;s production and resistance against ever-changing insect threats. It can be difficult to combine multiple methods for pest identification, including artificial intelligence, satellite imaging, and detectors. It is difficult to make sure that such innovations are available to farms and function well together. It&#39;s common for farmers as well as other agricultural personnel to require certification to properly utilize pest detection equipment and understand the findings. For these innovations to be widely adopted, the gap in understanding must be closed. Addressing these challenges requires an effective novel deep learning-based pest detection method. In this paper, a deep learning-based pest detection and classification model is developed to identify diseases caused by pests at an early stage. Initially, the images are collected from online datasets. Collected images underwent detection using the Adaptive Single Shot Detector (ASSD). To enhance the accuracy and efficiency of ASSD, the model parameters are fine-tuned by utilizing the Enhanced Running City Game Optimizer. This optimization process aims to improve the precision of pest detection. The classification of pests is done using the proposed Multi-scale and Dilated Graph Attention Network. This innovative network architecture is designed to categorize the pests effectively. The outcome of the proposed model is justified by comparing it with other classifiers and algorithms.},
  archive      = {J_EI},
  author       = {Vhatkar, Kapil Netaji},
  doi          = {10.1007/s12065-024-00995-8},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Evol. Intell.},
  title        = {An intellectual model of pest detection and classification using enhanced optimization-assisted single shot detector and graph attention network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing sand cat swarm optimization based on
multi-strategy mixing for solving engineering optimization problems.
<em>EI</em>, <em>18</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s12065-024-00996-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sand Cat Swarm Optimization (SCSO) algorithm is characterized by slow convergence speed and a tendency to become trapped in local optima. To address these issues, a hybrid multi-strategy variant known as HSCSO is proposed. The algorithm first employs an Elite Opposition-Based Learning strategy, which generates complementary candidate solutions, providing a more diverse set of initial solutions and improving the distribution of individual positions. Subsequently, a greedy strategy is utilized to combine the position update strategy of the golden sine algorithm, which has a fixed search pattern, with the position update strategy of the SCSO, which features random search characteristics. This combination retains the advantage of fast convergence while also providing a diverse set of random candidate solutions. Finally, a Random Inertia Weight is introduced to assist the SCSO algorithm in adaptively adjusting the search step size and optimization direction based on the different phases of the search process, thereby helping the algorithm effectively avoid local optima. The improved algorithm was compared with two classic original algorithms and six different algorithm variants on 25 benchmark functions. The experimental results indicate that HSCSO exhibits faster convergence and higher solution accuracy on most benchmark functions. Furthermore, the performance of HSCSO was evaluated on the CEC2019 and CEC2021 test suites, as well as several engineering optimization problems. The experimental results validate the competitiveness and superiority of the proposed HSCSO algorithm, demonstrating its potential for application in the field of global optimization.},
  archive      = {J_EI},
  author       = {Wang, Wen-chuan and Han, Zi-jun and Zhang, Zhao and Wang, Jun},
  doi          = {10.1007/s12065-024-00996-7},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Evol. Intell.},
  title        = {Enhancing sand cat swarm optimization based on multi-strategy mixing for solving engineering optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Draco lizard optimizer: A novel metaheuristic algorithm for
global optimization problems. <em>EI</em>, <em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12065-024-00998-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces a novel meta-heuristic optimization technique, termed the Draco lizard optimizer (DLO), grounded in the distinctive behaviors exhibited by the Draco lizard, particularly its proficient gliding mechanisms and adaptive ecological strategies. The DLO algorithm ingeniously translates these survival techniques into an efficient search paradigm, aimed at resolving intricate global optimization problems. To assess the optimization efficacy of the DLO algorithm, we embarked on a comparative experimentation framework, incorporating 29 benchmark functions sourced from the prestigious CEC2017 test suite, alongside a selection of five cutting-edge metaheuristic algorithms. The analysis of the experimental outcomes highlights the advantages of the DLO algorithm, which achieved an average ranking of 1.62 on the CEC2017 benchmark set, placing it in the top position. Additionally, the average computational time of the DLO algorithm is one-third that of the second-ranked CPO algorithm, confirming that the DLO algorithm not only possesses exceptional global search performance but also demonstrates higher search efficiency compared to contemporary optimization algorithms. Source code for DLO algorithm: https://github.com/XiaoweiWang-ai/DLO},
  archive      = {J_EI},
  author       = {Wang, Xiaowei},
  doi          = {10.1007/s12065-024-00998-5},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Evol. Intell.},
  title        = {Draco lizard optimizer: A novel metaheuristic algorithm for global optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced CEEMD hybrid model for VIX forecasting: Optimized
decision trees and ARIMA integration. <em>EI</em>, <em>18</em>(1), 1–12.
(<a href="https://doi.org/10.1007/s12065-024-00984-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on the time series analysis and forecasting of the Volatility Index (VIX) data in the United States. We employed the Complete Empirical Mode Decomposition (CEEMD) method to decompose the original VIX data into seven Intrinsic Mode Functions (IMFs). Subsequently, each IMF determined its stationarity using the Augmented Dickey-Fuller (ADF) test. For stationary IMFs, an AutoRegressive Integrated Moving Average (ARIMA) model was built, while non-stationary IMFs were fitted using machine learning regression algorithms. The sum of the predicted values of all IMFs was considered as the forecast VIX value. About the parameter selection in machine learning algorithms, we employed the Optuna library to optimize the model’s parameters. We selected the parameters that resulted in the lowest Mean Squared Error (MSE) on a validation set to be the optimal model parameters. Applying this methodology, we found that our hybrid model has greater predictive capabilities compared to simple machine learning models when the market fluctuates greatly. Specifically, the AdaBoost model outperformed a basic decision tree model and a random forest model in terms of MSE and MAE (mean absolute error) accuracy.},
  archive      = {J_EI},
  author       = {Liang, Zhuqin and Ismail, Mohd Tahir},
  doi          = {10.1007/s12065-024-00984-x},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Evol. Intell.},
  title        = {Advanced CEEMD hybrid model for VIX forecasting: Optimized decision trees and ARIMA integration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metaheuristic optimization and strategic behavior of
markovian vacation queue with retrial policy: Application to virtual
call center. <em>EI</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s12065-024-00987-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigation is concerned with social optimization and customers’ strategic behavior for a double orbit retrial queueing model with vacation, aiming to enhance the performance of virtual call centers. In many call center scenarios, if the server is busy, the arriving customer moves to premium/ordinary orbit, i.e., becomes a repeated customer; otherwise, if the server is accessible, the arriving customer joins the system to receive the required service. Once the service is completed, the server will look into the premium orbit to check whether there is any customer who needs service. If no new customer from premium/ordinary orbit or outside arrives and the system is empty, then the server takes a vacation. The customer’s decision to wait or balk from the system depends on the server’s status and the reward for receiving the service. By using a probability generating function and iterative approach, the long-run probability distribution of the queue size and other metrics, viz. equilibrium thresholds, entering probability, etc., have been obtained. Moreover, the social welfare function is analyzed based on two given information levels. The optimal solution is presented by solving the social welfare maximization problem using particle swarm optimization and harmony search techniques. The impact of different parameters on the performance metrics in Virtual Call Centers (VCC) is examined.},
  archive      = {J_EI},
  author       = {Dhibar, Sibasish and Jain, Madhu},
  doi          = {10.1007/s12065-024-00987-8},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Evol. Intell.},
  title        = {Metaheuristic optimization and strategic behavior of markovian vacation queue with retrial policy: Application to virtual call center},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising SMIB system stability: FOPID controller tuning
via harris hawks optimisation. <em>EI</em>, <em>18</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12065-024-00991-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current power system stabilizer designs, including fuzzy-PID and Lead-Lag compensators, need help with adaptability and complexity in diverse and evolving power system environments. Conventional tuning methods like the Newton–Raphson approach and optimization strategies like particle swarm optimization, genetic algorithms, and cuckoo search algorithms face challenges in achieving optimal performance for Fractional Order Proportional Integral Derivative (FOPID) controllers. There is a critical need for innovative tuning methods that offer enhanced adaptability and performance in complex power system stability analysis. This research contributes to the advancement of power system stability analysis, specifically in SMIB systems, offering insights into optimizing FOPID controllers utilizing the innovative Harris Hawks Optimization (HHO) algorithm. The work expects these findings to broaden the implications for power system control and enhance the stability of Single Machine Infinite Bus (SMIB) systems, thus fostering the resilience and reliability of modern electrical infrastructure. The FOPID controller encompasses fractional order parameters, encompassing proportional, integral, and derivative gain, integral order, and derivative order, each exerting a substantial influence on control responses and stability. This research harnesses HHO, an optimization technique inspired by nature, to fine-tune FOPID parameters. The investigation involves initializing the SMIB model, formulating an objective function to minimize control errors, and employing HHO iteratively to refine the FOPID controller. The outcomes reveal enhanced stability, diminished overshoot, accelerated settling time, and transient response.},
  archive      = {J_EI},
  author       = {Kirange, Yogesh Kalidas and Nema, Pragya},
  doi          = {10.1007/s12065-024-00991-y},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Evol. Intell.},
  title        = {Optimising SMIB system stability: FOPID controller tuning via harris hawks optimisation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary multitasking algorithm based on a dynamic
solution encoding strategy for the minimum s-club cover problem.
<em>EI</em>, <em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s12065-024-00999-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding a cohesive subgraph, notably the s-club model, which is a subgraph with diameter at most s, is a widely applied topic in social network analysis and group of objects modeling. In particular, the minimum s-club cover problem (min s-club cover) is a recently introduced variant in the literature which asks to cover the vertices of a graph with a minimum number of s-clubs. The existence of common connections among these highly connected components encourages the application of multitasking optimization to leverage the shared meaningful knowledge in the discovery of multiple s-clubs at the same time. Therefore, this study proposes a multitasking evolutionary algorithm to solve the minimum s-club cover problem. Our proposal is designed with an effective solution representation method and evolutionary operators for the variation in the number of clubs. Each solution is represented by two components, where the gene number of a component can be different for each individual and can change during performing evolutionary operations. We also propose a solution generation method based on a random greedy algorithm that helps to ensure individual quality and population diversity in the initial population. The proposed algorithm is evaluated on two datasets in the DIMACS library. This study then analyzed the influence of different factors of the input data on the proposed algorithm results. Based on statistical analysis of the performance results, it is clear that our proposal’s solution is superior to an existing algorithm on two-thirds of the experimental data set.},
  archive      = {J_EI},
  author       = {Thanh, Pham Dinh and Long, Nguyen Binh and Vinh, Le Sy and Binh, Huynh Thi Thanh},
  doi          = {10.1007/s12065-024-00999-4},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Evol. Intell.},
  title        = {Evolutionary multitasking algorithm based on a dynamic solution encoding strategy for the minimum s-club cover problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient plant disease prediction model based on machine
learning and deep learning classifiers. <em>EI</em>, <em>18</em>(1),
1–33. (<a href="https://doi.org/10.1007/s12065-024-01000-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture is the majorsource of the nation&#39;s economic growth, but the emergence of certain plant-related diseases has a negative effect on the sector&#39;s production. Diagnosing plant diseases is crucial to solving this issue, teaching farmers how to prevent diseases, and implementing effective management. The total yield may be negatively impacted by diseases if they are not discovered early, which would reduce the farmer&#39;s profits. Numerous researchers have presented various cutting-edge systems based on numerous techniques to address this issue. Numerous approaches have been used already by researchers for this purpose, but some techniques relating to vision have not yet been investigated. Six cutting-edge models for plant disease detection based on machine learning and deep learning were briefly deliberated in this research to analyze the efficiency of each approach. In this research, the Resnet-101 model is used to efficiently extract the feature, and the accuracy, sensitivity, and specificity of the metrics are measured to demonstrate the efficiency of each model. Recent research has shown average accuracies of 94.38%, 94.785, and 92.45% in the detection of Apple, potato, and strawberry diseases respectively.},
  archive      = {J_EI},
  author       = {Shinde, Nirmala and Ambhaikar, Asha},
  doi          = {10.1007/s12065-024-01000-y},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Evol. Intell.},
  title        = {An efficient plant disease prediction model based on machine learning and deep learning classifiers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient multilevel thresholding image segmentation
through improved elephant herding optimization. <em>EI</em>,
<em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12065-024-01001-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed an improved version of recently developed swarm-based metaheuristic algorithm elephant herding optimization (EHO) called Improved Elephant Herding Optimization (IEHO). In this IEHO, the opposition-based learning (OBL) rule and chaos-embedded sequences are incorporated with each iterative stage of EHO to maintain the proper balance between the exploration and exploitation phase. It regulates the movement of the search agents and avoids premature convergence. The effectiveness of the proposed model is evaluated in terms of finding the optimal threshold value in Multilevel thresholding (MTH) of image segmentation which separates the different objects of the images. The methods such as the Kapur entropy, Otsu and masi entropy are used as the objective function in this problem to determine the optimal threshold. The proposed IEHO’s performance is compared with the different variants of EHO, artificial bee colony (ABC) and artificial hummingbird algorithms (AHA). The simulation results regarding convergence speed, stability, and solution quality performance indicators, such as the structural similarity index (SSIM), feature similarity index (FSIM), and peak signal-to-noise ratio (PSNR) verify the viability of the above hybrid algorithm.},
  archive      = {J_EI},
  author       = {Chakraborty, Falguni and Roy, Provas Kumar},
  doi          = {10.1007/s12065-024-01001-x},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Evol. Intell.},
  title        = {An efficient multilevel thresholding image segmentation through improved elephant herding optimization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of accuracy and computational
complexity across 21 swarm intelligence algorithms. <em>EI</em>,
<em>18</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s12065-024-00997-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear, complex optimization problems are prevalent in many scientific and engineering fields. Traditional algorithms often struggle with these problems due to their high dimensionality and intricate nature, making them time-consuming. Many researchers have proposed new metaheuristic algorithms inspired by biological behaviors in nature, which comparatively show higher performance and accuracy than traditional optimization algorithms. Nature-inspired algorithms, particularly those based on swarm intelligence, offer adaptable and efficient solutions to these challenges. In recent years, swarm intelligence algorithms have made significant advancements. Classical and CEC benchmark suits are immersively useful for studying the performance of optimization algorithms. According to our literature survey, we identified that many algorithms were evaluated based on accuracy. Currently, swarm intelligence algorithms are used in many applications, and efficiency and computational complexity need to be evaluated. A broad-level study of the computational complexity and accuracy of popular swarm intelligence algorithms has not been done recently. Therefore this study we comprehensively evaluate and compare 21 bio-inspired swarm intelligence algorithms on eight non-separable unimodal, eight separable unimodal, five non-separable multimodal, seven separable multimodal functions, and two CEC 2018 many objective functions. We study the structure and mathematical model of the selected algorithms. Then we categorized selected algorithms into six different behavioral groups. We calculated the root mean square error between expected and actual values. Then we performed an RMSE cross-validation statistical test to understand how accurately an algorithm resolves an average problem. We found that Artificial Lizard Search Optimization (ALSO) is the most prominent algorithm in accuracy and efficiency. Besides that, Cat Swarm Optimization (CSO), Squirrel Search Algorithm (SSA), and Chimp Optimization Algorithm (CHOA-B) are also considered more universal algorithms. The Squirrel Search Algorithm (SSA) is ALSO’s second-best algorithm in time complexity. Wasp Swarm Algorithm (WSO), and Bat-Inspired Algorithm (BA) presented the lowest time complexity. Finally, several important issues and research directions are discussed.},
  archive      = {J_EI},
  author       = {Warnakulasooriya, Kolitha and Segev, Aviv},
  doi          = {10.1007/s12065-024-00997-6},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Evol. Intell.},
  title        = {Comparative analysis of accuracy and computational complexity across 21 swarm intelligence algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Student psychology based optimization algorithm integrating
differential evolution and hierarchical learning for solving data
clustering problems. <em>EI</em>, <em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12065-024-01003-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to accurately and efficiently perform data clustering in complex multidimensional data analysis and processing tasks is a challenging research problem. Traditional optimization algorithms often need help with the problems of quickly falling into local optimum and insufficient global search ability when dealing with high-dimensional, multi-peaked and complex structured data. In order to solve this challenge, a student psychology based optimization algorithm (GDLSPBO) that integrates differential evolution and hierarchical learning mechanisms was proposed, aiming to improve the accuracy, stability and global optimization ability of data clustering. GDLSPBO enhances the population diversity and prevents the algorithm from falling into local optimum by introducing the differential evolution mechanism. Simultaneously, the hierarchical learning strategy improves the algorithm’s search efficiency and local optimization ability. The experiments are validated on the CEC-BC-2017 benchmark functions. Several real datasets and the results show that GDLSPBO achieves an F-measure of 0.9595 and an Adjusted Rand coefficient of 0.8578 on the Cancer dataset, and the clustering accuracy on the Iris dataset reaches 93.33%, which is significantly better than that of other classical optimization algorithms. This indicates that GDLSPBO has a more substantial clustering effect and higher solution accuracy in solving complex data clustering problems. The experimental results verify that the global search ability and optimization accuracy of GDLSPBO on multidimensional complex data sets have been significantly improved, demonstrating its broad applicability and robustness in practical data clustering applications.},
  archive      = {J_EI},
  author       = {Bao, Yin-Yin and Wang, Jie-Sheng and Liu, Jia-Xu and Zhao, Xiao-Rui and Yang, Qing-Da and Zhang, Shi-Hui},
  doi          = {10.1007/s12065-024-01003-9},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Evol. Intell.},
  title        = {Student psychology based optimization algorithm integrating differential evolution and hierarchical learning for solving data clustering problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast implementation of extreme learning machine-based
directRanker for surrogate-assisted evolutionary algorithms.
<em>EI</em>, <em>18</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12065-024-01005-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted evolutionary algorithms (SAEAs) have been widely used to solve computationally expensive optimization problems. The extreme learning machine-based DirectRanker (ELDR) is a single-layer feed-forward neural network surrogate model designed for SAEAs. ELDR estimates the superiority of two solutions with a high estimation accuracy, even in high-dimensional problems. However, ELDR requires a long computation time as the problem dimensionality and the number of hidden neurons increase, thus making it difficult to apply it to high-dimensional problems. A surrogate model should be computationally efficient and enable rapid fitness estimations. Therefore, this paper proposes a fast implementation technique, i.e., fast version ELDR (fELDR) that achieves mathematically equivalent learning results with low computational complexity. Additionally, this paper proposes a pointwise score function to render the prediction results reusable. The experimental results confirmed the effectiveness of fELDR when compared with the original ELDR. The learning results of the proposed fELDR were equivalent to those of the original ELDR while reducing the training time by up to 97%, especially when using a large hidden layer on a high dimensionality problem. Moreover, due to the reusable prediction results, the computation time of the fELDR-assisted SAEA can be further decreased by 79.5% when compared with that of the original ELDR-assisted SAEA. The reduced training time and reusable prediction results of fELDR render it feasible to apply ELDR to high-dimensional optimization problems and realize a high prediction accuracy with a large number of hidden neurons.},
  archive      = {J_EI},
  author       = {Harada, Tomohiro},
  doi          = {10.1007/s12065-024-01005-7},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Evol. Intell.},
  title        = {Fast implementation of extreme learning machine-based directRanker for surrogate-assisted evolutionary algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy enhanced artificial rabbit optimization
algorithm for solving engineering optimization problems. <em>EI</em>,
<em>18</em>(1), 1–54. (<a
href="https://doi.org/10.1007/s12065-024-01002-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the limitations of the artificial rabbit optimization (ARO) algorithm in handling complex and large-scale optimization problems, this paper proposes an enhanced multi-strategy algorithm—MARO. The proposed algorithm integrates several strategies, including elite opposition-based learning, Levy flight, random walk, and adaptive weight factors, to significantly improve the search capability and convergence accuracy of ARO. First, the algorithm applies the elite opposition-based learning strategy in each iteration to generate opposite solutions, which enriches population diversity and enhances global search capacity, effectively preventing the algorithm from becoming trapped in local optima. Second, it incorporates Levy flight and random walk strategies to increase search randomness during the exploration phase, thus improving the probability of finding the global optimum. Finally, the adaptive weight factor is introduced during the exploitation phase to dynamically adjust the search direction based on different stages, achieving a balance between global exploration and local exploitation. Experimental results demonstrate that MARO consistently outperforms ARO and other popular algorithms across multiple international benchmark test suites. On the CEC2005 test functions, MARO shows significant improvements over CMAES and LSHADE on 9 functions. For the CEC2017 suite, MARO achieves the best average ranking on nearly three-quarters of the test functions, with an average rank of 1.38, significantly outperforming CMAES (3.48) and LSHADE (3.52). On the CEC2019 suite, MARO ranks first in 70% of the test functions. Statistical significance tests, including the Friedman and Wilcoxon tests, confirm that MARO&#39;s p-values are less than 0.05 when compared with benchmark algorithms, validating the algorithm&#39;s effectiveness. Moreover, MARO also exhibits outstanding optimization performance in real-world engineering problems such as pressure vessel design and reservoir scheduling optimization. The main contribution of this paper lies in the introduction of an innovative multi-strategy optimization framework, which enhances both exploration and exploitation capabilities. This framework effectively addresses the shortcomings of existing ARO algorithms in handling complex problems, providing a more competitive solution for practical optimization tasks.},
  archive      = {J_EI},
  author       = {He, Ni-ni and Wang, Wen-chuan and Wang, Jun},
  doi          = {10.1007/s12065-024-01002-w},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-54},
  shortjournal = {Evol. Intell.},
  title        = {Multi-strategy enhanced artificial rabbit optimization algorithm for solving engineering optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified random-oppositional chaotic artificial rabbit
optimization algorithm for solving structural problems and optimal
sizing of hybrid renewable energy system. <em>EI</em>, <em>18</em>(1),
1–63. (<a href="https://doi.org/10.1007/s12065-024-01004-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial rabbit optimization (ARO) algorithm replicates the survival skills of rabbits in the wild. However, like other metaheuristic approaches it possesses significant drawbacks in solving challenging problems, including sluggish convergence rate, poor exploration ability and trapped in local optima region. To alleviate these shortcomings, a novel strategy, namely Modified Random Opposition (MRO) and ten chaotic maps are integrated with ARO, termed as MROCARO. This implementation MRO technique boost the population diversity and permits the population to escape from local optima while integration of chaotic map enhances the exploitation capability. To estimate the effectiveness of the MROCARO method, the well-known CEC2005, CEC2017, CEC2019 and CEC2008lsgo test functions are considered. Moreover, non-parametric tests that include the Wilcoxon rank-sum and Friedman rank test are performed to analyze the significant difference among the compared algorithms. Furthermore, the efficiency of the MROCARO algorithm has been evaluated on various structural problems and optimal sizing of renewable energy systems. The experimental findings demonstrate that MROCARO performed optimum solution with 100% renewable sources with the lowest levelized cost of electricity of 0.0934 $/kWh as compared to other methods. Also, the simulation findings reveal that MROCARO has immense potential for addressing global optimization and structural problems as contrasted to other competing algorithms.},
  archive      = {J_EI},
  author       = {Mohapatra, Sarada and Lala, Himadri and Mohapatra, Prabhujit},
  doi          = {10.1007/s12065-024-01004-8},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-63},
  shortjournal = {Evol. Intell.},
  title        = {Modified random-oppositional chaotic artificial rabbit optimization algorithm for solving structural problems and optimal sizing of hybrid renewable energy system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning with industrial robots: Exploring the
impact of joint angles on cartesian coordinates using explainable AI.
<em>EI</em>, <em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12065-024-01006-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study uses Explainable Artificial Intelligence techniques to reveal the complex relationship between joint angles and Cartesian coordinates in the context of industrial robotic arms. By using machine learning and Explainable Artificial Intelligence algorithms, it is aimed to distinguish the dominant effect of individual joint angles on the x, y and z coordinates of the robotic end effector. Various machine learning algorithms have been applied on the data set and performance outputs have been obtained. According to these performance results, it has been observed that the RandomForest algorithm is more suitable for our study than other models with its low mean square error and high r-square score. Along with the selected machine learning algorithm, the data set was tried to be explained by passing it through SHapley Additive exPlanations (SHAP), Descriptive Machine Learning EXplanations (DALEX) and Explain Like I’m 5 (ELI5) models, which are Explainable Artificial Intelligence models. It has been observed that the SHAP model explains the effects of joint angles on Cartesian coordinates more consistently than other models, with an average sensitivity of 0.0125 value range.Our findings shed light on the explainability aspect of AI models and provide valuable information about the fundamental mechanisms governing the complex movements of industrial robot arms.},
  archive      = {J_EI},
  author       = {Özkurt, Cem},
  doi          = {10.1007/s12065-024-01006-6},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Evol. Intell.},
  title        = {Machine learning with industrial robots: Exploring the impact of joint angles on cartesian coordinates using explainable AI},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability-based multi-objective optimization of trusses
with greylag goose algorithm. <em>EI</em>, <em>18</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s12065-024-01011-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a multi-objective variant of the Greylag Goose Optimizer (MOGGO) to tackle complex structural optimization problems. Inspired by the cooperative behavior of geese in flight, MOGGO employs dynamic grouping to enhance problem-solving efficiency. Six truss structures undergo simultaneous topology, shape, and size optimization using MOGGO, aiming to maximize reliability while minimizing structural mass. By incorporating non-dominance sorting and archiving techniques, MOGGO extends the single-objective Greylag Goose Optimizer to effectively address trade-offs between competing objectives. Evaluation metrics and statistical tests demonstrate MOGGO&#39;s superior performance in handling large structural optimization problems, preserving more Pareto-optimal sets, and achieving greater convergence and variance in objective and decision spaces. MOGGO’s ability to manage conflicting objectives is further validated through diversity analysis, with swarm plots illustrating its superior convergence behavior across iterations. Overall, MOGGO proves to be an efficient and effective approach for addressing challenging reliability-based multi-objective structural optimization problems. Query ID=&quot;Q1&quot; Text=&quot;Please confirm if the author names are presented accurately and in the correct sequence (given name, middle name/initial, family name). Author 1 Given name: [specify authors given name] Last name [specify authors last name]. Also, kindly confirm the details in the metadata are correct.&quot;},
  archive      = {J_EI},
  author       = {Mashru, Nikunj and Tejani, Ghanshyam G. and Patel, Pinank},
  doi          = {10.1007/s12065-024-01011-9},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Evol. Intell.},
  title        = {Reliability-based multi-objective optimization of trusses with greylag goose algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing class imbalance in remote sensing using deep
learning approaches: A systematic literature review. <em>EI</em>,
<em>18</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s12065-024-01012-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is one of the major issues for the application of deep learning for remote sensing imagery. High-resolution remote sensing image sample sets are prone to cause the problem of sample imbalance among classes due to the skewed distribution of ground features, which has been a huge challenge to machine learning and data mining and aroused strong attention. Despite the emerging interest in Deep Learning (DL), empirical research on its effectiveness with imbalanced data in remote sensing remains scarce. Therefore, this study aims to systematically review existing studies using DL approaches for handling class imbalanced data in the field of remote sensing, focusing on its significance in critical applications such as land use land cover classification, pixel-wise segmentation, and object detection. This study presents the most widely used balancing algorithms published from 2016 to 2024. This survey divulges that while using DL technologies, either the data augmentation can be applied to the minority class images to generate more varied samples or using algorithm-level methods by incorporating modifications to the loss function such as cross-entropy, focal loss, dice loss by assigning higher weights to minority class samples, ensuring the model pays more attention to underrepresented classes during training. While traditional techniques such as data sampling and cost-sensitive learning continue to be relevant, emerging methods such as GAN, which leverage neural network feature learning capabilities, show promise. Our discussion identifies research gaps and provides insights to guide future efforts in utilizing DL for imbalanced data in remote sensing applications.},
  archive      = {J_EI},
  author       = {Sharma, Shweta and Gosain, Anjana},
  doi          = {10.1007/s12065-024-01012-8},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Evol. Intell.},
  title        = {Addressing class imbalance in remote sensing using deep learning approaches: A systematic literature review},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for obstacle-range estimation based on
echo-location using high-end sound with applications. <em>EI</em>,
<em>18</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s12065-024-01014-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient mapping algorithms have been implemented in different fields of robotics applications. Some of those applications are in the mapping of environments with difficult accessibility, the planning of service or emergency assistant robots, or as supporting tools in helping people with disabilities. In those examples, the essential task of a mapping algorithm is to detect obstacles which may interfere in motion paths. One of the most widely used characteristics in pulse-echo range estimation is the time of flight. However, the ultrasonic frequencies (range from 30 to 120 KHz) are poorly suitable for the long distances faced in in-door applications. In view of the importance of developing efficient mapping algorithms, the present manuscript reports on an efficient method based on high-frequency waves in order to range static objects under a controlled environment. In the present approach, the signal acquisition is made out-of-band of the processing algorithm. This process is carried out by using an active speaker to throw the control ping sound. The resulting audio is captured through a unidirectional dynamic microphone Shure® SV100. The audio signal processing is based on selective filtering via Gabor models and wavelet decimation de-noising. The proposed method is compared against two common methodologies for range estimation in robotics based on ultrasonic and infrared sensor. The current methodology demonstrated a competitive and, in some cases, better performance than the standard methods.},
  archive      = {J_EI},
  author       = {Guerrero-Díaz-de-León, J. Antonio and Hernández-Torres, Rubén and Macías, Siegfried and Macías-Díaz, Jorge E.},
  doi          = {10.1007/s12065-024-01014-6},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Evol. Intell.},
  title        = {A method for obstacle-range estimation based on echo-location using high-end sound with applications},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent approach for analyzing the effects of normal
tumor immune unhealthy diet model through unsupervised physics informed
neural-networks integrated with meta-heuristic algorithms. <em>EI</em>,
<em>18</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s12065-024-01007-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the development of integrated intelligent computing using physics-informed neural networks to solve the mathematical normal-tumor immune-unhealthy diet model. The model considers vitamin intervention as a moderating factor within one day. A nonlinear activation function called sigmoid was used for the model across three different scenarios to define the fitness or error function. For computing the optimized biases and weights of physics-informed neural networks, hybridization of heuristic algorithms, particularly particle swarm optimization and neural networks algorithm are employed from $$-10$$ to $$10$$ . The proposed technique&#39;s accuracy, reliability, and validity are demonstrated by consistently matching the results obtained using NDsolve from mathematics built-in function as a reference solution. Additionally, statistical analyses including absolute and mean squared errors are conducted to confirm further the precision and accuracy of the physics-informed neural networks. It is observed that the absolute errors and mean square error between the reference solution and proposed technique range from $${10}^{-2}$$ to $${10}^{-9}$$ and range from $${10}^{-6}$$ to $${10}^{-11}$$ for distinct cases. The novelty of this study highlights the importance of physics-informed neural networks for a balanced diet rich in vitamins for reducing the risk of deadly diseases, particularly cancer and the potential of machine learning algorithms in modelling and analyzing complex biological systems.},
  archive      = {J_EI},
  author       = {Aslam, Muhammad Naeem and Shaukat, Nadeem and Arshad, Muhammad Sarmad and Aslam, Muhammad Waheed and Hussain, Javed},
  doi          = {10.1007/s12065-024-01007-5},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Evol. Intell.},
  title        = {An intelligent approach for analyzing the effects of normal tumor immune unhealthy diet model through unsupervised physics informed neural-networks integrated with meta-heuristic algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEL: A strategy for resolving redundancy in entity pairs
within dual entity linker for relational triple extraction. <em>EI</em>,
<em>18</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s12065-024-01008-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the mainstream methods for relational triple extraction mainly employ joint models and have achieved significant results. However, these methods still face challenges in dealing with the complexity of unstructured text. In joint models based on labeled relational triple extraction, the BiRTE model suffers from the issue of redundant entity pairs, leading to the generation of incorrect triples. To address this issue, This paper introduces a Ground Entity Extractor which is designed to aid the Dual Entity Linker (DEL), in addition, adversarial training methods are introduced during the DEL training process. Experimental results demonstrate that the DEL model extracts entity pairs from two directions, when compared to the previous BIRTE model, it generates more accurate entity pairs. On the WebNLG dataset, The accuracy increased by 2.9%, and F1 improved by 1.1%. On the NYT10 dataset, accuracy increased by 1.0%, recall increased by 0.5%, and F1 score improved by 0.7%. This brings significant enhancements to the triple entity extraction task, and in comparison with 8 baseline models across all datasets, the DEL model achieves better results.},
  archive      = {J_EI},
  author       = {Hu, Songhua and Wang, Hengxin},
  doi          = {10.1007/s12065-024-01008-4},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Evol. Intell.},
  title        = {DEL: A strategy for resolving redundancy in entity pairs within dual entity linker for relational triple extraction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BIM model generation and protection: An automated solution
under deep learning and differential privacy. <em>EI</em>,
<em>18</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12065-024-01009-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the problems of low accuracy, low efficiency, and insufficient data security in building information modeling models, an automation scheme based on deep learning and differential privacy is introduced to improve the efficiency and security of construction projects. Firstly, this article utilized residual network and long short-term memory models to extract features from the spatial structure and temporal information of the building information modeling model. Secondly, it used the Adam optimizer for gradient descent to optimize network weights; then, for the evaluation of geometric accuracy and data integrity, this article adjusted the number of network layers and convolution kernel size. At the same time, this article integrated a differential privacy framework into the model, dynamically adjusted the privacy budget, and finally evaluated the building information modeling model for automated generation and protection. The research results indicated that the building information modeling model had an accuracy of 0.94 in aligning the edges of the structural framework, and the internal partition wall angle deviation was only 0.10. Its error in the area covered by internal partitions and floors was only 0.04, with a success rate of 0.82 for data leakage risk, a detection rate of 0.91, and a response time of 0.09 s. The model can ensure the security of critical information while achieving high accuracy and efficiency.},
  archive      = {J_EI},
  author       = {Zhang, Xuewei},
  doi          = {10.1007/s12065-024-01009-3},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Evol. Intell.},
  title        = {BIM model generation and protection: An automated solution under deep learning and differential privacy},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic PFRCosSim layer for solving filter redundancy
problem in CNNs applied on plant disease classification. <em>EI</em>,
<em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12065-024-01010-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved remarkable success in various artificial intelligence domains, particularly in pattern recognition, image processing, and speech recognition. However, the growing complexity of these models introduces challenges related to parameter redundancy, significantly impacting CNN performance. This paper addresses the issue of increasing parameter redundancy, focusing on the specific problem of filter redundancy during CNN training. The proposed approach involves regularization of the initialization filters to reduce redundancy at each convolutional layer. A novel layer, PFRCosSim, is introduced, computing the Cosine similarity between filters used in CNN training to ensure filter homogeneity. we reset the filters using an Orthogonal initialization based on the random choice of kernels concerning all filters in the same layers. The method is tested on a three-layer CNN model and extended to common architectures like LeNet and VGG16. Validation is performed through plant disease classification using datasets: Plant Pathology 2020 and Plant Disease Recognition. The application of this approach yields significant accuracy improvements, exceeding $$99 \%$$ .},
  archive      = {J_EI},
  author       = {Lagnaoui, Saloua and En-naimani, Zakariae and Haddouch, Khalid},
  doi          = {10.1007/s12065-024-01010-w},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Evol. Intell.},
  title        = {Stochastic PFRCosSim layer for solving filter redundancy problem in CNNs applied on plant disease classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid PSO-GA optimization for enhancing decision tree
performance in soil classification and crop cultivation prediction.
<em>EI</em>, <em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12065-024-01015-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture holds profound significance in the lives and livelihoods of Bangladesh’s population. The escalating population has contributed to a reduction in available arable land, exacerbating concerns about the feasibility of farming. In various regions of Bangladesh, there is a prevailing perception that certain areas are unsuitable for cultivation. Consequently, a substantial amount of land still needs to be tapped and explored for agricultural purposes, contributing to underutilization and hindering potential agricultural development in these regions. Considering these issues, this study seeks to forecast the optimal crop choices for specific soil classes, enabling individuals to make more informed decisions about crop cultivation on their land. Initially, the soil class is identified based on its unique characteristics within a given area, and subsequently, crop selection is determined based on these distinct soil classes. This study explores the performance of Particle Swarm Optimization (PSO) and Genetic Algorithm (GA). It proposes a hybrid PSO-GA technique to optimize the Decision Tree model for forecasting crop cultivation based on classifying soil. The performance of a standalone decision tree model is also measured. In soil classification, the hybrid PSO-GA approach demonstrates superior performance, achieving an accuracy of 96.04%, surpassing individual PSO, GA, and standalone decision tree methods. In crop cultivation prediction, the hybrid approach outperforms individual PSO, GA, and decision tree methods with an accuracy of 92.63%. The results highlight the efficacy of the integrated PSO-GA strategy in optimizing the DT model for precise agricultural predictions. This research contributes valuable insights for enhancing decision support systems in agriculture, providing a promising avenue for improved accuracy in soil classification and crop cultivation prediction.},
  archive      = {J_EI},
  author       = {Rahman, Fardowsi and Khan, Md. Ashikur Rahman and Alam, Mahbubul},
  doi          = {10.1007/s12065-024-01015-5},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Evol. Intell.},
  title        = {Hybrid PSO-GA optimization for enhancing decision tree performance in soil classification and crop cultivation prediction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="focm---8">FoCM - 8</h2>
<ul>
<li><details>
<summary>
(2025). Strong norm error bounds for quasilinear wave equations
under weak CFL-type conditions. <em>FoCM</em>, <em>25</em>(1), 303–350.
(<a href="https://doi.org/10.1007/s10208-024-09639-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present paper, we consider a class of quasilinear wave equations on a smooth, bounded domain. We discretize it in space with isoparametric finite elements and apply a semi-implicit Euler and midpoint rule as well as the exponential Euler and midpoint rule to obtain four fully discrete schemes. We derive rigorous error bounds of optimal order for the semi-discretization in space and the fully discrete methods in norms which are stronger than the classical $$H^1\times L^2$$ energy norm under weak CFL-type conditions. To confirm our theoretical findings, we also present numerical experiments.},
  archive      = {J_FoCM},
  author       = {Dörich, Benjamin},
  doi          = {10.1007/s10208-024-09639-w},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {303-350},
  shortjournal = {Found. Comput. Math.},
  title        = {Strong norm error bounds for quasilinear wave equations under weak CFL-type conditions},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exotic b-series and s-series: Algebraic structures and order
conditions for invariant measure sampling. <em>FoCM</em>,
<em>25</em>(1), 271–301. (<a
href="https://doi.org/10.1007/s10208-023-09638-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {B-Series and generalizations are a powerful tool for the analysis of numerical integrators. An extension named exotic aromatic B-Series was introduced to study the order conditions for sampling the invariant measure of ergodic SDEs. Introducing a new symmetry normalization coefficient, we analyze the algebraic structures related to exotic B-Series and S-Series. Precisely, we prove the relationship between the Grossman–Larson algebras over exotic and grafted forests and the corresponding duals to the Connes–Kreimer coalgebras and use it to study the natural composition laws on exotic S-Series. Applying this algebraic framework to the derivation of order conditions for a class of stochastic Runge–Kutta methods, we present a multiplicative property that ensures some order conditions to be satisfied automatically.},
  archive      = {J_FoCM},
  author       = {Bronasco, Eugen},
  doi          = {10.1007/s10208-023-09638-3},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {271-301},
  shortjournal = {Found. Comput. Math.},
  title        = {Exotic B-series and S-series: Algebraic structures and order conditions for invariant measure sampling},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational complexity of decomposing a symmetric matrix
as a sum of positive semidefinite and diagonal matrices. <em>FoCM</em>,
<em>25</em>(1), 223–269. (<a
href="https://doi.org/10.1007/s10208-023-09637-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study several variants of decomposing a symmetric matrix into a sum of a low-rank positive-semidefinite matrix and a diagonal matrix. Such decompositions have applications in factor analysis, and they have been studied for many decades. On the one hand, we prove that when the rank of the positive-semidefinite matrix in the decomposition is bounded above by an absolute constant, the problem can be solved in polynomial time. On the other hand, we prove that, in general, these problems as well as their certain approximation versions are all NP-hard. Finally, we prove that many of these low-rank decomposition problems are complete in the first-order theory of the reals, i.e., given any system of polynomial equations, we can write down a low-rank decomposition problem in polynomial time so that the original system has a solution iff our corresponding decomposition problem has a feasible solution of certain (lowest) rank.},
  archive      = {J_FoCM},
  author       = {Tunçel, Levent and Vavasis, Stephen A. and Xu, Jingye},
  doi          = {10.1007/s10208-023-09637-4},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {223-269},
  shortjournal = {Found. Comput. Math.},
  title        = {Computational complexity of decomposing a symmetric matrix as a sum of positive semidefinite and diagonal matrices},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast optimistic gradient descent ascent (OGDA) method in
continuous and discrete time. <em>FoCM</em>, <em>25</em>(1), 163–222.
(<a href="https://doi.org/10.1007/s10208-023-09636-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the framework of real Hilbert spaces, we study continuous in time dynamics as well as numerical algorithms for the problem of approaching the set of zeros of a single-valued monotone and continuous operator V. The starting point of our investigations is a second-order dynamical system that combines a vanishing damping term with the time derivative of V along the trajectory, which can be seen as an analogous of the Hessian-driven damping in case the operator is originating from a potential. Our method exhibits fast convergence rates of order $$o \left( \frac{1}{t\beta (t)} \right) $$ for $$\Vert V(z(t))\Vert $$ , where $$z(\cdot )$$ denotes the generated trajectory and $$\beta (\cdot )$$ is a positive nondecreasing function satisfying a growth condition, and also for the restricted gap function, which is a measure of optimality for variational inequalities. We also prove the weak convergence of the trajectory to a zero of V. Temporal discretizations of the dynamical system generate implicit and explicit numerical algorithms, which can be both seen as accelerated versions of the Optimistic Gradient Descent Ascent (OGDA) method for monotone operators, for which we prove that the generated sequence of iterates $$(z_k)_{k \ge 0}$$ shares the asymptotic features of the continuous dynamics. In particular we show for the implicit numerical algorithm convergence rates of order $$o \left( \frac{1}{k\beta _k} \right) $$ for $$\Vert V(z^k)\Vert $$ and the restricted gap function, where $$(\beta _k)_{k \ge 0}$$ is a positive nondecreasing sequence satisfying a growth condition. For the explicit numerical algorithm, we show by additionally assuming that the operator V is Lipschitz continuous convergence rates of order $$o \left( \frac{1}{k} \right) $$ for $$\Vert V(z^k)\Vert $$ and the restricted gap function. All convergence rate statements are last iterate convergence results; in addition to these, we prove for both algorithms the convergence of the iterates to a zero of V. To our knowledge, our study exhibits the best-known convergence rate results for monotone equations. Numerical experiments indicate the overwhelming superiority of our explicit numerical algorithm over other methods designed to solve monotone equations governed by monotone and Lipschitz continuous operators.},
  archive      = {J_FoCM},
  author       = {Boţ, Radu Ioan and Csetnek, Ernö Robert and Nguyen, Dang-Khoa},
  doi          = {10.1007/s10208-023-09636-5},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {163-222},
  shortjournal = {Found. Comput. Math.},
  title        = {Fast optimistic gradient descent ascent (OGDA) method in continuous and discrete time},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient random walks on riemannian manifolds.
<em>FoCM</em>, <em>25</em>(1), 145–161. (<a
href="https://doi.org/10.1007/s10208-023-09635-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to a version of Donsker’s theorem, geodesic random walks on Riemannian manifolds converge to the respective Brownian motion. From a computational perspective, however, evaluating geodesics can be quite costly. We therefore introduce approximate geodesic random walks based on the concept of retractions. We show that these approximate walks converge in distribution to the correct Brownian motion as long as the geodesic equation is approximated up to second order. As a result, we obtain an efficient algorithm for sampling Brownian motion on compact Riemannian manifolds.},
  archive      = {J_FoCM},
  author       = {Schwarz, Simon and Herrmann, Michael and Sturm, Anja and Wardetzky, Max},
  doi          = {10.1007/s10208-023-09635-6},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {145-161},
  shortjournal = {Found. Comput. Math.},
  title        = {Efficient random walks on riemannian manifolds},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extremal points and sparse optimization for generalized
kantorovich–rubinstein norms. <em>FoCM</em>, <em>25</em>(1), 103–144.
(<a href="https://doi.org/10.1007/s10208-023-09634-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A precise characterization of the extremal points of sublevel sets of nonsmooth penalties provides both detailed information about minimizers, and optimality conditions in general classes of minimization problems involving them. Moreover, it enables the application of fully corrective generalized conditional gradient methods for their efficient solution. In this manuscript, this program is adapted to the minimization of a smooth convex fidelity term which is augmented with an unbalanced transport regularization term given in the form of a generalized Kantorovich–Rubinstein norm for Radon measures. More precisely, we show that the extremal points associated to the latter are given by all Dirac delta functionals supported in the spatial domain as well as certain dipoles, i.e., pairs of Diracs with the same mass but with different signs. Subsequently, this characterization is used to derive precise first-order optimality conditions as well as an efficient solution algorithm for which linear convergence is proved under natural assumptions. This behavior is also reflected in numerical examples for a model problem.},
  archive      = {J_FoCM},
  author       = {Carioni, Marcello and Iglesias, José A. and Walter, Daniel},
  doi          = {10.1007/s10208-023-09634-7},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {103-144},
  shortjournal = {Found. Comput. Math.},
  title        = {Extremal points and sparse optimization for generalized Kantorovich–Rubinstein norms},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication lower bounds for nested bilinear algorithms
via rank expansion of kronecker products. <em>FoCM</em>, <em>25</em>(1),
55–101. (<a href="https://doi.org/10.1007/s10208-023-09633-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop lower bounds on communication in the memory hierarchy or between processors for nested bilinear algorithms, such as Strassen’s algorithm for matrix multiplication. We build on a previous framework that establishes communication lower bounds by use of the rank expansion, or the minimum rank of any fixed size subset of columns of a matrix, for each of the three matrices encoding a bilinear algorithm. This framework provides lower bounds for a class of dependency directed acyclic graphs (DAGs) corresponding to the execution of a given bilinear algorithm, in contrast to other approaches that yield bounds for specific DAGs. However, our lower bounds only apply to executions that do not compute the same DAG node multiple times. Two bilinear algorithms can be nested by taking Kronecker products between their encoding matrices. Our main result is a lower bound on the rank expansion of a matrix constructed by a Kronecker product derived from lower bounds on the rank expansion of the Kronecker product’s operands. We apply the rank expansion lower bounds to obtain novel communication lower bounds for nested Toom-Cook convolution, Strassen’s algorithm, and fast algorithms for contraction of partially symmetric tensors.},
  archive      = {J_FoCM},
  author       = {Ju, Caleb and Zhang, Yifan and Solomonik, Edgar},
  doi          = {10.1007/s10208-023-09633-8},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {55-101},
  shortjournal = {Found. Comput. Math.},
  title        = {Communication lower bounds for nested bilinear algorithms via rank expansion of kronecker products},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian beam ansatz for finite difference wave equations.
<em>FoCM</em>, <em>25</em>(1), 1–54. (<a
href="https://doi.org/10.1007/s10208-023-09632-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is concerned with the construction of Gaussian Beam (GB) solutions for the numerical approximation of wave equations, semi-discretized in space by finite difference schemes. GB are high-frequency solutions whose propagation can be described, both at the continuous and at the semi-discrete levels, by microlocal tools along the bi-characteristics of the corresponding Hamiltonian. Their dynamics differ in the continuous and the semi-discrete setting, because of the high-frequency gap between the Hamiltonians. In particular, numerical high-frequency solutions can exhibit spurious pathological behaviors, such as lack of propagation in space, contrary to the classical space-time propagation properties of continuous waves. This gap between the behavior of continuous and numerical waves introduces also significant analytical difficulties, since classical GB constructions cannot be immediately extrapolated to the finite difference setting, and need to be properly tailored to accurately detect the propagation properties in discrete media. Our main objective in this paper is to present a general and rigorous construction of the GB ansatz for finite difference wave equations, and corroborate this construction through accurate numerical simulations.},
  archive      = {J_FoCM},
  author       = {Biccari, Umberto and Zuazua, Enrique},
  doi          = {10.1007/s10208-023-09632-9},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {1-54},
  shortjournal = {Found. Comput. Math.},
  title        = {Gaussian beam ansatz for finite difference wave equations},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijcv---25">IJCV - 25</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Variational rectification inference for learning
with noisy labels. <em>IJCV</em>, <em>133</em>(3), 1434. (<a
href="https://doi.org/10.1007/s11263-024-02242-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Sun, Haoliang and Wei, Qi and Feng, Lei and Hu, Yupeng and Liu, Fan and Fan, Hehe and Yin, Yilong},
  doi          = {10.1007/s11263-024-02242-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1434},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Variational rectification inference for learning with noisy labels},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on computer vision approaches
for animal tracking and modeling 2023. <em>IJCV</em>, <em>133</em>(3),
1433. (<a href="https://doi.org/10.1007/s11263-024-02241-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-024-02241-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1433},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on computer vision approaches for animal tracking and modeling 2023},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on german conference on pattern
recognition (DAGM GCPR). <em>IJCV</em>, <em>133</em>(3), 1432. (<a
href="https://doi.org/10.1007/s11263-024-02212-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Goldluecke, Bastian},
  doi          = {10.1007/s11263-024-02212-6},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1432},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on german conference on pattern recognition (DAGM GCPR)},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSKNet: A foundation lightweight backbone for remote
sensing. <em>IJCV</em>, <em>133</em>(3), 1410–1431. (<a
href="https://doi.org/10.1007/s11263-024-02247-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection, semantic segmentation and change detection, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet backbone network sets new state-of-the-art scores on standard remote sensing classification, object detection, semantic segmentation and change detection benchmarks. Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at https://github.com/zcablii/LSKNet .},
  archive      = {J_IJCV},
  author       = {Li, Yuxuan and Li, Xiang and Dai, Yimain and Hou, Qibin and Liu, Li and Liu, Yongxiang and Cheng, Ming-Ming and Yang, Jian},
  doi          = {10.1007/s11263-024-02247-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1410-1431},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LSKNet: A foundation lightweight backbone for remote sensing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified frequency-assisted transformer framework for
detecting and grounding multi-modal manipulation. <em>IJCV</em>,
<em>133</em>(3), 1392–1409. (<a
href="https://doi.org/10.1007/s11263-024-02245-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and grounding multi-modal media manipulation ( $$\hbox {DGM}^4$$ ) has become increasingly crucial due to the widespread dissemination of face forgery and text misinformation. In this paper, we present the Unified Frequency-Assisted transFormer framework, named UFAFormer, to address the $$\hbox {DGM}^4$$ problem. Unlike previous state-of-the-art methods that solely focus on the image (RGB) domain to describe visual forgery features, we additionally introduce the frequency domain as a complementary viewpoint. By leveraging the discrete wavelet transform, we decompose images into several frequency sub-bands, capturing rich face forgery artifacts. Then, our proposed frequency encoder, incorporating intra-band and inter-band self-attentions, explicitly aggregates forgery features within and across diverse sub-bands. Moreover, to address the semantic conflicts between image and frequency domains, the forgery-aware mutual module is developed to further enable the effective interaction of disparate image and frequency features, resulting in aligned and comprehensive visual forgery representations. Finally, based on visual and textual forgery features, we propose a unified decoder that comprises two symmetric cross-modal interaction modules responsible for gathering modality-specific forgery information, along with a fusing interaction module for aggregation of both modalities. The proposed unified decoder formulates our UFAFormer as a unified framework, ultimately simplifying the overall architecture and facilitating the optimization process. Experimental results on the $$\hbox {DGM}^4$$ dataset, containing several perturbations, demonstrate the superior performance of our framework compared to previous methods, setting a new benchmark in the field.},
  archive      = {J_IJCV},
  author       = {Liu, Huan and Tan, Zichang and Chen, Qiang and Wei, Yunchao and Zhao, Yao and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02245-x},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1392-1409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unified frequency-assisted transformer framework for detecting and grounding multi-modal manipulation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-VLGM: Bi-level class-severity-aware vision-language graph
matching for text guided medical image segmentation. <em>IJCV</em>,
<em>133</em>(3), 1375–1391. (<a
href="https://doi.org/10.1007/s11263-024-02246-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical reports containing specific diagnostic results and additional information not present in medical images can be effectively employed to assist image understanding tasks, and the modality gap between vision and language can be bridged by vision-language matching (VLM). However, current vision-language models distort the intra-model relation and only include class information in reports that is insufficient for segmentation task. In this paper, we introduce a novel Bi-level class-severity-aware Vision-Language Graph Matching (Bi-VLGM) for text guided medical image segmentation, composed of a word-level VLGM module and a sentence-level VLGM module, to exploit the class-severity-aware relation among visual-textual features. In word-level VLGM, to mitigate the distorted intra-modal relation during VLM, we reformulate VLM as graph matching problem and introduce a vision-language graph matching (VLGM) to exploit the high-order relation among visual-textual features. Then, we perform VLGM between the local features for each class region and class-aware prompts to bridge their gap. In sentence-level VLGM, to provide disease severity information for segmentation task, we introduce a severity-aware prompting to quantify the severity level of disease lesion, and perform VLGM between the global features and the severity-aware prompts. By exploiting the relation between the local (global) and class (severity) features, the segmentation model can include the class-aware and severity-aware information to promote segmentation performance. Extensive experiments proved the effectiveness of our method and its superiority to existing methods. The source code will be released.},
  archive      = {J_IJCV},
  author       = {Chen, Wenting and Liu, Jie and Liu, Tianming and Yuan, Yixuan},
  doi          = {10.1007/s11263-024-02246-w},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1375-1391},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bi-VLGM: Bi-level class-severity-aware vision-language graph matching for text guided medical image segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MapTRv2: An end-to-end framework for online vectorized HD
map construction. <em>IJCV</em>, <em>133</em>(3), 1352–1374. (<a
href="https://doi.org/10.1007/s11263-024-02235-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-definition (HD) map provides abundant and precise static environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. In this paper, we present Map TRansformer, an end-to-end framework for online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. To speed up convergence, we further introduce auxiliary one-to-many matching and dense supervision. The proposed method well copes with various map elements with arbitrary shapes. It runs at real-time inference speed and achieves state-of-the-art performance on both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable and robust map construction quality in complex and various driving scenes. Code and more demos are available at https://github.com/hustvl/MapTR for facilitating further studies and applications.},
  archive      = {J_IJCV},
  author       = {Liao, Bencheng and Chen, Shaoyu and Zhang, Yunchi and Jiang, Bo and Zhang, Qian and Liu, Wenyu and Huang, Chang and Wang, Xinggang},
  doi          = {10.1007/s11263-024-02235-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1352-1374},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MapTRv2: An end-to-end framework for online vectorized HD map construction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dissecting out-of-distribution detection and open-set
recognition: A critical analysis of methods and benchmarks.
<em>IJCV</em>, <em>133</em>(3), 1326–1351. (<a
href="https://doi.org/10.1007/s11263-024-02222-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting test-time distribution shift has emerged as a key capability for safely deployed machine learning models, with the question being tackled under various guises in recent years. In this paper, we aim to provide a consolidated view of the two largest sub-fields within the community: out-of-distribution (OOD) detection and open-set recognition (OSR). In particular, we aim to provide rigorous empirical analysis of different methods across settings and provide actionable takeaways for practitioners and researchers. Concretely, we make the following contributions: (i) We perform rigorous cross-evaluation between state-of-the-art methods in the OOD detection and OSR settings and identify a strong correlation between the performances of methods for them; (ii) We propose a new, large-scale benchmark setting which we suggest better disentangles the problem tackled by OOD detection and OSR, re-evaluating state-of-the-art OOD detection and OSR methods in this setting; (iii) We surprisingly find that the best performing method on standard benchmarks (Outlier Exposure) struggles when tested at scale, while scoring rules which are sensitive to the deep feature magnitude consistently show promise; and (iv) We conduct empirical analysis to explain these phenomena and highlight directions for future research. Code: https://github.com/Visual-AI/Dissect-OOD-OSR},
  archive      = {J_IJCV},
  author       = {Wang, Hongjun and Vaze, Sagar and Han, Kai},
  doi          = {10.1007/s11263-024-02222-4},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1326-1351},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dissecting out-of-distribution detection and open-set recognition: A critical analysis of methods and benchmarks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2D semantic-guided semantic scene completion. <em>IJCV</em>,
<em>133</em>(3), 1306–1325. (<a
href="https://doi.org/10.1007/s11263-024-02244-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic scene completion (SSC) aims to simultaneously perform scene completion (SC) and predict semantic categories of a 3D scene from a single depth and/or RGB image. Most existing SSC methods struggle to handle complex regions with multiple objects close to each other, especially for objects with reflective or dark surfaces. This primarily stems from two challenges: (1) the loss of geometric information due to the unreliability of depth values from sensors, and (2) the potential for semantic confusion when simultaneously predicting 3D shapes and semantic labels. To address these problems, we propose a Semantic-guided Semantic Scene Completion framework, dubbed SG-SSC, which involves Semantic-guided Fusion (SGF) and Volume-guided Semantic Predictor (VGSP). Guided by 2D semantic segmentation maps, SGF adaptively fuses RGB and depth features to compensate for the missing geometric information caused by the missing values in depth images, thus performing more robustly to unreliable depth information. VGSP exploits the mutual benefit between SC and SSC tasks, making SSC more focused on predicting the categories of voxels with high occupancy probabilities and also allowing SC to utilize semantic priors to better predict voxel occupancy. Experimental results show that SG-SSC outperforms existing state-of-the-art methods on the NYU, NYUCAD, and SemanticKITTI datasets. Models and code are available at https://github.com/aipixel/SG-SSC .},
  archive      = {J_IJCV},
  author       = {Liu, Xianzhu and Xie, Haozhe and Zhang, Shengping and Yao, Hongxun and Ji, Rongrong and Nie, Liqiang and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02244-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1306-1325},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {2D semantic-guided semantic scene completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From gaze jitter to domain adaptation: Generalizing gaze
estimation by manipulating high-frequency components. <em>IJCV</em>,
<em>133</em>(3), 1290–1305. (<a
href="https://doi.org/10.1007/s11263-024-02233-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze, as a pivotal indicator of human emotion, plays a crucial role in various computer vision tasks. However, the accuracy of gaze estimation often significantly deteriorates when applied to unseen environments, thereby limiting its practical value. Therefore, enhancing the generalizability of gaze estimators to new domains emerges as a critical challenge. A common limitation in existing domain adaptation research is the inability to identify and leverage truly influential factors during the adaptation process. This shortcoming often results in issues such as limited accuracy and unstable adaptation. To address this issue, this article discovers a truly influential factor in the cross-domain problem, i.e., high-frequency components (HFC). This discovery stems from an analysis of gaze jitter-a frequently overlooked but impactful issue where predictions can deviate drastically even for visually similar input images. Inspired by this discovery, we propose an “embed-then-suppress&quot; HFC manipulation strategy to adapt gaze estimation to new domains. Our method first embeds additive HFC to the input images, then performs domain adaptation by suppressing the impact of HFC. Specifically, the suppression is carried out in a contrasive manner. Each original image is paired with its HFC-embedded version, thereby enabling our method to suppress the HFC impact through contrasting the representations within the pairs. The proposed method is evaluated across four cross-domain gaze estimation tasks. The experimental results show that it not only enhances gaze estimation accuracy but also significantly reduces gaze jitter in the target domain. Compared with previous studies, our method offers higher accuracy, reduced gaze jitter, and improved adaptation stability, marking the potential for practical deployment.},
  archive      = {J_IJCV},
  author       = {Liu, Ruicong and Wang, Haofei and Lu, Feng},
  doi          = {10.1007/s11263-024-02233-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1290-1305},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {From gaze jitter to domain adaptation: Generalizing gaze estimation by manipulating high-frequency components},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEO: Generative latent image animator for human video
synthesis. <em>IJCV</em>, <em>133</em>(3), 1277–1289. (<a
href="https://doi.org/10.1007/s11263-024-02231-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing. Project page: https://wyhsirius.github.io/LEO-project/ .},
  archive      = {J_IJCV},
  author       = {Wang, Yaohui and Ma, Xin and Chen, Xinyuan and Chen, Cunjian and Dantcheva, Antitza and Dai, Bo and Qiao, Yu},
  doi          = {10.1007/s11263-024-02231-3},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1277-1289},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LEO: Generative latent image animator for human video synthesis},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutual prompt leaning for vision language models.
<em>IJCV</em>, <em>133</em>(3), 1258–1276. (<a
href="https://doi.org/10.1007/s11263-024-02243-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and visual sub-branches, existing prompt approaches are mainly divided into text and visual prompts. Recent text prompt methods have achieved great performance by designing input-condition prompts that encompass both text and image domain knowledge. However, roughly incorporating the same image feature into each learnable text token may be unjustifiable, as it could result in learnable text prompts being concentrated on one or a subset of characteristics. In light of this, we propose a fine-grained text prompt (FTP) that decomposes the single global image features into several finer-grained semantics and incorporates them into corresponding text prompt tokens. On the other hand, current methods neglect valuable text semantic information when building the visual prompt. Furthermore, text information contains redundant and negative category semantics. To address this, we propose a text-reorganized visual prompt (TVP) that reorganizes the text descriptions of the current image to construct the visual prompt, guiding the image branch to attend to class-related representations. By leveraging both FTP and TVP, we enable mutual prompting between the text and visual modalities, unleashing their potential to tap into the representation capabilities of VLMs. Extensive experiments on 11 classification benchmarks show that our method surpasses existing methods by a large margin. In particular, our approach improves recent state-of-the-art CoCoOp by 4.79% on new classes and 3.88% on harmonic mean over eleven classification benchmarks.},
  archive      = {J_IJCV},
  author       = {Long, Sifan and Zhao, Zhen and Yuan, Junkun and Tan, Zichang and Liu, Jiangjiang and Feng, Jingyuan and Wang, Shengsheng and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02243-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1258-1276},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mutual prompt leaning for vision language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust deep object tracking against adversarial attacks.
<em>IJCV</em>, <em>133</em>(3), 1238–1257. (<a
href="https://doi.org/10.1007/s11263-024-02226-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the vulnerability of deep neural networks (DNNs) has attracted significant attention in recent years. While recent studies on adversarial attack and defense mainly reside in a single image, few efforts have been made to perform temporal attacks against video sequences. As the temporal consistency between frames is not considered, existing adversarial attack approaches designed for static images do not perform well for deep object tracking. In this work, we generate adversarial examples on top of video sequences to improve the tracking robustness against adversarial attacks under white-box and black-box settings. To this end, we consider motion signals when generating lightweight perturbations over the estimated tracking results frame-by-frame. For the white-box attack, we generate temporal perturbations via known trackers to degrade significantly the tracking performance. We transfer the generated perturbations into unknown targeted trackers for the black-box attack to achieve transferring attacks. Furthermore, we train universal adversarial perturbations and directly add them into all frames of videos, improving the attack effectiveness with minor computational costs. On the other hand, we sequentially learn to estimate and remove the perturbations from input sequences to restore the tracking performance. We apply the proposed adversarial attack and defense approaches to state-of-the-art tracking algorithms. Extensive evaluations on large-scale benchmark datasets, including OTB, VOT, UAV123, and LaSOT, demonstrate that our attack method degrades the tracking performance significantly with favorable transferability to other backbones and trackers. Notably, the proposed defense method restores the original tracking performance to some extent and achieves additional performance gains when not under adversarial attacks.},
  archive      = {J_IJCV},
  author       = {Jia, Shuai and Ma, Chao and Song, Yibing and Yang, Xiaokang and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-024-02226-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1238-1257},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust deep object tracking against adversarial attacks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Slimmable networks for contrastive self-supervised learning.
<em>IJCV</em>, <em>133</em>(3), 1222–1237. (<a
href="https://doi.org/10.1007/s11263-024-02211-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning makes significant progress in pre-training large models, but struggles with small models. Mainstream solutions to this problem rely mainly on knowledge distillation, which involves a two-stage procedure: first training a large teacher model and then distilling it to improve the generalization ability of smaller ones. In this work, we introduce another one-stage solution to obtain pre-trained small models without the need for extra teachers, namely, slimmable networks for contrastive self-supervised learning (SlimCLR). A slimmable network consists of a full network and several weight-sharing sub-networks, which can be pre-trained once to obtain various networks, including small ones with low computation costs. However, interference between weight-sharing networks leads to severe performance degradation in self-supervised cases, as evidenced by gradient magnitude imbalance and gradient direction divergence. The former indicates that a small proportion of parameters produce dominant gradients during backpropagation, while the main parameters may not be fully optimized. The latter shows that the gradient direction is disordered, and the optimization process is unstable. To address these issues, we introduce three techniques to make the main parameters produce dominant gradients and sub-networks have consistent outputs. These techniques include slow start training of sub-networks, online distillation, and loss re-weighting according to model sizes. Furthermore, theoretical results are presented to demonstrate that a single slimmable linear layer is sub-optimal during linear evaluation. Thus a switchable linear probe layer is applied during linear evaluation. We instantiate SlimCLR with typical contrastive learning frameworks and achieve better performance than previous arts with fewer parameters and FLOPs. The code is available at https://github.com/mzhaoshuai/SlimCLR .},
  archive      = {J_IJCV},
  author       = {Zhao, Shuai and Zhu, Linchao and Wang, Xiaohan and Yang, Yi},
  doi          = {10.1007/s11263-024-02211-7},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1222-1237},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Slimmable networks for contrastive self-supervised learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking the limits of reliable prediction via generated
data. <em>IJCV</em>, <em>133</em>(3), 1195–1221. (<a
href="https://doi.org/10.1007/s11263-024-02221-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open-world recognition of safety-critical applications, providing reliable prediction for deep neural networks has become a critical requirement. Many methods have been proposed for reliable prediction related tasks such as confidence calibration, misclassification detection, and out-of-distribution detection. Recently, pre-training has been shown to be one of the most effective methods for improving reliable prediction, particularly for modern networks like ViT, which require a large amount of training data. However, collecting data manually is time-consuming. In this paper, taking advantage of the breakthrough of generative models, we investigate whether and how expanding the training set using generated data can improve reliable prediction. Our experiments reveal that training with a large quantity of generated data can eliminate overfitting in reliable prediction, leading to significantly improved performance. Surprisingly, classical networks like ResNet-18, when trained on a notably extensive volume of generated data, can sometimes exhibit performance competitive to pre-training ViT with a substantial real dataset.},
  archive      = {J_IJCV},
  author       = {Cheng, Zhen and Zhu, Fei and Zhang, Xu-Yao and Liu, Cheng-Lin},
  doi          = {10.1007/s11263-024-02221-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1195-1221},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Breaking the limits of reliable prediction via generated data},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FastComposer: Tuning-free multi-subject image generation
with localized attention. <em>IJCV</em>, <em>133</em>(3), 1175–1194. (<a
href="https://doi.org/10.1007/s11263-024-02227-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300 $$\times $$ –2500 $$\times $$ speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available here ( https://github.com/mit-han-lab/fastcomposer ).},
  archive      = {J_IJCV},
  author       = {Xiao, Guangxuan and Yin, Tianwei and Freeman, William T. and Durand, Frédo and Han, Song},
  doi          = {10.1007/s11263-024-02227-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1175-1194},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FastComposer: Tuning-free multi-subject image generation with localized attention},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lidar panoptic segmentation in an open world. <em>IJCV</em>,
<em>133</em>(3), 1153–1174. (<a
href="https://doi.org/10.1007/s11263-024-02166-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing Lidar Panoptic Segmentation (LPS) is crucial for safe deployment of autnomous vehicles. LPS aims to recognize and segment lidar points w.r.t. a pre-defined vocabulary of semantic classes, including thing classes of countable objects (e.g., pedestrians and vehicles) and stuff classes of amorphous regions (e.g., vegetation and road). Importantly, LPS requires segmenting individual thing instances (e.g., every single vehicle). Current LPS methods make an unrealistic assumption that the semantic class vocabulary is fixed in the real open world, but in fact, class ontologies usually evolve over time as robots encounter instances of novel classes that are considered to be unknowns w.r.t. thepre-defined class vocabulary. To address this unrealistic assumption, we study LPS in the Open World (LiPSOW): we train models on a dataset with a pre-defined semantic class vocabulary and study their generalization to a larger dataset where novel instances of thing and stuff classes can appear. This experimental setting leads to interesting conclusions. While prior art train class-specific instance segmentation methods and obtain state-of-the-art results on known classes, methods based on class-agnostic bottom-up grouping perform favorably on classes outside of the initial class vocabulary (i.e., unknown classes). Unfortunately, these methods do not perform on-par with fully data-driven methods on known classes. Our work suggests a middle ground: we perform class-agnostic point clustering and over-segment the input cloud in a hierarchical fashion, followed by binary point segment classification, akin to Region Proposal Network (Ren et al. NeurIPS, 2015). We obtain the final point cloud segmentation by computing a cut in the weighted hierarchical tree of point segments, independently of semantic classification. Remarkably, this unified approach leads to strong performance on both known and unknown classes.},
  archive      = {J_IJCV},
  author       = {Chakravarthy, Anirudh S. and Ganesina, Meghana Reddy and Hu, Peiyun and Leal-Taixé, Laura and Kong, Shu and Ramanan, Deva and Osep, Aljosa},
  doi          = {10.1007/s11263-024-02166-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1153-1174},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lidar panoptic segmentation in an open world},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical active learning for low-altitude drone-view
object detection. <em>IJCV</em>, <em>133</em>(3), 1140–1152. (<a
href="https://doi.org/10.1007/s11263-024-02228-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various object detection techniques are employed on drone platforms. However, the task of annotating drone-view samples is both time-consuming and laborious. This is primarily due to the presence of numerous small-sized instances to be labeled in the drone-view image. To tackle this issue, we propose HALD, a hierarchical active learning approach for low-altitude drone-view object detection. HALD extracts unlabeled image information sequentially from different levels, including point, box, image, and class, aiming to obtain a reliable indicator of image information. The point-level module is utilized to ascertain the valid count and location of instances, while the box-level module screens out reliable predictions. The image-level module selects candidate samples by calculating the consistency of valid boxes within an image, and the class-level module selects the final selected samples based on the distribution of candidate and labeled samples across different classes. Extensive experiments conducted on the VisDrone and CityPersons datasets demonstrate that HALD outperforms several other baseline methods. Additionally, we provide an in-depth analysis of each proposed module. The results show that the performance of evaluating the informativeness of samples can be effectively improved by the four hierarchical levels.},
  archive      = {J_IJCV},
  author       = {Hu, Haohao and Han, Tianyu and Wang, Yuerong and Zhong, Wanjun and Yue, Jingwei and Zan, Peng},
  doi          = {10.1007/s11263-024-02228-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1140-1152},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical active learning for low-altitude drone-view object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In search of lost online test-time adaptation: A survey.
<em>IJCV</em>, <em>133</em>(3), 1106–1139. (<a
href="https://doi.org/10.1007/s11263-024-02213-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive survey of online test-time adaptation (OTTA), focusing on effectively adapting machine learning models to distributionally different target data upon batch arrival. Despite the recent proliferation of OTTA methods, conclusions from previous studies are inconsistent due to ambiguous settings, outdated backbones, and inconsistent hyperparameter tuning, which obscure core challenges and hinder reproducibility. To enhance clarity and enable rigorous comparison, we classify OTTA techniques into three primary categories and benchmark them using a modern backbone, the Vision Transformer. Our benchmarks cover conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C, as well as real-world shifts represented by CIFAR-10.1, OfficeHome, and CIFAR-10-Warehouse. The CIFAR-10-Warehouse dataset includes a variety of variations from different search engines and synthesized data generated through diffusion models. To measure efficiency in online scenarios, we introduce novel evaluation metrics, including GFLOPs, wall clock time, and GPU memory usage, providing a clearer picture of the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, revealing that (1) transformers demonstrate heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods relies on large batch sizes, and (3) stability in optimization and resistance to perturbations are crucial during adaptation, particularly when the batch size is 1. Based on these insights, we highlight promising directions for future research. Our benchmarking toolkit and source code are available at https://github.com/Jo-wang/OTTA_ViT_survey .},
  archive      = {J_IJCV},
  author       = {Wang, Zixin and Luo, Yadan and Zheng, Liang and Chen, Zhuoxiao and Wang, Sen and Huang, Zi},
  doi          = {10.1007/s11263-024-02213-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1106-1139},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {In search of lost online test-time adaptation: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WeakCLIP: Adapting CLIP for weakly-supervised semantic
segmentation. <em>IJCV</em>, <em>133</em>(3), 1085–1105. (<a
href="https://doi.org/10.1007/s11263-024-02224-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive language and image pre-training (CLIP) achieves great success in various computer vision tasks and also presents an opportune avenue for enhancing weakly-supervised image understanding with its large-scale pre-trained knowledge. As an effective way to reduce the reliance on pixel-level human-annotated labels, weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) and produce high-quality pseudo masks. Weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) as pseudo masks, but heavily relies on inductive biases like hand-crafted priors and digital image processing methods. For the vision-language pre-trained model, i.e. CLIP, we propose a novel text-to-pixel matching paradigm for WSSS. However, directly applying CLIP to WSSS is challenging due to three critical problems: (1) the task gap between contrastive pre-training and WSSS CAM refinement, (2) lacking text-to-pixel modeling to fully utilize the pre-trained knowledge, and (3) the insufficient details owning to the $$\frac{1}{16}$$ down-sampling resolution of ViT. Thus, we propose WeakCLIP to address the problems and leverage the pre-trained knowledge from CLIP to WSSS. Specifically, we first address the task gap by proposing a pyramid adapter and learnable prompts to extract WSSS-specific representation. We then design a co-attention matching module to model text-to-pixel relationships. Finally, the pyramid adapter and text-guided decoder are introduced to gather multi-level information and integrate it with text guidance hierarchically. WeakCLIP provides an effective and parameter-efficient way to transfer CLIP knowledge to refine CAM. Extensive experiments demonstrate that WeakCLIP achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 74.0% mIoU on the val set of PASCAL VOC 2012 and 46.1% mIoU on the val set of COCO 2014. The source code and model checkpoints are released at https://github.com/hustvl/WeakCLIP .},
  archive      = {J_IJCV},
  author       = {Zhu, Lianghui and Wang, Xinggang and Feng, Jiapei and Cheng, Tianheng and Li, Yingyue and Jiang, Bo and Zhang, Dingwen and Han, Junwei},
  doi          = {10.1007/s11263-024-02224-2},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1085-1105},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {WeakCLIP: Adapting CLIP for weakly-supervised semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual face forgery detection via historical distribution
preserving. <em>IJCV</em>, <em>133</em>(3), 1067–1084. (<a
href="https://doi.org/10.1007/s11263-024-02160-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery techniques have advanced rapidly and pose serious security threats. Existing face forgery detection methods try to learn generalizable features, but they still fall short of practical application. Additionally, finetuning these methods on historical training data is resource-intensive in terms of time and storage. In this paper, we focus on a novel and challenging problem: Continual Face Forgery Detection (CFFD), which aims to efficiently learn from new forgery attacks without forgetting previous ones. Specifically, we propose a Historical Distribution Preserving (HDP) framework that reserves and preserves the distributions of historical faces. To achieve this, we use universal adversarial perturbation (UAP) to simulate historical forgery distribution, and knowledge distillation to maintain the distribution variation of real faces across different models. We also construct a new benchmark for CFFD with three evaluation protocols. Our extensive experiments on the benchmarks show that our method outperforms the state-of-the-art competitors. Our code is available at https://github.com/skJack/HDP .},
  archive      = {J_IJCV},
  author       = {Sun, Ke and Chen, Shen and Yao, Taiping and Sun, Xiaoshuai and Ding, Shouhong and Ji, Rongrong},
  doi          = {10.1007/s11263-024-02160-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1067-1084},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Continual face forgery detection via historical distribution preserving},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive fuzzy positive learning for annotation-scarce
semantic segmentation. <em>IJCV</em>, <em>133</em>(3), 1048–1066. (<a
href="https://doi.org/10.1007/s11263-024-02217-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotation-scarce semantic segmentation aims to obtain meaningful pixel-level discrimination with scarce or even no manual annotations, of which the crux is how to utilize unlabeled data by pseudo-label learning. Typical works focus on ameliorating the error-prone pseudo-labeling, e.g., only utilizing high-confidence pseudo labels and filtering low-confidence ones out. But we think differently and resort to exhausting informative semantics from multiple probably correct candidate labels. This brings our method the ability to learn more accurately even though pseudo labels are unreliable. In this paper, we propose Adaptive Fuzzy Positive Learning (A-FPL) for correctly learning unlabeled data in a plug-and-play fashion, targeting adaptively encouraging fuzzy positive predictions and suppressing highly probable negatives. Specifically, A-FPL comprises two main components: (1) Fuzzy positive assignment (FPA) that adaptively assigns fuzzy positive labels to each pixel, while ensuring their quality through a T-value adaption algorithm (2) Fuzzy positive regularization (FPR) that restricts the predictions of fuzzy positive categories to be larger than those of negative categories. Being conceptually simple yet practically effective, A-FPL remarkably alleviates interference from wrong pseudo labels, progressively refining semantic discrimination. Theoretical analysis and extensive experiments on various training settings with consistent performance gain justify the superiority of our approach. Codes are at A-FPL .},
  archive      = {J_IJCV},
  author       = {Qiao, Pengchong and Wang, Yu and Liu, Chang and Shang, Lei and Sun, Baigui and Wang, Zhennan and Zheng, Xiawu and Ji, Rongrong and Chen, Jie},
  doi          = {10.1007/s11263-024-02217-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1048-1066},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive fuzzy positive learning for annotation-scarce semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic evaluation of uncertainty calibration in
pretrained object detectors. <em>IJCV</em>, <em>133</em>(3), 1033–1047.
(<a href="https://doi.org/10.1007/s11263-024-02219-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of deep learning based computer vision, the development of deep object detection has led to unique paradigms (e.g., two-stage or set-based) and architectures (e.g., Faster-RCNN or DETR) which enable outstanding performance on challenging benchmark datasets. Despite this, the trained object detectors typically do not reliably assess uncertainty regarding their own knowledge, and the quality of their probabilistic predictions is usually poor. As these are often used to make subsequent decisions, such inaccurate probabilistic predictions must be avoided. In this work, we investigate the uncertainty calibration properties of different pretrained object detection architectures in a multi-class setting. We propose a framework to ensure a fair, unbiased, and repeatable evaluation and conduct detailed analyses assessing the calibration under distributional changes (e.g., distributional shift and application to out-of-distribution data). Furthermore, by investigating the influence of different detector paradigms, post-processing steps, and suitable choices of metrics, we deliver novel insights into why poor detector calibration emerges. Based on these insights, we are able to improve the calibration of a detector by simply finetuning its last layer.},
  archive      = {J_IJCV},
  author       = {Huseljic, Denis and Herde, Marek and Hahn, Paul and Müjde, Mehmet and Sick, Bernhard},
  doi          = {10.1007/s11263-024-02219-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1033-1047},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Systematic evaluation of uncertainty calibration in pretrained object detectors},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting class-incremental learning with pre-trained
models: Generalizability and adaptivity are all you need. <em>IJCV</em>,
<em>133</em>(3), 1012–1032. (<a
href="https://doi.org/10.1007/s11263-024-02218-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. (1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. (2) Due to the distribution gap between pre-trained and downstream datasets, PTM can be further cultivated with adaptivity via model adaptation. We propose AdaPt and mERge (Aper), which aggregates the embeddings of PTM and adapted models for classifier construction. Aper is a general framework that can be orthogonally combined with any parameter-efficient tuning method, which holds the advantages of PTM’s generalizability and adapted model’s adaptivity. (3) Additionally, considering previous ImageNet-based benchmarks are unsuitable in the era of PTM due to data overlapping, we propose four new benchmarks for assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the effectiveness of Aper with a unified and concise framework. Code is available at https://github.com/zhoudw-zdw/RevisitingCIL .},
  archive      = {J_IJCV},
  author       = {Zhou, Da-Wei and Cai, Zi-Wen and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02218-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1012-1032},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight high-speed photography built on coded exposure
and implicit neural representation of videos. <em>IJCV</em>,
<em>133</em>(3), 991–1011. (<a
href="https://doi.org/10.1007/s11263-024-02198-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for compact cameras capable of recording high-speed scenes with high resolution is steadily increasing. However, achieving such capabilities often entails high bandwidth requirements, resulting in bulky, heavy systems unsuitable for low-capacity platforms. To address this challenge, leveraging a coded exposure setup to encode a frame sequence into a blurry snapshot and subsequently retrieve the latent sharp video presents a lightweight solution. Nevertheless, restoring motion from blur remains a formidable challenge due to the inherent ill-posedness of motion blur decomposition, the intrinsic ambiguity in motion direction, and the diverse motions present in natural videos. In this study, we propose a novel approach to address these challenges by combining the classical coded exposure imaging technique with the emerging implicit neural representation for videos. We strategically embed motion direction cues into the blurry image during the imaging process. Additionally, we develop a novel implicit neural representation based blur decomposition network to sequentially extract the latent video frames from the blurry image, leveraging the embedded motion direction cues. To validate the effectiveness and efficiency of our proposed framework, we conduct extensive experiments using benchmark datasets and real-captured blurry images. The results demonstrate that our approach significantly outperforms existing methods in terms of both quality and flexibility. The code for our work is available at https://github.com/zhihongz/BDINR .},
  archive      = {J_IJCV},
  author       = {Zhang, Zhihong and Yang, Runzhao and Suo, Jinli and Cheng, Yuxiao and Dai, Qionghai},
  doi          = {10.1007/s11263-024-02198-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {991-1011},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lightweight high-speed photography built on coded exposure and implicit neural representation of videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijfs---23">IJFS - 23</h2>
<ul>
<li><details>
<summary>
(2025). Retraction note: A fuzzy-social network multi-criteria group
decision-making framework for selection of renewable energy project: A
case of china. <em>IJFS</em>, <em>27</em>(1), 305. (<a
href="https://doi.org/10.1007/s40815-024-01958-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Su, Weihua and Zhang, Le and Zeng, Shouzhen and Jin, Huanhuan},
  doi          = {10.1007/s40815-024-01958-y},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {305},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: a fuzzy-social network multi-criteria group decision-making framework for selection of renewable energy project: a case of china},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Interaction power partitioned maclaurin
symmetric mean operators under q-rung orthopair uncertain linguistic
information. <em>IJFS</em>, <em>27</em>(1), 304. (<a
href="https://doi.org/10.1007/s40815-024-01950-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Yang, Zaoli and Garg, Harish},
  doi          = {10.1007/s40815-024-01950-6},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {304},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: Interaction power partitioned maclaurin symmetric mean operators under q-rung orthopair uncertain linguistic information},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Secure cloud storage for medical IoT data
using adaptive neuro-fuzzy inference system. <em>IJFS</em>,
<em>27</em>(1), 303. (<a
href="https://doi.org/10.1007/s40815-024-01949-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Mohiyuddin, Aqsa and Javed, Abdul Rehman and Chakraborty, Chinmay and Rizwan, Muhammad and Shabbir, Maryam and Nebhen, Jamel},
  doi          = {10.1007/s40815-024-01949-z},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {303},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: Secure cloud storage for medical IoT data using adaptive neuro-fuzzy inference system},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Multi-criteria group decision-making
approach for express packaging recycling under interval-valued fuzzy
information: Combining objective and subjective compatibilities.
<em>IJFS</em>, <em>27</em>(1), 302. (<a
href="https://doi.org/10.1007/s40815-024-01948-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Zheng, Chengli and Zhou, Yuanyuan},
  doi          = {10.1007/s40815-024-01948-0},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {302},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: multi-criteria group decision-making approach for express packaging recycling under interval-valued fuzzy information: combining objective and subjective compatibilities},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Health safety and environment risk
assessment using an extended BWM-COPRAS approach based on g-number
theory. <em>IJFS</em>, <em>27</em>(1), 301. (<a
href="https://doi.org/10.1007/s40815-024-01947-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Ghoushchi, Saeid Jafarzadeh and Nik, Masoud Soleimani and Pourasad, Yaghoub},
  doi          = {10.1007/s40815-024-01947-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {301},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: Health safety and environment risk assessment using an extended BWM-COPRAS approach based on G-number theory},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Green supply chain network optimization
under random and fuzzy environment. <em>IJFS</em>, <em>27</em>(1), 300.
(<a href="https://doi.org/10.1007/s40815-024-01946-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Yu, Zhang and Khan, Syed Abdul Rehman},
  doi          = {10.1007/s40815-024-01946-2},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {300},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: Green supply chain network optimization under random and fuzzy environment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: A new extended VIKOR approach using q-rung
orthopair fuzzy sets for sustainable enterprise risk management
assessment in manufacturing small and medium-sized enterprises.
<em>IJFS</em>, <em>27</em>(1), 299. (<a
href="https://doi.org/10.1007/s40815-024-01945-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Cheng, Sun and Jianfu, Sun and Alrasheedi, Melfi and Saeidi, Parvaneh and Mishra, Arunodaya Raj and Rani, Pratibha},
  doi          = {10.1007/s40815-024-01945-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {299},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: A new extended VIKOR approach using q-rung orthopair fuzzy sets for sustainable enterprise risk management assessment in manufacturing small and medium-sized enterprises},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy portfolio with a novel power membership function based
on GARCH and black–litterman model. <em>IJFS</em>, <em>27</em>(1),
267–298. (<a href="https://doi.org/10.1007/s40815-024-01777-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a fuzzy mean-semi-absolute deviation portfolio with novel power membership functions. The portfolio return is measured by the Black–Litterman model, which combines the market’s objective information and investors’ subjective preference by the GARCH model. In addition, we use semi-absolute deviation to measure portfolio risk instead of variance, which not only reduces the computational complexity but also considers the corresponding risk lower than the expected return. Considering the investors’ psychological satisfaction with return and risk, we propose two novel power membership functions of return and risk, respectively. We fully demonstrate the shapes of membership functions with different preference parameters and medium satisfaction levels and deeply discuss monotonicity, convexity and concavity using the first- and second-order derivatives by strict mathematical proof. Furthermore, a fuzzy mean-semi-absolute deviation portfolio is proposed based on the satisfaction maximization principle and the absolute value optimization theory. Finally, we give a numerical example and present the results of neutral, pessimistic, and optimistic portfolios. In terms of Sharpe ratio and satisfaction degree, the portfolio model with our proposed novel membership functions is superior to those with S-type membership functions (Watada (2001) Dynamical aspects in fuzzy decision making. Physica, Heidelberg). Moreover, it is more effective than that with standard deviation or absolute deviation to measure risk.},
  archive      = {J_IJFS},
  author       = {Deng, Xue and Chen, Shiting},
  doi          = {10.1007/s40815-024-01777-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {267-298},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Fuzzy portfolio with a novel power membership function based on GARCH and Black–Litterman model},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ELECTRE TRI-c with hesitant fuzzy sets and interval type 2
trapezoidal fuzzy numbers using stochastic parameters: Application to a
brazilian electrical power company problem. <em>IJFS</em>,
<em>27</em>(1), 250–266. (<a
href="https://doi.org/10.1007/s40815-024-01775-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ELECTRE TRI-C is a method for sorting problems with imprecise evaluations and stable criteria weights, typically for a single decision-maker. While some extensions have addressed uncertain criteria weights and outranking functions using hesitant fuzzy sets (HFS) and interval type 2 trapezoidal fuzzy numbers (IT2TrfN), there is a gap in handling situations where multiple decision-makers provide uncertain information. This paper presents an extension of the ELECTRE TRI-C method incorporating a stochastic framework to model HFS and IT2TrfN, thereby accommodating subjective judgments from multiple decision-makers. The extended method was validated by sorting 49 projects based on their criticality in a Brazilian electrical power company, involving three decision-makers. The application shows strong correlations in project rankings among decision-makers, but with some exceptions. However, significant variations in acceptability ratings for sorting among decision-makers lead to notable error dispersion, highlighting differences between ranking and sorting outcomes. The key contributions of our approach are as follows: (1) Integration of subjective judgments from multiple decision-makers using IT2TrFN and Monte Carlo Simulation for constructing outranking functions; (2) Aggregation of preferences from multiple decision-makers using HFS; (3) Stochastic processing of both quantitative and qualitative criteria; (4) Integration of linear equations to represent weight constraints; and (5) Introduction of a novel visualization method for comprehensive analysis of stochastic results, enhancing robustness analysis. The proposal’s advantages over alternative methods are also highlighted.},
  archive      = {J_IJFS},
  author       = {Pereira, Javier and de Oliveira, Elaine C. B. and Morais, Danielle C. and Costa, Ana Paula C. S. and Alencar, Luciana H.},
  doi          = {10.1007/s40815-024-01775-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {250-266},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {ELECTRE TRI-C with hesitant fuzzy sets and interval type 2 trapezoidal fuzzy numbers using stochastic parameters: Application to a brazilian electrical power company problem},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting turkey’s primary energy demand based on fuzzy
auto-regressive distributed lag models with symmetric and non-symmetric
triangular coefficients. <em>IJFS</em>, <em>27</em>(1), 237–249. (<a
href="https://doi.org/10.1007/s40815-024-01773-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to guide policymakers in allocating resources and planning for the future by consistently estimating energy data trends. Because of the complexity and uncertainty of energy demand behavior and many influencing factors, we decide to take advantage of a fuzzy regression model to determine the actual relationships in the energy demand system and provide an accurate forecast of energy demand. For this purpose, because of energy demand drivers, fuzzy possibilistic approaches with symmetric and non-symmetric triangular coefficients are integrated with the autoregressive distributed lag (ARDL) model, each in a time-series format with feedback mechanisms inside. After regularizing the L1 (Lasso regression) and L2 (ridge regression) metrics to minimize the overfitting problem, the optimal fuzzy-ARDL model is obtained. Turkey’s primary energy consumption is projected based on the best model by benchmarking the static and dynamic possibilistic fuzzy regression models according to their training and test values.},
  archive      = {J_IJFS},
  author       = {Eren, Miraç and Baets, Bernard De},
  doi          = {10.1007/s40815-024-01773-5},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {237-249},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Forecasting turkey’s primary energy demand based on fuzzy auto-regressive distributed lag models with symmetric and non-symmetric triangular coefficients},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Associated probabilities’ aggregations in fuzzy
collaborative filtering recommender system prediction. <em>IJFS</em>,
<em>27</em>(1), 204–236. (<a
href="https://doi.org/10.1007/s40815-024-01772-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the expert evaluations transformed in the multi-attribute decision-making (MADM) model are presented in the discrimination q-rung picture linguistic fuzzy numbers (q-RPLFNs). In the construction of a second-order additive fuzzy measure (TOAFM) the attributes’ interaction indexes and Shapley values are taken into account. The Shapley entropy maximum principle for identification of associated probabilities class (APC) of a TOAFM is constructed. Based on the APC of the TOAFM, a new aggregation operators’ class is constructed which represents some hybrid extensions of ordered weighted averaging (OWA), geometric (OWG), the Choquet integral averaging (CA) and geometric (CG) operators under discrimination q-rung orthopair fuzzy (q-ROF) and q-rung picture linguistic fuzzy (q-RPLF) information. These operators, constructed for the q-RPLF and q-ROF environments, take into account the overall pair interactions among attributes. Main properties on the correctness of extensions are proved: for the lower and upper capacities of order 2, all constructed operators consequently coincide with q-ROF and q-RPLF Choquet averaging and geometric operators, respectively. Constructed operators in the evaluation of prediction of fuzzy Collaborative Filtering Recommender Systems (CFRS) are used. New symmetric discrimination measures as some extensions of discrimination measures for the fuzzy CFRS are proposed. Users’ profile data by the constructed operators in the new similarity measure under q-rung picture linguistic environment are aggregated. The developed new approach is schematically described in such a way that it can be “embedded” in any existing CFRS model. An example is given to illustrate the results, for which the software designed to aggregate profile data for similarity comparison provides the use of new and well-known classical aggregation operators.},
  archive      = {J_IJFS},
  author       = {Sirbiladze, Gia and Garg, Harish and Khutsishvili, Irina and Midodashvili, Bidzina and Gugunava, Oleg},
  doi          = {10.1007/s40815-024-01772-6},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {204-236},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Associated probabilities’ aggregations in fuzzy collaborative filtering recommender system prediction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Z-cloud rough fuzzy-based PIPRECIA and CoCoSo integration to
assess agriculture decision support tools. <em>IJFS</em>,
<em>27</em>(1), 190–203. (<a
href="https://doi.org/10.1007/s40815-024-01771-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The livestock sector has exacerbated the problems of ensuring global food safety and greenhouse gas emissions. The rapid increase in livestock production has called to shed light on decision-support tools that develop sustainable production strategies. In this context, this study aims to expand the application of multiple-criteria decision analysis (MCDM) methods to assign weights to criteria and classify decision support tools for livestock with a high degree of certainty. In order to begin serious steps to address the global sustainability problem, this study extended the PIPRECIA method with a high-certainty fuzzy environment called Z-cloud rough numbers (ZCRNs) to record the weight of 19 criteria for decision support tools in livestock farming. An innovative and advanced method called CoCoSo has been utilized to rank decision-support tools for livestock farming. The methodology included two stages. The first phase involved developing the decision matrix. The second phase encompassed developing MCDM methods by clarifying the steps of the PIvot Pairwise RElative Criteria Importance Assessment (PIPRECIA) method for assigning weight to criteria, in addition to highlighting the steps of the CoCoSo method for classifying decision support tools in the livestock industry. The results of the PIPRECIA method extended to the fuzzy environment of ZCRNs confirmed that visualization and herd characteristics received the highest weight compared to the rest of the criteria of decision support tools. The CoCoSo results provided insight into ranking alternatives for livestock decision support tools. AgRECalc has the highest ranking, and FCFC has the lowest ranking. This study conducted an evaluation test to increase the chances of generalizing the results of ranking decision-support tools of the livestock industry.},
  archive      = {J_IJFS},
  author       = {Alnoor, Alhamzah and Muhsen, Yousif Raad and Husin, Nor Azura and Chew, XinYing and Zolkepli, Maslina Binti and Manshor, Noridayu},
  doi          = {10.1007/s40815-024-01771-7},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {190-203},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Z-cloud rough fuzzy-based PIPRECIA and CoCoSo integration to assess agriculture decision support tools},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bipolar-valued complex hesitant fuzzy dombi aggregating
operators based on multi-criteria decision-making problems.
<em>IJFS</em>, <em>27</em>(1), 162–189. (<a
href="https://doi.org/10.1007/s40815-024-01770-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex Hesitant Fuzzy sets are a powerful tool for depicting vagueness and uncertainty. This paper addresses to Bipolar-Valued Complex Hesitant Fuzzy sets (BVCHFSs) to decode inconsistent, complexity data because of including bipolarity being opposite polar, complexity dividing membership value into two parts, hesitation degree including several membership values. Then, we interpret some new rules such as addition, scalar multiplication, scalar power, multiplication, and present score function. Moreover, some aggregation operators based on BVCHFSs are presented, such as Bipolar-valued Complex Hesitant Fuzzy-Weighted Dombi Averaging operator (BVCHFWDA), Ordered and Hybrid concepts, and Bipolar valued Complex Hesitant Fuzzy-Weighted Dombi Geometric operator (BVCHFWDG), Ordered and Hybrid structures, and some properties, such as idempotency, monotonicity, and boundedness. Later on, the obtained operators are applied over an investment example to show originality and efficiency of suggested instructions. We test to merits and restrictions of the new instructions by comparing them with some existing measures based on bipolar complex fuzzy sets. The comparative analysis indicates that our discussed operators and distance measures over bipolar complex fuzzy sets are agreement especially for BVCHFWDA.},
  archive      = {J_IJFS},
  author       = {Özlü, Şerif},
  doi          = {10.1007/s40815-024-01770-8},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {162-189},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Bipolar-valued complex hesitant fuzzy dombi aggregating operators based on multi-criteria decision-making problems},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controllability and observability of non-homogeneous
granular descriptor fractional dynamical systems applied in electrical
circuit. <em>IJFS</em>, <em>27</em>(1), 144–161. (<a
href="https://doi.org/10.1007/s40815-024-01769-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the study of controllability and observability of fuzzy fractional descriptor dynamical system in terms of granular differentiability. The granular fuzzy solution for granular descriptor fractional dynamical system (GrDFDS) is obtained by using the Mittag–Leffler function and granular Laplace transform and the solution is represented in terms of the state transfer matrix. The controllability and observability of GrDFDS is analysed with the help of theorems using the controllability Gramian matrix and observability Gramian matrix. In order to illustrate the efficacy of our findings, we hereby give the controllability analysis of an engineering problem pertaining to the compatibility of a descriptor fractional electric circuit. The graph visually represents the outcome, indicating that the granular descriptor dynamical system of the electric circuit can be effectively controlled, leading to the successful transition of its state from the granular initial to the final state.},
  archive      = {J_IJFS},
  author       = {Srilekha, R. and Parthiban, V.},
  doi          = {10.1007/s40815-024-01769-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {144-161},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Controllability and observability of non-homogeneous granular descriptor fractional dynamical systems applied in electrical circuit},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplicative sampled-data control for interval type-2
fuzzy interconnected PDE systems under memory event-triggered scheme.
<em>IJFS</em>, <em>27</em>(1), 125–143. (<a
href="https://doi.org/10.1007/s40815-024-01768-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the multiplicative sampled-data control for the interconnected non-linear partial differential equation (PDE) systems with parameter uncertainties. First, an interval type-2 (IT2) Takagi–Sugeno fuzzy model is employed to reconstruct the studied system. In contrast to type-1 fuzzy sets, IT2 fuzzy sets can handle parameter uncertainties that type-1 fuzzy sets cannot handle, and they can characterize parameter uncertainties by utilizing upper and lower membership functions. Next, based on the IT2 fuzzy model, a sampled-data IT2 fuzzy controller containing multiplicative control gain uncertainties is designed to reduce the control cost, where a Bernoulli distribution is adopted to depict the stochastically occurring multiplicative gain uncertainties. Moreover, to conserve communication resources, a memory event-triggered strategy (METS) is employed to decrease the amount of useless data transmitted in the network channel. In contrast to the event-triggered strategy (ETS), the METS triggers these data with a small relative error between the current data and the latest published data, thereby achieving better control. Finally, an example is given to demonstrate the validity of the proposed methodology.},
  archive      = {J_IJFS},
  author       = {Zheng, Danjing and Song, Xiaona and Zhang, Liang and Song, Shuai and Peng, Zenglong},
  doi          = {10.1007/s40815-024-01768-2},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {125-143},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Multiplicative sampled-data control for interval type-2 fuzzy interconnected PDE systems under memory event-triggered scheme},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel robust control and optimal design for fuzzy unmanned
surface vehicles (USVs). <em>IJFS</em>, <em>27</em>(1), 110–124. (<a
href="https://doi.org/10.1007/s40815-024-01767-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel high-order robust control is proposed for unmanned surface vehicles (USVs) steering dynamic systems. We creatively adopt the fuzzy set-based characterization to describe the uncertainty. In this way, the fuzzy dynamical USV system is constructed. Then, the proposed control is proven to render the USV system uniformly bounded and uniformly ultimately bounded regardless of the uncertainty. Furthermore, a performance index is designed by considering both system performances and control costs. This index utilized the fuzzy characteristics of the uncertainty. We solve for the optimal control design parameter by minimizing this index. Finally, simulations are performed for different cases. By comparing the detailed simulation results, the effectiveness of the high-order control is demonstrated and the functions of the control parameter are analysed.},
  archive      = {J_IJFS},
  author       = {Li, Chenming and Zhao, Xu and Yu, Rongrong and Chen, Ye-Hwa and Lin, Fei},
  doi          = {10.1007/s40815-024-01767-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {110-124},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A novel robust control and optimal design for fuzzy unmanned surface vehicles (USVs)},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distance-based approach to fuzzy cognitive maps using
pythagorean fuzzy sets. <em>IJFS</em>, <em>27</em>(1), 93–109. (<a
href="https://doi.org/10.1007/s40815-024-01766-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Cognitive Maps (FCMs) have been attracting researchers from a wide application area due to being easy to apply and interpret. Since its proposal, the method has been improved to satisfy the diverse needs of practitioners such as solving different types of problems and representing particular types of uncertainty. The classical FCMs depend highly on the decision-maker judgments and the uncertainty inherent in the judgments deserves significant attention. Although there are several fuzzy extensions integrated into FCMs, the uncertainty caused by the lack of knowledge, the hesitancy of decision makers, and also the limited capacity of humans to deal with pre-defined rules should be considered. To address this issue, a new distance-based approach integrating Pythagorean Fuzzy Sets and FCMs is proposed. To the best of our knowledge, this is the first time this extension is integrated into FCMs. Besides allowing to represent the uncertainty until the end of the calculations, the new approach offers decision makers an easier and more flexible way to assess the strength of existing causal relationships. To provide a comparison between the proposed approach and the classical FCMs, two real-life applications are selected as case studies.},
  archive      = {J_IJFS},
  author       = {Bozdag, Erhan and Kadaifci, Cigdem},
  doi          = {10.1007/s40815-024-01766-4},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {93-109},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A distance-based approach to fuzzy cognitive maps using pythagorean fuzzy sets},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dissipative constraint-based saturation control for fuzzy
markov jump systems within a finite-time interval. <em>IJFS</em>,
<em>27</em>(1), 77–92. (<a
href="https://doi.org/10.1007/s40815-024-01761-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problems of finite-time boundedness and dissipative analysis for a class of discrete-time nonlinear Markov jump systems (MJSs) with disturbances. In particular, the Takagi-Sugeno fuzzy model is applied to the nonlinear plant, and the impact of time-varying actuator saturation is considered in the controller design. The main purpose of this paper is to develop a mode-dependent fuzzy saturation control for fuzzy MJSs over a finite-time interval. With the help of the Lyapunov stability theory and Abel lemma-based finite-sum inequality, it is established that convergence of all states are confirmed through the addressed control design. Correspondingly, the resulting closed-loop system is stochastically finite-time bounded and $$({\mathcal {Q}},{\mathcal {S}},{\mathcal {R}})$$ - $$\gamma$$ -dissipative under linear matrix inequality (LMI) framework. At last, two numerical examples are given to demonstrate the effectiveness and usefulness of the obtained LMI conditions.},
  archive      = {J_IJFS},
  author       = {Kavikumar, Ramasamy and Kaviarasan, Boomipalagan and Kwon, Oh-Min and Sakthivel, Rathinasamy},
  doi          = {10.1007/s40815-024-01761-9},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {77-92},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Dissipative constraint-based saturation control for fuzzy markov jump systems within a finite-time interval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel interval type-2 fuzzy CPT-TODIM method for
multi-criteria group decision making and its application to credit risk
assessment in supply chain finance. <em>IJFS</em>, <em>27</em>(1),
54–76. (<a href="https://doi.org/10.1007/s40815-024-01759-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assessment of credit risk in supply chain finance (SCF) stands as a pivotal procedure in facilitating enterprises to identify appropriate financing solutions, reduce financing costs, enhance capital utilization efficiency, and mitigate the risk of debt default. Multi-criteria group decision-making (MCGDM), a systematic evaluation tool, is widely used for the assessment of both qualitative and quantitative criteria. However, the conventional framework of MCGDM exhibits limitations in addressing scenarios characterized by high uncertainty in risk information, disparity in weights among decision-makers (DMs) and criteria, alongside complex and non-linear risk perception. To address these limitations, this paper introduces an analytical model that integrates Interval Type-2 Fuzzy Sets (IT2FSs), Cumulative Prospect Theory (CPT), and the TODIM (an acronym from Portuguese for Interactive and Multicriteria Decision Making) method to evaluate credit risk in SCF. Firstly, the IT2FSs are utilized to represent high uncertainty in risk assessment information of DMs. Secondly, the Dice Similarity is applied to determine the weights of DMs. Then, we seek to improve the Criterion Importance Through Intercriteria Correlation (CRITIC) method by addressing its limitations and further integrating it with the Bayesian Best–Worst Method (BBWM), offering a robust computational framework of integrated weights for criteria. Finally, the CPT-TODIM method based on IT2FSs is applied in a real case from Ping An Bank. Through rigorous sensitivity and comparative analyses conducted within the real-world context of SCF credit risk assessments, the proposed model’s theoretical robustness and practical applicability are emphatically validated.},
  archive      = {J_IJFS},
  author       = {Li, Wen and Wang, Luqi and Rehman, Obaid Ur},
  doi          = {10.1007/s40815-024-01759-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {54-76},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A novel interval type-2 fuzzy CPT-TODIM method for multi-criteria group decision making and its application to credit risk assessment in supply chain finance},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monotonic fuzzy systems with goniometric membership
functions. <em>IJFS</em>, <em>27</em>(1), 43–53. (<a
href="https://doi.org/10.1007/s40815-024-01758-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy logic-based systems are nowadays commonly used in nonlinear function approximation when incoming data are available. Their main advantage is that the resulting rules can be interpreted understandably. Nevertheless, when the data are noisy an overfitting may occur which leads to poor accuracy and generalization ability. Prior information about the nonlinear function may improve fuzzy system performance. In this paper the case when the function is monotonic with respect to some or all variables is considered. Sufficient conditions for the monotonicity of first-order Takagi–Sugeno fuzzy systems with raised cosine membership functions are derived. Performance of the proposed fuzzy system is tested on two benchmark datasets},
  archive      = {J_IJFS},
  author       = {Hušek, Petr},
  doi          = {10.1007/s40815-024-01758-4},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {43-53},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Monotonic fuzzy systems with goniometric membership functions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double uncertainty driving and integrated decision-making
under the mixed probabilistic hesitant environment. <em>IJFS</em>,
<em>27</em>(1), 27–42. (<a
href="https://doi.org/10.1007/s40815-024-01755-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic hesitant fuzzy sets are an emerging and popular tool in the decision-making field, skilled at describing subjective information. However, it does not take into account the cognitive limitations of decision makers and the complexity of the actual environment can lead to situations such as multiple values or missing probabilistic information. Considering these issues, we first develop the uncertain probabilistic hesitant fuzzy set (UPHFS) to propose the mixed hesitant fuzzy set (MHFS). Moreover, we further give the computational rules and prove the generalization and robustness of the MHFS. Then, we design two new ratio models, namely the mixed probabilistic hesitant crossover ratio (MPHCR) model and mixed preference probabilistic hesitant crossover ratio (MPPHCR) model, and construct an integrated decision-making model based on them. The integrated decision-making model is a collection of integrated decision models for deriving unknown and mixed probabilities and computing optimal decision outcomes, which is different from the previous studies than the similar studies. Therefore, the model results could be more robust and reliable. Further, based on the proposed fuzzy environments and new decision-making models, we give the whole calculation process and integrated decision-making steps. Lastly, an illustrated example of project investment is provided to apply the above methods, processes, and steps and shows their effectiveness.},
  archive      = {J_IJFS},
  author       = {Zhou, Wei and Luo, Danxue and Xu, Zeshui},
  doi          = {10.1007/s40815-024-01755-7},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {27-42},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Double uncertainty driving and integrated decision-making under the mixed probabilistic hesitant environment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel interval type-2 ANFIS modeling based on one-step type
reducer algorithm. <em>IJFS</em>, <em>27</em>(1), 13–26. (<a
href="https://doi.org/10.1007/s40815-024-01754-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel structure of Interval Type-2 Adaptive Network-Fuzzy Inference System (IT2-ANFIS) for modeling dynamic systems is proposed. Optimization algorithms are introduced to adjust the antecedent and consequent parameters of a fuzzy model. In order to avoid the classical iterative process commonly used in type reduction algorithms, a new one-step type reduction algorithm (OSTRA) is proposed, which in conjunction with IT2-ANFIS is tested on nonlinear dynamical system and one real system datasets. Furthermore, to validate the complete structure, experiments with numerical models are performed, in order to show the advantages of the proposed novel IT2-ANFIS structure, by obtaining better results than previously published T2-ANFIS structures. To illustrate a real application of the proposed modeling technique, a model obtained from a tractor steering wheel system was embedded in an electronic I/O board, to obtain a comparison of the on-line fuzzy model against the physical system, with satisfactory results in the approximation error.},
  archive      = {J_IJFS},
  author       = {Alberto-Rodríguez, Adrián and López-Morales, Virgilio and Ramos-Fernández, Julio Cesar},
  doi          = {10.1007/s40815-024-01754-8},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {13-26},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Novel interval type-2 ANFIS modeling based on one-step type reducer algorithm},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy rules data-driven equivalent model with multi-gradient
learning for discrete-time nearly optimal control. <em>IJFS</em>,
<em>27</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s40815-024-01727-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A behavior of switchable control directions is investigated for the non-holonomic robotic system considered as a class of unknown nonlinear discrete-time systems. The data-driven equivalent model is established by a multi-input fuzzy rule emulated network and the multi-gradient learning law is developed to tune all adjustable parameters. Thereafter, the nearly optimal controller is derived using the dynamics of the equivalent model and the closed-loop performance is analyzed through rigorous mathematical analysis. The experimental system is constructed to validate the effectiveness of the proposed scheme and the advantage of the multi-gradient approach.},
  archive      = {J_IJFS},
  author       = {Treesatayapun, C.},
  doi          = {10.1007/s40815-024-01727-x},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Fuzzy rules data-driven equivalent model with multi-gradient learning for discrete-time nearly optimal control},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijmir---10">IJMIR - 10</h2>
<ul>
<li><details>
<summary>
(2025). STCA: An action recognition network with spatio-temporal
convolution and attention. <em>IJMIR</em>, <em>14</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s13735-024-00350-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution and self-attention mechanisms are two commonly used methods in the field of video understanding. Convolution preserves spatiotemporal relationships in video data while reducing the number of parameters and computations. The self-attention mechanism captures global and long-distance dependencies in sequence data. To address the challenges of low accuracy and excessive parameters in networks for action recognition, we propose a new network that combines convolution and self-attention mechanisms (STCA). STCA consists of two modules: efficient spatiotemporal convolution (ESTConv) and spatiotemporal self-attention (STA). ESTConv extracts local spatiotemporal features of actions, enabling fast reasoning. STA consists of two sub-modules: the spatial self-attention (SA) and the temporal self-attention (TA). SA analyzes the spatial characteristics of actions, while TA analyzes their temporal characteristics. We conducted experiments on the Kinetics400, UCF101, HMDB51, and Something-Something V2 datasets to evaluate our network. Results show that STCA achieves accuracy comparable to the leading action recognition models while reducing parameters by over 20%, making it more lightweight than current best-performing models.},
  archive      = {J_IJMIR},
  author       = {Tian, Qiuhong and Miao, Weilun and Zhang, Lizao and Yang, Ziyu and Yu, Yang and Zhao, Yanying and Yao, Lan},
  doi          = {10.1007/s13735-024-00350-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {STCA: An action recognition network with spatio-temporal convolution and attention},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAMIR: Fine-tuning CLIP and multi-head cross-attention
mechanism for multimodal image retrieval with sketch and text features.
<em>IJMIR</em>, <em>14</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s13735-024-00352-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches and texts are two input modes of queries that are widely used in image retrieval tasks of different granularities. Text-based image retrieval (TBIR) is mainly used for coarse-grained retrieval, while sketch-based image retrieval (SBIR) aims to retrieve images based on hand-drawn sketches, which pose unique challenges due to the abstract nature of sketches. Existing methods mainly focus on retrieval based on a single modality but fail to explore the connections between multiple modalities comprehensively. In addition, the emerging contrastive language image pre-training (CLIP) model and powerful contrastive learning methods are underexplored in this field. We propose a novel multimodal image retrieval framework (CAMIR) to address these challenges. It obtains sketch and text features through a fine-tuned CLIP model, fuses the extracted features using multi-head cross-attention, and combines contrastive learning for retrieval tasks. In the indexing stage, we introduce Faiss, an open-source similarity search library developed by Meta AI Research, to enhance retrieval efficiency. Comprehensive experiments on the benchmark dataset Sketchy demonstrate the effectiveness of our proposed framework, achieving superior performance compared to existing methods while highlighting the potential of integrating sketch and text features for retrieval tasks.},
  archive      = {J_IJMIR},
  author       = {Yang, Fan and Ismail, Nor Azman and Pang, Yee Yong and Alsayed, Alhuseen Omar},
  doi          = {10.1007/s13735-024-00352-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {CAMIR: Fine-tuning CLIP and multi-head cross-attention mechanism for multimodal image retrieval with sketch and text features},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving skeleton-based action recognition with interactive
object information. <em>IJMIR</em>, <em>14</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s13735-024-00351-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose. However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions. We propose a new action recognition framework introducing object nodes to supplement absent interactive object information. We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes. Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes. At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure. Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack. Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information. Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks. The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7%, and on cross-view split, it is 99.2%. The project page: https://github.com/moonlight52137/ST-VGCN .},
  archive      = {J_IJMIR},
  author       = {Wen, Hao and Lu, Ziqian and Shen, Fengli and Lu, Zhe-Ming and Cui, Jialin},
  doi          = {10.1007/s13735-024-00351-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Improving skeleton-based action recognition with interactive object information},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-matrix guided reconstruction hashing for unsupervised
cross-modal retrieval. <em>IJMIR</em>, <em>14</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s13735-025-00353-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross-modal hashing, due to its independence from heavy label information, is more convenient for application to other fields. In recent years, this area has gained widespread attention and achieved great success. However, existing unsupervised cross-modal hashing methods still face some issues, such as simple fusion after feature extraction, the use of a single similarity measure to express data relationships, and guiding hash code learning through a single affinity matrix. To address these problems, we propose a new method called Dual-Matrix Guided Reconstruction Hashing for Unsupervised Cross-Modal Retrieval. We construct an effective matrix from the extracted raw semantic information to guide the generation of reconstructed hash codes for images and texts. Simultaneously, we construct another matrix for the extracted image and text features, guiding the generation of reconstructed hash codes using graph convolution, thus directing hash code learning through dual matrices. In evaluations on three standard datasets, our method achieved an average improvement of approximately 1.3% in MAP@5000 and 1.5% in MAP@50, particularly showing significant performance gains with shorter hash codes.},
  archive      = {J_IJMIR},
  author       = {Lin, Ziyong and Jiang, Xiaolong and Zhang, Jie and Li, Mingyong},
  doi          = {10.1007/s13735-025-00353-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Dual-matrix guided reconstruction hashing for unsupervised cross-modal retrieval},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task classification network for few-shot learning.
<em>IJMIR</em>, <em>14</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s13735-025-00354-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic information provides both internal coherence within categories and distinctiveness between categories that surpass mere visual concepts. Semantic information has been employed in Few-Shot Learning (FSL) to achieve additional performance improvements. Previous methods usually combine support image and semantic information to classify query image. However, in FSL, it is challenging to train a model on a limited base dataset such that the model can effectively fuse or interact with both modalities and obtain better feature representation on the novel dataset. To address this problem, we propose a Multi-task Classification Network (MCN) to decompose the current classification problem into a image-image classification problem and a semantic-image classification problem. Considering the issue that the results of image-image classification and semantic-image classification may not always be trustworthy, we introduce an Uncertainty-Aware Decision Module (UADM) which biases the final classification result towards the result with lower uncertainty in the two types of classification. Extensive experimental results on three datasets have consistently shown that our proposed method achieves impressive results. Particularly, compared to the baseline, we achieved a 2–3% improvement on the CUB, SUN, and Flower datasets in both the 5-way 1-shot and 5-way 5-shot settings.},
  archive      = {J_IJMIR},
  author       = {Ji, Zhong and Liu, Yuanheng and Wang, Xuan and Liu, Jingren and Cao, Jiale and Yu, YunLong},
  doi          = {10.1007/s13735-025-00354-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-task classification network for few-shot learning},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized RT-DETR for accurate and efficient video object
detection via decoupled feature aggregation. <em>IJMIR</em>,
<em>14</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s13735-025-00355-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection (VOD) is a challenging task, and image object detectors are difficult to detect degradation phenomena in certain video frames. However, existing research on VOD mostly trades high computational costs for accuracy, making it difficult to achieve a balance between accuracy and speed. This work proposes an optimized Real-Time Detection Transformer (RT-DETR) model for VOD that introduces a decoupled Feature Aggregation Module (FAM) to separately refine the localization and classification detection heads. This method only requires a minimal increase in the number of parameters to achieve significant improvements in accuracy. Specifically, we insert FAM before the localization detection head and classification detection head, and first freeze all parameters of the feature extractor and classification detection head to train only the parameters of the localization detection head to obtain more accurate localization results. Then, we freeze all parameters of the feature extractor and localization detection head to train only the parameters of the classification detection head to improve the final detection accuracy. We have conducted a large number of ablation experiments to verify the effectiveness of the method. Without using any post-processing methods, we achieved 90.0% mAP on the ImageNet-VID dataset, with only 77.9 M parameters and an average inference speed of 14.1ms.},
  archive      = {J_IJMIR},
  author       = {Chen, Hao and Huang, Wu and Zhang, Tao},
  doi          = {10.1007/s13735-025-00355-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Optimized RT-DETR for accurate and efficient video object detection via decoupled feature aggregation},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAMoE-MSA: Polarity-aware mixture of experts network for
multimodal sentiment analysis. <em>IJMIR</em>, <em>14</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s13735-025-00362-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) is a challenging task that aims to understand human emotions from text, visual, and audio modalities. Existing studies struggle to capture the monotonic relationship between emotional expressions. This monotonic relationship means that the emotional intensity changes consistently with the expression amplitude when considering emotional polarities, which is a crucial aspect of MSA tasks. To tackle this, we propose a polarity-aware mixture of experts network (PAMoE-MSA). PAMoE-MSA is capable of learning polarity-specific and polarity-common features to capture the monotonic relationship of emotional expressions from multimodal sentiment data. Our model consists of three experts: a positive expert, a negative expert, and a general expert. They are trained through a unique Guide Task, where the positive and negative experts are trained by non-neutral samples, while the general expert is trained by all samples. A gating mechanism is utilized to adaptively perceive the monotonic relationship within emotional expressions. Moreover, the self-supervised labels are introduced to preserve modality-specific information. The experts module is fed with the fusion features, which contain richer emotional information. To enhance model stability during the training phase, we employ multi-side contrastive learning before making predictions. Our evaluation of PAMoE-MSA on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets shows notable improvements over state-of-the-art methods, with increases of approximately 1.3% in Acc-7 for CMU-MOSI, 1.2% in Acc-2 for CMU-MOSEI, and 0.8% in F1-score for CH-SIMS.},
  archive      = {J_IJMIR},
  author       = {Huang, Changqin and Lin, Zhenheng and Han, Zhongmei and Huang, Qionghao and Jiang, Fan and Huang, Xiaodi},
  doi          = {10.1007/s13735-025-00362-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {PAMoE-MSA: Polarity-aware mixture of experts network for multimodal sentiment analysis},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFAFD: A few-shot learning method for cascading models with
parameter free attention and finite discrete space. <em>IJMIR</em>,
<em>14</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s13735-025-00357-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on existing learning models, multimodal approaches have demonstrated promising performance in the realm of few-shot learning, owing to contrastive language-image pretraining. However, shortcomings persist in multimodal fusion methods, particularly in aligning textual and visual features across different granularity levels, and in the independent feature extraction by encoders lacking interaction. Therefore, this paper proposes MFAFD: a cascaded model for few-shot learning featuring parameter free attention mechanisms and a finite discrete space. Initially, the model employs a parameter free attention module in the pretraining phase to facilitate cross-modal interactions, enhancing alignment between spatial features of images and generated text prior to extracting global features from images via CLIP. This bidirectional update of textual and visual information addresses the issue of feature alignment. During training, the model leverages a representation based on Finite Discrete Space (FDS), constructing a finite discrete space foundation for textual and image features, effectively bridging modal differences. Ultimately, using text as a baseline, the model predicts image classification based on similarity weights between images and text. Through quantitative and qualitative analyses, this study demonstrates that parameter free attention mechanisms and finite discrete space modules significantly enhance the performance of cascaded multimodal aggregation models. The model exhibits robust performance in few-shot classification across multiple datasets. The code is available at https://github.com/turelove999/MFDFA-dj .},
  archive      = {J_IJMIR},
  author       = {Xue, Lixia and Dong, Jiang and Wang, Ronggui and Yang, Juan},
  doi          = {10.1007/s13735-025-00357-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MFAFD: A few-shot learning method for cascading models with parameter free attention and finite discrete space},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image forgery classification and localization through vision
transformers. <em>IJMIR</em>, <em>14</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s13735-025-00358-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the easy availability of software over the Internet, any naive user can tamper the images for entertainment purposes or to defame a personality by circulating over social media networks. The practice of image tampering is a serious issue and can attract legal action if proven guilty. Forensic researchers employ various methods to detect and localize image forgeries. In this research, we use a Vision transformer (ViT) as a method for binary classification of images distinguishing forged and unforged images. Further, we use a pre-trained Segment Anything Model(SAM) which is fine-tuned with custom data to adaptively recognize patterns indicating forged regions within the images. SAM can localize these forged areas and is leveraged to create templates by extracting the identified regions. The proposed method is rigorously tested across various datasets, including CASIA v1.0, CASIA v2.0, MICC-F2000, MICC-F600, and Columbia. Through comprehensive experimentation, our approach showcases considerable promise yielding accuracy in image forgery classification and localization. Our model’s robustness and adaptability make it an attractive tool for forensic analysis in diverse scenarios, contributing to the advancement of multimedia forensics security research.},
  archive      = {J_IJMIR},
  author       = {Pawar, Digambar and Gowda, Raghavendra and Chandra, Krishna},
  doi          = {10.1007/s13735-025-00358-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Image forgery classification and localization through vision transformers},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VPC-VoxelNet: Multi-modal fusion 3D object detection
networks based on virtual point clouds. <em>IJMIR</em>, <em>14</em>(1),
1–11. (<a href="https://doi.org/10.1007/s13735-025-00360-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the impact of sparsity and disorder of point clouds on object detection accuracy, this paper proposes a multi-modal fusion network VPC-VoxelNet based on virtual point clouds. Firstly, virtual point clouds are constructed using image detection object information to increase the density of point clouds, thus improving the performance of object features; Secondly, increasing the dimensionality of point cloud features, distinguishing virtual point clouds and avoiding the accumulation of multi model errors; Finally, an optimized loss function such as the scale factor of the virtual point cloud is used to improve the training efficiency of the multi-modal network. The object detection network, VPC-VoxelNet, was tested on the KITTI dataset, and the detection accuracy was better than that of the classical 3D point cloud detection network and certain multi-modal information fusion networks, with a vehicle detection accuracy of 86.9%.},
  archive      = {J_IJMIR},
  author       = {Zhang, Qiang and Shi, Qin and Cheng, Teng and Zhang, Junning and Chen, Jiong},
  doi          = {10.1007/s13735-025-00360-0},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {VPC-VoxelNet: Multi-modal fusion 3D object detection networks based on virtual point clouds},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijmlc---39">IJMLC - 39</h2>
<ul>
<li><details>
<summary>
(2025). Beyond traditional visual object tracking: A survey.
<em>IJMLC</em>, <em>16</em>(2), 1435–1460. (<a
href="https://doi.org/10.1007/s13042-024-02345-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single object tracking is a vital task of many applications in critical fields. However, it is still considered one of the most challenging vision tasks. In recent years, computer vision, especially object tracking, witnessed the introduction or adoption of many novel techniques, setting new fronts for performance. In this survey, we visit some of the cutting-edge techniques in vision, such as Sequence Models, Generative Models, Self-supervised Learning, Unsupervised Learning, Reinforcement Learning, Meta-Learning, Continual Learning, and Domain Adaptation, focusing on their application in single object tracking. We propose a novel categorization of single object tracking methods based on novel techniques and trends. Also, we conduct a comparative analysis of the performance reported by the methods presented on popular tracking benchmarks. Moreover, we analyze the pros and cons of the presented approaches and present a guide for non-traditional techniques in single object tracking. Finally, we suggest potential avenues for future research in single-object tracking.},
  archive      = {J_IJMLC},
  author       = {Abdelaziz, Omar and Shehata, Mohamed and Mohamed, Mohamed},
  doi          = {10.1007/s13042-024-02345-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1435-1460},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Beyond traditional visual object tracking: A survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grouping attributes: An accelerator for attribute reduction
based on similarity. <em>IJMLC</em>, <em>16</em>(2), 1417–1433. (<a
href="https://doi.org/10.1007/s13042-024-02344-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of attribute reduction, the method of attribute selection holds significant importance. Different selection methods can significantly impact the efficiency and outcomes of attribute reduction. Presently, the common approach involves constructing a measure to evaluate the significance of attribute and subsequently selecting the optimal attribute to include in the reduction set based on this measure. However, most existing measures predominantly focus on the relationship between attributes and target concepts, overlooking the inter-relationships among attributes themselves. This paper defines the relationships between attributes based on the variation in significance and utilizes this concept to design a general algorithm for accelerating attribute reduction. Firstly, we introduce the concept of the similarity of attributes, where attributes exhibiting greater overlap in their classification abilities for the target set are deemed more similar. Subsequently, utilizing this concept, a cover is constructed over the original set of attributes. Then, an algorithm for accelerating attribute reduction is devised by integrating the constructed cover into a greedy attribute reduction algorithm. Finally, we conduct experiments on 12 UCI datasets. Compared to four benchmark algorithms, the proposed algorithm effectively reduces runtime while maintaining classification accuracy. The effectiveness of the algorithm is further validated using the Wilcoxon signed-rank test.},
  archive      = {J_IJMLC},
  author       = {Jia, Yunlong and Zhu, Ping},
  doi          = {10.1007/s13042-024-02344-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1417-1433},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Grouping attributes: An accelerator for attribute reduction based on similarity},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning cluster-wise label distribution for label
enhancement. <em>IJMLC</em>, <em>16</em>(2), 1403–1415. (<a
href="https://doi.org/10.1007/s13042-024-02343-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label enhancement (LE) refers to the process of recovering label distributions from logical labels for less ambiguity. Current LE techniques concentrate on learning each instance individually, which ignores the instance correlation. In this paper, we propose to learn a cluster-wise label distribution (CWLD) shared by all instances of the cluster to explore the instance correlation. The softmax-normalized sum of the CWLD and the logical label vector yields the label distribution. CWLD is learned in an iterative manner. Following instance clustering, the label distributions of all instances in each cluster are averaged. The asymmetric label correlation is then mined using heat conduction. This process is repeated until the label distribution has reached a point of convergence. Experiments were undertaken on thirteen real-world datasets compared with six state-of-the-art algorithms. Results demonstrate the effectiveness and superiority of our proposed method.},
  archive      = {J_IJMLC},
  author       = {Fan, Jun and Zhang, Heng-Ru and Min, Fan},
  doi          = {10.1007/s13042-024-02343-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1403-1415},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Learning cluster-wise label distribution for label enhancement},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relevance-aware visual entity filter network for multimodal
aspect-based sentiment analysis. <em>IJMLC</em>, <em>16</em>(2),
1389–1402. (<a
href="https://doi.org/10.1007/s13042-024-02342-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal aspect-based sentiment analysis, which aims to identify the sentiment polarities over each aspect mentioned in an image-text pair, has sparked considerable research interest in the field of multimodal analysis. Despite existing approaches have shown remarkable results in incorporating external knowledge to enhance visual entity information, they still suffer from two problems: (1) the image-aspect global relevance. (2) the entity-aspect local alignment. To tackle these issues, we propose a Relevance-Aware Visual Entity Filter Network (REF) for MABSA. Specifically, we utilize the nouns of ANPs extracted from the given image as bridges to facilitate cross-modal feature alignment. Moreover, we introduce an additional “UNRELATED” marker word and utilize Contrastive Content Re-sourcing (CCR) and Contrastive Content Swapping (CCS) constraints to obtain accurate attention weight to identify image-aspect relevance for dynamically controlling the contribution of visual information. We further adopt the accurate reversed attention weight distributions to selectively filter out aspect-unrelated visual entities for better entity-aspect alignment. Comprehensive experimental results demonstrate the consistent superiority of our REF model over state-of-the-art approaches on the Twitter-2015 and Twitter-2017 datasets.},
  archive      = {J_IJMLC},
  author       = {Chen, Yifan and Xiong, Haoliang and Li, Kuntao and Mai, Weixing and Xue, Yun and Cai, Qianhua and Li, Fenghuan},
  doi          = {10.1007/s13042-024-02342-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1389-1402},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Relevance-aware visual entity filter network for multimodal aspect-based sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering hidden patterns: Low-rank label correlations for
multi-label weak-label learning. <em>IJMLC</em>, <em>16</em>(2),
1371–1387. (<a
href="https://doi.org/10.1007/s13042-024-02341-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning has emerged as a prominent research area in machine learning, as each instance can be associated with multiple class labels. However, many multi-label learning algorithms assume that the label space is complete, whereas in real-world applications, we often only have access to partial label information. To address this issue, we propose a novel Multi-label Weak-label learning algorithm via Low-rank Label correlations (MW2L). First, we propagate the structural and semantic information from the feature space to the label space to effectively capture label-related information and recover lost labels. Second, we incorporate global and local low-rank label correlation information to ensure that the label-related matrix is informative. Last, we use label correlations to supplement the original weak-label matrix and form a unified learning framework. We evaluate the performance of our approach on several benchmark datasets and show that it outperforms state-of-the-art methods in terms of accuracy and robustness to weak-label noise. The proposed approach can effectively handle incomplete and noisy weak labels in multi-label learning and outperforms existing methods.},
  archive      = {J_IJMLC},
  author       = {Li, Tianli and Nasrudin, Mohammad Faidzul and Zhao, Dawei and Chen, Fei and Peng, Xing and Sarim, Hafiz Mohd},
  doi          = {10.1007/s13042-024-02341-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1371-1387},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Uncovering hidden patterns: Low-rank label correlations for multi-label weak-label learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new neural network method for solving bratu type equations
with rational polynomials. <em>IJMLC</em>, <em>16</em>(2), 1355–1369.
(<a href="https://doi.org/10.1007/s13042-024-02340-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bratu-type equation is a fundamental differential equation with numerous applications in engineering fields, such as radiative heat transfer, thermal reaction, and nanotechnology. This paper introduces a novel approach known as the rational polynomial neural network. In this approach, rational orthogonal polynomials are utilized within the neural network’s hidden layer. To solve the equation, the initial boundary value conditions of both the differential equation and the rational polynomial neural network are integrated into the construction of the numerical solution. This construction transforms the Bratu-type equation into a set of nonlinear equations, which are subsequently solved using an appropriate optimization technique. Finally, three sets of numerical examples are presented to validate the efficacy and versatility of the proposed rational orthogonal neural network method, with comparisons made across different hyperparameters. Furthermore, the experimental results are juxtaposed against traditional methods such as the Adomian decomposition method, genetic algorithm, Laplace transform method, spectral method, and multilayer perceptron, our method exhibits consistently optimal performance.},
  archive      = {J_IJMLC},
  author       = {He, Jilong and Cao, Cong},
  doi          = {10.1007/s13042-024-02340-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1355-1369},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A new neural network method for solving bratu type equations with rational polynomials},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel-patch combination loss for refined edge detection.
<em>IJMLC</em>, <em>16</em>(2), 1341–1354. (<a
href="https://doi.org/10.1007/s13042-024-02338-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental image characteristic, edge features encapsulate a wealth of information, serving as a crucial foundation in image segmentation networks for accurately delineating and partitioning object edges. Convolutional neural networks (CNNs) have gained prominence recently, finding extensive utility in edge detection. Previous methods primarily emphasized edge prediction accuracy, ignoring edge refinement. In this work, we introduce a novel encoder-decoder architecture that effectively harnesses hierarchical features. By extending the decoder horizontally, we progressively enhance resolution to preserve intricate details from the original image, thereby producing sharp edges. Additionally, we propose a novel loss function named the Pixel-Patch Combination Loss (P2CL), which employs distinct detection strategies in edge and non-edge regions to bolster network accuracy and yield crisp edges. Furthermore, considering the practicality of the algorithm, our method strikes a fine balance between accuracy and model size. It delivers precise and sharp edges while ensuring efficient model operation, thereby laying a robust foundation for advancements deployed on mobile devices or embedded systems. Our method was evaluated on three publicly available datasets, including BSDS500, Multicue, and BIPED. The experimental results show the superiority of our approach, achieving a competitive ODS F-score of 0.832 on the BSDS500 benchmark and significantly enhancing edge detection accuracy.},
  archive      = {J_IJMLC},
  author       = {Li, Wenlin and Zhang, Wei and Liu, Yanyan and Liu, Changsong and Jing, Rudong},
  doi          = {10.1007/s13042-024-02338-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1341-1354},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Pixel-patch combination loss for refined edge detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal 6-DoF object pose tracking: Integrating spatial
cues with monocular RGB imagery. <em>IJMLC</em>, <em>16</em>(2),
1327–1340. (<a
href="https://doi.org/10.1007/s13042-024-02336-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate six degrees of freedom (6-DoF) pose estimation is crucial for robust visual perception in fields such as smart manufacturing. Traditional RGB-based methods, though widely used, often face difficulties in adapting to dynamic scenes, understanding contextual information, and capturing temporal variations effectively. To address these challenges, we introduce a novel multi-modal 6-DoF pose estimation framework. This framework uses RGB images as the primary input and integrates spatial cues, including keypoint heatmaps and affinity fields, through a spatially aligned approach inspired by the Trans-UNet architecture. Our multi-modal method enhances both contextual understanding and temporal consistency. Experimental results on the Objectron dataset demonstrate that our approach surpasses existing algorithms across most categories. Furthermore, real-world tests confirm the accuracy and practical applicability of our method for robotic tasks, such as precision grasping, highlighting its effectiveness for real-world applications.},
  archive      = {J_IJMLC},
  author       = {Mei, Yunpeng and Wang, Shuze and Li, Zhuo and Sun, Jian and Wang, Gang},
  doi          = {10.1007/s13042-024-02336-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1327-1340},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-modal 6-DoF object pose tracking: Integrating spatial cues with monocular RGB imagery},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LWTD: A novel light-weight transformer-like CNN architecture
for driving scene dehazing. <em>IJMLC</em>, <em>16</em>(2), 1303–1326.
(<a href="https://doi.org/10.1007/s13042-024-02335-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of artificial intelligence and automation technology, interest in autonomous driving research is also growing. However, under heavy rain, fog, and other adverse weather conditions, the visual quality of the images is reduced due to suspended atmospheric particles that affect the vehicle’s visual perception system, which is not conducive to the autonomous driving system’s accurate perception of the road environment. To address these challenges, this article presents a computationally efficient end-to-end light-weight Transformer-like neural network called LWTD (Light-Weight Transformer-like DehazeNet) to reconstruct haze-free images for driving tasks, which based on the reformulated ASM theory without prior knowledge. First, a strategy for simplifying the atmospheric light and transmission map into a feature map is adopted, a CMT (Convolutional Mapping Transformer) module for the extraction of global features is developed, and the hazy image is decomposed into a base layer (global features) and a detail layer (local features) for Low-Level, Medium-Level, and High-Level stages. Meanwhile, a channel attention module is introduced to weigh and assign the weights of each feature, and to fuse them with the reformulated ASM (Atmospheric Scattering Model) model to restore the haze-free image. Second, a joint loss function of the graphical features is formulated to further direct the network to converge in the direction of abundant features. In addition, a dataset of real-world fog driving is constructed. Extensive experiments with synthetic and natural hazy images confirmed the superiority of the proposed method through quantitative and qualitative evaluations on various datasets. Furthermore, additional experiments validated the applicability of the proposed method for traffic participant detection and semantic segmentation tasks. The source code has been made publicly available on https://github.com/ZebGH/LWTD-Net.},
  archive      = {J_IJMLC},
  author       = {Zhang, Zhenbo and Feng, Zhiguo and Long, Aiqi and Wang, Zhiyu},
  doi          = {10.1007/s13042-024-02335-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1303-1326},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {LWTD: A novel light-weight transformer-like CNN architecture for driving scene dehazing},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation extraction method based on pre-trained model and
bidirectional semantic union. <em>IJMLC</em>, <em>16</em>(2), 1291–1302.
(<a href="https://doi.org/10.1007/s13042-024-02334-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is an important task in natural language processing, which aims to extract the semantic relationships between entities from unstructured text. In traditional relation extraction methods, semantic, contextual and deep representation extraction are not sufficient. In this paper, we propose a relation extraction model (RoBBS, RoBERTa + Bi-GRU + Self Attention) based on pre-training and bi-directional semantic union. Firstly, the RoBERTa pre-training model is used to extracting the contextual features of distant sentences. Then Bi-GRU is leveraged to realize bidirectional semantic union, comprehensively extracting bidirectional semantic information. Combining this network with the attention mechanism allows for assigning greater weights to the semantic information that has a more significant role in determining the relationship classes of sentences. This, in turn, makes feature selection more efficient. Finally, the relationship is classified using the Softmax function. The experimental results show that the model achieves F-score of 89.02% on the SemEval2010 task8 dataset outperforming state-of-the-art models.},
  archive      = {J_IJMLC},
  author       = {He, Xinyu and Yan, Ge and Han, Xue and Kan, Manfei and Ren, Yonggong},
  doi          = {10.1007/s13042-024-02334-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1291-1302},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Relation extraction method based on pre-trained model and bidirectional semantic union},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-way decision method based on COPRAS in the weak
probabilistic linguistic term set information systems. <em>IJMLC</em>,
<em>16</em>(2), 1265–1290. (<a
href="https://doi.org/10.1007/s13042-024-02333-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development and progress of technology, information becomes increasingly diverse, which poses higher demands on decision-making methods. Probabilistic linguistic term set (PLTS) is a tool that can more intuitively express the evaluations of decision makers (DMs). As a specialized form of PLTS with ignored probabilities, weak probabilistic linguistic term set (WPLTS) can describe incomplete or inaccurate evaluation information. Three-way decision (3WD) is an efficient decision-making method that reduces decision cost by adopting delayed decisions on the boundary domain. In this paper, we propose a novel 3WD method by combining 3WD with the complex proportional assessment (COPRAS) method under the WPLTS environment, named the WPLTS-3WD method. Firstly, we introduce the notion of the WPLTS information system. For a WPLTS information system, we propose a method of complementing the ignored probabilities and a new score function. Secondly, the objects are ranked by the COPRAS method. According to the ranking result, we define the dominance relation and dominance sets. Based on the dominance sets, the conditional probabilities can be estimated. By combining the conditional probabilities with relative loss functions, the expected losses will be obtained and the objects can be classified. Moreover, we propose two conversion functions that can convert real-valued and linguistic term evaluation information into PLTS evaluation information. Finally, we use the proposed WPLTS-3WD method to analyze the air quality of four cities. The rationality and advantages of our method are verified through experimental comparisons with other methods and parameter analysis.},
  archive      = {J_IJMLC},
  author       = {Yang, Hai-Long and Liu, Xu and Guo, Zhi-Lian},
  doi          = {10.1007/s13042-024-02333-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1265-1290},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A three-way decision method based on COPRAS in the weak probabilistic linguistic term set information systems},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 5G-SIID: An intelligent hybrid DDoS intrusion detector for
5G IoT networks. <em>IJMLC</em>, <em>16</em>(2), 1243–1263. (<a
href="https://doi.org/10.1007/s13042-024-02332-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The constrained resources of Internet of Things (IoT) devices make them susceptible to Distributed Denial-of-Service (DDoS) attacks that disrupt service availability by overwhelming systems. Thus, effective intrusion detection is critical to ensuring uninterrupted IoT activities. This research presents a scalable system that combines machine and deep learning models with optimized data processing to secure IoT devices against DDoS attacks. A real-world 5G-IoT network simulation dataset was used to evaluate performance. Robust feature selection identified the 10 most informative features from the high-dimensional data. These features were used to train eight classifiers, namely: k-Nearest Neighbors (KNN), Naive Bayes (NB), Decision Tree (DT), Random Forest (RF), Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), Long-Short-Term Memory (LSTM) and hybrid CNN-LSTM models for DDoS attack detection. Experiments demonstrated 99.99% and 99.98% accuracy for multiclass and binary classification using the proposed hybrid CNN-LSTM model. Crucially, time- and space-complexity analysis validates real-world feasibility. Unlike prior works, this system optimally balances accuracy, efficiency, and adaptability through a precisely engineered model architecture, outperforming existing models. In general, this accurate, efficient, and adaptable system addresses critical IoT security challenges, improving cyber resilience in smart cities and autonomous vehicles.},
  archive      = {J_IJMLC},
  author       = {Sadhwani, Sapna and Mathur, Aakar and Muthalagu, Raja and Pawar, Pranav M.},
  doi          = {10.1007/s13042-024-02332-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1243-1263},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {5G-SIID: An intelligent hybrid DDoS intrusion detector for 5G IoT networks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive consensus model for managing non-cooperative
behaviors in portfolio optimization for large companies. <em>IJMLC</em>,
<em>16</em>(2), 1219–1242. (<a
href="https://doi.org/10.1007/s13042-024-02331-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean–variance (MV) model provides numerous optimal portfolios for managing a firm&#39;s asset portfolio. Portfolio decisions in large corporations involve many interest groups, such as shareholders, bondholders, and employees, and require the assistance of large experts. However, experts from different departments with different cognitive levels and interests can differ or even conflict in their assessments of portfolios. To guarantee their interests, some experts may exhibit non-cooperative behavior, thus reducing the efficiency of reaching a consensus. To tackle this issue, the research aims to develop a large-scale group interactive portfolio optimization method that incorporates non-cooperative behaviors and leverages social network analysis (SN-LSGDM-NC-PO). First, various consensus feedback strategies based on minimum adjustment are formulated to provide advice during the negotiation process according to the global and local levels. Then, considering the acceptance of advice and the effect of expert adjustment on consensus, a new measure of non-cooperative behavior is designed. Non-cooperative behavior by experts can affect trust relations in a social network. Therefore, trust reward and penalty mechanisms, preference penalty mechanisms, and an exit mechanism are developed to manage different types of non-cooperative behavior. Experimental and comparison results demonstrate that the proposed SN-LSGDM-NC-PO algorithm can effectively manage the non-cooperative behaviors and reduce interaction consensus costs.},
  archive      = {J_IJMLC},
  author       = {Li, Danping and Hu, Shicheng},
  doi          = {10.1007/s13042-024-02331-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1219-1242},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An adaptive consensus model for managing non-cooperative behaviors in portfolio optimization for large companies},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight self-ensemble feedback recurrent network for
fast MRI reconstruction. <em>IJMLC</em>, <em>16</em>(2), 1201–1218. (<a
href="https://doi.org/10.1007/s13042-024-02330-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the speed of MRI acquisition is a key issue in modern medical practice. However, existing deep learning-based methods are often accompanied by a large number of parameters and ignore the use of deep features. In this work, we propose a novel Self-Ensemble Feedback Recurrent Network (SEFRN) for fast MRI reconstruction inspired by recursive learning and ensemble learning strategies. Specifically, a lightweight but powerful Data Consistency Residual Group (DCRG) is proposed for feature extraction and data stabilization. Meanwhile, an efficient Wide Activation Module (WAM) is introduced between different DCRGs to encourage more activated features to pass through the model. In addition, a Feedback Enhancement Recurrent Architecture (FERA) is designed to reuse the model parameters and deep features. Moreover, combined with the specially designed Automatic Selection and Integration Module (ASIM), different stages of the recurrent model can elegantly implement self-ensemble learning and synergize the sub-networks to improve the overall performance. Extensive experiments demonstrate that our model achieves competitive results and strikes a good balance between the size, complexity, and performance of the model.},
  archive      = {J_IJMLC},
  author       = {Li, Juncheng and Yang, Hanhui and Lui, Lok Ming and Zhang, Guixu and Shi, Jun and Zeng, Tieyong},
  doi          = {10.1007/s13042-024-02330-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1201-1218},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A lightweight self-ensemble feedback recurrent network for fast MRI reconstruction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended random forest for multivariate air quality
forecasting. <em>IJMLC</em>, <em>16</em>(2), 1175–1199. (<a
href="https://doi.org/10.1007/s13042-024-02329-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, an extended random forest algorithm for multivariate time series several steps forecasting is proposed. Peoposed method consists input layer and hidden layers involves random forest. In addition, a new algorithm is proposed in the third step to an ensemble of the tree’s outputs with the concept of correlation with the final results to reduce redundancy. In the output layer, a new algorithm is proposed to learn the weight of each random forest tree to calculate the result. Beijing PM25 and Italian air quality, were used to evaluate the proposed method. The results of the proposed method in this research were compared with the other state-of-the-art methods like deep learning and deep forest. We evaluated our proposed model based on evaluation metrics RMSE, MAE and MAPE and achieved good results. According to the results, the proposed method on the Beijing PM2.5 dataset’s RMSE and MAE value respectively are 40.97 and 24.81 for the average forecast for the next 1–6 h, 2.51 and 0.48 less than the best of the others. The average forecast for the next 1–3 h are 2.71 and 1.42 less than the best value of the others and are equal to 31.64 and 18.09. On the Italian air quality dataset, the RMSE, MAE and MAPE value for the next 1-h forecast are 0.6109, 0.4224 and 33.80 and better than the others.},
  archive      = {J_IJMLC},
  author       = {mirzadeh, Hossein and omranpour, Hesam},
  doi          = {10.1007/s13042-024-02329-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1175-1199},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Extended random forest for multivariate air quality forecasting},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced side information fusion framework for sequential
recommendation. <em>IJMLC</em>, <em>16</em>(2), 1157–1173. (<a
href="https://doi.org/10.1007/s13042-024-02328-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of side information in sequential recommendation (SR) is a recommendation system technique that combines a user’s historical behavior sequence with additional side information to provide more accurate personalized recommendations. Recent methods are based on self-attention mechanisms, incorporating side information as part of the attention matrix to update item representations. We believe that the integration method via self-attention mechanisms does not fully utilize side information. Therefore, we designed a new Enhanced Side Information Fusion framework (ESIF) for sequential recommendations. Specifically, we have altered the fusion strategy by using an attention matrix to simultaneously update the representations of items and side information, thereby increasing the use of side information. The attention matrix serves to balance various features, ensuring effective utilization of side information throughout the fusion process. We designed a Gated Linear Representation Fusion Module, comprising linear transformations and gated units. The linear transformation processes the input data, while the gated unit dynamically adjusts the degree of information flow based on the input. This module then combines the updated item representation with the side information representation for more efficient use of side information. Additionally, user interaction behavior data inevitably contains noise. The presence of noise can disrupt the model’s performance, affecting the accuracy and reliability of the results. Therefore, we introduced a denoising module in ESIF to enhance recommendation accuracy by reducing noise. Our experimental results demonstrate that ESIF achieves superior performance across five real-world datasets, surpassing the current state-of-the-art side information fusion SR models.},
  archive      = {J_IJMLC},
  author       = {Su, Zheng-Ang and Zhang, Juan and Fang, Zhijun and Gao, Yongbin},
  doi          = {10.1007/s13042-024-02328-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1157-1173},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Enhanced side information fusion framework for sequential recommendation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural network based time estimator for SAT solver.
<em>IJMLC</em>, <em>16</em>(2), 1145–1156. (<a
href="https://doi.org/10.1007/s13042-024-02327-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SAT-based formal verification is a systematic process to prove the correctness of computer hardware design based on formal specifications, providing an alternative to time-consuming simulations and ensuring design reliability and accuracy. Predicting the runtime of SAT solvers is important to effectively allocate verification resources and determine if the verification can be completed within time limits. Predicting SAT solver runtime is challenging due to variations in solving time across different solvers and dependence on problem complexity and solver mechanisms. Existing approaches rely on feature engineering and machine learning, but they have drawbacks in terms of expert knowledge requirements and time-consuming feature extraction. To address this, using graph neural networks (GNNs) for runtime prediction is considered, as they excel in capturing graph topology and relationships. However, directly applying existing GNNs to predict SAT solver runtime does not yield satisfactory results, as SAT solvers’ proving procedure is crucial. In this paper, we propose a novel model, TESS, that integrates the working mechanism of SAT solvers with graph neural networks (GNNs) for predicting solving time. The model incorporates a graph representation inspired by the CDCL paradigm, proposes adaptive aggregation for multilayer information and separate modules for conflict learning. Experimental results on multiple datasets validate the effectiveness, scalability, and robustness of our model, outperforming baselines in SAT solver runtime prediction.},
  archive      = {J_IJMLC},
  author       = {Liu, Jiawei and Xiao, Wenyi and Cheng, Hongtao and Shi, Chuan},
  doi          = {10.1007/s13042-024-02327-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1145-1156},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Graph neural network based time estimator for SAT solver},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design your own universe: A physics-informed agnostic method
for enhancing graph neural networks. <em>IJMLC</em>, <em>16</em>(2),
1129–1144. (<a
href="https://doi.org/10.1007/s13042-024-02326-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can fit both homophilic and heterophilic graphs. Empirical validations on benchmarks for homophilic, heterophilic graphs, and long-term graph datasets show that GNNs enhanced by our method significantly outperform their original counterparts.},
  archive      = {J_IJMLC},
  author       = {Shi, Dai and Han, Andi and Lin, Lequan and Guo, Yi and Wang, Zhiyong and Gao, Junbin},
  doi          = {10.1007/s13042-024-02326-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1129-1144},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Design your own universe: A physics-informed agnostic method for enhancing graph neural networks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-driven mixed integer programming approach for joint
chance-constrained optimal power flow under uncertainty. <em>IJMLC</em>,
<em>16</em>(2), 1111–1127. (<a
href="https://doi.org/10.1007/s13042-024-02325-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel mixed integer programming (MIP) reformulation for the joint chance-constrained optimal power flow problem under uncertain load and renewable energy generation. Unlike traditional models, our approach incorporates a comprehensive evaluation of system-wide risk without decomposing joint chance constraints into individual constraints, thus preventing overly conservative solutions and ensuring robust system security. A significant innovation in our method is the use of historical data to form a sample average approximation that directly informs the MIP model, bypassing the need for distributional assumptions to enhance solution robustness. Additionally, we implement a model improvement strategy to reduce the computational burden, making our method more scalable for large-scale power systems. Our approach is validated against benchmark systems, i.e., IEEE 14-, 57- and 118-bus systems, demonstrating superior performance in terms of cost-efficiency and robustness, with lower computational demand compared to existing methods.},
  archive      = {J_IJMLC},
  author       = {Qin, James Ciyu and Jiang, Rujun and Mo, Huadong and Dong, Daoyi},
  doi          = {10.1007/s13042-024-02325-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1111-1127},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A data-driven mixed integer programming approach for joint chance-constrained optimal power flow under uncertainty},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dap-SiMT: Divergence-based adaptive policy for simultaneous
machine translation. <em>IJMLC</em>, <em>16</em>(2), 1091–1110. (<a
href="https://doi.org/10.1007/s13042-024-02323-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of Simultaneous Machine Translation (SiMT), a robust read/write (R/W) policy is essential alongside a high-quality translation model. Traditional methods typically employ either a fixed wait-k policy in sync with a wait-k translation model or an adaptive policy that is co-developed with a dedicated translation model. This study introduces a more versatile approach by decoupling the adaptive policy from the translation model. Our rationale is based on the finding that an independent multi-path wait-k model, when combined with adaptive policies utilized in advanced SiMT systems, can perform competitively. Specifically, we present DaP, a divergence-based adaptive policy, which dynamically adjusts read/write decisions for any translation model, taking into account potential divergence in translation distributions resulting from future information. Extensive experiments across multiple benchmarks reveal that our method significantly enhances the balance between translation accuracy and latency, surpassing strong baselines.},
  archive      = {J_IJMLC},
  author       = {Zhao, Libo and Zeng, Ziqian},
  doi          = {10.1007/s13042-024-02323-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1091-1110},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Dap-SiMT: Divergence-based adaptive policy for simultaneous machine translation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical dual-view model for fake news detection
guided by discriminative lexicons. <em>IJMLC</em>, <em>16</em>(2),
1071–1090. (<a
href="https://doi.org/10.1007/s13042-024-02322-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news detection aims to automatically identify the credibility of source posts, mitigating potential societal harm and conserving human resources. Textual fake news detection methods can be categorized into pattern- and fact-based. Pattern-based models focus on identifying shared writing patterns in source posts, while fact-based models leverage auxiliary external knowledge. Researchers have recently attempted to merge these two views into a comprehensive detection system, achieving superior performance to single-view methods. However, existing dual-view methods often prioritize integrating single-view methods over exploring nuanced characteristics of both perspectives. To address this, we propose a novel hierarchical dual-view model for fake news detection guided by discriminative lexicons. First, we construct two lexicons based on distinct word usage tendencies in fake and real news and further augment them with synonyms sourced from large language models. We then devise a hierarchical attention network to derive semantic representations for the source post, incorporating a lexicon attention loss to guide the prioritization of words from the two lexicons. Subsequently, a lexicon-guided interaction network is employed to model the relations between the source post and its relevant articles, assigning authenticity-aware weights to each article. Finally, the representations of source post and relevant articles are concatenated for joint detection. According to experimental results, our model outperforms many competitive baselines in terms of the macro F1 score ranging from 1.1% to 10.5% on Weibo and from 3.2% to 10.8% on Twitter.},
  archive      = {J_IJMLC},
  author       = {Yang, Sijia and Li, Xianyong and Du, Yajun and Huang, Dong and Chen, Xiaoliang and Fan, Yongquan and Wang, Shumin},
  doi          = {10.1007/s13042-024-02322-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1071-1090},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A hierarchical dual-view model for fake news detection guided by discriminative lexicons},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-stage zero-shot object detection network based on
CLIP and pseudo-labeling. <em>IJMLC</em>, <em>16</em>(2), 1055–1070. (<a
href="https://doi.org/10.1007/s13042-024-02321-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of unknown objects is a challenging task in computer vision because, although there are diverse real-world detection object categories, existing object-detection training sets cover a limited number of object categories . Most existing approaches use two-stage networks to improve a model’s ability to characterize objects of unknown classes, which leads to slow inference. To address this issue, we proposed a single-stage unknown object detection method based on the contrastive language-image pre-training (CLIP) model and pseudo-labelling, called CLIP-YOLO. First, a visual language embedding alignment method is introduced and a channel-grouped enhanced coordinate attention module is embedded into a YOLO-series detection head and feature-enhancing component, to improve the model’s ability to characterize and detect unknown category objects. Second, the pseudo-labelling generation is optimized based on the CLIP model to expand the diversity of the training set and enhance the ability to cover unknown object categories. We validated this method on four challenging datasets: MSCOCO, ILSVRC, Visual Genome, and PASCAL VOC. The results show that our method can achieve higher accuracy and faster speed, so as to obtain better performance of unknown object detection. The source code is available at https://github.com/BJUTsipl/CLIP-YOLO .},
  archive      = {J_IJMLC},
  author       = {Li, Jiafeng and Sun, Shengyao and Zhang, Kang and Zhang, Jing and Zhuo, Li},
  doi          = {10.1007/s13042-024-02321-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1055-1070},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Single-stage zero-shot object detection network based on CLIP and pseudo-labeling},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic configuration network modeling method based on
information superposition and mixture correntropy. <em>IJMLC</em>,
<em>16</em>(2), 1041–1054. (<a
href="https://doi.org/10.1007/s13042-024-02320-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the generalizability and robustness of stochastic configuration networks (SCNs), this paper proposes a robust modeling method based on information superposition and mixture correntropy. First, the mapping information of the (sigmoid) activation function and its derivative function is superimposed, and the hidden layer parameters are randomly assigned through a supervisory mechanism to improve the diversity of the hidden layer mapping. Second, mixture correntropy is used to construct a robust loss function, and different Gaussian kernels are used to measure the contribution of training samples to suppress the negative impact of data noise on the accuracy of the model. Finally, the performance of the proposed modeling method is tested on functional approximation, four benchmark datasets, and historical data from the municipal solid waste incineration process. The experimental results show that the modeling method proposed in this paper has advantages in terms of generalizability and robustness.},
  archive      = {J_IJMLC},
  author       = {Yan, Aijun and Hu, Kaicheng and Wang, Dianhui},
  doi          = {10.1007/s13042-024-02320-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1041-1054},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Stochastic configuration network modeling method based on information superposition and mixture correntropy},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models for medicine: A survey.
<em>IJMLC</em>, <em>16</em>(2), 1015–1040. (<a
href="https://doi.org/10.1007/s13042-024-02318-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address challenges in the digital economy’s landscape of digital intelligence, large language models (LLMs) have been developed. Improvements in computational power and available resources have significantly advanced LLMs, allowing their integration into diverse domains for human life. Medical LLMs are essential application tools with potential across various medical scenarios. In this paper, we review LLM developments, focusing on the requirements and applications of medical LLMs. We provide a concise overview of existing models, aiming to explore advanced research directions and benefit researchers for future medical applications. We emphasize the advantages of medical LLMs in applications, as well as the challenges encountered during their development. Finally, we suggest directions for technical integration to mitigate challenges and potential research directions for the future of medical LLMs, aiming to meet the demands of the medical field better.},
  archive      = {J_IJMLC},
  author       = {Zheng, Yanxin and Gan, Wensheng and Chen, Zefeng and Qi, Zhenlian and Liang, Qian and Yu, Philip S.},
  doi          = {10.1007/s13042-024-02318-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1015-1040},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Large language models for medicine: A survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-term time series forecasting based on siamese network:
A perspective on few-shot learning. <em>IJMLC</em>, <em>16</em>(2),
999–1014. (<a href="https://doi.org/10.1007/s13042-024-02317-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-term time series forecasting (LTSF) plays a crucial role in various domains, utilizing a large amount of historical data to forecast trends over an extended future time range. However, in real-life scenarios, the performance of LTSF is often hindered by missing data. Few-shot learning aims to address the issue of data scarcity, but there is relatively little research on using few-shot learning to tackle sample scarcity in long-term time series forecasting tasks, and most few-shot learning methods rely on transfer learning. To address this problem, this paper proposes a Siamese network-based time series Transformer (SiaTST) for the task of LTSF in a few-shot setting. To increase the diversity of input scales and better capture local features in time series, we adopt a dual-level hierarchical input strategy. Additionally, we introduce a learnable prediction token (LPT) to capture global features of the time series. Furthermore, a feature fusion layer is utilized to capture dependencies among multiple variables and integrate information from different levels. Experimental results on 7 popular LSTF datasets demonstrate that our proposed model achieves state-of-the-art performance.},
  archive      = {J_IJMLC},
  author       = {Fan, Jin and Xiang, Jiaqian and Liu, Jie and Wang, Zheyu and Wu, Huifeng},
  doi          = {10.1007/s13042-024-02317-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {999-1014},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Long-term time series forecasting based on siamese network: A perspective on few-shot learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Color attention tracking with score matching.
<em>IJMLC</em>, <em>16</em>(2), 983–997. (<a
href="https://doi.org/10.1007/s13042-024-02316-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is an ordinary practice that deep networks are utilized to extract deep features from RGB images. Typically, the popular trackers adopt pre-trained ResNet as a backbone to extract target features, achieving excellent performance. Moreover, Staple has shown that color statistics have complementary cues, while the combination of color statistics and deep features in a unified deep framework has rarely been reported. Therefore, we employ color statistics to construct color attention maps, which are encoded into the deep network to guide the generation of target-aware feature maps. Additionally, DCF-based trackers have an online update module to dynamically update the tracking model, it is particularly necessary to collect reliable target samples. Hence, we refer to the template matching thought to design a score matching method, which is intended to score the tracked targets, this method has the advantage of considering the target extent. In this paper, we conduct sufficient ablation analyses on the color attention module and score matching method to verify their effectiveness. Furthermore, our approaches are combined into the DCF frameworks to construct two brand-new trackers, and both quantitative and qualitative results demonstrate that our trackers can perform favorably against recent and far more sophisticated trackers on multiple public benchmarks.},
  archive      = {J_IJMLC},
  author       = {He, Xuedong and Huang, Jiehui},
  doi          = {10.1007/s13042-024-02316-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {983-997},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Color attention tracking with score matching},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing automated street crime detection: A drone-based
system integrating CNN models and enhanced feature selection techniques.
<em>IJMLC</em>, <em>16</em>(2), 959–981. (<a
href="https://doi.org/10.1007/s13042-024-02315-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a pioneering solution to the growing challenge of escalating global crime rates through the introduction of an automated drone-based street crime detection system. Leveraging advanced Convolutional Neural Network (CNN) models, the system integrates several key components for analyzing images captured by drones. Initially, the Embedding Bilateral Filter (EBF) technique divides images into base and detail layers to enhance detection accuracy. The fusion model, IR with attention-based Conv-ViT, combines Inception-V3, ResNet-50, and Convolution Vision Transformer (Conv-ViT) to capture both shape and texture details efficiently. Further enhancement is achieved through the Improved Shark Smell Optimization Algorithm (ISSOA), which optimizes feature selection and minimizes redundancy in image extraction. Additionally, a Multi-scale Contextual Semantic Guidance Network (MCS-GNet) ensures robust image classification by integrating features from multiple layers to prevent data loss. Evaluation on the UCF-Crime and UCSD Ped2 datasets demonstrates superior accuracy, with remarkable results of 0.783 and 0.974, respectively. This innovative approach offers a promising solution to the arduous and continuous task of monitoring security camera feeds for suspicious activities, thereby addressing the pressing need for automated crime detection systems on a global scale.},
  archive      = {J_IJMLC},
  author       = {Vuyyuru, Lakshma Reddy and Purimetla, NagaMalleswara Rao and Reddy, Kancharakunt Yakub and Vellela, Sai Srinivas and Basha, Sk Khader and Vatambeti, Ramesh},
  doi          = {10.1007/s13042-024-02315-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {959-981},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Advancing automated street crime detection: A drone-based system integrating CNN models and enhanced feature selection techniques},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale frequency domain learning for texture
classification. <em>IJMLC</em>, <em>16</em>(2), 947–958. (<a
href="https://doi.org/10.1007/s13042-024-02314-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, methods for modeling images in frequency domain have attracted widespread attention. Frequency methods transform image into spectrum as model input by defining a set of basis functions, where each point in frequency domain represents image’s projection at the corresponding frequency, which helps to understand structure, texture, and other basic features of the image. Studies have shown that frequency learning methods based on fixed small-scale Discrete Cosine Transform (DCT) basis functions outperform spatial domain methods. However, frequency learning also faces the challenge of feature saturation bottleneck, particularly fixed DCT basis functions makes the frequency learning model more prone to reaching saturation. Therefore, we propose a multi-scale frequency domain learning approach that utilizes multi-scale DCT basis functions to extract diversified frequency features. Our method employs sampling and cropping techniques to extend the frequency features from a single-scale DCT basis functions mapping to a multi-scale mapping. This ensures that the image projection exists at as many frequencies as possible so that advanced features can be extracted over multiple frequency ranges to overcome the saturation bottleneck of network models. Experimental results demonstrate the superior performance of our approach in texture classification tasks.},
  archive      = {J_IJMLC},
  author       = {Zang, Liguang and Li, Yuancheng},
  doi          = {10.1007/s13042-024-02314-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {947-958},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-scale frequency domain learning for texture classification},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient federated learning with cross-resource client
collaboration. <em>IJMLC</em>, <em>16</em>(2), 931–945. (<a
href="https://doi.org/10.1007/s13042-024-02313-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed machine learning paradigm. Traditional federated learning performs well on the premise that all clients have the same learning ability or similar learning tasks. However, resource and data heterogeneity are inevitable among clients in real-world scenarios, leading to the situation that existing federated learning mechanisms cannot achieve high accuracy in short response time. In this study, an effective federated learning framework with cross-resource client collaboration, termed CEFL, is proposed to coordinate clients with different capacities to help each other, efficiently and adequately reflecting collective intelligence. Clients are categorized into different clusters based on their computational resources in the hierarchical framework. Resource-rich clusters use their knowledge to assist resource-limited clusters converge rapidly. Once resource-limited clusters have the ability to mentor others, resource-rich clusters learn from the resource-limited clusters in their favor to improve their own effectiveness. A cloud server provides tailored assistance to each cluster with a personalized model through an adaptive multi-similarity metric, in order for each cluster to learn the most useful knowledge. The experiments fully demonstrate that the proposed method not only has superior accuracy with significantly reduced latency but also improves the convergence rate compared to other state-of-the-art federated learning methods in addressing the problem of resource and data heterogeneity.},
  archive      = {J_IJMLC},
  author       = {Shen, Qi and Yang, Liu},
  doi          = {10.1007/s13042-024-02313-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {931-945},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Efficient federated learning with cross-resource client collaboration},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-enhanced recommendation via dynamic co-attention
and high-order connectivity. <em>IJMLC</em>, <em>16</em>(2), 919–930.
(<a href="https://doi.org/10.1007/s13042-024-02312-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) based recommender systems have shown promise in improving accuracy and interpretability. They reveal the intrinsic relationships of knowledge through the associations and paths between entities for personalized recommendations. However, existing approaches do not adequately consider the high-order connections between neighboring nodes in the relational graph, resulting in a lack of sufficient capture of structured information. In this paper, we propose a knowledge-enhanced recommendation model via dynamic co-attention and high-order connectivity (DCHC) to address this issue. First, we construct a hybrid graph by aligning users and items in the user-item bipartite graph with entities in the KG. As a result, we are able to simultaneously consider the interaction between users and items as well as the entity information in the KG, thereby gaining a more comprehensive understanding of user behavior and interests. Second, we explicitly model the high-order connections between entities through the hybrid structured graphs in an end-to-end manner. Therefore, we not only explored the complex interactive relationships between entities but also ensured the accurate capture of structural information in the graph. Third, we employ a dynamic co-attention mechanism to enhance the representation of users and items, effectively exploiting the potential correlation between them. We therefore effectively exploited the potential correlation between users and items and successfully integrating these relationships into their representations. Extensive experiments conducted on three benchmarks demonstrate that DCHC outperforms state-of-the-art KG-based recommendation methods.},
  archive      = {J_IJMLC},
  author       = {Wang, Dan-Dong and Min, Fan},
  doi          = {10.1007/s13042-024-02312-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {919-930},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Knowledge-enhanced recommendation via dynamic co-attention and high-order connectivity},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial recurrent neural network coordinated secured
transmission towards safeguarding confidentiality in smart industrial
internet of things. <em>IJMLC</em>, <em>16</em>(2), 891–917. (<a
href="https://doi.org/10.1007/s13042-024-02310-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces a new method to tackle the issue of exchanging cryptographic keys in the Industrial Internet of Things (IIoT). This study focuses on the inefficiency and lengthy evaluation procedures of conventional cryptographic key exchange algorithms, which are not appropriate for the rapid and constantly changing IIoT device environment. In the solution domain, the proposed approach uses synchronization of neural networks with vector valued and Recurrent Neural Networks (RNNs), merging drive-response mechanisms to enhance speed and efficiency in crucial operations. The research examines the influence of postponements on the generating arbitrary inputs and coordination challenges in RNNs that incorporate drive-response mechanisms for synchronized input vector creation. This article explains an elementary evaluation of coordination in Artificial Neural Networks (ANNs) by utilizing an RNN framework to structure ANNs for sharing session keys. The study provides multiple contributions: (1) employing the polynomial coordination technique to generate coordinated inputs for the ANN synchronization process using RNNs, (2) using Lyapunov formulas and inequality assessment methods to identify required control parameters and time-varying conditions for achieving synchronization in the drive-response systems proposed with polynomial and non-polynomial functions, (3) demonstrating the connection between polynomial and non-polynomial synchronization with numerical illustrations, and (4) designing symmetric layouts of ANNs to create a session keys in the IIoT network. The suggested technique outperforms existing methods in the literature by offering a quicker, more dependable solution for cryptographic key exchange, paving the way for improved and secure industrial applications. This new method not only fixes current inefficiencies but also paves the way for future improvements in secure communication in the IIoT environment.},
  archive      = {J_IJMLC},
  author       = {Sarkar, Arindam and Singh, Moirangthem Marjit and Sharma, Hanjabam Saratchandra},
  doi          = {10.1007/s13042-024-02310-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {891-917},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Artificial recurrent neural network coordinated secured transmission towards safeguarding confidentiality in smart industrial internet of things},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dynamic residual graph convolutional network with
global feature enhancement for traffic flow prediction. <em>IJMLC</em>,
<em>16</em>(2), 873–889. (<a
href="https://doi.org/10.1007/s13042-024-02307-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to achieving an accurate and reliable traffic flow prediction lies in modeling the complex and dynamic correlations among sensors. However, existing studies ignore the fact that such correlations are influenced by multiple dynamic factors and the original sequence features of the traffic data, which limits the deep modeling of such correlations and leads to a biased understanding of such correlations. The extraction strategies for global features are less developed, which has degraded the reliability of the predictions. In this study, a novel multi-dynamic residual graph convolutional network with global feature enhancement is proposed to solve these problems and achieve an accurate and reliable traffic flow prediction. First, multiple graph generators are proposed, which fully preserve the original sequence features of the traffic data and enable layered depth-wise modeling of the dynamic correlations among sensors through a residual mechanism. Second, an output module is proposed to explore extraction strategies for global features, by employing a residual mechanism and parameter sharing strategy to maintain the consistency of the global features. Finally, a new layered network architecture is proposed, which not only leverages the advantages of both static and dynamic graphs, but also captures the spatiotemporal dependencies among sensors. The superiority of the proposed model has been verified through extensive experiments on two real-world datasets.},
  archive      = {J_IJMLC},
  author       = {Li, Xiangdong and Yin, Xiang and Huang, Xiaoling and Liu, Weishu and Zhang, Shuai and Zhang, Dongping},
  doi          = {10.1007/s13042-024-02307-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {873-889},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-dynamic residual graph convolutional network with global feature enhancement for traffic flow prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RHGNN: Imposing relational inductive bias for heterogeneous
graph neural network. <em>IJMLC</em>, <em>16</em>(2), 855–871. (<a
href="https://doi.org/10.1007/s13042-024-02305-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph data are ubiquitous and extracting information from them is increasingly crucial. Existing approaches for modeling heterogeneous graphs often rely on the splitting strategy guided by meta-paths or relations. However, the splitting strategy falls short in capturing the rich semantic information and topological structure of heterogeneous graphs comprehensively, as well as the hidden semantic associations between various edge types. Moreover, establishing meta-paths for heterogeneous graphs requires substantial expert experience and domain knowledge, making the process time-consuming, laborious, and often inaccurate. To address these challenges, we propose a novel heterogeneous graph neural network called RHGNN, which overcomes the reliance on meta-paths by modeling the heterogeneous graph as a whole instead of splitting it, and jointly obtains personalized feature representations for nodes and edges. Specifically, we first introduce the pre-training strategy Type2vec to learn feature representations of edge types and extract the semantic and structural information of the entire heterogeneous graph. Then we view the heterogeneous graph as a homogeneous one, and design a new information propagation and aggregation mechanism for both nodes and edges to learn their representations. Based on the assumption that edges with the same type have similar representations, we introduce an explicit regularization method based on the relational inductive bias to impose smoothness constraints on the model. Experimental results demonstrate that RHGNN significantly outperforms state-of-the-art models on both node-level and edge-level tasks.},
  archive      = {J_IJMLC},
  author       = {Zhu, Shichao and Zhang, Shuai and Liu, Yang and Zhou, Chuan and Pan, Shirui and Li, Zhao and Chen, Hongyang},
  doi          = {10.1007/s13042-024-02305-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {855-871},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {RHGNN: Imposing relational inductive bias for heterogeneous graph neural network},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection and analysis of android malwares using hybrid dual
path bi-LSTM kepler dynamic graph convolutional network. <em>IJMLC</em>,
<em>16</em>(2), 835–853. (<a
href="https://doi.org/10.1007/s13042-024-02303-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In past decade, the android malware threats have been rapidly increasing with the widespread usage of internet applications. In respect of security purpose, there are several machine learning techniques attempted to detect the malwares effectively, but failed to achieve the accurate detection due to increasing number of features, more time consumption decreases in detection efficiency. To overcome these limitations, in this research work an innovative Hybrid dual path Bidirectional long short-term memory Kepler dynamic graph Convolutional Network (HBKCN) is proposed to analyze and detect android malwares effectively. First, the augmented abstract syntax tree is applied for pre-processing and extracts the string function from each malware. Second, the adaptive aphid ant optimization is utilized to choose the most appropriate features and remove irrelevant features. Finally, the proposed HBKCN classifies benign and malware apps based on their specifications. Four benchmark datasets, namely Drebin, VirusShare, Malgenome -215, and MaMaDroid datasets, are employed to estimate the effectiveness of the technique. The result demonstrates that the HBKCN technique achieved excellent performance with respect to a few important metrics compared to existing methods. Moreover, detection accuracies of 99.2%, 99.1%,99.8% and 99.8% are achieved for the considered datasets, respectively. Also, the computation time is greatly reduced, illustrating the efficiency of the proposed model in identifying android malwares.},
  archive      = {J_IJMLC},
  author       = {Lingayya, Sadananda and Kulkarni, Praveen and Salins, Rohan Don and Uppoor, Shruthi and Gurudas, V. R.},
  doi          = {10.1007/s13042-024-02303-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {835-853},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Detection and analysis of android malwares using hybrid dual path bi-LSTM kepler dynamic graph convolutional network},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reciprocal interlayer-temporal discriminative target model
for robust visual tracking. <em>IJMLC</em>, <em>16</em>(2), 819–834. (<a
href="https://doi.org/10.1007/s13042-024-02296-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most Siamese algorithms take little account of the information interaction between the target and search region, leading to tracking results that are often disturbed by the cumulative effect of target-like distractors between layers. In this paper, we propose a reciprocal interlayer-temporal discriminative target model for robust visual tracking. Firstly, an interlayer target-aware enhancement model is constructed, which establishes pixel-by-pixel correlation between the template and search region to achieve interlayer feature information interaction. This alleviates the cumulative error caused by the blindness of the target to search region during feature extraction, enhancing target perception. Secondly, to weaken the impact of interference, a temporal interference evaluation strategy is designed. It utilizes the interframe candidate propagation module to build associations among multi-candidates in the current frame and the previous frame. Then, the similar distractors are eliminated by using object inference constraint, so as to obtain a more accurate target location. Finally, we integrate the interlayer target-aware enhancement model and temporal interference evaluation strategy into the Siamese framework to achieve reciprocity for robust target tracking. Experimental results show that our tracking approach performs well, especially on seven benchmark datasets, including OTB-100, TC-128, DTB, UAV-123, VOT-2016, VOT-2018 and GOT-10k.},
  archive      = {J_IJMLC},
  author       = {Zhang, Huanlong and Ma, Zonghao and Zhao, Yanchun and Wang, Yong and Jiang, Bin},
  doi          = {10.1007/s13042-024-02296-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {819-834},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Reciprocal interlayer-temporal discriminative target model for robust visual tracking},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-graph aggregated graph neural network for
heterogeneous graph representation learning. <em>IJMLC</em>,
<em>16</em>(2), 803–818. (<a
href="https://doi.org/10.1007/s13042-024-02294-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural networks have attracted considerable attention for their proficiency in handling intricate heterogeneous structures. However, most existing methods model semantic relationships in heterogeneous graphs by manually defining meta-paths, inadvertently overlooking the inherent incompleteness of such graphs. To address this issue, we propose a multi-graph aggregated graph neural network (MGAGNN) for heterogeneous graph representation learning, which simultaneously leverages attribute similarity and high-order semantic information between nodes. Specifically, MGAGNN first employs the feature graph generator to generate a feature graph for completing the original graph structure. A semantic graph is then generated using a semantic graph generator, capturing higher-order semantic information through automatic meta-path learning. Finally, we aggregate the two candidate graphs to reconstruct a new heterogeneous graph and learn node embedding by graph convolutional networks. Extensive experiments on real-world datasets demonstrate the superior performance of the proposed method over state-of-the-art approaches.},
  archive      = {J_IJMLC},
  author       = {Zhu, Shuailei and Wang, Xiaofeng and Lai, Shuaiming and Chen, Yuntao and Zhai, Wenchao and Quan, Daying and Qi, Yuanyuan and Lv, Laishui},
  doi          = {10.1007/s13042-024-02294-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {803-818},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-graph aggregated graph neural network for heterogeneous graph representation learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial attack method based on enhanced spatial
momentum. <em>IJMLC</em>, <em>16</em>(2), 789–802. (<a
href="https://doi.org/10.1007/s13042-024-02290-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been widely applied in many fields, but it is found that they are vulnerable to adversarial examples, which can mislead the DNN-based models with imperceptible perturbations. Many adversarial attack methods can achieve great success rates when attacking white-box models, but they usually exhibit poor transferability when attacking black-box models. Momentum iterative gradient-based methods can effectively improve the transferability of adversarial examples. Still, the momentum update mechanism of existing methods may lead to a problem of unstable gradient update direction and result in poor local optima. In this paper, we propose an enhanced spatial momentum iterative gradient-based adversarial attack method. Specifically, we introduce the spatial domain momentum accumulation mechanism. Instead of only accumulating the gradients of data points on the optimization path in the gradient update process, we additionally accumulate the average gradients of multiple sampling points within the neighborhood of data points. This mechanism fully utilizes the contextual gradient information of different regions within the image to smooth the accumulated gradients and find a more stable gradient update direction, thus escaping from poor local optima. Empirical results on the standard ImageNet dataset demonstrate that our method can significantly improve the attack success rate of momentum iterative gradient-based methods and shows excellent attack performance not only against normally trained models but also against adversarial training and defense models, outperforming the state-of-the-art methods.},
  archive      = {J_IJMLC},
  author       = {Hu, Jun and Wei, Guanghao and Xia, Shuyin and Wang, Guoyin},
  doi          = {10.1007/s13042-024-02290-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {789-802},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Adversarial attack method based on enhanced spatial momentum},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPERM: Sequential pairwise embedding recommendation with
MI-FGSM. <em>IJMLC</em>, <em>16</em>(2), 771–787. (<a
href="https://doi.org/10.1007/s13042-024-02288-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual recommendation systems have shown remarkable performance by leveraging consumer feedback and the visual attributes of products. However, recent concerns have arisen regarding the decline in recommendation quality when these systems are subjected to attacks that compromise the model parameters. While the fast gradient sign method (FGSM) and iterative FGSM (I-FGSM) are well-studied attack strategies, the momentum iterative FGSM (MI-FGSM), known for its superiority in the computer vision (CV) domain, has been overlooked. This oversight raises the possibility that visual recommender systems may be vulnerable to MI-FGSM, leading to significant vulnerabilities. Adversarial training, a regularization technique designed to withstand MI-FGSM attacks, could be a promising solution to bolster model resilience. In this research, we introduce MI-FGSM for visual recommendation. We propose the Sequential Pairwise Embedding Recommender with MI-FGSM (SPERM), a model that incorporates visual, temporal, and sequential information for visual recommendations through adversarial training. Specifically, we employ higher-order Markov chains to capture consumers’ sequential behaviors and utilize visual pairwise ranking to discern their visual preferences. To optimize the SPERM model, we employ a learning method based on AdaGrad. Moreover, we fortify the SPERM approach with adversarial training, where the primary objective is to train the model to withstand adversarial inputs introduced by MI-FGSM. Finally, we evaluate the effectiveness of our approach by conducting experiments on three Amazon datasets, comparing it with existing visual and adversarial recommendation algorithms. Our results demonstrate the efficacy of the proposed SPERM model in addressing adversarial attacks while enhancing visual recommendation performance.},
  archive      = {J_IJMLC},
  author       = {Paul, Agyemang and Wan, Yuxuan and Chen, Boyu and Wu, Zhefu},
  doi          = {10.1007/s13042-024-02288-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {771-787},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {SPERM: Sequential pairwise embedding recommendation with MI-FGSM},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi-framelets: Robust graph neural networks via adaptive
framelet convolution. <em>IJMLC</em>, <em>16</em>(2), 755–770. (<a
href="https://doi.org/10.1007/s13042-024-02286-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to provide a novel design of a multiscale framelet convolution for spectral graph neural networks (GNNs). While current spectral methods excel in various graph learning tasks, they often lack the flexibility to adapt to noisy, incomplete, or perturbed graph signals, making them fragile in such conditions. Our newly proposed framelet convolution addresses these limitations by decomposing graph data into low-pass and high-pass spectra through a finely-tuned multiscale approach. Our approach directly designs filtering functions within the spectral domain, allowing for precise control over the spectral components. The proposed design excels in filtering out unwanted spectral information and significantly reduces the adverse effects of noisy graph signals. Our approach not only enhances the robustness of GNNs but also preserves crucial graph features and structures. Through extensive experiments on diverse, real-world graph datasets, we demonstrate that our framelet convolution achieves superior performance in node classification tasks. It exhibits remarkable resilience to noisy data and adversarial attacks, highlighting its potential as a robust solution for real-world graph applications. This advancement opens new avenues for more adaptive and reliable spectral GNN architectures.},
  archive      = {J_IJMLC},
  author       = {Yang, Mengxi and Shi, Dai and Zheng, Xuebin and Yin, Jie and Gao, Junbin},
  doi          = {10.1007/s13042-024-02286-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {755-770},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Quasi-framelets: Robust graph neural networks via adaptive framelet convolution},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="invent---6">INVENT - 6</h2>
<ul>
<li><details>
<summary>
(2025). The amplituhedron BCFW triangulation. <em>INVENT</em>,
<em>239</em>(3), 1009–1138. (<a
href="https://doi.org/10.1007/s00222-025-01316-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amplituhedron $\mathcal{A}_{n,k,4}$ is a geometric object, introduced by Arkani-Hamed and Trnka (2013) in the study of scattering amplitudes in quantum field theories. They conjecture that $\mathcal{A}_{n,k,4}$ admits a decomposition into images of BCFW positroid cells, arising from the Britto–Cachazo–Feng–Witten recurrence (2005). We prove that this conjecture is true.},
  archive      = {J_INVENT},
  author       = {Even-Zohar, Chaim and Lakrec, Tsviqa and Tessler, Ran J.},
  doi          = {10.1007/s00222-025-01316-1},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {1009-1138},
  shortjournal = {Invent. Math.},
  title        = {The amplituhedron BCFW triangulation},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curvature and sharp growth rates of log-quasimodes on
compact manifolds. <em>INVENT</em>, <em>239</em>(3), 947–1008. (<a
href="https://doi.org/10.1007/s00222-025-01315-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We obtain new optimal estimates for the $L^{2}(M)\to L^{q}(M)$ , $q\in (2,q_{c}]$ , $q_{c}=2(n+1)/(n-1)$ , operator norms of spectral projection operators associated with spectral windows $[\lambda ,\lambda +\delta (\lambda )]$ , with $\delta (\lambda )=O((\log \lambda )^{-1})$ on compact Riemannian manifolds $(M,g)$ of dimension $n\ge 2$ all of whose sectional curvatures are nonpositive or negative. We show that these two different types of estimates are saturated on flat manifolds or manifolds all of whose sectional curvatures are negative. This allows us to classify compact space forms in terms of the size of $L^{q}$ -norms of quasimodes for each Lebesgue exponent $q\in (2,q_{c}]$ , even though it is impossible to distinguish between ones of negative or zero curvature sectional curvature for any $q&gt;q_{c}$ .},
  archive      = {J_INVENT},
  author       = {Huang, Xiaoqi and Sogge, Christopher D.},
  doi          = {10.1007/s00222-025-01315-2},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {947-1008},
  shortjournal = {Invent. Math.},
  title        = {Curvature and sharp growth rates of log-quasimodes on compact manifolds},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hausdorff dimension of the apollonian gasket.
<em>INVENT</em>, <em>239</em>(3), 909–946. (<a
href="https://doi.org/10.1007/s00222-024-01311-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Apollonian gasket is a well-studied circle packing. Important properties of the packing, including the distribution of the circle radii, are governed by its Hausdorff dimension. No closed form is currently known for the Hausdorff dimension, and its computation is a special case of a more general and hard problem: effective, rigorous estimates of dimension of a parabolic limit set. In this paper we develop an efficient method for solving this problem which allows us to compute the dimension of the gasket to 128 decimal places and rigorously justify the error bounds. We expect our approach to generalise easily to other parabolic fractals.},
  archive      = {J_INVENT},
  author       = {Vytnova, Polina L. and Wormell, Caroline L.},
  doi          = {10.1007/s00222-024-01311-y},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {909-946},
  shortjournal = {Invent. Math.},
  title        = {Hausdorff dimension of the apollonian gasket},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analytic invariant of <span
class="math inline"><em>G</em><sub>2</sub></span> manifolds.
<em>INVENT</em>, <em>239</em>(3), 865–907. (<a
href="https://doi.org/10.1007/s00222-024-01310-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that the moduli space of holonomy $G_{2}$ -metrics on a closed 7-manifold can be disconnected by presenting a number of explicit examples. We detect different connected components of the $G_{2}$ -moduli space by defining an analytic refinement $\bar{\nu }(M, g) \in \mathbb{Z}$ of the defect invariant $\nu (M,\varphi )\in \mathbb{Z}/48$ of $G_{2}$ -structures $\varphi $ on a closed 7-manifold $M$ introduced by the first and third authors. The $\bar{\nu }$ -invariant is defined using $\eta $ -invariants and Mathai-Quillen currents on $M$ and we compute it for twisted connected sums à la Kovalev, Corti-Haskins-Nordström-Pacini and extra-twisted connected sums as constructed by the second and third authors. In particular, we find examples of $G_{2}$ -holonomy metrics in different components of the moduli space where the associated $G_{2}$ -structures are homotopic and other examples where they are not.},
  archive      = {J_INVENT},
  author       = {Crowley, Diarmuid and Goette, Sebastian and Nordström, Johannes},
  doi          = {10.1007/s00222-024-01310-z},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {865-907},
  shortjournal = {Invent. Math.},
  title        = {An analytic invariant of $G_{2}$ manifolds},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterations of symplectomorphisms and <span
class="math inline"><em>p</em></span> -adic analytic actions on the
fukaya category. <em>INVENT</em>, <em>239</em>(3), 801–864. (<a
href="https://doi.org/10.1007/s00222-024-01308-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the work of Bell on the dynamical Mordell-Lang conjecture, and by family Floer cohomology, we construct $p$ -adic analytic families of bimodules on the Fukaya category of a monotone or negatively monotone symplectic manifold, interpolating the bimodules corresponding to iterates of a symplectomorphism $\phi $ isotopic to the identity. This family can be thought of as a $p$ -adic analytic action on the Fukaya category. Using this, we deduce that the ranks of the Floer cohomology groups $HF(\phi ^{k}(L),L&#39;;\Lambda )$ are constant in $k\in {\mathbb{Z}}$ , with finitely many possible exceptions. We also prove an analogous result without the monotonicity assumption for generic $\phi $ isotopic to the identity by showing how to construct a $p$ -adic analytic action in this case. We give applications to categorical entropy and a conjecture of Seidel.},
  archive      = {J_INVENT},
  author       = {Kartal, Yusuf Barış},
  doi          = {10.1007/s00222-024-01308-7},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {801-864},
  shortjournal = {Invent. Math.},
  title        = {Iterations of symplectomorphisms and $p$ -adic analytic actions on the fukaya category},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizing lusztig’s total positivity. <em>INVENT</em>,
<em>239</em>(3), 707–799. (<a
href="https://doi.org/10.1007/s00222-024-01303-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the notion of $\Theta $ -positivity in real semisimple Lie groups. This notion at the same time generalizes Lusztig’s total positivity in split real Lie groups and invariant orders in Lie groups of Hermitian type. We show that there are four families of simple Lie groups which admit a positive structure relative to a subset $\Theta$ of simple roots, and investigate fundamental properties of $\Theta $ -positivity. We define and describe the positive and nonnegative unipotent semigroups and show that they give rise to a notion of positive $n$ -tuples in flag varieties.},
  archive      = {J_INVENT},
  author       = {Guichard, Olivier and Wienhard, Anna},
  doi          = {10.1007/s00222-024-01303-y},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {707-799},
  shortjournal = {Invent. Math.},
  title        = {Generalizing lusztig’s total positivity},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jgo---10">JGO - 10</h2>
<ul>
<li><details>
<summary>
(2025). Convergence of solutions to set optimization problems with
variable ordering structures. <em>JGO</em>, <em>91</em>(3), 677–699. (<a
href="https://doi.org/10.1007/s10898-024-01452-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider set optimization problems with variable ordering structures. Within the framework of the set less order relation with variable ordering structures, we investigate the existence, the upper convergence, and the lower convergence of solutions to such problems in the image spaces. For both the existence and the upper convergence of solutions, we employ new techniques to obtain various results without assuming the compactness of the constraint sets. Additionally, we utilize the domination property concerning the variable ordering cones to address the lower convergence of solutions. The obtained results are presented in several versions from different aspects for convenient comparison with existing results. Many examples are provided to illustrate the novelty of our results or to compare them with existing ones in the literature.},
  archive      = {J_JGO},
  author       = {Anh, L. Q. and Hien, D. V.},
  doi          = {10.1007/s10898-024-01452-7},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {677-699},
  shortjournal = {J. Glob. Optim.},
  title        = {Convergence of solutions to set optimization problems with variable ordering structures},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive nested monte carlo approach for multi-objective
efficient global optimization. <em>JGO</em>, <em>91</em>(3), 647–676.
(<a href="https://doi.org/10.1007/s10898-024-01442-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel algorithm, namely the adaptive nested Monte Carlo based multi-objective Efficient Global Optimization (ANMC-MOEGO), which aims to enhance efficiency and accuracy while minimizing programming complexity in contrast to traditional multi-objective Efficient Global Optimization (MOEGO). In this algorithm, the programming complexity is streamlined by employing Monte Carlo simulation for both hypervolume improvement (HVI) and expected hypervolume improvement (EHVI) calculations. Furthermore, the efficiency and accuracy of HVI and EHVI calculations are improved through the utilization of a novel technique called adaptive Monte Carlo hypercube boundaries (AMCHB), which is based on the bisection method. The algorithm is validated via a set of test functions from the open literature. The numerical results demonstrate that the ANMC-MOEGO algorithm produces solutions closer to the theoretical results, with improved distributions on the corresponding Pareto fronts compared to the algorithm without AMCHB technique. Moreover, when obtaining a better Pareto front, the proposed algorithm is found to be more time-efficient, achieving speedups of up to 22.57 times.},
  archive      = {J_JGO},
  author       = {Xu, Shengguan and Tan, Jianfeng and Zhang, Jiale and Chen, Hongquan and Gao, Yisheng},
  doi          = {10.1007/s10898-024-01442-9},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {647-676},
  shortjournal = {J. Glob. Optim.},
  title        = {Adaptive nested monte carlo approach for multi-objective efficient global optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inexact proximal methods for weakly convex functions.
<em>JGO</em>, <em>91</em>(3), 611–646. (<a
href="https://doi.org/10.1007/s10898-024-01460-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes and develops inexact proximal methods for finding stationary points of the sum of a smooth function and a nonsmooth weakly convex one, where an error is present in the calculation of the proximal mapping of the nonsmooth term. A general framework for finding zeros of a continuous mapping is derived from our previous paper on this subject to establish convergence properties of the inexact proximal point method when the smooth term is vanished and of the inexact proximal gradient method when the smooth term satisfies a descent condition. The inexact proximal point method achieves global convergence with constructive convergence rates when the Moreau envelope of the objective function satisfies the Kurdyka–Łojasiewicz (KL) property. Meanwhile, when the smooth term is twice continuously differentiable with a Lipschitz continuous gradient and a differentiable approximation of the objective function satisfies the KL property, the inexact proximal gradient method achieves the global convergence of iterates with constructive convergence rates.},
  archive      = {J_JGO},
  author       = {Khanh, Pham Duy and Mordukhovich, Boris S. and Phat, Vo Thanh and Tran, Dat Ba},
  doi          = {10.1007/s10898-024-01460-7},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {611-646},
  shortjournal = {J. Glob. Optim.},
  title        = {Inexact proximal methods for weakly convex functions},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New iterative algorithms for solving split variational
inclusions. <em>JGO</em>, <em>91</em>(3), 587–609. (<a
href="https://doi.org/10.1007/s10898-024-01444-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study a class of split variational inclusion (SVI) and regularized split variational inclusion (RSVI) problems in real Hilbert spaces. We discuss various analytical properties of the net generated by the RSVI and establish the existence and uniqueness of the solution to the RSVI. Using analytical properties of this net and under certain assumptions on the parameters and mappings associated with the SVI, we establish the strong convergence of the sequence generated by our proposed iterative algorithm. We also deduce another iterative algorithm by taking the regularization parameters to be zero in our proposed algorithm. We establish the weak convergence of the sequence generated by our new algorithm under certain assumptions. Moreover, we discuss two special cases of the SVI, namely the split convex minimization and the split variational inequality problems, and give several numerical examples.},
  archive      = {J_JGO},
  author       = {Dey, Soumitra and Izuchukwu, Chinedu and Taiwo, Adeolu and Reich, Simeon},
  doi          = {10.1007/s10898-024-01444-7},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {587-609},
  shortjournal = {J. Glob. Optim.},
  title        = {New iterative algorithms for solving split variational inclusions},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new modified halpern-type splitting algorithm for solving
monotone inclusion problems in reflexive banach spaces. <em>JGO</em>,
<em>91</em>(3), 559–585. (<a
href="https://doi.org/10.1007/s10898-025-01467-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly introduces a new modified Halpern-type splitting algorithm for solving the monotone inclusion problem in real reflexive Banach spaces. Furthermore, the strong convergence of the sequence generated by our algorithm is proved under some mild assumptions imposed on the operators and parameters. Finally, several numerical experiments are performed, which illustrate the effectiveness of our algorithm.},
  archive      = {J_JGO},
  author       = {Chen, Lulu and Cai, Gang and Cholamjiak, Prasit and Inkrong, Papatsara},
  doi          = {10.1007/s10898-025-01467-8},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {559-585},
  shortjournal = {J. Glob. Optim.},
  title        = {A new modified halpern-type splitting algorithm for solving monotone inclusion problems in reflexive banach spaces},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On generalized KKT points for the motzkin–straus program.
<em>JGO</em>, <em>91</em>(3), 535–557. (<a
href="https://doi.org/10.1007/s10898-024-01457-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 1965, T.S. Motzkin and E. G. Straus established an elegant connection between the clique number of a graph and the global maxima of a quadratic program defined on the standard simplex. Over the years, this seminal finding has inspired a number of studies aimed at characterizing the properties of the (local and global) solutions of the Motzkin–Straus program. The result has also been generalized in various ways and has served as the basis for establishing new bounds on the clique number and developing powerful clique-finding heuristics. Despite the extensive work done on the subject, apart from a few exceptions, the existing literature pays little or no attention to the Karush–Kuhn–Tucker (KKT) points of the program. In the conviction that these points might reveal interesting structural properties of the graph underlying the program, this paper tries to fill in the gap. In particular, we study the generalized KKT points of a parameterized version of the Motzkin–Straus program, which are defined via a relaxation of the usual first-order optimality conditions, and we present a number of results that shed light on the symmetries and regularities of certain substructures associated with the underlying graph. These combinatorial structures are further analyzed using barycentric coordinates, thereby providing a link to a related quadratic program that encodes local structural properties of the graph. This turns out to be particularly useful in the study of the generalized KKT points associated with a certain class of graphs that generalize the notion of a star graph. Finally, we discuss the associations between the generalized KKT points of the Motzkin–Straus program and the so-called replicator dynamics, thereby offering an alternative, dynamical-system perspective on the results presented in the paper.},
  archive      = {J_JGO},
  author       = {Beretta, Guglielmo and Torcinovich, Alessandro and Pelillo, Marcello},
  doi          = {10.1007/s10898-024-01457-2},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {535-557},
  shortjournal = {J. Glob. Optim.},
  title        = {On generalized KKT points for the Motzkin–Straus program},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative mix thresholding algorithm with continuation
technique for mix sparse optimization and application. <em>JGO</em>,
<em>91</em>(3), 511–534. (<a
href="https://doi.org/10.1007/s10898-024-01441-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mix sparse structure is inherited in a wide class of practical applications, namely, the sparse structure appears as the inter-group and intra-group manners simultaneously. In this paper, we propose an iterative mix thresholding algorithm with continuation technique (IMTC) to solve the $$\ell _0$$ regularized mix sparse optimization. The significant advantage of the IMTC is that it has a closed-form expression and low storage requirement, and it is able to promote the mix sparse structure of the solution. We prove the convergence property and the linear convergence rate of the ITMC to a local minimum; moreover, we show that the ITMC approaches an approximate true mix sparse solution within a tolerance relevant to the noise level under an assumption of restricted isometry property. We also apply the mix sparse optimization to model the differential optical absorption spectroscopy analysis with the wavelength misalignment, and numerical results indicate that the IMTC can exactly and quantitatively predict the existing materials and the factual wavelength misalignment simultaneously within 0.1 s, which meets the demand of improvement of the automatic analysis software.},
  archive      = {J_JGO},
  author       = {Hu, Yaohua and Lu, Jian and Yang, Xiaoqi and Zhang, Kai},
  doi          = {10.1007/s10898-024-01441-w},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {511-534},
  shortjournal = {J. Glob. Optim.},
  title        = {Iterative mix thresholding algorithm with continuation technique for mix sparse optimization and application},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complexity of linearized quadratic penalty for optimization
with nonlinear equality constraints. <em>JGO</em>, <em>91</em>(3),
483–510. (<a href="https://doi.org/10.1007/s10898-024-01456-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider a nonconvex optimization problem with nonlinear equality constraints. We assume that both, the objective function and the functional constraints, are locally smooth. For solving this problem, we propose a linearized quadratic penalty method, i.e., we linearize the objective function and the functional constraints in the penalty formulation at the current iterate and add a quadratic regularization, thus yielding a subproblem that is easy to solve, and whose solution is the next iterate. Under a new adaptive regularization parameter choice, we provide convergence guarantees for the iterates of this method to an $$\epsilon $$ first-order optimal solution in $${\mathcal {O}}({\epsilon ^{-2.5}})$$ iterations. Finally, we show that when the problem data satisfy Kurdyka–Lojasiewicz property, e.g., are semialgebraic, the whole sequence generated by the proposed algorithm converges and we derive improved local convergence rates depending on the KL parameter. We validate the theory and the performance of the proposed algorithm by numerically comparing it with some existing methods from the literature.},
  archive      = {J_JGO},
  author       = {Bourkhissi, Lahcen El and Necoara, Ion},
  doi          = {10.1007/s10898-024-01456-3},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {483-510},
  shortjournal = {J. Glob. Optim.},
  title        = {Complexity of linearized quadratic penalty for optimization with nonlinear equality constraints},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A branch-and-bound algorithm for parametric mixed-binary
nonlinear programs. <em>JGO</em>, <em>91</em>(3), 457–481. (<a
href="https://doi.org/10.1007/s10898-024-01447-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As rapid response to changes becomes more imperative, optimization under uncertainty has continued to grow in both the continuous and mixed-integer fields. We design a branch-and-bound (BB) algorithm for mixed-binary nonlinear optimization problems with parameters in general locations. At every node of the BB tree we apply a state-of-the-art algorithm we have recently developed to approximately optimize parametric programs containing objectives and constraints biconvex in the variables and parameters. Numerical results are included.},
  archive      = {J_JGO},
  author       = {Pangia, Andrew C. and Wiecek, Margaret M.},
  doi          = {10.1007/s10898-024-01447-4},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {457-481},
  shortjournal = {J. Glob. Optim.},
  title        = {A branch-and-bound algorithm for parametric mixed-binary nonlinear programs},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybridizing two linear relaxation techniques in
interval-based solvers. <em>JGO</em>, <em>91</em>(3), 437–456. (<a
href="https://doi.org/10.1007/s10898-024-01449-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deterministic global optimization, techniques for linear relaxation of a non-convex program are used in the lower bound calculation phase. To achieve this phase, most deterministic global optimization codes use reformulation-linearization techniques. However, there exist also two interval-based polyhedral relaxation techniques which produce reliable bounds without adding new auxiliary variables, and which can take into account mathematical operations and most transcendental functions: (i) the affine relaxation technique, used in the IBBA code, based on affine forms and affine arithmetic, and (ii) the extremal Taylor technique, used in the Ibex-Opt code, which is based on a specific interval-based Taylor form. In this paper, we describe how these two interval-based linear relaxation techniques can be hybridized. These two approaches appear to be complementary, and such a hybrid method performs well on a representative sample of constrained global optimization instances.},
  archive      = {J_JGO},
  author       = {Araya, Ignacio and Messine, Frédéric and Ninin, Jordan and Trombettoni, Gilles},
  doi          = {10.1007/s10898-024-01449-2},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {437-456},
  shortjournal = {J. Glob. Optim.},
  title        = {Hybridizing two linear relaxation techniques in interval-based solvers},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jim---36">JIM - 36</h2>
<ul>
<li><details>
<summary>
(2025). Evaluation of data augmentation and loss functions in
semantic image segmentation for drilling tool wear detection.
<em>JIM</em>, <em>36</em>(2), 1491–1503. (<a
href="https://doi.org/10.1007/s10845-023-02313-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tool wear monitoring is crucial for quality control and cost reduction in manufacturing processes, of which drilling applications are one example. Identification of the wear area in images of cutting inserts is important to building a reliable ground truth for the development of indirect monitoring approaches. Therefore, we present a semantic image segmentation pipeline for wear detection on microscopy images of cutting inserts. A broadly used convolutional neural net, namely a U-Net, is trained with different preprocessing and optimisation task configurations: On the one hand the problem is considered as binary problem, and on the other hand as multiclass problem by differentiating the wear into two different types. By comparing these two problem formulations we investigate whether the separation of the two wear structures improves the performance of the recognition of the wear types. For both problem formulations three loss functions, i. e., Cross Entropy, Focal Cross Entropy, and a loss based on the Intersection over Union (IoU), are investigated.The use of different augmentation intensities during training suggests adequate but not too excessive augmentation, and that with optimal augmentation the choice of loss function gets less important. Furthermore, models are trained on image tiles of different sizes, which has an impact on producing artefacts on the whole image predictions performed by the overlap-tile strategy. In summary, the best performing models are binary models, trained on data with moderate augmentation and an IoU-based loss function.},
  archive      = {J_JIM},
  author       = {Schlager, Elke and Windisch, Andreas and Hanna, Lukas and Klünsner, Thomas and Hagendorfer, Elias Jan and Feil, Tamara},
  doi          = {10.1007/s10845-023-02313-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1491-1503},
  shortjournal = {J. Intell. Manuf.},
  title        = {Evaluation of data augmentation and loss functions in semantic image segmentation for drilling tool wear detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NG-net: No-grasp annotation grasp detection network for
stacked scenes. <em>JIM</em>, <em>36</em>(2), 1477–1490. (<a
href="https://doi.org/10.1007/s10845-024-02321-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving a high grasping success rate in a stacked environment is the core of the robot’s grasping task. Most methods achieve a high grasping success rate by training the network on a dataset containing a large number of grasping annotations which requires a lot of manpower and material resources. Therefore, achieving a high grasping success rate for stacked scenes without grasping annotations is a challenging task. To address this, we propose a No-Grasp annotation grasp detection network for stacked scenes (NG-Net). Our network consists of two modules: an object selection module and a grasp generation module. Specifically, the object selection module performs instance segmentation on the raw point cloud to select the object with the highest score as the object to be grasped, and the grasp generation module uses mathematical methods to analyze the geometric features of the point cloud surface to achieve grasping pose generation without grasping annotations. Experiments show that on the modified IPA-Binpicking dataset G, NG-Net has an average grasp success rate of 97% in the stacked scene grasp experiment, 14–22% higher than PointNetGPD.},
  archive      = {J_JIM},
  author       = {Shi, Min and Hou, Jingzhao and Li, Zhaoxin and Zhu, Dengming},
  doi          = {10.1007/s10845-024-02321-6},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1477-1490},
  shortjournal = {J. Intell. Manuf.},
  title        = {NG-net: No-grasp annotation grasp detection network for stacked scenes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin-driven real-time suppression of delamination
damage in CFRP drilling. <em>JIM</em>, <em>36</em>(2), 1459–1476. (<a
href="https://doi.org/10.1007/s10845-023-02315-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delamination damage should be avoided because it severely affects the quality of CFRP products. This paper proposes a digital twin (DT) driven method for real-time suppression of delamination damage to ensure the highest quality hole exit. The relationship between the increase in thrust caused by tool wear and CFRP delamination was analyzed through extensive drilling experiments. The evolving twin models were developed to integrate the virtual space of the drilling process. Once the cutting parameters and thrust signals were input into the twin, the Gaussian process regression and mathematical models predicted the current tool wear and thrust curve, respectively. The feedback results from the DT dynamically interact with the real drilling operation after the optimization function solves the current critical feed rate (CFR). A DT scheme was designed, and the performance of the deployed DT was tested through an online service panel. The results show that the DT has excellent real-time prediction capability within 100 hole-making cycles, with maximum errors of 4.1% and 4.2% for tool wear and thrust prediction at the exit, respectively. Compared to conventional drilling (CD), DT technology provides closed-loop feedback on the time-varying CFR for each hole, resulting in no delamination mode I and up to 48.4% suppression of delamination mode III. This research has achieved intelligent virtual-real linkage in the CFRP drilling process, providing important theoretical support for effectively suppressing delamination damage in the automated production process.},
  archive      = {J_JIM},
  author       = {Chen, Jielin and Li, Shuang and Teng, Hanwei and Leng, Xiaolong and Li, Changping and Kurniawan, Rendi and Ko, Tae Jo},
  doi          = {10.1007/s10845-023-02315-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1459-1476},
  shortjournal = {J. Intell. Manuf.},
  title        = {Digital twin-driven real-time suppression of delamination damage in CFRP drilling},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible dual-mode sensor with accurate contact pressure
sensing and contactless distance detection functions for robotic
perception. <em>JIM</em>, <em>36</em>(2), 1445–1457. (<a
href="https://doi.org/10.1007/s10845-023-02314-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel flexible dual-mode sensor with both contact pressure and distance sensing abilities for robotic grasping and manipulation applications. The proposed flexible dual-mode sensor measures contactless distances by flat interdigitated electrodes, based on electrical field detection principle. Meanwhile the sensor detects contact pressures by truncated pyramid-shaped porous composites based on graphene nanoplate and silicone rubber. Both the functions of the sensor are encapsulated by cascading assembly, the different sensing units are nested and arranged to avoid coupling effects between different sensing signals. The structural design, working principle, and fabrication process to make the flexible dual-mode sensor were presented. Characterization tests showed that the developed flexible dual-mode sensor has a high sensitivity of 0.33 V/N and stability for contact pressure sensing, this sensor can also detect the distances between objects and sensor with high accuracy. The dual-mode sensor was then mounted onto a robotic arm to perform object’s grasping and collision experiments, results demonstrated that the sensor can accurately measure the distributed contact force and distance between objects for tactile perception. Thus, our proposed flexible dual-mode sensor would have great prospects in robotic safety detection and manipulation applications.},
  archive      = {J_JIM},
  author       = {Chen, Zhijian and Wang, Yancheng and Zhang, Zhongtan and Mei, Deqing and Liu, Weijie},
  doi          = {10.1007/s10845-023-02314-x},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1445-1457},
  shortjournal = {J. Intell. Manuf.},
  title        = {Flexible dual-mode sensor with accurate contact pressure sensing and contactless distance detection functions for robotic perception},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label oxide classification in float-zone silicon
crystal growth using transfer learning and asymmetric loss.
<em>JIM</em>, <em>36</em>(2), 1429–1444. (<a
href="https://doi.org/10.1007/s10845-023-02302-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Float-Zone (FZ) crystal growth process allows for producing higher purity silicon crystal with much lower concentrations of impurities, in particular low oxygen content. Nevertheless, the FZ process occasionally faces the problem of small contamination from oxidation. This can come in the form of a thin oxide layer that may form on un-melted polysilicon surface. The appearance of the oxide layer indicates degraded machine performance and the need for machine maintenance. Therefore, oxide investigation is important for improving both the FZ process and FZ machines, and the first step is oxide recognition. In this study, we characterized oxide into mainly three varieties, according to their surface texture characteristics, which are: (i) spot (ii) shadow and (iii) ghost curtain. We leveraged FZ images captured from the vision system integrated on the FZ machine to establish an oxide dataset. Targeted for data imbalance problem in our dataset, a method based on transfer learning and asymmetric loss for multi-label oxide classification is presented in this work. The results showed that the pre-trained model and the asymmetric loss used for training outperformed the baseline models and improved the classification performance. Furthermore, this study deeply investigated the effectiveness of the components of asymmetric loss. Finally, Gradient-weighted Class Activation Mapping (Grad-CAM) was employed to explain decision process of the models in order to adopt them in the industry.},
  archive      = {J_JIM},
  author       = {Chen, Tingting and Tosello, Guido and Calaon, Matteo},
  doi          = {10.1007/s10845-023-02302-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1429-1444},
  shortjournal = {J. Intell. Manuf.},
  title        = {Multi-label oxide classification in float-zone silicon crystal growth using transfer learning and asymmetric loss},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential contrast guidance for aeroengine fault
diagnosis with limited data. <em>JIM</em>, <em>36</em>(2), 1409–1427.
(<a href="https://doi.org/10.1007/s10845-023-02305-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven methods have high requirements for data samples and the ideal state is to have sufficient samples and labels for model training. However, due to the limited sample of aeroengine fault data, existing methods often cannot achieve good classification results. To solve this problem, a contrastive learning strategy guided by fault type differences for aeroengine fault diagnosis with limited samples is proposed. Different from the traditional contrastive learning paradigm using data augmentation, the proposed method uses the fault data to construct sample pairs, uses similarity comparison to learn fault features from limited data, and uses the learned fault features for fault diagnosis. A deep learning model for joint training of feature extractor and classifier is built to improve the fault diagnosis accuracy. Finally, the aeroengine dataset and bearing dataset are used to verify the effectiveness of the proposed method in the case of limited data. The experimental results show that compared with the most advanced methods, the proposed method can achieve higher fault diagnosis accuracy.},
  archive      = {J_JIM},
  author       = {He, Wenhui and Lin, Lin and Fu, Song and Tong, Changsheng and Zu, Lizheng},
  doi          = {10.1007/s10845-023-02305-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1409-1427},
  shortjournal = {J. Intell. Manuf.},
  title        = {Differential contrast guidance for aeroengine fault diagnosis with limited data},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective sustainable production planning for a hybrid
multi-stage manufacturing-remanufacturing system with grade-based
classification of recovered and remanufactured products. <em>JIM</em>,
<em>36</em>(2), 1385–1407. (<a
href="https://doi.org/10.1007/s10845-023-02308-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of multi-objective production planning in a hybrid manufacturing and remanufacturing system (HMRS), introducing several significant contributions. First, we propose a new formulation of the problem that extends the existing literature by introducing a multi-objective model. This model aims to minimize both total costs and $$CO_2$$ emissions within a hybrid system composed of various machines in charge of producing new and remanufactured products of different qualities. To efficiently solve this complex problem, we present an innovative approach that integrates several techniques, including NSGA-II, the entropy weight method and the TOPSIS technique. Our research focuses on the economic and environmental aspects of the remanufacturing process, seeking to determine the optimal manufacturing and remanufacturing plan. This plan aims to meet demand for new products and maximize satisfaction for remanufactured products of different qualities, while minimizing the total economic costs and $$CO_2$$ emissions incurred during the various manufacturing and remanufacturing stages, including set-up, production, inventory and disposal. To address the multi-objective nature of this problem, we develop a mathematical model and introduce an approach based on the non-dominated genetic sorting algorithm (NSGA-II). To help decision-making, we use the technique of performance ranking by similarity to the ideal solution (TOPSIS) in combination with the entropy weight method (EWM) to objectively obtain the optimal compromise solution from the Pareto front provided by NSGA-II. Finally, we conduct computational experiments to assess the environmental impact of carbon emissions associated with new, remanufactured and discarded products over a finite production horizon. We illustrate the adaptability of the proposed approach by applying it to two distinct remanufacturing strategies: one where remanufacturing is used to reduce waste, and one where demand for remanufactured products is critical, with a penalty cost associated with any shortfall in demand.},
  archive      = {J_JIM},
  author       = {Lahmar, Houria and Dahane, Mohammed and Mouss, Kinza Nadia and Haoues, Mohammed},
  doi          = {10.1007/s10845-023-02308-9},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1385-1407},
  shortjournal = {J. Intell. Manuf.},
  title        = {Multi-objective sustainable production planning for a hybrid multi-stage manufacturing-remanufacturing system with grade-based classification of recovered and remanufactured products},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal deep learning for explainable vision-based quality
inspection under visual interference. <em>JIM</em>, <em>36</em>(2),
1363–1384. (<a
href="https://doi.org/10.1007/s10845-023-02297-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based quality inspection is a key step to ensure the quality control of complex industrial products. However, accurate defect recognition for complex products with information-rich, structure-irregular and significantly different patterns is still a tough problem, since it causes the strong visual interference. This paper proposes a causal deep learning method (CDLM) to tackle the explainable vision-based quality inspection under visual interference. First, a structural causal model for defect recognition of complex industrial products is constructed and a causal intervention strategy to overcome the background interference is generated. Second, a defect-guided recognition neural network (DGRNN) is constructed, which can realize accurate defect recognition under the training of CDLM via feature-wise causal intervention using two sub-networks with feature difference mechanism. Finally, the causality between defect features and defective product labels can guide the DGRNN to complete the accurate and explainable learning of defect in a causal direction of optimization. Quantitative experiments show that the proposed method achieves recognition accuracy of 94.09% and 93.95% on two fabric datasets respectively, which outperforms the cutting-edge inspection models. Besides, Grad-CAM visualization experiments show that the proposed method successfully captures the data causality and realizes the explainable defect recognition.},
  archive      = {J_JIM},
  author       = {Liang, Tianbiao and Liu, Tianyuan and Wang, Junliang and Zhang, Jie and Zheng, Pai},
  doi          = {10.1007/s10845-023-02297-9},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1363-1384},
  shortjournal = {J. Intell. Manuf.},
  title        = {Causal deep learning for explainable vision-based quality inspection under visual interference},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated porosity segmentation in laser powder bed fusion
part using computed tomography: A validity study. <em>JIM</em>,
<em>36</em>(2), 1341–1361. (<a
href="https://doi.org/10.1007/s10845-023-02296-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection in laser powder bed fusion (LPBF) parts is a critical step for in their quality control. Ensuring the integrity of these parts is essential for a broader adoption of this manufacturing process in highly standardized industries such as aerospace. With many challenges to overcome, there is currently no standardized image analysis and segmentation process for the defect analysis of LPBF parts. This process is often manual and operator-dependent, which limits the repeatability and the reproducibility of the analytical methods applied, raising questions about the validity of the analysis. The pore segmentation step is critical for porosity analysis since the pore size and morphology metrics are calculated directly from the results of the segmentation process. In this work, Ti6Al4V specimens with purposely induced and controlled porosity were printed, scanned 5 times on two CT scan systems by two different operators, and then reconstructed as 3D volumes. The porosity in these specimens was analyzed using manual and Otsu thresholding and a convolutional neural network (CNN) deep learning segmentation algorithm. Then, a variance component estimation realized over 75 porosity analyses indicated that, independently of the operator and the CT scan system used, the CNN provided the best repeatability and reproducibility in the LPBF specimens of this study. Finally, a multimodal correlative study using higher resolution laser confocal microscopy observations was used for a multi-scale pore-to-pore comparison and as a reliability assessment of the segmentation algorithms. The validity of the CNN-based pore segmentation was thus assessed through improved repeatability, reproducibility, and reliability.},
  archive      = {J_JIM},
  author       = {Desrosiers, Catherine and Letenneur, Morgan and Bernier, Fabrice and Piché, Nicolas and Provencher, Benjamin and Cheriet, Farida and Guibault, François and Brailovski, Vladimir},
  doi          = {10.1007/s10845-023-02296-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1341-1361},
  shortjournal = {J. Intell. Manuf.},
  title        = {Automated porosity segmentation in laser powder bed fusion part using computed tomography: A validity study},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stacked encoded cascade error feedback deep extreme learning
machine network for manufacturing order completion time. <em>JIM</em>,
<em>36</em>(2), 1313–1339. (<a
href="https://doi.org/10.1007/s10845-023-02303-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel stacked encoded cascade error feedback deep extreme learning machine (SEC-E-DELM) network is proposed to predict order completion time (OCT) considering the historical production planning and control data. Usually, the actual OCT significantly deviates from the planned because of recessive disturbances. The disturbances do not shut down production but slow down the production that accumulates over time, causing significant deviation of actual time from planned. The generation of weight parameters in neural networks using a randomization approach has a significant effect on generalization performance. To predict the OCT, firstly, the stacked autoencoder is used to generate input connection weights for the network by learning a deep representation of the real data. Secondly, the learned distribution of the encoder is connected to the network output through output connection weights incrementally learned by the Moore–Penrose inverse. Thirdly, the new hidden unit is added one by one to the network, which receives input connections from the input units and the last layer of the encoder to avoid overfitting and improve model generalization. The input connection weights for the newly added hidden unit are analytically calculated by the error feedback function to enhance the convergence rate by further learning deep features. Lastly, the hidden unit keeps on adding one by one by receiving connections from input units and some of the existing hidden units to make a deep cascade architecture. An extensive comparative study demonstrates that calculating connection weights by the proposed method helps to significantly improve the generalization performance and robustness of the network.},
  archive      = {J_JIM},
  author       = {Khan, Waqar Ahmed and Masoud, Mahmoud and Eltoukhy, Abdelrahman E. E. and Ullah, Mehran},
  doi          = {10.1007/s10845-023-02303-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1313-1339},
  shortjournal = {J. Intell. Manuf.},
  title        = {Stacked encoded cascade error feedback deep extreme learning machine network for manufacturing order completion time},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Correction: Optimal data-driven control of manufacturing
processes using reinforcement learning: An application to wire arc
additive manufacturing. <em>JIM</em>, <em>36</em>(2), 1311. (<a
href="https://doi.org/10.1007/s10845-024-02450-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JIM},
  author       = {Mattera, Giulio and Caggiano, Alessandra and Nele, Luigi},
  doi          = {10.1007/s10845-024-02450-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1311},
  shortjournal = {J. Intell. Manuf.},
  title        = {Correction: optimal data-driven control of manufacturing processes using reinforcement learning: an application to wire arc additive manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Optimal data-driven control of manufacturing processes
using reinforcement learning: An application to wire arc additive
manufacturing. <em>JIM</em>, <em>36</em>(2), 1291–1310. (<a
href="https://doi.org/10.1007/s10845-023-02307-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, artificial intelligence (AI) has become a crucial Key Enabling Technology with extensive application in diverse industrial sectors. Recently, considerable focus has been directed towards utilizing AI for the development of optimal control in industrial processes. In particular, reinforcement learning (RL) techniques have made significant advancements, enabling their application to data-driven problem-solving for the control of complex systems. Since industrial manufacturing processes can be treated as MIMO non-linear systems, RL can be used to develop complex data-driven intelligent decision-making or control systems. In this work, the workflow for developing a RL application for industrial manufacturing processes, including reward function setup, development of reduced order models and control policy construction, is addressed, and a new process-based reward function is proposed. To showcase the proposed approach, a case study is developed with reference to a wire arc additive manufacturing (WAAM) process. Based on experimental tests, a Reduced Order Model of the system is obtained and a Deep Deterministic Policy Gradient Controller is trained with aim to produce a simple geometry. Particular attention is given to the sim-to-real process by developing a WAAM simulator which allows to simulate the process in a realistic environment and to generate the code to be deployed on the motion platform controller.},
  archive      = {J_JIM},
  author       = {Mattera, Giulio and Caggiano, Alessandra and Nele, Luigi},
  doi          = {10.1007/s10845-023-02307-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1291-1310},
  shortjournal = {J. Intell. Manuf.},
  title        = {Optimal data-driven control of manufacturing processes using reinforcement learning: An application to wire arc additive manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A digital solution for CPS-based machining path optimization
for CNC systems. <em>JIM</em>, <em>36</em>(2), 1261–1290. (<a
href="https://doi.org/10.1007/s10845-023-02289-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the deep integration of advanced information technology and advanced manufacturing technology has gradually become one of the main ways to achieve smart manufacturing. The computer numerical control (CNC) system is the basic equipment for machining and manufacturing, and the quality and efficiency of the system’s machining are the basis for supporting and ensuring smart manufacturing. However, the G-code used in CNC machining is usually generated with computer-aided manufacturing (CAM) according to a static model, and its tool path is relatively rough, with uneven adjacent paths and bad points in the path causing machining defects. To solve these problems, a modeling approach combining the basic elements of the intelligent CNC system with the human-cyber-physical system (HCPS) model is proposed, and a digital solution for tool path optimization is further proposed, integrating the redesign process of CAM tool path into cyber application. In addition, the process of tool path optimization is processed in steps, and a pipelined processing flow is established to accelerate the optimization process. Finally, the effectiveness of the proposed method is demonstrated using an example of process file optimization for a pentagram convex rib model.},
  archive      = {J_JIM},
  author       = {Zhang, Lipeng and Yu, Haoyu and Wang, Chuting and Hu, Yi and He, Wuwei and Yu, Dong},
  doi          = {10.1007/s10845-023-02289-9},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1261-1290},
  shortjournal = {J. Intell. Manuf.},
  title        = {A digital solution for CPS-based machining path optimization for CNC systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent g-code-based power prediction of ultra-precision
CNC machine tools through 1DCNN-LSTM-attention model. <em>JIM</em>,
<em>36</em>(2), 1237–1260. (<a
href="https://doi.org/10.1007/s10845-023-02293-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the most promising and advanced technology, ultra-precision machining (UPM) has dramatically increased its production volume for wide-range applications in various high-tech fields such as chips, optics, microcircuits, biotechnology, etc. The concomitantly negative environmental impact resulting from huge-volume UPM has attracted unprecedented attention from both academia and industry. Accurate energy prediction of ultra-precision machine tools (UPMTs) can provide significant insight into energy planning, machining strategy, and energy conservation. Data-driven models for predicting energy have become increasingly popular due to their high accuracy and low modeling difficulty. However, existing data-driven models only focus on ordinary precision machine tools, and their applications on UPMTs are hardly studied. To fill the gap, this paper proposed a data-driven model constructed with 1DCNN-LSTM-Attention layers for predicting the instantaneous power profile of a five-axes UPMT. In the data-preparation phase, an advanced G-code interpreter was developed to generate the working status dataset from the G-code command and accurately match them with the power data collected. Random hyperparameters searching method was adopted to tune the 1DCNN-LSTM-Attention structure for better accuracy in the model creation phase. Finally, the sensitivity of these hyperparameters on the model performance was analyzed. Results demonstrate that the learning rate, 1DCNN, LSTM and dense layer numbers are identified as critical parameters affecting the model performance. The optimized 1DCNN-LSTM-Attention model outperforms other models, achieving an R2 value of 0.93. This work first validate the feasibility of utilizing advanced machine learning techniques for predicting energy consumption in UPM field, which can further promoting energy-efficient and sustainable UPM practices by digitalizing the energy consumption process.},
  archive      = {J_JIM},
  author       = {Xu, Zhicheng and Selvaraj, Vignesh and Min, Sangkee},
  doi          = {10.1007/s10845-023-02293-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1237-1260},
  shortjournal = {J. Intell. Manuf.},
  title        = {Intelligent G-code-based power prediction of ultra-precision CNC machine tools through 1DCNN-LSTM-attention model},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using GANs to predict milling stability from limited data.
<em>JIM</em>, <em>36</em>(2), 1201–1235. (<a
href="https://doi.org/10.1007/s10845-023-02291-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Milling is a key manufacturing process that requires the selection of operating parameters that provide efficient performance. However, the presence of chatter, a self-excited vibration causing poor surface finish and potential damage to the machine and cutting tool, makes it challenging to select the appropriate parameters. To predict chatter, stability maps are commonly used, but their generation requires expensive data, making it difficult to employ these maps in industry. Therefore, there is a pressing need for an approach that can accurately predict stability maps using limited experimental data. This study introduces the new Encoder GAN (EGAN) approach based on Generative Adversarial Networks (GANs) that predicts stability maps using limited experimental data. The approach consists of the encoder, generator, and discriminator subnetworks and uses the trained encoder and generator to predict the target stability map. This versatile method can be applied to various tool setups and can accurately predict stability maps with limited experimental data (five to 10 cutting tests) even when there is little information available for unknown parameters. The study evaluates the proposed approach using both numerical data and experiments and demonstrates its superior performance compared to state-of-the-art benchmarks.},
  archive      = {J_JIM},
  author       = {Rezaei, Shahrbanoo and Cornelius, Aaron and Karandikar, Jaydeep and Schmitz, Tony and Khojandi, Anahita},
  doi          = {10.1007/s10845-023-02291-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1201-1235},
  shortjournal = {J. Intell. Manuf.},
  title        = {Using GANs to predict milling stability from limited data},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards scalability for resource reconfiguration in robotic
assembly line balancing problems using a modified genetic algorithm.
<em>JIM</em>, <em>36</em>(2), 1175–1199. (<a
href="https://doi.org/10.1007/s10845-023-02292-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assembly lines are still one of the most used manufacturing systems in modern-day production. Most research affects the building of new lines and, less frequently, the reconfiguration of existing lines. However, the first is insufficient to meet the reconfigurable production paradigm required by volatile market demands. Consequent reconfiguration of resources by production requests affects companies’ competitiveness. This paper introduces a problem-specific genetic algorithm for optimizing the reconfiguration of a Robotic Assembly Line Balancing Problem with Task Types, including additional company constraints. First, we present the greenfield and brownfield optimization objectives, then a mathematical problem formulation and the composition of the genetic algorithm. We evaluate our model against an Integer Programming baseline on a reconfiguration dataset with multiple equipment alternatives. The results demonstrate the capabilities of the genetic algorithm for the greenfield case and showcase the possibilities in the brownfield case. With a scalability improvement through computation time decrease of up to $$\sim $$ 2.75 $$\times $$ , reduced number of equipment and workstations, but worse objective values, the genetic algorithm holds the potential for reconfiguring assembly lines. However, the genetic algorithm has to be further optimized for the reconfiguration to leverage its full potential.},
  archive      = {J_JIM},
  author       = {Albus, Marcel and Hornek, Timothée and Kraus, Werner and Huber, Marco F.},
  doi          = {10.1007/s10845-023-02292-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1175-1199},
  shortjournal = {J. Intell. Manuf.},
  title        = {Towards scalability for resource reconfiguration in robotic assembly line balancing problems using a modified genetic algorithm},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human operator decision support for highly transient
industrial processes: A reinforcement learning approach. <em>JIM</em>,
<em>36</em>(2), 1159–1174. (<a
href="https://doi.org/10.1007/s10845-023-02295-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most industrial processes are not fully-automated. Although fast and low-level control can be handled by controllers, initializing and adjusting the reference, or setpoint, values, are commonly tasks assigned to human operators. A major challenge is the control policy variation among operators. In turn this can result in inconsistencies in the final product. In order to guide operators to pursue better and more consistent performance, researchers explore the optimal control policy through different approaches. Although in different applications, researchers use different approaches, an accurate process model is still crucial to the approaches. However, for a highly transient process (e.g., the startup of a manufacturing process), modeling can be challenging and inaccurate, and approaches highly relying on a process model may not work well. In this paper, we apply the idea of offline reinforcement learning (RL), which requires the RL agent to learn control policies from a previously collected dataset. More specifically, a modified advantage weighted regression is used to guide the agent to take the more advantageous actions. In addition, we train and verify the agent by using casting data of multiple human operators from an industrial twin-roll steel strip casting process.},
  archive      = {J_JIM},
  author       = {Ruan, Jianqi and Nooning, Bob and Parkes, Ivan and Blejde, Wal and Chiu, George and Jain, Neera},
  doi          = {10.1007/s10845-023-02295-x},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1159-1174},
  shortjournal = {J. Intell. Manuf.},
  title        = {Human operator decision support for highly transient industrial processes: A reinforcement learning approach},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embodied intelligence in manufacturing: Leveraging large
language models for autonomous industrial robotics. <em>JIM</em>,
<em>36</em>(2), 1141–1157. (<a
href="https://doi.org/10.1007/s10845-023-02294-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper delves into the potential of Large Language Model (LLM) agents for industrial robotics, with an emphasis on autonomous design, decision-making, and task execution within manufacturing contexts. We propose a comprehensive framework that includes three core components: (1) matches manufacturing tasks with process parameters, emphasizing the challenges in LLM agents’ understanding of human-imposed constraints; (2) autonomously designs tool paths, highlighting the LLM agents’ proficiency in planar tasks and challenges in 3D spatial tasks; and (3) integrates embodied intelligence within industrial robotics simulations, showcasing the adaptability of LLM agents like GPT-4. Our experimental results underscore the distinctive performance of the GPT-4 agent, especially in Component 3, where it is outstanding in task planning and achieved a success rate of 81.88% across 10 samples in task completion. In conclusion, our study accentuates the transformative potential of LLM agents in industrial robotics and suggests specific avenues, such as visual semantic control and real-time feedback loops, for their enhancement.},
  archive      = {J_JIM},
  author       = {Fan, Haolin and Liu, Xuan and Fuh, Jerry Ying Hsi and Lu, Wen Feng and Li, Bingbing},
  doi          = {10.1007/s10845-023-02294-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1141-1157},
  shortjournal = {J. Intell. Manuf.},
  title        = {Embodied intelligence in manufacturing: Leveraging large language models for autonomous industrial robotics},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A rotary extrusion system with a rectangular-orifice nozzle:
Toward adaptive resolution in material extrusion additive manufacturing.
<em>JIM</em>, <em>36</em>(2), 1123–1139. (<a
href="https://doi.org/10.1007/s10845-023-02288-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material extrusion additive manufacturing (MEAM) has revolutionized the production of complex designs while minimizing the amount of effort required due to its simple production pipeline. However, MEAM naturally comes with a well-known trade-off; higher build resolution often tends to enhance the product quality at the cost of a slower build rate. Nozzles, the standard tool for thermoplastic extrusion in MEAM, have evolved into a crucial component of the process for controlling the product’s build resolution. The purpose of this study is to investigate the details of a novel extrusion system that makes use of a rotating nozzle with an unconventional aperture, in contrast to its typical (i.e., circular-orifice) counterparts. The unique nozzle configuration that lacks axial symmetry allows for precise control over the effective dimension of the extrusion via rotational guiding. By positioning the oblong orifice at intermediate orientations, the presented approach seeks to provide continuously variable intralayer and interlayer resolutions for MEAM processes. This paper explores the distinctive characteristics of this new nozzle design as well as the potential uses of the novel extrusion system. The outcomes of the conducted tests demonstrate the proof-of-concept for creating variable bead width within the layers, in addition to adaptable layer heights throughout the 3D objects. Possible limitations of the new approach and future perspectives are discussed in detail.},
  archive      = {J_JIM},
  author       = {Gharehpapagh, Bahar and Dilberoglu, Ugur M. and Yaman, Ulas and Dolen, Melik},
  doi          = {10.1007/s10845-023-02288-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1123-1139},
  shortjournal = {J. Intell. Manuf.},
  title        = {A rotary extrusion system with a rectangular-orifice nozzle: Toward adaptive resolution in material extrusion additive manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural lumped parameter differential equations with
application in friction-stir processing. <em>JIM</em>, <em>36</em>(2),
1111–1121. (<a
href="https://doi.org/10.1007/s10845-023-02271-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lumped parameter methods aim to simplify the evolution of spatially-extended or continuous physical systems to that of a “lumped” element representative of the physical scales of the modeled system. For systems where the definition of a lumped element or its associated physics may be unknown, modeling tasks may be restricted to full-fidelity physics simulations. In this work, we consider data-driven modeling tasks with limited point-wise measurements of otherwise continuous systems. We build upon the notion of the Universal Differential Equation (UDE) to construct data-driven models for reducing dynamics to that of a lumped parameter and inferring its properties. The flexibility of UDEs allow for composing various known physical priors suitable for application-specific modeling tasks, including lumped parameter methods. The motivating example for this work is the plunge and dwell stages for friction-stir welding; specifically, (i) mapping power input into the tool to a point-measurement of temperature and (ii) using this learned mapping for process control.},
  archive      = {J_JIM},
  author       = {Koch, James and Choi, WoongJo and King, Ethan and Garcia, David and Das, Hrishikesh and Wang, Tianhao and Ross, Ken and Kappagantula, Keerti},
  doi          = {10.1007/s10845-023-02271-5},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1111-1121},
  shortjournal = {J. Intell. Manuf.},
  title        = {Neural lumped parameter differential equations with application in friction-stir processing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine-learning based process monitoring for automated
composites manufacturing. <em>JIM</em>, <em>36</em>(2), 1095–1110. (<a
href="https://doi.org/10.1007/s10845-023-02282-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated fibre placement (AFP) is an advanced robotic manufacturing technique which can overcome the challenges of traditional composite manufacturing. The interlaminar strength of AFP-manufactured composites depends on the in-situ thermal history during manufacturing. The thermal history is controlled by the choice of processing conditions and improper interfacial temperatures may result in insufficient bonding. Being able to better predict such maintenance issues in real-time is an important focus of smart manufacturing and Industry 4.0 to improve manufacturing operations. The data analysis of real-time temperature measurements during AFP composites manufacturing requires the temperature profiles from Finite Element Analysis (FEA) based simulations of the AFP process to better predict the quality of layup. However, the FEA simulations of the AFP process are computationally expensive. This study focuses on developing a digital tool enabling real-time process monitoring and predictive maintenance of the AFP process. The digital tool constitutes a machine learning-based surrogate model based on results from Finite Element Analysis (FEA) simulations of the AFP process to predict the in-situ thermal profile during AFP manufacturing. Multivariate Linear Regression, Multivariate Polynomial Regression, Support Vector Machine, Random Forest and Artificial Neural Network (ANN)-based models are compared to conclude that ANN based surrogate model performs best by predicting the important parameters of thermal profiles with a mean absolute percentage error of 1.56% on additional test data and reducing the time by four orders of magnitude as compared to FEA simulations. The predicted thermal profile can be compared with the real-time in-situ temperatures during manufacturing to predict the quality of the layup. A GUI application is developed to provide predicted thermal profiles data for analysis in conjunction with real-time temperatures during manufacturing enabling monitoring and predictive maintenance of the AFP process and paving way for the development of a digital twin of the AFP composites manufacturing process.},
  archive      = {J_JIM},
  author       = {Mujtaba, Ahmed and Islam, Faisal and Kaeding, Patrick and Lindemann, Thomas and Gangadhara Prusty, B.},
  doi          = {10.1007/s10845-023-02282-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1095-1110},
  shortjournal = {J. Intell. Manuf.},
  title        = {Machine-learning based process monitoring for automated composites manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thermal-structure finite element simulation system
architecture in a cloud-edge-end collaborative environment.
<em>JIM</em>, <em>36</em>(2), 1063–1094. (<a
href="https://doi.org/10.1007/s10845-023-02269-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the urgent need for finite element simulation, a thermal-structure finite element simulation system architecture is designed to shorten the simulation cycle and improve the mechanical structure design efficiency under a cloud-edge-end collaborative environment. Then, a calculation kernel of the boundary conditions is proposed, and a thermal-structure closed-loop iterative model is established and embedded into the finite element simulation system. The interactions among the simulation results and boundary conditions are considered, and the boundary conditions are corrected by the simulation results. Finally, the thermal-structure finite element simulation system architecture is verified. The optimal configuration of the central processing unit number and memory size for different finite element models is identified, and the simulation efficiency and throughput are improved significantly. In addition, the proposed thermal-structure finite element simulation system architecture with a cloud-edge-end is applied to conduct the thermal-structure behavior simulation for the feed drive system, spindle system, precision horizontal machining center, and gantry machining center. The machine tool designer without specialized knowledge about thermal-structure simulation is able to achieve a high simulation accuracy at the design stage, and the executing performance of the proposed thermal-structure finite element simulation system with cloud-edge-end architecture is far higher than that of the thermal-structure finite element simulation systems with cloud-end and cloud architectures. With the implementation of the simulation system with the cloud-edge-end architecture, the execution time is reduced from 2557 and 2082s to 1642 s as compared with the simulation systems with the cloud-end and cloud architectures, respectively. The simulation kernel is effective in simulating thermal-structure behaviors. The average deviations between the measured and simulated temperatures for the rear and front bearings are 4.36% and 3.15%, respectively. The average deviation between the measured and simulated deformations is 8.17%.},
  archive      = {J_JIM},
  author       = {Liu, Jialan and Ma, Chi and Wang, Shilong},
  doi          = {10.1007/s10845-023-02269-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1063-1094},
  shortjournal = {J. Intell. Manuf.},
  title        = {Thermal-structure finite element simulation system architecture in a cloud-edge-end collaborative environment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative adversarial networks and hessian locally linear
embedding for geometric variations management in manufacturing.
<em>JIM</em>, <em>36</em>(2), 1033–1062. (<a
href="https://doi.org/10.1007/s10845-023-02284-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric variations and uncertainty are generally observed on every manufactured workpiece and have a critical influence on the functional performance of mechanical parts. In computer-aided tolerancing, the Skin Model Shapes framework is recognized as a novel paradigm to embed the expected and observed geometric variations of mechanical products based on discrete geometry representation schemes. Currently, the generation of Skin Model Shapes is still limited due to the lack of knowledge-based parameter settings in the design process, and the consideration of enriched simulation and measurement data. In this paper, a novel method based on two distinct techniques, namely Generative Adversarial Networks (GAN) and Hessian Locally Linear Embedding (HLLE), is proposed to generate Skin Model Shapes without any explicitly defined parameters. A Wasserstein GAN structure is trained for generating patterns of geometric deviations based on simulation data. Geometric deviations on planar and cylindrical surfaces are considered in a training process since both types of surfaces are widely used in mechanical engineering. HLLE is used in the paper to extend the implementation of the proposed deviation mapping process from planar/cylindrical surfaces to other types of surfaces scattered in 3D space. The proposed Skin Model Shapes generation process enables the efficient generation of part representatives with geometric deviations without the need for extensive deviation modeling. Meanwhile, the proposed method overcomes the common limitation of simulating different types (e.g. rotational and freeform) of non-ideal surfaces on Skin Model Shapes. The implemented case studies show that our method can be used to generate hundreds of distinct Skin Model Shapes within seconds while the distributions of simulated geometric deviations on the surfaces are consistent with the measurement results. Meanwhile, the generated Skin Model Shapes can be used for further applications such as assembly simulation and tolerance analysis to obtain more realistic simulation results.},
  archive      = {J_JIM},
  author       = {Qie, Yifan and Schleich, Benjamin and Anwer, Nabil},
  doi          = {10.1007/s10845-023-02284-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1033-1062},
  shortjournal = {J. Intell. Manuf.},
  title        = {Generative adversarial networks and hessian locally linear embedding for geometric variations management in manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised learning for steel surface inspection using
magnetic flux leakage signal. <em>JIM</em>, <em>36</em>(2), 1021–1031.
(<a href="https://doi.org/10.1007/s10845-023-02286-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a semi-supervised learning model for detecting multi-defect classification and localization on the steel surface for industries with limited labeled datasets. This study uses 1-D data from magnetic flux leakage (MFL) testing, a powerful and cost-effective nondestructive inspection method for steel bars. Most steel surface defect systems are based on supervised learning classification with 2-D image datasets. However, acquiring labeled datasets for developing supervised learning models is practically limited in the actual steel manufacturing process. Furthermore, due to the frequent occurrence of multiple defect classes on the same steel bar, the problem of multi-defect classification and localization needs to be addressed. Therefore, this paper proposes a steel bar surface inspection system for multi-defect classification and localization based on a semi-supervised learning model and MFL signals. The proposed system solves the multi-defect classification and localization problem by reducing the feature dimension with an autoencoder. Then, it classifies the defects based on the semi-supervised support vector machines that require only a small portion of the labeled dataset. Also, the classification process is repeated on the overlapped small steel section to address the multi-defect classification and localization issue. When it was evaluated on an industry MFL inspection dataset, the accuracy ranged from 81% to 90% when the labeled data ratio varied from 2% to 90%.},
  archive      = {J_JIM},
  author       = {Park, Jae-Eun and Kim, Young-Keun},
  doi          = {10.1007/s10845-023-02286-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1021-1031},
  shortjournal = {J. Intell. Manuf.},
  title        = {Semi-supervised learning for steel surface inspection using magnetic flux leakage signal},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving imbalanced industrial datasets to enhance the
accuracy of mechanical property prediction and process optimization for
strip steel. <em>JIM</em>, <em>36</em>(2), 1003–1020. (<a
href="https://doi.org/10.1007/s10845-023-02275-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of imbalanced regression is widely prevalent in various intelligent manufacturing systems, significantly constraining the industrial application of machine learning models. Existing research has overlooked the impact of redundant data and has lost valuable information within unlabeled data, therefore, the effectiveness of the models is limited. To this end, we propose a novel model framework (sNN-ST, similarity-based nearest neighbor and Self-Training fusion) to address imbalanced regression in industrial big data. This approach comprises two main steps: first, we identify and remove redundant samples by analyzing the redundancy relationships among samples. Then, we perform pseudo-labeling on unlabeled data, selectively incorporating reliable and non-redundant samples into the labeled dataset. We validate the proposed method on two imbalanced regression datasets. Removing redundant data and effectively utilizing unlabeled data optimize the dataset&#39;s distribution and enhance its information entropy. Consequently, the processed dataset significantly improves the overall model performance. We used this model to conduct a Multi-Parameter Global Relative Sensitivity Analysis within a production system. This analysis optimized existing process parameters and improved product quality consistency. This research presents a promising approach to addressing imbalanced regression problems.},
  archive      = {J_JIM},
  author       = {Li, Feifei and He, Anrui and Song, Yong and Shen, Chengzhe and Wang, Fenjia and Yuan, Tieheng and Zhang, Shiwei and Xu, Xiaoqing and Qiang, Yi and Liu, Chao and Liu, Pengfei and Zhao, Qiangguo},
  doi          = {10.1007/s10845-023-02275-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1003-1020},
  shortjournal = {J. Intell. Manuf.},
  title        = {Improving imbalanced industrial datasets to enhance the accuracy of mechanical property prediction and process optimization for strip steel},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A component-based design approach for energy flexibility in
cyber-physical manufacturing systems. <em>JIM</em>, <em>36</em>(2),
975–1001. (<a href="https://doi.org/10.1007/s10845-023-02280-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy flexibility of manufacturing systems helps to meet sustainable manufacturing requirements and is getting significant importance with rising energy prices and noticeable climate changes. Considering the need to proactively enable energy flexibility in modern manufacturing systems, this work presents a component-based design approach that aims to embed energy flexibility in the design of cyber-physical production systems. To this end, energy management using Industry 4.0 technologies (e.g., Internet of Things and Cyber-physical Systems) is compared to the literature on energy flexibility in order to evaluate to what extent the energy flexibility practice takes advantage of Industry 4.0 technologies. Another dimension is the coverage of the life cycle of the manufacturing system which guarantees its sustainable design and the ability to achieve energy flexibility by configuring the energy consumption behaviour. A data-based design approach of the energy-flexible components is proposed in the spirit of the Reference Architectural Model Industrie 4.0 (RAMI 4.0), and then it is exemplified using an electric drive (as a component) in order to show the practical applicability of the approach. The energy consumption behaviour of the component is modelled using machine learning techniques. The digital twin of the studied component is developed using Visual Components virtual engineering environment, then its energy consumption behaviour is included in the model allowing the system integrator/decision-maker to configure the component based on the energy availability/price. Finally, external services in terms of an optimisation module and a deep learning module are connected to the digital twin.},
  archive      = {J_JIM},
  author       = {Assad, Fadi and Rushforth, Emma J. and Harrison, Robert},
  doi          = {10.1007/s10845-023-02280-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {975-1001},
  shortjournal = {J. Intell. Manuf.},
  title        = {A component-based design approach for energy flexibility in cyber-physical manufacturing systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EXplainable artificial intelligence for automatic defect
detection in additively manufactured parts using CT scan analysis.
<em>JIM</em>, <em>36</em>(2), 957–974. (<a
href="https://doi.org/10.1007/s10845-023-02272-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive Manufacturing (AM) and in particular has gained significant attention due to its capability to produce complex geometries using various materials, resulting in cost and mass reduction per part. However, metal AM parts often contain internal defects inherent to the manufacturing process. Non-Destructive Testing (NDT), particularly Computed Tomography (CT), is commonly employed for defect analysis. Today adopted standard inspection techniques are costly and time-consuming, therefore an automatic approach is needed. This paper presents a novel eXplainable Artificial Intelligence (XAI) methodology for defect detection and characterization. To classify pixel data from CT images as pores or inclusions, the proposed method utilizes Support Vector Machine (SVM), a supervised machine learning algorithm, trained with an Area Under the Curve (AUC) of 0.94. Density-Based Spatial Clustering with the Application of Noise (DBSCAN) is subsequently applied to cluster the identified pixels into separate defects, and finally, a convex hull is employed to characterize the identified clusters based on their size and shape. The effectiveness of the methodology is evaluated on Ti6Al4V specimens, comparing the results obtained from manual inspection and the ML-based approach with the guidance of a domain expert. This work establishes a foundation for automated defect detection, highlighting the crucial role of XAI in ensuring trust in NDT, thereby offering new possibilities for the evaluation of AM components.},
  archive      = {J_JIM},
  author       = {Bordekar, Harsh and Cersullo, Nicola and Brysch, Marco and Philipp, Jens and Hühne, Christian},
  doi          = {10.1007/s10845-023-02272-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {957-974},
  shortjournal = {J. Intell. Manuf.},
  title        = {EXplainable artificial intelligence for automatic defect detection in additively manufactured parts using CT scan analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online tool condition monitoring in micromilling using LSTM.
<em>JIM</em>, <em>36</em>(2), 935–955. (<a
href="https://doi.org/10.1007/s10845-023-02273-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality and cost-effective production in micro-milling involves the use of tools of diameter 50–800 $$\mu $$ m, at high rotational speeds, along complex tool paths. These tools are susceptible to high wear and unexpected breakage, and hence a high-precision tool condition monitoring system is required to predict the tool wear states. In this work, we propose a novel approach for high-precision tool condition monitoring in micro-milling using cutting force signals. The method correlates dominant frequency variations with the tool condition along its complete life cycle, considering both straight and circular tool paths to mimic real-life machining scenarios. Therefore, using multiple micro-milling experiments, dominant frequency was characterized using Wavelet transform and Short Time Fourier Transform, and a tool condition prognostic model was developed using LSTM networks. The model accurately predicts force signals with an RMSE less than 0.09, enabling indirect prediction of the tool condition.},
  archive      = {J_JIM},
  author       = {Manwar, Ashish and Varghese, Alwin and Bagri, Sumant and Joshi, Suhas S.},
  doi          = {10.1007/s10845-023-02273-3},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {935-955},
  shortjournal = {J. Intell. Manuf.},
  title        = {Online tool condition monitoring in micromilling using LSTM},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time monitoring and quality assurance for laser-based
directed energy deposition: Integrating co-axial imaging and
self-supervised deep learning framework. <em>JIM</em>, <em>36</em>(2),
909–933. (<a href="https://doi.org/10.1007/s10845-023-02279-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) has emerged as a promising solution for real-time monitoring of the quality of additively manufactured (AM) metallic parts. This study focuses on the Laser-based Directed Energy Deposition (L-DED) process and utilizes embedded vision systems to capture critical melt pool characteristics for continuous monitoring. Two self-learning frameworks based on Convolutional Neural Networks and Transformer architecture are applied to process zone images from different DED process regimes, enabling in-situ monitoring without ground truth information. The evaluation is based on a dataset of process zone images obtained during the deposition of titanium powder (Cp-Ti, grade 1), forming a cube geometry using four laser regimes. By training and evaluating the Deep Learning (DL) algorithms using a co-axially mounted Charged Couple Device (CCD) camera within the process zone, the down-sampled representations of process zone images are effectively used with conventional classifiers for L-DED process monitoring. The high classification accuracies achieved validate the feasibility and efficacy of self-learning strategies in real-time quality assessment of AM. This study highlights the potential of AI-based monitoring systems and self-learning algorithms in quantifying the quality of AM metallic parts during fabrication. The integration of embedded vision systems and self-learning algorithms presents a novel contribution, particularly in the context of the L-DED process. The findings open avenues for further research and development in AM process monitoring, emphasizing the importance of self-supervised in situ monitoring techniques in ensuring part quality during fabrication.},
  archive      = {J_JIM},
  author       = {Pandiyan, Vigneashwara and Cui, Di and Richter, Roland Axel and Parrilli, Annapaola and Leparoux, Marc},
  doi          = {10.1007/s10845-023-02279-x},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {909-933},
  shortjournal = {J. Intell. Manuf.},
  title        = {Real-time monitoring and quality assurance for laser-based directed energy deposition: Integrating co-axial imaging and self-supervised deep learning framework},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Post-processing of powder bed fused stainless steel:
Micro-machining and micro-electrical discharge machining. <em>JIM</em>,
<em>36</em>(2), 897–908. (<a
href="https://doi.org/10.1007/s10845-023-02277-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface quality is often a specific requirement when dealing with Additive Manufacturing and dimensional accuracy, especially during 3D printing of metals. Therefore, it is crucial to evaluate the material removal behavior of Powder Bed Fusion specimens during Micro-Mechanical Machining and Micro-Electrical Discharge Machining procedures to detect their machinability responses. In this paper, micro-machining of 17-4PH stainless steel samples produced by Laser Powder Bed Fusion is reported. Specifically, we performed Micro-Mechanical Machining and Micro-Electrical Discharge Machining operations, analysed the process performances, and compared the machining conditions. Additionally, we investigated the surface roughness and burrs extension as a function of geometrical configuration and process parameters. The outcomes of this work regard the processing conditions and parameters to optimize the machining operations on 3D-printed metal samples. This work allows the identification of specific features resulting from each process in relation to the material removal rate, giving the possibility to compare and evaluate the machining conditions for 3D parts post-processing.},
  archive      = {J_JIM},
  author       = {Abeni, Andrea and Quarto, Mariangela and Ginestra, Paola Serena},
  doi          = {10.1007/s10845-023-02277-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {897-908},
  shortjournal = {J. Intell. Manuf.},
  title        = {Post-processing of powder bed fused stainless steel: Micro-machining and micro-electrical discharge machining},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural-network-based automatic trajectory adaptation for
quality characteristics control in powder compaction. <em>JIM</em>,
<em>36</em>(2), 875–895. (<a
href="https://doi.org/10.1007/s10845-023-02274-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future manufacturing systems will have to become more intelligent to be able to guarantee a constantly high quality of products while simultaneously reducing labor-intensive quality-assurance tasks to address the shortage in workforce. In this work, we study the application of neural networks to the field of powder metallurgy and more specifically the production of green parts as part of a typical sintering process. More specifically, we explore the usage of neural-network-based predictions in closed-loop control. We train neural networks based on a series of produced workpieces, and use these networks in closed-loop production to predict quality characteristics like weight and dimensions of the workpiece in real-time. Based on these predictions an adaptive trajectory planner adjusts then trajectory key points and with this the final piston trajectories to bring and keep quality characteristics of workpieces within tolerance. We finally compare the control performance of this neural network-based approach with a pure sensor-based approach. Results indicate that both approaches are able to bring and keep quality characteristics within their tolerance limits, but that the neural network-based approach outperforms the sensor-based approach in the transient phase, whereas in steady state the neural network needed to be updated from time to time to reach the same high performance as the sensor-based approach. Since updating needs to be performed only from time to time, required expensive sensors can be shared among multiple machines and thus, costs can be reduced. At the same time the superior prediction performance of the neural-network-based approach in transient phases can be exploited to accelerate setting up times for new workpieces. Future work will target the automation of the recording of the training dataset, the exploration of further machine learning methods as well as the integration of additional sensor data to further improve predictions.},
  archive      = {J_JIM},
  author       = {MoradiMaryamnegari, Hoomaan and Hasseni, Seif-El-Islam and Ganthaler, Elias and Villgrattner, Thomas and Peer, Angelika},
  doi          = {10.1007/s10845-023-02274-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {875-895},
  shortjournal = {J. Intell. Manuf.},
  title        = {Neural-network-based automatic trajectory adaptation for quality characteristics control in powder compaction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Study on anti-interference detection of machining surface
defects under the influence of complex environment. <em>JIM</em>,
<em>36</em>(2), 853–874. (<a
href="https://doi.org/10.1007/s10845-023-02276-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When detecting surface defects in a complex industrial cutting environment, the defects are easily polluted and covered by interfering factors (chips or coolant residues). The defect of the surface images with interference factors is a novel problem in the existing studies, and it is also a difficulty in the detection field. Hence, this paper proposes a high-precision anti-interference detection method for surface defects under the influence of complex environment. The detection method provides a new research idea, which is divided into three main processes: interference regions location, interference regions repair, defect detection. The regions affected by interference factors are adaptively located through the proposed Efficient Channel Attention Network (ECANet)-DeeplabV3 + network model. The mean Pixel Accuracy (mPA) and mean Intersection over Union (mIoU) of ECANet-DeeplabV3 + network model for interference factor identification are 98.37% and 95.46%, respectively. The Criminisi algorithm is improved from priority, finding the best matching block, and searching regions. Directional repair based on the improved Criminisi algorithm is performed on the identified interfering regions removing the interfering factors in the image, which is the research core. Then, defect detection is performed on the repaired image using the improved superpixel technology. At the same time, the defect detection results provide a variety of surface defect information for the cutting staff, including defect types, the number of pixels in different defect regions, and the area ratio of different defect regions. This information improves predictive maintenance and surface quality control.},
  archive      = {J_JIM},
  author       = {Chen, Wei and Zou, Bin and Lei, Ting and Zheng, Qinbing and Huang, Chuanzhen and Li, Lei and Liu, Jikai},
  doi          = {10.1007/s10845-023-02276-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {853-874},
  shortjournal = {J. Intell. Manuf.},
  title        = {Study on anti-interference detection of machining surface defects under the influence of complex environment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-planar slicing for filled free-form geometries in
robot-based FDM. <em>JIM</em>, <em>36</em>(2), 833–851. (<a
href="https://doi.org/10.1007/s10845-023-02250-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-axis techniques in Additive Manufacturing (AM) unlock promising features such as supportless fabrication, reduced material consumption, improved surface quality and mechanical properties. Among those techniques, non-planar (NP) slicing promises to be the most suitable approach to fabricate 3D-components with significant curvature such as free-form geometries. Those are characterized by a layer thickness variation (LTV) along the curvature, which should be minimized. Industrial 6-axis robots are mandatory to achieve such performances. NP slicing generalization is challenging. On one side, there is a need to define a suitable contouring method compatible with the different geometrical features present in objects. On the other side, the generalized slicing method must be able to reconstruct the inner side not provided by superficial information provided by triangular mesh. In this work, a new algorithm to generate filled NP layer has been proposed using a contouring method that reduces the LTV. The bidirectional rectilinear infill strategy has been adapted for NP layers providing a more feasible toolpath to Fused Deposition Modeling (FDM) and such Direct processes where curved paths are detrimental. The proposed strategy has been validated by fabricating a tubular geometry with a robotized FDM system. Tubular geometry provides a sub-optimal solution of LTV known analytically for the contour. The infill algorithm has been tested with a complex surface applying the NP torus on a waved shape. Previous studies consider only contour providing an LTV ranging in +0% $$\div $$ -46%. This study considers only the inner side. The analytical LTV resulted in a range of +0% $$\div $$ -60%. The cross sections of the components were analyzed and compared with the analytical results. Although the proposed infill strategy does not maintain completely the contour layer thickness in the infill side, it shows to be able to cover more complex NP layers without saddle points.},
  archive      = {J_JIM},
  author       = {Insero, Federico and Furlan, Valentina and Giberti, Hermes},
  doi          = {10.1007/s10845-023-02250-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {833-851},
  shortjournal = {J. Intell. Manuf.},
  title        = {Non-planar slicing for filled free-form geometries in robot-based FDM},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin for product versus project lifecycles’
development in manufacturing and construction industries. <em>JIM</em>,
<em>36</em>(2), 801–831. (<a
href="https://doi.org/10.1007/s10845-023-02301-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twin, as an important enabling tool for digital transformation, has received increasing attention from researchers and practitioners since its definition was formalised. Especially in the global context and exacerbated by Covid-19, the applications of the digital twin have offered opportunities for many industries. While the digital twin has already been widely used in many sectors such as manufacturing and the construction industry—one of the key engines of economic development, is still lagging behind many other sectors. This study uses the systematic literature review to assess the applications of digital twin in manufacturing and construction respectively, the benefits it brings, and the impediments to its application. Based on this, a comparison is made of digital twin applications in the manufacturing and construction industries to draw lessons. This study concluded that although the use of digital twin in manufacturing is better than construction overall, it is still not reaching its full potential. Despite many benefits brought by the digital twin to construction during the project lifecycle, the construction sector faces even greater challenges than manufacturing in digital twin adoption. By comparison, this study drew five lessons to drive better adoption of the digital twin. The construction industry needs to accelerate the deployment of relevant hardware, promote the standard unification of digital twin, explore the whole lifecycle application of the digital twin, enhance data protection, and embrace changes. This study was limited in the scope of data collection. Future research could focus on gathering information from specific case studies, to produce more comprehensive perspectives.},
  archive      = {J_JIM},
  author       = {Abanda, F. H. and Jian, N. and Adukpo, S. and Tuhaise, V. V. and Manjia, M. B.},
  doi          = {10.1007/s10845-023-02301-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {801-831},
  shortjournal = {J. Intell. Manuf.},
  title        = {Digital twin for product versus project lifecycles’ development in manufacturing and construction industries},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic comparison of software agents and digital twins:
Differences, similarities, and synergies in industrial production.
<em>JIM</em>, <em>36</em>(2), 765–800. (<a
href="https://doi.org/10.1007/s10845-023-02278-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve a highly agile and flexible production, a transformational shift is envisioned whereby industrial production systems evolve to be more decentralized, interconnected, and intelligent. Within this vision, production assets collaborate with each other, exhibiting a high degree of autonomy. Furthermore, information about individual production assets is accessible throughout their entire life-cycles. To realize this vision, the use of advanced information technology is required. Two commonly applied software paradigms in this context are Software Agents (referred to as Agents) and Digital Twins (DTs). This work presents a systematic comparison of Agents and DTs in industrial applications. The goal of the study is to determine the differences, similarities, and potential synergies between the two paradigms. The comparison is based on the purposes for which Agents and DTs are applied, the properties and capabilities exhibited by these software paradigms, and how they can be allocated within the Reference Architecture Model Industry 4.0. The comparison reveals that Agents are commonly employed in the collaborative planning and execution of production processes, while DTs are generally more applied to monitor production resources and process information. Although these observations imply characteristic sets of capabilities and properties for both Agents and DTs, a clear and definitive distinction between the two paradigms cannot be made. Instead, the analysis indicates that production assets utilizing a combination of Agents and DTs would demonstrate high degrees of intelligence, autonomy, sociability, and fidelity. To achieve this, further standardization is required, particularly in the field of DTs.},
  archive      = {J_JIM},
  author       = {Reinpold, Lasse M. and Wagner, Lukas P. and Gehlhoff, Felix and Ramonat, Malte and Kilthau, Maximilian and Gill, Milapji S. and Reif, Jonathan T. and Henkel, Vincent and Scholz, Lena and Fay, Alexander},
  doi          = {10.1007/s10845-023-02278-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {765-800},
  shortjournal = {J. Intell. Manuf.},
  title        = {Systematic comparison of software agents and digital twins: Differences, similarities, and synergies in industrial production},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of cutting force via machine learning: State of
the art, challenges and potentials. <em>JIM</em>, <em>36</em>(2),
703–764. (<a href="https://doi.org/10.1007/s10845-023-02260-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cutting force is a critical factor that reflects the machining states and affects tool wear, cutting stability, and the quality of the machined surface. Accurate prediction of cutting force has been the subject of extensive research in machining technology for decades. Generally, the predicting methods are based on the physical principles of metal cutting processes and they can be divided into two main categories: calculation of cutting forces by using analytical models and numerical simulation of cutting forces with finite element analysis. With the advance of artificial intelligence and machine learning (ML), various algorithms have been developed to predict cutting force with high accuracy and high efficiency. This paper provides a comprehensive review of force prediction methods, with a focus on ML-based algorithms. The mechanisms and characteristics of various force prediction methods, such as analytical models and finite element analysis, as well as different ML-based algorithms, are introduced in detail. The challenges of current algorithms and their potential in long-term and real-time prediction are discussed. The review highlights the potential of ML-based algorithms in improving the accuracy and efficiency of cutting force prediction and emphasizes the need for further research to address the current challenges and advance the field of force prediction in metal-cutting processes.},
  archive      = {J_JIM},
  author       = {Liu, Meng and Xie, Hui and Pan, Wencheng and Ding, Songlin and Li, Guangxian},
  doi          = {10.1007/s10845-023-02260-8},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {703-764},
  shortjournal = {J. Intell. Manuf.},
  title        = {Prediction of cutting force via machine learning: State of the art, challenges and potentials},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jmiv---9">JMIV - 9</h2>
<ul>
<li><details>
<summary>
(2025). Curvature-guided color image restoration by saturation-value
total variation. <em>JMIV</em>, <em>67</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10851-024-01218-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel curvature-guided saturation-value total variation model for color image restoration. Specifically, we incorporate the curvature prior into the traditional variational model to guide the evolution in the direction that maintains the curvature information. Theoretically, we investigate the properties of the proposed model and give a detailed discussion based on the mathematical foundation about the existence of the solution. Numerically, we formulate an effective and efficient algorithm to solve the proposed minimization problem based on the framework of alternating direction method of multipliers. Numerical examples are presented to demonstrate that the performance of the proposed model is better than that of other testing methods for several testing color images.},
  archive      = {J_JMIV},
  author       = {Wang, Wei and Wang, Jingjie and Ng, Michael K.},
  doi          = {10.1007/s10851-024-01218-6},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Curvature-guided color image restoration by saturation-value total variation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the axial symmetry of 2D star-shaped sets. <em>JMIV</em>,
<em>67</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10851-024-01222-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential aspect of the study of shapes is the symmetry because of its importance from a theoretical point of view and its applicability in multiple real-life problems. In this manuscript, the axial symmetry of 2D star-shaped sets is analyzed. For such a purpose, different measures of axial symmetry of a star-shaped set are proposed and the concept of a best symmetry axis is also introduced. By means of them, families of symmetry measures for star-shaped sets quantifying the degree of symmetry of a set of that class are introduced. All of them are discussed in detail, providing their main properties and the existence of at least a best axis of symmetry, which could be not unique, for any star-shaped set. Some examples illustrate the concepts and results of the manuscript.},
  archive      = {J_JMIV},
  author       = {López-Díaz, María Concepción and López-Díaz, Miguel},
  doi          = {10.1007/s10851-024-01222-w},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {On the axial symmetry of 2D star-shaped sets},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A structured l-BFGS method with diagonal scaling and its
application to image registration. <em>JMIV</em>, <em>67</em>(1), 1–20.
(<a href="https://doi.org/10.1007/s10851-024-01215-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise an L-BFGS method for optimization problems in which the objective is the sum of two functions, where the Hessian of the first function is computationally unavailable while the Hessian of the second function has a computationally available approximation that allows for cheap matrix–vector products. This is a prototypical setting for many inverse problems. The proposed L-BFGS method exploits the structure of the objective to construct a more accurate Hessian approximation than in standard L-BFGS. In contrast with existing works on structured L-BFGS, we choose the first part of the seed matrix, which approximates the Hessian of the first function, as a diagonal matrix rather than a multiple of the identity. We derive two suitable formulas for the coefficients of the diagonal matrix and show that this boosts performance on real-life image registration problems, which are highly non-convex inverse problems. The new method converges globally and linearly on non-convex problems under mild assumptions in a general Hilbert space setting, making it applicable to a broad class of inverse problems. An implementation of the method is freely available.},
  archive      = {J_JMIV},
  author       = {Mannel, Florian and Aggrawal, Hari Om},
  doi          = {10.1007/s10851-024-01215-9},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {A structured L-BFGS method with diagonal scaling and its application to image registration},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cartoon–texture image decomposition using least squares and
low-rank regularization. <em>JMIV</em>, <em>67</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10851-024-01216-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel model for the decomposition of cartoon–texture images, which integrates the edge-aware weighted least squares (WLS) with low-rank regularization. Unlike conventional methodologies that depend on total variation-based penalty functions, our model represents cartoon images using an edge-preserving WLS penalty. This approach effectively enhances edges and suppresses texture through iterative updates of an edge-preserving weight matrix. For the texture component, we introduce a low-rank penalty function to capture the structured regularity of texture patterns. By leveraging the repetitive nature of texture, our low-rank models can accurately represent these components. We employ a prediction–correction approach based on a three-block separable alternating direction multiplier method to solve the minimization problem, providing closed-form solutions for all subproblems. We also provide a convergence proof for the proposed algorithm. Numerical experiments validate the efficacy of our proposed method in successfully separating cartoon and texture components while preserving edges.},
  archive      = {J_JMIV},
  author       = {Li, Kexin and Wen, You-wei and Chan, Raymond H.},
  doi          = {10.1007/s10851-024-01216-8},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Cartoon–Texture image decomposition using least squares and low-rank regularization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy circularity: A new fuzzy shape-based descriptor of the
object. <em>JMIV</em>, <em>67</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10851-024-01217-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new family of fuzzy shape measures, called fuzzy circularity, to evaluate the degree to which a considered fuzzy shape matches a fuzzy disk. A new family of fuzzy shape-based measures ranges the interval (0, 1] where a maximum value equal to 1 is reached if and only if the shape under consideration is a fuzzy disk. This family is theoretically well grounded having the behavior that corresponds to human perception and can be predicted in advance. Additionally, a new family of fuzzy shape-based measures is invariant to rotation, translation, and scaling of the considered fuzzy shape. Various experiments on both synthetically generated and real images are included to provide a better understanding of the behavior of the new measures and to confirm the theoretically proven results. The performance of the new family of fuzzy circularity is extensively tested on several standard, well-known image datasets such as MPEG-7 CE-1, Animal, Swedish Leaf, and Galaxy Zoo datasets. Experimental evaluations also illustrate the effectiveness and advantages of the new shape descriptors in various object classification and recognition tasks by comparing them with other known analysis approaches.},
  archive      = {J_JMIV},
  author       = {Ilić, Vladimir and Ralević, Nebojša M.},
  doi          = {10.1007/s10851-024-01217-7},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Fuzzy circularity: A new fuzzy shape-based descriptor of the object},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind image deconvolution: When patch-wise minimal pixels
prior meets fractional-order method. <em>JMIV</em>, <em>67</em>(1),
1–20. (<a href="https://doi.org/10.1007/s10851-024-01221-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deconvolution is a challenging issue in image processing. In blind image deconvolution, the typical approach involves iteratively estimating both the blur kernel and latent image until convergence to the blur kernel of the observed image is achieved. Recently, several approaches have been attempted to develop a sophisticated regularization to obtain the clean image. However, existing methods often struggle to effectively handle ringing artifacts and local blur. To overcome these limitations, we introduce a fractional-order variational model. This model alleviates the ringing artifacts through the selection of an optimal derivative. Subsequently, to refine the latent image further, we leverage the local prior, namely patch-wise minimal pixels (PMP) prior. Since the PMP prior of clean images blocks is much sparser than that of blurred ones, it is capable of discriminating between clean and blurred image blocks. We illustrate the effective integration of the fractional-order operations and the PMP prior within our proposed approach. Moreover, the convergence of our algorithm has been proved as the values of the objective function monotonically decrease. Extensive experiments on different datasets demonstrate the superiority of the proposed method compared with other methods in terms of reconstruction quality for blind deconvolution.},
  archive      = {J_JMIV},
  author       = {Wu, Tingting and Wan, Shaojie and Feng, Chenchen and Zhang, Hao and Zeng, Tieyong},
  doi          = {10.1007/s10851-024-01221-x},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Blind image deconvolution: When patch-wise minimal pixels prior meets fractional-order method},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anisotropic diffusion in riemannian colour geometry.
<em>JMIV</em>, <em>67</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s10851-024-01223-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anisotropic diffusion has long been an important tool in image processing. More recently, it has also found its way to colour imaging. Until now, mainly Euclidean colour spaces have been considered in this context, but recent years have seen a renewed interest in and importance of non-Euclidean colour geometry. The main contribution of this paper is the derivation of the equations for anisotropic diffusion in Riemannian colour geometry. It is demonstrated that it contains several well-known solutions such as Perona–Malik diffusion and Tschumperlé–Deriche diffusion as special cases. Furthermore, it is shown how it is non-trivially connected to Sochen’s general framework for low-level vision. The main significance of the method is that it decouples the coordinates used for solving the diffusion equation from the ones that define the metric of the colour manifold, and thus directs the magnitude and direction of the diffusion through the diffusion tensor. It also enables the use of non-Euclidean colour manifolds and metrics for applications such as denoising, inpainting, and demosaicing, based on anisotropic diffusion.},
  archive      = {J_JMIV},
  author       = {Farup, Ivar and Rivertz, Hans Jakob},
  doi          = {10.1007/s10851-024-01223-9},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Anisotropic diffusion in riemannian colour geometry},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New properties for full convex sets and full convex hulls.
<em>JMIV</em>, <em>67</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10851-024-01225-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Full convexity has been recently proposed as an alternative definition of digital convexity in $$\mathbb {Z}^n$$ . In contrast to classical definitions, fully convex sets are always connected and even simply connected whatever the dimension, while remaining digitally convex in the usual sense. In order to better understand the properties of full convexity, we present here two new and radically different characterizations of full convexity. The first one mimics the usual continuous convexity via segments inclusion. We show an equivalence of full convexity with this segment convexity in dimensions 1 and 2, and counterexamples starting from dimension 3. If we now ask that the cells touched by all d-simplices (instead of just 2-simplices aka segments) are within the cells touched by the digital set, we achieve an equivalence in arbitrary dimension d. The second characterization is recursive with respect to the dimension and relies on convexity of its axis-aligned projections. We provide several applications of these characterizations: the full convexity of balls and subsets of the hypercube, a natural measure of full convexity for digital sets, a new and faster algorithm to check the full convexity of digital sets. Finally, we study the main drawback of full convexity: Its envelope operator may not be an increasing operator. We characterize fully convex sets that have anti-monotonous subsets, and we show that they must be thin in a precise sense.},
  archive      = {J_JMIV},
  author       = {Feschet, Fabien and Lachaud, Jacques-Olivier},
  doi          = {10.1007/s10851-024-01225-7},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {New properties for full convex sets and full convex hulls},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new insight into the epipole from four point
correspondences in two calibrated views. <em>JMIV</em>, <em>67</em>(1),
1–12. (<a href="https://doi.org/10.1007/s10851-024-01227-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new insight into the epipole from four given image point correspondences in two calibrated views. Firstly, we propose an algebraic constraint on the relation between the epipole and a plane-induced homography. Secondly, we show a novel algorithm for determining the 10-degree curve about possible epipoles from four image point correspondences in two calibrated views by using algebraic method directly. In particular, we show that the problem of determining the epipole has at most four distinct real solutions when the left image plane is parallel to a certain face of the tetrahedron consisting of four control points. Furthermore, we also confirm that the upper bound of four distinct physically valid solutions is attainable. Lastly, we give some examples to validate our results.},
  archive      = {J_JMIV},
  author       = {Guo, Yang and Qu, Yingqi},
  doi          = {10.1007/s10851-024-01227-5},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {A new insight into the epipole from four point correspondences in two calibrated views},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="joh---17">JOH - 17</h2>
<ul>
<li><details>
<summary>
(2025). Lagrangian-based heuristics for production planning with
perishable products, scarce resources, and sequence-dependent setup
times. <em>JOH</em>, <em>31</em>(1), 1–35. (<a
href="https://doi.org/10.1007/s10732-024-09539-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a lot-sizing and scheduling problem apparent in the food industry that stemmed originally from the Brazilian meat production sector. More specifically, we consider a production environment in which various production lines share a set of scarce production resources. Therefore, only a subset of the existing production lines can simultaneously operate in each period under the limitations of the availability of resources. Moreover, we consider sequence-dependent setup times and costs, significant inventory holding costs, backlogging, and perishable products. The problem is formulated as a mixed integer programming model, and we propose four Lagrangian-based heuristics to find high-quality solutions for challenging instances. A computational study shows that proposed approaches are very competitive in solving the problem, outperforming methods already established in the literature.},
  archive      = {J_JOH},
  author       = {Soler, Willy A. Oliveira and Santos, Maristela O. and Akartunalı, Kerem},
  doi          = {10.1007/s10732-024-09539-w},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-35},
  shortjournal = {J. Heuristics},
  title        = {Lagrangian-based heuristics for production planning with perishable products, scarce resources, and sequence-dependent setup times},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybridizing constraint programming and meta-heuristics for
multi-mode resource-constrained multiple projects scheduling problem.
<em>JOH</em>, <em>31</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10732-024-09540-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multi-Mode Resource-Constrained Multiple Projects Scheduling Problem (MMRCMPSP) is an important combinatorial optimization problem for both real-world situations in industry and academic research. Its objective is to find the best schedule for activities across multiple projects that can be executed in different modes. The schedule must consider shared resource availability and satisfy precedence and time constraints. To tackle this problem, we propose a hybrid approach that combines constraint programming (CP) with meta-heuristic algorithms. We introduce and assess a CP model that incorporates all MMRCMPSP constraints. By leveraging the strengths of CP and meta-heuristics, our approach yields new upper bounds for various MMRCMPSP benchmark instances. Additionally, we evaluate our method using existing benchmark instances for single-project scheduling problems with multiple modes and provide improved solutions for many of them.},
  archive      = {J_JOH},
  author       = {Ahmeti, Arben and Musliu, Nysret},
  doi          = {10.1007/s10732-024-09540-3},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-37},
  shortjournal = {J. Heuristics},
  title        = {Hybridizing constraint programming and meta-heuristics for multi-mode resource-constrained multiple projects scheduling problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-echelon van-robot routing problem with sharing-curbside
satellites. <em>JOH</em>, <em>31</em>(1), 1–35. (<a
href="https://doi.org/10.1007/s10732-024-09541-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High population density and commercial-activity density in urban areas make land use for urban logistics systems even more challenging. Herein, a concept named sharing-curbside satellite (SS) is involved in two-echelon city logistics systems. Traditional vans are deployed in the 1st-echelon network, whereas ground-based robots are employed in the 2nd-echelon network. As a type of nondedicated satellites, the SS shares curbside spaces with the local traffic flow, and each SS can have multiple time windows for direct transshipment between vans and robots. The SS can provide a new mode for urban deliveries through temporary and nondedicated satellites at the neighborhood level. In this study, the two-echelon van-robot routing problem with SSs (2ERP-SS) is defined. The SS synchronization involves vans being used as part of SSs, each SS has multiple time windows, cargoes are transshipped directly between vans and robots, and the available transshipment capacity decreases over time. We develop a mixed-integer linear programming model. We provide a large neighborhood search (LNS) combined with a beam search algorithm, and employ an adaptive LNS (ALNS) for comparison. The effectiveness of the mathematical formulation and heuristics are evaluated through computational experiments, and practical management insights are elucidated.},
  archive      = {J_JOH},
  author       = {Li, Hongqi and Wang, Feilong and Xiong, Hanxi and Wang, Zhiqi},
  doi          = {10.1007/s10732-024-09541-2},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-35},
  shortjournal = {J. Heuristics},
  title        = {Two-echelon van-robot routing problem with sharing-curbside satellites},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptative multi-objective scatter search for solving the
dynamic bin packing problem. <em>JOH</em>, <em>31</em>(1), 1–69. (<a
href="https://doi.org/10.1007/s10732-024-09537-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the dynamic multi-objective bin packing problem, a combinatorial optimization problem within the cutting/packing problems family. A solution to this problem involves packing cooling cookies into boxes while following a specific production process. In such a case, items, each with its own capacity, arrive in batches at racks. The objective is to optimize (i) the number of boxes used, (ii) the average initial heat associated with each box, and (iii) the maximum time taken to move all boxes to the storefront. This problem is addressed through an adaptive multi-objective scatter search applied to a special dynamic bin packing problem. The method begins by constructing an initial Pareto front set, specifically, the initial reference set containing diversified solutions based on the three objective functions related to the studied problem. The subsequent stages of the method operate in three collaborative phases: (i) improving the reference set by optimizing a highlighted objective function, (ii) refining the final reference set by prioritizing the optimization of average initial heat, and (iii) addressing the objective related to the maximum time to establish the final solution. Hence, to facilitate the transition between the aforementioned three phases, a hybridization between the solution combination method and the reference update method is introduced. Finally, the proposed method’s performance is evaluated using benchmark and newly generated instances and compared with recent methods. The study includes both qualitative and quantitative analyses. To identify the best-performing method among those tested, three statistical tests are conducted: the Student’s t-test, the Sign test, and the Wilcoxon signed-rank test. Additionally, performance metrics such as the $$\varepsilon $$ -test, the binary coverage measure, and the net front contribution indicator are used for evaluation. The results achieved by the proposed method surpass those published in the literature, highlighting the method’s strength and effectiveness, and establishing it as a competitive solution for the dynamic multi-objective bin-packing problem.},
  archive      = {J_JOH},
  author       = {Aïder, Méziane and Boulebene, Sabrin and Hifi, Mhand},
  doi          = {10.1007/s10732-024-09537-y},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-69},
  shortjournal = {J. Heuristics},
  title        = {An adaptative multi-objective scatter search for solving the dynamic bin packing problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient backtracking heuristic for the resource
allocation problem with compatibility and exclusivity constraints.
<em>JOH</em>, <em>31</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s10732-024-09538-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a resource allocation problem featuring specific constraints, called exclusivity constraints, in addition to regular compatibility constraints. Resources are to be allocated to independent modules, each having a list of compatible resources. A resource can be allocated to several modules. However, some modules exhibit exclusivity constraints, requiring each of them to be allocated to one dedicated compatible resource, not shared with any other module. Such a resource allocation problem arises in the deployment of simulation modules on computational resources in a distributed simulation platform, where the simulation requester may require some modules to be allocated to dedicated resources for a better soft real-time execution, or for instrumentation purposes. In this paper, we introduce the problem of resource allocation with compatibility and exclusivity constraints and show it reduces to the list-coloring problem in a threshold graph. We deduce that our problem is NP-complete in the general case, while it can be solved in polynomial time, in two special cases. We propose a heuristic backtracking algorithm enhanced by pruning rules and exploiting the subproblems’ special structure. Compared to four list coloring heuristics adapted to our problem, our heuristic algorithm can be considered as the method of choice to find high-quality solutions in short computing times.},
  archive      = {J_JOH},
  author       = {Khassiba, Ahmed},
  doi          = {10.1007/s10732-024-09538-x},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {J. Heuristics},
  title        = {An efficient backtracking heuristic for the resource allocation problem with compatibility and exclusivity constraints},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 30 years of the journal of heuristics: A bibliometric
analysis. <em>JOH</em>, <em>31</em>(1), 1–55. (<a
href="https://doi.org/10.1007/s10732-024-09542-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Journal of Heuristics is an international journal that provides a forum to solve complex problems using heuristic solution tools. The journal was created in 1995, and in 2025, celebrates its 30th anniversary. Motivated by this special event, this article presents a bibliometric analysis of the journal. The article examines the journal publication and citation structure using the Scopus database, considering a wide range of issues including the most cited documents, productive authors, institutions, and countries. The work also develops a graphical visualization of the bibliographic data by using the VOS viewer software. This approach is studied with different bibliometric measures such as bibliographic coupling, co-citation, and co-occurrence of keywords. The results show that the Journal of Heuristics has maintained a solid quality of its publications over the years. Currently, the most popular topics are connected to heuristics, metaheuristics, local search, and tabu search. Researchers from the USA are the most productive. But countries such as France, the UK, Spain and Canada, have also a significant productivity in the journal.},
  archive      = {J_JOH},
  author       = {Flores-Sosa, Martha and Merigó, José M. and Sanchez-Valenzuela, Kenia},
  doi          = {10.1007/s10732-024-09542-1},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-55},
  shortjournal = {J. Heuristics},
  title        = {30 years of the journal of heuristics: A bibliometric analysis},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A memetic algorithm for the flexible periodic vehicle
routing problem. <em>JOH</em>, <em>31</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10732-024-09536-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flexible periodic vehicle routing problem (FPVRP) is an extension of the classic VRP in which customers are visited periodically in the course of a given time horizon. Periodicity and flexibility in providing services are two main features of the FPVRP, which introduce new challenges in solving this problem. To the best of our knowledge, there is only one single-solution-based algorithm in the literature for solving the FPVRP. In this paper, another technique called the MA-FPVRP is proposed for tackling this problem. Our population-based approach is a memetic algorithm made up of two main components: a genetic procedure that aims to find a suitable sequence of visits for each route, and a local search procedure consisting of four moves whose prime goal is to guide the search toward the promising areas. Considering 45 standard benchmark instances with unknown optimal solutions, the MA-FPVRP outperforms the preceding method in terms of both the solution quality and execution time. Using this evolutionary scheme, the average relative percent deviation (RPD) to the best-known solution values decreases from 0.52% to 0.15%, and the average execution time improves by approximately 31%. For all 10 large instances of the FPVRP, new best results are found using our algorithm. Considering the remaining 35 small and medium-size instances, our approach is superior to the previous method in terms of the solution quality, and it is more than two times faster. Besides, using the MA-FPVRP, new best solutions are obtained for 5 out of these 35 instances.},
  archive      = {J_JOH},
  author       = {Amiri, Banafsheh and Ziarati, Koorush and Sohrabi, Somayeh},
  doi          = {10.1007/s10732-024-09536-z},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Heuristics},
  title        = {A memetic algorithm for the flexible periodic vehicle routing problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming last mile delivery with heterogeneous
assistants: Drones and delivery robots. <em>JOH</em>, <em>31</em>(1),
1–42. (<a href="https://doi.org/10.1007/s10732-024-09543-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid global expansion of e-commerce and the increasing number of online shoppers, logistics service providers (LSPs) are exploring sustainable solutions to meet the rising demand. Thanks to developments in automation and robotic technologies, LSPs have now the opportunity to enhance their operations through the deployment of autonomous delivery solutions like drones and delivery robots. This paper investigates a practical delivery system to integrate these emerging technologies simultaneously into conventional van-only delivery system. Additionally, the effects of various assistant characteristics on operations are examined through broader assumptions. We introduce a mathematical model aiming to minimize delivery makespan and explore various valid inequalities to mitigate its complexity. A new hybrid metaheuristic algorithm combining genetic algorithm and large neighborhood search algorithm is also proposed for large scale instances. A three-layer coding and encoding method is also introduced for genetic algorithm to manage the complex structure of the problem. Finally, extensive numerical experiments are conducted to show the effectiveness of valid inequalities and the algorithm. The sensitivity analyses provide comparisons of various delivery configurations and offer valuable insights for the logistics industry to integrate these innovative delivery solutions into their daily operations. In our experiments, using a single drone reduces total delivery times by up to 23.57%, while a single robot contributes to a 37.19% improvement in the objective. The heterogeneous configuration offers a substantial 49.71% improvement compared to using only vans for deliveries.},
  archive      = {J_JOH},
  author       = {Chen, Cheng and Demir, Emrah and Hu, Xisheng and Huang, Hainan},
  doi          = {10.1007/s10732-024-09543-0},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-42},
  shortjournal = {J. Heuristics},
  title        = {Transforming last mile delivery with heterogeneous assistants: Drones and delivery robots},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary optimization of the area under precision-recall
curve for classifying imbalanced multi-class data. <em>JOH</em>,
<em>31</em>(1), 1–66. (<a
href="https://doi.org/10.1007/s10732-024-09544-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of imbalanced multi-class data is still so far one of the most challenging issues in machine learning and data mining. This task becomes more serious when classes containing fewer instances are located in overlapping regions. Several approaches have been proposed through the literature to deal with these two issues such as the use of decomposition, the design of ensembles, the employment of misclassification costs, and the development of ad-hoc strategies. Despite these efforts, the number of existing works dealing with the imbalance in multi-class data is much reduced compared to the case of binary classification. Moreover, existing approaches still suffer from many limits. These limitations include difficulties in handling imbalances across multiple classes, challenges in adapting sampling techniques, limitations of certain classifiers, the need for specialized evaluation metrics, the complexity of data representation, and increased computational costs. Motivated by these observations, we propose a multi-objective evolutionary induction approach that evolves a population of NLM-DTs (Non-Linear Multivariate Decision Trees) using the $$\theta $$ -NSGA-III ( $$\theta $$ -Non-dominated Sorting Genetic Algorithm-III) as a search engine. The resulting algorithm is termed EMO-NLM-DT (Evolutionary Multi-objective Optimization of NLM-DTs) and is designed to optimize the construction of NLM-DTs for imbalanced multi-class data classification by simultaneously maximizing both the Macro-Average-Precision and the Macro-Average-Recall as two possibly conflicting objectives. The choice of these two measures as objective functions is motivated by a recent study on the appropriateness of performance metrics for imbalanced data classification, which suggests that the mAURPC (mean Area Under Recall Precision Curve) satisfies all necessary conditions for imbalanced multi-class classification. Moreover, the NLM-DT adoption as a baseline classifier to be optimized allows the generation non-linear hyperplanes that are well-adapted to the classes ‘boundaries’ geometrical shapes. The statistical analysis of the comparative experimental results on more than twenty imbalanced multi-class data sets reveals the outperformance of EMO-NLM-DT in building NLM-DTs that are highly effective in classifying imbalanced multi-class data compared to seven relevant and recent state-of-the-art methods.},
  archive      = {J_JOH},
  author       = {Chabbouh, Marwa and Bechikh, Slim and Mezura-Montes, Efrén and Ben Said, Lamjed},
  doi          = {10.1007/s10732-024-09544-z},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-66},
  shortjournal = {J. Heuristics},
  title        = {Evolutionary optimization of the area under precision-recall curve for classifying imbalanced multi-class data},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A q-learning-based algorithm for the block relocation
problem. <em>JOH</em>, <em>31</em>(1), 1–41. (<a
href="https://doi.org/10.1007/s10732-024-09545-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Block Relocation Problem (BRP), also known as the Container Relocation Problem, is a challenging combinatorial optimization problem in block stacking systems and has many applications in real-world scenarios such as logistics and manufacturing industry. The BRP is about finding the optimal way to retrieve blocks from a storage area with the objective of minimizing the number of relocations. The BRPs have been studied for a long time, and have been solved primarily using conventional optimization techniques, including mathematical programming models, as well as both exact and heuristic algorithms. For the first time, this paper tackles the problem using a reinforcement learning method. We focus on one of the major variants of the BRP—the restricted BRP with duplicate priorities (RBRP-dup). We first model the RBRP-dup as a Markov decision process and then propose a Q-learning-based algorithm to solve the problem. The Q-learning-based algorithm contains two phases. In the learning phase, two innovative mechanisms: an optimal rule-integrated behaviour policy and a heuristic-based dynamic initialization method, are incorporated into the Q-learning model to reduce the size of the state-action space and accelerate convergence. In the optimization phase, the insights obtained in the learning phase are combined with a heuristic algorithm to improve decision-making. The performance of our proposed method is evaluated against the state-of-the-art exact algorithm and a commonly used heuristic algorithm based on benchmark instances from the literature. The computational experiments demonstrate the superiority of our proposed method regarding solution quality in large and complex instances.},
  archive      = {J_JOH},
  author       = {Liu, Liqun and Feng, Yuanjun and Zeng, Qingcheng and Chen, Zhijun and Li, Yaqiu},
  doi          = {10.1007/s10732-024-09545-y},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-41},
  shortjournal = {J. Heuristics},
  title        = {A Q-learning-based algorithm for the block relocation problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A heuristic algorithm to improving the coil slitting process
in the steel industry. <em>JOH</em>, <em>31</em>(1), 1–38. (<a
href="https://doi.org/10.1007/s10732-024-09546-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The steel industry is constantly facing problems and challenges that require optimisation to improve the production process. We present an algorithm to address a major challenge, the slitting problem, for a specific Spanish company. This problem arises when large steel coils need to be cut into smaller strips. Given the highly heterogeneous stock (coils come from previous operations), selecting the most suitable coils and defining the cutting patterns become very complicated due to operational and customer constraints. The company aims to reduce the leftovers and increase the service level (the difference between the weight requested by the customer and the weight supplied). The algorithm is currently in production and was validated using the company’s data and compared with an exact model. Results significantly improved the company’s operations, achieving a 50% reduction in leftovers and a much better service level in minutes, as opposed to the hours the company previously required. Although there are Mixed Integer Linear Optimization models that provide an optimal solution in small cases, they are not a viable alternative for the company because they require excessive computational time (even, in some cases, to obtain feasible solutions) and use overly expensive commercial solvers.},
  archive      = {J_JOH},
  author       = {Soto-Sánchez, Óscar and Sierra-Paradinas, María and Gallego, Micael and Alonso-Ayuso, Antonio and Gortázar, Francisco},
  doi          = {10.1007/s10732-024-09546-x},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-38},
  shortjournal = {J. Heuristics},
  title        = {A heuristic algorithm to improving the coil slitting process in the steel industry},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A local branching-based solution for the multi-period
cutting stock problem with tardiness, earliness, and setup costs.
<em>JOH</em>, <em>31</em>(1), 1–57. (<a
href="https://doi.org/10.1007/s10732-025-09547-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the Multi-Period Cutting Stock Problem with Due Dates and Setups (MPCSPDDS), an extension of the classical one-dimensional Cutting Stock Problem (CSP). The MPCSPDDS considers the due dates specified in cutting orders’ requests and setups required for transitioning between different cutting patterns. The challenge lies in minimizing tardiness and earliness during production, considering these as detrimental factors. Additionally, the proposed model assumes that a setup is necessary for the cutting machine when switching patterns. The contribution of this paper includes the proposition of an integer mathematical programming model and a matheuristic solution approach for two variants of the MPCSPDDS, employing column generation, a round-up heuristic, and the local branching matheuristic. Computational experiments show that our proposed solution method consistently yields, on average, high-quality feasible solutions compared to employing column generation and solving the problem with the generated columns using the CPLEX solver while maintaining a low computational cost.},
  archive      = {J_JOH},
  author       = {de Araújo Silva Oliveira, Elisama and Wanner, Elizabeth and de Sá, Elisangela Martins and de Souza, Sérgio Ricardo},
  doi          = {10.1007/s10732-025-09547-4},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-57},
  shortjournal = {J. Heuristics},
  title        = {A local branching-based solution for the multi-period cutting stock problem with tardiness, earliness, and setup costs},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heuristics for the run-length encoded burrows–wheeler
transform alphabet ordering problem. <em>JOH</em>, <em>31</em>(1), 1–29.
(<a href="https://doi.org/10.1007/s10732-025-09548-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Burrows–Wheeler Transform (BWT) is a string transformation technique widely used in areas such as bioinformatics and file compression. Many applications combine a run-length encoding (RLE) with the BWT in a way which preserves the ability to query the compressed data efficiently. However, these methods may not take full advantage of the compressibility of the BWT as they do not modify the alphabet ordering for the sorting step embedded in computing the BWT. Indeed, any such alteration of the alphabet ordering can have a considerable impact on the output of the BWT, in particular on the number of runs. For an alphabet $$\Sigma $$ containing $$\sigma $$ characters, the space of all alphabet orderings is of size $$\sigma !$$ . While for small alphabets an exhaustive investigation is possible, finding the optimal ordering for larger alphabets is not feasible. Therefore, there is a need for a more informed search strategy than brute-force sampling the entire space, which motivates a new heuristic approach. In this paper, we explore the non-trivial cases for the problem of minimizing the size of a run-length encoded BWT (RLBWT) via selecting a new ordering for the alphabet. We show that random sampling of the space of alphabet orderings usually gives sub-optimal orderings for compression and that a local search strategy can provide a large improvement in relatively few steps. We also inspect a selection of initial alphabet orderings, including ASCII, letter appearance, and letter frequency. While this alphabet ordering problem is computationally hard we demonstrate gain in compressibility.},
  archive      = {J_JOH},
  author       = {Major, Lily and Clare, Amanda and Daykin, Jacqueline W. and Mora, Benjamin and Zarges, Christine},
  doi          = {10.1007/s10732-025-09548-3},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {J. Heuristics},
  title        = {Heuristics for the run-length encoded Burrows–Wheeler transform alphabet ordering problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring the effectiveness and efficiency of simulation
optimization metaheuristic algorithms. <em>JOH</em>, <em>31</em>(1),
1–21. (<a href="https://doi.org/10.1007/s10732-025-09549-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms have proven capable as general-purpose algorithms for solving simulation optimization problems. Researchers and practitioners often compare different metaheuristic algorithms by examining one or more measures that are derived through empirical analysis. This paper presents a single measure that can be used to empirically compare different metaheuristic algorithms for optimization problems. This measure incorporates both the effectiveness and efficiency of the metaheuristic algorithm, which is especially important in simulation optimization applications because the number of simulation runs available to the analyst (i.e., the run budget) can vary significantly with each simulation study. Therefore, the trade-off between the effectiveness and efficiency of a metaheuristic algorithm must be examined. This single measure is especially useful for multi-objective optimization problems; however, determining this measure is non-trivial for two or more objective functions. Additional details for calculating this measure for multi-objective optimization problems are provided as well as a procedure for comparing two or more metaheuristic algorithms. Finally, computational results are presented and analyzed to compare the performance of metaheuristic algorithms using knapsack problems, pure binary integer programs, traveling salesman problems, and the average results obtained across a diverse set of optimization problems that include simulation and multi-objective optimization problems.},
  archive      = {J_JOH},
  author       = {Thengvall, Benjamin G. and Hall, Shane N. and Deskevich, Michael P.},
  doi          = {10.1007/s10732-025-09549-2},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {J. Heuristics},
  title        = {Measuring the effectiveness and efficiency of simulation optimization metaheuristic algorithms},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection for high-dimensional data using a
multivariate search space reduction strategy based scatter search.
<em>JOH</em>, <em>31</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s10732-025-09550-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In feature selection, the increasing of the dimensionality and the complexity of feature interactions make the problem challenging. Furthermore, searching for an optimal subset of features from a high-dimensional feature space is known to be an $$\mathcal{N}\mathcal{P}$$ -hard problem. To improve the efficiency and effectiveness of the search algorithm, feature grouping has emerged as a way to reduce the search space by clustering features according to a measure. In this work we propose to reduce the search space by applying a greedy algorithm, called Multivariate Greedy Predominant Groups Generator (MGPGG). MGPGG extends the idea of the Greedy Predominant Groups Generator (GPGG) algorithm by taking into account feature interaction among three or more features. For this purpose, MGPGG uses the Multivariate Symmetrical Uncertainty (MSU) to group features that share information about the class label. We also propose a Scatter Search strategy that integrates MGPGG to find small subsets of features with high predictive power. The proposed algorithm, called Multivariate Predominant Group-based Scatter Search (MPGSS), is tested on high-dimensional data from biomedical and text-mining fields. The proposal is compared with state-of-the-art feature selection strategies. Results show that MPGSS is competitive since it is capable of finding small subsets of features while keeping high predictive classification models.},
  archive      = {J_JOH},
  author       = {Garcia-Torres, Miguel},
  doi          = {10.1007/s10732-025-09550-9},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-33},
  shortjournal = {J. Heuristics},
  title        = {Feature selection for high-dimensional data using a multivariate search space reduction strategy based scatter search},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heuristic algorithm for integrated ship scheduling, routing
and stowage problem in multi-vessel roll-on/roll-off shipping.
<em>JOH</em>, <em>31</em>(1), 1–40. (<a
href="https://doi.org/10.1007/s10732-025-09551-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Roll-on/roll-off (RoRo) ships offer distinct advantages in the maritime industry when it comes to transporting wheeled cargos and super-large vehicles. As the scale of RoRo fleets continues to grow, RoRo shipping companies face the challenge of efficiently organizing multiple ships to meet transportation demands across various regions, ensuring order fulfillment, and minimizing costs. In light of these challenges, we introduced and explored the multi-vessel RoRo ship scheduling, routing and stowage problem (m-RSRSSP), and proposed a mixed-integer linear programming (MILP) model to address this problem. Compared with previous studies, this paper enriches fleet&#39;s decision-making and address scenarios where multiple cargos are considered at one port on the basis of integrating ship scheduling, routing and stowage problem of ro-ro ship, which is better aligned with the requirements of certain practical scenarios. Given the intricate nature of this model, we developed a heuristic algorithm rooted in tabu search, incorporating a nested greedy approach. Furthermore, we presented a case study involving deep-sea RoRo transportation between Northeast Asia and Europe. The experimental results validate the efficiency and reliability of the proposed heuristic algorithm in solving large-scale problems, and provide valuable strategies for the formulation of the RoRo fleet operation schemes.},
  archive      = {J_JOH},
  author       = {Zhao, Yuzhe and Peng, Peiyun and Zhou, Jingmiao and Wang, Yadong},
  doi          = {10.1007/s10732-025-09551-8},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-40},
  shortjournal = {J. Heuristics},
  title        = {Heuristic algorithm for integrated ship scheduling, routing and stowage problem in multi-vessel roll-on/roll-off shipping},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective population-based approach for the partial set
covering problem. <em>JOH</em>, <em>31</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s10732-025-09552-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The partial set covering problem (PSCP) is a significant combinatorial optimization problem that finds applications in numerous real-world scenarios. The objective of PSCP is to encompass a minimum number of subsets while ensuring the coverage of at least n elements. Due to its NP-hard nature, solving large-scale PSCP efficiently remains a critical issue in computational intelligence. To effectively tackle this challenge, we delve into a population-based approach that incorporates a modified tabu search, thereby striking a delicate balance between exploration and exploitation. To further enhance its efficacy, we employ the multiple path-relinking strategy and the fix-and-optimize process. Finally, the dynamic resource allocation scheme is utilized to save computing efforts. Comparative experiments of the proposed algorithm were conducted against three state-of-the-art competitors, across two distinct categories, encompassing 150 instances. The results significantly underscore the profound effectiveness of our proposed algorithm, as evidenced by the updating of 67 best-known solutions. Moreover, we conduct an in-depth analysis of the key components inherent to the algorithm, shedding light on their respective influences on the whole performance.},
  archive      = {J_JOH},
  author       = {Zhang, Ye and He, Jinlong and Zhou, Yupeng and Hu, Shuli and Cai, Dunbo and Tian, Naiyu and Yin, Minghao},
  doi          = {10.1007/s10732-025-09552-7},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-32},
  shortjournal = {J. Heuristics},
  title        = {An effective population-based approach for the partial set covering problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jota---19">JOTA - 19</h2>
<ul>
<li><details>
<summary>
(2025). Optimal control of several motion models. <em>JOTA</em>,
<em>205</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10957-025-02610-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to studying the dynamic optimization of several controlled crowd motion models in general planar settings. A set of necessary optimality conditions for optimal control problems involving crowd motion models with multiple agents and obstacles was derived and thoroughly analyzed. The analysis provides valuable insights into the primal and dual elements, as well as the degeneracy phenomena. The paper proposes several effective algorithms based on these necessary optimality conditions and presents various nontrivial illustrative examples along with their simulations. The implementation of all the considered motion models can be found via the link: https://github.com/tancao1128/Optimal_Control_of_Several_Motion_Models .},
  archive      = {J_JOTA},
  author       = {Cao, Tan H. and Chapagain, Nilson and Lee, Haejoon and Phung, Thi and Thieu, Nguyen Nang},
  doi          = {10.1007/s10957-025-02610-x},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-36},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Optimal control of several motion models},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pareto game of stochastic differential system with terminal
state constraint. <em>JOTA</em>, <em>205</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10957-025-02612-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on a type of Pareto game of stochastic differential equation with terminal state constraint. Firstly, we transform equivalently a nonlinear Pareto game problem with convex control space and terminal state constraint into a constrained stochastic optimal control problem. By virtue of duality theory and stochastic maximum principle, a necessary condition for Pareto efficient strategy is established. With some convex assumptions, we also give a sufficient condition for Pareto efficient strategy. Secondly, we consider a linear-quadratic Pareto game with terminal state constraint, and a feedback representation for Pareto efficient strategy is derived. Finally, as an application, we solve a government debt stabilization problem and give some numerical results.},
  archive      = {J_JOTA},
  author       = {Huang, Pengyan and Wang, Guangchen and Wang, Shujun},
  doi          = {10.1007/s10957-025-02612-9},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-30},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Pareto game of stochastic differential system with terminal state constraint},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Degenerate drifted wave equations in nondivergence form:
Nonlinear stabilization. <em>JOTA</em>, <em>205</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10957-025-02613-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the stabilization of degenerate 1-D wave equations in non divergence form with drift. The degeneracy takes place in one boundary point and the stabilization is obtained by a nonlinear damping in the nondegeneracy one.},
  archive      = {J_JOTA},
  author       = {Fragnelli, Genni and Mugnai, Dimitri},
  doi          = {10.1007/s10957-025-02613-8},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-36},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Degenerate drifted wave equations in nondivergence form: Nonlinear stabilization},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A euclidean distance matrix model for convex clustering.
<em>JOTA</em>, <em>205</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10957-025-02616-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering has been one of the most basic and essential problems in unsupervised learning due to various applications in many critical fields. The recently proposed sum-of-norms (SON) model by Pelckmans et al. (in: PASCAL workshop on statistics and optimization of clustering, 2005), Lindsten et al. (in: IEEE statistical signal processing workshop, 2011) and Hocking et al. (in: Proceedings of the 28th international conference on international conference on machine learning, 2011) has received a lot of attention. The advantage of the SON model is the theoretical guarantee in terms of perfect recovery, established by Sun et al. (J Mach Learn Res 22(9):1–32, 2018). It also provides great opportunities for designing efficient algorithms for solving the SON model. The semismooth Newton based augmented Lagrangian method by Sun et al. (2018) has demonstrated its superior performance over the alternating direction method of multipliers and the alternating minimization algorithm. In this paper, we propose a Euclidean distance matrix model based on the SON model. Exact recovery property is achieved under proper assumptions. An efficient majorization penalty algorithm is proposed to solve the resulting model. Extensive numerical experiments are conducted to demonstrate the efficiency of the proposed model and the majorization penalty algorithm.},
  archive      = {J_JOTA},
  author       = {Wang, Z. W. and Liu, X. W. and Li, Q. N.},
  doi          = {10.1007/s10957-025-02616-5},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-22},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {A euclidean distance matrix model for convex clustering},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimality conditions for interval-valued optimization
problems on riemannian manifolds under a total order relation.
<em>JOTA</em>, <em>205</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s10957-025-02618-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores fundamental properties of convex interval-valued functions defined on Riemannian manifolds. The study employs generalized Hukuhara directional differentiability to derive KKT-type optimality conditions for an interval-valued optimization problem on Riemannian manifolds. Based on the type of functions involved in optimization problems, we consider the following cases: objective function as well as constraints are real-valued; objective function is interval-valued and constraints are real-valued; objective function as well as constraints are interval-valued. The whole theory is justified with the help of examples. The order relation that we use throughout the paper is a total order relation defined on the collection of all closed and bounded intervals in $$\mathbb {R}$$ .},
  archive      = {J_JOTA},
  author       = {Bhat, Hilal Ahmad and Iqbal, Akhlad and Aftab, Mahwash},
  doi          = {10.1007/s10957-025-02618-3},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-29},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Optimality conditions for interval-valued optimization problems on riemannian manifolds under a total order relation},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random descent steps in a probability maximization scheme.
<em>JOTA</em>, <em>205</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10957-025-02619-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient computation of multivariate distribution functions calls for considerable effort. Hence coordinate descent and derivative-free approaches are attractive. This paper deals with constrained convex problems. We perform random descent steps in an approximation scheme that is an inexact cutting-plane method from a dual viewpoint. We prove that the scheme converges and present a computational study comparing different descent methods applied in the approximation scheme.},
  archive      = {J_JOTA},
  author       = {Csizmás, Edit and Drenyovszki, Rajmund and Szántai, Tamás and Fábián, Csaba I.},
  doi          = {10.1007/s10957-025-02619-2},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Random descent steps in a probability maximization scheme},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why study spherical convexity of non-homogeneous quadratics
and what makes it surprising? <em>JOTA</em>, <em>205</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10957-025-02620-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes necessary, sufficient, and equivalent conditions for the spherical convexity of non-homogeneous quadratic functions. By examining criteria for determining spherical convexity, we identified unique properties that differentiate spherically convex quadratic functions from their geodesically convex counterparts in both hyperbolic and Euclidean spaces. Since spherically convex functions over the entire sphere are constant, our analysis focuses on proper spherically convex subsets of the sphere. Our primary results concern non-homogeneous quadratic functions on the spherically convex set of unit vectors with positive coordinates. We also extend our findings to more general spherically convex sets. Additionally, the paper explores special cases of non-homogeneous quadratic functions where the defining matrix is of a specific type, such as positive, diagonal, or a Z-matrix. This study not only provides useful criteria for spherical convexity but also reveals surprising characteristics of spherically convex quadratic functions, contributing to a deeper understanding of convexity in spherical geometries.},
  archive      = {J_JOTA},
  author       = {Bolton, Ryan and Németh, Sándor Zoltán},
  doi          = {10.1007/s10957-025-02620-9},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Why study spherical convexity of non-homogeneous quadratics and what makes it surprising?},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic ISTA/FISTA adaptive step search algorithms for
convex composite optimization. <em>JOTA</em>, <em>205</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10957-025-02621-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and analyze stochastic variants of ISTA and a full backtracking FISTA algorithms (Beck and Teboulle in SIAM J Imag Sci 2(1):183–202, 2009; Scheinberg et al. in Found Comput Math 14(3):389–417, 2014) for composite optimization without the assumption that stochastic gradient is an unbiased estimator. This work extends analysis of inexact fixed step ISTA/FISTA in Schmidt et al. (Convergence rates of inexact proximal-gradient methods for convex optimization, 2022. arXiv:1109.2415 ) to the case of stochastic gradient estimates and adaptive step-size parameter chosen by backtracking. It also extends the framework for analyzing stochastic line-search method in Cartis and Scheinberg (Math Program 169(2):337-375, 2018) to the proximal gradient framework as well as to the accelerated first order methods.},
  archive      = {J_JOTA},
  author       = {Nguyen, Lam M. and Scheinberg, Katya and Tran, Trang H.},
  doi          = {10.1007/s10957-025-02621-8},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-37},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Stochastic ISTA/FISTA adaptive step search algorithms for convex composite optimization},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relaxed two-step inertial tseng’s extragradient method for
nonmonotone variational inequalities. <em>JOTA</em>, <em>205</em>(1),
1–27. (<a href="https://doi.org/10.1007/s10957-025-02622-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel inertial projection method for solving the variational inequality (VI) without imposing the restrictive assumption of monotonicity on the cost operator. We establish global convergence of the proposed method under the condition that the solution set of the associated Minty VI with it is non-empty. Our results improve upon and extend many important related results in this research direction, providing a more general and flexible framework for tackling non-monotone variational inequalities. To demonstrate the practical efficacy of our method, we give some numerical illustrations and apply the proposed algorithm to solve a network equilibrium flow problem, which is a fundamental problem in transportation infrastructure modeling. We also compare the performance of our algorithm with those of existing ones.},
  archive      = {J_JOTA},
  author       = {Viet Thong, Duong and Ky Anh, Pham and Tien Dung, Vu},
  doi          = {10.1007/s10957-025-02622-7},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-27},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Relaxed two-step inertial tseng’s extragradient method for nonmonotone variational inequalities},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum principle for optimal control of mean-field backward
doubly SDEs with delay. <em>JOTA</em>, <em>205</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10957-025-02624-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the control problems of mean-field backward doubly stochastic differential equations with delay in the form of an integral with respect to a finite regular measure. Using the standard variational method, we introduce a new type of anticipated mean-field doubly stochastic differential equations as adjoint equations and derive a necessary condition in form of the maximum principle for optimal control. Under appropriate assumptions, the sufficiency of the maximum principle is also established. Our results can be applied to a certain class of linear quadratic control problems and be used to study the mean-field game for a pension fund model with delayed surplus.},
  archive      = {J_JOTA},
  author       = {Wang, Meng},
  doi          = {10.1007/s10957-025-02624-5},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-24},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Maximum principle for optimal control of mean-field backward doubly SDEs with delay},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gevrey regularity for a fluid–structure interaction model.
<em>JOTA</em>, <em>205</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10957-025-02625-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A result of Gevrey regularity is ascertained for a semigroup which models a fluid–structure interaction problem. In this model, the fluid evolves in a piecewise smooth or convex geometry $$\mathcal {O}$$ . On a portion of the boundary, a fourth order plate equation is coupled with the fluid through pressure and matching velocities. The key to obtaining the conclusion of Gevrey regularity is an appropriate estimation of the resolvent of the associated $$C_0$$ -semigroup operator. Moreover, a numerical scheme and example is provided which empirically demonstrates smoothing of the fluid–structure semigroup.},
  archive      = {J_JOTA},
  author       = {Avalos, George and McKnight, Dylan and McKnight, Sara},
  doi          = {10.1007/s10957-025-02625-4},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Gevrey regularity for a Fluid–Structure interaction model},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for uncertain linear optimization problems through
polytopic approximation of the uncertainty set. <em>JOTA</em>,
<em>205</em>(1), 1–42. (<a
href="https://doi.org/10.1007/s10957-025-02626-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a globally convergent iterative method to solve uncertain constrained linear optimization problems. Due to the nondeterministic nature of such a problem, we use the min-max approach to convert the given problem into a deterministic one. We show that the robust feasible sets of the problem corresponding to the uncertainty set and the convex hull of the uncertainty set are identical. This result helps to reduce the number of inequality constraints of the problem drastically; often, this result reduces the semi-infinite programming problem of the min-max robust counterpart into a problem with a finite number of constraints. Following this, we provide a necessary and sufficient condition for the boundedness of the robust feasible set of the problem. Moreover, we explicitly identify the robust feasible set of the problem for polytopic and ellipsoidal uncertainty sets. We present an algorithm to construct an inner polytope of the convex hull of a general uncertainty set under a certain assumption. This algorithm provides a point-wise inner polytopic approximation of the convex hull with arbitrarily small precision. We employ this inner polytopic approximation corresponding to the uncertainty set and the infeasible interior-point technique to derive an iterative approach to solve general uncertain constrained linear optimization problems. Global convergence for the proposed method is reported. Numerical experiments illustrate the practical behaviour of the proposed method on discrete, star-shaped, disc-shaped, and ellipsoidal uncertainty sets.},
  archive      = {J_JOTA},
  author       = {Raushan, Ravi and Ghosh, Debdas and Zhao, Yong and Wei, Zhou},
  doi          = {10.1007/s10957-025-02626-3},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-42},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {A method for uncertain linear optimization problems through polytopic approximation of the uncertainty set},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Output-based receding horizon stabilizing control for linear
parabolic equations. <em>JOTA</em>, <em>205</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s10957-025-02628-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A receding horizon control framework is coupled with a Luenberger observer to construct an output-based control input stabilizing parabolic equations. The actuators and sensors are indicator functions of small subdomains, representing localized actuation and localized measurements. It is shown that, for a class of explicitly given sets of actuators and sensors, we can guarantee the stabilizing property of the constructed input. Results of numerical simulations are presented validating the theoretical findings.},
  archive      = {J_JOTA},
  author       = {Azmi, Behzad and Rodrigues, Sérgio S.},
  doi          = {10.1007/s10957-025-02628-1},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-34},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Output-based receding horizon stabilizing control for linear parabolic equations},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Facial approach for constructing stationary
points for mathematical programs with cone complementarity constraints.
<em>JOTA</em>, <em>205</em>(1), 1. (<a
href="https://doi.org/10.1007/s10957-025-02629-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOTA},
  author       = {Madariaga, Javier I. and Ramírez, Héctor},
  doi          = {10.1007/s10957-025-02629-0},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Correction: Facial approach for constructing stationary points for mathematical programs with cone complementarity constraints},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intersection theorem for a pair of set-valued maps and
its applications. <em>JOTA</em>, <em>205</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10957-025-02632-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an intersection theorem for a pair of set-valued maps under relaxed closedness and coercivity conditions is investigated. This theorem leads to minimax inequality and variational relation results, enabling us to solve such problems even when the usual closedness and compactness conditions are not satisfied.},
  archive      = {J_JOTA},
  author       = {Lotfipour, Maryam},
  doi          = {10.1007/s10957-025-02632-5},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {An intersection theorem for a pair of set-valued maps and its applications},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Existence and uniqueness of solutions of generalized mixed
variational inequalities. <em>JOTA</em>, <em>205</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10957-025-02636-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the generalized mixed variational inequality, which encompasses both the generalized variational inequality and the mixed variational inequality. The core contribution of this paper is twofold. Firstly, by utilizing the principles of degree theory, we establish certain sufficient conditions for the existence of solutions to the generalized mixed variational inequality. Additionally, we formulate a sufficient condition that ensures the uniqueness of these solutions. Secondly, we recognize that the conditions outlined in our theorem are inapplicable to the generalized mixed polynomial variational inequality, a subclass within the broader family of generalized mixed variational inequalities. To address this, we employ an exceptional family of elements and establish an existence and uniqueness theorem specifically tailored for the generalized mixed polynomial variational inequality.},
  archive      = {J_JOTA},
  author       = {Liu, Jian-Xun and Lan, Zhao-Feng and Huang, Zheng-Hai},
  doi          = {10.1007/s10957-025-02636-1},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-21},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Existence and uniqueness of solutions of generalized mixed variational inequalities},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solution existence and compactness analysis for nonsmooth
optimization problems. <em>JOTA</em>, <em>205</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s10957-025-02637-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the analysis of geometrical properties and behaviors of the optimal value and global optimal solutions for a class of nonsmooth optimization problems. We provide conditions under which the solution set of a nonsmooth and nonconvex optimization problem is non-empty and/or compact. We also examine related properties such as the compactness of the sublevel sets, the boundedness from below and the coercivity of the objective function to characterize the non-emptiness and the compactness of the solution set of the underlying optimization problem under the unboundedness of its associated feasible set.},
  archive      = {J_JOTA},
  author       = {Hung, Nguyen Canh and Chuong, Thai Doan and Anh, Nguyen Le Hoang},
  doi          = {10.1007/s10957-025-02637-0},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-25},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Solution existence and compactness analysis for nonsmooth optimization problems},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qualitative properties of robust benson efficient solutions
of uncertain vector optimization problems. <em>JOTA</em>,
<em>205</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10957-025-02638-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider both unconstrained and constrained uncertain vector optimization problems involving free disposal sets, and study the qualitative properties of their robust Benson efficient solutions. First, we discuss necessary and sufficient optimality conditions for the robust Benson efficient solutions of these problems using the linear scalarization method. Then, by utilizing this approach, we investigate the semicontinuity properties of the solution maps when the problem data is perturbed by parameters given in parameter spaces. Finally, we suggest concepts of approximate robust Benson efficient solutions and investigate Hausdorff well-posedness conditions for such problems with respect to these approximate solutions. Several examples are provided to illustrate the applicability and novelty of the results obtained in this study.},
  archive      = {J_JOTA},
  author       = {Anh, Lam Quoc and Thuy, Vo Thi Mong and Zhao, Xiaopeng},
  doi          = {10.1007/s10957-025-02638-z},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-37},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Qualitative properties of robust benson efficient solutions of uncertain vector optimization problems},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Second-order sufficient optimality conditions in the
calculus of variations. <em>JOTA</em>, <em>205</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s10957-025-02639-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some classic second-order sufficient optimality conditions in the calculus of variations are shown to be equivalent, while also introducing a new equivalent second-order condition which is extremely easy to apply: simply integrate a linear second-order initial value problem and check that the solution is positive over the problem domain.},
  archive      = {J_JOTA},
  author       = {Hager, William W.},
  doi          = {10.1007/s10957-025-02639-y},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-9},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Second-order sufficient optimality conditions in the calculus of variations},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jrtip---12">JRTIP - 12</h2>
<ul>
<li><details>
<summary>
(2025). CFP-PSPNet: A lightweight unmanned vessel water segmentation
algorithm. <em>JRTIP</em>, <em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01603-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate water segmentation is the first prerequisite for unmanned vessels to navigate safely and perform other operations. Aiming at the problems of low utilization rate of unmanned boat water image features and low accuracy of contour edge segmentation in complex inland river scenarios, this paper proposes a lightweight water segmentation algorithm: channel feature pyramid-pyramid scene parsing network (CFP-PSPNet), which realizes efficient and accurate segmentation of water area in complex scenarios. First, a cross transformation-channel feature pyramid (CT-CFP) is proposed, which improves the utilization of the original feature information by cross-fertilizing the feature information between different layers and realizes the improvement of the water segmentation accuracy; Secondly, a parallel semantic segmentation network CFP-PSPNet is designed, which extracts the image information by pyramid pooling module (PPM) and CT-CFP dual pyramid, which solves the problem of loss of detail information and edge information, so as to achieve the purpose of improving the accuracy; finally, Mobilenetv2 after the introduction of encoder-context-attention (ECA) is used as a feature extraction network, which reduces the number of parameters and computation of the network without affecting the segmentation accuracy and realizes the lightweight design of the network. Experiments are conducted on the open-source dataset USVInland, and the experimental results show that our CFP-PSPNet algorithm has a significant reduction in the number of parameters, an increase in detection speed by 81FPS, and mean intersection over union (MIoU) and accuracy rates of 97.71% and 98.75%, respectively, which are 1.41% and 0.74% higher than that of the original network. It is superior to other classical semantic segmentation algorithms.},
  archive      = {J_JRTIP},
  author       = {Yang, Xuecun and Song, Yijing and He, Lintao and Xue, Hang and Dong, Zhonghua and Zhang, Qingyun},
  doi          = {10.1007/s11554-024-01603-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {CFP-PSPNet: A lightweight unmanned vessel water segmentation algorithm},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECDet: Efficient oriented object detection on the aerial
image with cross-layer attention. <em>JRTIP</em>, <em>22</em>(1), 1–13.
(<a href="https://doi.org/10.1007/s11554-024-01617-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in horizontal object detection models, oriented object detection models have also made significant strides. Yet existing rotated object detectors often struggle to maintain high accuracy while processing high-resolution remote sensing images in real-time. To address these challenges, we propose a new lightweight model specifically for oriented object detection, named the efficient cross-layer attention detector (ECDet). ECDet integrates several efficient modules, including an efficient reparameterized Transformer-like backbone (ERepViT) to reduce computational costs, and the efficient cross-layer fusion neck (CLF-Neck), a lightweight alternative to traditional pyramid networks for feature fusion with attention mechanism. Additionally, we introduce the lightweight task interaction decoupled (LTID) head, which enhances task-specific performance by providing more detailed, task-aligned information for classification and regression with minimal computational cost. Furthermore, an ensemble loss combined with the phase shifting coder (PSC) mitigates the angle discontinuity issue in regression-based methods. Evaluations on the DOTAv1 and HRSC datasets show that ECDet runs 32% faster than RTMDet-S with higher accuracy, demonstrating its strong potential for practical application. The source code will be release at https://github.com/tianlianghai/ECDet .},
  archive      = {J_JRTIP},
  author       = {Lyu, Xueqiang and Tian, Lianghai and Teng, Shangzhi},
  doi          = {10.1007/s11554-024-01617-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ECDet: Efficient oriented object detection on the aerial image with cross-layer attention},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel-level attention based data compression for spike
camera. <em>JRTIP</em>, <em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01618-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of autonomous driving applications, there is an increasing demand for high-speed vision sensors like spike cameras. However, the data transmission and storage requirements are becoming increasingly burdensome due to the high temporal resolution, such as 40,000 Hz. To resolve this problem, we propose a pixel-level attention-based data compression method for the spike camera. First the input spike data are partitioned into two block types by pixel-level attention-based method. Then, the two blocks are condensed using different methods and side information marking is transmitted for spike decoding. Finally, the condensed spike and side information marking are compressed into a binary stream for storage. The experimental results show that our method achieves higher compression efficiency than conventional methods. The decompressed spike can also reconstruct the image with better visual quality.},
  archive      = {J_JRTIP},
  author       = {Li, Yansong and Huang, Xiaofeng and Li, Shangqia and Cui, Yan and Zhou, Yang and Song, Jian and Yin, Haibing},
  doi          = {10.1007/s11554-024-01618-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Pixel-level attention based data compression for spike camera},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rocknet: Lightweight network for real-time segmentation of
martian rocks. <em>JRTIP</em>, <em>22</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01619-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rock segmentation on the Martian is particularly critical for rover navigation, obstacle avoidance, and scientific target detection. We propose a lightweight network for real-time semantic segmentation of Martian rocks (RockNet). First, we propose the cross-dimension channel attention (CDCA) model to replace traditional downsample and upsample operation, which gives more weight to the channels with more useful information by adjusting the weight of each channel. Second, we modify the short-term dense concatenate model, we adopt dilated convolution to learn the feature with a larger receptive field, and through the skip connection structure, the degradation of the network can be reduced. Finally, we propose a feature fusion module (FFM) to fully fuse different levels of features. With only 0.86M parameters, our model gets 82.37% mIoU and 105.7 FPS running speed on the dataset of TWMARS.},
  archive      = {J_JRTIP},
  author       = {Wei, Pengfei and Sun, Zezhou and Tian, He},
  doi          = {10.1007/s11554-024-01619-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rocknet: Lightweight network for real-time segmentation of martian rocks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time underwater target detection based on improved
YOLOv7. <em>JRTIP</em>, <em>22</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-025-01621-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target detection is crucial for ocean exploration, but existing methods struggle to achieve satisfactory results due to the complexity of the underwater environment. To enhance the accuracy and real-time performance of underwater detection models, we propose an improved YOLOv7 model. We introduce a multi-granularity feature attention method based on the Efficient Channel Attention (ECA) to help the model better adapt to the diverse conditions in the underwater environment, reducing focus on redundant information. Utilizing coordinate convolution provides the network with spatial awareness of input image coordinates, enabling more effective localization of target objects and reducing interference from similar background elements. To accommodate the features of small and dense underwater targets, we use normalized Wasserstein distance to measure the similarity of bounding boxes. On the Underwater Robot Picking Contest 2019 (URPC 2019) dataset, the mean Average Precision (mAP) of our improved network has reached 86.19%, which represents a 1.57% increase compared to the original YOLOv7 network. Additionally, the frames per second (fps) has achieved 124, surpassing the performance of the original network. This improvement is significantly superior to conventional target detection models, providing a faster and more accurate advantage for underwater target detection tasks in complex underwater environments.},
  archive      = {J_JRTIP},
  author       = {Wu, Qingqi and Cen, Lihui and Kan, Shichao and Zhai, Yongping and Chen, Xiaofang and Zhang, Hong},
  doi          = {10.1007/s11554-025-01621-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time underwater target detection based on improved YOLOv7},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LKR-DETR: Small object detection in remote sensing images
based on multi-large kernel convolution. <em>JRTIP</em>, <em>22</em>(1),
1–14. (<a href="https://doi.org/10.1007/s11554-025-01622-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection in remote sensing imagery remains a challenging problem in computer vision. To address the inherent limitations of aerial imagery, such as densely packed objects with insufficient detail and occlusions caused by complex backgrounds, this study proposes LKR-DETR, an innovative object detection in remote sensing imagery framework based on RT-DETR. We propose a lightweight and efficient feature extraction module with large kernel convolution, which expands the receptive field while reducing parameters and computational costs. Furthermore, we present a novel multi-scale feature fusion structure based on wavelet transform convolution that effectively utilizes low-frequency information from low-level feature maps. Additionally, we introduce a lightweight image restoration module utilizing large kernel convolutions, which effectively recovers previously undetected details of small objects. To improve bounding box regression accuracy, the original GIoU loss is replaced with a Focaler-DIoU loss function. Compared to the benchmark model RT-DETR, the LKR-DETR model achieves a 2.5% improvement in $$mAP_{0.5}$$ and a 2.0% improvement in $$mAP_{0.5:0.95}$$ on the VisDrone2019-DET dataset, a 1.7% and 4.4% improvement on the DOTAv1.5 dataset, and a 3.4% and 2.4% improvement on the HIT-UAV dataset, while also reducing the parameter count and model size. Relative to other cutting-edge models, LKR-DETR attains superior detection accuracy while maintaining relatively low computational complexity, establishing it as an efficient solution for small object detection in remote sensing imagery.},
  archive      = {J_JRTIP},
  author       = {Dong, Ying and Xu, Fucheng and Guo, Jiahao},
  doi          = {10.1007/s11554-025-01622-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LKR-DETR: Small object detection in remote sensing images based on multi-large kernel convolution},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DHNet: A surface defect detection model utilizing
multi-scale convolutional kernels. <em>JRTIP</em>, <em>22</em>(1), 1–15.
(<a href="https://doi.org/10.1007/s11554-025-01623-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting micro-defects in densely populated printed circuit boards (PCBs) with complex backgrounds is a critical challenge. To address the problem, the DHNet, a small object detection network based on YOLOv8 employing multi-scale convolutional kernels is proposed for feature extraction and fusion. The lightweight VOVGSHet module is designed for feature fusion and a pyramid structure to efficiently leverage feature map relationships while minimizing model complexity and parameters. Otherwise, to optimize the original extraction structure and enhance multi-scale defect detection, convolutional kernels of varying sizes process the same input channels. Additionally, the incorporation of the Wise-IoU loss function improves small defect detection accuracy and efficiency. Moreover, extensive experiments on a custom PCB dataset demonstrate DHNet&#39;s effectiveness, achieving an outstanding mean Average Precision (mAP) of 84.5%, surpassing the original YOLOv8 network by 4.0%, with parameters only of 2.85 M. Model demonstrates a latency of 3.6 ms on NVIDIA 4090. However, YOLOv8n has a latency of 4.4 ms. Validation on public DeepPCB and NEU datasets further confirms DHNet&#39;s superiority, which can reach 99.1% and 79.9% mAP, respectively. Finally, successful deployment on the NVIDIA Jetson Nano platform validates DHNet&#39;s suitability for real-time defect detection in industrial applications.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yingying and Wang, Shuo and Wang, Jinhai and Zhao, Yu and Chen, Zhiwei},
  doi          = {10.1007/s11554-025-01623-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DHNet: A surface defect detection model utilizing multi-scale convolutional kernels},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSBF-YOLO: A lightweight model for tomato ripeness detection
in natural environments. <em>JRTIP</em>, <em>22</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-025-01624-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate tomato ripeness detection is essential for optimizing harvest timing and maximizing yield. Deep learning-based object detection has proven effective in this task. However, many existing algorithms have numerous parameters and substantial computational demands, making them unsuitable for agricultural environments with limited computational resources. Additionally, accurate detection becomes challenging with overlapping fruits, leaf occlusion, or complex backgrounds. To address these issues, this paper proposes a lightweight detection model, GSBF-YOLO. This model designs the GSim module to reduce parameters while maintaining detection accuracy. The C3Ghost module further reduces parameter count by replacing the traditional C3 module. The PANet multi-scale feature fusion network in the neck is replaced with the Bi-directional Feature Pyramid Network (BiFPN), which adjusts weights based on the importance of input features. Lastly, the fine-tuned FocalEIOU Loss function is used to calculate the bounding box regression loss, enhancing the model’s ability to adjust the weights of high-quality anchor boxes for better detection of targets in occlusion scenarios. Experimental results show that GSBF-YOLO reduces parameters and computational load by 42% and 45%, respectively, while mean Average Precision (mAP) increases by 1.9% and 1.6% on two datasets. The model achieves 110 Frames Per Second (FPS), meeting real-time detection requirements, and has fewer parameters and higher accuracy compared to models like YOLOv8. The research indicates that the proposed lightweight model can effectively detect tomato ripeness in natural environments.},
  archive      = {J_JRTIP},
  author       = {Hao, Fengqi and Zhang, Zuyao and Ma, Dexin and Kong, Hoiio},
  doi          = {10.1007/s11554-025-01624-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GSBF-YOLO: A lightweight model for tomato ripeness detection in natural environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real-time and general method for converting offline
skeleton-based action recognition to online ones. <em>JRTIP</em>,
<em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01625-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many motion sensors can directly acquire human skeletal data, and then extract features on the skeletal data through GCNs (graph convolutional networks) to perform action recognition. However, almost all state-of-the-art (SOTA) methods are offline methods, cannot perform online inference, wasting computational resources. The existing approach to transforming offline action recognition into online action recognition is to reconstruct the network structure of the offline method. This requires developers to have a deep understanding of the algorithm’s network structure and make extensive modifications, which results in slow development. To address the above issue, this paper points out that to convert offline methods to online ones, the key is removing outdated frame features and fusing new frame features. Furthermore, we propose a general and simple model called Encode One Frame (EOF), which achieves feature removal and fusion by a correlation matrix and the guidance of a teacher model. The EOF model has online inference capabilities, requiring only the input of the new frame of the current sample and the features encoded from the old sample. Based on the EOF model, we further propose the You Only Encode One Frame (YOEOF) algorithm to correct the cumulative errors generated during EOF model online inference. By coupling these proposals, YOEOF achieves online inference and outperforms some SOTA methods on public datasets. The deployment at the application level indicates that our method meets the requirements of high accuracy and real-time performance for dangerous action recognition.},
  archive      = {J_JRTIP},
  author       = {Dong, Liheng and He, Guiqing and Zhang, Zhaoxiang and Xu, Yuelei and Hui, Tian and Xu, Xin and Tao, Chengyang and Li, Huafeng},
  doi          = {10.1007/s11554-025-01625-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time and general method for converting offline skeleton-based action recognition to online ones},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid architecture for video frame prediction:
Combining convolutional LSTM and 3D CNN. <em>JRTIP</em>, <em>22</em>(1),
1–18. (<a href="https://doi.org/10.1007/s11554-025-01626-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame prediction represents a fundamental challenge in computer vision, necessitating precise modeling of both spatial and temporal dynamics within video sequences. This computational task holds substantial implications across diverse domains, including video compression optimization, robust object tracking systems, and advanced motion forecasting applications. In this investigation, we present a novel hybrid architecture that synthesizes the complementary strengths of Convolutional Long Short-Term Memory (ConvLSTM) networks and three-dimensional Convolutional Neural Networks (3D CNN) for enhanced frame prediction capabilities. Our methodological framework incorporates a ConvLSTM component that fundamentally augments the traditional LSTM architecture through the integration of convolutional operations, thereby facilitating sophisticated modeling of sequential dependencies. Concurrently, the 3D CNN component employs volumetric convolutional layers to extract rich spatio-temporal features from the input sequences. Rigorous empirical evaluation demonstrates the superior performance of the ConvLSTM architecture, which consistently yields reduced validation errors and elevated coefficients of determination. Specifically, the ConvLSTM model achieves a validation Mean Squared Error (MSE) of 0.0237 and an $${\textrm{R}}^{2}$$ value of 0.6951, substantially outperforming the 3D CNN model, which exhibits a validation MSE of 0.0471 and an $${\textrm{R}}^{2}$$ value of 0.3939. These empirical findings substantiate the efficacy of the ConvLSTM architecture in addressing the inherent complexities of video frame prediction, while simultaneously illuminating its considerable potential for deployment across various video processing and predictive modeling applications. The results provide compelling evidence for the advantages of incorporating convolutional operations within recurrent architectures for sequential visual data processing.},
  archive      = {J_JRTIP},
  author       = {Aravinda, C. V. and Al-Shehari, Taher and Alsadhan, Nasser A. and Shetty, Shashank and Padmajadevi, G. and Reddy, K. R. Udaya Kumar},
  doi          = {10.1007/s11554-025-01626-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel hybrid architecture for video frame prediction: Combining convolutional LSTM and 3D CNN},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMAL-YOLOv8: Real-time detection algorithm for insulator
defects based on optimized feature fusion. <em>JRTIP</em>,
<em>22</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s11554-025-01629-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of misdetection and missed detection caused by the diversity of insulator types and the complexity of defect textures in power transmission lines, and to meet the demands of collaborative inspection, we propose a real-time detection algorithm for insulator defects based on YOLOv8. First, considering the characteristics of the dataset sample sizes, we designed a lightweight OMAL-Neck structure that optimizes feature fusion, enhancing the utilization of feature information and improving detection performance for medium and large targets. Second, to address the issue of large parameter and computation requirements in the YOLOv8 detection head, we designed a lightweight and efficient detection head. This redesigned detection head incorporates PConv, further accelerating model inference speed. Lastly, to counteract the decline in detection accuracy due to model lightweighting, we integrated the C2f module with DySnakeConv, enhancing the feature extraction capability for tubular structures and complex textures, thereby preventing information loss. Experimental results demonstrate that compared to the baseline YOLOv8s, the proposed model increases FPS from 44 to 78 frames/s, reduces the number of parameters and computational complexity by 27 and 38%, respectively, and improves the mAP by 1.7%. The improved model offers significant advantages in both detection accuracy and real-time performance, enabling rapid and precise identification of insulators and their defects, thereby improving the efficiency of power line inspections.},
  archive      = {J_JRTIP},
  author       = {Ru, Hongfang and Zhang, Wenhao and Wang, Guoxin and Ding, Luyang},
  doi          = {10.1007/s11554-025-01629-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {OMAL-YOLOv8: Real-time detection algorithm for insulator defects based on optimized feature fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time recognition method for PCB chip targets based on
YOLO-GSG. <em>JRTIP</em>, <em>22</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01616-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industrial settings, the identification of chips on PCB boards is crucial for quality control and efficiency. However, achieving both speed and accuracy in chip detection remains a significant challenge. To address this issue, we propose the YOLO-GSG deep network model, which incorporates several novel modifications to the standard YOLO architecture. The key innovations include the replacement of the ELAN module with the C3Ghostnet module in the backbone network, improving feature extraction and reducing model complexity, and the introduction of the SE attention mechanism to minimize feature loss. Additionally, the GSnet module and GSConv convolution are integrated into the neck network to enhance feature fusion. The experimental results indicate that the YOLO-GSG algorithm achieves a mAP of 99.014%, with precision and recall improvements of 1.080% and 1.446% over the baseline YOLOv7 model. Additionally, the improved model has 24.478M parameters, 61.4 GFLOPs, and a model size of 50.8 MB. The model achieves an FPS of 231.55, representing a 12.8% speedup over the baseline. These results indicate that the YOLO-GSG model offers a superior balance of speed and accuracy for chip identification in industrial applications. This study contributes to the advancement of deep learning applications in industrial environments, providing a more efficient and effective tool for quality control in PCB manufacturing.},
  archive      = {J_JRTIP},
  author       = {Yue, Zeang and Li, Xun and Zhou, Huilong and Wang, Gaopin and Wang, Wenjie},
  doi          = {10.1007/s11554-024-01616-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time recognition method for PCB chip targets based on YOLO-GSG},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="kis---33">KIS - 33</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Taxonomy of deep learning-based intrusion
detection system approaches in fog computing: A systematic review.
<em>KIS</em>, <em>67</em>(2), 2017. (<a
href="https://doi.org/10.1007/s10115-024-02206-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Najafli, Sepide and Toroghi Haghighat, Abolfazl and Karasfi, Babak},
  doi          = {10.1007/s10115-024-02206-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {2017},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: taxonomy of deep learning-based intrusion detection system approaches in fog computing: a systematic review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-class margin climbing with cost-sensitive learning in
neural network classification. <em>KIS</em>, <em>67</em>(2), 1993–2016.
(<a href="https://doi.org/10.1007/s10115-024-02279-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large margin stands as an intuitive indicator of reliable classifiers, reflecting classifier robustness and generalizability. However, due to the intricate nonlinear mapping, directly defining margin as the optimization objective for multi-layer neural network is always challenging. On the other hand, the cost sensitivity coefficients of individual samples hold promise for shaping decision boundaries of neural network classification. A question arises as to whether optimizing neural network classifier can be guided to achieve larger classification margin by varying instance-level cost sensitivity factor. Inspired by above question, this paper proposes a heuristic hard mining strategy designed to progressively identify challenging samples and amplify the output margin through cost-sensitive learning. The refinement process adjusts the sample distribution when optimization reaches a local minimum to ensure the sustainable optimization, ultimately leading to margin climbing. Two hard mining algorithms are designed for binary and multi-class classification problems, which utilize distinct margin definitions based on different decision-making scenarios. In the proposed method, we focus on establishing individualized margin between distinct categories to more accurately characterize the inter-class margin. Empirical results demonstrate that our proposed methodology enhances both the accuracy and robustness in neural network classification.},
  archive      = {J_KIS},
  author       = {Zhang, Siyuan and Xie, Linbo and Chen, Ying and Zhang, Shanxin},
  doi          = {10.1007/s10115-024-02279-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1993-2016},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Inter-class margin climbing with cost-sensitive learning in neural network classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evidence-based approach for open-domain question
answering. <em>KIS</em>, <em>67</em>(2), 1969–1991. (<a
href="https://doi.org/10.1007/s10115-024-02269-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain question answering (ODQA) stands at the forefront of advancing natural language understanding and information retrieval. Traditional ODQA systems, which predominantly utilize a two-step process of information retrieval followed by reading module, face significant challenges in aligning retrieved passages with the contextual nuances of user queries. This paper introduces a novel methodology that leverages a semi-structured knowledge graph to enhance both the accuracy and relevance of answers in ODQA systems. Our model employs a threefold approach: firstly, it extracts and ranks evidence from a textual knowledge graph, a semi-structured knowledge graph where the nodes are real-world entities and the edges are sentences that two entities co-occur in, based on the contextual relationships relevant to the question. Secondly, it utilizes this ranked evidence to re-rank initially retrieved passages, ensuring that they align more closely with the query’s context. Thirdly, it integrates this evidence into a generative reading component to construct precise and context-rich answers. We compare our model, termed contextual evidence-based question answering (CEQA), against traditional and state-of-the-art ODQA systems across several datasets, including TriviaQA, Natural Questions, and SQuAD Open. Our extensive experiments and ablation studies show that CEQA significantly outperforms existing methods by improving both the relevance of retrieved passages and the accuracy of the generated answers, thereby establishing a new benchmark in ODQA.},
  archive      = {J_KIS},
  author       = {Jafarzadeh, Parastoo and Ensan, Faezeh},
  doi          = {10.1007/s10115-024-02269-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1969-1991},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An evidence-based approach for open-domain question answering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised multi-hop heterogeneous hypergraph embedding
with informative pooling for graph-level classification. <em>KIS</em>,
<em>67</em>(2), 1945–1968. (<a
href="https://doi.org/10.1007/s10115-024-02259-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heterogeneous graph analysis, existing self-supervised learning (SSL) methods face several key challenges. Primarily, these approaches are tailored for node-level tasks and fail to effectively capture global graph-level features, a crucial aspect for comprehensive graph understanding. Furthermore, they predominantly rely on meta-path-based techniques to unravel graph structures, a process that can be computationally intensive and often intractable for complex networks. Another significant limitation is their inability to account for nonpairwise relationships, a common characteristic in real-world networks like protein-protein interaction and collaboration networks, limiting their effectiveness in graph-level learning where high-order connectivity is essential. To address these issues, we propose an innovative SSL framework for heterogeneous hypergraph embedding, expressly designed to enhance graph-level classification. Our framework introduces multi-hop attention in hypergraph convolution, a significant leap from existing attention mechanisms specifically for hypergraphs that primarily focus on immediate neighborhoods. This multi-hop approach allows for an expansive capture of relational structures, both near and far, uncovering intricate patterns integral to accurate graph-level classification. Complementing this, we implement an informative graph-level attentive pooling mechanism that surpasses traditional aggregation methods. It intelligently synthesizes features, taking into account their structural and semantic importance within the hypergraph, thereby preserving critical contextual information. Furthermore, we refine our contrastive learning approach and introduce targeted negative sampling strategies, creating a more robust learning environment that excels at discerning nuanced graph-level features. Rigorous evaluation against established graph kernels, graph neural networks, and graph pooling methods on real-world datasets demonstrates our model’s superior performance, validating its effectiveness in addressing the complexities inherent in heterogeneous graph-level classification.},
  archive      = {J_KIS},
  author       = {Hayat, Malik Khizar and Xue, Shan and Yang, Jian},
  doi          = {10.1007/s10115-024-02259-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1945-1968},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Self-supervised multi-hop heterogeneous hypergraph embedding with informative pooling for graph-level classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oversampling algorithm for high-dimensional imbalanced
learning with class overlapping. <em>KIS</em>, <em>67</em>(2),
1915–1943. (<a
href="https://doi.org/10.1007/s10115-024-02276-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing standard learning methods suffer from poor performance in high-dimensional imbalanced learning with class overlapping. To tackle this problem, we propose a novel oversampling algorithm that aims to generate a robust ensemble of manifold dimensionality reduction, grid clustering, and information entropy criteria. Instead of simply balancing positive and negative numbers, the algorithm considers the difference in information entropy for interclass, which first reduces the dimensionality by manifold reduction, and then group data utilize grid clustering. Subsequently, calculate the oversampling weight of each group by information entropy and find seed samples based on entropy and neighborhood. Finally, SMOTE based on Beta distribution combined with standard classifiers achieve the rapid and precise classification for high-dimensional imbalanced datasets with class overlapping. Extensive experimental results on 20 real-world imbalanced datasets and compared with eight popular oversampling algorithms show that our proposed algorithm, while achieving good performance in terms of F-measure, G-mean, and AUPRC, can lead to robust performance under high-dimensional and overlapping. It is worth noting that our algorithm substantially reduces the number of synthetic samples against the quantity-balanced oversampling algorithms, and significantly reduces the generation of class overlapping.},
  archive      = {J_KIS},
  author       = {Yang, Xu and Xue, Zhen and Zhang, Liangliang and Wu, Jianzhen},
  doi          = {10.1007/s10115-024-02276-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1915-1943},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An oversampling algorithm for high-dimensional imbalanced learning with class overlapping},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on federated learning system: A new
paradigm to machine learning. <em>KIS</em>, <em>67</em>(2), 1811–1914.
(<a href="https://doi.org/10.1007/s10115-024-02257-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a machine learning technique that permits clients to train the model at a local site in a collaborative manner. It builds a global shared model on the basis of updates of the local model without exchanging data among multiple devices. Federated learning was introduced in 2016 with the goal of enabling local training as well as distributed machine learning training at the edge node’s level. It plays a vital role in terms of preserving the privacy of data while training the machine learning model on multiple devices. However, the introduction of federated learning into real-world applications exposes certain challenges in the training process, which affect the overall efficacy and efficiency of the federated learning model in real-world scenarios. As a result, an increasing number of researchers are now focusing on tackling the issues of FL and exploring various efficient research approaches to overcome these current obstacles. This paper systematically provides a detailed overview of federated learning, covering its definition, the need behind its development, privacy concepts, characteristics, and brief knowledge regarding different system components of federated learning. Different open-source frameworks that are available and used for implementing and solving problems related to federated learning have also been addressed in this article. Beyond this, the taxonomy of federated learning systems and different architectures for the same have also been discussed. In this paper, a brief comparison of related concepts with federated learning and a comparison among existing and popular federated learning studies proposed in different articles in the area of federated learning have also been summarized. In addition to the above-stated information, this article also provides brief information and a summary of various application areas of federated learning. Lastly, this paper briefly addresses the different challenges and prospects of research that lead to progress in this field.},
  archive      = {J_KIS},
  author       = {Chaudhary, Rajesh Kumar and Kumar, Ravinder and Saxena, Nitin},
  doi          = {10.1007/s10115-024-02257-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1811-1914},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A systematic review on federated learning system: A new paradigm to machine learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMNE: Link prediction using hypergraph motifs and network
embedding in social networks. <em>KIS</em>, <em>67</em>(2), 1787–1809.
(<a href="https://doi.org/10.1007/s10115-024-02255-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embeddings, which map nodes to low-dimensional vectors, facilitate link prediction, a pivotal aspect of complex network research. However, existing methods often overlook the complexities of hypergraphs and potent structures for modeling intricate relationships among multiple entities. This paper delves into link prediction within hypergraph motifs and network embedding (HMNE), crucial for diverse fields like knowledge graphs and bioinformatics. HMNE employs motifs to perform network embedding, representing nodes as hyper-nodes. HMNE utilizes the skip-gram model to get the embedding vectors by analyzing the sequence generated using a local random walk technique. Additionally, we consider hyper-motifs as super-nodes to highlight structural similarities between nodes. To further refine our methodology, we use the depth and breadth motif random walk strategy on the embedded network with hyper-nodes. This innovative approach enriches our understanding of network dynamics and enhances the predictive power of our model. We have thoroughly experimented the proposed method on several real-world datasets, and the results consistently demonstrate its usefulness.},
  archive      = {J_KIS},
  author       = {Zhang, Yichen and Lai, Shouliang and Peng, Zelu and Rezaeipanah, Amin},
  doi          = {10.1007/s10115-024-02255-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1787-1809},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HMNE: Link prediction using hypergraph motifs and network embedding in social networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label classification with label clusters.
<em>KIS</em>, <em>67</em>(2), 1741–1785. (<a
href="https://doi.org/10.1007/s10115-024-02270-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is the task of simultaneously predicting a set of labels for an instance, with global and local being the two predominant approaches. The global approach trains a single classifier to handle all classes simultaneously, while the local approach breaks down the problem into multiple binary problems. Despite extensive research, effectively capturing label correlations remains a challenge in both methods. In this paper, we introduce an approach that clusters the label space to create hybrid partitions (disjoint correlated label clusters), striking a balance between global and local strategies while leveraging both advantages. Our approach consists of (i) clustering the label space based on correlations, (ii) generating and validating the resulting hybrid partitions, (iii) selecting the best partitions, and (iv) evaluating their performance. We also compare our approach against an oracle, exhaustive search, and random search to assess how closely our hybrid partitions approximate the best possible partitions. The oracle selects the best partition using the test set, while the exhaustive approach relies on validation data. Experiments conducted on multiple multi-label datasets demonstrate that our method, along with random partitions, achieves results that are superior or competitive compared to traditional global and local approaches, as well as the state-of-the-art Ensemble of Classifier Chains. These findings suggest that conventional methods may not fully capture label correlations, and clustering the label space offers a promising solution.},
  archive      = {J_KIS},
  author       = {Gatto, Elaine Cecília and Ferrandin, Mauri and Cerri, Ricardo},
  doi          = {10.1007/s10115-024-02270-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1741-1785},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-label classification with label clusters},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEGODE: A structure-enhanced graph neural ordinary
differential equation network model for temporal link prediction.
<em>KIS</em>, <em>67</em>(2), 1713–1740. (<a
href="https://doi.org/10.1007/s10115-024-02261-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of temporal link prediction is to forecast potential future connections in a network by analyzing its structural underpinnings and tracking its temporal dynamics. However, existing methods for temporal link prediction are overly reliant on the most recent snapshots of the network, thereby limiting their ability to uncover and utilize the fundamental evolutionary patterns for effective dynamical inference. As a result, the predictive prowess of these models tends to be heightened for proximate future scenarios, as opposed to those farther into the horizon. Furthermore, the majority of the current methodologies overlook the influence of intricate higher-order and overarching structural dynamics, which could potentially enhance predictive accuracy. To tackle these challenges, we introduce a structure-enhanced graph neural ordinary differential equation (SEGODE), a comprehensive framework that leverages neural ordinary differential equations integrated with attention mechanisms to facilitate dynamic inference. The framework enhances the ability to snatch higher-order and global structures. To substantiate the viability of our novel model, we embarked on a comprehensive set of experiments conducted on seven real datasets. The outcomes of these rigorous tests demonstrate that our SEGODE approach not just demonstrates commendable performance in the task of link prediction but additionally has good results even when data is sparse.},
  archive      = {J_KIS},
  author       = {Fu, Jiale and Guo, Xuan and Hou, Jinlin and Yu, Wei and Shi, Hongjin and Zhao, Yanxia},
  doi          = {10.1007/s10115-024-02261-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1713-1740},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SEGODE: A structure-enhanced graph neural ordinary differential equation network model for temporal link prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved affinity propagation clustering algorithms: A
PSO-based approach. <em>KIS</em>, <em>67</em>(2), 1681–1711. (<a
href="https://doi.org/10.1007/s10115-024-02260-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional clustering algorithms such as K-means need to input the number of clusters before the start of the algorithm. Affinity propagation (AP) clustering algorithm solves this problem by considering each data point as a prospective cluster head (exemplar) and finding a set of appropriate exemplars by message passing. However, the AP clustering algorithm requires two parameters: preference and damping factor. Providing the parameters in advance poses the same issue faced in the traditional clustering algorithm. Moreover, all data points are not equally relevant for becoming cluster heads. To overcome these problems, we propose two parameter-free particle swarm optimization-based algorithms, PSO-APver1 and PSO-APver2. Furthermore, we introduce a novel version of mutant PSO where two cluster validity indices are used to judge the quality of the clustering solution. In PSO-APver2, we consider the internal data distribution using the square wave function to determine the initial preference value of data points. We conducted experiments on 8 real-world datasets to show the efficacy of our proposed algorithms over classic algorithms and two AP-based algorithms. We conducted the Friedman test followed by post hoc analysis to exhibit the significance of our work.},
  archive      = {J_KIS},
  author       = {Sinha, Ankita and Jana, Prasanta K.},
  doi          = {10.1007/s10115-024-02260-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1681-1711},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improved affinity propagation clustering algorithms: A PSO-based approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Additive consistency analysis for nested probabilistic
linguistic preference relations and its application in decision making.
<em>KIS</em>, <em>67</em>(2), 1651–1680. (<a
href="https://doi.org/10.1007/s10115-024-02231-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making problems often involve complex and uncertain evaluation criteria, making linguistic variables more practical than precise numerical values for describing information. Among various linguistic terms, the nested probabilistic linguistic term excels at expressing multidimensional information through its dual-layer structure, which captures both ordinal and nominal meanings. Using NPLTs to make expressions, this paper introduces nested probabilistic linguistic preference relations (NPLPRs) for effectively conveying preference information in decision-making scenarios and provides new operational rules for quantitative computations. To ensure the consistency of the preference relations, we define a consistency index and threshold to assess the additive consistency of NPLPRs and innovatively propose an automatic iteration algorithm to improve NPLPRs with unacceptable consistency. To reflect the evaluation focus and obtain the final results, we consider the weights of nominal terms when aggregating preferences with acceptable consistency. Finally, an illustrative example of selecting the most environmentally sustainable city demonstrates the application and advantages of our approach, supported by comparative analyses.},
  archive      = {J_KIS},
  author       = {Xiao, Jinglin and Wang, Xinxin and Xu, Zeshui},
  doi          = {10.1007/s10115-024-02231-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1651-1680},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Additive consistency analysis for nested probabilistic linguistic preference relations and its application in decision making},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning and similarity measures for
requirements-driven composition of semantic web services. <em>KIS</em>,
<em>67</em>(2), 1627–1649. (<a
href="https://doi.org/10.1007/s10115-024-02244-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The composition of web service is the best chance provided by Service-Oriented Computing and Service-Oriented Architecture as it gives real competitive benefits for some industrial and technological actors via presenting them with the probability to guarantee fast and inexpensive improvement of collaborative and distributed software applications. Here, a novel technique is introduced, which contains several phases for better web services. At first, the requirements specification phase is enabled with a set of requirements such as non-functional and functional requirements. Next to the requirements specification stage, the discovery stage is enabled to choose the suitable web services that have high-matching profiles with the developer’s requirement set. Here, for a semantic matching algorithm, a new hybrid similarity measure is developed. Additionally, among the group of candidate services that the discovery phase returned, the best service is selected during the selection step. Then, hybrid Squeeze_Long Short-Term Memory (Squeeze_LSTM) is used for choosing the best service and it is designed by the formation of SqueezeNet and LSTM. The Semantic Web Services are finally implemented. The efficiency of the Squeeze_LSTM is evaluated and has achieved a superior precision of 0.909, recall of 0.890, and response time of 6.461S.},
  archive      = {J_KIS},
  author       = {Bhuvaneswari, A. and Sumathi, K. and Sarveshwaran, Velliangiri and Sivasangari, A.},
  doi          = {10.1007/s10115-024-02244-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1627-1649},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hybrid deep learning and similarity measures for requirements-driven composition of semantic web services},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A proposed real-time decision support platform for moroccan
fixed mining production systems. <em>KIS</em>, <em>67</em>(2),
1597–1626. (<a
href="https://doi.org/10.1007/s10115-024-02271-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the competition in the mining markets and the rapid evolution of customer requirements, the Moroccan mining group OCP (Office Chérifien des Phosphates) has been forced to improve the performance of its production systems. Thus, continuous performance improvement and optimization of production processes are prerequisites to remain competitive. However, in Morocco, data analytics-based mining process improvements do not fully utilize the data generated during process execution. They lack prescriptive methodologies, which is the major goal of this work, to translate analytic results into improvement actions. Indeed, we propose a new platform for optimizing the production processes of a Moroccan mine based on knowledge extraction from data, allowing mine managers to rapidly and continuously improve the performance of their production chains. The platform will be an effective and efficient tool for mining companies to generate prescriptive action recommendations during the execution of the processes.},
  archive      = {J_KIS},
  author       = {Battas, Ilham and Behja, Hicham and El Ouazguiti, Mohamed},
  doi          = {10.1007/s10115-024-02271-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1597-1626},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A proposed real-time decision support platform for moroccan fixed mining production systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving density peak clustering on multi-dimensional time
series: Rediscover and subdivide. <em>KIS</em>, <em>67</em>(2),
1573–1596. (<a
href="https://doi.org/10.1007/s10115-024-02272-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density peak clustering (DPC) algorithm identifies patterns in high-dimensional data and obtains robust outcomes across diverse data types with minimal hyperparameters. However, DPC may produce inaccurate pattern sizes in multi-dimensional datasets and exhibit poor performance in recognizing similar patterns. To solve these issues, we propose the rediscover and subdivide density peak clustering algorithm (RSDPC), which follows three key strategies. The first strategy, rediscover, iteratively uncovers prominent patterns within the existing data. The second strategy, subdivide, partitions patterns into several similar subclasses. The third strategy, re-sort, rectifies errors from the preceding steps by incorporating critical distance and nearest distance considerations. The experimental results show that RSDPC is feasible and effective in synthetic and practical datasets compared with state-of-the-art works.},
  archive      = {J_KIS},
  author       = {Wang, Huina and Liu, Bo and Zhao, Huaipu and Qu, Guangzhi},
  doi          = {10.1007/s10115-024-02272-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1573-1596},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving density peak clustering on multi-dimensional time series: Rediscover and subdivide},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A customized balanced-objective genetic algorithm for task
scheduling in reconfigurable computing systems. <em>KIS</em>,
<em>67</em>(2), 1541–1571. (<a
href="https://doi.org/10.1007/s10115-024-02268-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfiguration and hardware implementation capabilities in reconfigurable computing (RC) systems make them more appropriate to recent computationally intensive applications. However, reaching optimal resource utilization remained as one of the main challenges in these systems. In order to implement more tasks in each reconfiguration interval, several decisive factors such as execution time, data communication cost, and required hardware resources must be analyzed simultaneously. In this paper, we proposed a novel balanced-objective task selector combined with a genetic algorithm to efficiently pick up the tasks of an application and occupy the resources as more as possible. The multi-objective fitness function of this algorithm adequately partitions the input application and provides the desirable intra and inter-cluster characteristics. Moreover, a new chromosome encoding technique has been developed to prevent precedence constraint violation of invalid solutions by removing forbidden regions in the search space. We classified the input applications with topological features such as first level parallel tasks (FLPT) and critical path length (CPL) for comprehensive evaluation. Several experiments are performed on randomly generated and real-world Directed Acyclic Graphs (DAGs), and the results are more satisfying in DAGs with more FLPTs and shorter CPLs where up to 28.63% makespan and 29.3% resource utilization improvement have been achieved in comparison with previous methods.},
  archive      = {J_KIS},
  author       = {Gholamrezanejad, Milad and Shahhoseini, Hadi Shahriar and Mohtavipour, Seyed Mehdi},
  doi          = {10.1007/s10115-024-02268-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1541-1571},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A customized balanced-objective genetic algorithm for task scheduling in reconfigurable computing systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSSN: A novel cache placement method based on adapted
shannon entropy and simple additive weighting method in named data
networking. <em>KIS</em>, <em>67</em>(2), 1507–1540. (<a
href="https://doi.org/10.1007/s10115-024-02266-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new strategy called PSSN to address the Cache Placement challenge in NDN. The paper first presents a novel clustering method based on the SAW decision-making approach and dynamically adapted Shannon weighting. Criteria such as the number of neighbors, hop count, router capacity, and CPU power are simultaneously considered for clustering and determining cluster heads. A key innovation in the proposed clustering is storing a copy of each content in each cluster to reduce duplicate content, increase content diversity, and consequently improve hit rate while reducing latency. Subsequently, for content placement, popularity of content, remaining router capacity, and hop count are analyzed concurrently using the SAW method adapted with the proposed approach. This ensures that popular content is placed closer to requesters. Throughout all stages of the method, the dynamic change in the status of content requests from users leads to a dynamic adjustment of the criteria weights. Simulation results using NDNsim demonstrate improvements in key parameters, with average enhancements of 17.8% and 9% for Hit rate and Delivery Time, respectively, as well as a 30.75% improvement in Load Balancing compared to recent methods.},
  archive      = {J_KIS},
  author       = {Soltani, Mohammad and Barekatain, Behrang and Hendessi, Faramarz and Beheshti, Zahra},
  doi          = {10.1007/s10115-024-02266-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1507-1540},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PSSN: A novel cache placement method based on adapted shannon entropy and simple additive weighting method in named data networking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-aware symmetric non-negative matrix factorization
for short text clustering. <em>KIS</em>, <em>67</em>(2), 1481–1506. (<a
href="https://doi.org/10.1007/s10115-024-02226-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text clustering is a significant yet challenging task, where short texts generated from the Internet are extremely sparse, noisy, and ambiguous. The sparse nature makes traditional clustering methods, e.g., k-means family and topic modeling, much less effective. Fortunately, recent arts of document distance, e.g., word mover’s distance, and document representation, e.g., BERT, can accurately measure the similarities of short texts, especially their nearest neighbors. Inspired by those arts and observations, we induce short text clusters by directly factorizing the informative affinity matrix of nearest neighbors into the product of the cluster assignment matrix, following the intuition that neighboring short texts tend to be assigned to the same cluster. However, due to the noisy nature of short texts, many of them can be regarded as outliers or near outliers, resulting in many noisy neighboring similarities within the affinity matrix. To further alleviate this problem, we enhance the affinity matrix factorization by (1) incorporating a sparse noisy matrix to directly capture noisy neighboring similarities and (2) regularizing the cluster assignment matrix by $$\ell _{2,1}$$ norm to eliminate hard-to-clustering short texts (called pseudo-outliers), so as to indirectly neglect noisy neighboring similarities corresponding to them. After this factorization for pre-clustering, we train a classifier over the resulting clusters and adopt it to assign each pseudo-outlier to one cluster finally. We call this novel clustering method as anomaly-aware symmetric non-negative matrix factorization ( $$\hbox {A}^{2}$$ snmf). Experimental results on benchmark short text datasets demonstrate that $$\hbox {A}^{2}$$ snmf performs very competitively with the existing baseline methods. The code is available at the website https://github.com/wizardbo/A3SNMF_functions .},
  archive      = {J_KIS},
  author       = {Li, Ximing and Guan, Yuanyuan and Fu, Bo and Luo, Zhongxuan},
  doi          = {10.1007/s10115-024-02226-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1481-1506},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Anomaly-aware symmetric non-negative matrix factorization for short text clustering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CETra: Online cluster tracking for clustering of streaming
data sources. <em>KIS</em>, <em>67</em>(2), 1455–1479. (<a
href="https://doi.org/10.1007/s10115-024-02267-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream clustering tasks may be applied to cluster streaming data objects (clustering by examples) or to cluster streaming data sources based on their temporal behavior (clustering by variables). We focus on the latter problem and propose CETra (Cluster evolution tracker)—the first online cluster tracking technique designed to provide information regarding cluster evolution in a streaming scenario of clustering by variables with efficient processing suitable for real-time problems. CETra can trace different intra and inter-cluster changes by considering not only statistics of interest but also the clusters’ membership, thus allowing a deeper understanding of the clustering results. Experimental evaluation using synthetic datasets and real data from meteorological sensors shows that CETra can track abrupt and gradual cluster transitions, while the competing method misses most of the gradual changes. Furthermore, CETra performs efficiently in a clustering environment for multiple streaming data sources, twice as fast as the related method.},
  archive      = {J_KIS},
  author       = {Sousa Lima, Afonso Matheus and de Sousa, Elaine Parros Machado},
  doi          = {10.1007/s10115-024-02267-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1455-1479},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CETra: Online cluster tracking for clustering of streaming data sources},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A path-based distance computation for non-convexity with
applications in clustering. <em>KIS</em>, <em>67</em>(2), 1415–1453. (<a
href="https://doi.org/10.1007/s10115-024-02275-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms are essential in data analysis, but evaluating their performance is challenging when the true labels are not available, especially for non-convex clusters. Traditional performance evaluation metrics struggle to identify clustering quality, often assigning higher scores for linearly separated clusters than the true clusters. We propose an original approach to distance computation that accounts for the data structure, thus improving the clustering quality evaluation for non-convex clusters without affecting other shapes of clusters. We also showcase the applicability of this method through a modified version of K-Means using the proposed method that is capable of correctly separating non-convex clusters. The validation included the analysis of performance and time complexity of 3 traditional clustering quality evaluation metrics and the K-Means clustering algorithm against their augmented versions with the proposed approach. This analysis conducted on 7 benchmark synthetic datasets and 6 real datasets with various numbers of examples and features of diverse characteristics and joint complexities: simple convex clusters, overlapped and imbalanced clusters, and non-convex clusters. Through these analyses, we show the ineffectiveness of traditional methods and that the proposed approach overcomes the weaknesses of traditional methods.},
  archive      = {J_KIS},
  author       = {Ardelean, Eugen-Richard and Portase, Raluca Laura and Potolea, Rodica and Dînșoreanu, Mihaela},
  doi          = {10.1007/s10115-024-02275-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1415-1453},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A path-based distance computation for non-convexity with applications in clustering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier detection in classification based on
feature-selection-based regression. <em>KIS</em>, <em>67</em>(2),
1399–1414. (<a
href="https://doi.org/10.1007/s10115-024-02264-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An outlier is a datum that is far from other data points in which it occurs. The appearance of outliers results in a complexity to obtain an accurate classification; numerous statistical and machine learning methods have been proposed to identify the outliers. This paper devotes a regression-based algorithm to the detection and identification of outlier before selecting a suitable classifier. The problem is firstly converted to an high-dimensional regression, then a novel method, based on the combination of multiple-correlation-coefficient-based feature selection for dimensional reduction and t-test for sparsification, is proposed, and an iterated algorithm is also given. Performance on simulated numerical data, low-dimensional iris data and high-dimensional DBWorld E-mail data demonstrate the superiority of the proposed method in outlier identification for classification.},
  archive      = {J_KIS},
  author       = {Su, Jinxia and Liu, Qiwen and Cui, Jingke},
  doi          = {10.1007/s10115-024-02264-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1399-1414},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Outlier detection in classification based on feature-selection-based regression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An energy-aware migration framework using metaheuristic
algorithm in cloud computing. <em>KIS</em>, <em>67</em>(2), 1373–1398.
(<a href="https://doi.org/10.1007/s10115-024-02224-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pervasive computing requires dynamic, energy-efficient cloud architecture to deploy services in VMs to distributed computing nodes. This mapping must ensure the service level agreement (SLA) runs VMs without disruption. This paper presents an energy-efficient metaheuristic Rock Hyrax algorithm-based cloud VM migration architecture. The male Rock Hyraxes find food and ensure the colony’s safety. His behavior has been mimicked in our proposed algorithm for finding energy-efficient compute nodes. A multi-objective VM migration function considers job submission deadlines. The proposed method reduces SLA violations and energy utilization while optimizing resource utilization. When evaluating the project, makespan, energy efficiency, and SLA violations were considered. The proposed algorithm is simulated on the CloudSim simulator considering both resources and jobs dynamic in nature. The suggested strategy outperforms ant colony optimization, particle swarm optimization, cuckoo optimization, and modified gray wolf optimization. The migration technique improves resource utilization by 18%, makespan time by 5%, SLA violation by 13%, and energy usage by 15%.},
  archive      = {J_KIS},
  author       = {Singhal, Saurabh and Sharma, Ashish},
  doi          = {10.1007/s10115-024-02224-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1373-1398},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An energy-aware migration framework using metaheuristic algorithm in cloud computing},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A timely personalized comments generation assistant based on
LSTM-SNP. <em>KIS</em>, <em>67</em>(2), 1351–1372. (<a
href="https://doi.org/10.1007/s10115-024-02198-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Posting personalized comments on an upcoming hot topic in time is very meaningful, which not only likely to attract more users to participate, but also affects other users’ point of view. LSTM-SNP is a variant of long short-term memory (LSTM) inspired by the nonlinear spiking mechanism in nonlinear spiking neural systems. In order to improve the efficiency and diversity of user-edited comments, we design a novel assistant based on the LSTM-SNP model. The assistant consists of two modules, one for predicting topic hotness and the other for generating comments with personalized expression features based on blog post and user information. Experimental results show that, this novel assistant not only predicts the upcoming hot topics accurately, but also outperforms the baseline model in terms of automatic evaluation and human discernment of comment generation. More importantly, the generated comments excel in terms of timeliness and personalization.},
  archive      = {J_KIS},
  author       = {Li, Yixiao and Wu, Yue and Li, Wenjia and Chen, Hui and Chen, Qi and Li, Yuehui},
  doi          = {10.1007/s10115-024-02198-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1351-1372},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A timely personalized comments generation assistant based on LSTM-SNP},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IC-SNI: Measuring nodes’ influential capability in complex
networks through structural and neighboring information. <em>KIS</em>,
<em>67</em>(2), 1309–1350. (<a
href="https://doi.org/10.1007/s10115-024-02262-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influential nodes are the important nodes that most efficiently control the propagation process throughout the network. Among various structural-based methods, degree centrality, k-shell decomposition, or their combination identify influential nodes with relatively low computational complexity, making them suitable for large-scale network analysis. However, these methods do not necessarily explore nodes’ underlying structure and neighboring information, which poses a significant challenge for researchers in developing timely and efficient heuristics considering appropriate network characteristics. In this study, we propose a new method (IC-SNI) to measure the influential capability of the nodes. IC-SNI minimizes the loopholes of the local and global centrality and calculates the topological positional structure by considering the local and global contribution of the neighbors. Exploring the path structural information, we introduce two new measurements (connectivity strength and effective distance) to capture the structural properties among the neighboring nodes. Finally, the influential capability of a node is calculated by aggregating the structural and neighboring information of up to two-hop neighboring nodes. Evaluated on nine benchmark datasets, IC-SNI demonstrates superior performance with the highest average ranking correlation of 0.813 with the SIR simulator and a 34.1% improvement comparing state-of-the-art methods in identifying influential spreaders. The results show that IC-SNI efficiently identifies the influential spreaders in diverse real networks by accurately integrating structural and neighboring information.},
  archive      = {J_KIS},
  author       = {Nandi, Suman and Curado Malta, Mariana and Maji, Giridhar and Dutta, Animesh},
  doi          = {10.1007/s10115-024-02262-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1309-1350},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {IC-SNI: Measuring nodes’ influential capability in complex networks through structural and neighboring information},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised feature selection with minimal redundancy
based on group optimization strategy for multi-label data. <em>KIS</em>,
<em>67</em>(2), 1271–1308. (<a
href="https://doi.org/10.1007/s10115-024-02258-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of intelligence technology, high-dimensional multi-label data exist in practical applications, which makes multi-label learning a significant challenge. Feature selection can obtain more distinguishable features to enhance recognition ability to address high-dimensional problems. Nowadays, most researchers usually evaluate the relevance between labels and features and the similarity between samples. They only focus on the local characteristics of samples without considering the global characteristics. To solve the above problems, in this paper, a novel feature selection approach for semi-supervised learning with minimal redundancy and group optimization strategy (SFGR) in multi-label scenario is proposed. First, a measure based on the Laplacian score and constrain score is utilized to evaluate the relevance between each feature and label. Meanwhile, the global structure of the data is considered via the creation of graphs and a priori information to obtain the feature subset with the highest relevance. Secondly, an optimization iteration algorithm based on a regularization term combining $$\text {l}_1$$ -norm and $$\text {l}_2$$ -norm is employed to ensure the sparsity of the feature weight matrix and minimize the redundancy. Moreover, a group optimal strategy is applied as a global search approach to fusion the feature subsets to obtain an approximate globally optimal feature subset. Eventually, experimental results on various multi-labeled datasets show that SFGR can perform better than other algorithms.},
  archive      = {J_KIS},
  author       = {Qing, Depeng and Zheng, Yifeng and Zhang, Wenjie and Ren, Weishuo and Zeng, Xianlong and Li, Guohe},
  doi          = {10.1007/s10115-024-02258-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1271-1308},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semi-supervised feature selection with minimal redundancy based on group optimization strategy for multi-label data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All is attention for multi-label text classification.
<em>KIS</em>, <em>67</em>(2), 1249–1270. (<a
href="https://doi.org/10.1007/s10115-024-02253-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification(MLTC) is a key task in natural language processing. Its challenge is to extract latent semantic features from text and effectively exploit label-associated features. This work proposes an MLTC model driven solely by attention mechanisms, which includes Graph Attention(GA), Class-Specific Attention(CSA), and Multi-Head Attention(MHA) modules. The GA module examines and records label dependencies by considering label semantic features as attributes of graph nodes. It uses graph embedding to maintain structural relationships within the label graph. Meanwhile, the CSA module produces distinctive features for each category by utilizing spatial attention scores, thereby improving classification accuracy. Then, the MHA module facilitates extensive feature interactions, enhancing the expressiveness of text features and supporting the handling of long-range dependencies. Experimental evaluations conducted on two MLTC datasets show that our proposed model outperforms existing MLTC algorithms, achieving state-of-the-art performance. These results highlight the effectiveness of our attention-based approach in tackling the complexity of MLTC tasks.},
  archive      = {J_KIS},
  author       = {Liu, Zhi and Huang, Yunjie and Xia, Xincheng and Zhang, Yihao},
  doi          = {10.1007/s10115-024-02253-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1249-1270},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {All is attention for multi-label text classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoRBS: A dynamic storytelling algorithm using a novel
contextualization approach for documents utilizing BERT features.
<em>KIS</em>, <em>67</em>(2), 1213–1248. (<a
href="https://doi.org/10.1007/s10115-024-02263-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storytelling is the process of connecting documents one after another representing the evolution of an event. Existing algorithms for storytelling connect events based on content overlaps between consecutive documents ignoring the role of the same term in different documents and the contemporary contexts (e.g., dynamic embeddings) of documents and terms. Due to the lack of role and contemporary contexts in the designs of the existing storytelling methods, the resultant stories frequently jump to documents with the keywords to form a chain but not a meaningful one. In this paper, we present a novel storytelling algorithm—Contextual Role-Based Storytelling (CoRBS)—that generates a chain of documents explaining the evolution of an event, addressing role and contemporary context issues of existing methods. CoRBS starts with a given document and moves forward temporally, stitching together role and context-driven documents to represent the evolution of the events that appear in the first document. We define the role of a term in a document as a distribution of similarities of the nearest neighbors of the term based on BERT embeddings of all terms of that document. Contemporary contexts are incorporated as a mechanism to discover a coherent next document while the story progresses. Our experiments demonstrate that CoRBS generates more meaningful stories compared to other baseline storytelling techniques.},
  archive      = {J_KIS},
  author       = {Nouri, Alireza and Hossain, M. Shahriar},
  doi          = {10.1007/s10115-024-02263-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1213-1248},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CoRBS: A dynamic storytelling algorithm using a novel contextualization approach for documents utilizing BERT features},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact lossy compression of tensors via neural tensor-train
decomposition. <em>KIS</em>, <em>67</em>(2), 1169–1211. (<a
href="https://doi.org/10.1007/s10115-024-02252-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TensorCodec, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions.TensorCodec incorporates three key ideas. The first idea is neural tensor-train decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be exploited by NTTD for improved approximation. In addition, we extend TensorCodec to enable the lossy compression of tensors with missing entries, often found in real-world datasets. Our analysis and experiments on 8 real-world datasets demonstrate that TensorCodec is (a) Concise: it gives up to $$7.38 \times $$ more compact compression than the best competitor with similar reconstruction error, (b) Accurate: given the same budget for compressed size, it yields up to $$3.33\times $$ more accurate reconstruction than the best competitor, (c) Scalable: Its empirical compression time is linear in the number of tensor entries, and it reconstructs each entry in logarithmic time. Our code and datasets are available at https://github.com/kbrother/TensorCodec .},
  archive      = {J_KIS},
  author       = {Kwon, Taehyung and Ko, Jihoon and Jung, Jinhong and Jang, Jun-Gi and Shin, Kijung},
  doi          = {10.1007/s10115-024-02252-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1169-1211},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Compact lossy compression of tensors via neural tensor-train decomposition},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book recommendation using sentiment analysis and ensembling
hybrid deep learning models. <em>KIS</em>, <em>67</em>(2), 1131–1168.
(<a href="https://doi.org/10.1007/s10115-024-02250-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An immense volume of user-generated content exists online due to the exponential growth of internet usage among individuals. However, this abundance presents substantial challenges for recommendation systems and online customer services. The diverse data, encompassing consumer emails, reviews, and comments, contain a broad spectrum of information, making it challenging to decipher the underlying emotions and nuances. In social media analytics, sentiment analysis has emerged as a pivotal tool to unveil the emotional context embedded within textual data, offering deeper insights into user attitudes, opinions, and sentiments. This study introduces a novel strategy to strengthen the performance and precision of sentiment analysis-based book recommendation systems through ensemble learning on hybrid deep learning models. These book recommendation models leverage customer ratings and reviews from a vast Amazon books dataset as input. Initially, we used TextBlob to assess the polarity of customer reviews, categorizing them into neutral, negative, and positive sentiments. Subsequently, the input data underwent preprocessing, tokenization, and word embedding using bidirectional encoder representations from transformers (BERT). To effectively analyze and filter processed review comments and ratings, the proposed ensemble model integrates a diverse array of hybrid deep learning architectures, including long short-term memory (LSTM), bidirectional LSTM (BiLSTM), gated recurrent unit (GRU), and convolutional neural network (CNN). Extensive experimentation validated the superiority of the proposed ensemble model, achieving an impressive accuracy and F1-score of 98.21%. The significance of the approach lies in its ability to provide more accurate and contextually relevant book recommendations by considering the nuanced emotions expressed in customer reviews. This contributes to enhancing user satisfaction and engagement with recommendation systems, ultimately improving the overall quality of personalized book suggestions. Evaluation metrics further validate the efficacy of the proposed model, underscoring its practical utility in real-world applications of sentiment-based book recommendation systems.},
  archive      = {J_KIS},
  author       = {Devika, P. and Milton, A.},
  doi          = {10.1007/s10115-024-02250-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1131-1168},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Book recommendation using sentiment analysis and ensembling hybrid deep learning models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion pattern mining. <em>KIS</em>, <em>67</em>(2),
1101–1129. (<a
href="https://doi.org/10.1007/s10115-024-02254-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a diffusion network, some nodes exhibit similar diffusion patterns as they have analogous influence reachabilities to the other nodes. When these nodes are selected as initially infected nodes, they tend to yield similar infection results. Mining diffusion patterns of nodes is of practical significance in various applications, such as online marketing and epidemic prevention. Nonetheless, few existing work has effectively addressed this problem. In this work, we investigate how to find out which nodes in a diffusion network share similar diffusion pattern based only on historical infection results. Toward this, we first reconstruct the structure of influence relationships in the network, and then infer the infection propagation probability on each influence relationship, based on which the influence reachability of each node can be estimated. We present a diffusion pattern similarity metric to quantify the similarity of influence reachabilities, and group nodes that share similar influence reachabilities via hierarchical clustering. Extensive experimental results on both synthetic and real-world networks verify the effectiveness and efficiency of our approach.},
  archive      = {J_KIS},
  author       = {Yan, Qian and Yang, Yulan and Yin, Kai and Gan, Ting and Huang, Hao},
  doi          = {10.1007/s10115-024-02254-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1101-1129},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Diffusion pattern mining},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward deep multi-view document clustering using enhanced
semantic embedding and consistent context semantics. <em>KIS</em>,
<em>67</em>(2), 1073–1100. (<a
href="https://doi.org/10.1007/s10115-024-02249-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view document clustering (MVDC) is a sophisticated approach in natural language processing that leverages multiple representations or views of data to improve clustering performance. Existing solutions are challenging due to inconsistency of document views, high dimensions, and sparseness in text documents. On the other hand, existing MVDC-based methods often depend on the performance of bag-of-words and pretrained language models. However, these models usually do not consider contextual semantics and are suitable for single-view document clustering. This paper addresses these challenges by proposing a deep MVDC model that utilizes enhanced semantic embedding and consistent context semantics (SECS). SECS uses semantic embedding to address high-dimensional challenges by considering complementary semantic information. Meanwhile, SECS takes advantage of the potential benefits of view-consistent context semantics based on pretrained language models. The proposed model captures intricate semantic relationships between words and documents through advanced embedding techniques, ensuring a richer and more nuanced representation of textual content. Furthermore, by incorporating consistent context semantics, SECS maintains contextual integrity across multiple views, leading to more coherent and meaningful clusters. Experimental results on benchmark datasets demonstrate the superiority of our model over state-of-the-art MVDC methods, highlighting its effectiveness in improving clustering quality and interpretability.},
  archive      = {J_KIS},
  author       = {Du, Yongsheng and Sun, Hongwei and Abdollahi, MohammadJavad},
  doi          = {10.1007/s10115-024-02249-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1073-1100},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Toward deep multi-view document clustering using enhanced semantic embedding and consistent context semantics},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Twitter sentiment analysis using ensemble of multi-channel
model based on machine learning and deep learning techniques.
<em>KIS</em>, <em>67</em>(2), 1045–1071. (<a
href="https://doi.org/10.1007/s10115-024-02256-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People post a lot of comments on the websites these days, as social media and the Internet are major parts of modern life. With so much information available online related to services, products, politics, stocks, etc., thus, using artificial intelligence to understand the emotions in these comments is highly beneficial for understanding public opinion. In particular, detecting sentiment polarity in customer reviews is crucial for businesses to make informed decisions. Despite the vast amount of information available on the internet, understanding the underlying emotions in user comments remains a challenge. Our work aims to bridge this gap by proposing a sophisticated sentiment analysis model that leverages state-of-the-art deep learning techniques. In this study, we present a sentiment analysis model that combines advanced deep learning neural networks: convolutional neural network, long short-term memory networks (LSTM), Bidirectional LSTM (BiLSTM), and Bidirectional Encoder Representations from Transformers (BERT). Accurate feature extraction plays a pivotal role in sentiment analysis applications. By merging pre-trained BERT with sophisticated neural networks, the devised model achieves an impressive accuracy of 94.95%. We evaluated the proposed model on a publicly available Twitter Sentiment Analysis dataset. The proposed ensemble multi-channel model outperforms several deep learning and machine learning techniques in sentiment analysis. Hence, we suggest the use of the ensemble model to accurately determine sentiments from tweets and other textual data.},
  archive      = {J_KIS},
  author       = {Tembhurne, Jitendra V. and Lakhotia, Kirtan and Agrawal, Anant},
  doi          = {10.1007/s10115-024-02256-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1045-1071},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Twitter sentiment analysis using ensemble of multi-channel model based on machine learning and deep learning techniques},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic regex synthesis methods for english: A comparative
analysis. <em>KIS</em>, <em>67</em>(2), 1013–1043. (<a
href="https://doi.org/10.1007/s10115-024-02232-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular expressions (short form regex) find their application in program script synthesis, machine translation, information extraction and web applications, such as input validations. Their expressiveness and flexibility make them decidedly the best tool for many challenging text extraction tasks. Writing regex manually has been labeled as a laborious, time consuming and error prone task even for skilled programmers. An abundance of regex generation from text queries at online platforms mainly Stackoverflow and Quora signifies the automatic regex synthesis problem. Despite their popularity, a criminal lack of comprehensive literature study on the problem has also been observed. We intend to perform a detailed review of a variety of methods available for regex synthesis, repair, and learn beneficial lessons for appropriate datasets with one earnest goal: to synthesize resource efficient and correct regexes for given textual description.},
  archive      = {J_KIS},
  author       = {Tariq, Sadia and Rana, Toqir Ahmad},
  doi          = {10.1007/s10115-024-02232-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1013-1043},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Automatic regex synthesis methods for english: A comparative analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chromosome segmentation and classification: An updated
review. <em>KIS</em>, <em>67</em>(2), 977–1011. (<a
href="https://doi.org/10.1007/s10115-024-02243-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Karyotyping is a study of chromosomes to identify various chromosomal aberrations related to structure and number. Chromosome image analysis involves challenging issues related to overlapping and touching of chromosomes. Chromosome segmentation and classification generally focus on separating overlapping and touching chromosomes. The analysis methods start from conventional image processing methods to advanced machine learning techniques. These methods are broadly classified into low-level and high-level methods. The low-level methods are thresholding-based approaches, edge detection, feature extraction techniques like active contours and watershed approaches and machine learning for classification. The high-level methods are deep learning algorithms like convolutional neural networks (CNNs), U-Net, autoencoder architectures. These methods help in improving accuracy and automate the process of chromosome segmentation and classification. High-level approaches can handle complexity in chromosome overlaps which provides better segmentation results. The approach learns complicated patterns and structures of chromosome images, which helps in achieving better classification accuracy. The challenges are: (i) working on large and annotated dataset for training deep learning models and (ii) suffer issues with new dataset even in they perform better during training phase. The solution for all these can be a hybrid approach that combines conventional method with modern approaches. This survey gives readers a basic understanding of automated karyotyping and future direction in this domain.},
  archive      = {J_KIS},
  author       = {Somasundaram, Devaraj and Madian, Nirmala and Goh, Kam Meng and Suresh, S.},
  doi          = {10.1007/s10115-024-02243-y},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {977-1011},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chromosome segmentation and classification: An updated review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mam---12">MAM - 12</h2>
<ul>
<li><details>
<summary>
(2025). Effective human oversight of AI-based systems: A signal
detection perspective on the detection of inaccurate and unfair outputs.
<em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-024-09701-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legislation and ethical guidelines around the globe call for effective human oversight of AI-based systems in high-risk contexts – that is oversight that reliably reduces the risks otherwise associated with the use of AI-based systems. Such risks may relate to the imperfect accuracy of systems (e.g., inaccurate classifications) or to ethical concerns (e.g., unfairness of outputs). Given the significant role that human oversight is expected to play in the operation of AI-based systems, it is crucial to better understand the conditions for effective human oversight. We argue that the reliable detection of errors (as an umbrella term for inaccuracies and unfairness) is crucial for effective human oversight. We then propose that Signal Detection Theory (SDT) offers a promising framework for better understanding what affects people’s sensitivity (i.e., how well they are able to detect errors) and response bias (i.e., the tendency to report errors given a perceived evidence of an error) in detecting errors. Whereas an SDT perspective on the detection of inaccuracies is straightforward, we demonstrate its broader applicability by detailing the specifics for an SDT perspective on unfairness detection, including the need to choose a standard for (un)fairness. Additionally, we illustrate that an SDT perspective helps to better understand the conditions for effective error detection by showing examples of task-, system-, and person-related factors that may affect the sensitivity and response bias of humans tasked with detecting unfairness associated with the use of AI-based systems. Finally, we discuss future research directions for an SDT perspective on error detection.},
  archive      = {J_MAM},
  author       = {Langer, Markus and Baum, Kevin and Schlicker, Nadine},
  doi          = {10.1007/s11023-024-09701-0},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Effective human oversight of AI-based systems: A signal detection perspective on the detection of inaccurate and unfair outputs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How ChatGPT changed the media’s narratives on AI: A
semi-automated narrative analysis through frame semantics. <em>MAM</em>,
<em>35</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11023-024-09705-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform a mixed-method frame semantics-based analysis on a dataset of more than 49,000 sentences collected from 5846 news articles that mention AI. The dataset covers the twelve-month period centred around the launch of OpenAI’s chatbot ChatGPT and is collected from the most visited open-access English-language news publishers. Our findings indicate that during the six months succeeding the launch, media attention rose tenfold—from already historically high levels. During this period, discourse has become increasingly centred around experts and political leaders, and AI has become more closely associated with dangers and risks. A deeper review of the data also suggests a qualitative shift in the types of threat AI is thought to represent, as well as the anthropomorphic qualities ascribed to it.},
  archive      = {J_MAM},
  author       = {Ryazanov, Igor and Öhman, Carl and Björklund, Johanna},
  doi          = {10.1007/s11023-024-09705-w},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Minds Mach.},
  title        = {How ChatGPT changed the media’s narratives on AI: A semi-automated narrative analysis through frame semantics},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical learning theory and occam’s razor: The core
argument. <em>MAM</em>, <em>35</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s11023-024-09703-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning theory is often associated with the principle of Occam’s razor, which recommends a simplicity preference in inductive inference. This paper distills the core argument for simplicity obtainable from statistical learning theory, built on the theory’s central learning guarantee for the method of empirical risk minimization. This core “means-ends” argument is that a simpler hypothesis class or inductive model is better because it has better learning guarantees; however, these guarantees are model-relative and so the theoretical push towards simplicity is checked by our prior knowledge.},
  archive      = {J_MAM},
  author       = {Sterkenburg, Tom F.},
  doi          = {10.1007/s11023-024-09703-y},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Minds Mach.},
  title        = {Statistical learning theory and occam’s razor: The core argument},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence (AI) and global justice.
<em>MAM</em>, <em>35</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s11023-024-09708-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a philosophically informed and robust account of the global justice implications of Artificial Intelligence (AI). We first discuss some of the key theories of global justice, before justifying our focus on the Capabilities Approach as a useful framework for understanding the context-specific impacts of AI on low- to middle-income countries. We then highlight some of the harms and burdens facing low- to middle-income countries within the context of both AI use and the AI supply chain, by analyzing the extraction of materials, which includes mineral extraction and the environmental harms associated with it, and the extraction of labor, which includes unethical labor practices, low wages, and the trauma experienced by some AI workers. We then outline some of the potential harms and benefits that AI poses, how these are distributed, and what global justice implications this has for low- to middle-income countries. Finally, we articulate the global justice significance of AI by utilizing the Capabilities Approach. We argue that AI must be considered from a global justice perspective given that, globally, AI puts significant downward pressure on several elements of well-being thereby making it harder for people to achieve threshold levels of the central human capabilities needed for a life of dignity.},
  archive      = {J_MAM},
  author       = {Sahebi, Siavosh and Formosa, Paul},
  doi          = {10.1007/s11023-024-09708-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Minds Mach.},
  title        = {Artificial intelligence (AI) and global justice},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Standards for belief representations in LLMs. <em>MAM</em>,
<em>35</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s11023-024-09709-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief. Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations.},
  archive      = {J_MAM},
  author       = {Herrmann, Daniel A. and Levinstein, Benjamin A.},
  doi          = {10.1007/s11023-024-09709-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Minds Mach.},
  title        = {Standards for belief representations in LLMs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cheaper spaces. <em>MAM</em>, <em>35</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11023-024-09704-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity spaces are standardly constructed by collecting pairwise similarity judgments and subjecting those to a dimension-reduction technique such as multidimensional scaling or principal component analysis. While this approach can be effective, it has some known downsides, most notably, it tends to be costly and has limited generalizability. Recently, a number of authors have attempted to mitigate these issues through machine learning techniques. For instance, neural networks have been trained on human similarity judgments to infer the spatial representation of unseen stimuli. However, these newer methods are still costly and fail to generalize widely beyond their initial training sets. This paper proposes leveraging prebuilt semantic vector spaces as a cheap alternative to collecting similarity judgments. Our results suggest that some of those spaces can be used to approximate human similarity judgments at low cost and high speed.},
  archive      = {J_MAM},
  author       = {Moullec, Matthieu and Douven, Igor},
  doi          = {10.1007/s11023-024-09704-x},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {Cheaper spaces},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness in algorithmic profiling: The AMAS case.
<em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-024-09706-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a controversial application of algorithmic profiling in the public sector, the Austrian AMAS system. AMAS was supposed to help caseworkers at the Public Employment Service (PES) Austria to allocate support measures to job seekers based on their predicted chance of (re-)integration into the labor market. Shortly after its release, AMAS was criticized for its apparent unequal treatment of job seekers based on gender and citizenship. We systematically investigate the AMAS model using a novel real-world dataset of young job seekers from Vienna, which allows us to provide the first empirical evaluation of the AMAS model with a focus on fairness measures. We further apply bias mitigation strategies to study their effectiveness in our real-world setting. Our findings indicate that the prediction performance of the AMAS model is insufficient for use in practice, as more than 30% of job seekers would be misclassified in our use case. Further, our results confirm that the original model is biased with respect to gender as it tends to (incorrectly) assign women to the group with high chances of re-employment, which is not prioritized in the PES’ allocation of support measures. However, most bias mitigation strategies were able to improve fairness without compromising performance and thus may form an important building block in revising profiling schemes in the present context.},
  archive      = {J_MAM},
  author       = {Achterhold, Eva and Mühlböck, Monika and Steiber, Nadia and Kern, Christoph},
  doi          = {10.1007/s11023-024-09706-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Fairness in algorithmic profiling: The AMAS case},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Submarine cables and the risks to digital
sovereignty. <em>MAM</em>, <em>35</em>(1), 1. (<a
href="https://doi.org/10.1007/s11023-024-09707-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Ganz, Abra and Camellini, Martina and Hine, Emmie and Novelli, Claudio and Roberts, Huw and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09707-8},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1},
  shortjournal = {Minds Mach.},
  title        = {Correction: Submarine cables and the risks to digital sovereignty},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An app a day will (probably not) keep the doctor away: An
evidence audit of health and medical apps available on the apple app
store. <em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-025-09710-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are more than 350,000 health apps available in public app stores. The extolled benefits of health apps are numerous and well documented. However, there are also concerns that poor-quality apps, marketed directly to consumers, threaten the tenets of evidence-based medicine and expose individuals to the risk of harm. This study addresses this issue by assessing the overall quality of evidence publicly available to support the effectiveness claims of health apps marketed directly to consumers. To assess the quality of evidence available to the public to support the effectiveness claims of health apps marketed directly to consumers, an audit was conducted of a purposive sample of apps available on the Apple App Store. We find the quality of evidence available to support the effectiveness claims of health apps marketed directly to consumers to be poor. Less than half of the 220 apps (44%) we audited state that they have evidence to support their claims of effectiveness and, of these allegedly evidence-based apps, more than 70% rely publicly on either very low or low-quality evidence. For the minority of app developers that do publish studies, significant methodological limitations are commonplace. Finally, there is a pronounced tendency for apps—particularly mental health and diagnostic apps—to either borrow evidence generated in other (typically offline) contexts or to rely exclusively on unsubstantiated, unpublished user metrics as evidence to support their effectiveness claims. Health apps represent a significant opportunity for individual consumers and healthcare systems. Nevertheless, this opportunity will be missed if the health apps market continues to be flooded by poor quality, poorly evidenced, and potentially unsafe apps. It must be accepted that a continuing lag in generating high-quality publicly available evidence of app effectiveness and safety is not inevitable: it is a choice. Just because it will be challenging to raise the quality of the evidence base publicly available to support the claims of health apps, this does not mean that the bar for evidence quality should be lowered. Innovation for innovation’s sake must not be prioritized over public health and safety.},
  archive      = {J_MAM},
  author       = {Morley, Jessica and Laitila, Joel and Ross, Joseph S. and Schamroth, Joel and Zhang, Joe and Floridi, Luciano},
  doi          = {10.1007/s11023-025-09710-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {An app a day will (Probably not) keep the doctor away: An evidence audit of health and medical apps available on the apple app store},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChatGPT-4 in the turing test. <em>MAM</em>, <em>35</em>(1),
1–10. (<a href="https://doi.org/10.1007/s11023-025-09711-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been considerable optimistic speculation on how well ChatGPT-4 would perform in a Turing Test. However, no minimally serious implementation of the test has been reported to have been carried out. This brief note documents the results of subjecting ChatGPT-4 to 10 Turing Tests, with different interrogators and participants. The outcome is tremendously disappointing for the optimists. Despite ChatGPT reportedly outperforming 99.9% of humans in a Verbal IQ test, it falls short of passing the Turing Test. In 9 out of the 10 tests conducted, the interrogators successfully identified ChatGPT-4 and the human participant. The probability of obtaining this result from a process in which the interrogator is really no better than chance at correct identification is calculated to be less than 1%. An additional question was posed to the interrogators at the end of each test: What led them to distinguish between the human and the machine? The interrogators, who effectively filtered out ChatGPT-4 from passing the Turing Test for intelligence, stated that they could identify the machine because it, in effect, responded more intelligently than the human. Subsequently, ChatGPT-4 was tasked with differentiating syntax from semantics and self-corrected when falling for the fallacy of equivocation. The curious situation is arrived at that passing the Turing Test for intelligence remains a challenge that ChatGPT-4 has yet to overcome, precisely because, as per the interrogators, its intellectual abilities surpass those of individual humans.},
  archive      = {J_MAM},
  author       = {Restrepo Echavarría, Ricardo},
  doi          = {10.1007/s11023-025-09711-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Minds Mach.},
  title        = {ChatGPT-4 in the turing test},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On twelve shades of green: Assessing the levels of
environmental protection in the artificial intelligence act.
<em>MAM</em>, <em>35</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11023-025-09713-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper examines twelve legal regimes related to the governance and regulation of both the environmental risks and opportunities brought forth by the use of AI systems and AI models in the Artificial Intelligence Act (‘AIA’) of EU law. The assessment of risks and opportunities of AI related to the environment includes the high-risk management procedures under Art. 9 of the AIA, the “fundamental rights impact assessment” of Art. 27, and the codes of conduct of Art. 95. These provisions are supplemented by further regulatory regimes, such as the proposal of EU directive on sustainable consumption and green claims, and Reg. (EU) 2023/588 on environmental and space sustainability, among others. The aim of the analysis is to specify which are the less or the more environmentally friendly regulatory regimes set up with the AIA. The claim is that Art. 9, 27 and 95 are among the less green pieces of the whole legislation.},
  archive      = {J_MAM},
  author       = {Pagallo, Ugo},
  doi          = {10.1007/s11023-025-09713-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Minds Mach.},
  title        = {On twelve shades of green: Assessing the levels of environmental protection in the artificial intelligence act},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The testimony gap: Machines and reasons. <em>MAM</em>,
<em>35</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11023-025-09712-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most people who have considered the matter have concluded that machines cannot be moral agents. Responsibility for acting on the outputs of machines must always rest with a human being. A key problem for the ethical use of AI, then, is to ensure that it does not block the attribution of responsibility to humans or lead to individuals being unfairly held responsible for things over which they had no control. This is the “responsibility gap”. In this paper, we argue that the claim that machines cannot be held responsible for their actions has unacknowledged implications for the conditions under which the outputs of AI can serve as reasons for belief. Following Robert Brandom, we argue that, because the assertion of a claim is an action, moral agency is a necessary condition for the giving and evaluating of reasons in discourse. Thus, the same considerations that suggest that machines cannot be held responsible for their actions suggest that they cannot be held to account for the epistemic value — or lack of value — of their outputs. If there is a responsibility gap, there is also a “testimony gap.” An under-recognised problem with the use of AI, then, is to ensure that it does not block the attribution of testimony to human beings or lead to individuals being held responsible for claims that they have not asserted. More generally, the “assertions” of machines are only capable of serving as justifications for belief or action where one or more people accept responsibility for them.},
  archive      = {J_MAM},
  author       = {Sparrow, Robert and Flenady, Gene},
  doi          = {10.1007/s11023-025-09712-5},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Minds Mach.},
  title        = {The testimony gap: Machines and reasons},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="meco---8">MECO - 8</h2>
<ul>
<li><details>
<summary>
(2025). An adaptive matrix-based evolutionary computation framework
for EEG feature selection. <em>MECO</em>, <em>17</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12293-024-00434-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) plays a significant role in emotion recognition because it contains abundant information. However, due to the highly correlated EEG channels, a lot of redundant EEG features exist, which not only potentially degrade the emotion recognition accuracy, but also bring high computational costs. To address this challenge, this paper proposes an adaptive matrix-based evolutionary computation framework (AMEC) to select as few informative EEG features as possible for effective emotion recognition. Unlike most existing EC algorithms that utilize vector-based operations, this framework leverages matrix-based operations to reduce feature redundancy and improve classification accuracy by dynamically adjusting the feature subset size according to the characteristics of the dataset. In such a way, the selection efficiency is largely improved. To verify the effectiveness and efficiency of this framework, the classical genetic algorithm, the typical particle swarm optimization algorithm, and the classical differential evolution algorithm, are respectively embedded into this framework for EEG feature selection, and then evaluated on three widely used public EEG datasets for emotion recognition. Compared with several state-of-the-art EEG feature selection algorithms, the devised framework is much more effective in terms of the classification accuracy and the computational efficiency. In addition, the experimental results further reveal that the selected feature subsets are very different for different genders. This indicates the demand of gender-sensitive EEG feature selection for emotion recognition.},
  archive      = {J_MECO},
  author       = {Duan, Danting and Sun, Bing and Yang, Qiang and Ye, Long and Zhang, Qin and Zhang, Jun},
  doi          = {10.1007/s12293-024-00434-2},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Memet. Comput.},
  title        = {An adaptive matrix-based evolutionary computation framework for EEG feature selection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated design of state transition rules in ant colony
optimization by genetic programming: A comprehensive investigation.
<em>MECO</em>, <em>17</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s12293-025-00435-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated design of Ant Colony Optimization (ACO) algorithms has become increasingly significant, particularly in addressing complex combinatorial optimization problems. Although existing methods have achieved some success, they still face limitations, particularly the high dependency on expert knowledge, pre-solved data, and challenges in interpretability. Genetic Programming (GP), as a proven technology, has shown potential in optimizing the automated design state transition rules of ACO. However, existing research on GP-ACO is insufficient, particularly in terms of experimental validation and systematic evaluation. To address these issues, this study conducts comprehensive experiments to explore several key questions: the generality of GP-ACO on homogeneously distributed maps, the impact of different ACO variants on the learning capabilities of GP-ACO, the effect of 2-opt local search on the learning capabilities of GP-ACO, the enhancement of learning capabilities through the addition of more global information, and the interpretability of GP-ACO. The findings indicate that GP-ACO exhibits robust generality; variations among ACO variants have minimal impact on learning performance; 2-opt local search can somewhat diminish the performance of GP-ACO in the Max–Min Ant System; additional global information can significantly enhance the learning capabilities of GP-ACO; and GP-ACO has good interpretability.},
  archive      = {J_MECO},
  author       = {Lin, Bo-Cheng and Mei, Yi and Zhang, Mengjie},
  doi          = {10.1007/s12293-025-00435-9},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Memet. Comput.},
  title        = {Automated design of state transition rules in ant colony optimization by genetic programming: A comprehensive investigation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event extraction based on self-data augmentation with large
language models. <em>MECO</em>, <em>17</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s12293-025-00436-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event extraction plays a crucial role in natural language processing (NLP), facilitating the transformation of unstructured text into structured representations. This conversion significantly enhances the performance of various applications, such as automated question answering and information retrieval systems. However, traditional event extraction methodologies often encounter challenges stemming from limited datasets, imbalanced sample distributions, the necessity for extra resources to annotate large datasets, and the potential for data quality degradation during the augmentation process. To surmount these obstacles, this study introduces an innovative self-data augmentation strategy that leverages a single large language model (LLM) to concurrently perform data augmentation and event extraction. By dynamically assessing and refining the quality of generated samples, this approach mitigates the inclusion of noisy data, ultimately bolstering the model’s performance. Demonstrable enhancements in precision, recall, and F1 scores across various model configurations underscore the efficacy of this strategy in managing small and imbalanced datasets. Furthermore, the incorporation of Logical Thoughts for Self-Data Augmentation (LoTSA) ensures the superior quality of augmented data, culminating in more accurate and reliable extraction outcomes.},
  archive      = {J_MECO},
  author       = {Yang, Lishan and Fan, Xi and Wang, Xiangyu and Wang, Xin and Chen, Qiuju},
  doi          = {10.1007/s12293-025-00436-8},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Memet. Comput.},
  title        = {Event extraction based on self-data augmentation with large language models},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel efficient bi-objective evolutionary algorithm for
frequent and high utility itemsets mining. <em>MECO</em>,
<em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s12293-025-00437-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining frequent and high utility itemsets (FHUIs) from transaction database is an important task in data mining. In order to overcome the difficulties of parameter setting and huge search space in traditional algorithms for mining FHUIs, the task of mining FHUIs was modeled as a bi-objective problem and then solved by multi-objective evolutionary algorithms (MOEAs) in previous works. However, MOEAs may be inefficient when the number of transactions and items in the transaction database becomes large. To address this problem, we propose a novel efficient bi-objective evolutionary algorithm for mining FHUIs (NBOEA-FHUI). In NBOEA-FHUI, a novel initialization strategy is proposed, which takes the support, utility, and diversity of the initial population into account. The proposed initial strategy can make the initial population have relative high utility and support values with high population diversity. To improve the quality of the offspring, a method for estimating the support and utility value of itemsets and an offspring generation strategy are proposed in NBOEA-FHUI. The support and utility values of itemsets which are roughly proportional to their true values can be calculated by the estimation method with little computation. The proposed offspring generation strategy can generate better offspring based on the estimated support and utility value. Experimental results on several real datasets demonstrate that the proposed algorithm has better performance than the state-of-the-art MOEAs in terms of the convergence speed, search efficiency, and solution accuracy in the task of mining FHUIs.},
  archive      = {J_MECO},
  author       = {Ma, Li and Li, Chongyang and Lu, Heng-yang and Fang, Wei and Lin, Jerry Chun-Wei},
  doi          = {10.1007/s12293-025-00437-7},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Memet. Comput.},
  title        = {A novel efficient bi-objective evolutionary algorithm for frequent and high utility itemsets mining},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A constrained large-scale lever evolutionary algorithm for
white-box problems and its application in spectral-energy efficiency
tradeoff of massive MIMO. <em>MECO</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12293-025-00438-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing constrained multi-objective evolutionary algorithms are not so efficient when handling constrained large-scale multi-objective problems (CLSMOPs). To overcome white-box CLSMOPs with definitive objective functions, a spatial–temporal lever evolutionary algorithm (STLEA), consisting of the lever evolutionary algorithm (LEA) and spatial–temporal preference strategy (STPS), is proposed. LEA ditches the thought of the mainstream algorithms for the similar problems, which changes the structure of the large-scale decision space, but handles the large-scale decision space by a certain small-scale decision space. Specifically, inspired by the lever principle, LEA explores the way to pry up the large-scale decision space, as the “load”, by the small-scale decision space, as the “force”. Meanwhile, LEA rotates the optimizations in between “load” and “force” for dual-balance: balance between objectives and constraints, and balance between convergence and diversity of solutions. STPS dynamically adjusts the proportion of optimizations in “load” and “force”. Different from existing preference strategies, which only consider the stage of the evolutionary procedure, STPS considers both stage, related to time, and varying scale of the decision space, related to space, for the comprehensive balance of feasibility, convergence, and diversity of solutions. Eleven representative and state-of-the-art constrained multi-objective evolutionary algorithms have been compared to the proposed STLEA to demonstrate its effectiveness through comparative experiments on through comparative experiments on CLSMOPs with equality and inequality constraints and 1000 decision variables and three typical MaMIMO-LU models with 1024 antennas and 128, 256, and 512 users. Experimental results show that STLEA achieves the best SE-EE tradeoff.},
  archive      = {J_MECO},
  author       = {Wang, Qingzhu and Li, Tianyang},
  doi          = {10.1007/s12293-025-00438-6},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Memet. Comput.},
  title        = {A constrained large-scale lever evolutionary algorithm for white-box problems and its application in spectral-energy efficiency tradeoff of massive MIMO},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clinical causal analysis via iterative active structure
learning. <em>MECO</em>, <em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s12293-025-00439-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning has achieved considerable success in clinical applications such as image-based diagnostics, predictive modeling for patient outcomes, and personalized treatment planning. However, the black-box nature of deep neural networks often results in poor interpretability and reliability of predictions. Traditional neural network architectures, focusing primarily on correlations, fall short in elucidating underlying causal medical mechanisms. Addressing this, causal discovery, aimed at elucidating the structure of causal graphical models from observational or experimental data, is gaining prominence in clinical fields demanding high reliability. Nevertheless, the complexity of search algorithms, the scarcity of real-world data, and the challenges in identifying unique results significantly hinder the reliability of these approaches. To overcome these challenges, we propose an iterative active structure learning approach to ensure reliable clinical causal analysis. Our method begins with the recovery of a causal structure, guided by a set of prior causal presence, followed by an iterative process of active refinement to enhance the output reliability. This involves using violations of known clinical mechanisms as structural constraints to guide successive rounds of learning, thereby correcting and refining the model iteratively. The process continues until there is a convergence between expertise and the data-derived solutions. Our experiments on real-world clinical data demonstrate that Our approach can improve the quality of causal findings and discover new causal associations beyond the basis of expert knowledge. Furthermore, our approach has yielded novel and significant insights from various datasets, which we explore in our discussion.},
  archive      = {J_MECO},
  author       = {Tao, Zhenchao and Chi, Meiyan and Chen, Lyuzhou and Ban, Taiyu and Tu, Qiang and Gao, Fei and Wang, Wei},
  doi          = {10.1007/s12293-025-00439-5},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Memet. Comput.},
  title        = {Clinical causal analysis via iterative active structure learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simro-dino: A rotary positional DINO with siamese structure
for traffic object detection under adverse conditions. <em>MECO</em>,
<em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s12293-025-00440-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection on traffic road is crucial for enabling real-time analysis of road conditions and can be applied in intelligent transportation systems. However, in real world, we may encounter cases of low visibility, occlusion and lens contamination, where general methods of object detection usually degrade. In order to address this problem, we propose a Rotary Positional DINO with Siamese Structure (SimRo-DINO) framework, which efficiently overcomes the difficulty associated with object detection under adverse conditions. Specifically, to extract salient detail features and distinguish them from extraneous interference information, we leverage siamese representation learning along with random masking, which is named Mask Siamese Subnetwork, improving the robustness under adverse conditions. Furthermore, to enhance the connection between features scattered by various interferences and capture latent positional information under adverse conditions, we introduce Rotary Position Embedding into Co-DINO framework, an end-to-end detector with the capacity of capturing long-range dependency relationships within images. Extensive experiments have been conduct on UA-DETRAC and our self-built dataset, both under the adverse conditions. The results from our experiments indicate a substantial advancement in mean Average Precision (mAP) of 2.3 and 2.5% on these two datasets, respectively, compared to the Co-DINO baseline. The related codes are publicly available at https://github.com/xhzhou123/SimRo-DINO .},
  archive      = {J_MECO},
  author       = {Lei, Meng and Zhou, Xinghan and Zhao, Tianju and Xu, Shifan and Zou, Liang},
  doi          = {10.1007/s12293-025-00440-y},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Memet. Comput.},
  title        = {Simro-dino: A rotary positional DINO with siamese structure for traffic object detection under adverse conditions},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task assisted multi-objective optimization algorithm
for autonomous underwater vehicle path planning. <em>MECO</em>,
<em>17</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s12293-025-00441-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the application of multi-objective optimization algorithms to craft feasible paths considering multiple factors has garnered significant attention in handling path planning problems for autonomous underwater vehicles. However, the construction of appropriate multi-objective problem models coupled with efficient search strategies emerges as a pivotal determinant influencing the performance of multi-objective path planning algorithms. This paper introduces a multi-task assisted multi-objective optimization algorithm (MAMO) tailored to address autonomous underwater vehicle path planning problems. The proposed multi-task framework encompasses two tasks: the original path planning task and a devised simple task. These two tasks have different decision spaces due to distinct encoding strategies. Additionally, two different yet interconnected multi-objective problem models are deployed in the above two tasks. Furthermore, two knowledge transfer strategies, domain mapping-based and reconstruction-based knowledge transfer strategies, are introduced to leverage the knowledge from the simple task to assist the original task. The efficacy of the proposed MAMO is compared against eight counterparts and evaluated on three autonomous underwater vehicle path planning cases with different numbers of obstacles. The empirical findings corroborate the efficacy of the algorithm proffered.},
  archive      = {J_MECO},
  author       = {Liu, Tianyu and Wu, Yu and Xu, He},
  doi          = {10.1007/s12293-025-00441-x},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Memet. Comput.},
  title        = {Multi-task assisted multi-objective optimization algorithm for autonomous underwater vehicle path planning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ml---32">ML - 32</h2>
<ul>
<li><details>
<summary>
(2025). An empirical study on impact of label noise on synthetic
tabular data generation. <em>ML</em>, <em>114</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s10994-024-06629-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic data has been actively used for various machine learning-based tasks due to its benefits such as massive reproducibility and privacy enhancement compared to using the original data. The quality of the generated synthetic dataset crucially depends on the quality of the original data, and the latter is often corrupted by label noise. While there have been studies on feature noise, how label noise affects synthetic data generation is under-explored. In this paper, we evaluate the impact of the noisy label on synthetic data generation with a focus on tabular data. One challenge is how to evaluate the quality of synthetic data under label noise. To this end, we design comprehensive experiments to measure the impact of label noise on synthetic data generation in different aspects: synthetic data quality, data utility, and convergence for training synthesizers and machine learning models for downstream tasks. The empirical results cover wide aspects of synthetic data generation under label noise and they show quality and utility degrades with higher noise levels while there is no significant effect on the synthesizer convergence observed.},
  archive      = {J_ML},
  author       = {Kim, Jeonghoon and Huang, Chao and Liu, Xin},
  doi          = {10.1007/s10994-024-06629-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Learn.},
  title        = {An empirical study on impact of label noise on synthetic tabular data generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing cross-validation variance through seed blocking in
hyperparameter tuning. <em>ML</em>, <em>114</em>(4), 1–48. (<a
href="https://doi.org/10.1007/s10994-024-06630-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter tuning plays a crucial role in optimizing the performance of predictive learners. Cross-validation (CV) is a widely adopted technique for estimating the error of different hyperparameter settings. Repeated cross-validation (RCV) is commonly employed to reduce the variability of CV errors. This study investigates the efficacy of blocking cross-validation partitions and algorithm initialization seeds during hyperparameter tuning. The proposed approach, termed Controlled Cross-Validation (CCV), reduces variability in error estimates, enabling fairer and more reliable comparisons of predictive model performance. We provide both theoretical and empirical evidence to demonstrate that this blocking approach lowers the variance of the estimates compared to RCV. Our experiments indicate that the algorithm’s internal random behavior often does not significantly affect CV error variability. We present extensive examples using real-world datasets to compare the effectiveness and efficiency of blocking the CV partitions when tuning the hyperparameters of different supervised predictive learning algorithms.},
  archive      = {J_ML},
  author       = {Merola, Giovanni Maria},
  doi          = {10.1007/s10994-024-06630-y},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-48},
  shortjournal = {Mach. Learn.},
  title        = {Reducing cross-validation variance through seed blocking in hyperparameter tuning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inductive learning of robot task knowledge from raw data and
online expert feedback. <em>ML</em>, <em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-024-06636-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing level of autonomy of robots poses challenges of trust and social acceptance, especially in human-robot interaction scenarios. This requires an interpretable implementation of robotic cognitive capabilities, possibly based on formal methods as logics for the definition of task specifications. However, prior knowledge is often unavailable in complex realistic scenarios. In this paper, we propose an offline algorithm based on inductive logic programming from noisy examples to extract task specifications (i.e., action preconditions, constraints and effects) directly from raw data of few heterogeneous (i.e., not repetitive) robotic executions. Our algorithm leverages on the output of any unsupervised action identification algorithm from video-kinematic recordings. Combining it with the definition of very basic, almost task-agnostic, commonsense concepts about the environment, which contribute to the interpretability of our methodology, we are able to learn logical axioms encoding preconditions of actions, as well as their effects in the event calculus paradigm. Since the quality of learned specifications depends mainly on the accuracy of the action identification algorithm, we also propose an online framework for incremental refinement of task knowledge from user’s feedback, guaranteeing safe execution. Results in a standard manipulation task and benchmark for user training in the safety-critical surgical robotic scenario, show the robustness, data- and time-efficiency of our methodology, with promising results towards the scalability in more complex domains.},
  archive      = {J_ML},
  author       = {Meli, Daniele and Fiorini, Paolo},
  doi          = {10.1007/s10994-024-06636-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Inductive learning of robot task knowledge from raw data and online expert feedback},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient projection-free online convex optimization using
stochastic gradients. <em>ML</em>, <em>114</em>(4), 1–61. (<a
href="https://doi.org/10.1007/s10994-024-06640-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Online Convex Optimization (OCO) problems subject to a compact convex set. An important class of projection-free online methods known as Frank–Wolfe-type (FW-type) methods have attracted considerable attention in the machine learning community, as they eschew the expensive projection operation and only require a simple linear minimization oracle in each round. Recently, the stochastic gradient technique has been integrated in FW-type online methods to circumvent the expensive full gradient computation and further reduce the per-round computational cost. However, these methods generally have high regret bounds due to high variance in gradient estimation. Although adopting a large minibatch in stochastic gradients can reduce the variance, it would in turn increase the per-round computational cost. In this paper, we develop efficient FW-type methods that only need stochastic gradients with small minibatch and achieve nearly optimal regret bounds with low per-round costs. We first explore the similarity between gradients of decision variables in consecutive rounds, and construct a lightweight variance-reduced estimator by utilizing historical gradient information. Based on this estimator, we propose a method named OFWRG for smooth problems in the stochastic setting. We prove that OFWRG achieves a nearly optimal regret bound with the lowest $$\mathcal {O}(1)$$ per-round computational cost. OFWRG is the first method with such nearly optimal result in this setting. We further extend OFWRG to OCO problems in other settings, including smooth problems in the adversarial setting and a class of non-smooth problems in the stochastic and adversarial settings. Our theoretical analyses show that these extensions of OFWRG achieve nearly optimal regret bounds and low per-round computational costs under mild conditions. Experimental results demonstrate the efficiency of our methods.},
  archive      = {J_ML},
  author       = {Xie, Jiahao and Zhang, Chao and Shen, Zebang and Qian, Hui},
  doi          = {10.1007/s10994-024-06640-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-61},
  shortjournal = {Mach. Learn.},
  title        = {Efficient projection-free online convex optimization using stochastic gradients},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibrated explanations for regression. <em>ML</em>,
<em>114</em>(4), 1–34. (<a
href="https://doi.org/10.1007/s10994-024-06642-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) methods are an integral part of modern decision support systems. The best-performing predictive models used in AI-based decision support systems lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature’s importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations, previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is below an arbitrary threshold. The extension for regression keeps all the benefits of Calibrated Explanations, such as calibration of the prediction from the underlying model with confidence intervals, uncertainty quantification of feature importance, and allows both factual and counterfactual explanations. Calibrated Explanations for regression provides fast, reliable, stable, and robust explanations. Calibrated Explanations for probabilistic regression provides an entirely new way of creating probabilistic explanations from any ordinary regression model, allowing dynamic selection of thresholds. The method is model agnostic with easily understood conditional rules. An implementation in Python is freely available on GitHub and for installation using both pip and conda, making the results in this paper easily replicable.},
  archive      = {J_ML},
  author       = {Löfström, Tuwe and Löfström, Helena and Johansson, Ulf and Sönströd, Cecilia and Matela, Rudy},
  doi          = {10.1007/s10994-024-06642-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-34},
  shortjournal = {Mach. Learn.},
  title        = {Calibrated explanations for regression},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforced logical reasoning over KGs for interpretable
recommendation system. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06646-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various domains, traditional recommendation systems have demonstrated significant benefits. However, their &quot;black box&quot; mechanisms have led to a crisis of trust among users. Interpretable recommendation systems have emerged as a solution by providing explanations for recommended items, thus enhancing transparency and user confidence. Another challenge to interpretable recommendation systems is data sparsity, which causes subpar recommendation performance. Addressing the challenges of model interpretability and data sparsity, this paper introduces the Knowledge Graphs-based Logic Reasoning Recommendation (KG-LRR) method, structured around an &quot;encoder-decoder&quot; architecture. The KG-LRR method tackles these issues by leveraging a knowledge graph for items to enhance the representation of users and items during the encoding process. It introduces a propositional logic reasoning model for decoding, rendering explanations in a more comprehensible manner. This dual approach ensures a balance between the recommendation system’s efficiency and interpretability. The KG-LRR method employs a neural network to simulate human-like propositional logical reasoning. This not only mitigates data sparsity issues but also explicates users’ interest in items. It provides deeper insights into users’ preferences and delivers robust interpretability. Experimental results across three public datasets-Yelp2018, Amazon-book, and Amazon-electronics-demonstrate that the KG-LRR model outperforms existing methods in terms of Recall and nDCG in top-k ranking recommendation scenarios. This validates its superior performance compared to prevailing interpretable recommendation techniques. In summary, the KG-LRR method offers a novel approach to enhance transparency and performance through an innovative &quot;encoder-decoder&quot; architecture. Its integration of knowledge graphs and propositional logic reasoning showcases promising outcomes in addressing current challenges within interpretable recommendation systems. Our code is available at https://github.com/siri-ya/KG-LRR .},
  archive      = {J_ML},
  author       = {Wang, Shirui and Xie, Bohan and Ding, Ling and Chen, Jianting and Xiang, Yang},
  doi          = {10.1007/s10994-024-06646-4},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Reinforced logical reasoning over KGs for interpretable recommendation system},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified convergence analysis for adaptive optimization with
moving average estimator. <em>ML</em>, <em>114</em>(4), 1–51. (<a
href="https://doi.org/10.1007/s10994-024-06650-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although adaptive optimization algorithms have been successful in many applications, there are still some mysteries in terms of convergence analysis that have not been unraveled. This paper provides a novel non-convex analysis of adaptive optimization to uncover some of these mysteries. Our contributions are three-fold. First, we show that an increasing or large enough momentum parameter for the first-order moment used in practice is sufficient to ensure the convergence of adaptive algorithms whose adaptive scaling factors of the step size are bounded. Second, our analysis gives insights for practical implementations, e.g., increasing the momentum parameter in a stage-wise manner in accordance with stagewise decreasing step size would help improve the convergence. Third, the modular nature of our analysis allows its extension to solving other optimization problems, e.g., compositional, min–max and bilevel problems. As an interesting yet non-trivial use case, we present algorithms for solving non-convex min–max optimization and bilevel optimization that do not require using large batches of data to estimate gradients or double loops as the literature do. Our empirical studies corroborate our theoretical results.},
  archive      = {J_ML},
  author       = {Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
  doi          = {10.1007/s10994-024-06650-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-51},
  shortjournal = {Mach. Learn.},
  title        = {Unified convergence analysis for adaptive optimization with moving average estimator},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the usefulness of the fit-on-test view on evaluating
calibration of classifiers. <em>ML</em>, <em>114</em>(4), 1–75. (<a
href="https://doi.org/10.1007/s10994-024-06652-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibrated uncertainty estimates are essential for classifiers used in safety-critical applications. If a classifier is uncalibrated, then there is a unique way to calibrate its uncertainty using the idealistic true calibration map corresponding to this classifier. Although the true calibration map is typically unknown in practice, it can be estimated with many post-hoc calibration methods which fit some family of potential calibration functions on a validation dataset. This paper examines the connection between such post-hoc calibration methods and calibration evaluation. Despite the negative connotations of fitting on test data in machine learning, we claim that fitting calibration maps on test data as part of the calibration evaluation process is a method worth considering, and we refer to this view as fit-on-test. This view enables the usage of any post-hoc calibration method as an evaluation measure, unlocking missed opportunities in development of evaluation methods. We prove that even ECE, which is the most common calibration evaluation method, is actually a fit-on-test measure. This observation leads us to a new method of tuning the number of bins in ECE with cross-validation. Fitting on test data can lead to test-time overfitting, and therefore, we discuss the limitations and concerns with the fit-on-test view. Our contributions also include: (1) enhancement of reliability diagrams with diagonal filling; (2) development of new calibration map families PL and PL3; and (3) an experimental study of which families perform strongly both as post-hoc calibrators and calibration evaluators.},
  archive      = {J_ML},
  author       = {Kängsepp, Markus and Valk, Kaspar and Kull, Meelis},
  doi          = {10.1007/s10994-024-06652-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-75},
  shortjournal = {Mach. Learn.},
  title        = {On the usefulness of the fit-on-test view on evaluating calibration of classifiers},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum causal entropy inverse constrained reinforcement
learning. <em>ML</em>, <em>114</em>(4), 1–44. (<a
href="https://doi.org/10.1007/s10994-024-06653-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements specific to that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide a practical implementation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and environments, and it is able to handle problems with stochastic dynamics and a continuous state-action space.},
  archive      = {J_ML},
  author       = {Baert, Mattijs and Mazzaglia, Pietro and Leroux, Sam and Simoens, Pieter},
  doi          = {10.1007/s10994-024-06653-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-44},
  shortjournal = {Mach. Learn.},
  title        = {Maximum causal entropy inverse constrained reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A prompt-driven framework for multi-domain knowledge
tracing. <em>ML</em>, <em>114</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10994-024-06660-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) models students’ knowledge states to predict future performance based on historical interactions. Due to data privacy concerns and budget constraints, the availability of high-quality student data differs across domains and it is essential to effectively utilize KT data from multiple domains. In this work, we propose a novel prompt-enhanced paradigm, i.e., promptKT, to utilize student data from multiple domains to improve KT performance simultaneously. Specifically, a unified Transformer based backbone model is first pre-trained using data from all the KT domains to capture the commonality across domains. Then, we design a novel soft domain prompt module to capture the distinctions among various domains and users. Our promptKT is evaluated on six public real-world educational datasets. The results demonstrate that our approach outperforms the majority of existing KT models in terms of AUC and accuracy. Furthermore, empirical analysis shows the decent transferability and adaptation of promptKT across multiple KT domains. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt-team/pykt-toolkit .},
  archive      = {J_ML},
  author       = {Liu, Zitao and Huang, Shuyan and Guo, Teng and Hou, Mingliang and Liang, Qianru},
  doi          = {10.1007/s10994-024-06660-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {A prompt-driven framework for multi-domain knowledge tracing},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capturing the context-aware code change via dynamic control
flow graph for commit message generation. <em>ML</em>, <em>114</em>(4),
1–23. (<a href="https://doi.org/10.1007/s10994-024-06671-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commit messages that summarize code changes of each commit in natural language help developers understand code changes without digging into implementation details, thus playing an essential role in comprehending software evolution. In constructing models for automatic commit message generation, prior research has focused on extracting information from the changed code hunks (i.e., code difference), while ignoring the unchanged code hunks (i.e., code context). However, most studies often neglect the fact that the code change is context-aware, that is the semantics of the code difference are heavily dependent on its code context. To take the code context into account, a key challenge arises: the extensive code context may overshadow the minuscule code difference in capturing the changed semantics, which is a disadvantage to commit message generation. In this paper, we propose the dynamic control flow graph (DCFG), which combines both the code contexts and code differences into one dynamic global–local structure. Based on DCFG, we design a novel framework termed capturing the context-aware code change for commit message generation ( $${\text {C}^4\text {MG}}$$ ), which attempts to model the changed semantics of the code change based on the relevant code context, while avoiding being misled by the overwhelming amount of unchanged code context. Extensive experiments demonstrate that benefiting from modeling the context-aware code change, $${\text {C}^4\text {MG}}$$ outperforms not only the state-of-the-art open-source models but also the large language models (e.g., LLaMA3, GPT-4o, and Gemini) on the commit message generation.},
  archive      = {J_ML},
  author       = {Du, Yali and Li, Ying and Ma, Yi-Fan and Li, Ming},
  doi          = {10.1007/s10994-024-06671-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Capturing the context-aware code change via dynamic control flow graph for commit message generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nettop: A light-weight network of orthogonal-plane features
for image recognition. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06672-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current light-weight CNN-based networks, convolutional operators are principally utilized to extract feature maps for image representation. However, such conventional operation can lead to lack of informative patterns for the learning process. It is because the operators have just been allocated to convolute on the spatial side of an input tensor. To deal with this deficiency, we propose a competent model to efficiently exploit the full-side features of a tensor. The proposed model is based on three novel concepts as follows. i) A novel grouped-convolutional operator is defined to produce complementary features in consideration of three plane-based volumes that have been correspondingly partitioned subject to three orthogonal planes (TOP) of a given tensor. ii) An effective perceptron block is introduced to take into account the TOP-based operator for orthogonal-plane feature extraction. iii) A light-weight backbone of TOP-based blocks (named NetTOP) is proposed to take advantage of the full-side informative patterns for image representation. Experimental results for image recognition on benchmark datasets have proved the prominent performance of the proposals. The code of NetTOP is available at https://github.com/nttbdrk25/NetTOP .},
  archive      = {J_ML},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10994-024-06672-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Nettop: A light-weight network of orthogonal-plane features for image recognition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HorNets: Learning from discrete and continuous signals with
routing neural networks. <em>ML</em>, <em>114</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s10994-024-06673-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction of neural network architectures suitable for learning from both continuous and discrete tabular data is challenging, as contemporary high-dimensional tabular data sets are often characterized by a relatively small set of instances and the request for efficient learning. We propose HorNets (Horn Networks), a neural network architecture with state-of-the-art performance on synthetic and real-life data sets from scarce-data tabular domains. HorNets are based on a clipped polynomial-like activation function, extended by a custom discrete-continuous routing mechanism that decides which part of the neural network to optimize based on the input’s cardinality. By explicitly modeling parts of the feature combination space or combining whole space in a linear attention-like manner, HorNets dynamically decide which mode of operation is the most suitable for a given piece of data with no explicit supervision. This architecture is one of the few approaches that reliably retrieves logical clauses (including noisy XNOR) and achieves state-of-the-art classification performance on 14 real-life biomedical high-dimensional data sets. HorNets are made freely available under a permissive license alongside a synthetic generator of categorical benchmarks.},
  archive      = {J_ML},
  author       = {Koloski, Boshko and Lavrač, Nada and Škrlj, Blaž},
  doi          = {10.1007/s10994-024-06673-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {HorNets: Learning from discrete and continuous signals with routing neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TCR: Topologically consistent reweighting for XGBoost in
regression tasks. <em>ML</em>, <em>114</em>(4), 1–52. (<a
href="https://doi.org/10.1007/s10994-024-06704-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient boosted tree ensembles (GBTEs) such as XGBoost continue to outperform other machine learning models on tabular data. However, the plethora of adjustable hyperparameters can exacerbate optimisation, especially in regression tasks with no intuitive performance measures such as accuracy and confidence. Automated machine learning frameworks alleviate the hyperparameter search for users, but if the optimisation procedure ends prematurely due to resource constraints, it is questionable whether users receive good models. To tackle this problem, we introduce a cost-efficient method to retrofit previously optimised XGBoost models by retraining them with a new weight distribution over the training instances. We base our approach on topological results, which allows us to infer model-agnostic weights for specific regions of the data distribution where the targets are more susceptible to input perturbations. By linking our theory to the training procedure of XGBoost regressors, we then establish a topologically consistent reweighting scheme, which is independent of the specific model instance. Empirically, we verify that our approach improves prediction performance, outperforms other reweighting methods and is much faster than a hyperparameter search. To enable users to find the optimal weights for their data, we provide guides based on our findings on 20 datasets. Our code is available at: https://github.com/montymaxzuehlke/tcr .},
  archive      = {J_ML},
  author       = {Zühlke, Monty-Maximilian and Kudenko, Daniel},
  doi          = {10.1007/s10994-024-06704-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {TCR: Topologically consistent reweighting for XGBoost in regression tasks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intramodal consistency in triplet-based cross-modal learning
for image retrieval. <em>ML</em>, <em>114</em>(4), 1–29. (<a
href="https://doi.org/10.1007/s10994-024-06710-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval requires building a common latent space that captures and correlates information from different data modalities, usually images and texts. Cross-modal training based on the triplet loss with hard negative mining is a state-of-the-art technique to address this problem. This paper shows that such approach is not always effective in handling intra-modal similarities. Specifically, we found that this method can lead to inconsistent similarity orderings in the latent space, where intra-modal pairs with unknown ground-truth similarity are ranked higher than cross-modal pairs representing the same concept. To address this problem, we propose two novel loss functions that leverage intra-modal similarity constraints available in a training triplet but not used by the original formulation. Additionally, this paper explores the application of this framework to unsupervised image retrieval problems, where cross-modal training can provide the supervisory signals that are otherwise missing in the absence of category labels. Up to our knowledge, we are the first to evaluate cross-modal training for intra-modal retrieval without labels. We present comprehensive experiments on MS-COCO and Flickr30k, demonstrating the advantages and limitations of the proposed methods in cross-modal and intra-modal retrieval tasks in terms of performance and novelty measures. We also conduct a case study on the ROCO dataset to assess the performance of our method on medical images and present an ablation study on one of our approaches to understanding the impact of the different components of the proposed loss function. Our code is publicly available on GitHub https://github.com/MariodotR/FullHN.git .},
  archive      = {J_ML},
  author       = {Mallea, Mario and Ñanculef, Ricardo and Araya, Mauricio},
  doi          = {10.1007/s10994-024-06710-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Intramodal consistency in triplet-based cross-modal learning for image retrieval},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain generalization via content factors isolation: A
two-level latent variable modeling approach. <em>ML</em>,
<em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-024-06717-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of domain generalization is to develop models that exhibit a higher degree of generality, meaning they perform better when evaluated on data coming from previously unseen distributions. Models obtained via traditional methods often cannot distinguish between label-specific and domain-related features in the latent space. To confront this difficulty, we propose formulating a novel data generation process using a latent variable model and postulating a partition of the latent space into content and style parts while allowing for statistical dependency to exist between them. In this model, the distribution of content factors associated with observations belonging to the same class depends on only the label corresponding to that class. In contrast, the distribution of style factors has an additional dependency on the domain variable. We derive constraints that suffice to recover the collection of content factors block-wise and the collection of style factors component-wise while guaranteeing the isolation of content factors. This allows us to produce a stable predictor solely relying on the latent content factors. Building upon these theoretical insights, we propose a practical and efficient algorithm for determining the latent variables under the variational auto-encoder framework. Our simulations with dependent latent variables produce results consistent with our theory, and real-world experiments show that our method outperforms the competitors.},
  archive      = {J_ML},
  author       = {Gao, Erdun and Bondell, Howard and Huang, Shaoli and Gong, Mingming},
  doi          = {10.1007/s10994-024-06717-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Domain generalization via content factors isolation: A two-level latent variable modeling approach},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing exchangeability in the batch mode with e-values and
markov alternatives. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06720-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of this paper is testing the assumption of exchangeability, which is the standard assumption in mainstream machine learning. The common approaches are online testing by betting (such as conformal testing) and the older batch testing using p-values (as in classical hypothesis testing). The approach of this paper is intermediate in that we are interested in batch testing by betting; as a result, p-values are replaced by e-values. As a first step in this direction, this paper concentrates on the Markov model as alternative. The null hypothesis of exchangeability is formalized as a Kolmogorov-type compression model, and the Bayes mixture of the Markov model w.r. to the uniform prior is taken as simple alternative hypothesis. Using e-values instead of p-values leads to a computationally efficient testing procedure. Two appendixes discuss connections with the algorithmic theory of randomness; in particular, the test proposed in this paper can be interpreted as a poor man’s version of Kolmogorov’s deficiency of randomness.},
  archive      = {J_ML},
  author       = {Vovk, Vladimir},
  doi          = {10.1007/s10994-024-06720-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Testing exchangeability in the batch mode with e-values and markov alternatives},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel density estimation for multiclass quantification.
<em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-024-06726-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several disciplines, like the social sciences, epidemiology, sentiment analysis, or market research, are interested in knowing the distribution of the classes in a population rather than the individual labels of the members thereof. Quantification is the supervised machine learning task concerned with obtaining accurate predictors of class prevalence, and to do so particularly in the presence of label shift. The distribution-matching (DM) approaches represent one of the most important families among the quantification methods that have been proposed in the literature so far. Current DM approaches model the involved populations using histograms of posterior probabilities. In this paper, we argue that their application to the multiclass setting is suboptimal since the histograms become class-specific, thus missing the opportunity to model inter-class information that may exist in the data. We propose a new representation mechanism based on multivariate densities that we model via kernel density estimation (KDE). The experiments we have carried out show our method, dubbed KDEy, yields superior quantification performance compared to previous DM approaches and other state-of-the-art quantification systems.},
  archive      = {J_ML},
  author       = {Moreo, Alejandro and González, Pablo and del Coz, Juan José},
  doi          = {10.1007/s10994-024-06726-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Kernel density estimation for multiclass quantification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical risk minimization in the interpolating regime with
application to neural network learning. <em>ML</em>, <em>114</em>(4),
1–52. (<a href="https://doi.org/10.1007/s10994-025-06738-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common strategy to train deep neural networks (DNNs) is to use very large architectures and to train them until they (almost) achieve zero training error. Empirically observed good generalization performance on test data, even in the presence of lots of label noise, corroborate such a procedure. On the other hand, in statistical learning theory it is known that over-fitting models may lead to poor generalization properties, occurring in e.g. empirical risk minimization (ERM) over too large hypotheses classes. Inspired by this contradictory behavior, so-called interpolation methods have recently received much attention, leading to consistent and optimally learning methods for, e.g., some local averaging schemes with zero training error. We extend this analysis to ERM-like methods for least squares regression and show that for certain, large hypotheses classes called inflated histograms, some interpolating empirical risk minimizers enjoy very good statistical guarantees while others fail in the worst sense. Moreover, we show that the same phenomenon occurs for DNNs with zero training error and sufficiently large architectures.},
  archive      = {J_ML},
  author       = {Mücke, Nicole and Steinwart, Ingo},
  doi          = {10.1007/s10994-025-06738-9},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {Empirical risk minimization in the interpolating regime with application to neural network learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compression and restoration: Exploring elasticity in
continual test-time adaptation. <em>ML</em>, <em>114</em>(4), 1–32. (<a
href="https://doi.org/10.1007/s10994-025-06739-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation is a task that a pre-trained source model is updated during inference with given test data from target domains with different distributions. However, frequent updates in a long time without resetting the model will bring two main problems, i.e., error accumulation and catastrophic forgetting. Although some recent methods have alleviated the problems by designing new loss functions or update strategies, they are still very fragile to hyperparameters or suffer from storage burden. Besides, most methods treat each target domain equally, neglecting the characteristics of each target domain and the situation of the current model, which will mislead the update direction of the model. To address the above issues, we first leverage the mean cosine similarity per test batch between the features output by the source and updated models to measure the change of target domains. Then we summarize the elasticity of the mean cosine similarity to guide the model to update and restore adaptively. Motivated by this, we propose a frustratingly simple yet efficient method called Elastic-Test-time ENTropy Minimization (E-TENT) to dynamically adjust the mean cosine similarity based on the built relationship between it and the momentum coefficient. Combined with the extra three minimal improvements, E-TENT exhibits significant performance gains and strong robustness on CIFAR10-C, CIFAR100-C and ImageNet-C along with various practical scenarios.},
  archive      = {J_ML},
  author       = {Li, Jingwei and Liu, Chengbao and Bai, Xiwei and Tan, Jie and Chu, Jiaqi and Wang, Yudong},
  doi          = {10.1007/s10994-025-06739-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Compression and restoration: Exploring elasticity in continual test-time adaptation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep errors-in-variables using a diffusion model.
<em>ML</em>, <em>114</em>(4), 1–25. (<a
href="https://doi.org/10.1007/s10994-025-06744-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Errors-in-Variables is the statistical concept used to explicitly model input variable errors caused, for example, by noise. While it has long been known in statistics that not accounting for such errors can produce a substantial bias, the vast majority of deep learning models have thus far neglected Errors-in-Variables approaches. Reasons for this include a significant increase of the numerical burden and the challenge in assigning an appropriate prior in a Bayesian treatment. To date, the attempts made to use Errors-in-Variables for neural networks do not scale to deep networks or are too simplistic to enhance the prediction performance. This work shows for the first time how Bayesian deep Errors-in-Variables models can increase the prediction performance. We present a scalable variational inference scheme for Bayesian Errors-in-Variables and demonstrate a significant increase in prediction performance for the case of image classification. Concretely, we use a diffusion model as input posterior to obtain a distribution over the denoised image data. We also observe that training the diffusion model on an unnoisy surrogate dataset can suffice to achieve an improved prediction performance on noisy data.},
  archive      = {J_ML},
  author       = {Faller, Josua and Martin, Jörg and Elster, Clemens},
  doi          = {10.1007/s10994-025-06744-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Deep errors-in-variables using a diffusion model},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncover the balanced geometry in long-tailed contrastive
language-image pretraining. <em>ML</em>, <em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-025-06745-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Contrastive Language-Image Pretraining (CLIP) has become the de facto standard for vision-language pretraining tasks, the exploration on the inherent long-tailed pretraining data distribution remains limited. From a neural collapse perspective, we show in principle that the vanilla CLIP training can be vulnerable to the long-tailed distributions, which might distort the representations with reduced inter-class separation and poor discriminative ability. To combat this issue, we propose an improved method, termed as Geometry-Balanced CLIP (GeoCLIP), which automatically constructs pseudo clusters and aligns them with a predefined equiangular geometric structure, thereby enjoying the theoretical merits of better maintaining the uniformity at the semantic level. Furthermore, we enhance GeoCLIP’s generality for real-world complex distributions by incorporating harmonized clusters that integrate both empirically observed data structures and theoretically optimal geometry. Extensive experiments across various benchmarks demonstrate the consistent superiority of GeoCLIP in achieving robust and transferable representation under long-tailed distributions. The source code will be publicly available.},
  archive      = {J_ML},
  author       = {Zhou, Zhihan and Ye, Yushi and Hong, Feng and Zhao, Peisen and Yao, Jiangchao and Zhang, Ya and Tian, Qi and Wang, Yanfeng},
  doi          = {10.1007/s10994-025-06745-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Uncover the balanced geometry in long-tailed contrastive language-image pretraining},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning with pre-trained conditional generative
models. <em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-025-06748-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is crucial in training deep neural networks on new target tasks. Current transfer learning methods always assume at least one of (i) Source and target task label spaces overlap, (ii) Source datasets are available, and (iii) Target network architectures are consistent with source ones. However, holding these assumptions is difficult in practical settings because the target task rarely has the same labels as the source task, the source dataset access is restricted due to storage costs and privacy, and the target architecture is often specialized to each task. To transfer source knowledge without these assumptions, we propose a transfer learning method that uses deep generative models and is composed of the following two stages: pseudo pre-training (PP) and pseudo semi-supervised learning (P-SSL). PP trains a target architecture with an artificial dataset synthesized by using conditional source generative models. P-SSL applies SSL algorithms to labeled target data and unlabeled pseudo samples, which are generated by cascading the source classifier and generative models to condition them with target samples. Our experimental results indicate that our method can outperform the baselines of scratch training and knowledge distillation.},
  archive      = {J_ML},
  author       = {Yamaguchi, Shin’ya and Kanai, Sekitoshi and Kumagai, Atsutoshi and Chijiwa, Daiki and Kashima, Hisashi},
  doi          = {10.1007/s10994-025-06748-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Transfer learning with pre-trained conditional generative models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model for intelligible interaction between agents that
predict and explain. <em>ML</em>, <em>114</em>(4), 1–40. (<a
href="https://doi.org/10.1007/s10994-025-06750-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has emerged as a powerful form of data modelling with widespread applicability beyond its roots in the design of autonomous agents. However, relatively little attention has been paid to the interaction between people and ML systems. In this paper we view interaction between humans and ML systems within the broader context of communication between agents capable of prediction and explanation. We formalise the interaction model by taking agents to be automata with some special characteristics and define a protocol for communication between such agents. We define One- and Two-Way Intelligibility as properties that emerge at run-time by execution of the protocol. The formalisation allows us to identify conditions under which run-time sequences are bounded, and identify conditions under which the protocol can correctly implement an axiomatic specification of intelligible interaction between a human and an ML system. We also demonstrate using the formal model to: (a) identify instances of One- and Two-Way Intelligibility in literature reports on humans interacting with ML systems providing logic-based explanations, as is done in Inductive Logic Programming (ILP); and (b) map interactions between humans and machines in an elaborate natural-language based dialogue-model to One- or Two-Way Intelligible interactions in the formal model.},
  archive      = {J_ML},
  author       = {Baskar, A. and Srinivasan, Ashwin and Bain, Michael and Coiera, Enrico},
  doi          = {10.1007/s10994-025-06750-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {A model for intelligible interaction between agents that predict and explain},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A contrastive neural disentanglement approach for query
performance prediction. <em>ML</em>, <em>114</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10994-025-06752-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach, referred to as contrastive disentangled representation for query performance prediction (CoDiR-QPP), to estimate search query performance by disentangling query content semantics from query difficulty. Our proposed approach leverages neural disentanglement to isolate the information need expressed in search queries from the complexities that affect retrieval performance. Motivated by empirical observations that varying query formulations for the same information need can significantly impact retrieval outcomes, we hypothesize that separating content semantics from query difficulty can enhance query performance prediction. Utilizing contrastive learning, CoDiR-QPP distinguishes between well-performing and poorly performing query variants, facilitating the estimation of a given query’s performance. Our extensive experiments on four standard benchmark datasets demonstrate that CoDiR-QPP outperforms state-of-the-art baselines in predicting query performance, offering improved semantic similarity computation and higher correlation metrics such as Kendall $$\tau$$ , Spearman $$\rho$$ , and scaled Mean Absolute Ranking Error (sMARE).},
  archive      = {J_ML},
  author       = {Salamat, Sara and Arabzadeh, Negar and Seyedsalehi, Shirin and Bigdeli, Amin and Zihayat, Morteza and Bagheri, Ebrahim},
  doi          = {10.1007/s10994-025-06752-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {A contrastive neural disentanglement approach for query performance prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Likelihood-ratio-based confidence intervals for neural
networks. <em>ML</em>, <em>114</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s10994-024-06639-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.},
  archive      = {J_ML},
  author       = {Sluijterman, Laurens and Cator, Eric and Heskes, Tom},
  doi          = {10.1007/s10994-024-06639-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Likelihood-ratio-based confidence intervals for neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pairwise learning to rank by neural networks revisited:
Reconstruction, theoretical analysis and practical performance.
<em>ML</em>, <em>114</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s10994-024-06644-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We reevaluate the pairwise learning to rank approach based on neural nets, called RankNet, and present a theoretical analysis of its architecture. We show mathematically that the model can, under certain conditions, learn reflexive, antisymmetric, and transitive relations, enabling simplified training and improved performance. Experimental results on the LETOR MSLR-WEB10K, MQ2007 and MQ2008 datasets show that the model outperforms numerous state-of-the-art methods (including a listwise approach), while being inherently simpler in structure and using a pairwise approach only.},
  archive      = {J_ML},
  author       = {Köppel, Marius and Segner, Alexander and Wagener, Martin and Pensel, Lukas and Karwath, Andreas and Kramer, Stefan},
  doi          = {10.1007/s10994-024-06644-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Pairwise learning to rank by neural networks revisited: Reconstruction, theoretical analysis and practical performance},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on self-supervised methods for visual
representation learning. <em>ML</em>, <em>114</em>(4), 1–56. (<a
href="https://doi.org/10.1007/s10994-024-06708-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.},
  archive      = {J_ML},
  author       = {Uelwer, Tobias and Robine, Jan and Wagner, Stefan Sylvius and Höftmann, Marc and Upschulte, Eric and Konietzny, Sebastian and Behrendt, Maike and Harmeling, Stefan},
  doi          = {10.1007/s10994-024-06708-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-56},
  shortjournal = {Mach. Learn.},
  title        = {A survey on self-supervised methods for visual representation learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring individual direct causal effects under
heterogeneous peer influence. <em>ML</em>, <em>114</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s10994-024-06729-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference is central to understanding the effectiveness of policies and designing personalized interventions. Causal inference involves estimating the causal effects of treatments on outcomes of interest after modeling appropriate assumptions. Most causal inference approaches assume that a unit’s outcome is independent of the treatments or outcomes of other units. However, this assumption is unrealistic when inferring causal effects in networks where a unit’s outcome can be influenced by the treatments and outcomes of its neighboring nodes, a phenomenon known as interference. Causal inference in networks should explicitly account for interference. In interference settings, the direct causal effect measures the impact of the unit’s own treatment while controlling for the treatments of peers. Existing solutions to estimating direct causal effects under interference consider either homogeneous influence from peers or specific heterogeneous influence mechanisms (e.g., based on local neighborhood structure). In this work, we define heterogeneous peer influence (HPI) as the general interference that occurs when a unit’s outcome may be influenced differently by different peers based on their attributes and relationships, or when each network node may have a different susceptibility to peer influence. This paper presents IDE-Net, a framework for estimating individual, i.e., unit-level, direct causal effects in the presence of HPI where the mechanism of influence is not known a priori. We first propose a structural causal model for networks that can capture different possible assumptions about network structure, interference conditions, and causal dependence and that enables reasoning about causal effect identifiability and discovery of potential heterogeneous contexts. We then propose a novel graph neural network-based estimator to estimate individual direct causal effects. We show empirically that state-of-the-art methods for individual direct effect estimation produce biased results in the presence of HPI, and that our proposed estimator is robust.},
  archive      = {J_ML},
  author       = {Adhikari, Shishir and Zheleva, Elena},
  doi          = {10.1007/s10994-024-06729-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Inferring individual direct causal effects under heterogeneous peer influence},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end explainability framework for spatio-temporal
predictive modeling. <em>ML</em>, <em>114</em>(4), 1–47. (<a
href="https://doi.org/10.1007/s10994-024-06733-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising adoption of AI models in real-world applications characterized by sensor data creates an urgent need for inference explanation mechanisms to support domain experts in making informed decisions. Explainable AI (XAI) opens up a new opportunity to extend black-box deep learning models with such inference explanation capabilities. However, existing XAI approaches for tabular, image, and graph data are ineffective in contexts with spatio-temporal data. In this paper, we fill this gap by proposing a XAI method specifically tailored for spatio-temporal data in sensor networks, where observations are collected at regular time intervals and at different locations. Our model-agnostic masking meta-optimization method for deep learning models uncovers global salient factors influencing model predictions, and generates explanations taking into account multiple analytical views, such as features, timesteps, and node locations. Our qualitative and quantitative experiments with real-world forecasting datasets show that our approach effectively extracts explanations of model predictions, and is competitive with state-of-the-art approaches.},
  archive      = {J_ML},
  author       = {Altieri, Massimiliano and Ceci, Michelangelo and Corizzo, Roberto},
  doi          = {10.1007/s10994-024-06733-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-47},
  shortjournal = {Mach. Learn.},
  title        = {An end-to-end explainability framework for spatio-temporal predictive modeling},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient descent fails to learn high-frequency functions and
modular arithmetic. <em>ML</em>, <em>114</em>(4), 1–30. (<a
href="https://doi.org/10.1007/s10994-025-06747-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function. A set of functions of the form $$x\rightarrow ax \bmod p$$ , where a is taken from $${{\mathbb {Z}}}_p$$ , has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of p-periodic functions on $${{\mathbb {Z}}}$$ and is tightly connected with a class of high-frequency periodic functions on the real line. We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that the variance of the gradient is negligibly small in both cases when either a frequency or the prime base p is large. This in turn prevents such a learning algorithm from being successful.},
  archive      = {J_ML},
  author       = {Takhanov, Rustem and Tezekbayev, Maxat and Pak, Artur and Bolatov, Arman and Assylbekov, Zhenisbek},
  doi          = {10.1007/s10994-025-06747-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {Gradient descent fails to learn high-frequency functions and modular arithmetic},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized median of means principle for bayesian
inference. <em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-025-06754-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of robustness is experiencing a resurgence of interest in the statistical and machine learning communities. In particular, robust algorithms making use of the so-called median of means estimator were shown to satisfy strong performance guarantees for many problems, including estimation of the mean, covariance structure as well as linear regression. In this work, we propose an extension of the median of means principle to the Bayesian framework, leading to the notion of the robust posterior distribution. In particular, we (a) quantify robustness of this posterior to outliers, (b) show that it satisfies a version of the Bernstein-von Mises theorem that connects Bayesian credible sets to the traditional confidence intervals, and (c) demonstrate that our approach performs well in applications.},
  archive      = {J_ML},
  author       = {Minsker, Stanislav and Yao, Shunan},
  doi          = {10.1007/s10994-025-06754-9},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Generalized median of means principle for bayesian inference},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mp---20">MP - 20</h2>
<ul>
<li><details>
<summary>
(2025). On the directional asymptotic approach in optimization
theory. <em>MP</em>, <em>209</em>(1), 859–937. (<a
href="https://doi.org/10.1007/s10107-024-02089-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a starting point of our research, we show that, for a fixed order $$\gamma \ge 1$$ , each local minimizer of a rather general nonsmooth optimization problem in Euclidean spaces is either M-stationary in the classical sense (corresponding to stationarity of order 1), satisfies stationarity conditions in terms of a coderivative construction of order $$\gamma $$ , or is asymptotically stationary with respect to a critical direction as well as order $$\gamma $$ in a certain sense. By ruling out the latter case with a constraint qualification not stronger than directional metric subregularity, we end up with new necessary optimality conditions comprising a mixture of limiting variational tools of orders 1 and $$\gamma $$ . These abstract findings are carved out for the broad class of geometric constraints and $$\gamma :=2$$ , and visualized by examples from complementarity-constrained and nonlinear semidefinite optimization. As a byproduct of the particular setting $$\gamma :=1$$ , our general approach yields new so-called directional asymptotic regularity conditions which serve as constraint qualifications guaranteeing M-stationarity of local minimizers. We compare these new regularity conditions with standard constraint qualifications from nonsmooth optimization. Further, we extend directional concepts of pseudo- and quasi-normality to arbitrary set-valued mappings. It is shown that these properties provide sufficient conditions for the validity of directional asymptotic regularity. Finally, a novel coderivative-like variational tool is used to construct sufficient conditions for the presence of directional asymptotic regularity. For geometric constraints, it is illustrated that all appearing objects can be calculated in terms of initial problem data.},
  archive      = {J_MP},
  author       = {Benko, Matúš and Mehlitz, Patrick},
  doi          = {10.1007/s10107-024-02089-w},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {859-937},
  shortjournal = {Math. Program.},
  title        = {On the directional asymptotic approach in optimization theory},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An asynchronous proximal bundle method. <em>MP</em>,
<em>209</em>(1), 825–857. (<a
href="https://doi.org/10.1007/s10107-024-02088-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a fully asynchronous proximal bundle method for solving non-smooth, convex optimization problems. The algorithm can be used as a drop-in replacement for classic bundle methods, i.e., the function must be given by a first-order oracle for computing function values and subgradients. The algorithm allows for an arbitrary number of master problem processes computing new candidate points and oracle processes evaluating functions at those candidate points. These processes share information by communication with a single supervisor process that resembles the main loop of a classic bundle method. All processes run in parallel and no explicit synchronization step is required. Instead, the asynchronous and possibly outdated results of the oracle computations can be seen as an inexact function oracle. Hence, we show the convergence of our method under weak assumptions very similar to inexact and incremental bundle methods. In particular, we show how the algorithm learns important structural properties of the functions to control the inaccuracy induced by the asynchronicity automatically such that overall convergence can be guaranteed.},
  archive      = {J_MP},
  author       = {Fischer, Frank},
  doi          = {10.1007/s10107-024-02088-x},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {825-857},
  shortjournal = {Math. Program.},
  title        = {An asynchronous proximal bundle method},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stackelberg risk preference design. <em>MP</em>,
<em>209</em>(1), 785–823. (<a
href="https://doi.org/10.1007/s10107-024-02083-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk measures are commonly used to capture the risk preferences of decision-makers (DMs). The decisions of DMs can be nudged or manipulated when their risk preferences are influenced by factors such as the availability of information about the uncertainties. This work proposes a Stackelberg risk preference design (STRIPE) problem to capture a designer’s incentive to influence DMs’ risk preferences. STRIPE consists of two levels. In the lower level, individual DMs in a population, known as the followers, respond to uncertainties according to their risk preference types. In the upper level, the leader influences the distribution of the types to induce targeted decisions and steers the follower’s preferences to it. Our analysis centers around the solution concept of approximate Stackelberg equilibrium that yields suboptimal behaviors of the players. We show the existence of the approximate Stackelberg equilibrium. The primitive risk perception gap, defined as the Wasserstein distance between the original and the target type distributions, is important in estimating the optimal design cost. We connect the leader’s optimality compromise on the cost with her ambiguity tolerance on the follower’s approximate solutions leveraging Lipschitzian properties of the lower level solution mapping. To obtain the Stackelberg equilibrium, we reformulate STRIPE into a single-level optimization problem using the spectral representations of law-invariant coherent risk measures. We create a data-driven approach for computation and study its performance guarantees. We apply STRIPE to contract design problems under approximate incentive compatibility. Moreover, we connect STRIPE with meta-learning problems and derive adaptation performance estimates of the meta-parameters.},
  archive      = {J_MP},
  author       = {Liu, Shutian and Zhu, Quanyan},
  doi          = {10.1007/s10107-024-02083-2},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {785-823},
  shortjournal = {Math. Program.},
  title        = {Stackelberg risk preference design},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding global minima via kernel approximations.
<em>MP</em>, <em>209</em>(1), 703–784. (<a
href="https://doi.org/10.1007/s10107-024-02081-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the global minimization of smooth functions based solely on function evaluations. Algorithms that achieve the optimal number of function evaluations for a given precision level typically rely on explicitly constructing an approximation of the function which is then minimized with algorithms that have exponential running-time complexity. In this paper, we consider an approach that jointly models the function to approximate and finds a global minimum. This is done by using infinite sums of square smooth functions and has strong links with polynomial sum-of-squares hierarchies. Leveraging recent representation properties of reproducing kernel Hilbert spaces, the infinite-dimensional optimization problem can be solved by subsampling in time polynomial in the number of function evaluations, and with theoretical guarantees on the obtained minimum. Given n samples, the computational cost is $$O(n^{3.5})$$ in time, $$O(n^2)$$ in space, and we achieve a convergence rate to the global optimum that is $$O(n^{-m/d + 1/2 + 3/d})$$ where m is the degree of differentiability of the function and d the number of dimensions. The rate is nearly optimal in the case of Sobolev functions and more generally makes the proposed method particularly suitable for functions with many derivatives. Indeed, when m is in the order of d, the convergence rate to the global optimum does not suffer from the curse of dimensionality, which affects only the worst-case constants (that we track explicitly through the paper).},
  archive      = {J_MP},
  author       = {Rudi, Alessandro and Marteau-Ferey, Ulysse and Bach, Francis},
  doi          = {10.1007/s10107-024-02081-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {703-784},
  shortjournal = {Math. Program.},
  title        = {Finding global minima via kernel approximations},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A normal fan projection algorithm for low-rank optimization.
<em>MP</em>, <em>209</em>(1), 681–702. (<a
href="https://doi.org/10.1007/s10107-024-02079-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise a method for minimizing a low-rank quasiconcave objective function over a polytope by first projecting the polytope’s normal fan, then using the projected fan to obtain candidate solutions. When the polytope’s maximal number of nonparallel edges is bounded by a polynomial in its dimension, our method solves the problem in time that is polynomial in the number of variables and exponential in the rank of the objective function. We discuss several problems from previous literature that can be solved efficiently using this method. In all cases, our proposed algorithm matches or improves on the running time of existing problem-specific algorithms, while providing the first polynomial-time algorithm we know of for finding a spanning tree on a graph with multiple edge weight types, such that the product of the different weight types is minimized.},
  archive      = {J_MP},
  author       = {Scott, James R. and Geunes, Joseph},
  doi          = {10.1007/s10107-024-02079-y},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {681-702},
  shortjournal = {Math. Program.},
  title        = {A normal fan projection algorithm for low-rank optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample complexity analysis for adaptive optimization
algorithms with stochastic oracles. <em>MP</em>, <em>209</em>(1),
651–679. (<a href="https://doi.org/10.1007/s10107-024-02078-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several classical adaptive optimization algorithms, such as line search and trust-region methods, have been recently extended to stochastic settings where function values, gradients, and Hessians in some cases, are estimated via stochastic oracles. Unlike the majority of stochastic methods, these methods do not use a pre-specified sequence of step size parameters, but adapt the step size parameter according to the estimated progress of the algorithm and use it to dictate the accuracy required from the stochastic oracles. The requirements on the stochastic oracles are, thus, also adaptive and the oracle costs can vary from iteration to iteration. The step size parameters in these methods can increase and decrease based on the perceived progress, but unlike the deterministic case they are not bounded away from zero due to possible oracle failures, and bounds on the step size parameter have not been previously derived. This creates obstacles in the total complexity analysis of such methods, because the oracle costs are typically decreasing in the step size parameter, and could be arbitrarily large as the step size parameter goes to 0. Thus, until now only the total iteration complexity of these methods has been analyzed. In this paper, we derive a lower bound on the step size parameter that holds with high probability for a large class of adaptive stochastic methods. We then use this lower bound to derive a framework for analyzing the expected and high probability total oracle complexity of any method in this class. Finally, we apply this framework to analyze the total sample complexity of two particular algorithms, STORM (Blanchet et al. in INFORMS J Optim 1(2):92–119, 2019) and SASS (Jin et al. in High probability complexity bounds for adaptive step search based on stochastic oracles, 2021. https://doi.org/10.48550/ARXIV.2106.06454 ), in the expected risk minimization problem.},
  archive      = {J_MP},
  author       = {Jin, Billy and Scheinberg, Katya and Xie, Miaolan},
  doi          = {10.1007/s10107-024-02078-z},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {651-679},
  shortjournal = {Math. Program.},
  title        = {Sample complexity analysis for adaptive optimization algorithms with stochastic oracles},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perseus: A simple and optimal high-order method for
variational inequalities. <em>MP</em>, <em>209</em>(1), 609–650. (<a
href="https://doi.org/10.1007/s10107-024-02075-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper settles an open and challenging question pertaining to the design of simple and optimal high-order methods for solving smooth and monotone variational inequalities (VIs). A VI involves finding $$x^\star \in {\mathcal {X}}$$ such that $$\langle F(x), x - x^\star \rangle \ge 0$$ for all $$x \in {\mathcal {X}}$$ . We consider the setting in which $$F: {\mathbb {R}}^d \rightarrow {\mathbb {R}}^d$$ is smooth with up to $$(p-1)^{\text {th}}$$ -order derivatives. For $$p = 2$$ , the cubic regularization of Newton’s method has been extended to VIs with a global rate of $$O(\epsilon ^{-1})$$ (Nesterov in Cubic regularization of Newton’s method for convex problems with constraints, Tech. rep., Université catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2006). An improved rate of $$O(\epsilon ^{-2/3}\log \log (1/\epsilon ))$$ can be obtained via an alternative second-order method, but this method requires a nontrivial line-search procedure as an inner loop. Similarly, the existing high-order methods based on line-search procedures have been shown to achieve a rate of $$O(\epsilon ^{-2/(p+1)}\log \log (1/\epsilon ))$$ (Bullins and Lai in SIAM J Optim 32(3):2208–2229, 2022; Jiang and Mokhtari in Generalized optimistic methods for convex–concave saddle point problems, 2022; Lin and Jordan in Math Oper Res 48(4):2353–2382, 2023). As emphasized by Nesterov (Lectures on convex optimization, vol 137, Springer, Berlin, 2018), however, such procedures do not necessarily imply the practical applicability in large-scale applications, and it is desirable to complement these results with a simple high-order VI method that retains the optimality of the more complex methods. We propose a $$p^{\text {th}}$$ -order method that does not require any line search procedure and provably converges to a weak solution at a rate of $$O(\epsilon ^{-2/(p+1)})$$ . We prove that our $$p^{\text {th}}$$ -order method is optimal in the monotone setting by establishing a lower bound of $$\Omega (\epsilon ^{-2/(p+1)})$$ under a generalized linear span assumption. A restarted version of our $$p^{\text {th}}$$ -order method attains a linear rate for smooth and $$p^{\text {th}}$$ -order uniformly monotone VIs and another restarted version of our $$p^{\text {th}}$$ -order method attains a local superlinear rate for smooth and strongly monotone VIs. Further, the similar $$p^{\text {th}}$$ -order method achieves a global rate of $$O(\epsilon ^{-2/p})$$ for solving smooth and nonmonotone VIs satisfying the Minty condition. Two restarted versions attain a global linear rate under additional $$p^{\text {th}}$$ -order uniform Minty condition and a local superlinear rate under additional strong Minty condition.},
  archive      = {J_MP},
  author       = {Lin, Tianyi and Jordan, Michael I.},
  doi          = {10.1007/s10107-024-02075-2},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {609-650},
  shortjournal = {Math. Program.},
  title        = {Perseus: A simple and optimal high-order method for variational inequalities},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-convex scenario optimization. <em>MP</em>,
<em>209</em>(1), 557–608. (<a
href="https://doi.org/10.1007/s10107-024-02074-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scenario optimization is an approach to data-driven decision-making that has been introduced some fifteen years ago and has ever since then grown fast. Its most remarkable feature is that it blends the heuristic nature of data-driven methods with a rigorous theory that allows one to gain factual, reliable, insight in the solution. The usability of the scenario theory, however, has been restrained thus far by the obstacle that most results are standing on the assumption of convexity. With this paper, we aim to free the theory from this limitation. Specifically, we focus on the body of results that are known under the name of “wait-and-judge” and show that its fundamental achievements maintain their validity in a non-convex setup. While optimization is a major center of attention, this paper travels beyond it and into data-driven decision making. Adopting such a broad framework opens the door to building a new theory of truly vast applicability.},
  archive      = {J_MP},
  author       = {Garatti, Simone and Campi, Marco C.},
  doi          = {10.1007/s10107-024-02074-3},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {557-608},
  shortjournal = {Math. Program.},
  title        = {Non-convex scenario optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated first-order methods for a class of semidefinite
programs. <em>MP</em>, <em>209</em>(1), 503–556. (<a
href="https://doi.org/10.1007/s10107-024-02073-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new storage-optimal first-order method, CertSDP, for solving a special class of semidefinite programs (SDPs) to high accuracy. The class of SDPs that we consider, the exact QMP-like SDPs, is characterized by low-rank solutions, a priori knowledge of the restriction of the SDP solution to a small subspace, and standard regularity assumptions such as strict complementarity. Crucially, we show how to use a certificate of strict complementarity to construct a low-dimensional strongly convex minimax problem whose optimizer coincides with a factorization of the SDP optimizer. From an algorithmic standpoint, we show how to construct the necessary certificate and how to solve the minimax problem efficiently. Our algorithms for strongly convex minimax problems with inexact prox maps may be of independent interest. We accompany our theoretical results with preliminary numerical experiments suggesting that CertSDP significantly outperforms current state-of-the-art methods on large sparse exact QMP-like SDPs.},
  archive      = {J_MP},
  author       = {Wang, Alex L. and Kılınç-Karzan, Fatma},
  doi          = {10.1007/s10107-024-02073-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {503-556},
  shortjournal = {Math. Program.},
  title        = {Accelerated first-order methods for a class of semidefinite programs},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sum-of-squares relaxations for polynomial min–max problems
over simple sets. <em>MP</em>, <em>209</em>(1), 475–501. (<a
href="https://doi.org/10.1007/s10107-024-02072-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider min–max optimization problems for polynomial functions, where a multivariate polynomial is maximized with respect to a subset of variables, and the resulting maximal value is minimized with respect to the remaining variables. When the variables belong to simple sets (e.g., a hypercube, the Euclidean hypersphere, or a ball), we derive a sum-of-squares formulation based on a primal-dual approach. In the simplest setting, we provide a convergence proof when the degree of the relaxation tends to infinity and observe empirically that it can be finitely convergent in several situations. Moreover, our formulation leads to an interesting link with feasibility certificates for polynomial inequalities based on Putinar’s Positivstellensatz.},
  archive      = {J_MP},
  author       = {Bach, Francis},
  doi          = {10.1007/s10107-024-02072-5},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {475-501},
  shortjournal = {Math. Program.},
  title        = {Sum-of-squares relaxations for polynomial min–max problems over simple sets},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence rates for sums-of-squares hierarchies with
correlative sparsity. <em>MP</em>, <em>209</em>(1), 435–473. (<a
href="https://doi.org/10.1007/s10107-024-02071-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work derives upper bounds on the convergence rate of the moment-sum-of-squares hierarchy with correlative sparsity for global minimization of polynomials on compact basic semialgebraic sets. The main conclusion is that both sparse hierarchies based on the Schmüdgen and Putinar Positivstellensätze enjoy a polynomial rate of convergence that depends on the size of the largest clique in the sparsity graph but not on the ambient dimension. Interestingly, the sparse bounds outperform the best currently available bounds for the dense hierarchy when the maximum clique size is sufficiently small compared to the ambient dimension and the performance is measured by the running time of an interior point method required to obtain a bound on the global minimum of a given accuracy.},
  archive      = {J_MP},
  author       = {Korda, Milan and Magron, Victor and Ríos-Zertuche, Rodolfo},
  doi          = {10.1007/s10107-024-02071-6},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {435-473},
  shortjournal = {Math. Program.},
  title        = {Convergence rates for sums-of-squares hierarchies with correlative sparsity},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polyhedral properties of RLT relaxations of nonconvex
quadratic programs and their implications on exact relaxations.
<em>MP</em>, <em>209</em>(1), 397–433. (<a
href="https://doi.org/10.1007/s10107-024-02070-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study linear programming relaxations of nonconvex quadratic programs given by the reformulation–linearization technique (RLT), referred to as RLT relaxations. We investigate the relations between the polyhedral properties of the feasible regions of a quadratic program and its RLT relaxation. We establish various connections between recession directions, boundedness, and vertices of the two feasible regions. Using these properties, we present a complete description of the set of instances that admit an exact RLT relaxation. We then give a thorough discussion of how our results can be converted into simple algorithmic procedures to construct instances of quadratic programs with exact, inexact, or unbounded RLT relaxations.},
  archive      = {J_MP},
  author       = {Qiu, Yuzhou and Yıldırım, E. Alper},
  doi          = {10.1007/s10107-024-02070-7},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {397-433},
  shortjournal = {Math. Program.},
  title        = {Polyhedral properties of RLT relaxations of nonconvex quadratic programs and their implications on exact relaxations},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The chvátal–gomory procedure for integer SDPs with
applications in combinatorial optimization. <em>MP</em>,
<em>209</em>(1), 323–395. (<a
href="https://doi.org/10.1007/s10107-024-02069-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study the well-known Chvátal–Gomory (CG) procedure for the class of integer semidefinite programs (ISDPs). We prove several results regarding the hierarchy of relaxations obtained by iterating this procedure. We also study different formulations of the elementary closure of spectrahedra. A polyhedral description of the elementary closure for a specific type of spectrahedra is derived by exploiting total dual integrality for SDPs. Moreover, we show how to exploit (strengthened) CG cuts in a branch-and-cut framework for ISDPs. Different from existing algorithms in the literature, the separation routine in our approach exploits both the semidefinite and the integrality constraints. We provide separation routines for several common classes of binary SDPs resulting from combinatorial optimization problems. In the second part of the paper we present a comprehensive application of our approach to the quadratic traveling salesman problem (QTSP). Based on the algebraic connectivity of the directed Hamiltonian cycle, two ISDPs that model the QTSP are introduced. We show that the CG cuts resulting from these formulations contain several well-known families of cutting planes. Numerical results illustrate the practical strength of the CG cuts in our branch-and-cut algorithm, which outperforms alternative ISDP solvers and is able to solve large QTSP instances to optimality.},
  archive      = {J_MP},
  author       = {de Meijer, Frank and Sotirov, Renata},
  doi          = {10.1007/s10107-024-02069-0},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {323-395},
  shortjournal = {Math. Program.},
  title        = {The Chvátal–Gomory procedure for integer SDPs with applications in combinatorial optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized minimum 0-extension problem and discrete
convexity. <em>MP</em>, <em>209</em>(1), 279–322. (<a
href="https://doi.org/10.1007/s10107-024-02064-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a fixed finite metric space $$(V,\mu )$$ , the minimum 0-extension problem, denoted as $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ , is equivalent to the following optimization problem: minimize function of the form $$\min \nolimits _{x\in V^n} \sum _i f_i(x_i) + \sum _{ij} c_{ij}\hspace{0.5pt}\mu (x_i,x_j)$$ where $$f_i:V\rightarrow \mathbb {R}$$ are functions given by $$f_i(x_i)=\sum _{v\in V} c_{vi}\hspace{0.5pt}\mu (x_i,v)$$ and $$c_{ij},c_{vi}$$ are given nonnegative costs. The computational complexity of $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ has been recently established by Karzanov and by Hirai: if metric $$\mu $$ is orientable modular then $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ can be solved in polynomial time, otherwise $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ is NP-hard. To prove the tractability part, Hirai developed a theory of discrete convex functions on orientable modular graphs generalizing several known classes of functions in discrete convex analysis, such as $$L^\natural $$ -convex functions. We consider a more general version of the problem in which unary functions $$f_i(x_i)$$ can additionally have terms of the form $$c_{uv;i}\hspace{0.5pt}\mu (x_i,\{u,v\})$$ for $$\{u,\!\hspace{0.5pt}\hspace{0.5pt}v\}\in F$$ , where set $$F\subseteq \left( {\begin{array}{c}V\\ 2\end{array}}\right) $$ is fixed. We extend the complexity classification above by providing an explicit condition on $$(\mu ,F)$$ for the problem to be tractable. In order to prove the tractability part, we generalize Hirai’s theory and define a larger class of discrete convex functions. It covers, in particular, another well-known class of functions, namely submodular functions on an integer lattice. Finally, we improve the complexity of Hirai’s algorithm for solving $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ on orientable modular graphs.},
  archive      = {J_MP},
  author       = {Dvorak, Martin and Kolmogorov, Vladimir},
  doi          = {10.1007/s10107-024-02064-5},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {279-322},
  shortjournal = {Math. Program.},
  title        = {Generalized minimum 0-extension problem and discrete convexity},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized nash equilibrium problems with mixed-integer
variables. <em>MP</em>, <em>209</em>(1), 231–277. (<a
href="https://doi.org/10.1007/s10107-024-02063-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider generalized Nash equilibrium problems (GNEPs) with non-convex strategy spaces and non-convex cost functions. This general class of games includes the important case of games with mixed-integer variables for which only a few results are known in the literature. We present a new approach to characterize equilibria via a convexification technique using the Nikaido–Isoda function. To any given instance of the GNEP, we construct a set of convexified instances and show that a feasible strategy profile is an equilibrium for the original instance if and only if it is an equilibrium for any convexified instance and the convexified cost functions coincide with the initial ones. We develop this convexification approach along three dimensions: We first show that for quasi-linear models, where a convexified instance exists in which for fixed strategies of the opponent players, the cost function of every player is linear and the respective strategy space is polyhedral, the convexification reduces the GNEP to a standard (non-linear) optimization problem. Secondly, we derive two complete characterizations of those GNEPs for which the convexification leads to a jointly constrained or a jointly convex GNEP, respectively. These characterizations require new concepts related to the interplay of the convex hull operator applied to restricted subsets of feasible strategies and may be interesting on their own. Note that this characterization is also computationally relevant as jointly convex GNEPs have been extensively studied in the literature. Finally, we demonstrate the applicability of our results by presenting a numerical study regarding the computation of equilibria for three classes of GNEPs related to integral network flows and discrete market equilibria.},
  archive      = {J_MP},
  author       = {Harks, Tobias and Schwarz, Julian},
  doi          = {10.1007/s10107-024-02063-6},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {231-277},
  shortjournal = {Math. Program.},
  title        = {Generalized nash equilibrium problems with mixed-integer variables},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hessian barrier algorithms for non-convex conic
optimization. <em>MP</em>, <em>209</em>(1), 171–229. (<a
href="https://doi.org/10.1007/s10107-024-02062-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key problem in mathematical imaging, signal processing and computational statistics is the minimization of non-convex objective functions that may be non-differentiable at the relative boundary of the feasible set. This paper proposes a new family of first- and second-order interior-point methods for non-convex optimization problems with linear and conic constraints, combining logarithmically homogeneous barriers with quadratic and cubic regularization respectively. Our approach is based on a potential-reduction mechanism and, under the Lipschitz continuity of the corresponding derivative with respect to the local barrier-induced norm, attains a suitably defined class of approximate first- or second-order KKT points with worst-case iteration complexity $$O(\varepsilon ^{-2})$$ (first-order) and $$O(\varepsilon ^{-3/2})$$ (second-order), respectively. Based on these findings, we develop new path-following schemes attaining the same complexity, modulo adjusting constants. These complexity bounds are known to be optimal in the unconstrained case, and our work shows that they are upper bounds in the case with complicated constraints as well. To the best of our knowledge, this work is the first which achieves these worst-case complexity bounds under such weak conditions for general conic constrained non-convex optimization problems.},
  archive      = {J_MP},
  author       = {Dvurechensky, Pavel and Staudigl, Mathias},
  doi          = {10.1007/s10107-024-02062-7},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {171-229},
  shortjournal = {Math. Program.},
  title        = {Hessian barrier algorithms for non-convex conic optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated tight lyapunov analysis for first-order methods.
<em>MP</em>, <em>209</em>(1), 133–170. (<a
href="https://doi.org/10.1007/s10107-024-02061-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a methodology for establishing the existence of quadratic Lyapunov inequalities for a wide range of first-order methods used to solve convex optimization problems. In particular, we consider (i) classes of optimization problems of finite-sum form with (possibly strongly) convex and possibly smooth functional components, (ii) first-order methods that can be written as a linear system on state-space form in feedback interconnection with the subdifferentials of the functional components of the objective function, and (iii) quadratic Lyapunov inequalities that can be used to draw convergence conclusions. We present a necessary and sufficient condition for the existence of a quadratic Lyapunov inequality within a predefined class of Lyapunov inequalities, which amounts to solving a small-sized semidefinite program. We showcase our methodology on several first-order methods that fit the framework. Most notably, our methodology allows us to significantly extend the region of parameter choices that allow for duality gap convergence in the Chambolle–Pock method when the linear operator is the identity mapping.},
  archive      = {J_MP},
  author       = {Upadhyaya, Manu and Banert, Sebastian and Taylor, Adrien B. and Giselsson, Pontus},
  doi          = {10.1007/s10107-024-02061-8},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {133-170},
  shortjournal = {Math. Program.},
  title        = {Automated tight lyapunov analysis for first-order methods},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convex hulls of monomial curves, and a sparse
positivstellensatz. <em>MP</em>, <em>209</em>(1), 113–131. (<a
href="https://doi.org/10.1007/s10107-024-02060-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the closed convex hull K of a monomial curve given parametrically as $$(t^{m_1},\ldots ,t^{m_n})$$ , with the parameter t varying in an interval I. We show, using constructive arguments, that K admits a lifted semidefinite description by $$\mathcal {O}(d)$$ linear matrix inequalities (LMIs), each of size $$\left\lfloor \frac{n}{2} \right\rfloor +1$$ , where $$d= \max \{m_1,\ldots ,m_n\}$$ is the degree of the curve. On the dual side, we show that if a univariate polynomial p(t) of degree d with at most $$2k+1$$ monomials is non-negative on $${\mathbb {R}}_+$$ , then p admits a representation $$p = t^0 \sigma _0 + \cdots + t^{d-k} \sigma _{d-k}$$ , where the polynomials $$\sigma _0,\ldots ,\sigma _{d-k}$$ are sums of squares and $$\deg (\sigma _i) \le 2k$$ . The latter is a univariate positivstellensatz for sparse polynomials, with non-negativity of p being certified by sos polynomials whose degree only depends on the sparsity of p. Our results fit into the general attempt of formulating polynomial optimization problems as semidefinite problems with LMIs of small size. Such small-size descriptions are much more tractable from a computational viewpoint.},
  archive      = {J_MP},
  author       = {Averkov, Gennadiy and Scheiderer, Claus},
  doi          = {10.1007/s10107-024-02060-9},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {113-131},
  shortjournal = {Math. Program.},
  title        = {Convex hulls of monomial curves, and a sparse positivstellensatz},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of smooth parametrizations on nonconvex
optimization landscapes. <em>MP</em>, <em>209</em>(1), 63–111. (<a
href="https://doi.org/10.1007/s10107-024-02058-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop new tools to study landscapes in nonconvex optimization. Given one optimization problem, we pair it with another by smoothly parametrizing the domain. This is either for practical purposes (e.g., to use smooth optimization algorithms with good guarantees) or for theoretical purposes (e.g., to reveal that the landscape satisfies a strict saddle property). In both cases, the central question is: how do the landscapes of the two problems relate? More precisely: how do desirable points such as local minima and critical points in one problem relate to those in the other problem? A key finding in this paper is that these relations are often determined by the parametrization itself, and are almost entirely independent of the cost function. Accordingly, we introduce a general framework to study parametrizations by their effect on landscapes. The framework enables us to obtain new guarantees for an array of problems, some of which were previously treated on a case-by-case basis in the literature. Applications include: optimizing low-rank matrices and tensors through factorizations; solving semidefinite programs via the Burer–Monteiro approach; training neural networks by optimizing their weights and biases; and quotienting out symmetries.},
  archive      = {J_MP},
  author       = {Levin, Eitan and Kileel, Joe and Boumal, Nicolas},
  doi          = {10.1007/s10107-024-02058-3},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {63-111},
  shortjournal = {Math. Program.},
  title        = {The effect of smooth parametrizations on nonconvex optimization landscapes},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Level constrained first order methods for function
constrained optimization. <em>MP</em>, <em>209</em>(1), 1–61. (<a
href="https://doi.org/10.1007/s10107-024-02057-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new feasible proximal gradient method for constrained optimization where both the objective and constraint functions are given by summation of a smooth, possibly nonconvex function and a convex simple function. The algorithm converts the original problem into a sequence of convex subproblems. Formulating those subproblems requires the evaluation of at most one gradient-value of the original objective and constraint functions. Either exact or approximate subproblems solutions can be computed efficiently in many cases. An important feature of the algorithm is the constraint level parameter. By carefully increasing this level for each subproblem, we provide a simple solution to overcome the challenge of bounding the Lagrangian multipliers and show that the algorithm follows a strictly feasible solution path till convergence to the stationary point. We develop a simple, proximal gradient descent type analysis, showing that the complexity bound of this new algorithm is comparable to gradient descent for the unconstrained setting which is new in the literature. Exploiting this new design and analysis technique, we extend our algorithms to some more challenging constrained optimization problems where (1) the objective is a stochastic or finite-sum function, and (2) structured nonsmooth functions replace smooth components of both objective and constraint functions. Complexity results for these problems also seem to be new in the literature. Finally, our method can also be applied to convex function constrained problems where we show complexities similar to the proximal gradient method.},
  archive      = {J_MP},
  author       = {Boob, Digvijay and Deng, Qi and Lan, Guanghui},
  doi          = {10.1007/s10107-024-02057-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {1-61},
  shortjournal = {Math. Program.},
  title        = {Level constrained first order methods for function constrained optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mva---23">MVA - 23</h2>
<ul>
<li><details>
<summary>
(2025). Adversarial learning for unguided single depth map
completion of indoor scenes. <em>MVA</em>, <em>36</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s00138-024-01652-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single depth map completion in the absence of any guidance from color images is a challenging, ill-posed problem in computer vision. Most of the conventional depth map completion approaches rely on information extracted from the corresponding color image and require heavy computations and optimization-based postprocessing functions, which cannot yield results in real time. Successful application of generative adversarial networks has led to significant progress in several computer vision problems including, color image inpainting. However, contrasting local and non-local features of depth maps compared to color images prevents the direct application of deep learning models designed for color image inpainting to depth map completion. Motivated by these challenges, in this work we propose to use deep adversarial learning to derive plausible estimates of missing depth information in a single degraded observation without any guidance from the corresponding RGB frame and any postprocessing. Different types of depth map degradations, such as simulated random and textual missing pixels as well as contiguous large holes found in Kinect depth maps, are effectively handled to reconstruct clean depth maps. An ablation study is also performed to investigate the contribution of our adversarial network architecture towards the recovery of missing scene depth information. We carry out an illustrative experimental analysis on the NYU-Depth V2 dataset and perform zero-shot generalization on the Middlebury and Matterport3D datasets, comparing our proposed method with several state-of-the-art algorithms. The experimental results demonstrate robustness and efficacy of the proposed approach.},
  archive      = {J_MVA},
  author       = {Medhi, Moushumi and Ranjan Sahay, Rajiv},
  doi          = {10.1007/s00138-024-01652-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adversarial learning for unguided single depth map completion of indoor scenes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A computer vision system for recognition and defect
detection for reusable containers. <em>MVA</em>, <em>36</em>(2), 1–19.
(<a href="https://doi.org/10.1007/s00138-024-01636-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small load carriers (SLCs) are standardized reusable containers used to transport and protect customer goods in many manufacturers. Throughout the life cycle of the SLCs, they will be collected, manually checked for defects (wear, cracks, and residue on the surface), and cleaned by specialized logistic companies. Human operators in small to medium-sized companies manually evaluate the defects due to the variety and degree of possible defects and varying customer needs. This manual evaluation is not scalable and prone to errors. This work aims to fill this gap by proposing a computer vision system that can recognize the SLC type for inventory management and perform defect detection automatically. First, we develop a camera portal, consisting of standard components, that capture the relevant surfaces of the SLC. A labeled dataset of 17,530 images of 34 different SLCs with their defect status was recorded using this camera portal. We trained a classification model (ConvNeXt) using our dataset to predict the different types of SLCs achieving 100% class prediction accuracy. For defect detection, we explore eight state-of-the-art (SOTA) anomaly detection models that achieved high rankings in the MVTec industrial anomaly detection benchmark. These models are trained using default hyperparameters and the two highest-scoring models were chosen and fine-tuned. The best-fine-tuned models based on “Area under the Receiver Operating Characteristic Curve (AUROC)” are PatchCore (0.811) and DRAEM (0.748). These results indicate that there is still potential for improvement in the automation of defect detection of SLCs.},
  archive      = {J_MVA},
  author       = {Wahyudi, Vincent and Ziegler, Cedric C. and Frieß, Matthias and Schramm, Stefan and Lang, Constantin and Eberhardt, Lars and Freund, Fabian and Dobhan, Alexander and Storath, Martin},
  doi          = {10.1007/s00138-024-01636-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A computer vision system for recognition and defect detection for reusable containers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-enhanced feature mapping network for
visible-infrared person re-identification. <em>MVA</em>, <em>36</em>(2),
1–17. (<a href="https://doi.org/10.1007/s00138-024-01646-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-Identification (VI-ReID) plays a pivotal role in surveillance systems, enabling the accurate identification of individuals across varying times and locations. Traditional methods struggle in low-light conditions, which motivates our research. We introduce an Attention-Enhanced Feature Mapping Network (AEFMNet) that addresses both intra-modal and inter-modal discrepancies. Our AEFMNet employs an Attention-based Feature Fusion Module (AFFM) to enhance global feature representation and a GCN-based Feature Mapping Module (GFMM) to reduce cross-modal feature gaps. The proposed network is further strengthened by a Joint Training Algorithm (JTA) that integrates multi-scale local and global features, enhancing cross-modal matching accuracy. Our approach achieves advanced performance on three large-scale data sets, demonstrating its effectiveness and robustness.},
  archive      = {J_MVA},
  author       = {Liu, Shuaiyi and Han, Ke},
  doi          = {10.1007/s00138-024-01646-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Attention-enhanced feature mapping network for visible-infrared person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Region gradient-guided diffusion model for underwater image
enhancement. <em>MVA</em>, <em>36</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s00138-024-01647-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is a critical challenge in marine visual perception and underwater robotics due to complex aquatic environments that severely degrade image quality. This paper introduces the region gradient-guided diffusion model (RGGDM), a novel framework that addresses the fundamental trade-off between local detail preservation and global consistency in UIE. RGGDM innovatively integrates a region gradient-guided mechanism with a hybrid Swin-ConvNeXt architecture, introducing a spatially adaptive denoising process governed by gradient discrepancies between input and target images. We propose a learnable parameter $$\delta $$ that dynamically modulates denoising intensity, focusing computational resources on semantically salient regions. Our approach is underpinned by rigorous mathematical analysis, demonstrating convergence properties under mild assumptions and providing theoretical guarantees for the model’s stability and effectiveness. The synergistic combination of Swin Transformer and ConvNeXt enhances feature representation, significantly improving both perceptual quality and pixel-level accuracy. Extensive experiments on benchmark datasets demonstrate RGGDM’s superior performance, consistently outperforming state-of-the-art methods across multiple evaluation metrics. Notably, RGGDM achieves a peak signal-to-noise ratio (PSNR) of 25.48 dB and an underwater image quality measure (UIQM) of 4.37 on the UIEB dataset. Furthermore, enhanced images show substantial improvements in downstream tasks such as SIFT feature matching, with an average increase of 132.19% in matching points. These results underscore RGGDM’s potential in advancing underwater visual perception and its broader implications for marine robotics and environmental monitoring applications.},
  archive      = {J_MVA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s00138-024-01647-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Region gradient-guided diffusion model for underwater image enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-core token mixer: A novel approach for underwater
image enhancement. <em>MVA</em>, <em>36</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01651-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is critical in various applications, including marine biology research, underwater archaeology, and autonomous underwater vehicle (AUV) navigation. The unpredictable nature of underwater environments frequently leads to degradation in contrast, color, and perceptual visual quality. Previous methods using the single receptive field to extract features are not capable of handling varying light conditions, which hinders detail preservation, color correction, and image quality improvement. To address these challenges, we propose Multi Core Token Mixer (MCTM) by introducing a distinctive multi-core mechanism. This mechanism is adept at extracting varied receptive fields, thereby enabling the model to capture the degradation at different scales caused by inhomogeneous underwater conditions. We performed experiments on three datasets (UIEB, EUVP, and UFO-120), and MCTM consistently outperforms existing models in image enhancement, color correction, and perceptual visual quality. Our work sets a new standard in the field and emphasizes the promise held by task-specific architectures that harness the power of Transformer models to tackle domain-specific challenges, particularly in UIE.},
  archive      = {J_MVA},
  author       = {Xu, Tianrun and Xu, Shiyuan and Chen, Xue and Chen, Feng and Li, Hongjue},
  doi          = {10.1007/s00138-024-01651-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-core token mixer: A novel approach for underwater image enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A concept-aware explainability method for convolutional
neural networks. <em>MVA</em>, <em>36</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01653-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Convolutional Neural Networks (CNN) outperform the classical models in a wide range of Machine Vision applications, their restricted interpretability and their lack of comprehensibility in reasoning, generate many problems such as security, reliability, and safety. Consequently, there is a growing need for research to improve explainability and address their limitations. In this paper, we propose a concept-based method, called Concept-Aware Explainability (CAE) to provide a verbal explanation for the predictions of pre-trained CNN models. A new measure, called detection score mean, is introduced to quantify the relationship between the filters of the model and a set of pre-defined concepts. Based on the detection score mean values, we define sorted lists of Concept-Aware Filters (CAF) and Filter-Activating Concepts (FAC). These lists are used to generate explainability reports, where we can explain, analyze, and compare models in terms of the concepts embedded in the image. The proposed explainability method is compared to the state-of-the-art methods to explain Resnet18 and VGG16 models, pre-trained on ImageNet and Places365-Standard datasets. Two popular metrics, namely, the number of unique detectors and the number of detecting filters, are used to make a quantitative comparison. Superior performances are observed for the suggested CAE, when compared to Network Dissection (NetDis) (Bau et al., in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017), Net2Vec (Fong and Vedaldi, in: Paper presented at IEEE conference on computer vision and pattern recognition (CVPR), 2018), and CLIP-Dissect (CLIP-Dis) (Oikarinen and Weng, in: The 11th international conference on learning representations (ICLR), 2023) methods.},
  archive      = {J_MVA},
  author       = {Gurkan, Mustafa Kagan and Arica, Nafiz and Yarman Vural, Fatos T.},
  doi          = {10.1007/s00138-024-01653-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A concept-aware explainability method for convolutional neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end unsupervised learning of latent-space clustering
for image segmentation via fully dense-UNet and fuzzy c-means loss.
<em>MVA</em>, <em>36</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01654-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental unsupervised approach in machine learning for grouping tasks. Image segmentation is one of the main applications of clustering and a preliminary requirement for most high-level applications in computer vision and scene understanding. However, parameter tuning requirements of conventional unsupervised image segmentation approaches limit their application. Deep learning approaches are capable of diverse and discriminate feature learning, however supervised learning paradigm and computational complexity of deep neural networks (DNNs) induces bottlenecks for real-time applications. We present unsupervised learning paradigm for fully dense-UNet (FDU-Net) model training with loss constraints: Semantic loss, Fuzzy C-means Clustering (FCM) loss, and Total Variation (TV) loss. Semantic loss works by selecting maximum activation class for each pixel spatial location and Simple Linear Iterative Clustering (SLIC)-based spatial refinement provides a coherent feature representation for model optimisation. FCM loss is based on the objective function of the conventional unsupervised Fuzzy C-means algorithm loss function. TV loss computes and minimises the spatial discontinuities in the FDU-Net activation maps. Loss constraints operate in tandem to ensure the control of false positives and false negatives. We conduct extensive experiments to compare our proposed method with unsupervised conventional and contemporary deep learning-driven (DL) methods. We experimentally demonstrate that the proposed method yields competitive quantitative and, most importantly, qualitative segmentation results, on the unseen images from the BSDS500 benchmark dataset. During inference, the segmentation quality of the proposed approach results is more significant than the contemporary DL-based and conventional clustering methods while reducing the computation cost by several folds.},
  archive      = {J_MVA},
  author       = {Khan, Zubair and Khan, Tehreem and Sattar, Mohsin and Yang, Jie},
  doi          = {10.1007/s00138-024-01654-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {End-to-end unsupervised learning of latent-space clustering for image segmentation via fully dense-UNet and fuzzy C-means loss},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WTT: Combining wavelet transform with transformer for remote
sensing image super-resolution. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01655-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, most deep learning-based super-resolution techniques primarily operate in the spatial domain, utilizing similar methods to process high- and low-frequency information in images. However, this often results in edge blurring. To address this issue, this paper introduces a novel structure that integrates wavelet transform and transformer mechanisms. The proposed method effectively segregates high- and low-frequency image information via discrete wavelet transform (DWT) and learns their correlations through a self-attention mechanism to enhance super-resolution outcomes. Specifically, the input image/feature is decomposed into four frequency domain components using DWT, which are concatenated to form a full-frequency domain feature map. A high-frequency feature map is constructed from three of these components. A new feature map is then generated using multi-head self-attention, with the full-frequency domain feature map serving as the query and value, and the high-frequency feature map as the key. The output feature map is produced by applying inverse DWT, with the new feature map serving as the low-frequency component and the original high-frequency components retained. Additionally, a parallel 1 × 1 convolution filter is employed to minimize information loss. Furthermore, a super-resolution network for remote sensing images is constructed by combining wavelet transform and transformer, incorporating hierarchical residual connections to enable the network to focus on learning high-frequency information. Experimental results on a publicly available remote sensing dataset demonstrate the superiority of the proposed method compared to existing approaches.},
  archive      = {J_MVA},
  author       = {Liu, Jingyi and Yang, Xiaomin},
  doi          = {10.1007/s00138-024-01655-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {WTT: Combining wavelet transform with transformer for remote sensing image super-resolution},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environmental factors-aware two-stream GCN for
skeleton-based behavior recognition. <em>MVA</em>, <em>36</em>(2), 1–12.
(<a href="https://doi.org/10.1007/s00138-024-01656-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of human behavior recognition, modeling human skeletons as spatio-temporal graphs using Graph Convolutional Networks (GCNs) has achieved outstanding performance. Existing GCN-based methods typically focus on the two-dimensional or three-dimensional features of the skeleton. However, the same action may represent different behaviors in different environments, making it suboptimal to consider only skeleton features for behavior recognition tasks with diverse scenarios. To simultaneously account for both skeleton features and environmental factors that influence human behaviors, this study proposed a novel two-stream Graph Convolutional Network, 2S-EGCN, which incorporates environmental factors for human behavior recognition. In this network, we designed an innovative environmental factor sampling strategy that samples fixed-scale environmental factors from variable-scale feature maps. To better integrate environmental factors with skeleton features, we further developed a Skeleton-Environment Interaction Module. This module uses a specific feature fusion method to combine environmental factors with skeleton features, allowing for the modeling of both pure skeleton information and skeleton information fused with environmental factors, thus improving behavior recognition accuracy. Extensive experiments conducted on the large Kinetics dataset demonstrate that our model outperforms the state-of-the-art, improving top-1 accuracy by 1.71–55.61% and achieving top-5 accuracy of 93.41%.},
  archive      = {J_MVA},
  author       = {Li, Zhuoran and Yan, Lianshan and Li, Hua and Wang, Yu},
  doi          = {10.1007/s00138-024-01656-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Environmental factors-aware two-stream GCN for skeleton-based behavior recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry-induced ambiguity in orientation estimation from
RGB images. <em>MVA</em>, <em>36</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01657-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of object orientation from RGB images is a core component in many modern computer vision pipelines. Traditional techniques mostly predict a single orientation per image, learning a one-to-one mapping between images and rotations. However, when objects exhibit rotational symmetries, they can appear identical from multiple viewpoints. This induces ambiguity in the estimation problem, making images map to rotations in a one-to-many fashion. In this paper, we explore several ways of addressing this problem. In doing so, we specifically consider algorithms that can map an image to a range of multiple rotation estimates, accounting for symmetry-induced ambiguity. Our contributions are threefold. Firstly, we create a data set with annotated symmetry information that covers symmetries induced through self-occlusion. Secondly, we compare and evaluate various learning strategies for multiple-hypothesis prediction models applied to orientation estimation. Finally, we propose to model orientation estimation as a binary classification problem. To this end, based on existing work from the field of shape reconstruction, we design a neural network that can be sampled to reconstruct the full range of ambiguous rotations for a given image. Quantitative evaluation on our annotated data set demonstrates its performance and motivates our design choices.},
  archive      = {J_MVA},
  author       = {Bertens, Tijn and Caasenbrood, Brandon and Saccon, Alessandro and Jalba, Andrei},
  doi          = {10.1007/s00138-024-01657-6},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Symmetry-induced ambiguity in orientation estimation from RGB images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ipdm: Identity preserving diffusion model for face sketch
and photo synthesis. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01658-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face sketch and photo synthesis is widely applied in industry and information fields, such as entertainment business and heterogeneous face retrieval. The key challenge lies in completing a face transformation with both good visual effects and face identity preservation. However, existing methods are still difficult to obtain a good synthesis due to the large model gap between the two different face domains. Recently, diffusion models have achieved great success in image synthesis, which allows us to extend its application in such a face generation task. Thus, we propose IPDM, which constructs a mapping of latent representation for domain-adaptive face features. The other proposed IDP utilizes auxiliary features to correct the latent features through their directions and supplementary identity information, so that the generation can keep face identity unchanged. The various evaluation results show that our method is superior to state-of-the-art methods in both identity preservation and visual effects.},
  archive      = {J_MVA},
  author       = {Tang, Duoxun and Jiang, Xinhang and Zhang, Ying and Dai, Yuhang and Lin, Ye},
  doi          = {10.1007/s00138-024-01658-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ipdm: Identity preserving diffusion model for face sketch and photo synthesis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personvit: Large-scale self-supervised vision transformer
for person re-identification. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01659-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (ReID) aims to retrieve relevant individuals in non-overlapping camera images and has a wide range of applications in the field of public safety. In recent years, with the development of Vision Transformer (ViT) and self-supervised learning techniques, the performance of person ReID based on self-supervised pre-training has been greatly improved. Person ReID requires extracting highly discriminative local fine-grained features of the human body, while traditional ViT is good at extracting context-related global features, making it difficult to focus on local human body features. To this end, this article introduces the recently emerged Masked Image Modeling (MIM) self-supervised learning method into person ReID, and effectively extracts high-quality global and local features through large-scale unsupervised pre-training by combining masked image modeling and discriminative contrastive learning, and then conducts supervised fine-tuning training in the person ReID task. This person feature extraction method based on ViT with masked image modeling (PersonViT) has the good characteristics of unsupervised, scalable, and strong generalization capabilities, overcoming the problem of difficult annotation in supervised person ReID, and achieves state-of-the-art results on publicly available benchmark datasets, including MSMT17, Market1501, DukeMTMC-reID, and Occluded-Duke. The code and pre-trained models of the PersonViT method are released at https://github.com/hustvl/PersonViT to promote further research in the person ReID field.},
  archive      = {J_MVA},
  author       = {Hu, Bin and Wang, Xinggang and Liu, Wenyu},
  doi          = {10.1007/s00138-025-01659-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Personvit: Large-scale self-supervised vision transformer for person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversified image style transfer—approaches, new methods and
directed variability control. <em>MVA</em>, <em>36</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s00138-025-01660-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image style transfer is to automatically redraw an input image in the style of another image, such as an artist’s painting. The disadvantage of conventional stylization algorithms is the uniqueness of result. If the user is not satisfied with the way the style was transferred, he has no option to remake the stylization. The paper provides an overview of existing style transfer methods that generate diverse results after each run and proposes two new methods. The first method enables diversity by concatenating a random vector into inner image representation inside the neural network and by reweighting image features accordingly in the loss function. The second method allows diverse stylizations by passing the stylized image through orthogonal transformations, which impact the way the target style is transferred. These blocks are trained to replicate patterns from additional pattern images, which serve as additional input and provide an interpretable way to control stylization variability for the end user. Qualitative and quantitative comparisons demonstrate that both methods are capable to generate different stylizations with higher variability achieved by the second method. The code of both methods is available on github.},
  archive      = {J_MVA},
  author       = {Ustyuzhanin, Alexander and Kitov, Victor and Kitov, Vladimir},
  doi          = {10.1007/s00138-025-01660-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Diversified image style transfer—approaches, new methods and directed variability control},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional cascaded multimodal attention for multiple
choice visual question answering. <em>MVA</em>, <em>36</em>(2), 1–16.
(<a href="https://doi.org/10.1007/s00138-025-01661-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a rapidly advancing field that aims to develop systems capable of answering questions based on image content. Performance of a VQA model largely depends on the effective integration of multimodal data. A sparsity-based Bidirectional Cascaded Multimodal Attention network has been proposed in this paper. This model leverages bidirectional attention between image and text modalities, enabling a deeper contextual understanding of one modality through the other. To encourage the focus of attention mechanism on the most relevant regions in the input, sparsity has been introduced in these interactions. In multiple choice VQA, answer options contain important context, and incorporating them with multimodal features using attention results in a comprehensive feature representation. The performance of the proposed model is assessed using the multiple-choice Visual7W dataset. To test the generalizability of the model, a modified VQAv2 dataset is prepared and evaluated. Through extensive experiments, the model demonstrates competitive performance, effectively handling diverse question types such as “what”, “where”, “who”, “why”, and “how”. A detailed analysis of attention maps for different question types highlights how the model focuses on various input regions. Visualizations of image, text, and cross-modal attention maps reveal the key areas that contributed to the model’s decision-making process.},
  archive      = {J_MVA},
  author       = {Upadhyay, Sushmita and Tripathy, Sanjaya Shankar},
  doi          = {10.1007/s00138-025-01661-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Bidirectional cascaded multimodal attention for multiple choice visual question answering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CorFormer: A hybrid transformer-CNN architecture for
corrosion segmentation on metallic surfaces. <em>MVA</em>,
<em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01663-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of periodic corrosion inspection in steel structures cannot be overstated. However, current manual inspection approaches are fraught with challenges: they are time-consuming, subjective, and pose risks. To address these limitations, extensive research has been conducted over the past decade gauging the feasibility of Convolutional Neural Networks (CNNs) for automation of corrosion inspection. Meanwhile, Transformer networks have recently emerged as powerful tools in computer vision due to their ability to model intricate global relationships. In this paper, a novel hybrid architecture, dubbed CorFormer, is proposed for effective and efficient automation of corrosion inspection. The CorFormer network fuses Transformer and CNN layers at different stages of the encoder, which captures global context through Transformer layers while leveraging the inherent inductive bias of CNNs. To bridge the semantic gap between features generated by Transformer and CNN layers, a Semantic Gap Merger (SGM) module is introduced after each feature merge operation. The encoder is complemented by a hierarchical decoder, able to decrypt complex features at large and small scales. CorFormer is compared against state-of-the-art CNN and Transformer architectures for corrosion segmentation, and is found to outperform the best alternative by 2.7% in terms of Intersection over Union (IoU) across 10 validation data splits. Furthermore, it enables real-time inspection at an impressive rate of 28 frames per second. Rigorous statistical tests provide support for the findings presented in this study, and an extensive ablation study validates all design choices.},
  archive      = {J_MVA},
  author       = {Subedi, Abhishek and Qian, Cheng and Sadeghian, Reza and Jahanshahi, Mohammad R.},
  doi          = {10.1007/s00138-025-01663-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CorFormer: A hybrid transformer-CNN architecture for corrosion segmentation on metallic surfaces},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretability of fingerprint presentation attack
detection systems: A look at the “representativeness” of samples against
never-seen-before attacks. <em>MVA</em>, <em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01666-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, fingerprint Presentation Attack Detection systems (PADs) are primarily based on deep learning architectures subjected to massive training. However, their performance decreases to never-seen-before attacks. With the goal of contributing to explaining this issue, we hypothesized that this limited ability to generalize is due to the lack of &quot;representativeness&quot; of the samples available for the PAD training. &quot;Representativeness&quot; is treated here from a geometrical perspective: the spread of samples into the feature space, especially near the decision boundaries. In particular, we explored the possibility of adopting three-dimensionality reduction methods to make the problem affordable through visual inspection. These methods enable visual inspection and interpretation by projecting data into two-dimensional spaces, facilitating the identification of weak areas in the decision regions estimated after the training phase. Our analysis delineates the benefits and drawbacks of each dimensionality reduction method and leads us to make substantial recommendations in the crucial phase of the training design.},
  archive      = {J_MVA},
  author       = {Carta, Simone and Casula, Roberto and Orrù, Giulia and Micheletto, Marco and Marcialis, Gian Luca},
  doi          = {10.1007/s00138-025-01666-z},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Interpretability of fingerprint presentation attack detection systems: A look at the “representativeness” of samples against never-seen-before attacks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Depthanything and SAM for UIE: Exploring large model
information contributes to underwater image restoration. <em>MVA</em>,
<em>36</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s00138-025-01662-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) remains a fundamental yet challenging problem in computer vision due to the complex physics of light propagation in aquatic environments. Traditional physics-based or learning-driven approaches often need more prior knowledge and representational capacity to generalize across diverse underwater conditions. This paper presents a novel theoretical framework for leveraging large-scale pre-trained models in UIE, explicitly addressing the fundamental limitations in existing methods through principled integration of depth and semantic priors. Our key contribution is twofold: First, we establish a rigorous information-theoretic foundation that quantifies how auxiliary features from foundation models enhance the representational capacity of UIE systems, providing theoretical guarantees through PAC-Bayesian bounds on generalization performance. Second, we propose a Feature Enhancement Strategy that optimally combines depth information from DepthAnything and semantic priors from the Segment Anything Model, guided by underwater optical physics. We introduce CAB-USRI, a physics-based algorithm for both baseline and theoretical validation. Our extensive experimentation on multiple benchmark datasets demonstrates that our approach consistently outperforms state-of-the-art methods by significant margins while maintaining theoretical interpretability. Our ablation studies reveal the crucial role of depth priors in underwater scenarios, establishing a clear connection between theoretical bounds and empirical performance. This work bridges the gap between foundation models and domain-specific tasks, providing theoretical insights and practical solutions for complex image restoration problems in challenging environments.},
  archive      = {J_MVA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s00138-025-01662-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Depthanything and SAM for UIE: Exploring large model information contributes to underwater image restoration},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-based power line cables and pylons detection for low
flying aircraft. <em>MVA</em>, <em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01664-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power lines are dangerous for low-flying aircraft, especially in low-visibility conditions. Thus, a vision-based system able to analyze the aircraft’s surroundings and to provide the pilots with a “second pair of eyes” can contribute to enhancing their safety. To this end, we develop a deep learning approach to jointly detect power line cables and pylons from images captured at distances of several hundred meters by aircraft-mounted cameras. In doing so, we combine a modern convolutional architecture with transfer learning and a loss function adapted to curvilinear structure delineation. We use a single network for both detection tasks and demonstrate its performance on two benchmarking datasets. We have also integrated it within an onboard system and run it inflight. We show with our experiments that it outperforms the prior distant cable detection method by Stambler et al. (in: International Conference on Robotics and Automation, 2019) on both datasets, while also successfully detecting pylons, given their annotations are available for the data.},
  archive      = {J_MVA},
  author       = {Gwizdała, Jakub and Oner, Doruk and Roy, Soumava Kumar and Shah, Mian Akbar and Eberhard, Ad and Egorov, Ivan and Krüsi, Philipp and Yakushev, Grigory and Fua, Pascal},
  doi          = {10.1007/s00138-025-01664-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Vision-based power line cables and pylons detection for low flying aircraft},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D face parsing based on 2D CPFNet: Conformal parameterized
face parsing network. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-025-01667-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face parsing is a fundamental component of many advanced face technologies, which assigns labels to each pixel on the face data. Although three-dimensional (3D) face parsing has the potential to outperform its two-dimensional (2D) counterpart, it remains challenging due to the high cost of processing 3D mesh data. Recent works have introduced various methods for 3D surface segmentation, but their performance is still limited and they consume large amounts of memory and computation. In this paper, we propose a “3D–2D–3D” strategy for 3D face parsing. First, we transform 3D face data into a topological disk-like 2D face image containing spatial and textural information via conformal parameterization. Subsequently, we use a specific 2D deep learning network called CPFNet to achieve 2D face image semantic segmentation with multiscale technology and feature aggregation. Finally, the 2D semantic result is inversely remapped to the 3D face data to achieve 3D face parsing. Experimental results show that both CPFNet and our “3D–2D–3D” strategy accomplish high-quality 3D face parsing and outperform some 2D networks and 3D methods in both qualitative and quantitative comparisons.},
  archive      = {J_MVA},
  author       = {Yang, M. and Sun, W. and Wang, Y. and Zhou, G. and Tong, J. and Zhou, P.},
  doi          = {10.1007/s00138-025-01667-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3D face parsing based on 2D CPFNet: Conformal parameterized face parsing network},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic volume measurement using nonlinear count-lines.
<em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01668-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic volume measurements are increasingly recognized as important for improving society’s infrastructure, by functions such as managing congestion and enhancing logistics. Dedicated traffic data capture devices that use sensors embedded in the road enable accurate measurements but have the problems of high cost and limited installation locations, which make it difficult to expand the coverage of traffic volume measurements. To address this issue, the approach that combines already deployed Closed-Circuit Television (CCTV) cameras with image recognition technology has attracted attention and offers practical performance in ordinary situations. One remaining problem is that accuracy is degraded by the presence of headlight flare at nighttime and occlusion by large vehicles on busy roads. In this paper, we propose a method for measuring traffic volume that automatically sets count-lines using the Kernel Support Vector Machine (Kernel SVM) at optimal positions less affected by these issues. In addition, to make the proposal robust to illumination changes and occlusion we introduce nonlinear count-lines. Extensive experiments on Japanese road video footage shows that our method improves accuracy by $$5.9\%$$ at night and $$2.1\%$$ in situations prone to occlusion compared to the most basic fixed count-line method. Additionally, experiments on a public dataset, UA-DETRAC, demonstrate the proposal’s effectiveness in countries other than Japan.},
  archive      = {J_MVA},
  author       = {Iwao, Yuwa and Yamamoto, Yota and Yaginuma, Hideki and Taniguchi, Yukinobu},
  doi          = {10.1007/s00138-025-01668-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Traffic volume measurement using nonlinear count-lines},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting few-shot learning via selective patch embedding by
comprehensive sample analysis. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-025-01669-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of limited data samples, few-shot learning continues to pose a significant challenge. A prevalent strategy in recent times has been to pre-train models on extensive datasets and subsequently transfer them to downstream few-shot tasks, which has demonstrated efficacy in enhancing performance. However, a persistent challenge lies in the inadequacy of pre-trained models to capture the essential features of the new downstream dataset. This issue is particularly acute in images containing multiple entities, where crucial features are often overlooked yet play a pivotal role in image classification. To address this challenge, we propose an innovative local information enhancement strategy that harnesses information from all samples to capture important local features in images and integrates them with global features. The objective of our strategy is to enhance class differentiation by ensuring distinct class prototypes in the embedding space through the incorporation of local information. By integrating local information, query samples exhibit closer alignment with the prototype of their respective classes, ultimately resulting in improved classification accuracy. To further bolster the performance of few-shot classification, we have refined the pre-trained model approach and augmented the dataset. Comprehensive ablation experiments demonstrate the specific impact of our approach on enhancing the accuracy of few-shot classification.},
  archive      = {J_MVA},
  author       = {Yang, Juan and Zhang, Yuliang and Wang, Ronggui and Xue, Lixia},
  doi          = {10.1007/s00138-025-01669-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Boosting few-shot learning via selective patch embedding by comprehensive sample analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced normal estimation of point clouds via fine-grained
geometric information learning. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01671-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud normal estimation is a fundamental task in 3D computer graphics, essential for downstream applications such as surface reconstruction and semantic segmentation. While recent advances in deep learning have significantly improved normal estimation accuracy, existing methods often struggle with capturing fine-grained geometric details. In this study, we propose a novel encoder that integrates a local gradient attention module and positional encoding to better capture subtle geometric variations. By introducing the gradient attention module, we effectively capture fine-grained information along the z-axis, while positional encoding using sine and cosine functions further amplifies these variations. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art methods, achieving up to a 2.53% improvement in accuracy on PCPNet dataset. Our work not only advances normal estimation but also demonstrates its potential for surface reconstruction tasks. The code is available at https://github.com/ABc90/gam-net-normal-main .},
  archive      = {J_MVA},
  author       = {Jin, Wei and Zhou, Jun and Wang, Mingjie and Li, Nannan and Wang, Weixiao and Liu, Xiuping},
  doi          = {10.1007/s00138-025-01671-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhanced normal estimation of point clouds via fine-grained geometric information learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SNFR: Salient neighbor decoding and text feature refining
for scene text recognition. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01672-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition methods are broadly categorized into serial and parallel. Serial methods achieve superior accuracy but are slower in speed. Parallel methods offer faster speed but may sacrifice accuracy. Current methods struggle to strike a balance between accuracy and inference speed, particularly facing challenges in both accuracy and speed. Therefore, we propose a new scene text recognizer called SNFR. It includes a simple yet efficient decoder, Salient Neighbor Decoder (SND), which achieves high accuracy recognition with lower computational cost for attention map calculation. SND generates a neighbor matrix by selecting salient positions, which guides the generation of all the character attention maps. We also propose a Text Feature Refining Module (TFRM) to capture the contextual relationship of text sequences, enhancing the overall feature representation of scene text. The experimental results demonstrate that our method achieves competitive performance on standard datasets and also shows superior performance on long text recognition.},
  archive      = {J_MVA},
  author       = {Lu, Tongwei and Fan, Huageng and Chen, Yuqian and Shao, Pengyan},
  doi          = {10.1007/s00138-025-01672-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SNFR: Salient neighbor decoding and text feature refining for scene text recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="nca---89">NCA - 89</h2>
<ul>
<li><details>
<summary>
(2025). Real-time arabic sign language recognition system using
sensory glove and machine learning. <em>NCA</em>, <em>37</em>(9),
6977–6993. (<a
href="https://doi.org/10.1007/s00521-025-11010-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to present a novel Arabic sign language recognition (SLR) strategy using sensory glove and machine learning. The article focuses on hand gesture recognition through the development of a glove-computer system designed for real-time hand posture detection and gesture-to-text translation. Gesture recognition plays a crucial role in enhancing interactions between humans and machines, making technology more intuitive and efficient. This technology has potential applications in various fields such as smart homes, gaming, automotive systems, and virtual reality. The primary goal of this article is then to create a supportive communication environment for individuals with speaking difficulties. The article began with the development of a sensory glove equipped with sensors to detect hand orientation and finger flexing, with data processed and transmitted wirelessly to a computer for machine learning prediction. A dynamic dataset, which included signs for letters and movement-based signs for words, was created and used to build two machine learning models: Support Vector Machine (SVM) model with feature extraction (SVM-FE model) and Long Short-Term Memory (LSTM) model. The proposed deep learning LSTM model demonstrated superior performance with accuracy of 99.6%. Based on these findings, a real-time recognition application was developed using the LSTM model, effectively showcasing the system&#39;s practical applicability in real-world scenarios.},
  archive      = {J_NCA},
  author       = {Halabi, Mohamad and Harkouss, Youssef},
  doi          = {10.1007/s00521-025-11010-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6977-6993},
  shortjournal = {Neural Comput. Appl.},
  title        = {Real-time arabic sign language recognition system using sensory glove and machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved transfer learning model for detection of
insulator defects in power transmission lines. <em>NCA</em>,
<em>37</em>(9), 6951–6976. (<a
href="https://doi.org/10.1007/s00521-025-11011-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulators are critical components of transmission lines but are prone to failures that can jeopardize the safe operation of electrical power systems. Accurate detection of insulator defects is essential for timely maintenance. With advancements in object detection algorithm and artificial intelligence, insulator defect detection has garnered significant attention. However, detection accuracy remains an issue. To address this, we propose an improved transfer learning model. Our approach incorporates the Mish activation function and a global context network module to enhance the model&#39;s performance. The improved YOLOv9 model is trained and tested using two public datasets: the insulator defect image dataset and the China power line insulator dataset. Experimental results demonstrate that our model achieves optimal detection precision and recall rates of 99.84 and 99.92%, respectively—improvements of 1.06 and 1.09% over the actual YOLOv9. Additionally, our model outperforms other algorithms, such as RTDETR and SSD, particularly in adapting to complex backgrounds and detecting small targets.},
  archive      = {J_NCA},
  author       = {Pradeep, V. and Baskaran, K. and Evangeline, S. Ida},
  doi          = {10.1007/s00521-025-11011-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6951-6976},
  shortjournal = {Neural Comput. Appl.},
  title        = {An improved transfer learning model for detection of insulator defects in power transmission lines},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDC-ViT: Source camera identification using pixel difference
convolution and vision transformer. <em>NCA</em>, <em>37</em>(9),
6933–6949. (<a
href="https://doi.org/10.1007/s00521-025-11004-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.},
  archive      = {J_NCA},
  author       = {Elharrouss, Omar and Akbari, Younes and Almadeed, Noor and Al-Maadeed, Somaya and Khelifi, Fouad and Bouridane, Ahmed},
  doi          = {10.1007/s00521-025-11004-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6933-6949},
  shortjournal = {Neural Comput. Appl.},
  title        = {PDC-ViT: Source camera identification using pixel difference convolution and vision transformer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective deep learning-based automatic prediction and
classification of alzheimer’s disease using EGELU-SZN technique.
<em>NCA</em>, <em>37</em>(9), 6915–6932. (<a
href="https://doi.org/10.1007/s00521-025-10994-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Alzheimer’s disease (AD) has become a serious hazard to human health. Therefore, an optimal strategy for formulating the treatment plan is the AD’s early diagnosis. In spite of this, no effective treatment or accurate diagnosis exists currently. Also, the pre-selection of brain regions is a complicated task. Therefore, an efficient AD classification is needed. Hence, by utilizing the Exponential Gaussian Error Linear Unit–Squeeze Net (EGELU-SZN) technique, an early prediction as well as classification of AD is proposed. Primarily, from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset, the input brain Magnetic Resonance Imaging (MRI) images are gathered. For the brain MRI images, a skull stripping process is executed. Here, by utilizing the Adaptive Median Otsu’s Thresholding (AMOT) technique, the skull along with cerebral tissues like fat and skin around the brain are removed. After that, to eliminate the noise, pre-processing is computed. After pre-processing, to segment the brain region accurately, segmentation is evaluated. Therefore, an effectual segmentation algorithm termed Rectilinear Mayfly Optimization-centric Automatic Seeded Region Growing algorithm (RMF-ASRG) has been utilized. Features are extracted as of the segmented brain region. Later, by utilizing the Reflective Correlation Principal Component Analysis (RCPCA) algorithm, the reduction of features is performed. Then, the predicted outcomes are classified as AD, Cognitive Normal (CN), as well as Mild Cognitive Impairment (MCI) by employing the EGELU-SZN Classifier. The proposed EGELU-SZN attained the accuracy, precision, recall, specificity, and sensitivity values of 95.9882%, 94.1661%, 93.2327%, 92.8744%, and 96.2327%, respectively, in the classification process. Experimental outcomes signified that superior performance was attained by the proposed methodology when analyzed with benchmark methodologies.},
  archive      = {J_NCA},
  author       = {Sathyabhama, B. and Kannan, M.},
  doi          = {10.1007/s00521-025-10994-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6915-6932},
  shortjournal = {Neural Comput. Appl.},
  title        = {An effective deep learning-based automatic prediction and classification of alzheimer&#39;s disease using EGELU-SZN technique},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing text understanding of decoder-based model by
leveraging parameter-efficient fine-tuning method. <em>NCA</em>,
<em>37</em>(9), 6899–6913. (<a
href="https://doi.org/10.1007/s00521-025-10975-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine reading comprehension (MRC) is a fundamental natural language understanding task in natural language processing, which aims to comprehend the text of a given passage and answer questions based on it. Understanding implicit information, deducing the logical structure of information, and connecting context from different pieces of information make the MRC task difficult. Most current state-of-the-art approaches for MRC are using encoder-based models. However, no earlier research proposed a decoder-only model for MRC question-answering datasets, although language models based on this category achieved unprecedented performance in different generative tasks. In this paper, we propose a parameter-efficient fine-tuning framework that effectively increases MRC capabilities on decoder-only large language models. This framework designs the process for MRC and introduces the low-rank adaptation (LoRA) method to effectively fine-tune the large model with many parameters, even with lower hardware resource requirements than the previous methods. In addition, we also integrate a quantized model inference strategy for the fine-tuned model to improve practicability further. We conducted experiments on four types of MRC datasets. After extensive experiments, our results show that our model achieved a significant performance boost over baselines and outperformed other strong models for MRC.},
  archive      = {J_NCA},
  author       = {Feroze, Wasif and Cheng, Shaohuan and Jimale, Elias Lemuye and Jakhro, Abdul Naveed and Qu, Hong},
  doi          = {10.1007/s00521-025-10975-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6899-6913},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing text understanding of decoder-based model by leveraging parameter-efficient fine-tuning method},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced neuro-adaptive PID sliding mode control for
robot manipulators: Promoting sustainable automation. <em>NCA</em>,
<em>37</em>(9), 6877–6898. (<a
href="https://doi.org/10.1007/s00521-025-10980-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control of robot manipulators presents significant challenges, primarily due to their complex, nonlinear dynamics. Another major difficulty arises from environmental and operational disturbances. Numerous control frameworks and approaches have been proposed in the literature to address these issues. However, many of these approaches are discontinuous or non-adaptive, which negatively affects their performance and makes them unsuitable for real-time applications. This paper proposes a novel integrated control scheme that combines adaptive control with an improved continuous second-order sliding mode control (CSOS), utilizing generalized artificial neural networks (GANN) for enhanced performance in robot manipulators. The proposed control method consists of two key components: An adaptive proportional-integral-derivative (PID) controller and an adaptive CSOS-based control module (CSOSSD-APID), designed to deliver superior transient and steady-state performance. In this approach, the adaptive CSOS benefits from GANN’s strong noise handling capabilities and its ability to estimate uncertainties effectively. This integration significantly enhances the robustness of robot manipulator control across various tracking tasks, using a single pre-trained GANN model with fine-tuned weights tailored to each task. Numerical simulations demonstrate the effectiveness and versatility of the proposed control scheme, particularly in managing highly time-varying trajectories while contributing to more sustainable and efficient automation practices.},
  archive      = {J_NCA},
  author       = {Elmogy, Ahmed and Alhemaly, Nagah and El-Ghaish, Hany and Elawady, Wael},
  doi          = {10.1007/s00521-025-10980-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6877-6898},
  shortjournal = {Neural Comput. Appl.},
  title        = {An enhanced neuro-adaptive PID sliding mode control for robot manipulators: Promoting sustainable automation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An active learning framework for adversarial training of
deep neural networks. <em>NCA</em>, <em>37</em>(9), 6849–6876. (<a
href="https://doi.org/10.1007/s00521-024-10851-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel approach to bolster the robustness of Deep Neural Network (DNN) models against adversarial attacks named “Targeted Adversarial Resilience Learning (TARL)”. The initial evaluation of a baseline DNN model reveals a significant accuracy decline when subjected to adversarial examples generated through techniques like FGSM, PGD, Carlini Wagner, and DeepFool attacks. To address this vulnerability, the article proposes an active learning framework, wherein the model iteratively identifies and learns from the most uncertain and misclassified instances. The key components of this approach include uncertainty estimation score in predicting the class of the input sample, selecting challenging samples based on this uncertainty score, labeling these challenging examples and augmenting them into the training set, and thereafter retraining the model with the expanded training set. The iterative active learning process, governed by parameters such as the number of iterations and batch size, demonstrates the potential to systematically enhance the resilience of DNN against adversarial threats. The proposed methodology has been investigated on several popular datasets such as the SARS-CoV-2 CT scan, MNIST, CIFAR-10, and Caltech-101, and demonstrated to be effective. Experiments illustrate that the learning framework improves the adversarial accuracies from 17.4% to 98.71% for the SARS-CoV-2 dataset, from 8.4% to 99.89% for the MNIST dataset, 1.6% to 78.84% for the CIFAR-10, and 12% to 92.92% for Caltech-101. Further, comparative analysis with several state-of-the-art methods suggests that the proposed framework offers superior defense against various attack methods and offers promising defensive mechanisms to deep neural networks.},
  archive      = {J_NCA},
  author       = {Ghosh, Susmita and Chatterjee, Abhiroop and Fiondella, Lance},
  doi          = {10.1007/s00521-024-10851-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6849-6876},
  shortjournal = {Neural Comput. Appl.},
  title        = {An active learning framework for adversarial training of deep neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced segmentation of optic disc and cup using
attention-based u-net with dense dilated series convolutions.
<em>NCA</em>, <em>37</em>(9), 6831–6847. (<a
href="https://doi.org/10.1007/s00521-025-10989-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delineating the boundaries of the optic disc and cup regions is a critical pre-requisite for glaucoma screening because it allows for precise measurement of key parameters, such as cup-to-disc ratio, which is a critical indicator of optic nerve head damage, a hallmark of glaucoma progression. Accurate segmentation enables early detection and monitoring of the disease, aiding in timely intervention to prevent vision loss. The main contribution of this research work is to develop an automated process to isolate and demarcate the optic disc and cup from retinal fundus images. To prevent the blood vessels from interfering with the segmentation process, a novel method is used for vessel mask generation and vessel inpainting. Most of the research works have used based encoder–decoder models like U-Net architecture or handcrafted feature extraction techniques such as hough transform, fuzzy clustering, etc. The proposed model has made significant modifications to the U-Net model. (1) Dual attention mechanism at every layer of decoder and (2) dense dilated series convolutions as skip connections to generate higher level feature map. The proposed model achieved benchmark accuracies - Dice score of 95.95% and IoU score of 92.22% for optic disc segmentation averaged over fivefold. For the task of outlining the optic cup region, it attained a Dice score of 88.7% and IoU of 79.72%.},
  archive      = {J_NCA},
  author       = {Kumar, G. Bharadwaja and Kumar, Soham},
  doi          = {10.1007/s00521-025-10989-x},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6831-6847},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced segmentation of optic disc and cup using attention-based U-net with dense dilated series convolutions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential analysis of alternative splicing events in gene
regions using residual neural networks. <em>NCA</em>, <em>37</em>(9),
6819–6829. (<a
href="https://doi.org/10.1007/s00521-025-10992-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several computational methods for the differential analysis of alternative splicing (AS) events among RNA-Seq samples typically rely on estimating isoform-level gene expression. However, these approaches are often error-prone due to the interplay of individual AS events, which results in different isoforms with locally similar sequences. Moreover, methods based on isoform-level quantification usually need annotated transcripts. In this work, we leverage the ability of deep learning networks to learn features from images and propose deepSpecas, a novel method for event-based AS differential analysis between two RNA-Seq samples. Our method does not rely on isoform abundance estimation, neither on a specific annotation. deepSpecas employs an image embedding scheme to represent the alignments of the two samples on the same region and utilizes a residual neural network to predict the AS events possibly expressed within that region. To our knowledge, deepSpecas is the first deep learning approach for performing an event-based AS analysis of RNA-Seq samples. To validate deepSpecas, we also address the lack of high quality AS benchmark datasets. For this purpose, we manually curated a set of regions exhibiting AS events. These regions were used for training our model and for assessing the predictions of our method. Our results highlight that deepSpecas achieves higher precision at the expense of a small reduction in sensitivity. The tool and the manually curated regions are available at https://github.com/sciccolella/deepSpecas .},
  archive      = {J_NCA},
  author       = {Ciccolella, Simone and Denti, Luca and Avila Cartes, Jorge and Della Vedova, Gianluca and Pirola, Yuri and Rizzi, Raffaella and Bonizzoni, Paola},
  doi          = {10.1007/s00521-025-10992-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6819-6829},
  shortjournal = {Neural Comput. Appl.},
  title        = {Differential analysis of alternative splicing events in gene regions using residual neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CTMBIDS: Convolutional tsetlin machine-based intrusion
detection system for DDoS attacks in an SDN environment. <em>NCA</em>,
<em>37</em>(9), 6795–6818. (<a
href="https://doi.org/10.1007/s00521-025-10976-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Defined Networks (SDN) face many security challenges today. A great deal of research has been done within the field of Intrusion Detection Systems (IDS) in these networks. Yet, numerous approaches still rely on deep learning algorithms, but these algorithms suffer from complexity in implementation, the need for high processing power, and high memory consumption. In addition to security issues, firstly, the number of datasets that are based on SDN protocols are very small. Secondly, the ones that are available encompass a variety of attacks in the network and do not focus on a single attack. For this reason, to introduce an SDN-based IDS with a focus on Distributed Denial of Service (DDoS) attacks, it is necessary to generate a DDoS-oriented dataset whose features can train a high-quality IDS. In this work, in order to address two important challenges in SDNs, in the first step, we generate three DDoS attack datasets based on three common and different network topologies. Then, in the second step, using the Convolutional Tsetlin Machine (CTM) algorithm, we introduce a lightweight IDS for DDoS attack dubbed &quot;CTMBIDS,&quot; with which we implement an anomaly-based IDS. The lightweight nature of the CTMBIDS stems from its low memory consumption and also its interpretability compared to the existing complex deep learning models. The low usage of system resources for the CTMBIDS makes it an ideal choice for an optimal software that consumes the SDN controller’s least amount of memory. Also, in order to ascertain the quality of the generated datasets, we compare the empirical results of our work with the DDoS attacks of the KDDCup99 benchmark dataset as well. Since the main focus of this work is on a lightweight IDS, the results of this work show that the CTMBIDS performs much more efficiently than traditional and deep learning based machine learning algorithms. Furthermore, the results also show that in most datasets, the proposed method has relatively equal or better accuracy and also consumes much less memory than the existing methods.},
  archive      = {J_NCA},
  author       = {Jafari Gohari, Rasoul and Aliahmadipour, Laya and Kuchaki Rafsanjani, Marjan},
  doi          = {10.1007/s00521-025-10976-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6795-6818},
  shortjournal = {Neural Comput. Appl.},
  title        = {CTMBIDS: Convolutional tsetlin machine-based intrusion detection system for DDoS attacks in an SDN environment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-supervised graph convolutional model for
recommendation with exponential moving average. <em>NCA</em>,
<em>37</em>(9), 6777–6793. (<a
href="https://doi.org/10.1007/s00521-024-10933-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation algorithms based on graph convolutional networks can integrate user and item node information along with their interaction topology, better capturing the intricate relationships between users and items, thereby enhancing the accuracy of recommender systems. However, existing methods often overlook the impact of noise in user behavior data on recommendation performance. Additionally, when there are too many convolutional layers in the graph, the node representations tend to smoothing, resulting in an inability to accurately distinguish user preferences. To address these issues, we propose a self-supervised graph convolutional model for recommendation with exponential moving average (SGCERec). Specifically, we first employ exponential moving average (EMA) techniques from the field of time-series analysis to denoise the raw user interaction data. Then, by applying layer filtering technique to update the propagation of information and the representation of nodes within the graph convolutional network, we effectively deepen the model hierarchy, enabling the model to gain a deeper understanding of the features and structures of the graph data, thereby improving the performance and effectiveness of the recommender systems. Finally, experimental results on three real datasets show that SGCERec outperforms state-of-the-art recommendation methods across various common evaluation metrics.},
  archive      = {J_NCA},
  author       = {Chen, Rui and Pang, Kangning and Wang, Zonglin and Liu, Qingfang and Tang, Cundong and Chang, Yanshuo and Huang, Min},
  doi          = {10.1007/s00521-024-10933-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6777-6793},
  shortjournal = {Neural Comput. Appl.},
  title        = {A self-supervised graph convolutional model for recommendation with exponential moving average},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic channel selection using multi-objective
prioritized jellyfish search (MPJS) algorithm for motor imagery
classification using modified DB-EEGNET. <em>NCA</em>, <em>37</em>(9),
6749–6776. (<a
href="https://doi.org/10.1007/s00521-025-10979-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain–computer interface (BCI) enables the device to communicate directly with the brain by decoding neural signals, particularly electroencephalograms (EEGs). EEG signals are used in a variety of applications, especially motor imagery detection, due to their noninvasive nature, real-time monitoring capabilities, and cost-effectiveness. Often, EEG data consists of multi-channel signals; the presence of multiple channels leads to computational complexity and the presence of redundant channel signals. To avoid these, the channel selection algorithm is currently being used, particularly optimization-based channel selection. However, the optimization-based channel selection method may have limitations, such as eliminating the most important channel due to poor initialization and failing to achieve optimal performance due to the lack of an efficient multi-objective fitness function. To address these limitations, we proposed a new channel selection mechanism called the multi-objective prioritized jellyfish search algorithm (MPJS), which has two significant improvements. First, domain-specific initialization is employed to select the most important channels at the initialization stage, which ensures that no important channels are omitted. Second, using a multi-objective fitness function instead of a single objective one to select the most relevant and informative channels ensures that the selected channels meet the number criteria and include candidates’ channels. Prior work primarily focused on two-class MI classification, with only a few studies examining four-class MI classification; however, these four-class classification methods fail to achieve optimal performance. To address these research gaps and achieve optimal performance in four-class MI detection, we proposed an improved double-branch EEGNET (DB-EEGNET). This proposed work performance was evaluated by using benchmark datasets, including BCI Competition IV-2008-2A, BCI Competition III-2008-A, and the High Gamma dataset (HGD). Our proposed MJPS channel selection and DB-EEGNET classification method outperformed the baseline algorithm on the BCI IV-IIA, IIIA, and HGD datasets, with an average accuracy of 83.9%, 84.46%, and 94.78%, respectively.},
  archive      = {J_NCA},
  author       = {Vadivelan, D. Senthil and Sethuramalingam, Prabhu},
  doi          = {10.1007/s00521-025-10979-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6749-6776},
  shortjournal = {Neural Comput. Appl.},
  title        = {Automatic channel selection using multi-objective prioritized jellyfish search (MPJS) algorithm for motor imagery classification using modified DB-EEGNET},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFuNet: An attention-based fusion network to classify texts
in a resource-constrained language. <em>NCA</em>, <em>37</em>(9),
6725–6748. (<a
href="https://doi.org/10.1007/s00521-024-10953-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of widespread Internet use and extensive social media interaction, the digital realm is accumulating vast amounts of unstructured text data. This unstructured data often contain undesirable information, necessitating time-consuming manual classification efforts. An intelligent text classification system capable of automatically categorizing digitized texts based on semantic meaning is crucial. However, this task is particularly challenging for low-resource languages like Bengali due to a shortage of annotated corpora, issues with out-of-vocabulary words, lack of domain-specific hyperparameter tuning, limited ability to extract generalized text features, and class imbalances within the corpus. AFuNet: an attention-based fusion network to classify texts in a resource-constrained language. AFuNet undergoes a comprehensive four-phase experimental process, including baseline model evaluation and hyperparameter tuning, late fusion and model selection, attention-based early fusion and model identification, and an ablation study with impact analysis. Fine-tuned based on five Bengali text classification corpora, AFuNet achieves impressive accuracies: 96.60 ± 0.2 (BTCC11), 85.37 ± 0.2 (OSBC), 97.35 ± 0.2 (BARD), 93.74 ± 0.2 (IndicNLP), and 96.51 ± 0.2 (ProthomAlo). In comparison with previous state-of-the-art models on these corpora, AFuNet demonstrates significant accuracy improvements ranging from 0.54% to 4.49%, showcasing its effectiveness in advancing text classification capabilities for the Bengali language.},
  archive      = {J_NCA},
  author       = {Hossain, Md. Rajib and Hoque, Mohammed Moshiul and Dewan, M. Ali Akber and Hoque, Enamul and Siddique, Nazmul},
  doi          = {10.1007/s00521-024-10953-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6725-6748},
  shortjournal = {Neural Comput. Appl.},
  title        = {AFuNet: An attention-based fusion network to classify texts in a resource-constrained language},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metamorphic testing of deep neural network-based autonomous
driving systems using behavioural domain adequacy. <em>NCA</em>,
<em>37</em>(9), 6677–6724. (<a
href="https://doi.org/10.1007/s00521-024-10794-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are crucial in autonomous driving systems (ADSs) for tasks like steering control, but model inaccuracies, biased training data, and incorrect runtime parameters can compromise their reliability. Metamorphic testing (MT) enhances reliability by generating follow-up tests from mutated DNN source inputs, identifying inconsistencies as defects. Various MT techniques for ADSs include generative/transfer models, neuron-based coverage maximization, and adaptive test selection. Despite these efforts, significant challenges remain, including the ambiguity of neuron coverage’s correlation with misbehaviour detection, a lack of focus on DNN critical pathways, inadequate use of search-based methods, and the absence of an integrated method that effectively selects sources and generates follow-ups. This paper addresses such challenges by introducing DeepDomain, a grey-box multi-objective test generation approach for DNN models. It involves adaptively selecting diverse source inputs and generating domain-oriented follow-up tests. Such follow-ups explore critical pathways, extracted by neuron contribution, with broader coverage compared to their source tests (inter-behavioural domain) and attaining high neural boundary coverage of the misbehaviour regions detected in previous follow-ups (intra-behavioural domain). An empirical evaluation of the proposed approach on three DNN models used in the Udacity self-driving car challenge, and 18 different MRs demonstrates that relying on behavioural domain adequacy is a more reliable indicator than coverage criteria for effectively guiding the testing of DNNs. Additionally, DeepDomain significantly outperforms selected baselines in misbehaviour detection by up to 94 times, fault-revealing capability by up to 79%, output diversity by 71%, corner-case detection by up to 187 times, identification of robustness subdomains of MRs by up to 33 percentage points, and naturalness by two times. The results confirm that state-of-the-art coverage metrics are inadequate in misbehaviour-inducing test generation. Furthermore, black-box diversity-based test generation is less effective than the grey-box approach.},
  archive      = {J_NCA},
  author       = {Kalaee, Akram and Parsa, Saeed},
  doi          = {10.1007/s00521-024-10794-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6677-6724},
  shortjournal = {Neural Comput. Appl.},
  title        = {Metamorphic testing of deep neural network-based autonomous driving systems using behavioural domain adequacy},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel augmentation strategy for credit scoring modeling.
<em>NCA</em>, <em>37</em>(9), 6663–6675. (<a
href="https://doi.org/10.1007/s00521-024-10452-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In last years, social lending platforms have been increasingly used as virtual environments where borrowers can directly interact with lenders without any intermediary. As a result, a reliable credit scoring strategy, i.e., assessing whether a client is able to fully repay a loan, became of utmost importance to reduce the risk of not repaying the lenders. In this context, machine learning tools are being increasingly adopted to design automatic credit scoring systems but the data imbalance problem still penalizes their predictive performance, i.e., the greatest majority of clients can afford the repayment and learning to classify ”bad” borrowers depends on few instances where the loan was not paid back. In this paper, we target the data imbalance problem and propose a novel data augmentation strategy to improve the predictive performance of credit scoring models. The proposed methodology performs data augmentation by injecting synthetic instances in the dataset generated along the decision boundary of the decision model. We assessed the effectiveness of the proposed augmentation strategy on a million-scale dataset from Lending Club, the largest Social Lending platform, and found that it improves the performance of several classification models, also in comparison to other state-of-the-art approaches.},
  archive      = {J_NCA},
  author       = {La Gatta, Valerio and Postiglione, Marco and Sperlì, Giancarlo},
  doi          = {10.1007/s00521-024-10452-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6663-6675},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel augmentation strategy for credit scoring modeling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViF-SD2E: A robust weakly-supervised framework for neural
decoding. <em>NCA</em>, <em>37</em>(9), 6645–6661. (<a
href="https://doi.org/10.1007/s00521-024-10958-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural decoding plays a vital role in the interaction between the brain and the outside world. Our task in this paper is to decode the movement track of a finger directly based on the neural data. Existing neural decoding solutions primarily perform some preprocessing operations on neural data before feeding them into existing models (such as LSTM) for decoding. However, these solutions either are prone to overfitting or cannot well exploit the spatial and temporal information. In our previous observations, there is a symmetrical phenomenon between the unsupervised decoded trajectory and the ground truth trajectory within the activity space. This precisely motivates us to propose (or derive) a robust weakly-supervised framework (or model structure), called ViF-SD2E, for neural decoding. In particular, it consists of a space-division (SD) module and an exploration–exploitation (2E) strategy, to effectively exploit both the spatial information of the outside world and the temporal information of neural activity, where the SD2E output is analogized with the weak 0/1 vision feedback (ViF) label for training. Extensive experiments demonstrate the effectiveness of our method, which can sometimes be comparable to supervised counterparts. Therefore, we redirect our attention to the information (hidden in data) ViF-SD2E conveys to us. In other words, we believe that the advantage of ViF-SD2E lies in the fact that its processing steps are objectively determined by the inherent attributes (i.e., symmetry) of the neural data, or rather, the model structure is fixed.},
  archive      = {J_NCA},
  author       = {Feng, Jingyi and Luo, Yong and Song, Shuang and Hu, Han},
  doi          = {10.1007/s00521-024-10958-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6645-6661},
  shortjournal = {Neural Comput. Appl.},
  title        = {ViF-SD2E: A robust weakly-supervised framework for neural decoding},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fixed/preassigned-time synchronization of complex networks
under aperiodically intermittent event-triggered control. <em>NCA</em>,
<em>37</em>(9), 6633–6643. (<a
href="https://doi.org/10.1007/s00521-024-10918-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the fixed-/preassigned-time synchronization of complex networks via aperiodically intermittent event-triggered control. A novel intermittent control lemma is developed to ensure fixed-time synchronization with the help of an event-triggered method. Some criteria are proposed to achieve the synchronization goal for complex networks within a fixed/preassigned time via two types of aperiodically intermittent event-triggered controllers. A numerical example is given to illustrate the validity of the new theoretical results.},
  archive      = {J_NCA},
  author       = {Dong, Ziyu and Hu, Yuanfa and Liu, Xiaoyang and Cao, Jinde},
  doi          = {10.1007/s00521-024-10918-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6633-6643},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fixed/preassigned-time synchronization of complex networks under aperiodically intermittent event-triggered control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust entropy regularized k-means clustering algorithm
for processing noise in datasets. <em>NCA</em>, <em>37</em>(9),
6617–6632. (<a
href="https://doi.org/10.1007/s00521-024-10899-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means is one of the clustering algorithms. Due to its simple implementation and powerful functionality, it is widely used in fields such as data mining, cluster analysis, data preprocessing, and unsupervised learning. However, the K-means algorithm suffers from the problem of being sensitive to outliers. If there are a certain number of outliers in a low-dimensional sample set, the resulting cluster centers will be greatly disturbed, affecting the clustering results. We can certainly detect outliers before clustering, but this phased approach has an impact on the accuracy of clustering results. To address this issue, we propose an improved robust Entropy Regularized K-Means clustering algorithm. Our method is based on the Entropy Regularized K-Means clustering algorithm and adds a weight value to the optimization function to ignore out-of-bounds data, and obtain a more accurate number of clusters in the dataset, thereby achieving synchronous clustering and detection. The advantages of this algorithm are strong anti-interference ability, the ability to ignore the influence of outliers on cluster centers, and synchronous clustering and detection. We tested our improved algorithm on artificial and real datasets, demonstrating that it can better determine cluster centers and find some outlier data.},
  archive      = {J_NCA},
  author       = {Jiang, Peilin and Cao, Junnan and Yu, Weizhong and Nie, Feiping},
  doi          = {10.1007/s00521-024-10899-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6617-6632},
  shortjournal = {Neural Comput. Appl.},
  title        = {A robust entropy regularized K-means clustering algorithm for processing noise in datasets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing intra-aural disease classification with
attention-based deep learning models. <em>NCA</em>, <em>37</em>(9),
6601–6616. (<a
href="https://doi.org/10.1007/s00521-025-10990-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ear diseases are defined as pathological conditions that indicate dysfunction or abnormal function of the ear organ, which is part of the auditory system of living organisms that regulates hearing and balance functions. These diseases usually manifest as conditions that affect the internal components of the ear structure and can manifest themselves with symptoms such as hearing loss, ear pain, balance problems, and fluid accumulation in the ear. The accuracy of the diagnosis depends on expert knowledge and subjective opinion. This method is prone to human error. This study presents a novel computer-aided diagnosis system for otoscope images of ear diseases, utilizing a vision transformer-based feature extractor combined with machine learning classifiers to provide accurate second opinions for ENT specialists. For this purpose, a new model based on state-of-the-art vision transformer feature extractor and machine learning models is proposed. In the experimental study, the dataset, comprising 880 eardrum images categorized into four classes (CSOM, earwax, myringosclerosis, and normal), was split into training (70%), validation (10%), and testing (20%) subsets. Each image was preprocessed to 420 × 380 pixels to fit the input dimensions of the models. The vision transformer architecture was utilized for feature extraction, followed by classification using various machine learning algorithms including kNN, SVM, and random forest. As a result, the model using vision transformer feature extractor and k-nearest neighbors (kNN) algorithm achieved 99.00% accuracy. In this study, a deep learning-based and computer-aided diagnosis system, in other words, a computational model, was developed instead of the current human error-prone disease diagnosis method used by ear nose throat (ENT) specialists. The main purpose of the deep learning-based decision support system is to support the diagnosis process where expert knowledge is difficult to access and to provide an alternative opinion to the expert diagnosis.},
  archive      = {J_NCA},
  author       = {Demircan, Furkancan and Ekinci, Murat and Cömert, Zafer},
  doi          = {10.1007/s00521-025-10990-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6601-6616},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing intra-aural disease classification with attention-based deep learning models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViT-SENet-tom: Machine learning-based novel hybrid
squeeze–excitation network and vision transformer framework for tomato
fruits classification. <em>NCA</em>, <em>37</em>(9), 6583–6600. (<a
href="https://doi.org/10.1007/s00521-025-10973-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tomatoes are essential fruits in numerous nations for their vast demand. It is very important to maintain the freshness of tomatoes. One of the primary challenges in the recent culinary landscape is accurately identifying healthy tomatoes while effectively eliminating damaged or rejected ones. Existing approaches employ various strategies for categorizing tomato fruit, but they often suffer from inaccuracies, slow detection, and suboptimal performance. Thus, motivated by this gap, in this paper, we propose a novel machine learning (ML) framework, ViT-SENet-Tom, which is a hybrid vision transformer (ViT) model with squeeze and excitation (SENet) block network for fast, accurate, and efficient tomato fruit classification. The framework works on three tomato classes, respectively, the ripe, unripe, and reject. In developing the proposed model, we utilized advanced and newly designed layers and functions. This integration created a more complex and sophisticated neural network, significantly enhancing efficiency and contributing to the model’s novelty. Our chosen dataset was small initially, but we implemented augmentation techniques to increase its size. This approach made our system more reliable, efficient, and effective. The hybrid ViT-SENet framework employs encoders and self-attention networks with squeeze and excitation channel functions to allow precise, robust, fast, and efficient tomato classification. In simulation, the framework achieves a training accuracy of 99.87% and validation accuracy of 93.87%, indicating the precise classification of tomatoes. Besides, this work tests accuracy using fivefold cross-validation. The highest accuracy seen at fold-5 is 99.90%. These testing results demonstrate the efficacy of the proposed framework in real-deployment scenarios. The implementation has the potential to provide enhanced and more sustainable food security and safety in future.},
  archive      = {J_NCA},
  author       = {Swapno, S M Masfequier Rahman and Nobel, S. M. Nuruzzaman and Islam, Md Babul and Bhattacharya, Pronaya and Mattar, Ebrahim A.},
  doi          = {10.1007/s00521-025-10973-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6583-6600},
  shortjournal = {Neural Comput. Appl.},
  title        = {ViT-SENet-tom: Machine learning-based novel hybrid squeeze–excitation network and vision transformer framework for tomato fruits classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-to-word dynamic interaction model for abstractive
sentence summarization. <em>NCA</em>, <em>37</em>(9), 6567–6581. (<a
href="https://doi.org/10.1007/s00521-024-10970-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstractive text summarization aims to capture important information from text and integrate contextual information to guide the summary generation. However, effective integration of important and relevant information remains a challenging problem. Existing graph-based methods only consider either word relations or structure information, but neglect the correlation between them. To simultaneously capture the word relations and structure information from sentences, we propose a novel Structure-to-Word dynamic interaction model for Abstractive Sentence Summarization (SWSum). Specifically, we first represent structure and word relation information of sentences by constructing semantic scenario graph and semantic word relation graph based on FrameNet. We subsequently stack multiple graph-based dynamic interaction layers that iteratively enhance their correlation to learn node representations. Finally, a graph fusion module is designed to obtain better overall graph representations, which provide an attention-based context vector for the decoder to generate summary. Experimental results demonstrate our model outperforms existing state-of-the-art methods on two popular benchmark datasets, i.e., Gigaword and DUC 2004.},
  archive      = {J_NCA},
  author       = {Guan, Yong and Guo, Shaoru and Li, Ru},
  doi          = {10.1007/s00521-024-10970-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6567-6581},
  shortjournal = {Neural Comput. Appl.},
  title        = {Structure-to-word dynamic interaction model for abstractive sentence summarization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). R-WhONet: Recalibrated wheel odometry neural network for
vehicular positioning using transfer learning. <em>NCA</em>,
<em>37</em>(9), 6547–6565. (<a
href="https://doi.org/10.1007/s00521-024-10046-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a transfer learning approach to recalibrate our previously developed Wheel Odometry Neural Network (WhONet) for vehicle positioning in environments where Global Navigation Satellite Systems (GNSS) are unavailable. The WhONet has been shown to possess the capability to learn the uncertainties in the wheel speed measurements needed for correction and accurate positioning of vehicles. These uncertainties may be manifested as tyre pressure changes from driving on muddy and uneven terrains or wheel slips. However, a common cause for concern for data-driven approaches, such as the WhONet model, is usually the inability to generalise the models to a new vehicle. In scenarios where machine learning models are trained in a specific domain but deployed in another domain, the model’s performance degrades. In real-life scenarios, several factors are influential to this degradation, from changes to the dynamics of the vehicle to new pattern distributions of the sensor’s noise, and bias will make the test sensor data vary from training data. Therefore, the challenge is to explore techniques that allow the trained machine learning models to spontaneously adjust to new vehicle domains. As such, we propose the Recalibrated-Wheel Odometry neural Network, based on transfer learning, that adapts the WhONet model from its source domain (a vehicle and environment on which the model is initially trained) to the target domain (a new vehicle on which the trained model is to be deployed). Through a performance evaluation on several GNSS outage scenarios—short-term complex driving scenarios such as on roundabouts, sharp cornering, hard-brake and wet roads (drifts), and on longer-term GNSS outage scenarios of 30s, 60s, 120s and 180s duration—we demonstrate that a model trained in the source domain does not generalise well to a new vehicle in the target domain. However, we show that our new proposed framework improves the generalisation of the WhONet model to new vehicles in the target domains by an average of 32% (i.e. 32% reduction in the vehicle position error estimation across the scenarios investigated).},
  archive      = {J_NCA},
  author       = {Onyekpe, Uche and Szkolnik, Alicja and Palade, Vasile and Kanarachos, Stratis and Fitzpatrick, Michael E.},
  doi          = {10.1007/s00521-024-10046-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6547-6565},
  shortjournal = {Neural Comput. Appl.},
  title        = {R-WhONet: Recalibrated wheel odometry neural network for vehicular positioning using transfer learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing financial time series forecasting through
topological data analysis. <em>NCA</em>, <em>37</em>(9), 6527–6545. (<a
href="https://doi.org/10.1007/s00521-024-10787-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological data analysis (TDA) is increasingly acknowledged within financial markets for its capacity to manage complexity and discern nuanced patterns and structures. It has been applied effectively to uncover intricate relationships and capture non-linear dependencies inherent in market data. This manuscript presents a groundbreaking study that delves into integrating features derived from TDA to improve the performance of forecasting models for univariate time series prediction. The research specifically examines whether incorporating features extracted from TDA-such as entropy, amplitude, and the number of points obtained from persistent diagrams can provide valuable supplementary information to the baseline forecasting model. Thus, the aim is to determine if these TDA-derived features can boost forecasting accuracy by offering additional insights that existing models might overlook. The N-BEATS model serves as the baseline forecasting model due to its robust generalization capabilities and flexibility in incorporating additional features into the model. The proposed methodology is compared against a univariate N-BEATS model without additional features and other strategies incorporating supplementary features such as temporal decomposition and time delay embeddings. The evaluation includes forecasting for six cryptocurrencies across four distinct time scenarios and four traditional financial instruments across two scenarios each, resulting in 32 datasets. The results obtained were promising, as the proposed method, $$\texttt {N-BEATS}_\mathrm {+TDA}$$ , achieved the best results in mean performance and mean ranking for the three metrics considered (MAPE, MAE, and RMSE). Significant differences were observed with the rest of the proposed methods using a significance level of $$\alpha = 0.10$$ , highlighting the effectiveness of integrating TDA features to enhance forecasting models.},
  archive      = {J_NCA},
  author       = {de Jesus, Luiz Carlos and Fernández-Navarro, Francisco and Carbonero-Ruz, Mariano},
  doi          = {10.1007/s00521-024-10787-x},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6527-6545},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing financial time series forecasting through topological data analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness-driven federated learning-based spam email
detection using clustering techniques. <em>NCA</em>, <em>37</em>(9),
6515–6526. (<a
href="https://doi.org/10.1007/s00521-024-10969-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the world of emails, spam messages present a significant challenge, leading to inconveniences and potential security risks. Addressing this issue, the task of spotting spam in emails is critical for ensuring secure and trustworthy communication. However, two prevalent approaches have their challenges. The centralized model, gathering data in one place, raises privacy issues. Conversely, the federated learning model, which focuses on privacy, can lead to a compromise in accuracy. This research paper presents a novel federated learning-based fair clustering technique for spam email detection. By addressing privacy concerns and aiming for accurate classification, the proposed approach Fair Clustering model combines the strengths of federated learning and data clustering. Through experimental evaluation, the Fair Clustering model is evaluated against both a centralized and federated learning model. Different metrics, such as accuracy, recall, precision, and F1-score, are used to evaluate and compare the performance of these models. The results demonstrate that the Fair Clustering model outperforms the federated learning model, showcasing the effectiveness of fair clustering in selecting representative clients and improving classification performance.},
  archive      = {J_NCA},
  author       = {Kaushal, Vishal and Sharma, Sangeeta},
  doi          = {10.1007/s00521-024-10969-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6515-6526},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fairness-driven federated learning-based spam email detection using clustering techniques},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stylometry-driven framework for urdu intrinsic plagiarism
detection: A comprehensive analysis using machine learning, deep
learning, and large language models. <em>NCA</em>, <em>37</em>(9),
6479–6513. (<a
href="https://doi.org/10.1007/s00521-024-10966-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting plagiarism in documents is a well-established task in natural language processing (NLP). Broadly, plagiarism detection is categorized into two types (1) intrinsic: to check the whole document or all the passages have been written by a single author; (2) extrinsic: where a suspicious document is compared with a given set of source documents to figure out sentences or phrases which appear in both documents. In the pursuit of advancing intrinsic plagiarism detection, this study addresses the critical challenge of intrinsic plagiarism detection in Urdu texts, a language with limited resources for comprehensive language models. Acknowledging the absence of sophisticated large language models (LLMs) tailored for Urdu language, this study explores the application of various machine learning, deep learning, and language models in a novel framework. A set of 43 stylometry features at six granularity levels was meticulously curated, capturing linguistic patterns indicative of plagiarism. The selected models include traditional machine learning approaches such as logistic regression, decision trees, SVM, KNN, Naive Bayes, gradient boosting and voting classifier, deep learning approaches: GRU, BiLSTM, CNN, LSTM, MLP, and large language models: BERT and GPT-2. This research systematically categorizes these features and evaluates their effectiveness, addressing the inherent challenges posed by the limited availability of Urdu-specific language models. Two distinct experiments were conducted to evaluate the impact of the proposed features on classification accuracy. In experiment one, the entire dataset was utilized for classification into intrinsic plagiarized and non-plagiarized documents. Experiment two categorized the dataset into three types based on topics: moral lessons, national celebrities, and national events. Both experiments are thoroughly evaluated through, a fivefold cross-validation analysis. The results show that the random forest classifier achieved an exceptional accuracy of 98.81% in experiment 1. On the other hand, in experiment 2, the extreme gradient boosting classifier attained an overall accuracy of 99.00% highlighting its superior capability in distinguishing nuanced stylistic features across different topics. Overall, machine learning models showcasing superior performance utilizing the proposed set of stylometry features over deep learning approaches and LLMs.},
  archive      = {J_NCA},
  author       = {Manzoor, Muhammad Faraz and Farooq, Muhammad Shoaib and Abid, Adnan},
  doi          = {10.1007/s00521-024-10966-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6479-6513},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stylometry-driven framework for urdu intrinsic plagiarism detection: A comprehensive analysis using machine learning, deep learning, and large language models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructing a smoothed leaky ReLU using a linear
combination of the smoothed ReLU and identity function. <em>NCA</em>,
<em>37</em>(9), 6465–6478. (<a
href="https://doi.org/10.1007/s00521-024-10935-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have made tremendous progress in solving many challenging problems. Good activation functions can improve the performance of CNNs. The existing activation functions exhibit inconsistent performance gains across different training settings, models, datasets and tasks. To solve this problem, we propose a general smoothed approximation for the maximum function $$\max (x_i, \alpha x_i)$$ using the linear combination of the smoothed rectified linear unit and the identity function. And we use exponential moving average to training the negative slope in this smoothed approximation. To validate the effectiveness of our approach, we also present a smoothed approximation case named leaky power function linear unit (LPFLU) to compare with the current state-of-the-art activation functions. Experimental results demonstrate that our LPFLU outperforms the existing state-of-the-art activation functions in improved robustness across different training settings, models, datasets and tasks.},
  archive      = {J_NCA},
  author       = {Zhu, Meng and Min, Weidong and Li, Jiahao and Liu, Mengxue and Deng, Ziyang and Zhang, Yao},
  doi          = {10.1007/s00521-024-10935-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6465-6478},
  shortjournal = {Neural Comput. Appl.},
  title        = {Constructing a smoothed leaky ReLU using a linear combination of the smoothed ReLU and identity function},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge AI-powered marine pollution classification with
customized CNN model. <em>NCA</em>, <em>37</em>(9), 6449–6463. (<a
href="https://doi.org/10.1007/s00521-024-10959-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing production of disposable plastic products contributes greatly to marine pollution and its impact on the marine ecosystem and organisms consuming ocean-derived food. To address this issue, this paper proposes a new customized convolutional neural network (CNN) model for categorizing the level of marine pollution in underwater ocean regions using image classification. The customized CNN model is developed and compared with five preexisting models, including DenseNet121, Inception-ResNetV2, InceptionV3, VGG-19, and VGG-16. The results show that the customized model achieves an accuracy of 99.5% and performs optimally according to various performance metrics. The model is implemented on an edge AI device, such as Raspberry Pi, to bring it to practical use.},
  archive      = {J_NCA},
  author       = {Palanisamy, Sanjai and Bonny, Talal and Nasir, Nida and Al Shabi, Mohammad and Al Shammaa, Ahmed},
  doi          = {10.1007/s00521-024-10959-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6449-6463},
  shortjournal = {Neural Comput. Appl.},
  title        = {Edge AI-powered marine pollution classification with customized CNN model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized medical recommendation system with machine
learning. <em>NCA</em>, <em>37</em>(9), 6431–6447. (<a
href="https://doi.org/10.1007/s00521-024-10916-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a machine learning-based healthcare recommendation system designed to provide personalized medical advice by accurately predicting diseases from patient symptoms. The system utilizes a comprehensive symptom–disease dataset, leveraging support vector classifier (SVC) and random forest (RF) models, achieving outstanding accuracies of 97.75% in disease prediction. These results surpass those of similar studies, such as one employing hybrid CNN and fuzzy logic techniques, which achieved 99% accuracy but relied on smaller datasets with limited diversity. The proposed system not only excels in diagnosis but also integrates tailored recommendations, including medication, dietary plans, and exercise regimens, to address the specific needs of patients. These personalized recommendations enhance practical utility, offering a patient-centered approach that promotes proactive health management. By focusing on diseases with high and moderate predictive performance, the system addresses both common and complex conditions effectively. The study demonstrates the transformative potential of machine learning in developing scalable and efficient healthcare systems, bridging the gap between accurate prediction and actionable treatment strategies. Future research will aim to incorporate larger and more diverse datasets, address underrepresented diseases, and refine feature engineering to enhance model generalizability and the system&#39;s overall effectiveness.},
  archive      = {J_NCA},
  author       = {Hassan, Basma M. and Elagamy, Shahd Mohamed},
  doi          = {10.1007/s00521-024-10916-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6431-6447},
  shortjournal = {Neural Comput. Appl.},
  title        = {Personalized medical recommendation system with machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collision avoidance and routing based on location access
(CARLA) of mobile robots. <em>NCA</em>, <em>37</em>(9), 6401–6430. (<a
href="https://doi.org/10.1007/s00521-024-10914-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces a new path-planning robotic system methodology called Collision Avoidance and Routing based on Location Access (CARLA) for use in critical environments such as hospitals and crises where quick action and saving human lives are vital. The main focus of our framework is on accuracy and fast responses, such as delivering tools or items in a specific area while avoiding collisions with other robots and obstacles. CARLA is designed to provide quick responses during emergencies, unlike most existing algorithms that are integrated into site control units or distributed among mobile robots on-site. By being loaded onto a remote server node rather than individual robots, CARLA helps to conserve the robots&#39; capabilities, hardware resources, and power consumption. Additionally, our system utilizes cloud computing and Fog servers technology to improve data transmission times between the cloud and smart devices, especially for applications with strict timing requirements like emergency response. The Fog platform is also leveraged to enhance on-site access to real-time interaction and location-based services by bringing processing power closer to the robots from far-off Cloud servers. CARLA has various applications, such as in factories and warehouses, where mobile robots need to be selected and directed by a central control system remotely. The proposed framework consists of three main modules: Robot Knowledge Module, Robot Selection Module, and Route Reservation Module, which will all be discussed in detail in this paper. The results of simulations using this framework show that the robots have improved flexibility and efficiency in terms of computing paths and successfully fulfiling requests without colliding, compared to traditional methods used in similar scenarios.},
  archive      = {J_NCA},
  author       = {ElSayyad, Shimaa Ezzat and Saleh, Ahmed I. and Ali, Hesham A. and Saraya, M. S. and Rabie, Asmaa H. and Abdelsalam, Mohamed M.},
  doi          = {10.1007/s00521-024-10914-8},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6401-6430},
  shortjournal = {Neural Comput. Appl.},
  title        = {Collision avoidance and routing based on location access (CARLA) of mobile robots},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exponential synchronization of bi-directional associative
memory neural networks with delay on arbitrary time domains.
<em>NCA</em>, <em>37</em>(8), 6383–6400. (<a
href="https://doi.org/10.1007/s00521-024-10820-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the exponential synchronization of bi-directional associative memory neural networks with delays on a family of different time domains. By utilizing the theory of time scales, we provide stabilization results that are applicable to continuous-time, discrete-time, and general nonuniform hybrid time domains. Our approach employs a unified matrix-measure theory, a recent alternative to traditional Lyapunov functions, to establish exponential synchronization and design effective feedback laws. Notably, our methodology does not require symmetry or diagonality in the control gain matrix, distinguishing it from prior works. Furthermore, we explore various special cases of the considered systems and provide a detailed discussion highlighting the advantages of our findings over existing results. The effectiveness of our proposed criteria is demonstrated through small-scale and medium-scale simulated numerical examples across different time domains. Additionally, we apply our results to an example from the literature, showcasing the broad applicability and improved performance of our method in comparison to previous approaches.},
  archive      = {J_NCA},
  author       = {Kumar, Vipin and Heiland, Jan and Benner, Peter},
  doi          = {10.1007/s00521-024-10820-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6383-6400},
  shortjournal = {Neural Comput. Appl.},
  title        = {Exponential synchronization of bi-directional associative memory neural networks with delay on arbitrary time domains},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composite neural learning-based adaptive actuator failure
compensation control for full-state constrained autonomous surface
vehicle. <em>NCA</em>, <em>37</em>(8), 6369–6381. (<a
href="https://doi.org/10.1007/s00521-024-10651-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies composite neural learning-based adaptive failure compensation control issues for the autonomous surface vehicle with full-state constraints. Initially, the control strategy solve the problems of computational complexity and state constraints and eliminate the negative effect of filter error on tracking performance by integrating with the command-filtered backstepping technique and barrier Lyapunov functions. Then, a composite neural learning framework is established, where the effect caused by approximation error on tracking accuracy can be efficiently reduced by constructing the serial-parallel estimation model to obtain the estimations of the system states. Furthermore, an adaptive resilient trajectory tracking controller is designed, which can ensure that all the signals of the closed-loop system are semi-globally uniformly ultimately bounded satisfying the preset constraints even if the expected actuator faults occur suddenly. Finally, the feasibility and superiority of the designed control strategy are clarified by simulation results.},
  archive      = {J_NCA},
  author       = {Song, Shuai and Jiang, Yu and Song, Xiaona and Stojanovic, Vladimir},
  doi          = {10.1007/s00521-024-10651-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6369-6381},
  shortjournal = {Neural Comput. Appl.},
  title        = {Composite neural learning-based adaptive actuator failure compensation control for full-state constrained autonomous surface vehicle},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlation-based pruning algorithm with weight compensation
for feedforward neural networks. <em>NCA</em>, <em>37</em>(8),
6351–6367. (<a
href="https://doi.org/10.1007/s00521-024-10932-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing neural network architectures through effective pruning techniques has become essential to balancing model complexity and accuracy. This study introduces a novel correlation-based approach to systematically reduce network size by identifying and removing redundant neurons based on their activation correlations. By selectively pruning neurons while compensating for their contributions, the method maintains model fidelity across diverse datasets. Results demonstrate substantial architecture reductions with minimal performance impact: For the MNIST dataset, the number of neurons in hidden layers was reduced from 128-128 to 118-93, while maintaining a high accuracy of 97.59%. Comparative analysis indicates that this pruning approach achieves competitive or superior results compared to state-of-the-art methods while reducing computational complexity and memory requirements by up to 25%. The findings highlight the potential of correlation-driven pruning strategies to optimize neural networks, making them more efficient and adaptable to resource-constrained environments.},
  archive      = {J_NCA},
  author       = {Ebid, Shaimaa E. K. and El-Tantawy, Samah and Shawky, Doaa and Abdel-Malek, Hany L.},
  doi          = {10.1007/s00521-024-10932-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6351-6367},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correlation-based pruning algorithm with weight compensation for feedforward neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polyp image segmentation based on improved planet
optimization algorithm using reptile search algorithm. <em>NCA</em>,
<em>37</em>(8), 6327–6349. (<a
href="https://doi.org/10.1007/s00521-024-10667-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To recognize the potential for colon polyps to develop into cancer over time, early diagnosis is crucial for preventative healthcare. Timely identification significantly improves the prognosis and treatment outcomes for colorectal cancer patients. Image segmentation is crucial in medical image analysis for accurate diagnosis and treatment planning. Therefore, in this study, we present an alternative multilevel thresholding polyp segmentation method (MPOA) to enhance the segmentation of polyp images. The proposed method is based on enhancing the planet optimization algorithm (POA) by integrating operators from the reptile search algorithm (RSA). The evaluation of the developed MPOA is tested with different polyp images and compared with other image segmentation approaches. The results highlight the superior capability of MPOA, as evidenced by various performance measures in effectively segmenting polyp images. Furthermore, metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and fitness values demonstrate that MPOA outperforms the basic version of POA and other methods. The evaluation outcomes underscore the significant impact of RSA in enhancing the performance of POA for the segmentation of polyp images.},
  archive      = {J_NCA},
  author       = {Abd Elaziz, Mohamed and Al-qaness, Mohammed A. A. and Al-Betar, Mohammed Azmi and Ewees, Ahmed A.},
  doi          = {10.1007/s00521-024-10667-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6327-6349},
  shortjournal = {Neural Comput. Appl.},
  title        = {Polyp image segmentation based on improved planet optimization algorithm using reptile search algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ResdenseNet: A lightweight dense ResNet enhanced with
depthwise separable convolutions and its applications for early plant
disease classification. <em>NCA</em>, <em>37</em>(8), 6305–6326. (<a
href="https://doi.org/10.1007/s00521-024-10972-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, artificial intelligence has undergone robust development, leading to the emergence of numerous autonomous AI applications. However, a crucial challenge lies in optimizing computational efficiency and reducing training time while maintaining high accuracy with limited hardware resources. This paper introduces ResdenseNet, a model built upon the MobileNet, DenseNet, and ResNet architectures. ResdenseNet combines dense blocks and residual blocks from the DenseNet and ResNet architectures. In these dense blocks, the standard convolutional units are replaced by depthwise separable convolutional units, a significant part of the MobileNet architecture. The experimental outcomes are contrasted with established models and their iterations, including ResNet-50, ResNet-101, MobileNet-V1, MobileNet-V2, DenseNet-121, and DenseNet-169. The proposed model is tested on benchmark and proposed datasets, showcasing its efficiency in reducing computations and accelerating the training process. Emphasizing hyperparameter importance, ResdenseNet, optimized with a growth rate of 64, 6 layers, and ReLU activation, achieves an accuracy of (98.73%) and a F1-score of (98.20%) on the wheat and barley dataset. The results indicate that ResdenseNet significantly decreases the number of parameters to 0.72M and efficiently shortens training time to 5983.54 s. Particularly noteworthy is ResdenseNet’s superiority over other models in terms of having the fewest parameters, the shortest training time, and the highest accuracy, especially when dealing with wheat, barley, and maize datasets.},
  archive      = {J_NCA},
  author       = {Nagpal, Jyoti and Goel, Lavika},
  doi          = {10.1007/s00521-024-10972-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6305-6326},
  shortjournal = {Neural Comput. Appl.},
  title        = {ResdenseNet: A lightweight dense ResNet enhanced with depthwise separable convolutions and its applications for early plant disease classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning framework for automatic assessment of
presence in virtual reality using multimodal behavioral cues.
<em>NCA</em>, <em>37</em>(8), 6283–6303. (<a
href="https://doi.org/10.1007/s00521-024-10943-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective development of virtual reality (VR) applications is heavily reliant on the evaluation of user experience (UX). However, traditional methods such as questionnaires have inherent limitations which hinder their ability to capture nuanced behavioral responses and impede the agility of VR content creation: They are time-consuming, burdensome for users, and require significant human effort for interpretation. This study introduces an automated framework aimed at addressing the limitations of questionnaire-based evaluations to assess UX in VR. Our primary focus is to validate the concept of this framework through assessment of the sense of presence (SOP), a crucial psychological perception with significant impact on VR UX. Our proposed framework utilizes a deep neural network (DNN) to analyze patterns in multimodal behavioral cues, including facial expressions, head movements, and hand movements, to predict scores from the Igroup Presence Questionnaire (IPQ). Additionally, we introduce two statistical profiles: the Visual Entropy Profile (VEP), which offers insights into visual complexity by depicting scene entropy, and the Experiential Presence Profile (EPP), which is designed to capture users’ historical SOP levels to enable personalized baseline and sensitivity estimation. The proposed framework achieves a significant correlation between actual and predicted IPQ scores, with a Spearman’s rank correlation coefficient of 0.7303, showcasing the potential of DNNs in analyzing complex behavioral signals and automating SOP assessment. This study represents a pioneering effort in leveraging DNNs for the automatic assessment of SOP and paves the way for future advances in automatically assessing VR UX and unlocking new opportunities in the field.},
  archive      = {J_NCA},
  author       = {Pannattee, Peerawat and Shimada, Shogo and Yem, Vibol and Nishiuchi, Nobuyuki},
  doi          = {10.1007/s00521-024-10943-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6283-6303},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep learning framework for automatic assessment of presence in virtual reality using multimodal behavioral cues},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing link prediction in graph data augmentation through
graphon mixup. <em>NCA</em>, <em>37</em>(8), 6267–6282. (<a
href="https://doi.org/10.1007/s00521-024-10923-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction in complex networks is a fundamental problem with applications in diverse domains, from social networks to biological systems. Traditional approaches often struggle to capture intricate relationships in graphs, leading to suboptimal predictions. To address this, we introduce a novel method called graphon mixup (GM), which leverages the power of graphons to enhance link prediction. The augmentation strategy involves generating a synthetic graph by combining the original graph with a graphon-based synthetic graph. This process, expressed as a weighted combination of adjacency matrices, strategically blends real and synthetic information, enriching the training dataset. GM formulates link prediction as a joint optimization problem, aligning the characteristics of the synthetic graph with the true underlying structure. The objective is to minimize cross-entropy loss between predicted and true edge probabilities. A detailed computational complexity analysis evaluates the time and space requirements, aiding in understanding the efficiency and scalability of GM across different datasets and network sizes. Empirical validation on benchmark datasets demonstrates GM’s effectiveness in consistently improving average precision across diverse network types. The proposed method enhances the generalization capabilities of link prediction models, providing a more robust framework capable of accurate predictions even in the presence of noise or unseen patterns.},
  archive      = {J_NCA},
  author       = {Sultana, Tangina and Hossain, Md. Delowar and Morshed, Md. Golam and Lee, Young-Koo},
  doi          = {10.1007/s00521-024-10923-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6267-6282},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing link prediction in graph data augmentation through graphon mixup},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier-resistant state estimation for memristor-based BAM
neural networks with probabilistic time-varying delays. <em>NCA</em>,
<em>37</em>(8), 6251–6265. (<a
href="https://doi.org/10.1007/s00521-024-10890-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers the issue of nonfragile state estimation (SE) for memristor-based bidirectional associative memory (MBAM) neural networks with probabilistic time-varying delays. The primary objective is to develop an effective estimator for accurately assessing neuron states, which are crucial in various engineering applications. Furthermore, we consider a scenario in which the measurement outputs of the neural networks may be influenced by abnormal disturbances, which could have a negative impact on the performance of the estimator. In this case, a factitious saturation constraint is introduced to mitigate the adverse effects on the designed outlier-resistant estimator, thereby improving the reliability of the estimator. Through constructing sensible Lyapunov–Krasovskii functional (LKF), a delay-dependent criterion is derived to guarantee the exponential stability of the augmented system. Finally, the effectiveness of the desired estimation scheme is demonstrated via two simulation examples.},
  archive      = {J_NCA},
  author       = {Shao, Xiaoguang and Zhang, Jie and Lyu, Ming and Lu, Yanjuan},
  doi          = {10.1007/s00521-024-10890-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6251-6265},
  shortjournal = {Neural Comput. Appl.},
  title        = {Outlier-resistant state estimation for memristor-based BAM neural networks with probabilistic time-varying delays},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable energy consumption and speed prediction in
sustainable cities using deep learning. <em>NCA</em>, <em>37</em>(8),
6233–6249. (<a
href="https://doi.org/10.1007/s00521-024-10850-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of sustainable cities has drawn a lot of attention due to the quick expansion of metropolitan areas as well as the growing problems brought on by resource scarcity and climate change. Cities that prioritize sustainable practices are those that minimize their negative effects on the environment, maximize resource efficiency, and improve the standard of living for their citizens. Therefore, for sustainable cities, this paper uses the vehicle energy dataset (VED) to estimate travel times and calculate vehicle energy consumption. The dataset contains 12,609,170 road elevation tracks, 12,203,044 speed limit tracks, and 12,281,719 speed limit records with direction. An open-source routing engine called Valhalla is utilized to do a variety of tasks, including finding paths, matching maps, and creating maneuvers based on paths. The three primary stages of the suggested model are data pre-processing, feature extraction, and result interpretation. In the data pre-processing stage, null values are first eliminated and data normalization is implemented. Then, three techniques known as the gated recurrent unit (GRU), recurrent neural network (RNN), and long short-term memory (LSTM) are used to optimize the model. Finally, the results are interpreted through the use of SHAP (SHapley Additive explanations) in explainable artificial intelligence (XAI) techniques. The LSTM model yields the best prediction results, achieving 15.2662 RMSE, 11.7266 MAE, and 0.6696 R2 at 8 batch size, according to the evaluation results. Additional experiments are carried out in batch sizes of 8, 16, 32, and 64.The lowest metrics are produced by batch sizes of 64, while the best metrics are produced by batch sizes of 8.},
  archive      = {J_NCA},
  author       = {Abd El-Latif, Eman I. and El-dosuky, Mohamed},
  doi          = {10.1007/s00521-024-10850-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6233-6249},
  shortjournal = {Neural Comput. Appl.},
  title        = {Explainable energy consumption and speed prediction in sustainable cities using deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance-based mutual congestion feature selection with
genetic algorithm for high-dimensional medical datasets. <em>NCA</em>,
<em>37</em>(8), 6217–6232. (<a
href="https://doi.org/10.1007/s00521-024-10837-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection poses a challenge in high-dimensional datasets, where the number of features exceeds the number of observations, as seen in microarray, gene expression, and medical datasets. There is not a universally optimal feature selection method applicable to any data distribution, and as a result, the literature consistently endeavors to address this issue. One recent approach in feature selection is termed frequency-based feature selection. However, existing methods in this domain tend to overlook feature values, focusing solely on the distribution in the response variable. In response,this paper introduces the Distance-based Mutual Congestion (DMC) as a filter method that considers both the feature values and the distribution of observations in the response variable. DMC sorts the features of datasets, and the top 5% are retained and clustered by KMeans to mitigate multicollinearity. This is achieved by randomly selecting one feature from each cluster. The selected features form the feature space, and the search space for the Genetic Algorithm with Adaptive Rates (GAwAR) will be approximated using this feature space. GAwAR approximates the combination of the top 10 features that maximizes prediction accuracy within a wrapper scheme. To prevent premature convergence, GAwAR adaptively updates the crossover and mutation rates. The hybrid DMC-GAwAR is applicable to binary classification datasets, and experimental results demonstrate its superiority over some recent works.},
  archive      = {J_NCA},
  author       = {Nematzadeh, Hossein and Mani, Joseph and Nematzadeh, Zahra and Akbari, Ebrahim and Mohamad, Radziah},
  doi          = {10.1007/s00521-024-10837-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6217-6232},
  shortjournal = {Neural Comput. Appl.},
  title        = {Distance-based mutual congestion feature selection with genetic algorithm for high-dimensional medical datasets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive loss optimization for enhanced learning
performance: Application to image-based rock classification.
<em>NCA</em>, <em>37</em>(8), 6199–6215. (<a
href="https://doi.org/10.1007/s00521-024-10965-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the geoscience domain, mainly within the oil and gas industry, getting the correct category of rock samples is crucial. Machine learning models deployed for rock classification often use the categorical cross-entropy loss function. This loss function may struggle when the rocks are either very similar or have too much intraclass variability. Furthermore, categorical cross-entropy loss may ignore subtle but significant differences between classes. This results in the ignoring of hard-to-classify samples and fails to prioritize learning from these challenging patterns. This can lead to models biased toward more common classes or mislabeling of underrepresented rock types, especially when dealing with noisy or inconsistent data. To bridge those gaps, we propose a new hybrid loss function. It combines the traditional categorical cross-entropy loss function with Online Hard Example Mining (OHEM), a method originally formulated for object detection tasks focusing on hard-to-classify samples. We designed this function to be adjustable, making it adaptable to various challenges inherent to the rock classification. We evaluated this technique on a diverse, highly heterogeneous, and challenging dataset provided by Shell Brazil. The available techniques were tested and encountered problems with challenging classes. However, our new technique improved classification accuracy for both challenging and easier samples. In addition to rock classification, this technique may serve as a blueprint for addressing complicated classification problems in other fields.},
  archive      = {J_NCA},
  author       = {Salavati, Soroor and Mendes Júnior, Pedro Ribeiro and Rocha, Anderson and Ferreira, Alexandre},
  doi          = {10.1007/s00521-024-10965-x},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6199-6215},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive loss optimization for enhanced learning performance: Application to image-based rock classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLAME: Fire detection in videos combining a deep neural
network with a model-based motion analysis. <em>NCA</em>,
<em>37</em>(8), 6181–6197. (<a
href="https://doi.org/10.1007/s00521-024-10963-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the catastrophic natural events posing hazards to human lives and infrastructures, fire is the phenomenon causing more frequent damages. Thanks to the spread of smart cameras, video fire detection is gaining more attention as a solution to monitor wide outdoor areas where no specific sensors for smoke detection are available. However, state-of-the-art fire detectors assure a satisfactory Recall but exhibit a high false-positive rate that renders the application practically unusable. In this paper, we propose FLAME, an efficient and adaptive classification framework to address fire detection from videos. The framework integrates a state-of-the-art deep neural network for frame-wise object detection, in an automatic video analysis tool. The advantages of our approach are twofold. On the one side, we exploit advances in image detector technology to ensure a high Recall. On the other side, we design a model-based motion analysis that improves the system’s Precision by filtering out fire candidates occurring in the scene’s background or whose movements differ from those of the fire. The proposed technique, able to be executed in real-time on embedded systems, has proven to surpass the methods considered for comparison on a recent literature dataset representing several scenarios. The code and the dataset used for designing the system have been made publicly available by the authors at ( https://mivia.unisa.it/large-fire-dataset-with-negative-samples-lfdn/ ).},
  archive      = {J_NCA},
  author       = {Gragnaniello, Diego and Greco, Antonio and Sansone, Carlo and Vento, Bruno},
  doi          = {10.1007/s00521-024-10963-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6181-6197},
  shortjournal = {Neural Comput. Appl.},
  title        = {FLAME: Fire detection in videos combining a deep neural network with a model-based motion analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging ChatGPT for enhanced stock selection and
portfolio optimization. <em>NCA</em>, <em>37</em>(8), 6163–6179. (<a
href="https://doi.org/10.1007/s00521-024-10928-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The public release of ChatGPT represents a significant milestone in generative AI technology, enabling the autonomous generation of content based on pre-training. This breakthrough presents new opportunities for advancements in the field of portfolio selection. This paper aims to introduce a comprehensive portfolio selection method by applying ChatGPT for stock selection and combining it with optimization algorithms to jointly optimize portfolio selection. Compared to randomly selected stocks, the portfolios optimized using ChatGPT-selected stocks and solved with the egret swarm optimization algorithm (ESOA) demonstrate higher diversification and lower volatility, leading to superior portfolio optimization results. Additionally, to validate ESOA’s superiority, its performance is compared against genetic algorithm (GA) and particle swarm optimization (PSO) on five metrics: risk, expected return, Sharpe ratio, objective value, and penalty term. Under equivalent experimental setting, ESOA exhibits a better ability to balance the relationship between risk and return.},
  archive      = {J_NCA},
  author       = {Huang, Zhendai and Liao, Bolin and Hua, Cheng and Cao, Xinwei and Li, Shuai},
  doi          = {10.1007/s00521-024-10928-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6163-6179},
  shortjournal = {Neural Comput. Appl.},
  title        = {Leveraging ChatGPT for enhanced stock selection and portfolio optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image quality assessment by enabling inter-patch message
passing via graph convolutional networks. <em>NCA</em>, <em>37</em>(8),
6145–6161. (<a
href="https://doi.org/10.1007/s00521-024-10893-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a crucial problem posing great challenges to the image quality assessment (IQA), that is, how to accurately regress the visual quality score of an entire image from its patches. The vast majority of existing patch-based IQA methods treat each patch independently. In this paper, we innovatively enable inter-patch message passing (MP) for the proposed IQA via graph convolutional networks (IQG). The patches are embedded into the graph by treating the low-dimensional vector representation of each patch as a node and the inter-patch intrinsic correlation as an edge. Since the intrinsic correlation is not directly available, an adaptive edge generator is proposed to adaptively construct the directed weighted edges by separately obtaining the patch-connected mask and the edge weights. To mitigate the overfitting that may occur when adaptive MP is enabled, we attach an embedding approach that creates the undirected unweighted edge between any two patches to enable each node in the graph to connect to every other node, thus passing the information that otherwise would be neglected. Extensive experiments demonstrate the state-of-the-art performance of our proposed IQG in complete scenarios, including full-reference and no-reference IQA tasks on benchmark IQA databases.},
  archive      = {J_NCA},
  author       = {Liu, Yufan and Guo, Jiefeng},
  doi          = {10.1007/s00521-024-10893-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6145-6161},
  shortjournal = {Neural Comput. Appl.},
  title        = {Image quality assessment by enabling inter-patch message passing via graph convolutional networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing online shopping with FITMI: A realistic
virtual try-on solution. <em>NCA</em>, <em>37</em>(8), 6125–6144. (<a
href="https://doi.org/10.1007/s00521-024-10843-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital age, consumers increasingly rely on online shopping for convenience and accessibility. However, a significant drawback of online shopping is the inability to physically try on clothing before purchasing. This limitation often leads to uncertainty regarding fit and style, resulting in customer post-purchase dissatisfaction and higher return rates. Research indicates that online items are three times more likely to be returned than in-store ones, especially during the pandemic. To address this challenge, we propose a virtual try-on method called FITMI, an enhanced Latent Diffusion Textual Inversion model for virtual try-on purposes. The proposed architecture aims to bridge the gap between traditional in-store try-ons and online shopping by offering users a realistic and interactive virtual try-on experience. Although virtual try-on solutions already exist, recent advancements in artificial intelligence have significantly enhanced their capabilities, enabling more sophisticated and realistic virtual try-on experiences than ever before. Building on these advancements, FITMI surpasses ordinary virtual try-ons relying on generative adversarial networks, often producing unrealistic outputs. Instead, FITMI utilizes latent diffusion models to generate high-quality images with detailed textures. As a web application, FITMI facilitates virtual try-ons by seamlessly integrating images of users with garments from catalogs, providing a true-to-life representation of how the items would look. This approach differentiates us from competitors. FITMI is validated using two widely recognized benchmarks: the Dress-Code and Viton-HD datasets. Additionally, FITMI acts as a trusted style advisor, enhancing the shopping experience by recommending complementary items to elevate the chosen garment and suggesting similar options based on user preferences.},
  archive      = {J_NCA},
  author       = {Samy, Tassneam M. and Asham, Beshoy I. and Slim, Salwa O. and Abohany, Amr A.},
  doi          = {10.1007/s00521-024-10843-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6125-6144},
  shortjournal = {Neural Comput. Appl.},
  title        = {Revolutionizing online shopping with FITMI: A realistic virtual try-on solution},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-orthogonal-oppositional enhanced african vultures
optimization for combined heat and power economic dispatch under
uncertainty. <em>NCA</em>, <em>37</em>(8), 6097–6123. (<a
href="https://doi.org/10.1007/s00521-024-10715-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper attempts to conceptualize a potent methodology by combining the African vultures optimization algorithm (AVOA) with a multi-orthogonal-oppositional strategy (M2OS), named AVO-M2OS, to address the nonconvexity and multidimensional nature of the combined heat and power economic dispatch (CHPED) problem under both crisp and uncertainty aspects. The AVO-M2OS uses the M2OS to simultaneously explore the search region, improving solutions’ diversity as well as solution quality. Therefore, AVO-M2OS can perform deeper exploration and exploitation features and thus mitigate the trapping at local optima, especially when tackling the more complicated nature of the CHPED problem. A three-stage analysis is conducted to assess the effectiveness of the proposed AVO-M2OS algorithm. During the first stage, the algorithm’s performance is evaluated on benchmark problems such as CEC 2005 and CEC 2019, employing statistical verifications and convergence characteristics. In the second stage, the significance of the results is evaluated using the nonparametric Friedman test to demonstrate that the results did not occur by chance. The results indicate that the AVO-M2OS algorithm outperforms the best existing algorithm (AVOA) by an average rank of the Friedman test exceeding 26% for the CEC 2005 suite while outperforming the gray wolf optimization (GWO) by 60% for the CEC 2019 suite. Moreover, the AVO-M2OS demonstrates exceptional performance compared to existing state-of-the-art algorithms, surpassing the best algorithm available by an average rank of the Friedman test that exceeds 41%. Finally, the AVO-M2OS’s applicability is achieved by minimizing the operational costs by finding the optimal power and heat generation scheduling for the CHPED problem. The recorded results realize that the AVO-M2OS algorithm offers accurate performance compared to competing optimizers, where it saves the operational cost of the 48-unit system by 24% on the original AVO variant. Furthermore, the uncertainty aspect of CHPED, called UCHPED, is investigated using intuitionistic fuzzy numbers (IFN) to simulate the fluctuation based on customers’ varying energy demands across different time periods, posing a significant challenge for timely and equitable energy allocation. The optimization results show that the suggested AVO-M2OS algorithm provides more robust and reliable optimal solutions compared to other methods in most of the studied benchmark functions, including the CHPED problem, addressing both crisp and uncertain aspects. Therefore, it is a potential alternative for both real-world operations and modeling situations.},
  archive      = {J_NCA},
  author       = {Rizk-Allah, Rizk M. and Snášel, Václav and Hassanien, Aboul Ella},
  doi          = {10.1007/s00521-024-10715-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6097-6123},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-orthogonal-oppositional enhanced african vultures optimization for combined heat and power economic dispatch under uncertainty},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIG-DARTS: Towards effective differentiable architecture
search by gradually mitigating the initial-channel gap between search
and evaluation. <em>NCA</em>, <em>37</em>(8), 6085–6096. (<a
href="https://doi.org/10.1007/s00521-024-10681-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) based on differentiable methods has made significant progress in both search cost (GPU-days and GPU memory consumption) and network performance. However, there still exists a large gap between the search and evaluation due to the unaffordable search cost, which will cause the searched architecture to be suboptimal in the evaluation. Based on the observation of the large initial-channel gap between search and evaluation, this paper is the first to propose to gradually mitigate the initial-channel gap as the search stage proceeds to elevate the performance of evaluation architecture; meanwhile, we remove poorly performing candidate operations after each search stage to keep an acceptable search cost. To further alleviate the excessive growth of search cost brought by the progressive increase of initial-channels, this paper proposes to separate the search space, by which an individual search space with reduced candidate operations is built for normal cell and reduction cell, respectively. Moreover, this paper proposes a stability-aware stopping strategy to alleviate the problem of invalid search to reduce the search cost in GPU-days. By conducting experiments on CIFAR10 and CIFAR100 datasets, the results show that the proposed method can achieve state-of-the-art performance with a small search cost.},
  archive      = {J_NCA},
  author       = {Hao, Debei and Pei, Songwei},
  doi          = {10.1007/s00521-024-10681-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6085-6096},
  shortjournal = {Neural Comput. Appl.},
  title        = {MIG-DARTS: Towards effective differentiable architecture search by gradually mitigating the initial-channel gap between search and evaluation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-variant quadratic programming solving by using
finitely-activated RNN models with exact settling time. <em>NCA</em>,
<em>37</em>(8), 6067–6084. (<a
href="https://doi.org/10.1007/s00521-024-10922-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are well established with comprehensive models capable of solving zero finding problems. Most of the conventional designs apply the infinite activation, but may not be practical for implementation. This paper presents model designs of finitely-activated zeroing neural networks (structure-like RNNs), possessing the finite-time convergence property as well, for solving time-variant convex quadratic programming. Two techniques for realizing finite activation are provided, based on which novel activation functions (AFs) are constructed, including the conic AFs and the finitely-valued power-rate AFs. Theoretical analyses of finite-time convergence are presented in detail and settling time is exactly established for each model. It is shown that finitely-valued AFs can approximate or even outperform the original power-rate AFs. The proposed neural network models are applied to solve an example of time-variant quadratic programming, and the repetitive motion planning of redundant robots with joint angle and joint velocity constraints, where the anti-disturbance capability of the finitely-activated integral neural network models has been examined and verified. The obtained numerical results demonstrate effectiveness of the computing schemes.},
  archive      = {J_NCA},
  author       = {Sun, Mingxuan and Zhang, Yu and Wang, Liming and Wu, Yuxin and Zhong, Guomin},
  doi          = {10.1007/s00521-024-10922-8},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6067-6084},
  shortjournal = {Neural Comput. Appl.},
  title        = {Time-variant quadratic programming solving by using finitely-activated RNN models with exact settling time},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PartialST: Partial spatial–temporal learning for urban flow
prediction. <em>NCA</em>, <em>37</em>(8), 6053–6066. (<a
href="https://doi.org/10.1007/s00521-024-10888-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate urban flow prediction plays a crucial role in transportation management, as it enables optimized resource allocation and improved traffic efficiency. Although current methods have made advances, they still encounter challenges such as high computational overhead and the risk of overfitting complex models. To tackle these issues, we introduce the partial channel connection to urban flow prediction, aiming to reduce complexity while keeping competitive performance. The partial channel connection selectively engages a specific subset of channels for operations in a single step, while preserving the identity mapping for the remaining channels. This approach ensures comprehensive training of all channels throughout the entirety of the training process and mitigates computational overheads at each step. In this paper, we apply the partial channel connection across various spatial and temporal encoders, undertaking a thorough investigation into their predictive accuracy. Based on these insights, we design a model named PartialST for urban flow prediction, which effectively captures the temporal and spatial correlations. We evaluate PartialST through comparative experiments against other state-of-the-art methods on two real-world datasets. The results demonstrate not only the superior performance of our model over other comparative models but also the effectiveness of the partial channel connection approach.},
  archive      = {J_NCA},
  author       = {Wang, Yong and Li, Xiaoyu and Zhang, Xinxin and Liu, Rui and Gong, Yongshun},
  doi          = {10.1007/s00521-024-10888-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6053-6066},
  shortjournal = {Neural Comput. Appl.},
  title        = {PartialST: Partial spatial–temporal learning for urban flow prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated decision-making process for risk analysis of
decentralized finance. <em>NCA</em>, <em>37</em>(8), 6021–6051. (<a
href="https://doi.org/10.1007/s00521-024-10839-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized finance is upending the financial system through innovative, open, and interoperable financial solutions. Decentralized finance is a rapidly emerging field based on distributed ledger technology. Decentralized banking protocols are witnessing a perfect storm in terms of growth. However, because these financial innovations pose specific risks to consumers, creators, regulators, and other stakeholders, this emerging subject demands careful investigation. The current study tries to categorize and rate the risks connected with decentralized finance. The current study seeks to identify the multiple risks associated with decentralized finance through a thorough literature analysis. Data gathered from specialists in prior research were incorporated into the study used for empirical analysis. As MCDM techniques, IVFF-based DEMATEL, AHP, and TOPSIS are first used, and then the IVFF-ARAS method and sensitivity analysis are used for performance evaluation and verification. The findings of this study have several ramifications for legislators, businesspeople, technologists, and practitioners. These stakeholders can concentrate on these weaknesses in the future and provide longer-lasting solutions.},
  archive      = {J_NCA},
  author       = {Kirişci, Murat},
  doi          = {10.1007/s00521-024-10839-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6021-6051},
  shortjournal = {Neural Comput. Appl.},
  title        = {An integrated decision-making process for risk analysis of decentralized finance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based gait cycle segmentation using
instantaneous knee and hip-extension angles for biomechanical analysis.
<em>NCA</em>, <em>37</em>(8), 6009–6019. (<a
href="https://doi.org/10.1007/s00521-024-10720-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biomechanical analysis of human movement, particularly gait, is crucial in fields such as clinical medicine, sports, and rehabilitation. While traditional motion capture (Mocap) systems are effective, they are often limited by their complexity, high cost, and the unnatural settings they require in terms of the gesture and motion environment. Emerging tools like inertial sensors and markerless video-based systems offer greater flexibility but encounter challenges in motion cycle segmentation, as they present kinematic data as time series, adding new difficulties to the analysis. This paper introduces a novel machine learning-based system for automatic gait cycle segmentation using features extracted from two easily measurable lower limb kinematic variables: hip and knee extension angles. The proposed method leverages instantaneous information from these angles for segmentation, ensuring versatility and independence from specific data collection methods. This allows for rapid segmentation and potential implementation on lower-performance processors. Experimental results demonstrate the high accuracy and efficiency of the proposed algorithm segmenting the gait cycle. The F1-score was 0.997. By using readily available hip and knee kinematic data and identifying crucial biomechanical relationships, our method offers a versatile and practical solution for motion analysis across various clinical and sports applications.},
  archive      = {J_NCA},
  author       = {Solórzano, Brayan David and Chavez, Susana and Giraldo, Luis Felipe and De la Portilla, Christian Cifuentes},
  doi          = {10.1007/s00521-024-10720-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6009-6019},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine learning-based gait cycle segmentation using instantaneous knee and hip-extension angles for biomechanical analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating contextual intelligence with mixture of experts
for signature and anomaly-based intrusion detection in CPS security.
<em>NCA</em>, <em>37</em>(8), 5991–6007. (<a
href="https://doi.org/10.1007/s00521-024-10967-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of IoT and cyber-physical systems (CPSs) in smart homes and critical infrastructures has led to the possibility of physical damage from system compromises. Security failures in sectors like power, transport, and public safety can have more severe physical impacts than just information loss. Intrusion detection systems (IDSs) are crucial in a defense-in-depth approach. We propose a detection engine to prevent CPS from transitioning into unsafe states beyond critical limits, thresholds, and behavioral normalicies. A novel host-based IDS using a mixture-of-experts (MoE) model is introduced in the CPS security paradigm. For signature-based protection, we developed a context-aware CPS-SNORT ruleset for deep packet inspection (DPI) of Gcode instructions (NIST RS-274/ISO 6983-1:2009) used in numerical control of machines like CNCs and 3D printers. A new Gcode dataset was developed on a CPS test bed. In a supervised learning approach, we achieved over 99% accuracy with random tree for known attack detection. In a semi-supervised approach, logistic regression achieved 85% accuracy. For behavioral anomaly detection, LSTM achieved 99.9% accuracy, outperforming isolation forest and local outlier factor.},
  archive      = {J_NCA},
  author       = {Rahim, Kashif and Nasir, Zia Ul Islam and Ikram, Nassar and Qureshi, Hassaan Khaliq},
  doi          = {10.1007/s00521-024-10967-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5991-6007},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating contextual intelligence with mixture of experts for signature and anomaly-based intrusion detection in CPS security},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taking class imbalance into account in open set recognition
evaluation. <em>NCA</em>, <em>37</em>(8), 5975–5989. (<a
href="https://doi.org/10.1007/s00521-024-10960-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks have been employed increasingly often, which correlates with them receiving growing user trust. However, such systems cannot identify samples from unknown classes and often induce an incorrect decision with high confidence. This is aimed to be solved by open set recognition methods. The presented work looks at the evaluation protocols of existing approaches in the field. A particular focus is being placed on the impact of class imbalance, especially in the dichotomy between known and unknown samples, which is a rarely considered factor in the experimental environment. The work analyzes current evaluation strategies—regarding dataset construction and metric selection—noting that the class imbalance can significantly impact the obtained results. We analyze the effect of using the popular baseline metrics (accuracy, balanced accuracy, and F1-score) for method quality assessment and introduce a protocol extension to four recognition quality measures that can be built upon those baselines. The analysis of base measures revealed that the choice of baseline metric could significantly impact the computed criterion values when the class imbalance of the recognized problem appears. The proposed experimental environment was used in an example experiment on commonly used computer vision datasets. As an outcome of problem analysis, we present a set of guidelines for evaluating open set recognition methods.},
  archive      = {J_NCA},
  author       = {Komorniczak, Joanna and Ksieniewicz, Paweł},
  doi          = {10.1007/s00521-024-10960-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5975-5989},
  shortjournal = {Neural Comput. Appl.},
  title        = {Taking class imbalance into account in open set recognition evaluation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A momentum accelerated stochastic method and its application
on policy search problems. <em>NCA</em>, <em>37</em>(8), 5957–5973. (<a
href="https://doi.org/10.1007/s00521-024-10883-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the dramatic increase in model complexity and problem scales in the machine learning area, researches on the first-order stochastic methods and its accelerated variants for non-convex problems have attracted wide research interest. However, most works on convergence analysis of accelerated methods focus on general convex or strongly convex objective functions. In this paper, we consider an accelerated scheme coming from dynamic systems and ordinary differential equations, which has a simpler and more direct form than the traditional scheme. We construct auxiliary sequences of iteration points as analysis tools, which can be interpreted as extension of Nesterov’s estimate sequence in non-convex case. We analyze the convergence property under different cases when momentum parameters are fixed or varying over iterations. For non-smooth and general convex objective functions, we give a relaxed step-size requirement to ensure convergence. For the non-convex policy search problem in classical reinforcement learning, we propose an accelerated stochastic policy gradient method with restart technique and construct numerical experiments to verify its effectiveness.},
  archive      = {J_NCA},
  author       = {Jiang, Boou and Yuan, Ya-xiang},
  doi          = {10.1007/s00521-024-10883-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5957-5973},
  shortjournal = {Neural Comput. Appl.},
  title        = {A momentum accelerated stochastic method and its application on policy search problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do as you teach: A multi-teacher approach to self-play in
deep reinforcement learning. <em>NCA</em>, <em>37</em>(8), 5945–5956.
(<a href="https://doi.org/10.1007/s00521-024-10829-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A long-running challenge in the reinforcement learning (RL) community has been to train a goal-conditioned agent in sparse reward environment such that it also generalizes to unseen goals. We propose a novel goal-conditioned RL algorithm; Multi-Teacher Asymmetric Self-Play, which allows multiple agents (i.e., the teachers) to create a successful curriculum for another agent (i.e., the student) and empirically demonstrate its effectiveness on complex domains like FetchReach and a novel driving simulator designed for goal-conditioned RL. Our results show a 30-40% improvement over the baseline while also improving the learning speed of the student. We attribute this improvement in performance to the better exploration and coverage of the state space by multiple teacher agents. In addition, the results show that completely new students can learn offline from the goals generated by teachers trained with a previous student, reducing the computational cost by around 95%. This is crucial in the context of application domains where repeatedly training a teacher agent is expensive or even infeasible.},
  archive      = {J_NCA},
  author       = {Kharyal, Chaitanya and Gottipati, Sai Krishna and Sinha, Tanmay Kumar and Abdollahi, Fatemeh and Das, Srijita and Taylor, Matthew E.},
  doi          = {10.1007/s00521-024-10829-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5945-5956},
  shortjournal = {Neural Comput. Appl.},
  title        = {Do as you teach: A multi-teacher approach to self-play in deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSUBF-net: Trans-spatial UNet-like network with bi-direction
fusion for segmentation of adenoid hypertrophy in CT. <em>NCA</em>,
<em>37</em>(8), 5927–5943. (<a
href="https://doi.org/10.1007/s00521-024-10824-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adenoid hypertrophy stands as a common cause of obstructive sleep apnea–hypopnea syndrome in children. It is characterized by snoring, nasal congestion, and growth disorders. Computed tomography (CT) emerges as a pivotal medical imaging modality, utilizing X-rays and advanced computational techniques to generate detailed cross-sectional images. Within the realm of pediatric airway assessments, CT imaging provides an insightful perspective on the shape and volume of enlarged adenoids. Despite the advances of deep learning methods for medical imaging analysis, there remains an emptiness in the segmentation of adenoid hypertrophy in CT scans. To address this research gap, we introduce TSUBF-Net (Trans-Spatial UNet-like Network based on Bi-direction Fusion), a 3D medical image segmentation framework. TSUBF-Net is engineered to effectively discern intricate 3D spatial interlayer features in CT scans and enhance the extraction of boundary-blurring features. Notably, we propose two innovative modules within the U-shaped network architecture: the Trans-Spatial Perception (TSP) module and the Bi-directional Sampling Collaborated Fusion (BSCF) module. These two modules are in charge of operating during the sampling process and strategically fusing down-sampled and up-sampled features, respectively. Furthermore, we introduce the Sobel loss term, which optimizes the smoothness of the segmentation results and enhances model accuracy. Extensive 3D segmentation experiments are conducted on several datasets. TSUBF-Net is superior to the state-of-the-art methods with the lowest HD95: 7.03, IoU: 85.63, and DSC: 92.26 on our own AHSD dataset. The results in the other two public datasets also demonstrate that our methods can robustly and effectively address the challenges of 3D segmentation in CT scans.},
  archive      = {J_NCA},
  author       = {Zhou, Rulin and Feng, Yingjie and Wang, Guankun and Zhong, Xiaopin and Wu, Zongze and Wu, Qiang and Zhang, Xi},
  doi          = {10.1007/s00521-024-10824-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5927-5943},
  shortjournal = {Neural Comput. Appl.},
  title        = {TSUBF-net: Trans-spatial UNet-like network with bi-direction fusion for segmentation of adenoid hypertrophy in CT},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SE-GCL: An event-based simple and effective graph
contrastive learning for text representation. <em>NCA</em>,
<em>37</em>(8), 5913–5926. (<a
href="https://doi.org/10.1007/s00521-024-10686-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text representation learning is significant as the cornerstone of natural language processing. In recent years, graph contrastive learning (GCL) has been widely used in text representation learning due to its ability to represent and capture complex text information in a self-supervised setting. However, the current mainstream graph contrastive learning methods often require the incorporation of domain knowledge or cumbersome computations to guide the data augmentation process, which significantly limits the application efficiency and scope of GCL. Additionally, many methods learn text representations only by constructing word-document relationships, which overlooks the rich contextual semantic information in the text. To address these issues and exploit representative textual semantics, we present an event-based, simple, and effective graph contrastive learning (SE-GCL) for text representation. Precisely, we extract event blocks from text and construct internal relation graphs to represent inter-semantic interconnections, which can ensure that the most critical semantic information is preserved. Then, we devise a streamlined, unsupervised graph contrastive learning framework to leverage the complementary nature of the event semantic and structural information for intricate feature data capture. In particular, we introduce the concept of an event skeleton for core representation semantics and simplify the typically complex data augmentation techniques found in existing graph contrastive learning to boost algorithmic efficiency. We employ multiple loss functions to prompt diverse embeddings to converge or diverge within a confined distance in the vector space, ultimately achieving a harmonious equilibrium. We conducted experiments on the proposed SE-GCL on four standard data sets (AG News, 20NG, SougouNews, and THUCNews) to verify its effectiveness in text representation learning. The accuracy achieved on the respective datasets is 91.56, 86.76, 98.03, and 97.79%, demonstrating superior performance on most datasets compared to baseline methods.},
  archive      = {J_NCA},
  author       = {Meng, Tao and Ai, Wei and Li, Jianbin and Wang, Ze and Li, Keqin},
  doi          = {10.1007/s00521-024-10686-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5913-5926},
  shortjournal = {Neural Comput. Appl.},
  title        = {SE-GCL: An event-based simple and effective graph contrastive learning for text representation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked hybrid attention with laplacian query fusion and
tripartite sequence matching for medical image segmentation.
<em>NCA</em>, <em>37</em>(8), 5891–5911. (<a
href="https://doi.org/10.1007/s00521-024-10934-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is pivotal in computer-aided diagnosis systems, demanding high precision and contextual understanding. Vision Transformer-based approaches have gained much attention recently due to their excellent performance and ability to capture long-range dependencies in medical images. However, research shows they suffer from inadequate multi-scale feature integration, poor object localization, and inconsistent mask predictions, leading to sub-optimal segmentation performance. This paper addresses these challenges by redefining semantic medical image segmentation through learnable object queries within an enhanced transformer framework with a masked hybrid attention querying mechanism, optimizing multi-scale feature fusion, object localization, and instance-specific segmentation. First, this study presents a novel transformer-based masked hybrid attention mechanism using Laplacian query fusion on learnable query features and incorporating a novel tripartite sequence matching technique as part of the enhanced decoder block to improve the consistency of mask predictions and optimize decoder queries. The designed hybrid multi-head self- and cross-attention mechanisms aim to selectively integrate multi-scale features, ensuring optimal feature combinations for precise segmentation. Secondly, multiple class tokens are incorporated to improve object localization and capture class-specific characteristics within the transformer framework, leveraging the transformer decoders’ ability to learn distinct instance representations. Experimental results and extensive ablation studies demonstrate the effectiveness of the proposed approach on three publicly available datasets, obtaining better segmentation results compared to various state-of-the-art approaches using various evaluation metrics. Specifically, the proposed model achieves a Dice Score of 95.25%, 92.75%, and 85.25% on LUNA, ISIC, and DRIVE datasets, respectively.},
  archive      = {J_NCA},
  author       = {Ekong, Favour and Yu, Yongbin and Patamia, Rutherford Agbeshi and Sarpong, Kwabena and Ukwuoma, Chiagoziem C. and Wang, Xiangxiang and Ukot, Akpanika Robert and Cai, Jingye},
  doi          = {10.1007/s00521-024-10934-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5891-5911},
  shortjournal = {Neural Comput. Appl.},
  title        = {Masked hybrid attention with laplacian query fusion and tripartite sequence matching for medical image segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMVL4AVD: A deep multi-view learning model for automated
vulnerability detection. <em>NCA</em>, <em>37</em>(8), 5873–5889. (<a
href="https://doi.org/10.1007/s00521-024-10892-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated vulnerability detection is crucial to protect software systems. However, state-of-the-art approaches mainly focus on a single view of the source code, which often leads to incomplete code representation and low detection accuracy. To solve these problems, this paper proposes a novel automatic vulnerability detection model, DMVL4AVD, based on deep multi-view learning that represents source codes from three distinct views: code sequences, code property graphs, and code metrics. Different deep models are employed to extract features from each view. Firstly, the [CLS] vectors derived from encoder layers 1 to 12 of GraphCodeBERT are used as code sequence features which contain rich semantic information. Next, the gated graph neural network (GGNN) is exploited to learn the features of nodes in the code property graph, encompassing both syntactic and dependency information of the source code. During the extraction of graph features, node representation is augmented by incorporating the degree centrality of each node, along with its corresponding code and type attributes, resulting in a more comprehensive depiction of the graph&#39;s structure. Statistical metrics generated by the code analysis tool SourceMonitor are then processed through a 1-dimensional (1-D) CNN to produce metric features. Fused features from these three views are learned by a multilayer perceptron (MLP) to yield final classification results. Experimental results demonstrate the superiority of DMVL4AVD over existing approaches. The model performs significantly better than the studied baselines, achieving an average increase in accuracy of 6.79% and an average boost of 6.94% in precision compared to the approaches in the literature.},
  archive      = {J_NCA},
  author       = {Du, Xiaozhi and Zhou, Yanrong and Du, Hongyuan},
  doi          = {10.1007/s00521-024-10892-x},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5873-5889},
  shortjournal = {Neural Comput. Appl.},
  title        = {DMVL4AVD: A deep multi-view learning model for automated vulnerability detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing affordable EEG to act as a quantitative EEG for
inattention treatment using MATLAB. <em>NCA</em>, <em>37</em>(8),
5849–5871. (<a
href="https://doi.org/10.1007/s00521-024-10835-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lack of attention is a chronic behavior in attention deficit hyperactivity disorder (ADHD), autism spectrum disorder (ASD), and other disorders that harm academic and social performance. ADHD is a disorder whose typical symptoms include inattention, hyperactivity, and impulsivity. They have a major impact on the affected person’s function and development. The electroencephalogram (EEG) device is a diagnostic tool, whereas the quantitative EEG (QEEG) device is a diagnostic and therapeutic tool for most mental disorders. QEEG applies the neurofeedback method in treatment. Neurofeedback is a technique for training brain functions and is an alternative to the traditional oral treatment of inattention disorders due to its numerous side effects. The proposed software can upgrade most EEG devices in hospitals and clinics into QEEGs capable of neurofeedback. The upgrading tools and stages are introduced in this study. The cost of upgrading an EEG device is 25 times less than the purchase price of a QEEG device. The EEG device (Open BCI) has been upgraded with MATLAB to function as a QEEG system, integrating a variety of feature extraction methods for inattention detection such as fractal dimension (FD), wavelet transform (WT), multi-resolution techniques (MR), and empirical mode decomposition (EMD) which signified a notable progress in the field. Furthermore, the implemented software is easily customizable to include any forthcoming superior techniques that may arise. Earlier research distinguished the differences between states of relaxation and concentration using a simple fixed threshold. In this paper, short training has been utilized to calculate adaptive thresholds to optimize individual effects. Different thresholding techniques were employed with the EMD_Dt technique to distinguish between focused and unfocused epochs. The adaptive threshold method results have been more accurate reaching the benchmark of 99.82%, as opposed to the fixed threshold method, which reaches an accuracy of 97.73%. The findings were assessed through a pilot study involving 3483 epochs collected across 24 sessions from male and female children aged between 5 and 16. The proposed QEEG software was evaluated to be Specific, Measurable, Achievable, Realistic, and Timed (SMART) with an effect size of 0.85528336, which is significant.},
  archive      = {J_NCA},
  author       = {Magdy Rady, Radwa and Elsalamawy, Doaa and Rizk, M. R. M. and Abdel Alim, Onsy and Diaa Moussa, Nancy},
  doi          = {10.1007/s00521-024-10835-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5849-5871},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing affordable EEG to act as a quantitative EEG for inattention treatment using MATLAB},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Historical states modeling for visual tracking.
<em>NCA</em>, <em>37</em>(7), 5831–5848. (<a
href="https://doi.org/10.1007/s00521-024-10921-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting additional spatiotemporal information from video sequences is critical for accurately perceiving target appearance changes during visual tracking. However, most learning-based trackers utilize only a single search image and template from a video for training, resulting in a lack of temporal information and low data utilization. To address these issues, we present an innovative Trajectory Guided Tracking (TGTrack) framework, which leverages the historical states of the target to predict its current location. Specifically, we construct trajectory tokens derived from tracking results in historical frames, integrating the position and scale information of the target. We propose a trajectory prediction module to utilize these trajectory tokens to generate the potential scope of current target. Furthermore, to enhance the inference efficiency of the tracker, we eliminate manually customized heads and post-processing steps. Consequently, we achieve a good balance between inference speed and effectiveness. Extensive experimental results demonstrate that our TGTrack achieves leading performance across multiple benchmarks.},
  archive      = {J_NCA},
  author       = {Shi, Junze and Yu, Yang and Hui, Bin and Shi, Jian and Luo, Haibo},
  doi          = {10.1007/s00521-024-10921-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5831-5848},
  shortjournal = {Neural Comput. Appl.},
  title        = {Historical states modeling for visual tracking},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A surface defect detection instrument for large aperture
spherical optical elements. <em>NCA</em>, <em>37</em>(7), 5815–5829. (<a
href="https://doi.org/10.1007/s00521-024-10889-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spherical optical elements are an important classification of modern optical system components. Surface defects on optical elements can cause serious problems to the optical system. Fast and accurate defect detection for large aperture (200 mm) spherical optical elements is a challenge for industrial applications due to the tiny scale of the defects and the non-flat and super-smooth surface. A surface defect detection instrument for large aperture spherical optical elements is established in this paper, while the main contributions are: (1) A 5-axis motion system with microscopic imaging and multi-angle lighting system is designed for defect detection. (2) To image the spherical element surface clearly and completely, an adaptive path planning method that suits the 5-axis motion system is raised. (3) Aiming at tiny and weak defects of precise optical elements, an effective defect detection algorithm based on reverse attention is proposed for the instrument, which outperforms the baseline methods. Experiments display the instrument’s superior capability to existing instruments that, for a spherical optical element of 200 mm aperture and 200 mm radius of the sphere surface, the instrument can achieve surface defect detection with the precision of 2 μm in less than 10 min.},
  archive      = {J_NCA},
  author       = {Li, Mingwei and Shi, Yali and Zhang, Zhengtao and Tao, Xian and Shang, Xiuqin},
  doi          = {10.1007/s00521-024-10889-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5815-5829},
  shortjournal = {Neural Comput. Appl.},
  title        = {A surface defect detection instrument for large aperture spherical optical elements},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent recognition system of in-service tire damage
driven by strong combination augmentation and contrast fusion.
<em>NCA</em>, <em>37</em>(7), 5795–5813. (<a
href="https://doi.org/10.1007/s00521-024-10898-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of computer technology, dynamic detection of damage to tire in-service has become feasible. However, current methods often struggle with accuracy limitations when confronted with specific working conditions and external factors. To address this challenge, we propose an intelligent recognition system for in-service tire damage driven by Strong Combination Augmentation and Contrast Fusion. The system uses a key feature learning enhancement method to address the problem. It uses the Hough transform and the Perceptual Hash algorithm to perform secondary feature comparison, enabling tire region detection even in low-resolution and high-interference scenarios. To effectively eliminate interference caused by wear, stains, and similar factors, we also introduce an efficient damage detection network called CA-EffNet. This network employs a strategic approach that combines various augmentation techniques and parameters with contrast fusion within a supervised learning framework. By integrating these elements, CA-EffNet expands the feature exploration space and effectively captures key damage features. The results show that the system efficiently achieves real-time detection within just 0.7 s at speeds of up to 15 km/h, meeting strict detection requirements. These results highlight the potential of the system to significantly advance the field of tire damage detection and ultimately contribute to safer roads.},
  archive      = {J_NCA},
  author       = {Shen, Dagang and Cao, Jinfeng and Liu, Peng and Guo, Jihong},
  doi          = {10.1007/s00521-024-10898-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5795-5813},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intelligent recognition system of in-service tire damage driven by strong combination augmentation and contrast fusion},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fake news detection based on multi-modal domain adaptation.
<em>NCA</em>, <em>37</em>(7), 5781–5793. (<a
href="https://doi.org/10.1007/s00521-024-10896-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of social media has led to the unguaranteed authenticity of news, and the role of fake news detection in cybersecurity governance has become increasingly prominent. In this paper, we mine information from multiple modalities, such as the text and images of news, and propose a multi-modal fake news detection model based on the multi-stage domain adaptation for the differences existing between source and task domains and between different modalities. The multi-modal feature extraction network of BERT combined with EfficientNet is used to deeply analyze the features of social media data, and the multi-modal domain adaptation network is used to reduce the domain shift of different domains and different modalities of news data and to capture the correlation between events by adversarial ideas. Experimental results on public datasets of Weibo and Twitter show that the model significantly improves the effectiveness of the fake news detection task.},
  archive      = {J_NCA},
  author       = {Wang, Xiaopei and Meng, Jiana and Zhao, Di and Meng, Xuan and Sun, Hewen},
  doi          = {10.1007/s00521-024-10896-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5781-5793},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fake news detection based on multi-modal domain adaptation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive study of fisheye image compression and
perception for autonomous driving. <em>NCA</em>, <em>37</em>(7),
5765–5780. (<a
href="https://doi.org/10.1007/s00521-024-10831-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisheye cameras are widely used in various fields, including automotive contexts for $$360^{\circ }$$ near-field vision around vehicles, as well as in photography, robotics, underwater imaging, and virtual reality. However, conventional image compression techniques do not take into account the specific characteristics of fisheye images, such as radial distortion and wide-angle field of view, especially when operating at low bit rates. This can lead to degradation of image quality and distortion of geometric features that are essential for computer vision (CV) applications such as object detection, semantic segmentation, and motion estimation. Recent studies have highlighted the impact of various noise factors on automotive camera sensors, the challenges of correcting radial lens distortion, and the effects of image compression artifacts on fisheye camera visual perception tasks. In this work, a comprehensive study of fisheye image compression and perception using deep learning-based techniques is conducted. It is demonstrated that deep learning-based techniques achieve better compression performance and perceptual quality than conventional techniques, particularly at low bitrates crucial for automotive applications.},
  archive      = {J_NCA},
  author       = {Barakat, Basem and Sobh, Ibrahim and Wong, Chup-Chung and Islam, Muahmmad},
  doi          = {10.1007/s00521-024-10831-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5765-5780},
  shortjournal = {Neural Comput. Appl.},
  title        = {A comprehensive study of fisheye image compression and perception for autonomous driving},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting and assessing weak adhesion in structural single
lap joints using a machine learning pipeline with lamb waves data.
<em>NCA</em>, <em>37</em>(7), 5751–5764. (<a
href="https://doi.org/10.1007/s00521-024-10819-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adhesive joints are widely used in industries such as aerospace and automotive due to their lightweight and high mechanical performance. However, weak adhesion remains a significant issue affecting the structural integrity of these joints. Current detection methods of weak adhesion rely on destructive testing, which limits the widespread use of adhesive primary structures. This study proposes a novel nondestructive testing (NDT) technique to detect, evaluate the intensity, and localize weak adhesion in single lap joints (SLJs) using lamb waves (LWs) and machine learning (ML). The aim is to develop a ML-based pipeline capable of identifying weak adhesion with high accuracy and sensitivity, based on data from simulated and experimental SLJ samples. The proposed technique integrates LW data with convolutional neural networks (CNNs) in a ML pipeline for weak adhesion detection in SLJs. The use of a large simulated dataset combined with transfer learning allows for effective adaptation to experimental conditions, improving both the detection and localization of damage. This approach offers a significant advancement over traditional destructive testing techniques. The pipeline begins with the generation of simulated LW time-series data for SLJs with varying adhesion levels, damage locations, and sizes. After preprocessing, the data are input into a CNN, which is initially trained on synthetic data. Transfer learning is employed to fine-tune the model using a small experimental dataset. The final trained model is then applied to detect weak adhesion, estimate its intensity, and localize the damage. The proposed pipeline demonstrated high performance in both simulated and experimental datasets: regarding detection, the algorithm achieved over 95.3% accuracy in identifying damage from simulated data and near 100% detection of damaged cases in experimental data; for intensity estimation, the algorithm showed an average loss of approximately 45 MPa for weak adhesion intensity in experimental validation, with an average error of about 140 MPa and a best-case error of just near 3.6 MPa; in terms of localization, the average localization error was approximately 8 mm in the synthetic validation dataset; with respect to flexibility, the methodology is adaptable to different damage characteristics, such as existence, intensity, and localization, without requiring substantial modifications. Summing up, this study presents a novel NDT approach using ML and LW data that significantly improves the detection, evaluation, and localization of weak adhesion in adhesive joints. Its high accuracy and adaptability have the potential to enhance structural health monitoring, ensuring the safety and durability of bonded structures in critical industries.},
  archive      = {J_NCA},
  author       = {Ramalho, Gabriel M. F. and Lopes, António M. and da Silva, Lucas F. M.},
  doi          = {10.1007/s00521-024-10819-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5751-5764},
  shortjournal = {Neural Comput. Appl.},
  title        = {Detecting and assessing weak adhesion in structural single lap joints using a machine learning pipeline with lamb waves data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid meta-heuristic algorithm for optimization of
capuchin search algorithm for high-dimensional biological data
classification. <em>NCA</em>, <em>37</em>(7), 5719–5750. (<a
href="https://doi.org/10.1007/s00521-024-10815-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is a preprocessing technique that diminishes redundant and non-informative features to enhance data classification methods. This technique has gained global significance with the expansion of real-world data, particularly high-dimensional biological datasets. This study introduces a distinctive wrapper-based FS model, built upon the capuchin search algorithm (CapSA). CapSA is a recent swarm intelligence algorithm inspired by the foraging behaviors of Capuchin monkeys. Despite the strengths of the standard CapSA, it has notable limitations that this paper aims to address through purposeful modifications to the algorithm. These modifications include incorporating genetic algorithm operators (crossover and mutation), a dynamic mechanism for determining the number of leaders, and incorporating adaptive inertia weight. The proposed enhanced variant, named CapSA-CM, aims to achieve a more effective balance between the exploration and exploitation phases of the algorithm. The proposed methods are assessed using high-dimensional, low-sample biological datasets. The CapSA-CM approach is validated by comparing its efficacy with basic and hybrid meta-heuristic algorithms. Statistical analysis demonstrates the superiority of the CapSA-CM in terms of feature reduction, accuracy rates, and fitness values compared to the original CapSA and other comparable algorithms.},
  archive      = {J_NCA},
  author       = {Jaber, Iyad and Hassouneh, Yousef and Khemaja, Maha},
  doi          = {10.1007/s00521-024-10815-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5719-5750},
  shortjournal = {Neural Comput. Appl.},
  title        = {A hybrid meta-heuristic algorithm for optimization of capuchin search algorithm for high-dimensional biological data classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMU-trans: Imputing missing motion capture data with
unsupervised transformers. <em>NCA</em>, <em>37</em>(7), 5699–5717. (<a
href="https://doi.org/10.1007/s00521-024-10946-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion capture (mocap) systems are extensively utilized in healthcare for monitoring rehabilitation programs, facilitating clinical gait assessments for early Alzheimer’s diagnosis, managing walking disorders, and developing exoskeleton suits. However, like many other healthcare technologies, mocap systems have some flaws, like missing markers and occlusions. Given mocap data’s sequential and temporal nature, understanding marker relationships and capturing global dependencies are crucial for effective human motion recovery applications. To address these challenges, we proposed an unsupervised transformers framework for human motion recovery, called IMU-Trans. We evaluated our framework’s generalizability across two clinical datasets and tested its robustness by adjusting the missing marker rates, comparing its performance against low-dimensional Kalman filtering, long short-term memory (LSTM), and gated recurrent unit (GRU) models. Our experimental results demonstrated that IMU-Trans outperforms state-of-the-art models by training in an unsupervised manner. The closest competitor, GRU, demonstrated an RMSE of 1.35 ± 0.82, 2.36 ± 1.26, 3.43 ± 1.73, and 4.39 ± 2.18 cm for 20%, 30%, 40%, and 50% missing rates, respectively. IMU-Trans outperformed GRU with an RMSE of 1.26 ± 0.60, 2.06 ± 0.88, 2.68 ± 1.04, and 3.05 ± 1.22 for the same rates. Notably, our framework performs well even with higher missing data rates, creating opportunities for advancements in data analytics and indicating a promising future for motion capture in healthcare.},
  archive      = {J_NCA},
  author       = {Avdan, Goksu and Onal, Sinan and Lu, Chao},
  doi          = {10.1007/s00521-024-10946-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5699-5717},
  shortjournal = {Neural Comput. Appl.},
  title        = {IMU-trans: Imputing missing motion capture data with unsupervised transformers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigation variable-based multi-objective particle swarm
optimization for UAV path planning with kinematic constraints.
<em>NCA</em>, <em>37</em>(7), 5683–5697. (<a
href="https://doi.org/10.1007/s00521-024-10945-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is essential for unmanned aerial vehicles (UAVs) as it determines the path that the UAV needs to follow to complete a task. This work addresses this problem by introducing a new algorithm called navigation variable-based multi-objective particle swarm optimization (NMOPSO). It first models path planning as an optimization problem via the definition of a set of objective functions that include optimality and safety requirements for UAV operation. The NMOPSO is then used to minimize those functions through Pareto optimal solutions. The algorithm features a new path representation based on navigation variables to include kinematic constraints and exploit the maneuverable characteristics of the UAV. It also includes an adaptive mutation mechanism to enhance the diversity of the swarm for better solutions. Comparisons with various algorithms have been carried out to benchmark the proposed approach. The results indicate that the NMOPSO performs better than not only other particle swarm optimization variants but also other state-of-the-art multi-objective and meta-heuristic optimization algorithms. Experiments have also been conducted with real UAVs to confirm the validity of the approach for practical flights. The source code of the algorithm is available at https://github.com/ngandng/NMOPSO .},
  archive      = {J_NCA},
  author       = {Duong, Thi Thuy Ngan and Bui, Duy-Nam and Phung, Manh Duong},
  doi          = {10.1007/s00521-024-10945-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5683-5697},
  shortjournal = {Neural Comput. Appl.},
  title        = {Navigation variable-based multi-objective particle swarm optimization for UAV path planning with kinematic constraints},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Velocity control of a stephenson III six-bar linkage-based
gait rehabilitation robot using deep reinforcement learning.
<em>NCA</em>, <em>37</em>(7), 5671–5682. (<a
href="https://doi.org/10.1007/s00521-024-10944-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lower limb rehabilitation robots can help to improve the locomotor capabilities of patients experiencing gait impairments and help medical workers by reducing strain on them. However, since commercially available exoskeletons are expensive and there is a lack of number of physiotherapists many patients are still not able to get proper rehabilitation training. The closed-loop linkage mechanisms have recently drawn much attention in the realization of gait rehabilitation robots. Such mechanisms are affordable and capable of providing suitable trajectories for gait training therapy. In this work, we have proposed a fully operational one degree-of-freedom mechanism which can generate complex naturalistic lower limb trajectories. Although in theory, it is assumed that the constant speed applied at the input crank is sufficient to control the system, in reality, the external forces exerted by human legs and the inertia of the links can greatly alter the rotational velocity at the crank, which may negatively affect the training process. Therefore, we have explored the performance of a deep reinforcement learning-based control algorithm designed to regulate the speed of the input crank to reach satisfactory performance needed for gait rehabilitation training. Experimental evaluations with healthy human subjects were conducted to demonstrate that the mechanism is capable of directing lower limbs on naturalistic gait trajectories with a required walking speed.},
  archive      = {J_NCA},
  author       = {Kapsalyamov, Akim and Brown, Nicholas A. T. and Goecke, Roland and Jamwal, Prashant K. and Hussain, Shahid},
  doi          = {10.1007/s00521-024-10944-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5671-5682},
  shortjournal = {Neural Comput. Appl.},
  title        = {Velocity control of a stephenson III six-bar linkage-based gait rehabilitation robot using deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrusion detection system model: A white-box decision tree
with feature selection optimization. <em>NCA</em>, <em>37</em>(7),
5655–5670. (<a
href="https://doi.org/10.1007/s00521-024-10942-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection has been an active development area due to its importance in highly digitally connected ecosystems. Most of the existing developments have focused on the use of complex machine learning models that are black-box in nature. There is an urgent need to investigate a more transparent model approach for determining the features associated with intrusion detection. In this paper, a feature selection is proposed for a decision tree (DT)-based classifier. In particular, a stochastic optimization technique based on differential evolution (DE) is used to create the DT for optimizing feature selection. The contribution of this paper is twofold. First, a white-box machine learning model using DT is implemented. Second, an optimal feature reduction approach is embedded in the process of building the DT. The results demonstrate an improvement over the non-feature selection approach and the black-box neural network and are comparable to other state-of-the-art models. This shows that it is possible to achieve high performance despite using a minimal transparent model by eliminating non-contributing features. This is the essence of Occam’s razor principle, which states that a more condensed model contributes to better generalization. There is an evident improvement in the generalization of the DT model after optimization of features. Despite often being associated with a weaker machine learning model, the results show comparative results on independent datasets, indicating the suitability for such a task. It is worth mentioning that the final model only utilizes a fraction of the full feature set. Although the generalization performance only improved less than 1% in comparison with the non-feature selection counterpart, the proposed approach suggests that a condensed model yielding a similar performing model should be considered.},
  archive      = {J_NCA},
  author       = {Wong, W. K. and Juwono, Filbert H. and Eswaran, Sivaraman and Motelebi, Foad},
  doi          = {10.1007/s00521-024-10942-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5655-5670},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intrusion detection system model: A white-box decision tree with feature selection optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Course time scheduling problem for distance education
considering server load balancing: A case of an engineering faculty.
<em>NCA</em>, <em>37</em>(7), 5635–5653. (<a
href="https://doi.org/10.1007/s00521-024-10941-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the Covid-19 pandemic and mass disaster, universities have had to continue their courses with distance education. Internet servers in educational institutions slowed down for some periods, and courses could not be processed. The interruptions are due to the intensity of users on the servers and the internet congestion in the country during some time periods. For these reasons, the course time scheduling problem for distance education is discussed in this study. This study aims to balance the number of users in the system to prevent internet congestion. To solve the problem, two different mathematical models are developed. The first model obtains the balanced course scheduling for any time period. The second model is provided by the course assignment, considering internet congestion that occurred during any period. The proposed models were tested in a real case study dealing with the charting of courses offered in all departments of the engineering faculty of a university in Turkey. A problem-specific heuristic model is developed to solve the problem since a solution could not be obtained from the mathematical models in polynomial time due to the large size of the actual data set. The comparative results obtained from mathematical models and problem-specific heuristics are reported, and their performance is discussed. According to the comparative results, problem-specific heuristic outperforms mathematical models in obtaining a balanced schedule and solution time.},
  archive      = {J_NCA},
  author       = {Alakaş, Hacı Mehmet and Pınarbaşı, Mehmet and Sarımehmet, Bedirhan and Eren, Tamer},
  doi          = {10.1007/s00521-024-10941-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5635-5653},
  shortjournal = {Neural Comput. Appl.},
  title        = {Course time scheduling problem for distance education considering server load balancing: A case of an engineering faculty},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Radial basis function network-based optimization of the hard
self-propelled rotary turning titanium. <em>NCA</em>, <em>37</em>(7),
5607–5634. (<a
href="https://doi.org/10.1007/s00521-024-10940-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machining of difficult-to-cut materials such as high-temperature metals is challenging due to their low machinability resulting in reduced productivity and high manufacturing cost. This investigation develops and optimizes the hard self-propelled rotary turning (HSPRT) operation, in which an efficient self-propelled rotary tool is proposed and fabricated. The HSPRT responses (total carbon emission—TC, machined roughness—MR, and noise emission—EN) are minimized using optimal process variables (inclination angle—A, turning depth—D, turning speed—V, and rake angle—R). The TC, MR, and EN models are developed in terms of the HSPRT inputs using the radial basis function network and response surface method, while the weights were computed using the removal effects of criteria, EQUAL, and rank order centroid methods. The improved quantum-behaved particle swarm optimization algorithm and method based on the multi-attributive border approximation area comparison were applied to produce feasible solutions and select the best optimal point. The optimization findings of the V, D, f, and R were 32 deg., 0.2 mm, 137 m/min, and 20 deg., while the TC, MR, and EN were saved by 42.8%, 24.1%, and 20.0%, respectively. The HSPRT performances were primarily affected by the turning depth and speed, respectively. The valuable outcomes could be applied to the practice to boost HSPRT performances, while the developed HSPRT operation could be utilized for machining alloys and hardened steels. The technique could be applied to treat optimization problems for other rotary turning processes.},
  archive      = {J_NCA},
  author       = {Nguyen, Trung-Thanh and Dang, Xuan-Ba},
  doi          = {10.1007/s00521-024-10940-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5607-5634},
  shortjournal = {Neural Comput. Appl.},
  title        = {Radial basis function network-based optimization of the hard self-propelled rotary turning titanium},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite-time fault-tolerant control of attitude control
system of quadrotor UAV based on neural network disturbance observer.
<em>NCA</em>, <em>37</em>(7), 5597–5606. (<a
href="https://doi.org/10.1007/s00521-024-10927-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a new finite-time fault-tolerant control scheme for a quadrotor unmanned aerial vehicle (UAV). Firstly, a novel neural network disturbance observer with an auxiliary system is designed to compensate for actuator faults and external disturbances. In addition, in order to ensure the system’s rapidity, the finite-time theory is incorporated into the observer design and the controller design. The hyperbolic tangent function is also introduced to process the input signals, which ensures that the UAV receives relatively the smooth input signals. The design scheme takes into account the rapidity and robustness of the system, which makes the performance of the UAV better. Finally, the advantages of the proposed scheme are demonstrated through comparative experiments and the feasibility of the scheme is further verified through actual physical experiments.},
  archive      = {J_NCA},
  author       = {Li, Boning and Chen, Ming and Qi, Shuchang and Peng, Kaixiang},
  doi          = {10.1007/s00521-024-10927-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5597-5606},
  shortjournal = {Neural Comput. Appl.},
  title        = {Finite-time fault-tolerant control of attitude control system of quadrotor UAV based on neural network disturbance observer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepCancer: Deep learning for brain tumor detection-based
application system. <em>NCA</em>, <em>37</em>(7), 5577–5596. (<a
href="https://doi.org/10.1007/s00521-024-10926-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain tumor is the abnormal cells that growth in the brain, and it is considered as one of the most dangerous diseases that lead to the cause of death. Diagnosis at early is important for increasing the survival rate from the brain tumors. Specialists can identify the tumors manually, but it is very time and effort consuming, and are subject to human error, especially when dealing with large amounts of images. The automatic identification algorithms-based applications can facilitate the process. This study aimed to investigate the possibility of detecting brain cancer based on images using Deep Learning (DL) techniques and statistical operations. The features were extracted using two models of Convolutional Neural Network (CNN), (VGG-19 and AlexNet), then they were used to generate new datasets for statistical operations. CNN is used to extract features with distinct details from brain MRI images. The data were trained in three different training–testing data splitting ratios. Then, the features were classified based on the KNN, RF, and SVM to find the best accuracy of brain MRI image. At the end, the obtained classification accuracy was in favor of statistical operations especially for Large-Value, and Merge between features using KNN (99.1) and SVM (99.1). The features that extracted used in this study can provide high influence on the classification accuracy. The results across all three training–testing data splitting ratios were almost similar, and this approves that the brain cancer can be identified with high accuracy even if the training dataset sizes were minimal.},
  archive      = {J_NCA},
  author       = {AlShowarah, Suleyman A.},
  doi          = {10.1007/s00521-024-10926-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5577-5596},
  shortjournal = {Neural Comput. Appl.},
  title        = {DeepCancer: Deep learning for brain tumor detection-based application system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hmltnet: Multi-modal fake news detection via hierarchical
multi-grained features fused with global latent topic. <em>NCA</em>,
<em>37</em>(7), 5559–5575. (<a
href="https://doi.org/10.1007/s00521-024-10924-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the adverse impact of fake news, especially multi-modal fake news, on public decision-making and social governance, multi-modal fake news detection has lately attracted increasing attention. However, many existing methods ultimately exploit multi-modal features without detail fusion to complete detection and insufficiently consider the intrinsic features in news content, resulting in poor performance. To tackle these issues, we propose a network for multi-modal fake news detection that uses hierarchical multi-grained features fused with global latent topic (HMLTNet). Specifically, we first construct Hierarchical Multi-grained Encoding Module to capture convolutional and hierarchical textual features. Then, Cross-modal Shared Attention Module completes detail compensation in the multi-modal features by fusing textual and visual features and jointly modeling inter- and intra-modality correlations. Finally, the global latent topic features are excavated and stocked from multi-modal features by utilizing Latent Topic Memory Module. Furthermore, we design an Enhanced Similarity Module and introduce a dense-like strategy together to alleviate the adverse effects of cross-modal semantic gap. Extensive experiments on three public datasets indicate that the presented network reaches the best accuracy compared to state-of-the-art methods.},
  archive      = {J_NCA},
  author       = {Cui, Shaoguo and Gong, Linfeng and Li, Tiansong},
  doi          = {10.1007/s00521-024-10924-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5559-5575},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hmltnet: Multi-modal fake news detection via hierarchical multi-grained features fused with global latent topic},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified u-net with attention gate for enhanced automated
brain tumor segmentation. <em>NCA</em>, <em>37</em>(7), 5521–5558. (<a
href="https://doi.org/10.1007/s00521-024-10919-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the formidable challenges encountered in automated brain tumor segmentation, including the complexities of irregular shapes, ambiguous boundaries, and intensity variations across MRI modalities. Manual segmentation, plagued by subjectivity and time constraints, further exacerbates the problem. To address these issues, we propose a modified U-Net architecture with an integrated attention gate. The proposed model demonstrates high performance, with notable Dice Similarity Coefficient (DSC) and Jaccard Index (JI) values across various tumor classes, consistently exceeding 0.93 and 0.87, respectively. Incorporating Contrast-Limited Adaptive Histogram Equalization and Histogram Equalization improves segmentation accuracy, particularly in cases of Meningioma. Comparative analyses against established models reveal a DSC of 0.9521 and a JI of 0.9093, underscoring the superiority of our method. Validation in the BraTS 2021 dataset underscores the robustness of the method, achieving high DSC and JI scores in four MRI modalities, with the T2 modality demonstrating the highest performance (DSC: 0.9216, JI: 0.8556). While acknowledging these achievements, we recognize challenges related to dataset specificity and computational intensity associated with the attention gate. Future research efforts should address these issues to improve the generalizability and applicability of the method in real-world scenarios. In addition to presenting a novel automated brain tumor segmentation method, this study contributes comprehensive result values and comparative analyses with previous research, providing valuable insights into the evolving landscape of medical image analysis.},
  archive      = {J_NCA},
  author       = {Saifullah, Shoffan and Dreżewski, Rafał and Yudhana, Anton and Wielgosz, Maciej and Caesarendra, Wahyu},
  doi          = {10.1007/s00521-024-10919-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5521-5558},
  shortjournal = {Neural Comput. Appl.},
  title        = {Modified U-net with attention gate for enhanced automated brain tumor segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNA sequence design model for multi-scene fusion.
<em>NCA</em>, <em>37</em>(7), 5499–5520. (<a
href="https://doi.org/10.1007/s00521-024-10905-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its unique properties and excellent sequence design methods, DNA finds wide applications in computing, information storage, molecular circuits, and biological diagnosis. Previous efforts to enhance the efficiency and precision of DNA sequence design have led to the proposal of various universal DNA sequence design methods. These methods optimize the arrangement of the four bases to reduce sequence similarity and meet specific criteria. However, prior investigations have predominantly focused on sequence design within single-scene frameworks, overlooking the complexities associated with designing for multi-scene fusion, such as ion-bridge mismatch, tri-base sequence design, and others. To address this gap, we fused four common scenes and introduced two novel constraint models to facilitate DNA sequence design for multi-scene fusion. Additionally, we developed a dynamic virus spread algorithm as the core for optimizing DNA sequences and evaluated it using 23 well-known benchmark functions. Furthermore, our algorithm outperformed eight popular swarm evolutionary algorithms in eight dominant results. Finally, we simulated the optimization of four distinct scenes, demonstrating that our sequences met expected performance levels in their respective areas. Thus, our work provides a practical tool for designing DNA sequences tailored to various specific applications.},
  archive      = {J_NCA},
  author       = {Yao, Yao and Zheng, Yanfen and Cui, Shuang and Hou, Yaqing and Zhang, Qiang and Wei, Xiaopeng},
  doi          = {10.1007/s00521-024-10905-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5499-5520},
  shortjournal = {Neural Comput. Appl.},
  title        = {DNA sequence design model for multi-scene fusion},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient and scalable semi-supervised framework for
semantic segmentation. <em>NCA</em>, <em>37</em>(7), 5481–5497. (<a
href="https://doi.org/10.1007/s00521-024-10891-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has succeeded remarkably in various applications, such as autonomous vehicles and robotic systems. However, the training process for such techniques necessitates a significant amount of labeled data. Although semi-supervised frameworks can alleviate this issue, advanced approaches typically require multiple baseline models to form a dual model, which is costly in space computation complexity. To relieve the undesired computational cost for systems with precious computation and memory resources, we propose an efficient and scalable semi-supervised learning framework to significantly improve the performance with a few additional parameters concerning the baseline models. This framework includes a pseudo-dual module and a self-rectification module. The overall structure comprises three parts: an encoder, a shallow decoder, and a deep decoder. The deep decoder is connected to a deep layer of the encoder, and the shallow decoder is connected to a shallow layer. As knowledge distillation transfers knowledge from one model to another, the pseudo-dual module can distill knowledge from the ensemble of two decoders to improve the encoder, which can implicitly form a pseudo-dual model. The self-rectification module calculates class-wise likelihoods according to the similarity between features and class prototypes learned from different decoders and rectifies low-confidence pseudo-labels. The effectiveness of such rectification is justified theoretically and numerically. In our experiments with DeepLabV2, our methods outperform others in mIoU by over 1.21% with 1/8 labeled data using the Cityscapes dataset and by 0.38% with 1/8 labeled data using PASCAL VOC 2012 datasets. In most cases, our approach can also save more than 30% of memory costs during training. Nevertheless, the effectiveness of the proposed approach also depends on the quality of pseudo-labels generated by backbone models and may encounter challenges when handling data with highly imbalanced classes.},
  archive      = {J_NCA},
  author       = {Hao, Huazheng and Xiao, Hui and Xiong, Junjie and Dong, Li and Yan, Diqun and Liang, Dongtai and Zhuang, Jiayan and Peng, Chengbin},
  doi          = {10.1007/s00521-024-10891-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5481-5497},
  shortjournal = {Neural Comput. Appl.},
  title        = {An efficient and scalable semi-supervised framework for semantic segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGAN-CRCM: A novel multiple generative adversarial network
and coarse refinement-based cognizant method for image inpainting.
<em>NCA</em>, <em>37</em>(7), 5459–5480. (<a
href="https://doi.org/10.1007/s00521-024-10886-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is a recognized method for restoring the properties of pixels in damaged or incomplete images in computer vision technology. Some recent techniques based on generative adversarial network (GAN) image inpainting have outperformed traditional approaches due to their excellent deep learning capability and adaptability to various image domains. Since residual networks (ResNet) also gained popularity over time due to their property as a generative model, offering better feature representation and compatibility with other architectures, how could we leverage both of these models to result in even greater success in image inpainting? This paper proposes a novel architecture for image inpainting based on GAN and residual networks. Our proposed architecture consists of three models: Transpose Convolution-based GAN, Fast ResNet-Convolutional Neural Network, and Co-Modulation GAN. Transpose Convolution-based GAN is our newly designed architecture. It produces guided and blind image inpainting, and FR-CNN performs the object removal case. Co-Mod GAN acts as a refinement layer because it refines the results from Transpose Convolution-based GAN and FR-CNN. To train and evaluate our proposed architecture on publicly available benchmark datasets: CelebA, Places2, and ImageNet are used. Our approach proves our hypothesis, and our proposed model acquires the highest accuracy of 96.59% in the ImageNet dataset, FR-CNN acquires the highest accuracy of 96.70% in the Places2 dataset, and Co-Mod GAN acquires the highest accuracy of 96.16% in the CelebA dataset. Through an analysis of both qualitative and quantitative comparisons, it is evident that our proposed model exceeds existing architectures in performance.},
  archive      = {J_NCA},
  author       = {Asad, Nafiz Al and Pranto, Md. Appel Mahmud and Shiam, Shbiruzzaman and Akand, Musaddeq Mahmud and Yousuf, Mohammad Abu and Hasan, Khondokar Fida and Moni, Mohammad Ali},
  doi          = {10.1007/s00521-024-10886-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5459-5480},
  shortjournal = {Neural Comput. Appl.},
  title        = {MGAN-CRCM: A novel multiple generative adversarial network and coarse refinement-based cognizant method for image inpainting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Develop a novel, faster mask region-based convolutional
neural network model with leave-one-subject-out to predict freezing of
gait abnormalities of parkinson’s disease. <em>NCA</em>, <em>37</em>(7),
5441–5457. (<a
href="https://doi.org/10.1007/s00521-024-10832-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common symptom of severe Parkinson’s disease (PD) is Freezing of Gait (FoG), a gait disorder that causes sudden difficulty in initiating or maintaining walking. FoG frequently leads to falls and has a detrimental impact on a patient’s regular life. Real-time detection algorithms identify FoG occurrences using wearable sensors. Anticipating FoG in advance allows for pre-emptive cueing, which may prevent the episodes or reduce their severity and duration. This research proposes a Faster Mask Region-based Convolutional Neural Network (FMRCNN) signal processing approach for FoG identification. The model captured gyroscope, magnetometer, and tri-axial accelerometer signals using an inertial measurement device on the left side of the abdomen. The experimental results demonstrate a reduction in the equal error rate to 1.9% in the Leave-One-Subject-Out (LOSO) Cross-Validation (CV) with Long Short Term Memory (LSTM) assessment. Additionally, the tenfold CV evaluation enhances specificity and sensitivity by 0.045 and 0.017, respectively, compared to previous best results. It takes only 0.52 ms to detect a 256-data section. The proposed work uses the LOSO-CV-LSTM to evaluate various machine learning (ML) and deep learning (DL) techniques for FoG detection. The proposed system not only detects FoG but also enhances the automation of PD detection and therapy at an earlier stage. The results demonstrate that the proposed system improves performance measures compared to existing systems.},
  archive      = {J_NCA},
  author       = {Ezhilarasi, J. and Senthil Kumar, T.},
  doi          = {10.1007/s00521-024-10832-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5441-5457},
  shortjournal = {Neural Comput. Appl.},
  title        = {Develop a novel, faster mask region-based convolutional neural network model with leave-one-subject-out to predict freezing of gait abnormalities of parkinson’s disease},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-triggered adaptive fuzzy inverse optimal control of
steer-by-wire vehicle systems. <em>NCA</em>, <em>37</em>(7), 5429–5439.
(<a href="https://doi.org/10.1007/s00521-024-10768-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an event-triggered adaptive fuzzy inverse optimal control issue is investigated for the steer-by-wire vehicle (SBWV) systems. Firstly, fuzzy logic systems are adopted to model the unknown nonlinear dynamics and an auxiliary nonlinear system is established. Then, an event-triggered mechanism (ETM) is established to reduce the numbers of controller execution times. Subsequently, based on designed auxiliary nonlinear system and ETM, an event-triggered adaptive fuzzy inverse optimal control algorithm is proposed by employing the backstepping control technique. It is proved that the developed control method can ensure the stability of SBWV systems and the tracking error converges to the neighborhood of zero. Finally, the simulation results are provided to demonstrate the effectiveness of presented control approach.},
  archive      = {J_NCA},
  author       = {Zhang, Jiaming and Zuo, Yi and Tong, Shaocheng},
  doi          = {10.1007/s00521-024-10768-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5429-5439},
  shortjournal = {Neural Comput. Appl.},
  title        = {Event-triggered adaptive fuzzy inverse optimal control of steer-by-wire vehicle systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel w13 deep CNN structure for improved semantic
segmentation of multiple objects in remote sensing imagery.
<em>NCA</em>, <em>37</em>(7), 5397–5427. (<a
href="https://doi.org/10.1007/s00521-024-10765-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel convolutional neural network (CNN) architecture designed for semantic segmentation in remote sensing images. The proposed W13 Net model addresses the inherent challenges of segmentation tasks through a carefully crafted architecture, combining the strengths of multistage encoding–decoding, skip connections, combined weighted output, and concatenation techniques. Compared with different segmentation models, the suggested model performs better. A comprehensive analysis of different segmentation models has been carried out, resulting in an extensive comparison between the proposed W13 Net and five existing state-of-the-art segmentation architectures. Utilizing two standardized datasets, the Dense Labeling Remote Sensing Dataset Termed (DLRSD), and the Mohammad Bin Rashid Space Center (MBRSC) Dubai Aerial Imagery Dataset, the evaluation entails training, testing, and validation across different classes. The W13 Net demonstrates adaptability, generalization capabilities, and superior results in key metrics, all while displaying robustness across a variety of datasets. A number of metrics, including accuracy, precision, recall, F1 score, and IOU, were used to evaluate the system’s performance. According to the experimental results, the W13 Net model obtained an accuracy of 87.8%, precision of 0.88, recall of 0.88, F1 score of 0.88, and IOU of 0.74. The suggested model showed a significant improvement in segmentation IOU, with an increase of up to 18%, when compared to other with the recent segmentation models taking into consideration the model’s comparatively low number of parameter (2.2 million) in comparison with the recent models.},
  archive      = {J_NCA},
  author       = {Elgamily, Khaled Mohammed and Mohamed, M. A. and Abou-Taleb, Ahmed Mohamed and Ata, Mohamed Maher},
  doi          = {10.1007/s00521-024-10765-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5397-5427},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel w13 deep CNN structure for improved semantic segmentation of multiple objects in remote sensing imagery},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stress detection based EEG under varying cognitive tasks
using convolution neural network. <em>NCA</em>, <em>37</em>(7),
5381–5395. (<a
href="https://doi.org/10.1007/s00521-024-10737-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One tool for promoting mental health is human stress detection through multitasks of electroencephalography (EEG) recordings. This study proposed a short-term stress detection approach using VGGish as a feature extraction and convolution neural network (CNN) as a classifier based on EEG signals from the SAM 40 dataset. This database was recently available and was collected from 40 patients using 32 channels to identify performance on four tasks including Stroop color-word test (SCWT), answering arithmetic problems, finding mirror-identical images, and relaxing. Each task took 25 s to complete and was then repeated three times to record three trials. This means that the total EEG data contain 480 signals for four tasks recorded using 120 trials per task. The primary objective of this research was to track the amount of short-term stress that patients experienced while they engaged in the four mental tasks. Moreover, the VGGish-CNN model is applied to the SAM 40 dataset using five stages including signal preprocessing, segmentation, filtration, spectrogram, and classification process. We compared the VGGish-CNN model and the VGGish model for stress-based EEG classification to determine the best classification accuracy. The proposed approach for stress detection is the preliminary study that achieved an accuracy of 99.25% using the VGGish-CNN model on the SAM 40 dataset. Next, k-fold cross validation is performed to verify the efficiency of the VGGish-CNN model. This study can advance the application of brain–computer interface (BCI) and its use to identify patterns in EEG data that invoke stress-related inferences to aid in the diagnosis of mental disorders. In the future, investigation of human stress using EEG data will be useful in neurorehabilitation.},
  archive      = {J_NCA},
  author       = {Afify, Heba M. and Mohammed, Kamel K. and Hassanien, Aboul Ella},
  doi          = {10.1007/s00521-024-10737-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5381-5395},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stress detection based EEG under varying cognitive tasks using convolution neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust pointer meter reading method for inspection robots
in real industrial scenarios. <em>NCA</em>, <em>37</em>(7), 5369–5379.
(<a href="https://doi.org/10.1007/s00521-024-10682-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real industrial scenarios, inspection robots can replace humans to automatically read pointer meters, which greatly improves productivity and safety. However, existing automatic reading methods perform poorly in complex robot operating environments. To this end, we propose an automatic reading method for pointer meters, which can be better applied to inspection robot working conditions. Firstly, we propose a meter detection network, Yolo_Meter, which combines an attention mechanism and an adaptive feature fusion module. This network can accurately locate the meter from the perspective of robots and crop out images that are suitable for automatic meter readings. Secondly, we propose an oriented pointer detection network (OPDNet) to fit the tip position of the pointer precisely. Thirdly, we design a deep neural network OCR_Meter to obtain the scale and unit information of the meter by text detection and a filtering algorithm, which is adaptable to multiple types of meters. Finally, we propose a polar pixel method for locating the main scale lines and design the local angle method to calculate the readings of the pointer meters. Adequate experiments demonstrate the high accuracy and robustness of our method in real-world scenarios, with an average global error of only 0.73%.},
  archive      = {J_NCA},
  author       = {Huang, Zhiqing and Wang, Yuchao and Zhang, Yanxin and Zhang, Chenguang},
  doi          = {10.1007/s00521-024-10682-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5369-5379},
  shortjournal = {Neural Comput. Appl.},
  title        = {A robust pointer meter reading method for inspection robots in real industrial scenarios},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to predict the traffic accident assistance
based on deep learning. <em>NCA</em>, <em>37</em>(7), 5343–5368. (<a
href="https://doi.org/10.1007/s00521-024-10939-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the World Health Organization, thousands of people die every year in road traffic accidents. A crucial problem is the prediction of medical assistance in these accidents. For this purpose, we propose a new deep learning model whose goal is to distinguish whether a traffic accident requires medical assistance. The proposed perspective is general, so the model is valid for any dataset from any city. For this purpose, we present a model divided into three differentiated stages. In the first pre-processing stage, a general data treatment is performed, from data collection and cleaning to balancing. Secondly, the post-processing stage employs genetic and boosting algorithms to obtain the importance of all the data set variables used in the prediction. In the last stage, Model Training, a new model based on two-dimensional convolutional neural networks is applied to obtain a prediction of the need for medical assistance in traffic accidents. Finally, we test the effectiveness and accuracy of the proposed model by applying it to traffic accident datasets in six different cities. The obtained experimental results show that our framework achieves higher accuracy in all cities compared to six state-of-the-art models, confirming its suitability and applicability, even in real time.},
  archive      = {J_NCA},
  author       = {Vicent, José F. and Curado, Manuel and Oliver, José L. and Pérez-Sala, Luis},
  doi          = {10.1007/s00521-024-10939-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5343-5368},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel approach to predict the traffic accident assistance based on deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical prediction of ureter stone size using an
integrated CFD-ML approach. <em>NCA</em>, <em>37</em>(7), 5325–5341. (<a
href="https://doi.org/10.1007/s00521-024-10880-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ureteral flow parameters provide significant details about its physical attributes. Ureter is a single transport medium for urine transmission from kidney to ureter and its health is very important for a healthy human body. Understanding the fluid flow behavior can contribute toward the ureter health monitoring including estimation of any kind of blockage in the flow. Using ANSYS Fluent, Computational Fluid Dynamics (CFD) analysis and the grid independence study are carried out through iterative simulation process to achieve the solution independence. The CFD modeling provides tools and techniques to observe varying fluid parameters such as pressure, velocity and effect of the flow on smooth walls. Fluid Structure Interaction (FSI), an effective technique to analyze the effects of such flows on the ureter walls is also employed. Although the exact modeling of the ureter wall is not possible due to its complex physical parameters, some of its available physiological properties can be used to visualize the model of the ureter numerically. The present study is intended to predict the ureter stone size by using the FSI analysis. The simulations are carried out by increasing the stone size gradually from 1.7 to 3.4 mm and the input flow parameters are compared with the output flow parameters within the same solution setup and boundary conditions via artificial neural network in MATLAB. The output results obtained from the FSI simulations are then utilized to generate a prediction model for the ureter stone size. It is observed that the increasing stone size has a significant effect on the ureter wall, causing high stress regions in the point of interaction. The findings also revealed that the predicted size of the ureter stone is the closest to the actual size and with the least mean squared error at 80 optimal neurons.},
  archive      = {J_NCA},
  author       = {Ashraf, Muhammad Mubashar and Kamal, Khurram and Fahad, Muhammad and Noor, N. F. M. and Ratlamwala, Tahir Abdul Hussain},
  doi          = {10.1007/s00521-024-10880-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5325-5341},
  shortjournal = {Neural Comput. Appl.},
  title        = {Numerical prediction of ureter stone size using an integrated CFD-ML approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-pass end-to-end neural decompilation using copying
mechanism. <em>NCA</em>, <em>37</em>(7), 5309–5323. (<a
href="https://doi.org/10.1007/s00521-024-10735-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional decompilers utilize countless hardcoded rules written by subject matter experts, making them inflexible. Some recent systems address this using deep learning. The current consensus is that these systems have to include considerable domain knowledge and iterative heuristic components to solve parts of the decompilation problem, particularly the problem of predicting identifiers and literals. In this paper, we present a single-pass end-to-end neural decompilation system that utilizes copying mechanism. The copying mechanism is able to copy the literals and (offsets of) variables directly from the assembly code, in a single step, as part of the single forward pass through the model. Additionally, we take a further step toward decompiling real-world code by addressing important programming constructs like switch statements, function definitions, and function calls. We compile a dataset of real-world programming competition code and evaluate our model on it. The method achieves a program accuracy of 73% on the hardest complexity level of our generated dataset and 51% on the real-world examples without any additional error correction (EC) techniques, which surpasses the results of previous works without EC.},
  archive      = {J_NCA},
  author       = {Szalay, Gergő and Poór, Máté Bálint and Pintér, Balázs and Gregorics, Tibor},
  doi          = {10.1007/s00521-024-10735-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5309-5323},
  shortjournal = {Neural Comput. Appl.},
  title        = {Single-pass end-to-end neural decompilation using copying mechanism},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Driving behaviors analysis for public transport drivers in
kuwait: A machine learning approach to drivers safety. <em>NCA</em>,
<em>37</em>(7), 5289–5307. (<a
href="https://doi.org/10.1007/s00521-024-10964-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research paper addresses the critical concern of evaluating driving behaviors among bus drivers in Kuwait to enhance road safety and prevent accidents. Real driving data from 73 bus drivers working in Kuwait Public Transport Company (KPTC), collected through Teltonika devices, forms the basis of the quantitative analysis. The OPTICS (Ordering Points to Identify the Clustering Structure) algorithm and Expectation–Maximization (EM) clustering were employed using Gaussian Mixture Models (EM-GMM) to classify drivers into distinct behavioral categories. Correlation analyses were then conducted to pinpoint factors influencing risky driving. It was revealed that over speeding is the predominant contributor, accounting for 84.89% of unsafe behaviors. Predictive modeling is undertaken using Gradient Boosted Trees (GBT) and discriminant analysis, with GBT emerging as the most effective, achieving the highest accuracy. Risk indices for each driver cluster are calculated, showing that 28% of drivers exhibit unsafe practices. The probability of accidents for drivers with hazardous tendencies was determined to be 0.772, while the general likelihood of accidents among bus drivers in Kuwait is calculated at 0.318. Surprisingly, no significant correlation is found between age and driving behavior, highlighting the influence of factors such as psychological conditions, fatigue, weather, and road conditions on driving conduct. The findings contribute valuable insights for developing targeted interventions to mitigate risky driving behaviors and enhance overall road safety in the region.},
  archive      = {J_NCA},
  author       = {AlKheder, Sharaf and Al-Saleh, Hanaa},
  doi          = {10.1007/s00521-024-10964-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5289-5307},
  shortjournal = {Neural Comput. Appl.},
  title        = {Driving behaviors analysis for public transport drivers in kuwait: A machine learning approach to drivers safety},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chicken moth flame optimization and region-based convolution
neural network for water quality prediction. <em>NCA</em>,
<em>37</em>(7), 5271–5288. (<a
href="https://doi.org/10.1007/s00521-024-10878-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water is an important source for the sustenance of life, and its quality has a direct impact on the environment and public health. Water is utilized for various practices, such as agriculture, industry, and drinking. Geo-environmental pollution caused by various types of waste such as municipal, industrial, medical, solid, and agricultural fields makes the water unsuitable for usage. Water quality is primarily impacted by the discharge of agricultural and industrial effluents into the environment, which disrupts biological systems. Predicting water quality is crucial for environmental monitoring, ecosystem sustainability, and aquaculture. Accurate water quality prediction is essential for sustainable water management. Hence, the quality of the water should be maximized by managing water resources. In this research, an optimization-enabled deep learning model named chicken moth flame–region-based convolution neural network (CMF-RCNN) is introduced to predict water quality. Here, hidden properties of water are analyzed and utilized to predict the water quality characteristics. Moreover, input data are normalized using Z-score normalization and optimal features are selected via correlation analysis. Later, the water quality is predicted from the selected features using RCNN. The prediction performance of RCNN is enhanced by fine-tuning its weights by utilizing CMF. Moreover, the performance of CMF-RCNN is analyzed with respect to existing water quality prediction models, and the CMF-RCNN attained superior performance with precision of 0.927, recall of 0.946, and F1-score of 0.936, respectively.},
  archive      = {J_NCA},
  author       = {Jose, D. Justin and Sulochana, C. Helen},
  doi          = {10.1007/s00521-024-10878-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5271-5288},
  shortjournal = {Neural Comput. Appl.},
  title        = {Chicken moth flame optimization and region-based convolution neural network for water quality prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="npl---22">NPL - 22</h2>
<ul>
<li><details>
<summary>
(2025). HGBL: A fine granular hierarchical multi-label text
classification model. <em>NPL</em>, <em>57</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s11063-024-11713-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical multi-label text classification is vital for natural language processing (NLP). However, existing research rarely makes full use of the interaction between labels and text features that are crucial to hierarchical multi-label text classification. To address this issue, a novel model named hierarchy-guided BiLSTM guided contrastive learning classification (HGBL) is proposed, which successfully enhances the interaction between labels and text features by incorporating global context and embedding the idea of contrastive learning into this model. During modeling, Graphormer is adopted to model the dependencies between labels, and the bidirectional recurrent network (BiLSTM) is used to integrate global context including label features. Afterwards, the contrastive learning module embeds hierarchical awareness into the fine-tuned bidirectional encoder representations from transformers (BERT) by training the value of the loss. Experimental results on NYT, WOS and RCV1-V2 datasets show that HGBL exhibits significant competitive advantages compared with 19 competitors in terms of several indicators and can be used effectively for hierarchical multi-label text classification problems.},
  archive      = {J_NPL},
  author       = {Zhang, Chaoqun and Dai, Linlin and Liu, Chengxing and Zhang, Longhao},
  doi          = {10.1007/s11063-024-11713-x},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Neural Process. Lett.},
  title        = {HGBL: A fine granular hierarchical multi-label text classification model},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual learning in medicine: A systematic literature
review. <em>NPL</em>, <em>57</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11063-024-11709-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual Learning (CL) is a novel AI paradigm in which tasks and data are made available over time; thus, the trained model is computed on the basis of a stream of data. CL-based approaches are able to learn new skills and knowledge without forgetting the previous ones, with no guaranteed access to previously encountered data, and mitigating the so-called “catastrophic forgetting” phenomenon. Interestingly, by making AI systems able to learn and improve over time without the need for large amounts of new data or computational resources, CL can help at reducing the impact of computationally-expensive and energy-intensive activities; hence, CL can play a key role in the path towards more green AIs, enabling more efficient and sustainable uses of resources. In this work, we describe different methods proposed in the literature to solve CL tasks; we survey different applications, highlighting strengths and weaknesses, with a particular focus on the biomedical context. Furthermore, we discuss how to make the methods more robust and suitable for a wider range of applications.},
  archive      = {J_NPL},
  author       = {Bruno, Pierangela and Quarta, Alessandro and Calimeri, Francesco},
  doi          = {10.1007/s11063-024-11709-7},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Neural Process. Lett.},
  title        = {Continual learning in medicine: A systematic literature review},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LPM-net: A data-driven resource-efficient predictive motion
planner for mobile robots. <em>NPL</em>, <em>57</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11063-024-11671-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data-driven predictive motion planner for mobile robots, referred to as LPM-Net, has been proposed in this paper. Conventional predictive motion planners are computationally expensive, often resulting in insufficient throughput on mobile robot hardware. LPM-Net is an imitation learning-assisted local predictive non-holonomic motion planner that is capable of learning from conventional motion planners regarded as paradigm models and replicating their behavior while satisfying the same kinodynamic constraints. In addition, LPM-Net is compatible with GPU and TPU hardware, allowing for faster and more efficient processing. LPM-Net uses convolutional and recurrent long short-term memory deep neural networks to predict steering commands. This has improved computational efficiency which allows autonomous vehicles to be equipped with more cost-effective computers. In the present study, LPM-Net was tuned to mimic the behavior of a model predictive controller paradigm model. Measurements in this study demonstrate that the proposed mimic planner, LPM-Net, consumes approximately half the processing power of the conventional predictive planner, albeit with a slight increase in hesitation when reaching goals.},
  archive      = {J_NPL},
  author       = {Amirhosseini, Fakhreddin and Nilforoushan, Zahra and Leili Mirtaheri, Seyedeh},
  doi          = {10.1007/s11063-024-11671-4},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Neural Process. Lett.},
  title        = {LPM-net: A data-driven resource-efficient predictive motion planner for mobile robots},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LieCConv: An image classification algorithm based on lie
group convolutional neural network. <em>NPL</em>, <em>57</em>(1), 1–21.
(<a href="https://doi.org/10.1007/s11063-024-11691-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Lie group convolutional neural networks (LG-CNNs), the calculation and storage of Lie group distances have quadratic space complexity. In order to improve the memory utilization efficiency of LG-CNNs, a novel Lie group convolutional neural network called LieCConv is proposed. LieCConv utilizes an innovative sampling algorithm and a linear space complexity calculation and storage approach for Lie group distances, substantially enhancing network memory efficiency. Firstly, LieCConv employs a novel sampling algorithm called array-neighborhood sampling (ANS) in the downsampling stage. ANS only requires neighborhood information to obtain an excellent sample set with a low threshold of use. The sample set generated by ANS reflects the distribution of the original set. Then, LieCConv adopts a batch calculation and storage scheme for Lie group distances, which effectively declines the space complexity of calculating and storing Lie group distances from quadratic complexity to linear complexity, reducing the memory consumption during training. Finally, the contrast between ANS and farthest point sampling was presented, demonstrating that ANS better captures the distribution characteristics of the original dataset. The memory usage of LieCConv and LieConv was compared, revealing that LieCConv reduces the memory usage for calculating and storing Lie group distances to less than 500 MB. And the performance of LieCConv was evaluated on RotMNIST, RotFashionMNIST and TT100K, validating that LieCConv is universal and effective.},
  archive      = {J_NPL},
  author       = {Zhang, Yunjie and Luo, Xizhao and Tao, Chongben and Qin, Bo and Yang, Anjia and Cao, Feng},
  doi          = {10.1007/s11063-024-11691-0},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Neural Process. Lett.},
  title        = {LieCConv: An image classification algorithm based on lie group convolutional neural network},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harborfront anomaly detection. <em>NPL</em>, <em>57</em>(1),
1–16. (<a href="https://doi.org/10.1007/s11063-024-11696-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating high-quality datasets for the task of video anomaly detection is challenging due to a subjective anomaly definition and the rarity of anomalies, which oust the possibility of obtaining statistically significant data. This results in datasets where anomalies are placed in a single category, and are often considered less relevant from a security standpoint. Instead, we propose to create video anomaly datasets based on a framework utilizing object annotations to ease the annotation process and allow users to decide on the anomaly definition. Furthermore, this allows for a fine-grained evaluation w.r.t. anomaly types, which represents a novelty in the area of video anomaly detection. The framework is demonstrated using the existing thermal long-term drift (LTD) dataset, identifying and evaluating five different types of anomalies (appearance, motion, localization, density, and tampering) on six test sets. State-of-the-art anomaly detection methods are evaluated and found to underperform on the thermal anomaly detection dataset, which emphasizes a need for an adjustable anomaly definition in order to produce better anomaly datasets and models that generalize towards practical use. We share the code of the proposed framework to extract anomaly types along with object annotations for the LTD dataset at https://github.com/jagob/harborfront-vad .},
  archive      = {J_NPL},
  author       = {Dueholm, Jacob V. and Siemon, Mia and Ionescu, Radu T. and Moeslund, Thomas B. and Nasrollahi, Kamal},
  doi          = {10.1007/s11063-024-11696-9},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Neural Process. Lett.},
  title        = {Harborfront anomaly detection},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring dual coupledness for effective pruning in object
detection. <em>NPL</em>, <em>57</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11063-024-11697-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning offers an efficient approach to compressing models deployed on resource-constrained devices. In this paper, we introduce a novel method called Dual-Coupledness Object Detection Pruning (DCODP), specifically designed for object detection models. Taking into account the complexity of model coupling, our algorithm utilizes a depth-first search approach to identify interlayer coupling within the model. It then groups sublayers with the same parent layer together. Filters corresponding to feature maps with strong coupling are pruned within the layer, and the same pruning operation is applied to the corresponding indices in other coupled layers. In order to prove the validity of our method, extensive experiments are conducted on PASCAL VOC2007, PASCAL VOC2012 and MS COCO2017. The results show that our DCODP achieves a significant reduction of 50% in parameters and an average of more than 70% impressive score.},
  archive      = {J_NPL},
  author       = {Xiaohui, Guan and Wenzhuo, Huang and Yaguan, Qian and Xinxin, Sun},
  doi          = {10.1007/s11063-024-11697-8},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Neural Process. Lett.},
  title        = {Exploring dual coupledness for effective pruning in object detection},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fixed-time synchronization of caputo/conformable
fractional-order inertial cohen-grossberg neural networks via
event-triggered one/two-phase hybrid impulsive control. <em>NPL</em>,
<em>57</em>(1), 1–57. (<a
href="https://doi.org/10.1007/s11063-024-11703-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the fixed-time synchronization (FXS) problems of Caputo/conformable fractional-order inertial Cohen-Grossberg neural networks (FOICGNNs) by one/two-phase hybrid impulsive control (HIC) through event-triggered update strategies. By utilizing the properties of fractional calculus, several novel inequalities regarding the fixed-time convergence of hybrid impulsive systems (HIS) are obtained. We especially discuss and compare the cases of Caputo and conformable fractional order to gain deep insight into fractional calculus. By applying the Lyapunov stability theory, two hybrid controllers, which consist of event-triggered continuous controllers and impulsive controllers, are designed to realize the FXS of FOICGNNs. It’s worth pointing out that, we unprecedentedly study and compare the differences of the one-phase HIC and two-phase HIC, where a novel nonlinear impulsive controller is proposed and designed to obtain fixed-time convergence in the impulsive control phase. In addition, the exclusion of Zeno behavior is proved for the designed event-triggered strategy. Finally, several numerical examples are provided to illustrate the feasibility of the proposed control approach and the correctness of the theoretical results.},
  archive      = {J_NPL},
  author       = {Xiong, Yao and Li, Yesheng and Lv, Haifei and Wu, Wei and Xie, Songhua and Chen, Mengwei and Hu, Changkui and Li, Min},
  doi          = {10.1007/s11063-024-11703-z},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-57},
  shortjournal = {Neural Process. Lett.},
  title        = {Fixed-time synchronization of Caputo/Conformable fractional-order inertial cohen-grossberg neural networks via event-triggered One/Two-phase hybrid impulsive control},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network clustering for multi-task learning. <em>NPL</em>,
<em>57</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11063-024-11712-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multi-Task Learning (MTL) technique has been widely studied by worldwide researchers. The majority of current MTL studies adopt the hard parameter sharing structure, where hard layers tend to learn general representations over all tasks and specific layers are prone to learn specific representations for each task. Since the specific layers directly follow the hard layers, the MTL model needs to estimate this direct change (from general to specific) as well. To alleviate this problem, we introduce the novel cluster layer, which groups tasks into clusters during training procedures. In a cluster layer, the tasks in the same cluster are further required to share the same network. By this way, the cluster layer produces the general presentation for the same cluster, while produces relatively specific presentations for different clusters. The cluster layers are used as transitions between the hard layers and the specific layers. Thus, the MTL model can learn general representations to specific representations gradually. We evaluate our model with MTL document classification, and the results demonstrate the cluster layer is quite efficient in MTL.},
  archive      = {J_NPL},
  author       = {Mu, Zhiying and Gao, Dehong and Guo, Sensen},
  doi          = {10.1007/s11063-024-11712-y},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Neural Process. Lett.},
  title        = {Network clustering for multi-task learning},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly learning type-aware relations and inter-aspect with
graph convolutional networks for aspect sentiment analysis.
<em>NPL</em>, <em>57</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11063-024-11715-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for aspect-level sentiment analysis by leveraging the relationships between dependent types and aspects. The proposed method involves simplifying the type-aware graph convolutional network and designing a graph convolution module specifically for extracting relations between aspect words. The process begins with constructing an ordinary dependency graph for each sentence using a dependency tree. This graph is then refined by considering syntactic dependencies between context words and aspect-specific words, resulting in an aspect-focused graph. The aspect-focused graph, along with the corresponding embedding matrices, is fed into the aspect-focused GCN to capture the essential aspects and context words. Moreover, an inter-aspect GCN is employed to extract the dependencies between aspect words and other aspect words, utilizing the representations learned by the focused aspect GCN based on the inter-aspect graph. The L-layer of the GCN incorporates a bidirectional attentional mechanism to extract interrelationships, thus enhancing sentiment polarity judgment. Through interactive learning of aspect-specific affective features, the model acquires an understanding of the relationships between important text and aspect words, as well as the relationships among aspect words. Experimental results on five benchmark datasets demonstrate the superior performance of our proposed method compared to state-of-the-art approaches, exhibiting a significant improvement over the regular GCN model.},
  archive      = {J_NPL},
  author       = {Zong, Liansong and Hu, Dongfeng and Gui, Qingchi and Zhang, Pengfei and Wang, Jie},
  doi          = {10.1007/s11063-024-11715-9},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Neural Process. Lett.},
  title        = {Jointly learning type-aware relations and inter-aspect with graph convolutional networks for aspect sentiment analysis},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic evaluation of english translation based on
multi-granularity interaction fusion. <em>NPL</em>, <em>57</em>(1),
1–17. (<a href="https://doi.org/10.1007/s11063-025-11716-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest neural machine translation automatic evaluation method uses pre-trained context word vectors to extract semantic features and directly concatenates them into the neural network to predict translation quality. However, the direct operation can easily lead to a lack of interaction between features, and the layer-by-layer prediction is prone to losing fine-grained matching information. To address these issues, we propose a multi-granularity interactive fusion English translation automatic evaluation, which introduces middle and late information fusion methods. First, we use a bilinear attention distribution to capture high-order cross language feature interactions. By stacking multiple high-order interaction blocks and equipping them with an index linear unit without parameters for middle fusion in a parameter-free manner. Second, we use fine-grained accurate matching sentence shift distance and sentence-level cosine similarity for late fusion. The experimental results on the WMT’21 Metrics Task benchmark dataset show that the proposed method can effectively improve its correlation with human evaluation and achieve comparable performance with the best participating system.},
  archive      = {J_NPL},
  author       = {Chen, Xibo and Yang, Yonghe and Hu, Haize},
  doi          = {10.1007/s11063-025-11716-2},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Neural Process. Lett.},
  title        = {Automatic evaluation of english translation based on multi-granularity interaction fusion},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature enhancement-based few-shot bearing surface defect
image classification method. <em>NPL</em>, <em>57</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11063-025-11720-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of intelligent bearing surface defect classification based on deep neural networks remains challenging in real factories, due to the scarcity of defect samples. Under real-working conditions, with less training samples, the paper proposes a few-shot bearing defect image classification network which can recognize different bearing surface defects image, including notch, reddish rust, scratching, incising, conformity, pitting and mill scale. Based on general metric learning neural network framework, a local feature extraction layer is designed, which calculates the auto-correlation vector of global feature in a sliding region to enhance detail features. Additionally, a similar feature attention module emphasizes the the regions of similarity between the query set and the class prototype center to overcome the influence of background noise on classification. To validate the effectiveness of the proposed network, comparative experiments were conducted using the benchmark dataset miniImageNet, achieving classification accuracies of 59% in the 5-way 1-shot setting and 76% in the 5-way 5-shot setting respectively. Furthermore, to assess its performance in a real-factory condition, a self-made dataset of bearing defects from a factory was employed. The proposed network achieved a remarkable classification accuracy of 88% in the 5-way 5-shot setting. These experimental results confirm the practical application value of our few-shot bearing surface defect image classification network, demonstrating its ability to accurately recognize various bearing defects with limited training samples.},
  archive      = {J_NPL},
  author       = {Cang, Yan and Zhang, Xuanshang},
  doi          = {10.1007/s11063-025-11720-6},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Neural Process. Lett.},
  title        = {Feature enhancement-based few-shot bearing surface defect image classification method},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional feature interaction for conversational
aspect-based quadruple sentiment analysis. <em>NPL</em>, <em>57</em>(1),
1–19. (<a href="https://doi.org/10.1007/s11063-025-11721-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational aspect-level quadruple sentiment analysis (DiaASQ) is proposed as a new task that aims to extract target-aspect-opinion-sentiment quadruples in dialogues. However, this task faces the problem of complex context matching and multiple utterance feature modeling, which creates difficulties in extracting quadruples from multiple intersecting utterances. To address this problem, this paper proposes a Multi-dimensional Dialogue Feature Interaction (MDFI) approach. This method models dialogue features through an interactive network structure to capture interactions between utterance features. The approach adds two layers of ResNet to achieve deep association fusion based on multi-head self-attention. It superimposes the associated features of replies, speakers, and dialogue threads layer by layer and enhances the capability of conversation representation through linear augmentation. Our model outperforms the DiaASQ benchmark model in global utterance, intra-utterance, and cross-utterance quadruple extraction. In particular, the ZH dataset shows an improvement of 7.42 in global utterance and 9.66 in cross-utterance.},
  archive      = {J_NPL},
  author       = {Zhao, Zhongyang and Zhang, Long and Zheng, Qiusheng and Zhang, Junshuai},
  doi          = {10.1007/s11063-025-11721-5},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Neural Process. Lett.},
  title        = {Multi-dimensional feature interaction for conversational aspect-based quadruple sentiment analysis},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Value creation for healthcare ecosystems through artificial
intelligence applied to physician-to-physician communication: A
systematic review. <em>NPL</em>, <em>57</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s11063-025-11725-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study reviews the role of artificial intelligence (AI) in enhancing healthcare through an analysis of physician-to-physician communication. It seeks to identify the best practices for extracting value from professional medical chats (PMCs) and assess the impact of AI on patient outcomes and healthcare systems, emphasizing the integration of ethical and responsible AI practices. We conducted an extensive systematic literature review using the Web of Science Core Collection. Searches encompassed English-language articles published between January 2019 and July 2023 using keywords related to AI, machine learning, natural language processing, and physician communication. Of the 247 articles screened, 13 met the inclusion criteria given their in-depth analysis of AI in healthcare communication, methodological soundness, and relevance to clinical outcomes. The review provides insights into interprofessional communication dynamics, the advancement of NLP and deep learning in medical dialogues, and strategies for effective human-machine collaboration. Ethical considerations and the need for transparency in AI applications are key to these central findings. This study highlights the untapped potential of physician-generated real-world data in creating value for healthcare ecosystems. It advocates for a multidisciplinary strategy encompassing communication, education, and collaboration to advance AI in healthcare responsibly. Moreover, it suggests that by combining existing techniques in the AI discipline, including neural networks, generative AI, and genetic algorithms, as well as keeping a “physician in the loop” when building AI systems, we can have a significant impact on healthcare delivery and medical research.},
  archive      = {J_NPL},
  author       = {Rubinstein, Beny and Matos, Sergio},
  doi          = {10.1007/s11063-025-11725-1},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Neural Process. Lett.},
  title        = {Value creation for healthcare ecosystems through artificial intelligence applied to physician-to-physician communication: A systematic review},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AUMEs: AU detection-based dual-stream multi-task 3DCNN for
micro-expression recognition. <em>NPL</em>, <em>57</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11063-025-11726-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions are brief, involuntary facial movements that can reveal real emotions. However, their short duration and low intensity pose a challenge for feature extraction and learning of neural networks. To overcome this challenge, we propose AUMEs, a 3DCNN-based multi-task learning framework that utilizes deep learning-based Lagrangian motion magnification and optical flow computation methods to enhance spatio-temporal features of micro-expressions, thus solving the problem of weak micro-expression motion intensity. AUMEs also use AU detection as a parallel task to improve the accuracy of micro-expression recognition by transferring knowledge from the AU detection task, and focal loss is utilized in model training to handle category imbalance in the micro-expression dataset. AUMEs achieve competitive results compared with existing SOTA methods on the CASMEII and SAMM datasets, achieving accuracy (Acc.) of 81.05% and 79.85%, UF1 score reaches 0.8880 and 0.7450 on the five-category task, and on the three-category UAR reached 89.02% and 75.86% and 0.8880 and 0.7450 for UF1. Furthermore, in both dataset analyses, the multi-task approach surpassed the single-task method across both the five-category and three-category classifications.},
  archive      = {J_NPL},
  author       = {Shi, Hu and Wang, Yanxia and Wang , Renjie and Liu, Dan},
  doi          = {10.1007/s11063-025-11726-0},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Neural Process. Lett.},
  title        = {AUMEs: AU detection-based dual-stream multi-task 3DCNN for micro-expression recognition},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot object detection based on global domain adaptation
strategy. <em>NPL</em>, <em>57</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11063-025-11727-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming to detect novel objects from only a few annotated samples, few-shot object detection (FSOD) has undergone remarkable development. Previous works rarely pay attention to the perspective of gradient propagation to optimize existing methods, therefore failing to make full use of information for novel objects in gradient propagation. We propose a method to solve this problem based on two-stage fine-tuning. A domain adaptation module with multi-constraints is used to promote the spread of gradients, a classification promotion network is used to improve the effect of classification, and a multi-path mask head is added to enrich RoI features. Experiments on PASCAL VOC and COCO datasets show that our model significantly raises the performance compared with previous methods (up to 1–5 $$\%$$ in average).},
  archive      = {J_NPL},
  author       = {Gong, Xiaolin and Cai, Youpeng and Wang, Jian and Liu, Daqing and Ma, Yongtao},
  doi          = {10.1007/s11063-025-11727-z},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Neural Process. Lett.},
  title        = {Few-shot object detection based on global domain adaptation strategy},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GrapHisto: A robust representation of graph-structured data
for graph convolutional networks. <em>NPL</em>, <em>57</em>(1), 1–27.
(<a href="https://doi.org/10.1007/s11063-025-11728-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning from graphs is an established branch of AI research motivated by the relevance of applications that involve graph-structured data. The most popular instance is the graph neural network (GNN). On the other hand, due to the promising results of deep learning models in the most diverse fields of application, several efforts have been made to replicate these successes when dealing with graphical data. A prominent specimen of the kind is the graph convolutional network (GCN). Along these lines, the paper propose a novel approach for processing graphs that exploits the capabilities of convolutional neural networks (CNNs) to learn from images. This is achieved by means of a new representation of graphs, called GrapHisto, that portrays graphs in the form of characteristic “pictures”. The GrapHisto is in the form of graph-specific, unique tensors encapsulating the graph topology and its features (i.e., the labels associated with vertexes and edges). This representation is fed to a CNN, and the resulting machine is termed GrapHisto-CNN. The paper provides some theoretical investigations of the properties of the approach, and proposes solutions to some practical issues. An experimental evaluation of the GrapHisto-CNN is reported, revolving around two setups: classification of synthetically-generated graphs, and molecule classification form the dataset QM9. The results show that the approach is effective and robust, and that it compares favorably with GNNs and GCNs.},
  archive      = {J_NPL},
  author       = {Benini, Marco and Bongini, Pietro and Trentin, Edmondo},
  doi          = {10.1007/s11063-025-11728-y},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Neural Process. Lett.},
  title        = {GrapHisto: A robust representation of graph-structured data for graph convolutional networks},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECTTLNER: An effective cross-task transferring learning
method for low-resource named entity recognition. <em>NPL</em>,
<em>57</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11063-025-11729-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition is a fundamental task in natural language processing that significantly impacts the performance of its downstream tasks. Cross-task transfer learning methods are more naturally suited for low-resource named entity recognition compared to cross-language and cross-domain transfer learning methods. Existing cross-task transfer learning methods improve the performance of the low-resource named entity recognition by leveraging relevant information from other auxiliary tasks, such as sentence-level and token-level information. However, these methods do not fully exploit token-level information of entities, leaving room for improvement in low-resource named entity recognition. To futher improve the performance of the low-resource named entity recognition, this paper proposes a simple and effective cross-task transfer learning method called ECTTLNER, which introduces Sentence Contains Entities, Sentence Entity Number, Token Is Entity, and Token Boundary Label prediction tasks into named entity recognition and performs multi-task learning together with the main sequence labeling task. Experimental results on three NER datasets demonstrate that ECTTLNER outperforms a set of state-of-the-art baseline models, and achieves more than a 2.6% improvement in F1-score over these baseline models, particularly in low-resource scenarios.},
  archive      = {J_NPL},
  author       = {Xu, Yiwu and Chen, Yun},
  doi          = {10.1007/s11063-025-11729-x},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Neural Process. Lett.},
  title        = {ECTTLNER: An effective cross-task transferring learning method for low-resource named entity recognition},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power analysis attacks on NVM crossbar-based neuromorphic
systems. <em>NPL</em>, <em>57</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11063-025-11730-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new adversarial attack strategy against neuromorphic systems using analysis of power consumption. Specifically, we show that neuromorphic designs based on non-volatile memory crossbars can leak important information about loss sensitivity in their power profile. Adversaries can use this information to craft evasion attacks even if they don’t know the dataset that the model was trained on. In our experiments, we show that these types of attacks are effective against both single-layer and multilayer neuromorphic implementations of neural networks, and they can be made query-efficient through Bayesian optimization. We also provide theoretical insights into the relationship between the loss sensitivity and the power consumption measurements, showing that, for single-layer networks, the correlation coefficient of these two metrics scales inversely with the square root of the input size. Finally, this paper proposes that low bitwidth quantization could be an effective defense strategy against the class of attacks discussed herein.},
  archive      = {J_NPL},
  author       = {Merkel, Cory and Su, Allen},
  doi          = {10.1007/s11063-025-11730-4},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Neural Process. Lett.},
  title        = {Power analysis attacks on NVM crossbar-based neuromorphic systems},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent emerging techniques in explainable artificial
intelligence to enhance the interpretable and understanding of AI models
for human. <em>NPL</em>, <em>57</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s11063-025-11732-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Explainable Artificial Intelligence (XAI) aim to bridge the gap between complex artificial intelligence (AI) models and human understanding, fostering trust and usability in AI systems. However, challenges persist in comprehensively interpreting these models, hindering their widespread adoption. This study addresses these challenges by exploring recently emerging techniques in XAI. The primary problem addressed is the lack of transparency and interpretability in AI models to humanity for institution-wide use, which undermines user trust and inhibits their integration into critical decision-making processes. Through an in-depth review, this study identifies the objectives of enhancing the interpretability of AI models and improving human understanding of their decision-making processes. Various methodological approaches, including post-hoc explanations, model transparency methods, and interactive visualization techniques, are investigated to elucidate AI model behaviours. We further present techniques and methods to make AI models more interpretable and understandable to humans including their strengths and weaknesses to demonstrate promising advancements in model interpretability, facilitating better comprehension of complex AI systems by humans. In addition, we provide the application of XAI in local use cases. Challenges, solutions, and open research directions were highlighted to clarify these compelling XAI utilization challenges. The implications of this research are profound, as enhanced interpretability fosters trust in AI systems across diverse applications, from healthcare to finance. By empowering users to understand and scrutinize AI decisions, these techniques pave the way for more responsible and accountable AI deployment.},
  archive      = {J_NPL},
  author       = {Mathew, Daniel Enemona and Ebem, Deborah Uzoamaka and Ikegwu, Anayo Chukwu and Ukeoma, Pamela Eberechukwu and Dibiaezue, Ngozi Fidelia},
  doi          = {10.1007/s11063-025-11732-2},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Neural Process. Lett.},
  title        = {Recent emerging techniques in explainable artificial intelligence to enhance the interpretable and understanding of AI models for human},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Master–slave finite-time synchronization of chaotic
fractional-order neural networks under hybrid sampled-data control: An
LMI approach. <em>NPL</em>, <em>57</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11063-025-11733-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a hybrid controller with a sampled data control is investigated to achieve finite-time master–slave synchronization of delayed fractional-order neural networks (DFONNs). A Lyapunov-Krasovskii functional is constructed to obtain the sufficient conditions that incorporate delay information. For the first time, the asymptotic stability of the error system is guaranteed in a finite-time using the inequality technique and a sampled-data hybrid controller. The obtained conditions are expressed via linear matrix inequality. Notably, the proposed approach outperforms existing methods, demonstrating improved results in a comparative analysis. An explicit formula is utilized to calculate the settling time, which is significantly influenced by the fractional order $$0&lt;\beta \le 1$$ . The superior performance of the proposed control method is evident, showcasing its effectiveness through numerical simulations and addressing the synchronization problem in DFONNs.},
  archive      = {J_NPL},
  author       = {Kiruthika, R. and Manivannan, A.},
  doi          = {10.1007/s11063-025-11733-1},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Neural Process. Lett.},
  title        = {Master–Slave finite-time synchronization of chaotic fractional-order neural networks under hybrid sampled-data control: An LMI approach},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parkinsons detection from gait time series classification
using modified metaheuristic optimized long short term memory.
<em>NPL</em>, <em>57</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s11063-025-11735-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative conditions are defined by the progressive deterioration and death of nerve cells in the core neural system. Most neurodegenerative conditions are not curable. While there have been significant improvements and techniques used to treat these diseases early diagnosis continues to play a crucial role in the entire approach. Conditions are often diagnosed only once they start negatively impacting the daily life of those affected. Early detection and timely preventative treatment can help improve patient subjective well-being. This study examines the application of a non-invasive gait analysis technique for the detection of Parkinson’s disease. Publicly available data collected from patients suffering from Parkinson’s along with control groups is utilized and combined with long-short-term neural networks to construct models capable of detecting signs on Parkinson’s disorder. However, because of the significant reliance of models on appropriate parameters selection, metaheuristic algorithms are used to fine tune the selection process, and a modified variation of the strongly founded PSO algorithm was proposed. Several contemporary optimizers are compared based on their ability to optimize model performance. This suggested approach achieved the superior outcomes with an accuracy of 89.92%. The constructed models have been evaluated to determine feature importance using game theory based methods.},
  archive      = {J_NPL},
  author       = {Markovic, Filip and Jovanovic, Luka and Spalevic, Petar and Kaljevic, Jelena and Zivkovic, Miodrag and Simic, Vladimir and Shaker, Hotefa and Bacanin, Nebojsa},
  doi          = {10.1007/s11063-025-11735-z},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Neural Process. Lett.},
  title        = {Parkinsons detection from gait time series classification using modified metaheuristic optimized long short term memory},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: LPM-net: A data-driven resource-efficient
predictive motion planner for mobile robots. <em>NPL</em>,
<em>57</em>(1), 1. (<a
href="https://doi.org/10.1007/s11063-025-11736-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NPL},
  author       = {Amirhosseini, Fakhreddin and Nilforoushan, Zahra and Mirtaheri, Seyedeh Leili},
  doi          = {10.1007/s11063-025-11736-y},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1},
  shortjournal = {Neural Process. Lett.},
  title        = {Correction: LPM-net: a data-driven resource-efficient predictive motion planner for mobile robots},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="paaa---38">PAAA - 38</h2>
<ul>
<li><details>
<summary>
(2025). Plant leaf image segmentation in natural scenes: A
multi-layer graph queries propagation approach. <em>PAAA</em>,
<em>28</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s10044-024-01380-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate leaf segmentation is crucial for optimizing plant recognition and enhancing leaf identification precision. However, leaf segmentation encounters challenges when working with images captured in natural scenes. These images often contain intricate backgrounds with soil artifacts, overlapping leaves, plant elements, shadows, and variations in lighting. To address these issues, we propose an approach for segmenting leaf images using a multi-layer graph-based propagation method. The process begins with spatial localization of the leaf, aiding in detecting the foreground template, describing the central area of the leaf. Subsequently, a multi-level decomposition of the image into homogeneous regions is accomplished to capture image details at different scales. We then construct a graph based on this structure, connecting each region to its neighbors with weighted edges based on shared areas or edges across different resolutions. This graph is used to rank regional similarities to the leaf by propagating ranking scores from the foreground template to the image boundaries. As a result, we obtain a saliency map, which is used to extract the leaf from its surroundings. Finally, the resulting binary mask is refined using random forests to achieve optimal separation between the leaf and the background. Experiments conducted on a widely used dataset demonstrate that our method outperforms several state-of-the-art segmentation methods.},
  archive      = {J_PAAA},
  author       = {Lyasmine, Adada and Idir, Filali and Samia, Bouzefrane},
  doi          = {10.1007/s10044-024-01380-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Plant leaf image segmentation in natural scenes: A multi-layer graph queries propagation approach},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A partitioning incremental algorithm using adaptive
mahalanobis fuzzy clustering and identifying the most appropriate
partition. <em>PAAA</em>, <em>28</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01360-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of determining the most appropriate number of clusters in a fuzzy Mahalanobis partition. First, a new fuzzy Mahalanobis incremental algorithm is constructed to search for an optimal fuzzy Mahalanobis partition with $$2,\,3,\ldots$$ clusters. Among these partitions, selecting the one with the most appropriate number of clusters is based on appropriately modified existing fuzzy indexes. In addition, the Fuzzy Mahalanobis Minimal Distance index is defined as a natural extension of the recently proposed Mahalanobis Minimal Distance index for non-fuzzy clustering. The new fuzzy Mahalanobis incremental algorithm was tested on several artificial data sets and the color image segmentation problems from real-world applications: art images, nature photography images, and medical images. The algorithm includes multiple usage of the global optimization algorithm DIRECT. But unlike previously known fuzzy Mahalanobis indexes, the proposed Fuzzy Mahalanobis Minimal Distance index ensures accurate results even when applied to complex real-world applications. A possible disadvantage could be the need for longer CPU time. Furthermore, besides effective identification of the partition with the most appropriate number of clusters, it is shown how to use the proposed Fuzzy Mahalanobis Minimal Distance index to search for an acceptable partition, which proved particularly useful in the above-mentioned real-world applications.},
  archive      = {J_PAAA},
  author       = {Scitovski, Rudolf and Sabo, Kristian and Grahovac, Danijel and Martínez-Álvarez, Francisco and Ungar, Sime},
  doi          = {10.1007/s10044-024-01360-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A partitioning incremental algorithm using adaptive mahalanobis fuzzy clustering and identifying the most appropriate partition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive optimization of low rank decomposition and its
application on fabric defect detection. <em>PAAA</em>, <em>28</em>(1),
1–15. (<a href="https://doi.org/10.1007/s10044-024-01363-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications of fabric defect detection, low-rank decomposition is the effective method. Sparse matrices represent defect results, so sparse terms are the focus of this application. Because the characteristics of each observation matrix differ, the weight of sparse term also differ. Therefore, this paper proposes adaptive weight for the model, allowing it to find suitable weight for different observation matrices and thereby improving the accuracy of model. During the matrix separation process of the model, elements that should belong to the sparse matrix may be separated into the noise matrix. To address this, this paper establishes new constraints to achieve a deeper separation between the two. While establishing the corresponding algorithmic framework, this paper also considers the fluctuations in the model’s solution process and proposes a new definition for the penalty factors. This aims to improve algorithm efficiency and reduce CPU time. This paper also provides a convergence analysis of the proposed method. In the dataset of fabric defects, it was shown that the star and dot types had the best results in TPR and F-measure, with TPR of 85.15% and 81.56%, and f-measure of 70.51% and 65.40%, respectively. Indicating that the method proposed in this paper has the fastest calculation speed.},
  archive      = {J_PAAA},
  author       = {Shi, Wenya and Chen, Zhixiang and Liang, Jiuzhen and Jiang, Daihong},
  doi          = {10.1007/s10044-024-01363-z},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive optimization of low rank decomposition and its application on fabric defect detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse embeddings learning for multi-view clustering.
<em>PAAA</em>, <em>28</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01364-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which improves clustering performance by exploring complementarity and consistency among multiple distinct feature sets, is attracting more and more researchers due to its wide applications in various fields e.g., pattern recognition and data mining. Traditional approaches usually explore above characteristics by mapping different views to a unified embedding through view-specific mapping matrices or neural networks. Then the unified embedding is fed into conventional single view clustering algorithms for final clustering results. However, a unified embedding is not enough to model distinct or even conflict multiple view characteristics due to their diverse representation abilities. Moreover, clustering and embedding learning are divided into two separate parts, which may bring in a gap between the class label and the learned embedding. To alleviate above problems, both unified and view-specific embeddings are learned, and a shared operator tensor and view-specific latent variables are introduced for their relationship modeling. Besides, a Kullback-Liebler divergence based objective is developed as a clustering oriented constraint, which leads to more clustering friendly embedding learned. Extensive experiments are conducted on six widely used datasets, achieving better results compared with several state-of-the-art approaches.},
  archive      = {J_PAAA},
  author       = {Li, Yongzhen and Liao, Husheng},
  doi          = {10.1007/s10044-024-01364-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Diverse embeddings learning for multi-view clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SA-DETR: Saliency attention-based DETR for salient object
detection. <em>PAAA</em>, <em>28</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01379-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researches on the Salient Object Detection (SOD) task have made many advances based on deep learning methods. However, most methods have focused on predicting a fine mask rather than finding the most salient objects. Most datasets for the SOD task also focus on evaluating pixel-wise accuracy rather than “saliency”. In this study, we used the Salient Objects in Clutter (SOC) dataset to conduct research that focuses more on the saliency of objects. We propose a architecture that extends the cross-attention mechanism of Transformer to the DETR architecture to learn the relationship between the global image semantics and the objects. We extended module with Saliency Attention (SA) to the network, namely SA-DETR, to detect salient objects based on object-level saliency. Our proposed method with cross- and saliency-attentions shows superior results in detecting salient objects among multiple objects compared to other methods. We demonstrate the effectiveness of our proposed method by showing that it outperforms the state-of-the-art performance of the existing SOD method by 4.7% and 0.2% in MAE and mean E-measure, respectively.},
  archive      = {J_PAAA},
  author       = {Nam, Kwangwoon and Kim, Jeeheon and Kim, Heeyeon and Chung, Minyoung},
  doi          = {10.1007/s10044-024-01379-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {SA-DETR: Saliency attention-based DETR for salient object detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stgcn-pad: A spatial-temporal graph convolutional network
for detecting abnormal pedestrian motion patterns at grade crossings.
<em>PAAA</em>, <em>28</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01382-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Spatial-Temporal Graph Convolutional Network-based Pedestrians’ behaviors Anomaly Detection system (STGCN-PAD) for grade crossings. The behaviors of pedestrians are represented in a structured manner by skeleton trajectories that are generated using a pose estimation model. The ST-GCN components are sequentially applied to capture the spatial dependencies between skeleton key points within a single video frame and the temporal relationships for each of them. Based on these features, the system reconstructs input trajectories with a constant sliding window size, and the reconstruction error is used to distinguish abnormal behaviors from those normal. To accelerate the processing of extracted multi-dimensional feature maps, an MLP-Mixer model-based reconstruction network is developed as an alternative to the traditional convolution neural network. Only trajectories of normal walking behavior are included for model training. Anomalies, such as lingering and squatting activities, can be identified as outliers by observing the magnitude of reconstruction errors. The case studies demonstrate the salient feasibility and efficiency of the proposed system, which achieves at least comparable performance (approximately 88% in the AUC evaluation metric) with several state-of-the-art approaches while using the MLP-Mixer model accelerates model inference by 10× relative to our previous effort (Song et al. in Appl Intell 53:21676–21691, 2023).},
  archive      = {J_PAAA},
  author       = {Song, Ge and Qian, Yu and Wang, Yi},
  doi          = {10.1007/s10044-024-01382-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Stgcn-pad: A spatial-temporal graph convolutional network for detecting abnormal pedestrian motion patterns at grade crossings},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Appropriateness of distances in nearest neighbour
classification: A monometric perspective. <em>PAAA</em>, <em>28</em>(1),
1–23. (<a href="https://doi.org/10.1007/s10044-024-01373-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the non-parametric classification methods, the nearest neighbour classifier (NNC) holds a pre-eminent position. Given a training or sample set $${\mathcal {S}}$$ the choice one needs to make is on the value of k and the distance function d to be employed. Towards improving the efficacy of an NNC, there are many works—both theoretical and empirical—that help in choosing a suitable value of k. However, works that deal with the appropriateness of a distance d for a given $${\mathcal {S}}$$ are largely empirical. In this work, we address the following two posers for a given $${\mathcal {S}}$$ : (1) How to identify a potentially appropriate distance d? (2) What qualities should an appropriate d possess? Our investigations show that every distance function d determines a landscape on the underlying data space and only if the class boundaries align with this landscape can this d be appropriate. In view of this, we construct a relational graph $${\mathcal {G}}_{{\mathcal {S}},d}$$ , in fact, a poset, on the given $${\mathcal {S}}$$ using d. With the help of $${\mathcal {G}}_{{\mathcal {S}},d}$$ , we choose a $${\mathcal {T}} \subset {\mathcal {S}}$$ to be used in a condensed-NN algorithm. Terming it the NEN algorithm, firstly, we show empirically that the training error of this NEN algorithm is reflective of the appropriateness of d. Towards providing a theoretical justification to our claims based on empiricism, we investigate the problem of classification in the setting of monometric spaces, wherein it emerges that the suitability of d is essentially related to the embeddability of $${\mathcal {G}}_{{\mathcal {S}},d}$$ in the monometric space ( $${\mathcal {X}}, \preceq _d,d$$ ).},
  archive      = {J_PAAA},
  author       = {Gupta, Megha and Jayaram, Balasubramaniam},
  doi          = {10.1007/s10044-024-01373-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Appropriateness of distances in nearest neighbour classification: A monometric perspective},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class preserving projections and data augmentation for
appearance-based face recognition. <em>PAAA</em>, <em>28</em>(1), 1–12.
(<a href="https://doi.org/10.1007/s10044-024-01388-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer Vision and Biometrics benefit from the recent advances in Pattern Recognition and Artificial Intelligence, which tends to make model-based face recognition more efficient. Also, deep learning combined with data augmentation tends to enrich the training sets used for learning tasks. Nevertheless, face recognition still is challenging, especially because of imaging issues that occur in practice, such as changes in lighting, appearance, head posture and facial expression. In order to increase the reliability of face recognition, we propose a novel supervised appearance-based face recognition method which creates a low-dimensional orthogonal subspace that enforces the face class separability. The proposed approach uses data augmentation to mitigate the problem of training sample scarcity. Unlike most face recognition approaches, the proposed approach is capable of handling efficiently grayscale and color face images, as well as low and high-resolution face images. Moreover, proposed supervised method presents better class structure preservation than typical unsupervised approaches, and also provides better data preservation than typical supervised approaches as it obtains an orthogonal discriminating subspace that is not affected by the singularity problem that is common in such cases. Furthermore, a soft margins Support Vector Machine classifier is learnt in the low-dimensional subspace and tends to be robust to noise and outliers commonly found in practical face recognition. To validate the proposed method, an extensive set of face identification experiments was conducted on three challenging public face databases, comparing the proposed method with methods representative of the state-of-the-art. The proposed method tends to present higher recognition rates in all databases. In addition, the experiments suggest that data augmentation also plays an essential role in the appearance-based face recognition, and that the CIELAB color space (L*a*b) is generally more efficient than RGB for face recognition as it attenuates lighting variations.},
  archive      = {J_PAAA},
  author       = {Soldera, John and Scharcanski, Jacob},
  doi          = {10.1007/s10044-024-01388-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Class preserving projections and data augmentation for appearance-based face recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeFormer: Local patch-based edge detection transformer on
point clouds. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01386-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge points on 3D point clouds can clearly convey 3D geometry and surface characteristics, therefore, edge detection is widely used in many vision applications with high industrial and commercial demands. However, the fine-grained edge features are difficult to detect effectively as they are generally densely distributed or exhibit small-scale surface gradients. To address this issue, we present a learning-based edge detection network, named EdgeFormer, which mainly consists of two stages. Based on the observation that spatially neighboring points tend to exhibit high correlation, forming the local underlying surface, we convert the edge detection of the entire point cloud into a point classification based on local patches. Therefore, in the first stage, we construct local patch feature descriptors that describe the local neighborhood around each point. In the second stage, we classify each point by analyzing the local patch feature descriptors generated in the first stage. Due to the conversion of the point cloud into local patches, the proposed method can effectively extract the finer details. The experimental results show that our model demonstrates competitive performance compared to six baselines.},
  archive      = {J_PAAA},
  author       = {Xie, Yifei and Tu, Zhikun and Yang, Tong and Zhang, Yuhe and Zhou, Xinyu},
  doi          = {10.1007/s10044-024-01386-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {EdgeFormer: Local patch-based edge detection transformer on point clouds},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive dual-branch network for long-tailed visual
recognition. <em>PAAA</em>, <em>28</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01387-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, deep learning techniques have been widely applied to visual tasks, leading to remarkable breakthroughs. However, data in real-world scenarios often exhibit a long-tailed distribution, where head classes contain significantly more samples than tail classes. Models trained on such imbalanced data tend to bias the head classes, resulting in poor performance on tail classes. Moreover, due to the scarcity of samples in the tail classes, it is challenging for models to learn robust representations for those classes. Inspired by the success of contrastive learning in representation learning, we propose a Contrastive Dual-Branch Network (CDBN) for long-tailed visual recognition. CDBN integrates an imbalance learning branch and a contrastive learning branch to address the challenges of imbalanced data. The imbalance learning branch leverages traditional methods to address data imbalance, while the contrastive learning branch follows the principles of contrastive learning. Specifically, it uses two distinct data augmentation techniques to process the same batch of samples, generating positive sample pairs for enhanced learning. A contrastive auxiliary loss is then introduced to minimize the distance between these pairs in the normalized embedding space. Furthermore, we propose a Cumulative Fusion Strategy (CFS) to guide the model in progressively prioritizing tail classes throughout training. We conducted extensive experiments on the CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets and compared our method with various advanced algorithms. The results demonstrate that our method substantially enhances performance across all datasets, achieving state-of-the-art results on several benchmarks. Our code is available at https://github.com/mmzbyxx/CDBN .},
  archive      = {J_PAAA},
  author       = {Miao, Jie and Zhai, Junhai and Han, Ling},
  doi          = {10.1007/s10044-024-01387-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Contrastive dual-branch network for long-tailed visual recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting optimized forgery representation space for
general fake face detection. <em>PAAA</em>, <em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01391-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery has become more realistic with deep learning in computer vision, posing a significant challenge to trustworthy face identification. Existing works have achieved considerable accuracy within the dataset by formulating the detection as a binary classification problem. These methods attempt to amplify the category differences between real and fake faces but ignore the optimization of representation space for learning the specific forgery information within samples, which results in the intra-class distribution collapse and poor generalization in unseen domains. To mitigate this issue, we propose a novel forgery detection framework that combines contrastive learning with supervised learning, named Contrastive Learning Against face Forgery (CLAF). Specifically, a dual branch learning framework is involved in extracting the consistent forgery feature distribution first. Then, we consider the similarity, variance, and covariance constraint term for the representation space, which can better preserve the specific forgery information within each sample for generalization detection. The generalization performance is confirmed on FaceForensics++, Celeb-DF, and DFDC. Extensive experiment results demonstrate the effectiveness of our framework in improving generalization.},
  archive      = {J_PAAA},
  author       = {Yang, Gaoming and Zuo, Bang and Fang, Xianjin and Zhang, Ji},
  doi          = {10.1007/s10044-024-01391-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Exploiting optimized forgery representation space for general fake face detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving distantly supervised named entity recognition by
emphasizing uncertain examples. <em>PAAA</em>, <em>28</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01392-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distantly supervised named entity recognition (DS-NER) aims to acquire knowledge from noisy labels. Recently, label re-weighting and label correction based frameworks have been recognized as promising approaches for DS-NER. These methods mainly handle easy or hard examples, yet neglect the impact of uncertain examples that are predicted correctly sometimes and incorrectly some other times during optimization. In this paper, we propose UE-NER, an Uncertainty Estimation method for DS-NER, which estimates the uncertainty of training examples and emphasizes uncertain ones, thus leads to more accurate and robust performance. To enable uncertainty reasoning, we formulate DS-NER as a span-level classification problem and the variance in predicted probability of the correct class across iterations of minibatch SGD is taken as the uncertainty measure. We further design an enhanced encoder to combine the power of the named entity and other spans in the sentence to boost recognition performance. Experimental results on two benchmark datasets demonstrate the superiority of the proposed UE-NER over existing DS-NER methods.},
  archive      = {J_PAAA},
  author       = {Nie, Binling and Shao, Yiming and Wang, Yigang},
  doi          = {10.1007/s10044-024-01392-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Improving distantly supervised named entity recognition by emphasizing uncertain examples},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multi-target vehicle trajectory prediction based
on multi-scale graph convolution. <em>PAAA</em>, <em>28</em>(1), 1–17.
(<a href="https://doi.org/10.1007/s10044-024-01396-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target vehicle trajectory prediction holds significant importance in the field of autonomous driving. Accurate trajectory prediction can enhance the safety and efficiency of autonomous vehicles, reducing traffic accidents and congestion. However, existing methods often fall short when dealing with complex traffic scenarios. Traditional approaches typically rely on single-scale spatiotemporal feature extraction, which struggles to fully capture the complex dynamics of traffic across different temporal and spatial scales, especially in high-density traffic environments. To address these challenges, this thesis proposes a multi-target vehicle trajectory prediction method based on a Multi-Scale Graph Convolutional Network (MSGCN). This method integrates high-definition semantic maps and employs a spatiotemporal multi-head attention mechanism alongside an adaptive dynamic weighting module to achieve efficient multi-target vehicle trajectory prediction. Specifically, this thesis constructs a dynamic feature repository using vehicle subgraphs and lane subgraphs to stabilize model weight fluctuations, thereby more accurately reflecting actual traffic conditions. Experimental results on the Argoverse dataset demonstrate the effectiveness of our method. Specifically, our approach reduces the average displacement error (mADE) by 7% and enhances the final displacement error (mFDE) by 19% when compared to existing state-of-the-art models. Our code is made available at https://github.com/Garegreen/EfficientMSGCN .},
  archive      = {J_PAAA},
  author       = {Gu, Xiang and Wang, Jing and Cheng, Dengyang and Li, Chao and Huang, Qiwei},
  doi          = {10.1007/s10044-024-01396-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Efficient multi-target vehicle trajectory prediction based on multi-scale graph convolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning learning curves. <em>PAAA</em>, <em>28</em>(1),
1–13. (<a href="https://doi.org/10.1007/s10044-024-01394-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning curves depict how a model’s expected performance changes with varying training set sizes, unlike training curves, showing a gradient-based model’s performance with respect to training epochs. Extrapolating learning curves can be useful for determining the performance gain with additional data. Parametric functions, that assume monotone behaviour of the curves, are a prevalent methodology to model and extrapolate learning curves. However, learning curves do not necessarily follow a specific parametric shape: they can have peaks, dips, and zigzag patterns. These unconventional shapes can hinder the extrapolation performance of commonly used parametric curve-fitting models. In addition, the objective functions for fitting such parametric models are non-convex, making them initialization-dependent and brittle. In response to these challenges, we propose a convex, data-driven approach that extracts information from available learning curves to guide the extrapolation of another targeted learning curve. Our method achieves this through using a learning curve database. Using the initial segment of the observed curve, we determine a group of similar curves from the database and reduce the dimensionality via Functional Principle Component Analysis FPCA. These principal components are used in a semi-parametric kernel ridge regression (SPKR) model to extrapolate targeted curves. The solution of the SPKR can be obtained analytically and does not suffer from initialization issues. To evaluate our method, we create a new database of diverse learning curves that do not always adhere to typical parametric shapes. Our method performs better than parametric non-parametric learning curve-fitting methods on this database for the learning curve extrapolation task.},
  archive      = {J_PAAA},
  author       = {Turan, O. Taylan and Tax, David M. J. and Viering, Tom J. and Loog, Marco},
  doi          = {10.1007/s10044-024-01394-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning learning curves},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised learning with deep laplacian support vector
machine. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01395-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is a rapidly growing field that can effectively extract latent features from data and use them to make predictions based on the learned features, but most models just sum the loss of each sample without considering the relationship between samples. On the other hand, the traditional Laplacian Support Vector Machine (LapSVM) can effectively utilize samples and the relationship between samples by constructing a Laplacian graph, and performs well on semi-supervised data. In this paper, we combine LapSVM and deep learning and propose Deep Laplacian Support Vector Machine. Our approach is to first use a Deep Neural Network to extract the latent features from the image, then based on the extracted feature information and a small amount of original label information, we use LapSVM for classification, build a loss function, and finally iteratively update the two parts together. We evaluate our method on several benchmark datasets and demonstrate that it outperforms other semi-supervised learning methods.},
  archive      = {J_PAAA},
  author       = {Chen, Hangyu and Xie, Xijiong and Li, Di},
  doi          = {10.1007/s10044-024-01395-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Semi-supervised learning with deep laplacian support vector machine},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust approach for outlier detection based on the ratio
of number of reverse neighbors to neighbors. <em>PAAA</em>,
<em>28</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10044-024-01372-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is an important issue in data mining, which has a wide range of applications in medicine, economics, video search, and credit card fraud detection. Many outlier detection methods have recently been developed. Most of the existing methods act based on the distance or density. Since each of these methods has its inherent disadvantage, we proposed a method which has the advantages of both distance-based and density-based methods. The proposed method is inspired by the basic idea that outliers are usually more distant neighbors to their nearest neighbors. The proposed method consists of three different parts. Each of these parts considers the distance, density, or location of objects, and finally we reach an optimal and efficient algorithm by combining these parts. Our algorithm is based on k nearest neighbor; in addition, we also use another kind of adaptive and extended neighborhood in order to provide more accurate results. Furthermore, the proposed method is robust and has little sensitivity to changes in parameter k. Numerical experiments and comparing with well-known algorithms are performed on both synthetic and real datasets in order to prove the efficiency and robustness of the proposed method.},
  archive      = {J_PAAA},
  author       = {Heydari-Gharaei, Reza and Sharifi, Rasoul and Kashef, Shima and Nezamabadi-pour, Hossein},
  doi          = {10.1007/s10044-024-01372-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A robust approach for outlier detection based on the ratio of number of reverse neighbors to neighbors},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFM-UNET: Enhancing medical image segmentation with
multi-scale and multi-view frequency fusion. <em>PAAA</em>,
<em>28</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01384-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation benefits greatly from accurate and efficient models. Although CNNs and Transformer-based models are widely regarded as foundational methods in the realm of medical image segmentation, each has inherent drawbacks: Convolutional Neural Networks (CNNs) frequently face challenges when it comes to accurately capturing long-range relationships because of their limited receptive fields. Conversely, Transformers excel at capturing long-range relationships but come with a high computational cost. To address these challenges, State Space Models (SSMs) like Mamba have emerged as a promising alternative, providing an effective method to represent long-range interactions while maintaining a linear complexity. In this study, we present the Multi-Scale and Multi-View Frequency Mamba UNet (MSFM-UNet), a model specifically designed to leverage Mamba’s unique strengths for improving medical image segmentation. Additionally, the Multi-Scale Feature Aggregation (MSFA) effectively merges the feature outputs generated by each encoder block with those from the decoder. Furthermore, the Multi-View Frequency Enhancement (MVFA) is employed to simultaneously capture global and local perspectives, combining frequency domain attributes to improve the representation of features across multiple scales. We performed a comprehensive evaluation of MSFM-UNet on four widely recognized public datasets: ISIC17, ISIC18, Synapse, and ACDC. The experimental results clearly demonstrate that MSFM-UNet outperforms the current leading models in medical image segmentation. The code is made publicly available at https://github.com/qczggaoqiang/MSFM-UNet .},
  archive      = {J_PAAA},
  author       = {Gao, Qiang and Wang, Yi and Zhou, Feiyan and Wen, Jing and Li, Yong and Fang, Bin and Chen, Peng and Du, Lan and Chen, Cunjian},
  doi          = {10.1007/s10044-024-01384-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSFM-UNET: Enhancing medical image segmentation with multi-scale and multi-view frequency fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent sparse subspace learning and visual domain
classification via balanced distribution alignment and hilbert–schmidt
metric. <em>PAAA</em>, <em>28</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s10044-024-01390-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Getting machine learning (ML) to perform accurate prediction needs a sufficient number of labeled samples. However, due to the either lack or small number of labeled samples in most domains, it is often beneficial to use domain adaptation (DA) and transfer learning (TL) to leverage a related auxiliary source domain to optimize the performance on target domain. In fact, the purpose of TL and DA is to use the labeled sample information (i.e., samples and the corresponding labels) for training the classifier to categorize the unlabeled samples. In this paper, we aim to propose a novel semi-supervised transfer learning method entitled “Latent Sparse subspace learning and visual domain classification via Balanced distribution alignment and Hilbert–Schmidt metric (LSBH)”. LSBH uses the latent sparse domain transfer learning for visual adaptation (LSDT) to adapt the samples with different distributions or feature spaces across domains and prevent the creation of local common subspace for source and target domains via the simultaneous learning of latent space and sparse reconstruction. LSBH proposes a novel robust classifier which maintains performance and accuracy even when faced with variations across the source and target domains. To this end, it utilizes the following two criteria in the optimization problem: maximum mean discrepancy and Hilbert–Schmidt independence criterion to reduce the marginal and conditional distribution disparities of domains and increase the dependency between samples and labels at the classification step. LSBH obtains the optimal coefficients for the classifier, which results in the minimum error in the loss function by solving the optimization problem. Thus, the error minimizing of the loss function is a part of the optimization problem. Also, to maintain the geometric structure of data in the classification step, the neighborhood graph of samples is used. The efficiency of the proposed method has been evaluated on different visual datasets and has been compared with new and prominent methods of domain adaptation and transfer learning. The results induce the superior performance of LSBH compared to the other state-of-the-art methods in label prediction.},
  archive      = {J_PAAA},
  author       = {Noori Saray, Shiva and Balafar, Mohammad-Ali and Tahmoresnezhad, Jafar},
  doi          = {10.1007/s10044-024-01390-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Latent sparse subspace learning and visual domain classification via balanced distribution alignment and Hilbert–Schmidt metric},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gated normalization unit for image restoration.
<em>PAAA</em>, <em>28</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01393-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration has been an integral part of image processing research with the goal of converting degraded images into clear ones. While some networks have achieved state-of-the-art results through architecture and module design, little attention has been paid to the adaptation of normalization methods in image restoration tasks. Normalization methods are crucial in deep learning. In this work, we attempt to combine gating mechanisms with normalization methods. Gated mechanisms are popular in feature extraction and information filtering, and combining them with normalization methods has potential for designing image restoration algorithms. Firstly, we propose a Simple Gated Attention Unit (SGAU), a block using a simple gating mechanism to validate the potential of gating mechanisms. Then, we propose a new normalization block, Gated Instance Normalization (GIN), and introduce a new normalization method, Global Response Normalization (GRN), for image restoration tasks. Both GIN and GRN combine gating mechanisms with normalization methods for feature extraction, fusion, and integration. Finally, we propose a two-stage network, Gated Normalization Network (GNNet), utilizing GIN and GRN as blocks to effectively extract and filter information. Deep separable convolutions are used in the deep layers to reduce parameters while preserving spatial information, improving local feature perception. An improved cross-stage feature fusion (ICSFF) block is used for feature information transfer between stages, and a supervised attention module (SAM) is used as input to the second stage network from the first stage output. Through various image restoration tasks, we achieve 32.93 dB PSNR on GoPro, 30.42 dB PSNR on HIDE for image deblurring, 39.94 dB PSNR on SIDD for real-world denoising, and good performance in Gaussian white noise denoising and image deraining tasks. Moreover, the GIN and GRN only generated a small number of gated weight and bias parameters, and compared to other multi-stage networks, the model size is reduced, and computational complexity is well balanced.},
  archive      = {J_PAAA},
  author       = {Wang, Qingyu and Wang, Haitao and Zang, Luyang and Jiang, Yi and Wang, Xinyao and Liu, Qiang and Huang, Dehai and Hu, Binding},
  doi          = {10.1007/s10044-024-01393-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Gated normalization unit for image restoration},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-state perception consistency constraints network for
person re-identification. <em>PAAA</em>, <em>28</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01398-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) remains challenging due to pose variations and scale changes across non-overlapping camera views. In this work, we propose a Multi-state Perception Consistency Constraints Network (MPCC-Net) that extracts discriminative and robust features for person Re-ID. MPCC-Net consists of three primary components. First, a multi-state fused backbone network processes multi-scale and multi-view information. Second, perception consistency constraints enhance feature stability. Third, partition attention modules focus on different body parts to improve local discrimination. Comprehensive experiments on benchmark datasets demonstrate MPCC-Net’s competitive performance, effectively addressing pose and scale variations for accurate person Re-ID. Our source code will also be publicly available at: https://github.com/sesamecandy/MPCC-Net},
  archive      = {J_PAAA},
  author       = {Zhou, Mengting and Lian, Guoyun and Ouyang, Xinyu and Du, Jingyu and Song, Qiqi and Yang, Jinfeng},
  doi          = {10.1007/s10044-024-01398-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-state perception consistency constraints network for person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-weighted subspace clustering via adaptive rank
constrained graph embedding. <em>PAAA</em>, <em>28</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10044-024-01405-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, subspace clustering methods have attracted wide attention in partitioning high-dimensional data from a union of underlying subspaces, in which the data distribution is mainly explored to compensate for the absence of label information. However, for practical applications, subspace clustering still suffers from redundant and noisy features, which brings about disturbed reconstruction loss and restricts trustworthy graph learning. In this paper, we propose a robust subspace clustering framework via Self-weighted feature learning and adaptive rank constrained graph embedding (SWARG) to address the limitations of existing graph-based subspace clustering models. Specifically, a feature self-weighted learning term is introduced to the sparse subspace clustering framework to alleviate the disturbed contributions from the noisy and redundant features. As such, a few discriminative features will act as remarkable contributions in representing data samples. Meanwhile, the profile-based graph embedding term further preserving the contribution behavior information of data samples that distributed around the same subspace. Moreover, the adaptive rank-constraint graph embedding method is considered to guarantee discriminative structure for different components of representation matrix with flexible entropy-based similarity preserving. To solve the proposed model, we then develop an efficient alternative direction updating algorithm, together with convergence and complexity analysis. Finally, experimental results on toy databases and benchmark databases demonstrate the effectiveness of the proposed SWARG model compared to a series of state-of-the-art models. Our code is available at http://github.com/ty-kj/SAWRG .},
  archive      = {J_PAAA},
  author       = {Jiang, Kun and Yang, Zhihai and Sun, Qindong},
  doi          = {10.1007/s10044-024-01405-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Self-weighted subspace clustering via adaptive rank constrained graph embedding},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ST-HViT: Spatial-temporal hierarchical vision transformer
for action recognition. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01407-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) is an important task in the field of computer vision, and its primary goal is to analyze and understand human activities in videos. In addition to containing the spatial information of static images, videos also contain unique temporal information, which makes the information contained in the videos even richer. However, the training cost required to fully learn the spatial-temporal information of the videos is quite expensive for a model. In light of this, we propose a novel two-stream network structure to effectively capture the spatial-temporal information in video data. We perform masked autoencoders (MAE) pre-training, aiming to reduce the training burden of the model through this asymmetric encoder-decoder pre-training method. In addition, we propose a new multi-scale decoder component that combines transposed convolutional upsampling and convolutional downsampling. It fully utilizes the multi-scale features of the encoder to achieve excellent performance. On two challenging video datasets, Kinetics 400 (K400) and Something-Something-v2 (SSv2), we achieve state-of-the-art performance with 85.9 $$\%$$ and 75.3 $$\%$$ Top-1 accuracy, respectively.},
  archive      = {J_PAAA},
  author       = {Xia, Limin and Fu, Weiye},
  doi          = {10.1007/s10044-024-01407-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ST-HViT: Spatial-temporal hierarchical vision transformer for action recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional spatio-temporal generative adversarial network
for video super-resolution. <em>PAAA</em>, <em>28</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01409-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial and periodic training method plays an essential role in video super-resolution, which can generate spatial high frequency detail and temporal consistency relation. However, this approach is based on unidirectional loop whose first frame can only use its own feature information, while the last frame can use all feature information of whole sequence. The biggest problem caused by this information imbalance is that early video frames are poorly reconstructed. To address these issues, we propose a novel video super-resolution model, called Bidirectional Spatio-Temporal Generative Adversarial Network (Bi-STGAN), to generate fine detail and temporal consistency video by explicitly introducing the backward branch. Specifically, Bi-STGAN adopts an elaborately designed bidirectional branch structure so that the high-resolution frames estimated from front to back can be used as input for subsequent iterations from back to front. The advantage of Bi-STGAN is to enhance information gathering by utilizing information from past and future frames which can be cyclically passed through the time series. The experimental results show that compared with the baselines, Bi-STGAN achieves competitive improvement of 15.20% for LPIPS and 2.87dB for PSNR on the REDS4 dataset, thereby demonstrating the superiority of our state-of-the-art model on video super-resolution.},
  archive      = {J_PAAA},
  author       = {Yang, Peng and Chen, Zhangquan and Sun, Yuankang and Hu, Zhongjian and Li, Bing},
  doi          = {10.1007/s10044-024-01409-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Bidirectional spatio-temporal generative adversarial network for video super-resolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced clustering contrastive learning for long-tailed
visual recognition. <em>PAAA</em>, <em>28</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s10044-025-01410-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world deep learning training data often follow a long-tailed (LT) distribution, where a few classes (head classes) have the most samples and many classes (tail classes) have very few samples. Models trained on LT datasets typically achieve high accuracy on head classes, but suffer from poor performance on tail classes. To address this challenge, strategies based on supervised contrastive learning have been explored. However, existing methods often focus on either reducing the dominance of head class features or expanding the feature space of tail classes, but rarely achieve a balanced feature distribution across both. In this paper, we propose Balanced clustering contrastive learning (BCCL) to balance the feature space between the head and tail classes more effectively. The proposed approach introduces two main components. First, we employ queue-based clustering to extract multiple centroids. This addresses the intra-minibatch class absence issue and maintains intra-class balance. Second, we expand the feature space of tail classes based on class frequency to enhance their expressiveness. An evaluation of four LT datasets, CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018, demonstrates that BCCL consistently outperforms the existing methods. These results establish the ability of BCCL to maintain a balanced feature space in diverse environments. Our code is available at https://github.com/GGTINE/BCCL .},
  archive      = {J_PAAA},
  author       = {Kim, Byeong-il and Ko, Byoung Chul},
  doi          = {10.1007/s10044-025-01410-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Balanced clustering contrastive learning for long-tailed visual recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separability and scatteredness (s&amp;s) ratio-based
efficient SVM regularization parameter, kernel, and kernel parameter
selection. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-025-01411-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning a regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels. In this case a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through various forms of deterministic or probabilistic grid-search. The existing methods rely heavily on exhaustive searches and provide very limited insight into the underlying data characteristics, resulting in excessive computational complexity. This work addresses this issue by proposing a statistical framework that directly relates the dataset’s separability and scatteredness to the choice of optimal hyperparameters. By stochastically analyzing the behavior of the regularization parameter, the method shows that the SVM performance can be modeled as a function of the newly defined separability and scatteredness (S&amp;S) ratio of the data. The Separability is a measure of the distance between classes, and the scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost function, an S&amp;S ratio-based table provides the optimum RP. The data S&amp;S ratio is a powerful value that can automatically evaluate linear or non-linear separability before using the SVM algorithm. The provided lookup S&amp;S ratio-based table can also provide the optimum kernel and its parameters before using the SVM algorithm. Consequently, the computational complexity of the CV grid-search is reduced to only the computational complexity of one-time use of the SVM. The simulation results on the real dataset confirm the superiority of the proposed approach in the sense of efficiency and computational complexity over the grid-search methods. The method performs better or comparable to the existing state of-the-art methods with a significantly reduced computational cost.},
  archive      = {J_PAAA},
  author       = {Shamsi, Mahdi and Beheshti, Soosan},
  doi          = {10.1007/s10044-025-01411-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Separability and scatteredness (S&amp;S) ratio-based efficient SVM regularization parameter, kernel, and kernel parameter selection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ultra-fast computation of fractal dimension for RGB images.
<em>PAAA</em>, <em>28</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10044-025-01415-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fractal dimension (FD) is a quantitative parameter widely used to analyze digital images in many application fields such as image segmentation, feature extraction, object recognition, texture analysis, and image compression and denoising, among many others. A variety of algorithms have been previously proposed for estimating the FD, however most of them are limited to binary or gray-scale images only. In recent years, several authors have proposed algorithms for computing the FD of color images. Nevertheless, almost all these methods are computationally inefficient when analyzing large images. Nowadays, color images can be very large in size, and there is a growing trend toward even larger datasets. This implies that the time required to calculate the FD of such datasets can become extremely long. In this paper we present a very efficient GPU algorithm, implemented in CUDA, for computing the FD of RGB color images. Our solution is an extension to RGB of the differential box-counting (DBC) algorithm for gray-scale images. Our implementation simplifies the box-counting computation to very simple operations which are easily combined across iterations. We evaluated our algorithm on two distinct hardware/software platforms using a set of images of increasing size. The performance of our method was compared against two recent FD algorithms for RGB images: a fast box-merging GPU algorithm, and the most advanced approach based on extending the DBC method. The results showed that our GPU algorithm performed very well and achieved speedups of up to 7.9× and 6172.6× regarding these algorithms, respectively. In addition, our algorithm achieved average error rates similar to those obtained by the two reference algorithms when estimating the FD for synthetic images with known FD values, and even outperformed them when processing large images. These results suggest that our GPU algorithm offers a highly reliable and ultra-fast solution for estimating the FD of color images.},
  archive      = {J_PAAA},
  author       = {Ruiz de Miras, Juan and Li, Yurong and León, Alejandro and Arroyo, Germán and López, Luis and Torres, Juan Carlos and Martín, Domingo},
  doi          = {10.1007/s10044-025-01415-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Ultra-fast computation of fractal dimension for RGB images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DE-NAF: Decoupled neural attenuation fields for sparse-view
CBCT reconstruction. <em>PAAA</em>, <em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-025-01416-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cone-beam computed tomography (CBCT) acquires three-dimensional internal images, particularly effective for high-mineral density structures like bones. However, especially in sparse view scenarios, its ability to visualize low-density soft tissues is limited, restricting its clinical applications. To address this problem, this study proposes a method called Decoupled Neural Attenuation Fields (DE-NAF). Specifically, DE-NAF utilizes an Adaptive Hybrid Encoder that includes both hash encoding and 3D feature grid encoding methods. This approach decouples the CBCT reconstruction into two components. Hash encoding is used for high-mineral density structures, such as bones, owing to its superior encoding quality and ability to retain features of high-mineral density structures during hash conflict resolution. The 3D feature grid is employed for low-density soft tissues, such as muscles, as it effectively preserves the feature information of low-density soft tissues. The Adaptive Hybrid Encoder extracts these features, which are then decoded by a multilayer perceptron (MLP) decoder to predict X-ray attenuation values for precise reconstruction. In addition, a loss of structural perception was introduced to enhance tissue contrast and detail, further aiding CBCT reconstruction. Extensive experiments demonstrated that DE-NAF effectively addresses the limitations of CBCT in imaging low-density soft tissues, maintaining complete structural integrity and exceeding other methods in reconstruction quality.},
  archive      = {J_PAAA},
  author       = {Zhao, Tianning and Ding, Guoping and Liu, Zhenyang and Hu, Peng and Wei, Hangping and Tan, Min and Ding, Jiajun},
  doi          = {10.1007/s10044-025-01416-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DE-NAF: Decoupled neural attenuation fields for sparse-view CBCT reconstruction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task model with attribute-specific heads for person
re-identification. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-025-01421-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) has become an important task in digital surveillance for enhancing security, efficient monitoring, and enabling various applications in smart cities and public safety systems. Person ReID with attributes is a challenging task due to different camera views create significant difficulties in capturing each person’s unique identity and detailed attributes. In this work, we propose a multi-task model that not only performs unique person ReID but also simultaneously predicts attributes. Our model jointly utilizes a shared backbone network, which can be either ResNet50 or EfficientNet, along with generalized mean (GeM) pooling to achieve efficient feature extraction. It also applies attribute-specific heads to predict various characteristics such as gender, age, type of clothes, color, and alongside the ReID classification. This multi-task approach utilizes the shared features across tasks, gives comprehensive attribute predictions, and may further contribute to identification in surveillance scenarios. We evaluate our model on two commonly used publicly available datasets, Market1501 and DukeMTMC-reID, demonstrating how our approach can improve both in ReID accuracy and give reliable attribute predictions. These results reveal that our multi-task model can be competitive, providing a holistic solution for practical applications in surveillance where both identification and attributes are important. The approach has shown the potential of unifying ReID with attribute prediction to develop more robust and advanced surveillance systems. The code of this experiment is publicly accessible at https://github.com/TripleTheGreatDali/ReIDMTMASH .},
  archive      = {J_PAAA},
  author       = {Ahmed, Md Foysal and Oyshee, Adiba An Nur},
  doi          = {10.1007/s10044-025-01421-0},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-task model with attribute-specific heads for person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new method for tuning the CNN pre-trained models as a
feature extractor for malware detection. <em>PAAA</em>, <em>28</em>(1),
1–19. (<a href="https://doi.org/10.1007/s10044-024-01381-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advancements in Android malware detection, current approaches face notable challenges, particularly in handling obfuscation techniques, achieving high detection accuracy, and maintaining computational efficiency. Traditional static and dynamic analysis methods often struggle with evolving malware tactics and providing lightweight execution, which necessitates models that can dynamically adapt to these challenges. To address these needs, this study presents TuneDroid, a novel approach that optimizes CNN model configurations for both improved detection rates and resilience to obfuscation. By leveraging image-based visualization of code, TuneDroid enables CNNs to recognize high-level visual patterns that remain consistent even with code modifications, thereby enhancing robustness against common evasion tactics. TuneDroid utilizes Bayesian optimization for dynamically tuning pre-trained Convolutional Neural Network (CNN) models. This optimization process selects optimal pre-trained models, layer configurations, and positions, significantly enhancing detection performance. Using a dataset of 3000 benign and 3000 malicious apps, where DEX code is converted into images, TuneDroid achieved accuracy rates of 99.44% on the validation set and 98.00% on the testing set. In comparison, static end-to-end models without hyperparameter tuning yielded lower accuracies, not exceeding 90.50% and 91.17%. The robustness of TuneDroid’s performance is demonstrated through extensive experiments, including precision, recall, F1-score, and comparisons with baseline models. These results highlight the importance of dynamic tuning in maximizing the effectiveness of CNN-based malware detection. This work stands out by focusing on the dynamic tuning of deep learning models for Android app security, demonstrating substantial improvements in detection accuracy and showcasing the potential of Bayesian optimization in this context.},
  archive      = {J_PAAA},
  author       = {Bakır, Halit},
  doi          = {10.1007/s10044-024-01381-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new method for tuning the CNN pre-trained models as a feature extractor for malware detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFFNet: Multi-scale feature fusion network with semantic
optimization for crowd counting. <em>PAAA</em>, <em>28</em>(1), 1–16.
(<a href="https://doi.org/10.1007/s10044-024-01385-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the crowd-counting system has strong strength to ensure safety in public places with the practical necessity of dense crowd analysis. However, until now, it has been difficult to obtain high-quality density maps due to complex background interference, congested crowd and large-scale variations. To address this issue, this paper presents a multi-scale feature fusion network (MSFFNet) based on CNN (convolution neural network), which is capable of detecting enough semantic features to understand crowds in sparse and highly congested scenes. In this method, a large majority of encoded features are fused adaptively rather than separated extraction components. Therefore, it enhances the ability to extend the range of receptive field sizes and reduce computation cost. MSSFNet is consists of three modules: grouped feature extractor, fusion block and decoder. The feature extractor is based on first 13 convolution layers of VGG16, which extract low-level features from crowd images. The fusion block computes the weights in each group from the contrast features and average them from convolutional layers later concatenated pooling layer with a feature map. The decoder capably extracts relevant information while enduring spatial resolution. Additionally, we designed two-stream module and semantic optimization module (SOM) with decoder which instantaneously enhance the crowd head positions and reduce background noises by re-weighting features. Extensive experiments on four public datasets (ShanghaiTech Part_A and Part_B, UCF_CC_50, UCF_QNRF and JHU-Crowd++), validate that MSFFNet can perform efficiently in complex background noises and capture head sizes in sparse, congested and various weather situation.},
  archive      = {J_PAAA},
  author       = {Rohra, Avinash and Yin, Baoqun and Bilal, Hazrat and Kumar, Aakash and Ali, Munawar and Li, Yang},
  doi          = {10.1007/s10044-024-01385-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSFFNet: Multi-scale feature fusion network with semantic optimization for crowd counting},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight multidimensional feature network for small
object detection on UAVs. <em>PAAA</em>, <em>28</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10044-024-01389-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV small object detection has essential applications in the military, search and rescue, and smart cities, providing critical support for target recognition in complex environments. However, the existing UAV small object detection models usually have many parameters and high computational complexity, limiting their deployment and application in practical scenarios to some extent. In this study, we propose a UAV detector with Lightweight Multidimensional Feature Network (LMF-UAV), aiming to reduce the number of parameters and computation of the model while guaranteeing accuracy, which constructs the Lightweight Multidimensional Feature Network (LMF-Net) for lightweight feature extraction, and Efficient Expressive Network (EENet) for efficient feature fusion. Neural architecture search utilizes the Dual-branch Cross-stage Universal Inverted Bottleneck to enable the network to select the most suitable structure at different layers according to requirements, thereby improving the computational efficiency of LMF-Net while maintaining performance. EENet uses the Channel-wise Partial Convolution Stage to reduce redundant computation and memory access and fuse spatial features more effectively. First, LMF-Net extracts features from the images collected by UAV and obtains three multi-scale feature maps. Second, EENet performs feature fusion on three feature maps of different scales to obtain three feature representatives. Finally, the decoupled head detects the feature map and outputs the final result. The bounding box regression loss function uses Wasserstein distance to evaluate box similarity and enhance the model’s sensitivity to small targets. The experimental results demonstrate that on the VisDrone dataset, mAP50-95 of LMF-UAV reaches 24.6%, while parameters are only 14.7M, FLOPs are only 61.8G, showing a good balance between performance and efficiency.},
  archive      = {J_PAAA},
  author       = {Yang, Wenyuan and He, Qihan and Li, Zhongxu},
  doi          = {10.1007/s10044-024-01389-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A lightweight multidimensional feature network for small object detection on UAVs},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilize trajectory information for small target
classification. <em>PAAA</em>, <em>28</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s10044-024-01397-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small target recognition has always been challenging in image processing systems. When targets are far from the camera, target features tend to have low quality, limiting the amount of useful information for detection systems. Consequently, classic Detection Before Tracking (DBT) algorithms find great difficulty in separating targets from their background based on their visual properties. In this study, we proposed a Track Before Detect (TBD) approach that tracks potential targets in multiple frames, reducing the false alarm rate and enhancing the detection robustness to clutter. Then, we utilize target trajectory information to distinguish actual targets from any background noise. The proposed approach reframes the classic target image classification challenge to a multivariate time series classification problem, using target trajectory coordinates (x, y) as features. The proposed approach achieved a remarkable 97% accuracy in classifying targets from noise using only ten data points (half a second of tracking). Furthermore, it successfully classified targets into specific categories (airplane, drone, bird) with a 96% accuracy rate over a 1.5 s window (30 data points).},
  archive      = {J_PAAA},
  author       = {Alkentar, Saad and Assalem, Abdulkarim and Alsahwa, Bassem},
  doi          = {10.1007/s10044-024-01397-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Utilize trajectory information for small target classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved nearest neighbour classifier. <em>PAAA</em>,
<em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01399-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A windowed version of the Nearest Neighbour (WNN) classifier for images is described. While its construction is inspired by the architecture of Artificial Neural Networks, the underlying theoretical framework is based on approximation theory. We illustrate WNN on the datasets MNIST and EMNIST of images of handwritten digits. In order to calibrate the parameters of WNN, we first study it on MNIST. We then apply WNN with these parameters to EMNIST resulting in an error rate of 0.76% which significantly outperforms traditional classification methods like Support Vector Machines. By expansions of the training set, an error rate down to 0.42% is achieved.},
  archive      = {J_PAAA},
  author       = {Setterqvist, Eric and Kruglyak, Natan and Forchheimer, Robert},
  doi          = {10.1007/s10044-024-01399-1},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An improved nearest neighbour classifier},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Texture discrimination via hilbert curve path based
information quantifiers. <em>PAAA</em>, <em>28</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01400-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of the spatial arrangement of colors and roughness/smoothness of figures is relevant due to its wide range of applications. This paper proposes a texture characterization method that extracts data from images using the Hilbert curve. Three information theory quantifiers are then computed: permutation entropy, permutation complexity, and Fisher information measure. The proposal exhibits some important properties: (i) it allows discrimination between figures according to varying degrees of correlations (as measured by the Hurst exponent), (ii) it is invariant to rotation and symmetry transformations, (iii) it is invariant to image scaling, (iv) it can be used for both black and white and color images. Validations have been performed not only using synthetic images but also using the well-known Brodatz image database.},
  archive      = {J_PAAA},
  author       = {Bariviera, Aurelio F. and Hansen, Roberta and Pastor, Verónica E.},
  doi          = {10.1007/s10044-024-01400-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Texture discrimination via hilbert curve path based information quantifiers},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A light-weight backbone to adapt with extracting grouped
dilation features. <em>PAAA</em>, <em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01401-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing grouped dilation features (GDFs) improved the learning ability of MobileNetV1 in image representation. However, the computational complexity is still at a high level, while the performance is a modest degree. This expensive cost is principally caused by the backbone of MobileNetV1 taking deep feature maps in several latest layers. To mitigate these issues, we propose a light-weight network (called CGDF-Net) with an adaptative architecture to effectively extract grouped dilation features. CGDF-Net is structured by two main contributions: (i) Its backbone is improved by simply replacing several latest layers of MobileNetV1 with a pointwise convolutional layer for reducing the computational complexity; (ii) Embedding an attention mechanism into the GDF block to form a completed GDF perceptron (CGDF) that directs the learning process into the significant properties of objects in images instead of the trivial ones. Experimental results on benchmark datasets for image recognition have validated that the proposed CGDF-Net network obtained good performance with a small computational cost in comparison with MobileNets and other light-weight models. For instance, CGDF-Net obtained 60.86% with 3.53M learnable parameters on Stanford Dogs, up to 6% better than MoblieNetV1-GDF (54.9%, 3.39M) and 9% versus MoblieNetV1 (51.6%, 3.33M). Meantime, the performance of CGDF-Net on ImageNet-100 is 85.22%, about 6% $$\sim$$ 8% higher than MobileNetV1-GDF’s (79.14%) and MobileNetV1’s (77.01%), respectively. The code of CGDF-Net is available at https://github.com/nttbdrk25/CGDFNet .},
  archive      = {J_PAAA},
  author       = {Nguyen, Thanh Tuan and Pham, Hoang Anh and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10044-024-01401-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A light-weight backbone to adapt with extracting grouped dilation features},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMIHCT: Improved multi-stage image inpainting with hybrid
CNN and transformer. <em>PAAA</em>, <em>28</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01402-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at problems such as the poor effect of large-hole image inpainting, the limitation of local information reconstruction of the convolutional neural network, and a surge in parameters caused by stacking a large number of convolutional modules in the network model. We make full use of the advantages of convolutional neural network and transformer and propose an improved multi-stage inpainting method with hybrid CNN and transformer. The method achieves a balance between performance and parameters. Specifically, rough results are first generated using a coarse inpainting network with skip connections and lightweight Taylor transformer modules. Then, a local refinement network with coordinate attention is used to perform local refinement, optimizing local details while weakening the influence of unreasonable content in the distance. Finally, to compensate for the inability of local refinement networks to refine the overall pattern over long distances, global refinement is performed using a network that is consistent with the structure of the coarse inpainting network to make the reconstructed image more realistic and natural. Results show that the method outperforms the state of the arts on three publicly available datasets. The code is made available at https://github.com/Sheeran2000/IMIHCT .},
  archive      = {J_PAAA},
  author       = {Ning, Tao and Wang, Xingfang and Ding, Hongwei},
  doi          = {10.1007/s10044-024-01402-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {IMIHCT: Improved multi-stage image inpainting with hybrid CNN and transformer},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMAM-NET: Ultrasound thyroid nodule malignancy grading
network based on multi-subnet attention mechanism. <em>PAAA</em>,
<em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01404-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and Objectives: Thyroid nodules are one of the most common thyroid diseases, and their incidence has been on the rise in recent years. Ultrasound imaging, due to its low cost and no ionizing radiation, has become the preferred method for imaging thyroid nodules. Accurate assessment of the malignancy grade of thyroid nodules is crucial to ensure the accuracy of subsequent examination and treatment. Texture and shape are key features for determining the nature of thyroid nodules. Despite the excellent performance of convolutional neural networks (CNNs) in image feature extraction and aggregation, the low resolution and high noise characteristics of ultrasound images still pose challenges for existing CNN models in identifying texture and shape. Methods: To address this challenge, we propose a thyroid nodule malignancy grading network based on a multi-subnet attention mechanism (UMAM-NET). In the feature extraction stage, we innovatively introduce the multi-subnet attention module. The module designs two parallel subnets, aiming to enhance the model’s attention to the texture and shape of thyroid nodules. Results: Compared to other deep learning models, the proposed UMAM-NET performs better in the malignant grading task of thyroid nodules. It demonstrates excellent performance on public datasets, achieving the best results in Recall (93.1%), F1-score (95.4%), and Accuracy (98.4%). Similarly, it also shows outstanding performance on the sub-collected dataset, with Recall (91.8%), F1-score (92.0%), and Accuracy (94.4%). Conclusion: Our proposed UMAM-NET, based on multi-subnet attention mechanism, provides a promising approach for accurate assessment of thyroid nodule malignancy grade, which can be of great value in clinical practice.},
  archive      = {J_PAAA},
  author       = {Bi, Hui and Wang, Fan and Xiong, YuHao and Dong, ZhaoHui and Jiang, Yibo and Zhao, Tong and Zheng, Yineng},
  doi          = {10.1007/s10044-024-01404-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {UMAM-NET: Ultrasound thyroid nodule malignancy grading network based on multi-subnet attention mechanism},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSFE-YOLO: A traffic sign detection algorithm with
pixel-wise spatial feature enhancement. <em>PAAA</em>, <em>28</em>(1),
1–15. (<a href="https://doi.org/10.1007/s10044-024-01406-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection is essentially important for intelligent driving. Existing detection algorithms typically incorporate self-attention mechanisms to model the dependencies among image elements, such as patches or pixels. When using a patch as a token, the positional information within the patch could be lost. When using a pixel as a token, an increase in the number of tokens can lead to a significant increase in computational complexity. To balance these two extreme situations, a pixel only needs to focus on pixels from the surrounding area. Therefore, we propose a local attention module termed Pixel-wise Spatial Feature Enhancement (PSFE), which uses pixels as tokens to enhance the spatial information of feature maps, and each pixel’s self-attention only acts on a local region to reduce computational complexity. Furthermore, we design a Bidirectional Res2Net (BR) module that generates multiple feature maps with different channel numbers from an input feature map, and then restores them to one feature map with the original input size through bidirectional fusion, greatly enriching the receptive field information contained in the feature map. We conducted experiments on the GTSDB, TT100K, and CCTSDB 2021 datasets to comprehensively evaluate our method, and the experimental results showed that our method has superior performance.},
  archive      = {J_PAAA},
  author       = {Zhang, Jianming and Wang, Zulou and Yi, Yao and Kuang, Li-Dan and Zhang, Jin},
  doi          = {10.1007/s10044-024-01406-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PSFE-YOLO: A traffic sign detection algorithm with pixel-wise spatial feature enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="sac---29">SAC - 29</h2>
<ul>
<li><details>
<summary>
(2025). Penalized empirical likelihood estimation and EM algorithms
for closed-population capture–recapture models. <em>SAC</em>,
<em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10557-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capture–recapture experiments are widely used to estimate the abundance of a finite population. Based on capture–recapture data, the empirical likelihood (EL) method has been shown to outperform the conventional conditional likelihood (CL) method. However, the current literature on EL abundance estimation ignores behavioral effects, and the EL estimates may not be stable, especially when the capture probability is low. We make three contributions in this paper. First, we extend the EL method to capture–recapture models that account for behavioral effects. Second, to overcome the instability of the EL method, we propose a penalized EL (PEL) estimation method that penalizes large abundance values. We then investigate the asymptotics of the maximum PEL estimator and the PEL ratio statistic. Third, we develop standard expectation–maximization (EM) algorithms for PEL to improve its practical performance. The EM algorithm is also applicable to EL and CL with slight modifications. Our simulation and a real-world data analysis demonstrate that the PEL method successfully overcomes the instability of the EL method and the proposed EM algorithm produces more reliable results than existing optimization algorithms.},
  archive      = {J_SAC},
  author       = {Liu, Yang and Li, Pengfei and Liu, Yukun},
  doi          = {10.1007/s11222-024-10557-8},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Penalized empirical likelihood estimation and EM algorithms for closed-population capture–recapture models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal distributed subsampling under heterogeneity.
<em>SAC</em>, <em>35</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-024-10558-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed subsampling approaches have been proposed to process massive data in a distributed computing environment, where subsamples are taken from each site and then analyzed collectively to address statistical problems when the full data is not available. In this paper, we consider that each site involves a common parameter and site-specific nuisance parameters and then formulate a unified framework of optimal distributed subsampling under heterogeneity for general optimization problems with convex loss functions that could be nonsmooth. By establishing the consistency and asymptotic normality of the distributed subsample estimators for the common parameter of interest, we derive the optimal subsampling probabilities and allocation sizes under the A- and L-optimality criteria. A two-step algorithm is proposed for practical implementation and the asymptotic properties of the resultant estimator are established. For nonsmooth loss functions, an alternating direction method of multipliers method and a random perturbation procedure are proposed to obtain the subsample estimator and estimate the covariance matrices for statistical inference, respectively. The finite-sample performance of linear regression, logistic regression and quantile regression models is demonstrated through simulation studies and an application to the National Longitudinal Survey of Youth Dataset is also provided.},
  archive      = {J_SAC},
  author       = {Shao, Yujing and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-024-10558-7},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Optimal distributed subsampling under heterogeneity},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCA-uCPD: An ensemble method for multiple change-point
detection in moderately high-dimensional data. <em>SAC</em>,
<em>35</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10553-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-point detection (CPD) receives extensive studies due to its wide applications in various fields. However, CPD remains a challenging problem for complex data with medium or high dimensions, high correlations, outliers or heavy-tailed distribution. This article proposes an integrated change-point detection method called PCA-uCPD, which utilizes principal components analysis (PCA) to project the original data series into uncorrelated principal components (PCs). Subsequently, we apply existing univariate change-point detection methods to the mapped PCs, followed by a proposed refining technique to obtain the ultimate change-point estimates for the original data sequences. The proposed method admits a flexible architecture that is thus capable of dealing with complex data. Theoretical justifications have been provided to guarantee the feasibility of the proposed methods. Moreover, we conduct simulations to assess performance across various data-generating scenarios. The efficacy of PCA-uCPD is further demonstrated through applications in both genetic and financial datasets.},
  archive      = {J_SAC},
  author       = {Qin, Shanshan and Tan, Zhenni and Wei, Weidong and Wu, Yuehua},
  doi          = {10.1007/s11222-024-10553-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {PCA-uCPD: An ensemble method for multiple change-point detection in moderately high-dimensional data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power enhancing probability subsampling using side
information. <em>SAC</em>, <em>35</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10556-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the subsampling technique for hypothesis testing in generalized linear models with large-scale datasets, focusing on testing simple null hypotheses against composite linear alternatives. We propose a subsample-based test statistic and show that it converges to non-central chi-square distributions under Pitman’s local alternatives. The optimal subsampling distribution that maximizes power requires iterative calculations on the full data, which is computationally infeasible. Furthermore, it depends on the true parameter, which cannot be consistently estimated under Pitman’s local alternatives. We maximize a lower bound of the non-central parameter to define the power enhancing probability and utilize side information under the alternative to replace the true parameter. Extensive simulations and an application to a real dataset on flight delays and cancellations show that the proposed method offers a computationally viable solution for hypothesis testing in the realm of big data.},
  archive      = {J_SAC},
  author       = {Gao, Junzhuo and Wang, Lei and Wang, Haiying},
  doi          = {10.1007/s11222-024-10556-9},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Power enhancing probability subsampling using side information},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference issue in multiscale geographically and temporally
weighted regression. <em>SAC</em>, <em>35</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-024-10559-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographically and temporally weighted regression (GTWR) models assume that all of the varying coefficients operate at a same spatiotemporal scale, which reduces the flexibility of local regression models. Multiscale geographically and temporally weighted regression (MGTWR) models increase flexibility, enhance interpretability, and provide more comprehensive information by allowing regression coefficients to vary across different spatiotemporal scales. However, a limitation of the MGTWR models is that, to date, statistical inference regarding the local coefficient estimates has not been feasible. Formally, “hat matrix”, which projects the observed response variable vector onto the fitting response variable, was not available in the MGTWR model. This paper settles this limitation by reformulating the GTWR model into a Generalized Additive Model, extending this framework to the MGTWR model and then deriving standard deviations for the local coefficient estimates in the MGTWR model. In addition, we also obtain the number of effective parameters for the overall fit of the MGTWR model and for each covariate within the model. This statistic is crucial for comparing the goodness of fit between MGTWR, GTWR and classical global regression models, as well as for adjusting multiple tests. Simulation studies and a real-world example demonstrate these advances to the MGTWR framework.},
  archive      = {J_SAC},
  author       = {Hong, Zhimin and Wang, Ruoxuan and Wang, Zhiwen and Du, Wala},
  doi          = {10.1007/s11222-024-10559-6},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Inference issue in multiscale geographically and temporally weighted regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constrained least squares simplicial-simplicial regression.
<em>SAC</em>, <em>35</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10560-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. For this setting, constrained least squares, where the regression coefficients themselves lie within the simplex, is proposed. The model is transformation-free but the adoption of a power transformation is straightforward, it can treat more than one compositional datasets as predictors and offers the possibility of weights among the simplicial predictors. Among the model’s advantages are its ability to treat zeros in a natural way and a highly computationally efficient algorithm to estimate its coefficients. Resampling based hypothesis testing procedures are employed regarding inference, such as linear independence, and equality of the regression coefficients to some pre-specified values. The strategy behind the formulation of the new model implemented is related to an existing methodology, that is of the same spirit, showcasing how other similar models can be employed as well. Finally, the performance of the proposed technique and its comparison to the existing methodology takes place using simulation studies and real data examples.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail},
  doi          = {10.1007/s11222-024-10560-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Constrained least squares simplicial-simplicial regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical investigations of boosting with pseudo-outcome
imputation for missing responses. <em>SAC</em>, <em>35</em>(2), 1–18.
(<a href="https://doi.org/10.1007/s11222-024-10561-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosting techniques have gained increasing interest in both machine learning and statistical research. However, many of these methods are primarily designed for complete datasets, which limits their applicability to handle incomplete data such as missing observations. In this paper, we propose the pseudo-outcome strategy to account for missingness effects and describe a functional gradient descent algorithm. Numerical studies demonstrate the satisfactory performance of the proposed method in finite sample settings. Furthermore, we apply the proposed method to analyze the KLIPS Data.},
  archive      = {J_SAC},
  author       = {Bian, Yuan and Yi, Grace Y. and He, Wenqing},
  doi          = {10.1007/s11222-024-10561-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Empirical investigations of boosting with pseudo-outcome imputation for missing responses},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analysis of the modality and flexibility of the inverse
stereographic normal distribution. <em>SAC</em>, <em>35</em>(2), 1–12.
(<a href="https://doi.org/10.1007/s11222-025-10563-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular data arises in various fields including robotics, biology, geology and material sciences. Modelling such data requires flexible distribution families on the hypertorus. Common choices are the von Mises and the wrapped normal distributions. In this work we investigate the inverse stereographic normal distribution as an alternative distribution family both from a theoretical and applied perspective. We first generalize unimodality results to arbitrary dimensions by proving that the inverse stereographic normal distribution is unimodal if and only if all eigenvalues of the covariance matrix are less than or equal to 0.5. We then analyze the flexibility by fitting mixtures of shifted inverse stereographic normal distributions via gradient descent to several examples of toroidal data.},
  archive      = {J_SAC},
  author       = {Hinz, Florian B. and Mahmoud, Amr H. and Lill, Markus A.},
  doi          = {10.1007/s11222-025-10563-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {An analysis of the modality and flexibility of the inverse stereographic normal distribution},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic post-selection inference for regularized
graphical models. <em>SAC</em>, <em>35</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s11222-025-10564-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymptotically valid inference is obtained for graphical model edge parameters after selection using the same data set as the one used for inference. We consider Gaussian and (trans)elliptical graphical models, where the edge selection and consequent sparse estimation is operated by applying the $$\ell _1$$ , elastic net, smoothly clipped absolute deviation, or minimax concave penalty to an appropriately regular loss function. The polyhedral lemma is used to carry out conditional inference which is asymptotically valid in the (possibly wrong) selected graphical model. Simulation studies show how the method yields valid inference in a variety of finite-sample settings.},
  archive      = {J_SAC},
  author       = {Guglielmini, Sofia and Claeskens, Gerda},
  doi          = {10.1007/s11222-025-10564-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Asymptotic post-selection inference for regularized graphical models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient modeling of quasi-periodic data with seasonal
gaussian process. <em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10565-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-periodicity refers to a pattern in a function where it appears periodic at a certain frequency but exhibits evolving amplitudes over time. This is often the case in practical settings such as the modeling of case counts of infectious disease or the population dynamics of species over time. In this paper, we consider a class of Gaussian processes, called seasonal Gaussian Processes (sGP), for model-based inference of such quasi-periodic behavior. We illustrate that the exact sGP can be efficiently fitted using its state space representation for equally spaced time points. However, for large datasets with irregular spacing, the exact approach becomes computationally inefficient and unstable. To address this, we develop a continuous finite dimensional approximation for sGP using the seasonal B-spline (sB-spline) basis constructed by damping B-splines with sinusoidal functions. We prove the covariance convergence rate of the proposed approximation to the true sGP as the number of basis functions increases, and show its superior approximation quality through numerical studies. We also provide a unified and interpretable way to define priors for the sGP, based on the notion of predictive standard deviation. Finally, we implement the proposed inference method on several real data examples to illustrate its practical usage.},
  archive      = {J_SAC},
  author       = {Zhang, Ziang and Brown, Patrick and Stafford, Jamie},
  doi          = {10.1007/s11222-025-10565-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Efficient modeling of quasi-periodic data with seasonal gaussian process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy k-expectiles clustering. <em>SAC</em>, <em>35</em>(2),
1–19. (<a href="https://doi.org/10.1007/s11222-025-10566-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper the Fuzzy K-expectiles clustering model is proposed. The model takes into account the asymmetry inherent in the data distribution, extending its applicability to a broader spectrum of data than the Fuzzy K-means. To achieve this, the Fuzzy K-expectiles clustering model introduces the cluster centroid expectile, and assigns data points based on expectile distances. An adaptive asymmetry parameter is specified for each variable and for each cluster The performance of the adaptive Fuzzy K-expectiles model is compared with other clustering models suggested in the literature. To show the performances of the proposed model three simulation studies and three applications to real datasets are presented.},
  archive      = {J_SAC},
  author       = {D’Urso, Pierpaolo and De Giovanni, Livia and Federico, Lorenzo and Vitale, Vincenzina},
  doi          = {10.1007/s11222-025-10566-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Fuzzy K-expectiles clustering},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geographically weighted quantile regression for count data.
<em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10568-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a methodological framework known as geographically weighted quantile regression (GWQR) has emerged for spatial data analysis. This framework offers the abilities to simultaneously explore spatial heterogeneity or nonstationarity in regression relationships and to estimate various conditional quantile functions. However, the current configuration of GWQR is limited to the analysis of continuous dependent variables. Discrete count data are observed in many disciplines. Whenever modeling such outcomes is necessary, the conventional GWQR approach is inadequate and fails to provide comprehensive insights into count data. To address this issue, this study aims to extend the GWQR framework originally designed for continuous dependent variables to accommodate count outcomes. We introduce an approach called geographically weighted count quantile regression (GWCQR), wherein the model specification is based on the smoothing of count responses through a jittering procedure. A semiparametric counterpart that allows for the inclusion of both spatially varying and invariant coefficients is also discussed. Finally, the proposed techniques are applied to a dataset of dengue fever in Taiwan as an empirical illustration.},
  archive      = {J_SAC},
  author       = {Chen, Vivian Yi-Ju and Wang, Shi-Ting},
  doi          = {10.1007/s11222-025-10568-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Geographically weighted quantile regression for count data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anisotropic multidimensional smoothing using bayesian tensor
product p-splines. <em>SAC</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11222-025-10569-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a highly efficient fully Bayesian approach for anisotropic multidimensional smoothing. The main challenge in this context is the Markov chain Monte Carlo (MCMC) update of the smoothing parameters as their full conditional posterior comprises a pseudo-determinant that appears to be intractable at first sight. As a consequence, existing implementations are computationally feasible only for the estimation of two-dimensional tensor product smooths, which is, however, too restrictive for many applications. In this paper, we break this barrier and derive closed-form expressions for the log-pseudo-determinant and its first and second order partial derivatives. These expressions are valid for arbitrary dimension and very fast to evaluate, which allows us to set up an efficient MCMC sampler with derivative-based Metropolis–Hastings (MH) updates for the smoothing parameters. We derive simple formulas for low-dimensional slices and averages to facilitate visualization and investigate hyperprior sensitivity. We show that our new approach outperforms previous suggestions in the literature in terms of accuracy, scalability and computational cost and demonstrate its applicability through an illustrating temperature data example from spatio-temporal statistics.},
  archive      = {J_SAC},
  author       = {Bach, Paul and Klein, Nadja},
  doi          = {10.1007/s11222-025-10569-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Anisotropic multidimensional smoothing using bayesian tensor product P-splines},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logit unfolding choice models for binary data. <em>SAC</em>,
<em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10570-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete choice models with non-monotonic response functions are important in many areas of application, especially political sciences and marketing. This paper describes a novel unfolding model for binary data that allows for heavy-tailed shocks to the underlying utilities. One of our key contributions is a Markov chain Monte Carlo algorithm that requires little or no parameter tuning, fully explores the support of the posterior distribution, and can be used to fit various extensions of our core model that involve (Bayesian) hypothesis testing on the latent construct. Our empirical evaluations of the model and the associated algorithm suggest that they provide better complexity-adjusted fit to voting data from the United States House of Representatives.},
  archive      = {J_SAC},
  author       = {Lei, Rayleigh and Rodriguez, Abel},
  doi          = {10.1007/s11222-025-10570-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Logit unfolding choice models for binary data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Particle gibbs for likelihood-free inference of stochastic
volatility models. <em>SAC</em>, <em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10571-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic volatility models (SVMs) are widely used in finance and econometrics for analyzing and interpreting volatility. Real financial data are often observed to have heavy tails, which violate a Gaussian assumption and may be better modeled using the stable distribution. However, the intractable density of the stable distribution hinders the use of common computational methods such as Markov chain Monte Carlo (MCMC) for parameter inference of SVMs. In this paper, we propose a new particle Gibbs sampler as a strategy to handle SVMs with intractable likelihoods in the approximate Bayesian computation (ABC) setting. The proposed sampler incorporates a conditional auxiliary particle filter, which can help mitigate the weight degeneracy often encountered when using ABC. Simulation studies demonstrate the efficacy of our sampler for inferring SVM parameters when compared to existing particle Gibbs samplers based on the conditional bootstrap filter, and for inferring both SVM and stable distribution parameters when compared to existing particle MCMC samplers. As a real data application, we apply the proposed sampler for fitting an SVM to S&amp;P 500 Index time-series data during the 2008–2009 financial crisis.},
  archive      = {J_SAC},
  author       = {Hou, Zhaoran and Wong, Samuel W. K.},
  doi          = {10.1007/s11222-025-10571-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Particle gibbs for likelihood-free inference of stochastic volatility models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time inference for smoothing quantile regression on
streaming datasets with heterogeneity detection. <em>SAC</em>,
<em>35</em>(2), 1–40. (<a
href="https://doi.org/10.1007/s11222-025-10572-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data is generated at high speed and in large quantities over time, and it calls for online learning to deal with it. In this paper, a new online updating method is established for smoothing quantile regression to make inferences in real-time. The renewable estimators are updated only by the current dataset and summary statistics of historical datasets. This method is adapted to the streaming datasets containing small samples. Theoretically, it is proved that renewable estimators have consistency and asymptotic normality and equivalence to pooled offline estimators based on all datasets. The dynamic bandwidth selection is applied to estimate the asymptotic covariance matrix in an online manner, which is theoretically highly asymptotically efficient. In particular, the renewable estimator provides asymptotic confidence intervals that are asymptotically smaller than those generated by existing methods, thereby improving the accuracy of interval estimation. Additionally, our approach addresses the common assumption of homogeneous models by accommodating non-parametric heterogeneity and detecting and removing anomalous data batches through an online screening process. Meanwhile, numerical simulations verify the theoretical results and outcomes on real datasets illustrate that our method is adapted to real streaming data situations.},
  archive      = {J_SAC},
  author       = {Wang, Yidan and Zhang, Lingyun and Gai, Yujie},
  doi          = {10.1007/s11222-025-10572-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-40},
  shortjournal = {Stat. Comput.},
  title        = {Real-time inference for smoothing quantile regression on streaming datasets with heterogeneity detection},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploratory analysis of dynamic networks using latent
functions. <em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10573-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks, which capture the evolving interactions among entities, have flourished in various scientific fields. We propose a framework for exploratory analysis of these dynamic networks by representing them as latent functions. This framework comprises several visualization tools based on functional data analysis, specifically tailored for addressing typical tasks such as community detection, central node identification, and change point discovery. Besides, we develop a computationally efficient algorithm to obtain the latent functions. Through comprehensive simulation studies conducted under commonly investigated settings, we demonstrate the effectiveness of these tools. Furthermore, we apply the proposed tools to three representative and intriguing real-world networks, yielding enlightening discoveries. An R package for implementing the proposed methods, along with supplementary materials for this article, is available online.},
  archive      = {J_SAC},
  author       = {Shi, Haosheng and Dai, Wenlin},
  doi          = {10.1007/s11222-025-10573-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Exploratory analysis of dynamic networks using latent functions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference and goodness-of-fit test in functional
data via error distribution function. <em>SAC</em>, <em>35</em>(2),
1–16. (<a href="https://doi.org/10.1007/s11222-025-10574-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A kernel distribution estimator (KDE) is proposed for the error distribution in the functional data, which is computed from the residuals of the B-spline trajectories over all the measurements. The maximal stochastic process between the KDE and the error distribution is shown to converge to a Gaussian process with mean zero and specified covariance function under some mild conditions. Thus, a simultaneous confidence band (SCB) is constructed for the error distribution based on the KDE in the dense functional data. The proposed SCB is applicable in not only the independent functional data but also the functional time series. In addition, the symmetric test is proposed for the error distribution, as well as a goodness-of-fit test for mean function by using the bootstrap method. Simulation studies examine the finite sample performance of the SCB and show the bootstrap method performs well in numerical studies. The proposed theory is illustrated by the electroencephalogram (EEG) functional data.},
  archive      = {J_SAC},
  author       = {Zhong, Chen},
  doi          = {10.1007/s11222-025-10574-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Statistical inference and goodness-of-fit test in functional data via error distribution function},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EM algorithm for fitting matrix-variate normal
distributions on interval-censored and missing data. <em>SAC</em>,
<em>35</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11222-025-10575-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix-variate distributions are powerful tools for modeling three-way datasets that often arise in longitudinal and multidimensional spatio-temporal studies. However, observations in these datasets can be missing or subject to some detection limits because of the restriction of the experimental apparatus. Here, we develop an efficient EM-type algorithm for maximum likelihood estimation of parameters, in the context of interval-censored and/or missing data, utilizing the matrix-variate normal distribution. This algorithm provides closed-form expressions that rely on truncated moments, offering a reliable approach to parameter estimation under these conditions. Results obtained from the analysis of both simulated data and real case studies concerning water quality monitoring are reported to demonstrate the effectiveness of the proposed method.},
  archive      = {J_SAC},
  author       = {Lachos, Victor H. and Tomarchio, Salvatore D. and Punzo, Antonio and Ingrassia, Salvatore},
  doi          = {10.1007/s11222-025-10575-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {An EM algorithm for fitting matrix-variate normal distributions on interval-censored and missing data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing common degree-correction parameters of multilayer
networks. <em>SAC</em>, <em>35</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-025-10576-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph (or network) is a mathematical structure that has been widely used to model relational data. As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems. One active research problem in multilayer networks analysis is to study the common properties of the networks. In this paper, we study whether multilayer networks share the same degree-correction parameters, which is a special case of the widely studied common invariant subspace problem. We first attempt to answer this question by means of hypothesis testing. The null hypothesis states that the multilayer networks share the same degree-correction parameters, and under the alternative hypothesis, there exist at least two networks that have different degree-correction parameters. We propose a weighted degree difference test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power. Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided.},
  archive      = {J_SAC},
  author       = {Yuan, Mingao and Yao, Qianqian},
  doi          = {10.1007/s11222-025-10576-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Testing common degree-correction parameters of multilayer networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online structural break detection in financial durations.
<em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10577-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Durations between events of interest such as intra-day transactions of assets can reflect the volatility of asset prices in financial markets. The diverse dynamics of these intervals, which we refer to as financial durations, offer valuable insights into market behavior for investors. Inspection of streaming price data for structural breaks and timely and accurate detection of transitions between different duration patterns within a trading day enables practitioners to update parameters of suitable duration models. In this article, an innovative Ensemble Penalized Estimating Function (E-PEF) approach is proposed to effectively detect change points in the logarithmic autoregressive conditional duration models for financial durations. As a quasi-score-based online detection approach, this methodology leverages Mahalanobis distances and the block bootstrap sampling method to compute critical values for finite sample time series. The online structural break detection rule is informed by comparing observed quasi-scores in the monitoring period with pre-calculated critical values from training data in an ensemble manner. Extensive simulations demonstrate that the E-PEF method has fast structural break detection performance, while effectively controlling the probability of false detection. In the real data application, we applied our method to identify structural breaks for four assets, explored their relationships with summarized changes in volatility patterns, and noted several considerations for practitioners in the financial market.},
  archive      = {J_SAC},
  author       = {Wang, Yanzhao and Zhang, Yaohua and Zou, Jian and Ravishanker, Nalini},
  doi          = {10.1007/s11222-025-10577-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Online structural break detection in financial durations},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Function-on-function regression models with nonlinear
dynamic effect and linear concurrent effect. <em>SAC</em>,
<em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10578-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a function-on-function regression model that predicts a functional response by both a nonlinear dynamic effect of a functional predictor and a linear concurrent effect of another functional predictor. The nonlinear dynamic effect is characterized by taking an integral of a time-dependent two-dimensional smooth surface and the linear concurrent effect is modeled through a time-varying coefficient. The model structure combines the flexibility of nonlinear modeling with the interpretability of the linear concurrent effect. To approximate the two-dimensional surface, we use tensor product basis expansions, and for the time-varying coefficient in the concurrent effect, we employ B-spline expansions. The expansion parameters for each effect are estimated iteratively to account for the mutual dependencies between these two estimated effects. Each iteration of parameter estimation involves solving a penalized least squares problem. We establish the asymptotic properties of our estimator. The numerical performance of the proposed method is illustrated by simulation studies and two real data applications.},
  archive      = {J_SAC},
  author       = {Jia, Shifan and Shi, Haolun and Guan, Tianyu},
  doi          = {10.1007/s11222-025-10578-x},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Function-on-function regression models with nonlinear dynamic effect and linear concurrent effect},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust bayesian functional principal component analysis.
<em>SAC</em>, <em>35</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s11222-025-10580-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a robust Bayesian functional principal component analysis (RB-FPCA) method that utilizes the skew elliptical class of distributions to model functional data, which are observed over a continuous domain. This approach effectively captures the primary sources of variation among curves, even in the presence of outliers, and provides a more robust and accurate estimation of the covariance function and principal components. The proposed method can also handle sparse functional data, where only a few observations per curve are available. We employ annealed sequential Monte Carlo for posterior inference, which offers several advantages over conventional Markov chain Monte Carlo algorithms. To evaluate the performance of our proposed model, we conduct simulation studies, comparing it with well-known frequentist and conventional Bayesian methods. The results show that our method outperforms existing approaches in the presence of outliers and performs competitively in outlier-free datasets. Finally, we demonstrate the effectiveness of our method by applying it to environmental and biological data to identify outlying functional observations. The implementation of our proposed method and applications are available at https://github.com/SFU-Stat-ML/RBFPCA .},
  archive      = {J_SAC},
  author       = {Zhang, Jiarui and Cao, Jiguo and Wang, Liangliang},
  doi          = {10.1007/s11222-025-10580-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Robust bayesian functional principal component analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on diffusion processes related to a general growth
model. <em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10562-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers two stochastic diffusion processes associated with a general growth curve that includes a wide family of growth phenomena. The resulting processes are lognormal and Gaussian, and for them inference is addressed by means of the maximum likelihood method. The complexity of the resulting system of equations requires the use of metaheuristic techniques. The limitation of the parameter space, typically required by all metaheuristic techniques, is also provided by means of a suitable strategy. Several simulation studies are performed to evaluate to goodness of the proposed methodology, and an application to real data is described.},
  archive      = {J_SAC},
  author       = {Albano, Giuseppina and Barrera, Antonio and Giorno, Virginia and Torres-Ruiz, Francisco},
  doi          = {10.1007/s11222-025-10562-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Inference on diffusion processes related to a general growth model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density regression via dirichlet process mixtures of normal
structured additive regression models. <em>SAC</em>, <em>35</em>(2),
1–20. (<a href="https://doi.org/10.1007/s11222-025-10567-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within Bayesian nonparametrics, dependent Dirichlet process mixture models provide a flexible approach for conducting inference about the conditional density function. However, several formulations of this class make either restrictive modelling assumptions or involve intricate algorithms for posterior inference. We propose a flexible and computationally convenient approach for density regression based on a single-weights dependent Dirichlet process mixture of normal distributions model for univariate continuous responses. We assume an additive structure for the mean of each mixture component and incorporate the effects of continuous covariates through smooth functions. The key components of our modelling approach are penalised B-splines and their bivariate tensor product extension. Our method also seamlessly accommodates categorical covariates, linear effects of continuous covariates, varying coefficient terms, and random effects, which is why we refer our model as a Dirichlet process mixture of normal structured additive regression models. A notable feature of our method is the simplicity of posterior simulation using Gibbs sampling, as closed-form full conditional distributions for all model parameters are available. Results from a simulation study demonstrate that our approach successfully recovers the true conditional densities and other regression functionals in challenging scenarios. Applications to three real datasets further underpin the broad applicability of our method. An R package, DDPstar, implementing the proposed method is provided.},
  archive      = {J_SAC},
  author       = {Rodríguez-Álvarez, María Xosé and Inácio, Vanda and Klein, Nadja},
  doi          = {10.1007/s11222-025-10567-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Density regression via dirichlet process mixtures of normal structured additive regression models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel latent class models for cross-classified
categorical data: Model definition and estimation through stochastic EM.
<em>SAC</em>, <em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10579-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an extension of the multilevel latent class model for dealing with multilevel cross-classified categorical data. Cross-classified data structures arise when observations are simultaneously nested within two or more groups, for example, children nested within both schools and neighborhoods. More specifically, we propose extending the standard hierarchical latent class model, which contains mixture components at two levels, say for children and schools, by including a separate set of mixture components for each of the higher-level crossed classifications, say for schools and neighborhoods. Because of the complex dependency structure arising from the cross-classified nature of the data, it is no longer possible to obtain maximum likelihood estimates of the model parameters, for example, using the EM algorithm. As a solution to the estimation problem, we propose an approximate estimation approach using a stochastic version of the EM algorithm. The performance of this approach, which resembles Gibbs sampling, was investigated through a set of simulation studies. Moreover, the application of the new model is illustrated using an Italian dataset on the quality of university experience at degree programme level, with degree programmes nested in both universities and fields of study.},
  archive      = {J_SAC},
  author       = {Columbu, S. and Piras, N. and Vermunt, J. K.},
  doi          = {10.1007/s11222-025-10579-w},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Multilevel latent class models for cross-classified categorical data: Model definition and estimation through stochastic EM},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control charts for monitoring weibull quantile under
generalized hybrid and progressive censoring schemes. <em>SAC</em>,
<em>35</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-025-10581-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose control charts to monitor quantiles of Weibull distribution for Type-I generalized hybrid censoring scheme and Type-II progressive censoring scheme. Parametric bootstrap method is employed to derive the control limits. Monte Carlo simulations are carried out to assess the in-control and out-of-control performance of the proposed charts. The phase-I analysis evaluates the performance of the charts through average run lengths for various combinations of quantile, false-alarm rate, sample size, and censoring parameters. Chart performance is thoroughly investigated in the phase-II analysis for several choices of shifts in the scale and shape parameters of Weibull distribution along with different censoring schemes. The charts for both censoring schemes have been demonstrated to be highly effective in detecting out-of-control signals, both in terms of magnitude and speed. The proposed charts are illustrated through applications in reliability and clinical practices. While both charts show similar performance in terms of speed, the chart with the optimal Type-II progressive censoring scheme outperforms the one with the Type-I generalized hybrid censoring scheme in terms of magnitude in both examples.},
  archive      = {J_SAC},
  author       = {Modok, Bidhan and Chowdhury, Shovan and Kundu, Amarjit},
  doi          = {10.1007/s11222-025-10581-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Control charts for monitoring weibull quantile under generalized hybrid and progressive censoring schemes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using prior-data conflict to tune bayesian regularized
regression models. <em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10582-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional regression models, variable selection becomes challenging from a computational and theoretical perspective. Bayesian regularized regression via shrinkage priors like the Laplace or spike-and-slab prior are effective methods for variable selection in $$p&gt;n$$ scenarios provided the shrinkage priors are configured adequately. We propose an empirical Bayes configuration using checks for prior-data conflict: tests that assess whether there is disagreement in parameter information provided by the prior and data. We apply our proposed method to the Bayesian LASSO and spike-and-slab shrinkage priors in the linear regression model and assess the variable selection performance of our prior configurations through a high-dimensional simulation study. Additionally, we apply our method to proteomic data collected from patients admitted to the Albany Medical Center in Albany NY in April of 2020 with COVID-like respiratory issues. Simulation results suggest our proposed configurations may outperform competing models when the true regression effects are small.},
  archive      = {J_SAC},
  author       = {Biziaev, Timofei and Kopciuk, Karen and Chekouo, Thierry},
  doi          = {10.1007/s11222-025-10582-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Using prior-data conflict to tune bayesian regularized regression models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Directional data analysis: Spherical cauchy or poisson
kernel-based distribution? <em>SAC</em>, <em>35</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s11222-025-10583-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2020, two novel distributions for the analysis of directional data were introduced: the spherical Cauchy distribution and the Poisson kernel-based distribution. This paper provides a detailed exploration of both distributions within various analytical frameworks. To enhance the practical utility of these distributions, alternative parametrizations that offer advantages in numerical stability and parameter estimation are presented, such as implementation of the Newton–Raphson algorithm for parameter estimation, while facilitating a more efficient and simplified approach in the regression framework. Additionally, a two-sample location test based on the log-likelihood ratio test is introduced. This test is designed to assess whether the location parameters of two populations can be assumed equal. The maximum likelihood discriminant analysis framework is developed for classification purposes, and finally, the problem of clustering directional data is addressed, by fitting finite mixtures of Spherical Cauchy or Poisson kernel-based distributions. Empirical validation is conducted through comprehensive simulation studies and real data applications, wherein the performance of the spherical Cauchy and Poisson kernel-based distributions is systematically compared.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail and Papastamoulis, Panagiotis and Kato, Shogo},
  doi          = {10.1007/s11222-025-10583-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Directional data analysis: Spherical cauchy or poisson kernel-based distribution?},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="soco---67">SOCO - 67</h2>
<ul>
<li><details>
<summary>
(2025). Free-text keystroke authentication using transformers: A
comparative study of architectures and loss functions. <em>SOCO</em>,
<em>29</em>(2), 1259–1272. (<a
href="https://doi.org/10.1007/s00500-025-10524-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keystroke biometrics is a promising approach for user identification and verification, leveraging the unique patterns in individuals’ typing behavior. In this paper, we propose a Transformer-based network that employs self-attention to extract informative features from keystroke sequences, surpassing the performance of traditional Recurrent Neural Networks. We explore two distinct architectures, namely bi-encoder and cross-encoder, and compare their effectiveness in keystroke authentication. Furthermore, we investigate different loss functions, including triplet, batch-all triplet, and WDCL loss, along with various distance metrics such as Euclidean, Manhattan, and cosine distances. These experiments allow us to optimize the training process and enhance the performance of our model. To evaluate our proposed model, we employ the Aalto desktop keystroke dataset. The results demonstrate that the bi-encoder architecture with batch-all triplet loss and cosine distance achieves the best performance, yielding an exceptional Equal Error Rate of 0.0186%. Furthermore, alternative algorithms for calculating similarity scores are explored to enhance accuracy. Notably, the utilization of a one-class Support Vector Machine reduces the Equal Error Rate to an impressive 0.0163%. The outcomes of this study indicate that our model surpasses the previous state-of-the-art in free-text keystroke authentication. These findings contribute to advancing the field of keystroke authentication and offer practical implications for secure user verification systems.},
  archive      = {J_SOCO},
  author       = {Momeni, Saleh and BabaAli, Bagher},
  doi          = {10.1007/s00500-025-10524-z},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1259-1272},
  shortjournal = {Soft Comput.},
  title        = {Free-text keystroke authentication using transformers: A comparative study of architectures and loss functions},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing demand forecasting through combination of anomaly
detection and continuous improvement. <em>SOCO</em>, <em>29</em>(2),
1243–1258. (<a
href="https://doi.org/10.1007/s00500-025-10452-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demand forecasting has emerged as a crucial element in supply chain management. It is essential to identify anomalous data and continuously improve the forecasting model with new data. However, existing literature fails to comprehensively cover both aspects of anomaly detection and continuous improvement in demand forecasting. This study proposes an enhanced model to improve accuracy in the demand forecasting. The proposed model introduces a novel data handling method that incorporates an anomaly detection autoencoder, improved with anomaly correction mechanisms. The data handling approach simultaneously detects data anomalies, distinguishes between expected and unexpected anomalies, and corrects anomalous data, ensuring cleaner input for demand forecasting. Then, the proposed model employs a long short-term memory architecture for demand forecasting, enhanced with a continuous improvement method. Thus, the model not only forecasts demand but also retrains the model when the anomaly data surpasses the predetermined threshold, thereby improving the accuracy of forecasting. The results show that the proposed model outperforms other models in detecting data anomalies, achieving an average precision-recall of 0.922, a receiver operating characteristic value of 0.739, and a significance level of less than 0.05. Finally, the model exhibits superior performance in demand forecasting, with average mean squared error, root mean squared error, and mean absolute error values of 33.167, 4.347, and 1.509, respectively, all with a significance level of less than 0.05.},
  archive      = {J_SOCO},
  author       = {Jahani, Meysam and Zojaji, Zahra and Raji, Fatemeh},
  doi          = {10.1007/s00500-025-10452-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1243-1258},
  shortjournal = {Soft Comput.},
  title        = {Enhancing demand forecasting through combination of anomaly detection and continuous improvement},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving trajectory tracking of robot manipulators via PID
control with neural network compensation. <em>SOCO</em>, <em>29</em>(2),
1227–1241. (<a
href="https://doi.org/10.1007/s00500-025-10439-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this manuscript, a PID controller based on adaptive neural networks for manipulating robots with n degrees of freedom is presented. The neural network is given by a two-layer perceptron that compensates for the unknown dynamics of the system. The weights of the output layer are estimated online by the proposed adaptation law, while the weights and thresholds in the hidden layer are random constants. A theoretical contribution of this paper is the rigorous analysis of the introduced control scheme. In addition, to demonstrate the effectiveness of the proposed controller, real-time experiments were carried out in a pendulum and a two degrees of freedom manipulator, where the proposed controller outperforms other PID and neural network controllers previously reported in the literature.},
  archive      = {J_SOCO},
  author       = {Moran-Armenta, Marco and Aguilar-Avelar, Carlos and Gandarilla, Isaac and Meza-Sánchez, Marlen and Moreno-Valenzuela, Javier},
  doi          = {10.1007/s00500-025-10439-9},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1227-1241},
  shortjournal = {Soft Comput.},
  title        = {Solving trajectory tracking of robot manipulators via PID control with neural network compensation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel pruning method driven by similarity of feature
extraction capability. <em>SOCO</em>, <em>29</em>(2), 1207–1226. (<a
href="https://doi.org/10.1007/s00500-025-10470-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning is a method to compress convolutional neural networks, which can significantly reduce the number of model parameters and the computational amount. Current methods that focus on the internal parameters of a model and feature mapping information rely on artificially set a priori criteria or reflect filter attributes by partial feature mapping, which lack the ability to analyze and discriminate the channel feature extraction and ignore the basic reasons for the similarity of the channels. This study developed a pruning method based on similar structural features of channels, called SSF. This method focuses on analysing the ability to extract similar features between channels and exploring the characteristics of channels producing similar feature mapping. First, adaptive threshold coding was introduced to numerically transform the channel characteristics into structural features, and channels with similar coding results could generate highly similar feature mapping. Secondly, the spatial distance was calculated for the structural features matrix to obtain the similarity between channels. Moreover, in order to keep rich channel classes in the pruned network, different class cuts were made on the basis of similarity to randomly remove some of the channels. Thirdly, considering the differences in the overall similarity of different layers, this study determined the appropriate pruning ratio for different layers on the basis of the channel dispersion degree reflected by the similarity. Finally, extensive experiments were conducted on image classification tasks, and the experimental results demonstrated the superiority of the SSF method over many existing techniques. On ILSVRC-2012, the SSF method reduced the floating-point operations (FLOPs) of the ResNet-50 model by 57.70% while reducing the Top-1 accuracy only by 1.01%.},
  archive      = {J_SOCO},
  author       = {Sun, Chuanmeng and Chen, Jiaxin and Li, Yong and Wang, Yu and Ma, Tiehua},
  doi          = {10.1007/s00500-025-10470-w},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1207-1226},
  shortjournal = {Soft Comput.},
  title        = {Channel pruning method driven by similarity of feature extraction capability},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting cervical cancer detection with a multi-stage
architecture and complementary information fusion. <em>SOCO</em>,
<em>29</em>(2), 1191–1206. (<a
href="https://doi.org/10.1007/s00500-025-10491-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer is one of the most fatal and prevalent illnesses affecting women globally. Early detection of cervical cancer is crucial for effective treatment. Pap smear tests are commonly used, but population-based screening is time-consuming, expensive, and requires expert physicians. Computer-Aided Diagnosis (CAD) has shown promise in addressing this challenge. However, accurately predicting the disease using a single model can be difficult due to the complex data patterns involved. This research proposes a multi-stage architecture to improve cervical cancer screening. Initially, three pre-trained models are employed for image classification, after which the proposed advanced fusion technique is applied to combine the predictions. Additionally, we introduce a filtering approach in the third stage to refine the predictions. Unlike traditional fusion methods, the proposed architecture considers the confidence score of the base classifiers in making the final predictions on test samples. To enhance the performance of the models, we incorporate advanced augmentation techniques, including CutMix, CutOut, and MixUp. We assessed the performance of the proposed framework using a 5-fold cross-validation technique on two benchmark datasets. We evaluated the performance of the proposed framework through 5-fold cross-validation on two benchmark datasets. Remarkably, our framework achieved a classification accuracy of 97.62% and an F1-score of 97.64% on the SIPaKMeD dataset, demonstrating its effectiveness in accurately categorizing various cell types in the dataset.},
  archive      = {J_SOCO},
  author       = {Sahoo, Pranab and Saha, Sriparna and Sharma, Saksham Kumar and Mondal, Samrat},
  doi          = {10.1007/s00500-025-10491-5},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1191-1206},
  shortjournal = {Soft Comput.},
  title        = {Boosting cervical cancer detection with a multi-stage architecture and complementary information fusion},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genetic-based approach for vehicle routing problem with
fuzzy alpha-cut constraints. <em>SOCO</em>, <em>29</em>(2), 1169–1189.
(<a href="https://doi.org/10.1007/s00500-025-10465-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s business environment, logistics and supply chain management are especially important for the timely delivery of materials and goods. Delivery must not only be fast, it also needs to be performed within a specific time frame. Therefore, this study examines a vehicle routing problem (VRP) that considers multiple goals and allows vehicles to reach their destinations within a time window with a crashed traveling time. Two objectives are considered, the minimization of total cost and the maximization of customer satisfaction. Firstly, a fuzzy multi-objective linear programming (FMOLP) model with alpha-cut technique and epsilon-constraint method is proposed to transform the multi-objective problem into a single-objective mathematical model, and then an improved genetic algorithm (IGA) is constructed to solve large-scale problems. The performances of the proposed methods are evaluated through several case studies. Design of experiments and sensitivity analysis are applied to evaluate the performance and robustness of IGA. The results show that both the FMOLP and IGA are effective, and the IGA can efficiently obtain a near-optimal solution.},
  archive      = {J_SOCO},
  author       = {Kang, He-Yau and Lee, Amy H. I.},
  doi          = {10.1007/s00500-025-10465-7},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1169-1189},
  shortjournal = {Soft Comput.},
  title        = {A genetic-based approach for vehicle routing problem with fuzzy alpha-cut constraints},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mechanical properties estimation of multi-layer friction
stir plug welded aluminium plates using time-series neural network
models. <em>SOCO</em>, <em>29</em>(2), 1147–1168. (<a
href="https://doi.org/10.1007/s00500-025-10429-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-layer friction stir plug welding can be used to fix the thick aluminium plates. Optical microscopy and tensile tests are utilized to study the microstructural and mechanical characteristics of the welded aluminium plates. However, finding the relation between the indexes of the process and the mechanical properties would be challenging. The present work aims to devise a time-series machine learning model including a recurrent neural network (RNN) and nonlinear autoregressive network with the external state (NARX) to estimate the mechanical properties of the repaired aluminium plate using the force-extension plot. The ultimate tensile strength, yield strength, impact energy and elongation of the repaired aluminium plate can be calculated based on a force-extension plot trained and extracted using the developed networks. In addition, the Bayesian technique is employed to recalculate the optimal hyperparameters of RNN and NARX, targeting the lowest root mean square error (RMSE) between the target and the estimated force during the testing. The investigated methods (RNN and NARX) with the addition of classical estimation methods, including decision tree and support vector regression, are modelled in MATLAB, and the outcomes prove the proposed NARX model efficiency in terms of lower RMSE in comparison with support vector regression, decision tree and RNN.},
  archive      = {J_SOCO},
  author       = {Qazani, Mohammad Reza Chalak and Sajed, Moosa and Pedrammehr, Siamak and Seyedkashi, Seyed Mohammad Hossein},
  doi          = {10.1007/s00500-025-10429-x},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1147-1168},
  shortjournal = {Soft Comput.},
  title        = {Mechanical properties estimation of multi-layer friction stir plug welded aluminium plates using time-series neural network models},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A GA-FGM-RTA combined model for predicting seawall
settlement in under insufficient data volume. <em>SOCO</em>,
<em>29</em>(2), 1133–1146. (<a
href="https://doi.org/10.1007/s00500-025-10496-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One widely concerned issue in the field of seawall settlement prediction is the difficulty in establishing prediction models in the context of insufficient data volume. Fractional-order grey model (FGM), as an extension of the traditional grey model incorporating fractional calculus, has been used to solve the insufficient data volume problem in temporal prediction. Due to the non-integer nature of the fractional order of FGM, traditional parameter estimation methods often lead to increased instability and uncertainty and are no longer applicable, necessitating the adoption of more complex algorithms for estimating the fractional order. To solve this issue, a novel GA-FGM-RTA combined model was proposed for predicting seawall settlement with insufficient data volume, where a genetic algorithm (GA) with enhanced search capabilities was employed to optimal the fractional order and a real-time tracing algorithm (RTA) was applied to provide a dynamic prediction. A case study of Haiyan seawall in Zhejiang province, China was selected to validate the proposed model. We also compared the proposed GA-FGM-RTA model with the fractional-order GM(1,1), integer-order GM(1,1), and fractal theory model. Results exhibit that the proposed GA-FGM-RTA combined model outperforms other relevant models under the same algorithm frame.},
  archive      = {J_SOCO},
  author       = {Qin, Peng and Cheng, Chunmei and Su, Huaizhi},
  doi          = {10.1007/s00500-025-10496-0},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1133-1146},
  shortjournal = {Soft Comput.},
  title        = {A GA-FGM-RTA combined model for predicting seawall settlement in under insufficient data volume},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast resistivity imaging of transient electromagnetic using
an extreme learning machine. <em>SOCO</em>, <em>29</em>(2), 1121–1131.
(<a href="https://doi.org/10.1007/s00500-025-10535-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transient electromagnetic method (TEM) is widely used in geophysical exploration. In TEM data interpretation, nonlinear inversion plays an important role. However, traditional TEM nonlinear inversion adopts the OCCAM imaging method, which merely presents the approximate shape of the stratum model, with poor inversion accuracy and much iteration time. To solve the above problems, a novel nonlinear inversion approach based on extreme learning machine (ELM) is proposed in this paper. The ELM is required to establish the input–output mapping relationship of the inversion network only once through the analytical method, which is different from the traditional neural network method that demands iterative gradient learning and is prone to fall into the local optimum. Moreover, the ELM inversion network by randomly assigning the hidden layer parameters is capable of mapping the observed TEM data and quickly producing resistivity images, which avoids time-consuming iteration and inversion calculations. The presented approach is applied to both synthetic and field examples. The results show that compared with the traditional nonlinear inversion algorithms (BP and OCCAM), the proposed method achieves better inversion accuracy and significantly reduces the calculation time, which verifies the effectiveness of the ELM algorithm for TEM data interpretation. Additionally, the research provides a new method and technology for TEM data inversion.},
  archive      = {J_SOCO},
  author       = {Li, Ruiyou and Zhang, Yong and Li, Guang and Li, Ruiheng and Hu, Jia and Li, Min},
  doi          = {10.1007/s00500-025-10535-w},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1121-1131},
  shortjournal = {Soft Comput.},
  title        = {Fast resistivity imaging of transient electromagnetic using an extreme learning machine},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distance based similarity measure on trapezoidal
intuitionistic fuzzy numbers and its applications. <em>SOCO</em>,
<em>29</em>(2), 1107–1119. (<a
href="https://doi.org/10.1007/s00500-025-10427-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are numerous uses for trapezoidal intuitionistic fuzzy sets (TraIFSs), which include membership and non-membership functions in the form of trapezoidal fuzzy sets (TraFSs), for handling data that is ambiguous. The TraIFS distance based similarity metrics are designed to illustrate the similarities among various categories of sensitive fuzzy data. Nonetheless, certain current similarity metrics fail to satisfy the similarity axioms. Moreover, in other circumstances, they could not be utilized effectively. In this article, a novel distance based similarity measure between any two trapezoidal intuitionistic fuzzy numbers (TraIFNs) is defined and some of its important properties are proved and validated by numerical examples. It consists of two interrelated modules. In the first module consists of distance based similarity measure between TraIFNs and it is used for ranking procedure. For the second module, Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) method is used for real life application problems under TraIFN environment. The effectiveness of the proposed distance based similarity measure between TraIFNs is examined by solving the real life applications such as multi-criteria decision making (MCDM) method, pattern recognition problems and also compared over familiar existing methods. Finally, we obtain a general conclusions and future scope of the proposed method.},
  archive      = {J_SOCO},
  author       = {Dhanasekaran, P. and Kalidasan, S.},
  doi          = {10.1007/s00500-025-10427-z},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1107-1119},
  shortjournal = {Soft Comput.},
  title        = {A distance based similarity measure on trapezoidal intuitionistic fuzzy numbers and its applications},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel cluster based reliable security enhancement in FANET
directed by game theory. <em>SOCO</em>, <em>29</em>(2), 1091–1106. (<a
href="https://doi.org/10.1007/s00500-025-10502-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of interconnected UAVs has given rise to the creation of flying ad hoc networks (FANETs) aimed at efficiently facilitating network-dependent services. However, FANET encountered considerable challenges in achieving reliability due to security issues influenced by the existence of malicious nodes. These nodes continuously engage in communication and present a significant threat. These issues are precisely addressed in this article by introducing a novel methodology enhancing network security through a game theory-driven decision tree approach. The proposed strategy involves the implementation of a cluster mechanism based on path similarity techniques. Additionally, an optimized strategy put forth to minimize cluster overhead. Comparing with state-of-the-art protocols in identifying malicious nodes within the network, the results indicate a notable enhancement averaging at 52.84%. Moreover, in aspects such as delay reduction, precision rate increase, and message drop minimization, the experimental outcomes exhibit average improvements of 42.17%, 65.24%, and 46.63%, respectively.},
  archive      = {J_SOCO},
  author       = {Gupta, Shikha and Sharma, Neetu},
  doi          = {10.1007/s00500-025-10502-5},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1091-1106},
  shortjournal = {Soft Comput.},
  title        = {A novel cluster based reliable security enhancement in FANET directed by game theory},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear convergence factor-based manta ray foraging
optimization algorithm for combined economic emission dispatch problem.
<em>SOCO</em>, <em>29</em>(2), 1053–1089. (<a
href="https://doi.org/10.1007/s00500-025-10402-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization objective of Combined Economic Emissions Dispatch (CEED) must simultaneously ensure that full generation costs and pollutant emissions are minimized to meet load demands and constraints. Though the CEED problem itself has multi-objective to be optimized (e.g., fuel cost and emissions), it can be converted into a single objective with a price penalty factor. A nonlinear convergence factor-based manta ray foraging optimization algorithm was proposed for solving the CEED problems. Six nonlinear convergence functions, including the Gaussian function, sine function, cosine function, tangent function, power function and exponential function (named S1MRFO ~ S6MRFO), are introduced in the spiral foraging behavior stage based on the hypotrochoid manta ray foraging optimization algorithm (HYMRFO). These modifications enhance the MRFO algorithm&#39;s search capabilities and avoid getting trapped in local minima. The CEC-BC-2017 benchmark functions were used to examine the performance of six revised mathematical spiral feeding strategies for MRFO algorithm, and the S6MRFO algorithm with the best results was chosen. Combining the S6MRFO algorithm with other intelligent optimization algorithms, such as the Grey Wolf Optimizer (GWO), Arithmetic Optimization Algorithm (AOA), Multi-Verse Optimizer (MVO), Harris Hawk Optimization (HHO), Whale Optimization Algorithm (WOA), Sine Cosine Algorithm (SCA) and Ant Lion Optimizer (ALO) are compared together for optimal performance. Finally, the CEED cases with twenty units of 2500 MW and six units with four different loads (150 MW, 175 MW, 200 MW, and 225 MW) were solved by MRFO algorithm based on nonlinear convergence factors. The simulation results show that in all four test cases, the suggested strategy has the lowest fuel cost and the fewest hazardous emissions, where total costs are about 3% less than PSO and emissions are about 5.6% less.},
  archive      = {J_SOCO},
  author       = {Zhang, Xing-Yue and Wang, Jie-Sheng and Zhu, Jun-Hua and Bao, Yin-Yin and Zheng, Yue and Hao, Wen-Kuo},
  doi          = {10.1007/s00500-025-10402-8},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1053-1089},
  shortjournal = {Soft Comput.},
  title        = {Nonlinear convergence factor-based manta ray foraging optimization algorithm for combined economic emission dispatch problem},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global vision, local focus: The semantic enhancement
transformer network for crowd counting. <em>SOCO</em>, <em>29</em>(2),
1035–1052. (<a
href="https://doi.org/10.1007/s00500-025-10506-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic crowd counting has made significant progress in recent years. However, due to the challenge of multi-scale variations, convolutional neural networks (CNNs) with fixed-size kernels cannot effectively address this difficulty, leading to a severe limitation on counting performance. To alleviate this issue, we propose a semantic enhancement Transformer crowd counting network (named SET) to improve the semantic encoding relationships in crowd scenes. The SET integrates global attention from Transformer, learnable local attention, and inductive bias from CNNs into a counting model. Firstly, we introduce an efficient Transformer encoder to extract low-level global features of crowd scenes. Secondly, we propose a hybrid convtrans module to dynamically learn appropriate weights for different regions, aiding in enhancing the model’s global visual understanding. Finally, in order to guide the model to focus better on crowd regions, we jointly employ a segmentation attention module and a feature aggregation module to aggregate semantic and spatial features at multiple levels, thus obtaining finer-grained features. We conduct extensive experiments on four challenging datasets, including ShanghaiTech Part A/B, UCF-QNRF, and JHU-Crowd++, achieving excellent results.},
  archive      = {J_SOCO},
  author       = {Wang, Mingtao and Zhou, Xin and Chen, Yuanyuan},
  doi          = {10.1007/s00500-025-10506-1},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1035-1052},
  shortjournal = {Soft Comput.},
  title        = {Global vision, local focus: The semantic enhancement transformer network for crowd counting},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semantic approach for cultural heritage ontology matching
and integration based on textual and multimedia information.
<em>SOCO</em>, <em>29</em>(2), 1019–1034. (<a
href="https://doi.org/10.1007/s00500-025-10517-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, where more and more complex tasks are demanded by automatic agents, often provided with artificial intelligence, the problem of data management and information extraction has assumed an increasingly central role. In particular, the process of obtaining information from data has to face manifold issues due to the large volume and heterogeneity of data sources. In this context, a domain or a concept of the real world can have several representations in different data sources. Therefore, there is a need for methods and tools for automatically reorganizing and reusing information embedded in data. The main purpose of this work is to describe a framework for ontology matching and merging based on a novel approach that exploits multimedia information as a combination of text and images. We also present and discuss a case study on Cultural Heritage to assess and evaluate the effectiveness of our proposed approach.},
  archive      = {J_SOCO},
  author       = {Rinaldi, Antonio Maria and Russo, Cristiano and Tommasino, Cristian},
  doi          = {10.1007/s00500-025-10517-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1019-1034},
  shortjournal = {Soft Comput.},
  title        = {A semantic approach for cultural heritage ontology matching and integration based on textual and multimedia information},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the combined compromise solution method for group
decision-making under intuitionistic fuzziness: An application to
blended english teaching quality evaluation in vocational colleges.
<em>SOCO</em>, <em>29</em>(2), 1005–1017. (<a
href="https://doi.org/10.1007/s00500-025-10417-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the quality of blended English teaching in vocational colleges involves a comprehensive approach that considers instructional design, implementation, student engagement, and learning outcomes. By addressing these dimensions and continuously seeking improvement, vocational colleges can enhance the effectiveness of their blended learning programs. The evaluation of blended English teaching quality in vocational colleges involves a multiple-attribute group decision-making (MAGDM) process. Currently, the CoCoSo approach was utilized to address MAGDM challenges. Intuitionistic fuzzy sets (IFSs) serve as tools to represent uncertain data during the evaluation process. This study introduces the intuitionistic fuzzy number CoCoSo (IFN-CoCoSo) approach, designed to handle MAGDM by incorporating Hamming distance, Euclidean distance and logarithmic distance within IFSs. A numerical study on the evaluation of blended English teaching quality in vocational colleges is provided to validate the IFN-CoCoSo approach. The major contributions of this study are assembled: (1) Extension of the CoCoSo approach to IFSs; (2) Use of entropy to derive weights based on three kinds of distance measures within IFSs; (3) Development of the IFN-CoCoSo approach to manage MAGDM using these distances within IFSs; (4) Provision of a numerical study and comparative analysis to validate the proposed approach for evaluating blended English teaching quality in vocational colleges.},
  archive      = {J_SOCO},
  author       = {Xie, Bin and Yuan, Hongmiao},
  doi          = {10.1007/s00500-025-10417-1},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {1005-1017},
  shortjournal = {Soft Comput.},
  title        = {Enhancing the combined compromise solution method for group decision-making under intuitionistic fuzziness: An application to blended english teaching quality evaluation in vocational colleges},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling of the blockchain-empowered cloud 4D printing
services collaboration digital twin platform oriented on supply–demand.
<em>SOCO</em>, <em>29</em>(2), 977–1004. (<a
href="https://doi.org/10.1007/s00500-025-10461-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the widespread adoption of digital twin and blockchain technologies within the frameworks of Industry 4.0 and intelligent manufacturing has intensified the emphasis on globalization and social collaborative manufacturing. This study aims to address the fragmentation of global information and enhance the collaborative utilization and optimal sharing of underutilized manufacturing resources and capabilities through the development of the C4DPSC_DT platform. To meet these demands, our research focuses on the domain of 4D printing and introduces the blockchain-empowered C4DPSC_DT platform, which is based on dynamic supply and demand principles. This platform is meticulously analyzed to unveil its complex nature, characterized by the integration of physical entities, virtual twin representations, and collaborative services, forming five distinct attributes under this triadic fusion. To further enhance the adaptability of the C4DPSC_DT platform, we propose an adaptive collaborative microservice architecture. Additionally, to emphasize the collaborative service attributes of the platform, we introduce the blockchain-empowered C4DPSC framework. This framework incorporates a collaborative multi-chain blockchain structure and the HotStuff consensus algorithm, facilitating the creation of a secure, distributed, traceable, and transparent environment for sharing C4DP resources and collaboration in C4DP services, thereby establishing decentralized and trustworthy collaborative service connections. Moreover, we systematically decompose the collaborative process into five stages, providing a comprehensive lifecycle solution for the systematic analysis and understanding of the 4D Printing Service collaboration process. Finally, through detailed case studies, we empirically validate the exceptional performance of the C4DPSC_DT platform, confirming its superior functionality and robustness.},
  archive      = {J_SOCO},
  author       = {Liu, Jiajia and Zainudin, Edi Syams and As’arry, Azizan Bin and Ismai, Mohd Idris Shah Bin and Zhang, Chenglei},
  doi          = {10.1007/s00500-025-10461-x},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {977-1004},
  shortjournal = {Soft Comput.},
  title        = {Modeling of the blockchain-empowered cloud 4D printing services collaboration digital twin platform oriented on supply–demand},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear complex dynamic system identification based on a
novel recurrent neural network. <em>SOCO</em>, <em>29</em>(2), 957–976.
(<a href="https://doi.org/10.1007/s00500-025-10457-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed a novel Modified Jordan Recurrent Neural Network (MJRNN) model to identify complex nonlinear dynamical systems. Due to its capabilities, nonlinear dynamic system identification using artificial neural networks is the most commonly used method in control system engineering. The structure of the proposed model is an extended version of the original Jordan recurrent neural network model. The parameter update equations are obtained using the back-propagation optimization algorithm, the most frequently used method as a learning approach for the training of the proposed model’s parameters. The effectiveness of the proposed neural network model is evaluated in comparison to other neural network models such as the Jordan recurrent neural network (JRNN), Elman recurrent neural network (ERNN), Diagonal recurrent neural network (DRNN), and feed-forward neural network (FFNN) models. The robustness of the proposed model is also tested with parameter variation and disturbance signals. The simulation results have shown that the proposed model performs better than the other neural network models.},
  archive      = {J_SOCO},
  author       = {Saini, Kartik and Kumar, Narendra and Bhushan, Bharat and Kumar, Rajesh},
  doi          = {10.1007/s00500-025-10457-7},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {957-976},
  shortjournal = {Soft Comput.},
  title        = {Nonlinear complex dynamic system identification based on a novel recurrent neural network},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse fuzzy graph colouring and its application.
<em>SOCO</em>, <em>29</em>(2), 945–956. (<a
href="https://doi.org/10.1007/s00500-025-10460-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel colouring method called I-fuzzy graph colouring for inverse fuzzy graphs. It defines the chromatic number of inverse fuzzy graphs and explores the relationship between the resultant I-fuzzy graph’s chromatic number and those of individual inverse fuzzy graphs. Various operations, including I-fuzzy union, I-fuzzy intersection, quasi I-fuzzy union, and restricted I-fuzzy union, are employed to establish these relationships. Moreover, we investigate the chromatic number relationship by replacing edge values. The practical application of I-fuzzy graph colouring is demonstrated in determining the simultaneous intake of drugs. Additionally, we observe the MCA problem in inverse fuzzy graphs by considering only I-fuzzy graph colouring functions.},
  archive      = {J_SOCO},
  author       = {Purakkal, Kadeeja Mole Koyalinte and Kalathodi, Sameena},
  doi          = {10.1007/s00500-025-10460-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {945-956},
  shortjournal = {Soft Comput.},
  title        = {Inverse fuzzy graph colouring and its application},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy-bayesian belief network approach to compute
efficiency as a metric for IoT systems. <em>SOCO</em>, <em>29</em>(2),
933–944. (<a href="https://doi.org/10.1007/s00500-025-10510-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In IoT applications, a multitude of devices communicate through interconnected networks, collaborating on decision-making tasks. Ensuring the efficiency of such IoT systems is crucial and requires addressing the management of resources. The article primarily focuses on the computation of efficiency in IoT applications, specifically considering metrics such as availability, functional correctness, and throughput. These metrics directly correlate with packet transfer information. Our main objective is to establish the efficiency of an IoT application by employing metrics related to packet transfer rate, utilizing a fuzzy-based Bayesian belief model. Key metrics such as availability, functional correctness, and throughput are commonly utilized to gauge the performance of packet transfer. To achieve this, we propose a Bayesian Belief Network (BBN) model that incorporates these metrics. To calculate efficiency, we have employed a fuzzy-based approach within the Fuzzy Inference System (FIS), which allows for efficient computation when crisp values of availability, functional correctness, and throughput are provided.},
  archive      = {J_SOCO},
  author       = {Pandey, Rishabh Deo and Snigdh, Itu},
  doi          = {10.1007/s00500-025-10510-5},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {933-944},
  shortjournal = {Soft Comput.},
  title        = {A fuzzy-bayesian belief network approach to compute efficiency as a metric for IoT systems},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing teaching learning based optimization algorithm
through group discussion strategy for CEC 2017 benchmark problems.
<em>SOCO</em>, <em>29</em>(2), 895–932. (<a
href="https://doi.org/10.1007/s00500-025-10409-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristics are utilized to handle challenging optimization problems, since conventional optimization techniques for such problems often fail or become stuck in the local optimum. Teaching-learning based optimization algorithm (TLBO) is a prominent meta-heuristic that mimics the teaching-learning process. It is initially employed to tackle unconstrained optimization problems. The main obstacle to meta-heuristics is premature convergence. The strategy of forming groups of students is introduced in the teaching-learning process for mutual discussions and joint projects. Along with some advantages, like increased creativity, diversity of ideas, access to new information, and more critical thinking, the group discussion strategy also has few disadvantages, such as disagreements over ideas, group size sensitivity, dependency on others, and lengthened the completion time. If properly implemented, the strategy can be a useful tool in the teaching-learning process. To increase an algorithm local search capabilities and to produce solutions of diverse nature, it is essential to have sufficient knowledge about the optimum solution with enough population diversity. Through group discussion sufficient information with diverse views about the solution of the problem is obtained. Therefore, in this work, the group discussion strategy is embedded to the learner phase of TLBO to enhance it. In the strategy, the group of randomly selected individuals correspond to each candidate solution is made to minimize the happening of premature convergence. This strategy depends on the group size parameter; for which the sensitivity analysis is also performed to obtain the right/optimal value. The suggested algorithm is known as GTLBO, and the group size parameter sensitivity analysis generates its four versions, referred to as GTLBO2–GTLBO5, where the digits 2–5 represent the group size value. The performance of the proposed algorithms is evaluated by the unconstrained benchmark problems of CEC 2017. The comparison of the obtained simulations’ results of the newly designed and some state-of-the-art algorithms on the tested problems exhibits that the suggested algorithms take the first, third, fourth, and fifth ranks in the top five ranks, which highlights the importance of the introduced strategy in the sense of enhancing TLBO.},
  archive      = {J_SOCO},
  author       = {Sagheer, Muhammad and Asif Jan, Muhammad and Shah, Zahir and Mashwani, Wali Khan and Adeeb Khanum, Rashida and Shutaywi, Meshal},
  doi          = {10.1007/s00500-025-10409-1},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {895-932},
  shortjournal = {Soft Comput.},
  title        = {Enhancing teaching learning based optimization algorithm through group discussion strategy for CEC 2017 benchmark problems},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-based optimization in epidemics prevention.
<em>SOCO</em>, <em>29</em>(2), 875–893. (<a
href="https://doi.org/10.1007/s00500-025-10494-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a method for knowledge-based optimization of vaccination assignments is proposed, which combines multiobjective optimization algorithms with counter-epidemic strategies known from epidemiology. In the paper, a model based on real-life illness costs is used, which allows taking into account the age of individuals exposed to a simulated epidemic. Using this model, strategies based on the age and on the graph node degree (the number of contacts each individual has) are studied. The optimization algorithms work on a graph which represents a “known” network of contacts and the solutions are subsequently used for controlling an epidemic spreading on another graph representing an “actual” network of contacts. The optimized solutions are tested on “actual” graphs with a varying degree of overlap with the “known” graph on which the optimization algorithms work. In the experiments, the age-based strategy was found to perform best, and the degree-based strategy turned out to be the second-best approach. Both these strategies outperformed other approaches not using knowledge from epidemiology to improve optimization results.},
  archive      = {J_SOCO},
  author       = {Michalak, Krzysztof},
  doi          = {10.1007/s00500-025-10494-2},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {875-893},
  shortjournal = {Soft Comput.},
  title        = {Knowledge-based optimization in epidemics prevention},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new mathematical optimization-based method for the
m-invariance problem. <em>SOCO</em>, <em>29</em>(2), 861–873. (<a
href="https://doi.org/10.1007/s00500-025-10514-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy preserving dynamic data publication aims at protecting data while simultaneously preserving its utility when the data is published dynamically. For static data (i.e., data published only once), privacy is based on concepts such as k-anonymity and $$\epsilon $$ -differential privacy. In contrast, for dynamic data, the notions of m-invariance and $$\tau $$ -safety are considered. However, most current approaches focus solely on guaranteeing m-invariance and $$\tau $$ -safety without paying attention to the quality of the solution, such as maximizing utility. We propose a new heuristic approach for the NP-hard combinatorial problem of m-invariance and $$\tau $$ -safety, which is based on a mathematical optimization column generation scheme. The quality of a solution to m-invariance and $$\tau $$ -safety can be measured by the Information Loss (IL), a value in [0, 100], the closer to 0 the better. We show that our approach improves by far current heuristics, reducing IL by more than $$60\%$$ and, in some instances, by more than $$95\%$$ .},
  archive      = {J_SOCO},
  author       = {Tobar Nicolau, Adrián and Castro, Jordi and Gentile, Claudio},
  doi          = {10.1007/s00500-025-10514-1},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {861-873},
  shortjournal = {Soft Comput.},
  title        = {A new mathematical optimization-based method for the m-invariance problem},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Period regulated particle swarm optimization algorithm.
<em>SOCO</em>, <em>29</em>(2), 839–860. (<a
href="https://doi.org/10.1007/s00500-025-10428-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population cycles refer to the regular fluctuations in population size over time. Inspired by this phenomenon, we introduce a period-regulating factor into the particle swarm optimization (PSO) algorithm and propose a modified period-regulated particle swarm optimization (PPSO) algorithm. This algorithm allows for dynamic parameter adjustments in each particle, maintaining a balance between exploitation and exploration. To enhance the diversity of the swarm, we introduce a selection mechanism that enables particles to choose between the global optimum and a new learning object called the mean optimum. Additionally, we incorporate a multi-particle mutation mechanism to improve the particles’ ability to escape local optima. A set of benchmark functions and classical engineering problems are used to verify the superiority of the PPSO algorithm. The results show that the PPSO can provide a very competitive performance compared to some popular PSO variants and meta-heuristic algorithms. Furthermore, this algorithm retains the advantages of simplicity of implementation inherent in PSO.},
  archive      = {J_SOCO},
  author       = {Liu, Zhilong and Jiang, Huhai},
  doi          = {10.1007/s00500-025-10428-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {839-860},
  shortjournal = {Soft Comput.},
  title        = {Period regulated particle swarm optimization algorithm},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved snake optimizer based on forced switching mechanism
and variable spiral search for practical applications problems.
<em>SOCO</em>, <em>29</em>(2), 803–838. (<a
href="https://doi.org/10.1007/s00500-025-10404-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an improved snake optimizer (ISO) enhances snake optimizer (SO) by introducing novel mechanisms for improving convergence ability and stability. Based on the “No Free Lunch” theory, this paper discusses the development status of swarm intelligence algorithms. ISO introducing a chaotic mapping strategy allows the population to generate uniform and randomly distributed initial values at the beginning of ISO. A forced switching mechanism improving the balance between two different updating phases is introduced to address the slow convergence rate based on SO. A variant strategy of whale optimization algorithm is used to make further improvement for exploration of SO. The optimal domain perturbation strategy is used to find a better value near the result after above process. These enhancements result a more robust and effective optimization framework suiting to solve complex continuous optimization problems across various domains. The convergence ability and stability are validated by comparing ISO with SO and eight other classical or latest algorithms with dimensionality setting as Dim = 10 and Dim = 20 in the test functions from CEC-2021. The convergence efficiency achieves 94/160 and the convergence stability achieves 77/160. The ratio of convergence efficiency achieves $$100\%$$ and the ratio of convergence stability achieves $$96.25\%$$ in the applicable functions which are half of CEC-2021 functions. They represent the problems which ISO performs well. The practicality of ISO is further illustrated using three continuous optimization problems. Based on the simulation results, the discretization of ISO is prospected to solve combinatorial optimization problems.},
  archive      = {J_SOCO},
  author       = {Wang, Yanfeng and Xin, Bingqing and Wang, Zicheng and Sun, Junwei},
  doi          = {10.1007/s00500-025-10404-6},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {803-838},
  shortjournal = {Soft Comput.},
  title        = {Improved snake optimizer based on forced switching mechanism and variable spiral search for practical applications problems},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spark workflow task scheduling with deadline and privacy
constraints in hybrid cloud networks. <em>SOCO</em>, <em>29</em>(2),
783–801. (<a href="https://doi.org/10.1007/s00500-025-10486-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing adoption of hybrid clouds in organizations stems from their ability to bolster private cloud resources with additional public cloud capacity when required. However, scheduling distributed applications, such as workflow tasks, on hybrid cloud resources presents new and intricate challenges. A significant concern revolves around the potential exposure of private data and tasks within third-party, public cloud infrastructures, especially within sensitive domains like healthcare applications. The complexity escalates when considering the selection of resources from multiple cloud providers due to the fluctuating resource computation prices and data transmission costs. This paper presents the Spark Workflow Task Scheduling to Hybrid Cloud (SWSHC) framework, designed to schedule Spark workflows precisely while adhering to deadline and task privacy constraints within a hybrid cloud setting. Our innovative approach encompasses developing and implementing three pivotal components: deadline division, stage order optimization, and task scheduling mechanisms. We segregate the workflow deadline for each stage to bridge the gaps between stages effectively. Additionally, job prioritization is achieved using the maximum rank rule. The proposed solution considers diverse factors, including interval pricing variations, utilization of heterogeneous VM instances, intra- and inter-bandwidth considerations, and the efficient utilization of private cloud resources. Through meticulous calibration of our algorithm and comprehensive experimentation with various realistic workflows, our findings unequivocally demonstrate that SWSHC surpasses existing solutions in the current literature the cost by up to 40–70% in terms of cost efficiency and resource utilization.},
  archive      = {J_SOCO},
  author       = {Rajput, Kamran Yaseen and Xiaoping, Li and Lakhan, Abdullah},
  doi          = {10.1007/s00500-025-10486-2},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {783-801},
  shortjournal = {Soft Comput.},
  title        = {Spark workflow task scheduling with deadline and privacy constraints in hybrid cloud networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fault-tolerant scheduling strategy through proactive and
clustering techniques for scientific workflows in cloud computing.
<em>SOCO</em>, <em>29</em>(2), 755–781. (<a
href="https://doi.org/10.1007/s00500-025-10485-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing offers solutions for various scientific and business applications. Large-scale scientific applications, which are organized as scientific workflows, are carried out using cloud computing. However, the higher failure rates in cloud computing can be attributed to the numerous servers and components dealing with intense workloads. This study presents a fault-tolerant scheduling approach using proactive and clustering methods for scientific workflows in cloud computing. Initially, the task clustering issue is addressed by consolidating multiple short-duration tasks into a single job to improve the runtime performance of workflow executions. Subsequently, an automated workflow scheduling strategy is outlined with four key stages: monitoring, analysis, planning, and execution. During monitoring, clustered jobs and the capacities of available cloud resources are observed. In the analysis phase, the accuracy of failure prediction is enhanced by employing the Group Method of Data Handling (GMDH) neural network prior to any faults or failures. The planning stage introduces a novel hybrid multi-objective algorithm, MOPSO-aSA, based on MOPSO and adaptive simulated annealing (SA), to streamline workflow scheduling in error-prone execution environments. Moreover, the reliability of application execution is maintained through re-clustering and migration techniques following any faults or failures. Finally, based on the experimental findings, it is evident that the proposed strategy surpasses other methods in terms of makespan, total cost, energy consumption, and failure rate.},
  archive      = {J_SOCO},
  author       = {Farhood, Suha Mubdir and Khorsand, Reihaneh and Hussein, Nashwan Jasim and Ramezanpour, Mohammadreza},
  doi          = {10.1007/s00500-025-10485-3},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {755-781},
  shortjournal = {Soft Comput.},
  title        = {A fault-tolerant scheduling strategy through proactive and clustering techniques for scientific workflows in cloud computing},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A surrogate archive assisted multi-objective evolutionary
algorithm under limited computational budget. <em>SOCO</em>,
<em>29</em>(2), 723–753. (<a
href="https://doi.org/10.1007/s00500-025-10432-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of archives has received significant attention within the realm of multi-objective optimization. However, there exists a substantial gap in exploring archives within surrogate-assisted multi-objective evolutionary algorithms (SAMOEAs). In this paper, a novel framework called surrogate archive assisted multi-objective evolutionary algorithm (SAAMOEA) is introduced. The framework uses a surrogate archive to guide the evolution of the algorithm. This archive serves as an external database to store individuals that perform better during evolution, reducing the randomness of the individuals. Firstly, the surrogate archive is initialized using multiple MOEAs, and the surrogate constructed by the individuals in this initialization archive are used to approximate the exact function. Secondly, a cluster-based model management strategy is proposed that employs K-means method to fit the Pareto front and identify infilled individuals. Simultaneously, an environment selection method is proposed. The method determines whether infilled individuals are used to update the surrogate by calculating a convergence quality metric. The quality of the individuals in the surrogate archive is maintained inversely through the two strategies described above to provide conditions for accurate model construction. Furthermore, the paper presents three hypotheses about the relationship between the Pareto front of the surrogate model and the Pareto front of the exact function evaluation. The SAAMOEA was compared to ten state-of-the-art (SOTA) algorithms, and simulation experiments were conducted on 12 benchmark functions to obtain non-parametric estimation results and distribution plots of the true PF. Additionally, SAAMOEA was validated on an airfoil design optimization problem. The HV results and optimized parameter results demonstrate the performance advantages of the algorithm.},
  archive      = {J_SOCO},
  author       = {Wang, Le and Fan, Qinqin and Yan, Xuefeng},
  doi          = {10.1007/s00500-025-10432-2},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {723-753},
  shortjournal = {Soft Comput.},
  title        = {A surrogate archive assisted multi-objective evolutionary algorithm under limited computational budget},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing voltage stability and load shedding optimization
through a fusion of gravitational search algorithm and particle swarm
optimization with deep learning. <em>SOCO</em>, <em>29</em>(2), 701–721.
(<a href="https://doi.org/10.1007/s00500-025-10447-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, A novel approach for optimizing load shedding during power system stress conditions is introduced by combining gravitational search and particle swarm optimization (GSA-PSO) with Deep Learning. This approach aims to determine the most effective load-shedding strategy for specific buses, to prevent revenue loss and voltage instability in power systems. The smallest eigenvalue sensitivity of the load flow Jacobian matrix as an indicator is used to identify the buses for load shedding. Furthermore, inequality constraints are calculated for the current operational state and the anticipated load in the next interval. To evaluate the performance of the proposed approach, experiments were conducted on two different power systems: the IEEE 30-bus and 14-bus systems. The GSAPSO-Deep Learning approach was implemented in the Python environment. The results obtained using the proposed method were compared with those of other models, as well as their variants, using statistical inference.},
  archive      = {J_SOCO},
  author       = {Ahmadipour, Masoud and Ali, Zaipatimah and Ridha, Hussein Mohammed},
  doi          = {10.1007/s00500-025-10447-9},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {701-721},
  shortjournal = {Soft Comput.},
  title        = {Enhancing voltage stability and load shedding optimization through a fusion of gravitational search algorithm and particle swarm optimization with deep learning},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning for healthcare 5.0: A comprehensive
survey, taxonomy, challenges, and solutions. <em>SOCO</em>,
<em>29</em>(2), 673–700. (<a
href="https://doi.org/10.1007/s00500-025-10508-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Healthcare 5.0 heralds a groundbreaking revolution in digital healthcare, superseding the achievements of its predecessor, Healthcare 4.0. Integrating cutting-edge technologies such as the Internet of Medical Things (IoMT), smart wearables, and the extraordinary capabilities of Artificial Intelligence (AI), Healthcare 5.0 envisions a unified framework that grants seamless access to health records, fosters interconnectedness among individuals, resources, and institutions, and empowers intelligent responses to medical concerns. However, the realization of Healthcare 5.0 faces a significant challenge in the form of high-speed data transmission using smart devices. Conventional AI approaches relying on centralized data processing raise compelling concerns surrounding information privacy and scalability within the Healthcare 5.0 context. Amidst this backdrop, federated learning emerges as a beacon of hope, offering a decentralized AI paradigm that facilitates on-device machine learning without compromising end-user privacy through centralized data export. Safeguarding data integrity, federated learning holds the key to unlocking the full potential of Healthcare 5.0. In this pioneering study, we conduct an extensive survey, exploring the transformative implications of federated learning within the realm of Healthcare 5.0. By shedding light on recent advancements tailored to this paradigm, we delve into the fundamental concepts of resource-awareness, privacy preservation, incentivization, and personalization, all within the framework of federated learning. Moreover, we meticulously scrutinize key parameters including security, sparsification, quantization, robustness, scalability, and privacy, providing an authentic evaluation of the current progress in federated learning for Healthcare 5.0. This comprehensive survey serves as an indispensable cornerstone for the evolution of Healthcare 5.0, offering invaluable insights into its unique requirements and untapped potential. By harnessing the capabilities of federated learning in this context, we envisage a transformative era in digital healthcare, fostering a more interconnected, secure, and intelligent healthcare landscape for the betterment of society.},
  archive      = {J_SOCO},
  author       = {Amin, Muhammad Sadiq and Ahmad, Shabir and Loh, Woong-Kee},
  doi          = {10.1007/s00500-025-10508-z},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {673-700},
  shortjournal = {Soft Comput.},
  title        = {Federated learning for healthcare 5.0: A comprehensive survey, taxonomy, challenges, and solutions},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STA-CN-BiGRU: A spatial-temporal attention based ChebNet and
BiGRU model for traffic flow prediction. <em>SOCO</em>, <em>29</em>(2),
663–672. (<a href="https://doi.org/10.1007/s00500-025-10464-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is critical to the collaborative decision support function of traffic management. However, it remains a challenging problem because of the inherent nonlinear dynamics, stochasticity, and spatial-temporal correlation of traffic flows. In this paper, a traffic flow prediction model called Spatial-Temporal Attention based ChebNet and BiGRU model (STA-CN-BiGRU), which consists of several layers of ST-Residual Block, is proposed. Firstly, ChebNet is used to learn the spatial dependence of traffic flow caused by the topology of the road network. BiGRU is combined with CNN for capturing the spatial and multi-scale temporal correlation of traffic flows. Then, in order to incorporate the different importance levels of temporal and spatial dependencies for each node, a hybrid spatio-temporal attention module is incorporated into the traffic flow prediction model and then a ST-Residual Block is constructed. Further, the outputs of multiple component traffic flows are weighted fused to obtain the final predictions. Finally, the performance of the proposed model has been extensively evaluated on real-world datasets in terms of one-step ahead prediction, multi-step ahead prediction, and ablation study. The results show that the proposed model can improve the prediction accuracy over the baselines, especially outperforming other models in long-term prediction.},
  archive      = {J_SOCO},
  author       = {Huifang, Feng and Rui, Yang},
  doi          = {10.1007/s00500-025-10464-8},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {663-672},
  shortjournal = {Soft Comput.},
  title        = {STA-CN-BiGRU: A spatial-temporal attention based ChebNet and BiGRU model for traffic flow prediction},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trustworthy deep learning for encrypted traffic
classification. <em>SOCO</em>, <em>29</em>(2), 645–662. (<a
href="https://doi.org/10.1007/s00500-025-10462-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network traffic classification refers to the identification of collected network traffic data of various applications, which is widely used in research fields such as network resource allocation, traffic scheduling and intrusion detection systems. With the widespread application of encryption technology in the network, encrypted traffic classification has become a hot research topic. At present, most existing methods only focus on the accuracy of network traffic classification. Yet, few work studies the reliability of the classification model, which plays an important role in network regulation and network security. In this paper, we propose a novel traffic classification method based on trustworthy deep learning model, which can effectively improve the reliability of encrypted traffic classification models by correcting the confidence of model output. Specifically, we firstly perform data preprocessing on the original network traffic, and then adopt a ConvNet for feature learning and a ClassifyNet for traffic classification in the initial stage. At the same time, we utilize a trustworthy confidence criterion to design a ConfidNet trained according to the probability of the true class. The ConfidNet can provide a reliable confidence measure for the prediction of the classification model. Finally, we demonstrate the effectiveness of our framework through comprehensive experiments on two benchmark datasets ISCX VPN-nonVPN and USTC-TFC2016, and show that our method can improve the reliability of the classification model and has a good ability to identify misclassified samples compared with state-of-the-art methods.},
  archive      = {J_SOCO},
  author       = {Li, Zheng and Liu, Yanbei and Zhang, Changqing and Shan, Wanjin and Zhang, Haifeng and Zhu, Xiaoming},
  doi          = {10.1007/s00500-025-10462-w},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {645-662},
  shortjournal = {Soft Comput.},
  title        = {Trustworthy deep learning for encrypted traffic classification},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach of multi-channel attention mechanism for
long-sequential multivariate time-series prediction problem.
<em>SOCO</em>, <em>29</em>(2), 629–644. (<a
href="https://doi.org/10.1007/s00500-025-10501-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-dependent and long sequential time-series is always considered as a challenging problem. It normally demands the robust machine learning (ML) or deep learning (DL)-based techniques to efficiently preserve the long-ranged dependencies within intricate time-series datasets. In recent years, transformer-based architectures have emerged as a powerful tool across various data mining fields, particularly in natural language processing (NLP). These architectures are widely regarded for their ability to effectively handle long-range dependencies in sequence-based data structures, such as text and time-dependent datasets. Consequently, transformers have paved the way for advancements in complex long-sequence time-series forecasting tasks. However, recent studies have highlighted limitations in transformer-based predictive models, particularly in their ability to fully capture the joint spatial and temporal representations within input sequences. To address these challenges, we propose a novel approach in this paper, multi-channel attention transformer for time-series (MAT4TS). Our MAT4TS model enhances the performance of long-sequence time-series forecasting by extending the conventional self-attention mechanism. This enhanced self-attention is complemented by additional multi-channel filtering and embedding processes, leveraging convolutional neural network (CNN)-based architectures. By integrating these processes, our model is able to extract richer spatial–temporal information from long-range sequences, ultimately leading to more accurate prediction outcomes. The comprehensive comparative studies within real-world long-sequential time-series datasets demonstrated the effectiveness as well as superiority of our proposed MAT4TS model for dealing with long-ranged time-series prediction problem in comparing with previous state-of-the-art baselines.},
  archive      = {J_SOCO},
  author       = {Vo, Tham and My, Linh Nguyen Thi},
  doi          = {10.1007/s00500-025-10501-6},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {629-644},
  shortjournal = {Soft Comput.},
  title        = {A novel approach of multi-channel attention mechanism for long-sequential multivariate time-series prediction problem},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zoom method for association rules in multi-granularity
formal context. <em>SOCO</em>, <em>29</em>(2), 613–627. (<a
href="https://doi.org/10.1007/s00500-025-10472-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on a granularity tree, this paper investigates the changes in association rules before and after attribute granularity transformation in a formal context. It introduces zoom algorithms to update association rules. The zoom-in algorithm is applied to refine the attribute granularity from coarse to fine, while the zoom-out algorithm achieves the attribute granularity from fine to coarse. These zoom algorithms enable the direct manipulation of association rules in the original formal context, using concepts as a bridge to generate association rules in the new context. This approach eliminates the need for reconstructing the concept lattice when attribute granularity changes. Experimental results demonstrate that the algorithm proposed in this paper significantly reduces computational workload and shortens running time compared to the classical algorithm flow.},
  archive      = {J_SOCO},
  author       = {Niu, Lihui and Mi, Jusheng and Bai, Yuzhang and Li, Zhongling and Li, Meizheng},
  doi          = {10.1007/s00500-025-10472-8},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {613-627},
  shortjournal = {Soft Comput.},
  title        = {Zoom method for association rules in multi-granularity formal context},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust security risk estimation for android apps using
nearest neighbor approach and hamming distance. <em>SOCO</em>,
<em>29</em>(2), 593–611. (<a
href="https://doi.org/10.1007/s00500-025-10489-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Android-based devices such as smart phones, tablets, smart watches, and virtual reality headsets have found increasing use in our daily lives. Along with the development of various applications for these devices, new malicious apps are released by intruders, which are more difficult to identify and deal with because they exploit more sophisticated techniques. Although methods have been provided to calculate the security risk and identify malicious apps in Android operating system, but with the expansion of the level and depth of the threats, the need for more effective methods in this context is still required. In this paper, we have devised a new algorithm to calculate the security risk of Android apps, which can be used to identify malicious apps from benign ones. In this algorithm, to estimate the security risk of an unknown input app, its nearest neighbors to malicious apps and its nearest neighbors to normal apps are computed separately using Hamming distance. Then, the security risk of the input app can be computed using a simple formulation. After implementing this algorithm, its parameter for the number of neighbors using various real datasets is adjusted. The extensive experiments conducted on these data show the superiority of the proposed method over the previously proposed ones in terms of detection rate. Our additional experimentations show the robustness of the proposed algorithm in adversarial situations.},
  archive      = {J_SOCO},
  author       = {Deypir, Mahmood and Zoughi, Toktam},
  doi          = {10.1007/s00500-025-10489-z},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {593-611},
  shortjournal = {Soft Comput.},
  title        = {Robust security risk estimation for android apps using nearest neighbor approach and hamming distance},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the maximization of the likelihood for the generalized
gamma distribution: The modified maximum likelihood approach.
<em>SOCO</em>, <em>29</em>(2), 579–591. (<a
href="https://doi.org/10.1007/s00500-025-10498-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum likelihood (ML) estimation of parameters of the generalized gamma (GG) distribution has been considered in several papers, and some of them stated that the ML estimation has some computational difficulties. Therefore, different approaches including numerical methods have been proposed for the ML estimation of parameters of the GG distribution. However, it is known that using numerical methods may have some drawbacks, e.g., non-convergence of iterations, multiple roots, and convergence to the wrong root. In this study, we rehabilitate the ML procedure via the modified ML (MML) methodology and obtain the likelihood equations in which two of them have explicit solutions, and the remaining one should be solved numerically. Since the MML methodology explicitly solves two of three likelihood equations, the mentioned drawbacks are alleviated. We also propose a simple algorithm to obtain the estimates of the parameters of the GG distribution. Then, the GG distribution is used for modeling the real data sets, and the performance of the proposed algorithm is compared with the Broyden–Fletcher–Goldfarby–Shanno (BFGS) and Nelder–Mead (NM) algorithms. The results show that the proposed algorithm is preferable to the BFGS and NM algorithms in terms of computational sense when considering the GG distribution.},
  archive      = {J_SOCO},
  author       = {Arslan, Talha and Acitas, Sukru and Senoglu, Birdal},
  doi          = {10.1007/s00500-025-10498-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {579-591},
  shortjournal = {Soft Comput.},
  title        = {On the maximization of the likelihood for the generalized gamma distribution: The modified maximum likelihood approach},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale risk spillover analysis of china’s stock market
industry: Evidence supported by a novel hybrid model based on signal
decomposition technology. <em>SOCO</em>, <em>29</em>(2), 559–577. (<a
href="https://doi.org/10.1007/s00500-025-10438-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exists a complex interplay between the aggregate risk of the stock market and the risks inherent to various industries. However, existing studies have largely overlooked the disparities in risk contagion effects between different industries and the overall market across diverse time scales, and there remains instability in delineating risk frequency domains. Consequently, it becomes challenging to fully unravel the intricate correlation dynamics between different industries and the broader stock market across varying temporal dimensions. Therefore, this paper takes the China stock market as a representative of emerging markets, selects eleven distinct industry indices along with the Shanghai-Shenzhen 300 Index (CSI 300 Index), and introduces the Variable Mode Decomposition (VMD) algorithm to extract intricate information from time series data. Additionally, the Fuzzy Entropy (FE) algorithm is employed to effectively reconstruct different frequency domains. Furthermore, this paper integrates the strengths of the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model, the Copula function, and the Conditional Value at Risk (CoVaR) model. By constructing the novel VMD-FE-GARCH-Copula-CoVaR hybrid model, this research aims to explore the risk contagion characteristics of various industries and the Shanghai-Shenzhen 300 Index across different time scales, offering a fresh perspective for the paper of risk contagion within stock market industries.},
  archive      = {J_SOCO},
  author       = {Linjie, Zhan and Zhenpeng, Tang},
  doi          = {10.1007/s00500-025-10438-w},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {559-577},
  shortjournal = {Soft Comput.},
  title        = {Multiscale risk spillover analysis of china’s stock market industry: Evidence supported by a novel hybrid model based on signal decomposition technology},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On hierarchical clustering-based identification of PWA model
with model structure selection and application to automotive actuators
for HiL simulation. <em>SOCO</em>, <em>29</em>(2), 543–558. (<a
href="https://doi.org/10.1007/s00500-025-10458-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of the electronic control unit (ECU) motivates dynamic models with high precision to simulate mechatronic systems for various analysis and design tasks like hardware-in-the-loop (HiL) simulation. Unlike traditional physical models which are established based on the research experience or the mechanics mechanism, in this study, a novel data-driven modeling approach is presented based on the piecewise affine (PWA) identification method. In this work, the highly nonlinear dynamic of automotive actuators is well approximated by the PWA model. To obtain experimental data that can accurately reflect the characteristics of actuators, a test bench was first built. On this basis, the PWA identification of automotive is composed of the data clustering, the model structure selection, and the model parameter estimation. The proposed clustering method improves the widely used balanced iterative reducing and clustering using hierarchies (BIRCH) by introducing a refinement phase for handling clusters with arbitrary shapes. The model structure selection and the parameter estimation are jointly solved by using the optimization method. The presented method is demonstrated with an academic example and an automotive throttle, and the results show that the proposed method can achieve a high model quality, which means that the normalized root mean squared error (NRMSE) is 0.03 and the absolute maximal prediction error can reach 2.43°. Compared to other models like a physical model or a fuzzy model, the improvement using the proposed model can be up to 50%. Thus, the quality of the proposed PWA model is sufficient for HiL simulation.},
  archive      = {J_SOCO},
  author       = {Ren, Zhenxing},
  doi          = {10.1007/s00500-025-10458-6},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {543-558},
  shortjournal = {Soft Comput.},
  title        = {On hierarchical clustering-based identification of PWA model with model structure selection and application to automotive actuators for HiL simulation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unification of methods for new types of fuzzy sets: General
approximation spaces with relational morphisms. <em>SOCO</em>,
<em>29</em>(2), 521–542. (<a
href="https://doi.org/10.1007/s00500-025-10440-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general method that can be used to unify the theoretical results for several new MV-valued fuzzy sets, such as intuitionistic fuzzy sets, neutrosophic fuzzy sets, fuzzy soft sets, or multivalued fuzzy sets and their mutual combinations. The results created in this way have properties identical to analogous results for classic MV-valued fuzzy sets, and it is not necessary to prove these results separately for each new type of fuzzy sets. To illustrate this general method, we will show how two related fuzzy Chang topologies can be derived in these new types of fuzzy sets from general approximation spaces.},
  archive      = {J_SOCO},
  author       = {Močkoř, Jiří},
  doi          = {10.1007/s00500-025-10440-2},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {521-542},
  shortjournal = {Soft Comput.},
  title        = {Unification of methods for new types of fuzzy sets: General approximation spaces with relational morphisms},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addition-meet fuzzy relational inequality systems,
product-join relational inequality systems and their generalizations.
<em>SOCO</em>, <em>29</em>(2), 509–520. (<a
href="https://doi.org/10.1007/s00500-025-10473-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real unit interval valued Addition-min FRI systems are generalized to MV-algebra valued Addition-meet and Product-join fuzzy relational inequality systems, and the latter further to residual lattice valued Product-join relational inequality system. MV-algebra valued Addition-meet FRI systems are further generalized in two ways; on the one hand they are defined on involutive residual lattices and on the other hand as Addition-MV-product systems. It is proved that each such FRI system has a minimal solution or, respectively a maximal solution. The number of such solutions is discussed. Several illustrative examples are given. Through residual lattices and MV-algebras, these systems are linked in a profound way to mathematical fuzzy logic.},
  archive      = {J_SOCO},
  author       = {Turunen, Esko},
  doi          = {10.1007/s00500-025-10473-7},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {509-520},
  shortjournal = {Soft Comput.},
  title        = {Addition-meet fuzzy relational inequality systems, product-join relational inequality systems and their generalizations},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional online sequence extreme learning machine and
switching strategy for soft-sensor model of SMB chromatography
separation process. <em>SOCO</em>, <em>29</em>(2), 485–507. (<a
href="https://doi.org/10.1007/s00500-025-10444-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulated Moving Bed (SMB) chromatography separation is a novel absorptive separation technique with high separation capacity, and it is challenging to make the process run stably at the desired operating point for a long time. Therefore, powerful, and adaptive soft sensor models are important for the overall stability, efficiency, and optimization of the SMB chromatographic separation process. Extreme Learning Machine (ELM), known for its robust generalization capability, can serve as a valuable soft-sensor model for predicting economic and technical indicators such as purity and yield in SMB chromatography separation processes. To improve the prediction accuracy of ELM, avoid stochasticity and learn incrementally, a bidirectional online sequential ELM (BOSELM) is proposed. BOSELM learns data with fixed or varying block sizes on a one-by-one or block-by-block basis (data chunks) without the need to define the size of the network beforehand and determines the output weights based on the analysis of the sequentially arriving data. On the other hand, BOSELM also makes use of the bidirectional ELM&#39;s ability to explore the number of hidden nodes to reduce the number of hidden nodes without affecting the learning efficiency, improving the speed and adaptability of model training. The moving window (MW) strategy was adopted to adaptively correct the BOSELM (MW-BOSELM) to address the issue of decreasing model prediction accuracy caused by changes in process conditions. Additionally, the MW strategy kernel-extreme learning machine (MW-KELM) exhibits higher prediction accuracy than MW-BOSELM at certain instances. To enhance the adaptability of the model, an adaptive hybrid soft-sensor model is proposed to intelligently switch between BOSELM and KELM. Comparative analysis with previous models like BELM, OSELM and BOSELM highlights the superiority of the proposed hybrid adaptive soft-sensor model. Thus, these models help to improve the operational efficiency, product quality and economics of the SMB chromatographic separation process.},
  archive      = {J_SOCO},
  author       = {Sun, Yong-Cheng and Wang, Jie-Sheng and Xing, Cheng and Shang-Guan, Yi-Peng and Wang, Xiao-Tian and Zhang, Song-Bo},
  doi          = {10.1007/s00500-025-10444-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {485-507},
  shortjournal = {Soft Comput.},
  title        = {Bidirectional online sequence extreme learning machine and switching strategy for soft-sensor model of SMB chromatography separation process},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ring structure of rough sets. <em>SOCO</em>, <em>29</em>(2),
471–483. (<a href="https://doi.org/10.1007/s00500-025-10476-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough set theory has been extensively studied in regard to its lattice structure. However, this article concerns with the (commutative) ring structure of rough set theory. We show that a finite approximation space can be identified by a cube free natural number $$n$$ by providing an isomorphism between lattice of rough sets and lattice of ideals of the ring $$\mathbb {Z}_{n}$$ . We introduce a ring structure on the rough sets via the ring structure on ideals of $$\mathbb {Z}_{n}$$ . Moreover, we also classify all the rings which are isomorphic to the rings formed by the rough sets.},
  archive      = {J_SOCO},
  author       = {Kumar, Arun and Dewan, Bisham},
  doi          = {10.1007/s00500-025-10476-4},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {471-483},
  shortjournal = {Soft Comput.},
  title        = {Ring structure of rough sets},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamical behavior and chaos control of the conflicting
information propagation on a homogeneous network system. <em>SOCO</em>,
<em>29</em>(2), 457–469. (<a
href="https://doi.org/10.1007/s00500-025-10430-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple pieces of information regularly propagate in a social network. Different political party supporters utilize social systems not only for campaigning, publicity but also for opposing the opinions of other parties. They always try to create some agenda against the opposition. It is interesting to recognize the pattern when two conflicting pieces of information interact on social networks. Here, we present a nonlinear model of opposite information spread in a homogeneous network system. We considered two kinds of users, supporting two conflicting news stories at a time with the ability to protect their opinions from others. We obtained fixed points, their existence, and stability conditions. Here, we watch that social network system experience flip bifurcation and hopf bifurcation. We had chaos in the dynamics, which shows the uncertainty in the observation. Moreover, we suggested a strategy for controlling the complex dynamics of information spread on social networks in emergencies.},
  archive      = {J_SOCO},
  author       = {Jain, Ankur and Dhar, Joydip and Gupta, Vijay K.},
  doi          = {10.1007/s00500-025-10430-4},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {457-469},
  shortjournal = {Soft Comput.},
  title        = {Dynamical behavior and chaos control of the conflicting information propagation on a homogeneous network system},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence analysis of a picard–CR iteration process for
nonexpansive mappings. <em>SOCO</em>, <em>29</em>(2), 435–455. (<a
href="https://doi.org/10.1007/s00500-025-10515-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel hybrid iteration process, namely the Picard–CR iteration process. We apply the proposed iteration process for the numerical reckoning of fixed points of generalized $$\alpha $$ -nonexpansive mappings. We establish weak and strong convergence results of generalized $$\alpha $$ -nonexpansive mappings. This study demonstrates the superiority of the hybrid approach in terms of convergence speed. Moreover, we numerically compare the proposed iteration process with other well-known ones from the literature. In the comparison, we consider two problems: finding a fixed point of a generalized $$\alpha $$ -nonexpansive mapping and finding roots of a complex polynomial. In the second problem, we use the so-called polynomiography in the analysis. The results showed that the proposed iteration scheme is better than other three-parameter iteration schemes from the literature. Using the proven fixed-point results, we also obtain solutions to fractional differential equations.},
  archive      = {J_SOCO},
  author       = {Nawaz, Bashir and Ullah, Kifayat and Gdawiec, Krzysztof},
  doi          = {10.1007/s00500-025-10515-0},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {435-455},
  shortjournal = {Soft Comput.},
  title        = {Convergence analysis of a Picard–CR iteration process for nonexpansive mappings},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using covering approaches to study concept lattices.
<em>SOCO</em>, <em>29</em>(2), 425–434. (<a
href="https://doi.org/10.1007/s00500-025-10488-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A covering can be used for problem solving. In this paper, we use covering methods to study isomorphic relationships among various concept lattices. We use the lattices induced by coverings to characterize various concept lattices, and set up isomorphic relationships between the lattices induced by coverings and the various types of concept lattices. Using these isomorphic relationships, we transform attribute and object reductions for these lattices into a covering reduction. We also study the granular reduction for object-induced three-way concept lattices and provide an algorithm for identifying all granular reducts.},
  archive      = {J_SOCO},
  author       = {Liu, Guilong and Gao, Xiuwei},
  doi          = {10.1007/s00500-025-10488-0},
  journal      = {Soft Computing},
  month        = {1},
  number       = {2},
  pages        = {425-434},
  shortjournal = {Soft Comput.},
  title        = {Using covering approaches to study concept lattices},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoverGAN: Cover photo generation from text story using
layout guided GAN. <em>SOCO</em>, <em>29</em>(1), 405–423. (<a
href="https://doi.org/10.1007/s00500-025-10436-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating cover photos from story text is a non trivial challenge to solve. Existing approaches focus on generating only images from given text prompt. To the best of our knowledge, non of these approaches focus on generating cover photos from a text story. The paper addresses this issue by introducing multi-object image generation with text title from a text story. We split the problem into three steps:- understanding semantics of text story, predicting layout of objects, and generating a cover photo. At start, a semantic relation was encoded between text story objects using a Scene graph, then features from graph neural network were concatenated with single object features from scene graph to create an object layout. All of these features were then passed on to the image generating part. Image generation was further divided into two phases. In the first phase, the image is generated using a scene graph image generation model. While in the second phase, the results of first phase were further enhanced using image translation model conditioned on the object layout. In final phase we generated title of the given story based on generated image. In our experiments, we used custom dataset of text stories with three animal categories along with the COCO dataset. For the image generating part, we evaluated our approach with state of the art models known as scene_gen and sg2im. Our method generated high-resolution informative cover photo with story title by positioning the objects at right locations as specified in the text story.},
  archive      = {J_SOCO},
  author       = {Cheema, Adeel and Naeem, M. Asif},
  doi          = {10.1007/s00500-025-10436-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {405-423},
  shortjournal = {Soft Comput.},
  title        = {CoverGAN: Cover photo generation from text story using layout guided GAN},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of machine learning and deep learning techniques
on reverse vaccinology – a systematic literature review. <em>SOCO</em>,
<em>29</em>(1), 391–403. (<a
href="https://doi.org/10.1007/s00500-025-10480-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reverse vaccinology (RV) is recognized as a productive method of vaccine discovery since it may be used to create vaccines for a variety of infectious pathogens. With the potential for machine learning (ML) algorithms to enable quick and precise predictions of vaccine candidates against new infections, RV is of particular relevance. Despite the fact that ML has been used successfully in the past, Deep learning (DL) model-based RV approaches have not been used widely. DL techniques are known to provide more complicated models and better performance for AI applications. This paper supports and reviews the roles of machine learning and Deep Learning in predicting potential vaccine candidates and discovery processes. Our study involved a systematic evaluation of selected publications, identified through a combination of prior knowledge and keyword searches across freely accessible databases. A meticulous screening process, considering contextual relevance, abstract quality, methodology, and full-text content, was employed. The literature review, conducted with a rigorous methodology, encompasses a thorough analysis of articles focusing on machine learning and deep learning techniques.},
  archive      = {J_SOCO},
  author       = {Alashwal, Hany and Kochunni, Nishi Palakkal and Hayawi, Kadhim},
  doi          = {10.1007/s00500-025-10480-8},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {391-403},
  shortjournal = {Soft Comput.},
  title        = {Application of machine learning and deep learning techniques on reverse vaccinology – a systematic literature review},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-term time series prediction based on evolutionary
interpolation of chebyshev polynomials with internal smoothing.
<em>SOCO</em>, <em>29</em>(1), 375–389. (<a
href="https://doi.org/10.1007/s00500-025-10424-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel short-term time series forecasting scheme based on evolutionary interpolation of Chebyshev polynomials is presented in this paper. The uniqueness of the proposed scheme lies in the higher density of Chebyshev nodes at the ends of the interpolation interval. Thus, the structural representation of the algebraic interpolant closer to the present moment of time becomes more accurate compared to the older Chebyshev nodes. The internal smoothing scheme is used to find a balance between the ability of the algebraic interpolant to reflect the local dynamics and to suppress the unwanted effects outside the interpolation interval. Evolutionary optimization algorithms are used to define near-optimal corrections of nodal values of the time series. The proposed nonlinear mapping scheme used on the last elements of an equally spaced time series enables a more accurate extrapolation of the soft algebraic interpolant. To our knowledge, this is the first attempt to employ non-uniform Chebyshev nodes for the prediction of an equally spaced time series. Computational experiments with several standard time series are used to demonstrate the efficacy and the accuracy of the proposed one-step-ahead forecasting scheme.},
  archive      = {J_SOCO},
  author       = {Saunoriene, Loreta and Cao, Jinde and Landauskas, Mantas and Ragulskis, Minvydas},
  doi          = {10.1007/s00500-025-10424-2},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {375-389},
  shortjournal = {Soft Comput.},
  title        = {Short-term time series prediction based on evolutionary interpolation of chebyshev polynomials with internal smoothing},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Representations of binary relations and object reduction of
attribute-oriented concept lattices. <em>SOCO</em>, <em>29</em>(1),
365–373. (<a href="https://doi.org/10.1007/s00500-025-10418-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, two representations of binary relations by means of Galois adjunctions and the newly introduced complete distributors are constructed. A binary relation can be uniquely determined by a Galois adjunction, a complete distributor respectively. Under a new description of formal contexts, instead of attribute reduction, the object reduction problem of attribute-oriented concept lattice is studied and related algorithm is designed.},
  archive      = {J_SOCO},
  author       = {Yao, Wei and Zhou, Chang-Jie},
  doi          = {10.1007/s00500-025-10418-0},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {365-373},
  shortjournal = {Soft Comput.},
  title        = {Representations of binary relations and object reduction of attribute-oriented concept lattices},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASAQ—ant-miner: Optimized rule-based classifier.
<em>SOCO</em>, <em>29</em>(1), 355–364. (<a
href="https://doi.org/10.1007/s00500-025-10474-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ant-Miner, a rule-based classifier, has been extensively utilized for classification tasks. However, it features numerous controlling parameters that significantly impact its performance. The standard Ant-Miner (AM) often encounters issues such as slow or premature convergence and high selective pressure. Several studies have proposed innovative methods to enhance AM’s performance by examining the quality function, heuristic strategies, and pheromone update mechanisms. Recent research has also focused on ant selection for pheromone updates and term selection strategies. Despite substantial efforts, existing studies are limited by challenges like local optima entrapment, premature or slow convergence, high selective pressure, exhaustive searches, and the generation of specific yet low-quality rules. This paper introduces a novel Ant-Miner algorithm (ASAQ—AM) that implements novel Ant-Selection strategies for pheromone updating and data classification, alongside an Advanced Quality function. This novel quality function employs hierarchical thresholds to evaluate ants based on coverage, rule length, and accuracy. The selected ants contribute to pheromone updates on their respective paths guided by the proposed quality function. The effectiveness of the ASAQ—AM approach is assessed using four publicly available datasets and standard benchmark performance metrics, including accuracy and F1-score. The results demonstrate that the proposed method outperforms the basic Ant-Miner, state-of-the-art variants, and several data mining approaches in terms of accuracy, F1-score, and convergence speed.},
  archive      = {J_SOCO},
  author       = {Ayub, Umair and Almas, Bushra},
  doi          = {10.1007/s00500-025-10474-6},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {355-364},
  shortjournal = {Soft Comput.},
  title        = {ASAQ—Ant-miner: Optimized rule-based classifier},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure signal and image transmissions using chaotic
synchronization scheme under cyber-attack in the communication channel.
<em>SOCO</em>, <em>29</em>(1), 339–353. (<a
href="https://doi.org/10.1007/s00500-025-10511-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, problem of secure message (signal and image) transmission is studied. The message is encrypted by masking it with a chaotic system state and then transmitted to receiver-side via a communication channel. It is assumed that the communication channel may be prone to a cyber-attack. The transmission system dynamics in presence of the cyber-attack in the communication channel is modeled by a singular dynamical system. Then, at the receiver-side, a novel observer is designed to recover the message together with the cyber-attack signal. Estimation of the cyber-attack is crucial for monitoring the communication channel and enable a network supervisor to mitigate the adverse consequences and make an appropriate strategy. It is assumed neither restrictions on the message nor on the cyber-attack signals. This makes the proposed scheme, the best solution in the real applications. Moreover, comparing to the existing methods which two public and private channels are required, only one communication channel is used in this paper for transmission the encrypted signal which results in a cheaper solution. Solving the optimization problem can be easily performed with the help of the existing toolboxes such as YALMIP and CVX software in MATLAB. Finally, numerical simulations for signal and image transmission are presented to justify the applicability and superior performance of the proposed scheme.},
  archive      = {J_SOCO},
  author       = {Nobakht, Shaghayegh and Ahmadi, Ali-Akbar},
  doi          = {10.1007/s00500-025-10511-4},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {339-353},
  shortjournal = {Soft Comput.},
  title        = {Secure signal and image transmissions using chaotic synchronization scheme under cyber-attack in the communication channel},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genomic signal processing approach for identification and
classification of coronavirus sequences. <em>SOCO</em>, <em>29</em>(1),
321–338. (<a href="https://doi.org/10.1007/s00500-024-10377-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corona disease has caused a variety of problems for people since it was formed and spread around the world. In this study, diagnosis and differentiation of this disease have been investigated in the form of genomic sequences. The proposed approach is based on a combination of several digital signal processing algorithms that include discrete Fourier transform and comb notch filter. More than 100,000 genomic sequences from different geographical locations and different variants have been tested in this research by various machine learning models. The use of KNN and SVM classifier models has resulted in the accurate diagnosis and differentiation of about 99% of coronavirus samples from the influenza virus. The proposed approach provides the possibility to generalize this method and improve machine learning models and get better results.},
  archive      = {J_SOCO},
  author       = {Khodaei, Amin and Mozaffari-Tazehkand, Behzad and Sharifi, Hadi},
  doi          = {10.1007/s00500-024-10377-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {321-338},
  shortjournal = {Soft Comput.},
  title        = {A genomic signal processing approach for identification and classification of coronavirus sequences},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the selected strategies and multiple selected
paths for digital music subscription services using the DSA-NRM approach
consideration of various stakeholders. <em>SOCO</em>, <em>29</em>(1),
299–320. (<a href="https://doi.org/10.1007/s00500-024-10323-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music has become a part of many people&#39;s lives. Early adopters used to buy tapes or CDs to listen to music, which were difficult to preserve and easily damaged. With the digital transfor-mation of the industry, users nowadays can listen to various genres/styles of music online through digital music platforms at any time. As the types and styles of music are numerous, some digital music platforms have begun to consider providing more diverse ways of delivering music listening services. For example, various music classification systems allows users to easily find all albums by the same singer; different styles of music streaming services save users time in searching for songs of the same type; and the ability to share playlists allows users to share their listening playlists with family and friends. This service model increases peer discussion topics and exposure to albums, songs, and singers. The study summarizes the driving factors influencing the adoption of digital music subscription services using expert interviews and literature reviews. The study outlines four adoption-driving dimensions (service benefits, service efficiency, behavioral attribution, and adoption intention) and 16 evaluation criteria. Besides, this study integrates Desire and Satisfaction Analysis (DSA) and Network Relation Map (NRM) to propose a DSA-NRM analysis to evaluate the adoption strategy and optimal development path for digital music subscription services. Based on the four quadrants of music subscription services, this study proposes four selected strategies (attention strategy, sustainment strategy, adjustment strategy, and focus strategy). The research results show that the SB (service benefits) aspect has high desire and low satisfaction and can adopt selected strategy D (focus Strategy). The AI (adoption intention) aspect is an aspect with intense desire and low satisfaction and can use strategy C (adjustment strategy) chosen. The AI (adoption intention) aspect can dominate other aspects, and the SE (service effectiveness) can be dominated by different aspects.},
  archive      = {J_SOCO},
  author       = {Tsai, Kuo-Pao and Yang, Feng-Chao and Lu, Chin-Lung and Lin, Chia-Li},
  doi          = {10.1007/s00500-024-10323-y},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {299-320},
  shortjournal = {Soft Comput.},
  title        = {Exploring the selected strategies and multiple selected paths for digital music subscription services using the DSA-NRM approach consideration of various stakeholders},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RCS: A fast path planning algorithm for unmanned aerial
vehicles. <em>SOCO</em>, <em>29</em>(1), 275–298. (<a
href="https://doi.org/10.1007/s00500-025-10481-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is a major problem in autonomous vehicles. In recent years, with the increase in applications of unmanned aerial vehicles (UAVs), one of the main challenges is path planning, particularly in adversarial environments. In this paper, we consider the problem of planning a collision-free path for a UAV in a polygonal domain from a source point to a target point. Based on the characteristics of UAVs, we assume two basic limitations on the generated paths: an upper bound on the turning angle at each turning point (maximum turning angle) and a lower bound on the distance between two consecutive turns (minimum route leg length). We describe an algorithm that runs in $$O(n^2 \log n)$$ time and finds a feasible path in accordance with the above limitations, where n is the number of obstacle vertices. As shown by experiments, the output of the algorithm is much closer to the shortest path with this requirement (always below about 10% of the shortest path), in a much smaller time (up to 10,000 quicker, in our experiments). We further demonstrate how to decompose the algorithm into two phases, a preprocessing and a query phase. In this way, given a fixed start point and a set of obstacles, we can preprocess a data structure of size $$O(n^2)$$ in $$O(n^2 \log n)$$ time, such that for any query target point we can find a path with the given requirements in $$O(n \log n)$$ time. Finally, we modify the algorithm to find a feasible (almost shortest) path that reaches the target point within a given range of directions.},
  archive      = {J_SOCO},
  author       = {Ranjbar Divkoti, Mohammad Reza and Nouri-Baygi, Mostafa},
  doi          = {10.1007/s00500-025-10481-7},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {275-298},
  shortjournal = {Soft Comput.},
  title        = {RCS: A fast path planning algorithm for unmanned aerial vehicles},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The location problem of emergency materials in uncertain
environment. <em>SOCO</em>, <em>29</em>(1), 261–273. (<a
href="https://doi.org/10.1007/s00500-025-10400-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hazards and unpredictability of emergencies have made people pay more and more attention to emergency response. A reasonable reserve of emergency materials can play an important role in post-disaster rescue. This paper uses the uncertain comprehensive evaluation method to grade the emergency materials, and establishes a location model for the uncertain emergency materials. The location of the reserve point is determined by maximizing the rescue satisfaction, the maximum number of service demand points under the maximum coverage, and pursuing the minimum sum of the distance from the demand point to the reserve point. Based on the uncertainty theory, the uncertain emergency materials location model is converted into an equivalent deterministic emergency materials location model, and the model is solved by a tabu search algorithm. Finally, a numerical experiment is given to illustrate the idea of the uncertain model. Four sites were selected through a tabu search algorithm.},
  archive      = {J_SOCO},
  author       = {Xiao, Jihe and Sheng, Yuhong},
  doi          = {10.1007/s00500-025-10400-w},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {261-273},
  shortjournal = {Soft Comput.},
  title        = {The location problem of emergency materials in uncertain environment},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A game theoretic approach for pricing of red blood cells
under supply and demand uncertainty and government role. <em>SOCO</em>,
<em>29</em>(1), 237–260. (<a
href="https://doi.org/10.1007/s00500-024-10308-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing global population and reduced blood donations due to COVID-19 necessitate efficient management and pricing strategies for red blood cells (RBCs). Pricing mechanisms must address the perishable nature of RBCs and supply–demand uncertainties to minimize wastage. Government intervention in blood donation and healthcare costs further emphasizes their role in addressing these challenges. This paper utilizes mathematical modelling to investigate pricing within the RBC supply chain, involving stakeholders like governments, public hospitals, and blood banks. Employing a government-led Stackelberg game framework, it considers the precedence of government decisions. Objective functions of each participant incorporate economic and social aspects in both centralized and decentralized scenarios. The study starts with optimizing RBC pricing in blood banks and then establishes coordination mechanisms via revenue-sharing contracts to improve supply chain efficiency, cut costs, and minimize waste—an approach novel in the RBC supply chain context. Finally, the effectiveness of the proposed model is evaluated through a numerical example based on real-world data. Furthermore, sensitivity analyses are conducted on key parameters to provide valuable insights for management decision-making.},
  archive      = {J_SOCO},
  author       = {Kamrantabar, Minoo and Yaghoubi, Saeed and Fander, Atieh},
  doi          = {10.1007/s00500-024-10308-x},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {237-260},
  shortjournal = {Soft Comput.},
  title        = {A game theoretic approach for pricing of red blood cells under supply and demand uncertainty and government role},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective fuzzy robust optimization model for
open-pit mine planning under uncertainty. <em>SOCO</em>, <em>29</em>(1),
213–235. (<a href="https://doi.org/10.1007/s00500-024-10365-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open-pit mines, transporting ore from the pit to unloading sites is one of the most significant factors driving both cost and environmental impact. However, uncertainty in truck arrival times and ore transport rates due to varying weather conditions, physical limitations, and mental states of the workforce makes efficient allocation a challenge. This study addresses this challenge by proposing a multi-objective mathematical model that optimizes cost, production, and environmental objectives. Fuzzy Robust Optimization (FRO) incorporates uncertainty into the model, and the Linear Programming Metric (LP-Metric) is employed to solve it. A case study using the Chadormalu mine demonstrates the model&#39;s effectiveness in planning shift transport. Sensitivity analysis confirms the model&#39;s ability to handle uncertainty. Notably, the optimality robustness factor solely influences the objective function&#39;s value (representing the sum of relative deviations from optimal function values). Feasibility robustness, however, has no impact on any measurements.},
  archive      = {J_SOCO},
  author       = {Soleimani Bafghi, Sayed Abolghasem and Hosseini Nasab, Hasan and Fakhrzad, Mohammad Bagher and Soltani, Roya and Yarahmadi Bafghi, Ali reza},
  doi          = {10.1007/s00500-024-10365-2},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {213-235},
  shortjournal = {Soft Comput.},
  title        = {A multi-objective fuzzy robust optimization model for open-pit mine planning under uncertainty},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benders decomposition for the multi-agent location and
scheduling problem on unrelated parallel machines. <em>SOCO</em>,
<em>29</em>(1), 195–212. (<a
href="https://doi.org/10.1007/s00500-024-10395-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The machine location and job scheduling problem is becoming a new and important research topic with a wide range of practical applications. It jointly optimizes the machine location, job assignment, and job sequence decision to yield a globally optimal solution. This paper investigates the multiple-agent scheduling and location problem on unrelated parallel machines with the objective of minimizing the sum of the fixed location cost of the machines, transportation cost of the jobs between the locations of the jobs and machines, and total weighted completion time of the jobs of one agent while keeping the total weighted completion time of each other agent no greater than a given limit. To solve this problem, we develop a tailored Benders decomposition algorithm, which decomposes the problem into a Benders master problem and a Benders subproblem. The Benders master problem determines which locations are chosen to set up machines and which jobs are assigned to each machine. The resulting Benders subproblem finds the optimal job sequence of the jobs assigned to each machine in use, which is solved by an exact dynamic programming algorithm. To speed up the convergence of the developed algorithm, some improvement strategies, including warm start strategy and weak Benders cut generation, are introduced. Computational experiments on randomly generated instances show the effectiveness and efficiency of developed algorithm and improvement strategies. Moreover, management insights are drawn from the sensitivity analyses of some key parameters.},
  archive      = {J_SOCO},
  author       = {Liu, Jun and Yang, Yongjian and Li, Wencan and Jiang, Hua and Guo, Tianwen and Yang, Feng},
  doi          = {10.1007/s00500-024-10395-w},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {195-212},
  shortjournal = {Soft Comput.},
  title        = {Benders decomposition for the multi-agent location and scheduling problem on unrelated parallel machines},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quadratic and lagrange interpolation-based butterfly
optimization algorithm for numerical optimization and engineering design
problem. <em>SOCO</em>, <em>29</em>(1), 157–194. (<a
href="https://doi.org/10.1007/s00500-024-10339-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a novel and improved Butterfly Optimization Algorithm (BOA), known as LQBOA, to solve BOA’s inherent limitations. The LQBOA uses Lagrange interpolation and simple quadratic interpolation techniques with adapted parameter settings to enhance the search strategy of BOA and to achieve a better balance of diversification and intensification. LQBOA’s performance was examined on 45 traditional benchmark issues as well as the IEEE CEC 2017 benchmark suites with 10, 30 and 50 dimensions, and the results were compared against popular state-of-the-art and modified algorithms. It was discovered that in more than 85% of cases, the proposed LQBOA outperformed the compared algorithms. The Friedman rank test and Wilcoxon rank test were used to validate the suggested LQBOA’s rank and significance. Additionally, convergence and diversity analyses were performed to investigate its convergence speed and searching behavior, respectively. Furthermore, LQBOA has been successfully deployed to solve twelve real-world issues including several engineering design problems and two multiple gravity assist spacecraft trajectory problems. The results of these problems were compared to those of a wide range of algorithms, demonstrating the superior performance of the proposed LQBOA. In conclusion, LQBOA makes a significant addition to the optimization area, and its application could greatly improve the performance of numerous optimization jobs.},
  archive      = {J_SOCO},
  author       = {Sharma, Sushmita and Saha, Apu Kumar and Chakraborty, Sanjoy and Deb, Suman and Sahoo, Saroj Kumar},
  doi          = {10.1007/s00500-024-10339-4},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {157-194},
  shortjournal = {Soft Comput.},
  title        = {Quadratic and lagrange interpolation-based butterfly optimization algorithm for numerical optimization and engineering design problem},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative enhancement method of train operation planning
featuring express and local modes for urban rail transit lines.
<em>SOCO</em>, <em>29</em>(1), 127–155. (<a
href="https://doi.org/10.1007/s00500-024-10364-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An urban rail system’s ability to operate effectively and efficiently depends on its train operation plan. This paper proposes a strategy to optimize the train operation plan in an urban rail line with a single terminal depot by decreasing the travel cost for passengers and increasing resource utilization effectiveness from the operators’ point of view. We put forward a two-stage problem-solving approach that can effectively solve the train operation plan employing express and local type services. A mathematical model is formulated to address multi-level problems including line planning, train scheduling, train circulation planning, and an assignment for passengers’ travel. A two-stage iterative bi-objective algorithm is designed to suggest a targeted optimization for passengers’ traveling expenses and the enterprise’s running expenses. The first phase aims at optimizing the regularity of service to maximize the number of travelers achieved and the bi-objective function. The second phase, however, readjusts the stop plan for express operations to improve the objective until reaching the best possible setting. Finally, a real-world case study with three instances setting different departure ratios for local and express operations demonstrates how the train operation plan created using our approach lowers operational costs while meeting service level requirements with superior performance.},
  archive      = {J_SOCO},
  author       = {Zhou, Wenliang and Oldache, Mehdi and Xu, Guangming},
  doi          = {10.1007/s00500-024-10364-3},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {127-155},
  shortjournal = {Soft Comput.},
  title        = {Cooperative enhancement method of train operation planning featuring express and local modes for urban rail transit lines},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing stock prediction ability through news perspective
and deep learning with attention mechanisms. <em>SOCO</em>,
<em>29</em>(1), 117–126. (<a
href="https://doi.org/10.1007/s00500-025-10437-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News is reaching investors at an unprecedented rate since pop-up notifications and news recommendations are so common on websites and applications. For investors, these news sources have established themselves as an essential resource for stock market information. To study the impact of news on stock prediction and further enhance its predictive ability, this study innovatively merges the capacity of attention mechanisms to focus crucial information with the ability of temporal convolutional network (TCN) to discern temporal patterns, proposing a new algorithm named “Attention-TCN”. The results indicate that the Attention-TCN model has the smallest prediction error compared to well-known stock prediction algorithms such as Long Short-Term Memory and Gated Recurrent Units. The study demonstrates that incorporating a news perspective and utilizing a TCN model combined with an attention mechanism can hold the promise of helping investors to make more informed decisions and achieving higher returns.},
  archive      = {J_SOCO},
  author       = {Yang, Mei and Fu, Fanjie and Ni, Du and Xiao, Zhi},
  doi          = {10.1007/s00500-025-10437-x},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {117-126},
  shortjournal = {Soft Comput.},
  title        = {Enhancing stock prediction ability through news perspective and deep learning with attention mechanisms},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leader-follower green traffic assignment problem with online
supervised machine learning solution approach. <em>SOCO</em>,
<em>29</em>(1), 103–116. (<a
href="https://doi.org/10.1007/s00500-025-10407-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a bi-level green traffic assignment with network design problem. At the upper level, the objective function evaluates a total traffic Carbon monoxide (CO) gas emissions problem to provide a macroscopic viewpoint of the system manager. At the lower-level, a traffic assignment network design problem is considered to individually optimize users’ travel times, with certain links being potential candidates for network addition. Although the lower-level objective function is convex with linear constraints, the proposed bi-level problem is np-hard, and even finding a near-optimal solution is an np-hard task. To address the solution approach, we applied an online supervised machine learning (SML) algorithm which solves the proposed bi-level problem within a reasonable running time. Additionally, a mat-heuristic algorithm is proposed to compare the results with the online SML algorithm. To validate the online SML algorithm, we conducted experiments using real urban transportation examples in medium and large-sized networks.},
  archive      = {J_SOCO},
  author       = {Sadra, M. and Zaferanieh, M. and Yazdimoghaddam, J.},
  doi          = {10.1007/s00500-025-10407-3},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {103-116},
  shortjournal = {Soft Comput.},
  title        = {Leader-follower green traffic assignment problem with online supervised machine learning solution approach},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight and efficient model for botnet detection in
IoT using stacked ensemble learning. <em>SOCO</em>, <em>29</em>(1),
89–101. (<a href="https://doi.org/10.1007/s00500-025-10425-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient botnet detection is of great security importance and has been the focus of researchers in recent years. Botnet detection is also a difficult task due to the difficulty in distinguishing it from normal traffic. At the same time, detecting these botnets require a lot of computation resources using traditional methods and this limitation makes it even more difficult to detect them on Internet of Things (IoT) devices. Considering the massive IoT data, an efficient and lightweight approach for detecting and predicting IoT botnet attacks is required. In this paper, multiple lightweight machine learning methods including a deep Multilayer Perceptron (MLP) method and a Random Forest (RF) method are integrated into a stacked ensemble learning model to detect botnet attacks in IoT devices. This integration is based on applying lasso regression on features and utilizing a logistic regression in ensemble learning, which leads to increasing the accuracy of botnet detection and reducing its computational complexity. The performance evaluation of the proposed model is examined from two perspectives: accuracy and lightweight characteristics; and for this purpose, a real-world UNSW (BoT-IoT) dataset is used. A comparative study with competitive neural network methods demonstrates that our approach delivers a better outcome. Experimental results reveal that this method has a higher efficiency in all metrics than competing methods including accuracy, precision, recall, and F1 score with values of 99.3%, 99.2%, 99%, and 99.1%, respectively. Besides, the results showed that the proposed method requires at least 36% less CPU and 38% less memory compared to the competing methods, which makes the proposed method to be suitable for IoT devises with limited resources.},
  archive      = {J_SOCO},
  author       = {Esmaeilyfard, Rasool and Shoaei, Zohre and Javidan, Reza},
  doi          = {10.1007/s00500-025-10425-1},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {89-101},
  shortjournal = {Soft Comput.},
  title        = {A lightweight and efficient model for botnet detection in IoT using stacked ensemble learning},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). European option pricing under a generalized fractional
brownian motion heston exponential hull–white model with transaction
costs by the deep galerkin method. <em>SOCO</em>, <em>29</em>(1), 69–88.
(<a href="https://doi.org/10.1007/s00500-025-10433-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new financial model called the generalized fractional Brownian motion Heston exponential Hull–White model, which has stochastic volatility and interest rate, long memory, and heavy tail distribution. Based on the market price of the volatility and delta hedging strategies, we propose a partial differential equation (PDE) to obtain the European option price. To do this, portfolio changes contain long one position of the European call option and shares of the underlying assets (stock, zero coupon bond, volatility), where we use the mentioned model to obtain the price. Due to transaction costs, the resulting equation is a fully nonlinear PDE, which we use the Deep Galerkin Method (DGM) to solve it. Also, we present the proof of the convergence of the method to this class of equations, which includes two parts: the convergence of the loss function to zero and the convergence of the neural network to the exact solution of the equation. We finally present numerical results to show the model and method’s effectiveness.},
  archive      = {J_SOCO},
  author       = {Motameni, Mahsa and Mehrdoust, Farshid and Najafi, Ali Reza},
  doi          = {10.1007/s00500-025-10433-1},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {69-88},
  shortjournal = {Soft Comput.},
  title        = {European option pricing under a generalized fractional brownian motion heston exponential Hull–White model with transaction costs by the deep galerkin method},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Construction of a novel five-dimensional hamiltonian
conservative hyperchaotic system and its application in image
encryption. <em>SOCO</em>, <em>29</em>(1), 53–67. (<a
href="https://doi.org/10.1007/s00500-025-10443-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the theory of Euler’s equations, this paper constructs a novel five-dimensional Hamiltonian conservative hyperchaotic system with the aim of systematically exploring and analyzing its dynamic characteristics, while validating its potential application value in the field of information security. A comprehensive and multi-faceted dynamic analysis of the new system is conducted, encompassing energy variations, equilibrium point distributions, Poincaré section structures, and the computation of Lyapunov exponents. The analysis reveals that the system exhibits excellent chaotic properties and demonstrates remarkable conservative hyperchaotic behavior across a wide range of initial values and parameters. Notably, a significant enhancement in the maximum Lyapunov exponent is observed, further highlighting the system’s potential and promising applications in information security. Additionally, by adjusting the initial values, the coexistence of nested phenomena across multiple energy levels is observed, providing intriguing perspectives for further investigation of the system. Finally, this paper proposes an image encryption scheme based on this system and is successfully applied in the field of image encryption. Experiments show that compared with the information entropy of similar documents in the past four years, the information entropy of this scheme increased by 0.02% on average, and the highest value reached 7.9995, which confirmed that the encryption algorithm has excellent confidentiality performance and provided new ideas and methods for the development of the field of information security.},
  archive      = {J_SOCO},
  author       = {Yan, Minxiu and Li, Shuyan},
  doi          = {10.1007/s00500-025-10443-z},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {53-67},
  shortjournal = {Soft Comput.},
  title        = {Construction of a novel five-dimensional hamiltonian conservative hyperchaotic system and its application in image encryption},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of SaRT–SVM algorithm for leakage pattern
recognition of hydraulic check valve. <em>SOCO</em>, <em>29</em>(1),
37–51. (<a href="https://doi.org/10.1007/s00500-024-10371-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Check valves are key components in hydraulic systems. The cross-port leakage in check valves is a common fault that affects their performances. The vibration and pressure fluctuations excited by leaks are weak, therefore leakage is difficult to be identified and classified by intelligent algorithms and non-destructive testing methods. To maximize the performance of leak pattern recognition, we had improved the sequential minimal optimisation algorithm for enhancing the classification performance and tested it with the University of California Irvine Machine Learning Repository. Furthermore, combining with the search and rescue team (SaRT) algorithm, we propose SaRT-SVM algorithm. Two important parameters γ and C of support vector machine (SVM) were optimised and compared with response surface and other algorithms. We analysed the SaRT–SVM method for leakage pattern recognition and validated the robustness of the developed method by applying the method on multiple fault samples of each fault mode, additional different noises, and another independent data collection. The results showed that the SaRT–SVM algorithm exhibited excellent classification performance and robustness when applied to the leakage pattern recognition of hydraulic check valves under the influence of different noises.},
  archive      = {J_SOCO},
  author       = {Tong, Chengbiao and Sepehri, Nariman},
  doi          = {10.1007/s00500-024-10371-4},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {37-51},
  shortjournal = {Soft Comput.},
  title        = {Application of SaRT–SVM algorithm for leakage pattern recognition of hydraulic check valve},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stabilization of impulsive fuzzy dynamic systems involving
caputo short-memory fractional derivative. <em>SOCO</em>,
<em>29</em>(1), 17–36. (<a
href="https://doi.org/10.1007/s00500-024-10353-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main focus of this paper is to investigate the asymptotic stability of impulsive fuzzy fractional dynamic systems (IFFDSs) under the Caputo fractional derivative concept in the short-memory sense. To address the issue of asymptotic instability in IFFDSs, a linear feedback controller is introduced. The paper proposes two methods for achieving this, namely Lyapunov’s direct method (LDM) which is known for its high efficiency in surveying the stability theory of dynamic systems, and the direct evaluation method (DEM), which utilizes the properties of the Laplace transform, the Mittag–Leffler function, and Gronwall–Bellman inequality. The effectiveness of the proposed methods is demonstrated through numerical examples.},
  archive      = {J_SOCO},
  author       = {An, Truong Vinh and Van Hoa, Ngo and Thao, Nguyen Trang},
  doi          = {10.1007/s00500-024-10353-6},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {17-36},
  shortjournal = {Soft Comput.},
  title        = {Stabilization of impulsive fuzzy dynamic systems involving caputo short-memory fractional derivative},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KMSBOT: Enhancing educational institutions with an
AI-powered semantic search engine and graph database. <em>SOCO</em>,
<em>29</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s00500-024-10329-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving field of education, a semantic search engine is essential to efficiently retrieve knowledge experts’ data. Universities and colleges continuously generate a vast amount of educational and research data. A semantic search engine can assist students and staff in efficiently searching for required information in such a big data pool. The existing systems have limitations in providing personalized recommendations that align with the individual learning objectives of students and scholars, thus hindering their educational experience. To address this, this paper proposed a KMSBOT. This novel recommendation system effectively summarizes academic data and provides tailored information for students, research scholars, and faculty, enhancing educational experiences. This paper meticulously details the development of KMSBOT, which comprises a neo4j-based knowledge graph technique, the NLP method for data structuring, and the KNN machine learning model for classification. The system employs a three-module approach, utilizing data structuring, NLP processing, and semantic search engine integration. By leveraging Neo4j, NLTK, and BERT in Python, this proposed work ensures optimal performance metrics such as time, accuracy, and loss value. The proposed solution addresses traditional recommendation systems’ limitations and contributes to a brighter future, improving user satisfaction and engagement in academic environments.},
  archive      = {J_SOCO},
  author       = {Subramanian, D. Venkata and Chandra, J. and Immanuel, V. Ashok and Rohini, V.},
  doi          = {10.1007/s00500-024-10329-6},
  journal      = {Soft Computing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Soft Comput.},
  title        = {KMSBOT: Enhancing educational institutions with an AI-powered semantic search engine and graph database},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
