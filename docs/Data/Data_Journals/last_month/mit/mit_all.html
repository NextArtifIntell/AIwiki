<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>mit_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="mit">MIT</h1>
<h2 id="neco---5">NECO - 5</h2>
<ul>
<li><details>
<summary>
(2025). Uncovering dynamical equations of stochastic decision models
using data-driven SINDy algorithm. <em>NECO</em>, <em>37</em>(3),
569–587. (<a href="https://doi.org/10.1162/neco_a_01736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision formation in perceptual decision making involves sensory evidence accumulation instantiated by the temporal integration of an internal decision variable toward some decision criterion or threshold, as described by sequential sampling theoretical models. The decision variable can be represented in the form of experimentally observable neural activities. Hence, elucidating the appropriate theoretical model becomes crucial to understanding the mechanisms underlying perceptual decision formation. Existing computational methods are limited to either fitting of choice behavioral data or linear model estimation from neural activity data. In this work, we made use of sparse identification of nonlinear dynamics (SINDy), a data-driven approach, to elucidate the deterministic linear and nonlinear components of often-used stochastic decision models within reaction time task paradigms. Based on the simulated decision variable activities of the models and assuming the noise coefficient term is known beforehand, SINDy, enhanced with approaches using multiple trials, could readily estimate the deterministic terms in the dynamical equations, choice accuracy, and decision time of the models across a range of signal-to-noise ratio values. In particular, SINDy performed the best using the more memory-intensive multi-trial approach while trial-averaging of parameters performed more moderately. The single-trial approach, although expectedly not performing as well, may be useful for real-time modeling. Taken together, our work offers alternative approaches for SINDy to uncover the dynamics in perceptual decision making and, more generally, for first-passage time problems.},
  archive      = {J_NECO},
  author       = {Lenfesty, Brendan and Bhattacharyya, Saugat and Wong-Lin, KongFatt},
  doi          = {10.1162/neco_a_01736},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {569-587},
  shortjournal = {Neural Comput.},
  title        = {Uncovering dynamical equations of stochastic decision models using data-driven SINDy algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradual domain adaptation via normalizing flows.
<em>NECO</em>, <em>37</em>(3), 522–568. (<a
href="https://doi.org/10.1162/neco_a_01734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard domain adaptation methods do not work well when a large gap exists between the source and target domains. Gradual domain adaptation is one of the approaches used to address the problem. It involves leveraging the intermediate domain, which gradually shifts from the source domain to the target domain. In previous work, it is assumed that the number of intermediate domains is large and the distance between adjacent domains is small; hence, the gradual domain adaptation algorithm, involving self-training with unlabeled data sets, is applicable. In practice, however, gradual self-training will fail because the number of intermediate domains is limited and the distance between adjacent domains is large. We propose the use of normalizing flows to deal with this problem while maintaining the framework of unsupervised domain adaptation. The proposed method learns a transformation from the distribution of the target domains to the gaussian mixture distribution via the source domain. We evaluate our proposed method by experiments using real-world data sets and confirm that it mitigates the problem we have explained and improves the classification performance.},
  archive      = {J_NECO},
  author       = {Sagawa, Shogo and Hino, Hideitsu},
  doi          = {10.1162/neco_a_01734},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {522-568},
  shortjournal = {Neural Comput.},
  title        = {Gradual domain adaptation via normalizing flows},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward a free-response paradigm of decision making in
spiking neural networks. <em>NECO</em>, <em>37</em>(3), 481–521. (<a
href="https://doi.org/10.1162/neco_a_01733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have attracted significant interest in the development of brain-inspired computing systems due to their energy efficiency and similarities to biological information processing. In contrast to continuous-valued artificial neural networks, which produce results in a single step, SNNs require multiple steps during inference to achieve a desired accuracy level, resulting in a burden in real-time response and energy efficiency. Inspired by the tradeoff between speed and accuracy in human and animal decision-making processes, which exhibit correlations among reaction times, task complexity, and decision confidence, an inquiry emerges regarding how an SNN model can benefit by implementing these attributes. Here, we introduce a theory of decision making in SNNs by untangling the interplay between signal and noise. Under this theory, we introduce a new learning objective that trains an SNN not only to make the correct decisions but also to shape its confidence. Numerical experiments demonstrate that SNNs trained in this way exhibit improved confidence expression, reduced trial-to-trial variability, and shorter latency to reach the desired accuracy. We then introduce a stopping policy that can stop inference in a way that further enhances the time efficiency of SNNs. The stopping time can serve as an indicator to whether a decision is correct, akin to the reaction time in animal behavior experiments. By integrating stochasticity into decision making, this study opens up new possibilities to explore the capabilities of SNNs and advance SNNs and their applications in complex decision-making scenarios where model performance is limited.},
  archive      = {J_NECO},
  author       = {Zhu, Zhichao and Qi, Yang and Lu, Wenlian and Wang, Zhigang and Cao, Lu and Feng, Jianfeng},
  doi          = {10.1162/neco_a_01733},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {481-521},
  shortjournal = {Neural Comput.},
  title        = {Toward a free-response paradigm of decision making in spiking neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving recall in sparse associative memories that use
neurogenesis. <em>NECO</em>, <em>37</em>(3), 437–480. (<a
href="https://doi.org/10.1162/neco_a_01732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation of future low-power neuromorphic solutions requires specialist spiking neural network (SNN) algorithms that are optimized for neuromorphic settings. One such algorithmic challenge is the ability to recall learned patterns from their noisy variants. Solutions to this problem may be required to memorize vast numbers of patterns based on limited training data and subsequently recall the patterns in the presence of noise. To solve this problem, previous work has explored sparse associative memory (SAM)—associative memory neural models that exploit the principle of sparse neural coding observed in the brain. Research into a subcategory of SAM has been inspired by the biological process of adult neurogenesis, whereby new neurons are generated to facilitate adaptive and effective lifelong learning. Although these neurogenesis models have been demonstrated in previous research, they have limitations in terms of recall memory capacity and robustness to noise. In this article, we provide a unifying framework for characterizing a type of SAM network that has been pretrained using a learning strategy that incorporated a simple neurogenesis model. Using this characterization, we formally define network topology and threshold optimization methods to empirically demonstrate greater than 10 4 times improvement in memory capacity compared to previous work. We show that these optimizations can facilitate the development of networks that have reduced interneuron connectivity while maintaining high recall efficacy. This paves the way for ongoing research into fast, effective, low-power realizations of associative memory on neuromorphic platforms.},
  archive      = {J_NECO},
  author       = {Warr, Katy and Hare, Jonathon and Thomas, David},
  doi          = {10.1162/neco_a_01732},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {437-480},
  shortjournal = {Neural Comput.},
  title        = {Improving recall in sparse associative memories that use neurogenesis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Replay as a basis for backpropagation through time in the
brain. <em>NECO</em>, <em>37</em>(3), 403–436. (<a
href="https://doi.org/10.1162/neco_a_01735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How episodic memories are formed in the brain is a continuing puzzle for the neuroscience community. The brain areas that are critical for episodic learning (e.g., the hippocampus) are characterized by recurrent connectivity and generate frequent offline replay events. The function of the replay events is a subject of active debate. Recurrent connectivity, computational simulations show, enables sequence learning when combined with a suitable learning algorithm such as backpropagation through time (BPTT). BPTT, however, is not biologically plausible. We describe here, for the first time, a biologically plausible variant of BPTT in a reversible recurrent neural network, R2N2, that critically leverages offline replay to support episodic learning. The model uses forward and backward offline replay to transfer information between two recurrent neural networks, a cache and a consolidator, that perform rapid one-shot learning and statistical learning, respectively. Unlike replay in standard BPTT, this architecture requires no artificial external memory store. This approach outperforms existing solutions like random feedback local online learning and reservoir network. It also accounts for the functional significance of hippocampal replay events. We demonstrate the R2N2 network properties using benchmark tests from computer science and simulate the rodent delayed alternation T-maze task.},
  archive      = {J_NECO},
  author       = {Cheng, Huzi and Brown, Joshua W.},
  doi          = {10.1162/neco_a_01735},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {403-436},
  shortjournal = {Neural Comput.},
  title        = {Replay as a basis for backpropagation through time in the brain},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tmlr---115">TMLR - 115</h2>
<ul>
<li><details>
<summary>
(2025). Lognormal mutations and their use in detecting surreptitious
fake images. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=0RJvZY0h6O">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many cases, adversarial attacks against fake detectors employ algorithms specifically crafted for automatic image classifiers. These algorithms perform well, thanks to an excellent ad hoc distribution of initial attacks. However, these attacks are easily detected due to their specific initial distribution. Consequently, we explore alternative black-box attacks inspired by generic black-box optimization tools, particularly focusing on the \lognormal{} algorithm that we successfully extend to attack fake detectors. Moreover, we demonstrate that this attack evades detection by neural networks trained to flag classical adversarial examples. Therefore, we train more general models capable of identifying a broader spectrum of attacks, including classical black-box attacks designed for images, black-box attacks driven by classical optimization, and no-box attacks. By integrating these attack detection capabilities with fake detectors, we develop more robust and effective fake detection systems.},
  archive      = {J_TMLR},
  author       = {Olivier Teytaud and Mariia Zameshina and Tom Sander and Pierre Fernandez and Furong Ye and Laurent Najman and Thomas Bäck and Ismail Labiad},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Lognormal mutations and their use in detecting surreptitious fake images},
  url          = {https://openreview.net/forum?id=0RJvZY0h6O},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loss-to-loss prediction: Scaling laws for all datasets.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=1Avb4jYjLb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While scaling laws provide a reliable methodology for predicting train loss across compute scales for a single data distribution, less is known about how these predictions should change as we change the distribution. In this paper, we derive a strategy for predicting one loss from another and apply it to predict across different pre-training datasets and from pre-training data to downstream task data. Our predictions extrapolate well even at 20x the largest FLOP budget used to fit the curves. More precisely, we find that there are simple shifted power law relationships between (1) the train losses of two models trained on two separate datasets when the models are paired by training compute (train-to-train), (2) the train loss and the test loss on any downstream distribution for a single model (train-to-test), and (3) the test losses of two models trained on two separate train datasets (test-to-test). The results hold up for pre-training datasets that differ substantially (some are entirely code and others have no code at all) and across a variety of downstream tasks. Finally, we find that in some settings these shifted power law relationships can yield more accurate predictions than extrapolating single-dataset scaling laws.},
  archive      = {J_TMLR},
  author       = {David Brandfonbrener and Nikhil Anand and Nikhil Vyas and Eran Malach and Sham M. Kakade},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Loss-to-loss prediction: Scaling laws for all datasets},
  url          = {https://openreview.net/forum?id=1Avb4jYjLb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust high-dimensional mean estimation with low data size,
an empirical study. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=1QeI99nH9k">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust statistics aims to compute quantities to represent data where a fraction of it may be arbitrarily corrupted. The most essential statistic is the mean, and in recent years, there has been a flurry of theoretical advancement for efficiently estimating the mean in high dimensions on corrupted data. While several algorithms have been proposed that achieve near-optimal error, they all rely on large data size requirements as a function of dimension. In this paper, we perform an extensive experimentation over various mean estimation techniques where data size might not meet this requirement due to the high-dimensional setting. For data with inliers generated from a Gaussian with known covariance, we find experimentally that several robust mean estimation techniques can practically improve upon the sample mean, with the quantum entropy scaling approach from Dong \etal (NeurIPS 2019) performing consistently the best. However, this consistent improvement is conditioned on a couple of simple modifications to how the steps to prune outliers work in the high-dimension low-data setting, and when the inliers deviate significantly from Gaussianity. In fact, with these modifications, they are typically able to achieve roughly the same error as taking the sample mean of the uncorrupted inlier data, even with very low data size. In addition to controlled experiments on synthetic data, we also explore these methods on large language models, deep pretrained image models, and non-contextual word embedding models that do not necessarily have an inherent Gaussian distribution. Yet, in these settings, a mean point of a set of embedded objects is a desirable quantity to learn, and the data exhibits the high-dimension low-data setting studied in this paper. We show both the challenges of achieving this goal, and that our updated robust mean estimation methods can provide significant improvement over using just the sample mean. We additionally publish a library of Python implementations of robust mean estimation algorithms, allowing practitioners and researchers to apply these techniques and to perform further experimentation.},
  archive      = {J_TMLR},
  author       = {Cullen Anderson and Jeff M. Phillips},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Robust high-dimensional mean estimation with low data size, an empirical study},
  url          = {https://openreview.net/forum?id=1QeI99nH9k},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DivIL: Unveiling and addressing over-invariance for out-of-
distribution generalization. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=2Zan4ATYsh">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution generalization is a common problem that expects the model to perform well in the different distributions even far from the train data. A popular approach to addressing this issue is invariant learning (IL), in which the model is compiled to focus on invariant features instead of spurious features by adding strong constraints during training. However, there are some potential pitfalls of strong invariant constraints. Due to the limited number of diverse environments and over-regularization in the feature space, it may lead to a loss of important details in the invariant features while alleviating the spurious correlations, namely the over-invariance, which can also degrade the generalization performance. We theoretically define the over-invariance and observe that this issue occurs in various classic IL methods. To alleviate this issue, we propose a simple approach Diverse Invariant Learning (DivIL) by adding the unsupervised contrastive learning and the random masking mechanism compensatory for the invariant constraints, which can be applied to various IL methods. Furthermore, we conduct experiments across multiple modalities across 12 datasets and 6 classic models, verifying our over-invariance insight and the effectiveness of our DivIL framework. Our code is available at https://github.com/kokolerk/DivIL.},
  archive      = {J_TMLR},
  author       = {Jiaqi WANG and Yuhang Zhou and Zhixiong Zhang and Qiguang Chen and Yongqiang Chen and James Cheng},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {DivIL: Unveiling and addressing over-invariance for out-of- distribution generalization},
  url          = {https://openreview.net/forum?id=2Zan4ATYsh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging causality, individual fairness, and adversarial
robustness in the absence of structural causal model. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=2nRcWy3RLM">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the essential need for comprehensive considerations in responsible AI, factors such as robustness, fairness, and causality are often studied in isolation. Adversarial perturbation, used to identify vulnerabilities in models, and individual fairness, aiming for equitable treatment of similar individuals, despite initial differences, both depend on metrics to generate comparable input data instances. Previous attempts to define such joint metrics often lack general assumptions about data and were unable to reflect counterfactual proximity. To address this, our paper introduces a \emph{causal fair metric} formulated based on causal structures encompassing sensitive attributes and protected causal perturbation. To enhance the practicality of our metric, we propose metric learning as a method for metric estimation and deployment in real-world problems in the absence of structural causal models. We also demonstrate the applications of the causal fair metric in classifiers. Empirical evaluation of real-world and synthetic datasets illustrates the effectiveness of our proposed metric in achieving an accurate classifier with fairness, resilience to adversarial perturbations, and a nuanced understanding of causal relationships.},
  archive      = {J_TMLR},
  author       = {Ahmad Reza Ehyaei and Golnoosh Farnadi and Samira Samadi},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Bridging causality, individual fairness, and adversarial robustness in the absence of structural causal model},
  url          = {https://openreview.net/forum?id=2nRcWy3RLM},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selective prediction via training dynamics. <em>TMLR</em>.
(<a href="https://openreview.net/forum?id=2wgnepQjyF">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selective Prediction is the task of rejecting inputs a model would predict incorrectly on. This involves a trade-off between input space coverage (how many data points are accepted) and model utility (how good is the performance on accepted data points). Current methods for selective prediction typically impose constraints on either the model architecture or the optimization objective; this inhibits their usage in practice and introduces unknown interactions with pre-existing loss functions. In contrast to prior work, we show that state-of-the-art se- lective prediction performance can be attained solely from studying the (discretized) training dynamics of a model. We propose a general framework that, given a test input, monitors metrics capturing the instability of predictions from intermediate models (i.e., checkpoints) obtained during training w.r.t. the final model’s prediction. In particular, we reject data points exhibiting too much disagreement with the final prediction at late stages in training. The proposed rejection mechanism is domain-agnostic (i.e., it works for both discrete and real-valued prediction) and can be flexibly combined with existing selective prediction approaches as it does not require any train-time modifications. Our experimental evaluation on image classification, regression, and time series problems shows that our method beats past state-of-the-art accuracy/utility trade-offs on typical selective prediction benchmarks.},
  archive      = {J_TMLR},
  author       = {Stephan Rabanser and Anvith Thudi and Kimia Hamidieh and Adam Dziedzic and Israfil Bahceci and Akram Bin Sediq and HAMZA SOKUN and Nicolas Papernot},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Selective prediction via training dynamics},
  url          = {https://openreview.net/forum?id=2wgnepQjyF},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). APR-CNN: Convolutional neural networks for the adaptive
particle representation of large microscopy images. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=5qKI2dkrjL">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present APR-CNN, a novel class of convolutional neural networks designed for efficient and scalable three-dimensional microscopy image analysis. APR-CNNs operate natively on a sparse, multi-resolution image representation known as the Adaptive Particle Representation (APR). This significantly reduces memory and compute requirements compared to traditional pixel-based CNNs. We introduce APR-native layers for convolution, pooling, and upsampling, along with hybrid architectures that combine APR and pixel layers to balance accuracy and computational efficiency. We show in benchmarks that APR-CNNs achieve comparable segmentation accuracy to pixel-based CNNs while drastically reducing memory usage and inference time. We further showcase the potential of APR-CNNs in large-scale volumetric image analysis, reducing inference times from weeks to days. This opens up new avenues for applying deep learning to large, high-resolution, three-dimensional biomedical datasets with constrained computational resources.},
  archive      = {J_TMLR},
  author       = {Joel Jonsson and Bevan Leslie Cheeseman and Ivo Sbalzarini},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {APR-CNN: Convolutional neural networks for the adaptive particle representation of large microscopy images},
  url          = {https://openreview.net/forum?id=5qKI2dkrjL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimPLR: A simple and plain transformer for efficient object
detection and segmentation. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=6LO1y8ZE0F">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to detect objects in images at varying scales has played a pivotal role in the design of modern object detectors. Despite considerable progress in removing hand-crafted components and simplifying the architecture with transformers, multi-scale feature maps and pyramid designs remain a key factor for their empirical success. In this paper, we show that shifting the multiscale inductive bias into the attention mechanism can work well, resulting in a plain detector ‘SimPLR’ whose backbone and detection head are both non-hierarchical and operate on single-scale features. We find through our experiments that SimPLR with scale-aware attention is plain and simple architecture, yet competitive with multi-scale vision transformer alternatives. Compared to the multi-scale and single-scale state-of-the-art, our model scales better with bigger capacity (self-supervised) models and more pre-training data, allowing us to report a consistently better accuracy and faster runtime for object detection, instance segmentation, as well as panoptic segmentation. Code is released at \url{https://github.com/kienduynguyen/SimPLR}.},
  archive      = {J_TMLR},
  author       = {Duy Kien Nguyen and Martin R. Oswald and Cees G. M. Snoek},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {SimPLR: A simple and plain transformer for efficient object detection and segmentation},
  url          = {https://openreview.net/forum?id=6LO1y8ZE0F},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair principal component analysis (PCA):
Minorization-maximization algorithms for fair PCA, fair robust PCA and
fair sparse PCA. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=6jTQrr3APY">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new iterative algorithm to solve the fair PCA (FPCA) problem. We start with the max-min fair PCA formulation originally proposed in \cite{samadi1} and derive a simple and efficient iterative algorithm which is based on the minorization-maximization (MM) approach. The proposed algorithm relies on the relaxation of a semi-orthogonality constraint which is proved to be tight at every iteration of the algorithm. The vanilla version of the proposed algorithm requires solving a semi-definite program (SDP) at every iteration, which can be further simplified to a quadratic program by formulating the dual of the surrogate maximization problem. We also propose two important reformulations of the fair PCA problem: a) fair robust PCA - which can handle outliers in the data, and b) fair sparse PCA - which can enforce sparsity on the estimated fair principal components. The proposed algorithms are computationally efficient and monotonically increase their respective design objectives at every iteration. An added feature of the proposed algorithms is that they do not require the selection of any hyperparameter (except for the fair sparse PCA case where a penalty parameter that controls the sparsity has to be chosen by the user). We numerically compare the performance of the proposed methods with two of the state-of-the-art approaches on synthetic data sets and real-life data sets.},
  archive      = {J_TMLR},
  author       = {Prabhu babu and Petre Stoica and Astha Saini},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Fair principal component analysis (PCA): Minorization-maximization algorithms for fair PCA, fair robust PCA and fair sparse PCA},
  url          = {https://openreview.net/forum?id=6jTQrr3APY},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explaining explainability: Recommendations for effective use
of concept activation vectors. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=7CUluLpLxV">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept-based explanations translate the internal representations of deep learning models into a language that humans are familiar with: concepts. One popular method for finding concepts is Concept Activation Vectors (CAVs), which are learnt using a probe dataset of concept exemplars. In this work, we investigate three properties of CAVs: (1) inconsistency across layers, (2) entanglement with other concepts, and (3) spatial dependency. Each property provides both challenges and opportunities in interpreting models. We introduce tools designed to detect the presence of these properties, provide insight into how each property can lead to misleading explanations, and provide recommendations to mitigate their impact. To demonstrate practical applications, we apply our recommendations to a melanoma classification task, showing how entanglement can lead to uninterpretable results and that the choice of negative probe set can have a substantial impact on the meaning of a CAV. Further, we show that understanding these properties can be used to our advantage. For example, we introduce spatially dependent CAVs to test if a model is translation invariant with respect to a specific concept and class. Our experiments are performed on natural images (ImageNet), skin lesions (ISIC 2019), and a new synthetic dataset, Elements. Elements is designed to capture a known ground truth relationship between concepts and classes. We release this dataset to facilitate further research in understanding and evaluating interpretability methods.},
  archive      = {J_TMLR},
  author       = {Angus Nicolson and Lisa Schut and Alison Noble and Yarin Gal},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Explaining explainability: Recommendations for effective use of concept activation vectors},
  url          = {https://openreview.net/forum?id=7CUluLpLxV},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph neural networks through the lens of message
passing: A common perspective to homophily and architecture design.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=8rxtL0kZnX">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the current learning methodologies and benchmarking datasets in the hypergraph realm are obtained by \emph{lifting} procedures from their graph analogs, leading to overshadowing specific characteristics of hypergraphs. This paper attempts to confront some pending questions in that regard: Q1 Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HNNs)? Q2 How do models that employ unique characteristics of higher-order networks perform compared to lifted models? Q3 Do well-established hypergraph datasets provide a meaningful benchmark for HNNs? To address them, we first introduce a novel conceptualization of homophily in higher-order networks based on a Message Passing (MP) scheme, unifying both the analytical examination and the modeling of higher-order networks. Further, we investigate some natural strategies for processing higher-order structures within HNNs (such as keeping hyperedge-dependent node representations or performing node/hyperedge stochastic samplings), leading us to the most general MP formulation up to date --MultiSet. Finally, we conduct an extensive set of experiments that contextualize our proposals.},
  archive      = {J_TMLR},
  author       = {Lev Telyatnikov and Maria Sofia Bucarelli and Guillermo Bernardez and Olga Zaghen and Simone Scardapane and Pietro Lio},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Hypergraph neural networks through the lens of message passing: A common perspective to homophily and architecture design},
  url          = {https://openreview.net/forum?id=8rxtL0kZnX},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially personalized federated learning: Breaking the
curse of data heterogeneity. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=8tMMCf4YYn">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a partially personalized formulation of Federated Learning (FL) that strikes a balance between the flexibility of personalization and cooperativeness of global training. In our framework, we split the variables into global parameters, which are shared across all clients, and individual local parameters, which are kept private. We prove that under the right split of parameters, it is possible to find global parameters that allow each client to fit their data perfectly, and refer to the obtained problem as overpersonalized. For instance, the shared global parameters can be used to learn good data representations, whereas the personalized layers are fine-tuned for a specific client. Moreover, we present a simple algorithm for the partially personalized formulation that offers significant benefits to all clients. In particular, it breaks the curse of data heterogeneity in several settings, such as training with local steps, asynchronous training, and Byzantine-robust training.},
  archive      = {J_TMLR},
  author       = {Konstantin Mishchenko and Rustem Islamov and Eduard Gorbunov and Samuel Horváth},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Partially personalized federated learning: Breaking the curse of data heterogeneity},
  url          = {https://openreview.net/forum?id=8tMMCf4YYn},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-pass detection of jailbreaking input in large
language models. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=42v6I5Ut9a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just a forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks.},
  archive      = {J_TMLR},
  author       = {Leyla Naz Candogan and Yongtao Wu and Elias Abad Rocamora and Grigorios Chrysos and Volkan Cevher},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Single-pass detection of jailbreaking input in large language models},
  url          = {https://openreview.net/forum?id=42v6I5Ut9a},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analytical model for overparameterized learning under
class imbalance. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=69RntSRF5K">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study class-imbalanced linear classification in a high-dimensional Gaussian mixture model. We develop a tight, closed form approximation for the test error of several practical learning methods, including logit adjustment and class dependent temperature. Our approximation allows us to analytically tune and compare these methods, highlighting how and when they overcome the pitfalls of standard cross-entropy minimization. We test our theoretical findings on simulated data and imbalanced CIFAR10, MNIST and FashionMNIST datasets.},
  archive      = {J_TMLR},
  author       = {Eliav Mor and Yair Carmon},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {An analytical model for overparameterized learning under class imbalance},
  url          = {https://openreview.net/forum?id=69RntSRF5K},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relax and penalize: A new bilevel approach to mixed-binary
hyperparameter optimization. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=A1R1cQ93Cb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, bilevel approaches have become very popular to efficiently estimate high-dimensional hyperparameters of machine learning models. However, to date, binary parameters are handled by continuous relaxation and rounding strategies, which could lead to inconsistent solutions. In this context, we tackle the challenging optimization of mixed-binary hyperparameters by resorting to an equivalent continuous bilevel reformulation based on an appropriate penalty term. We propose an algorithmic framework that, under suitable assumptions, is guaranteed to provide mixed-binary solutions. Moreover, the generality of the method allows to safely use existing continuous bilevel solvers within the proposed framework. We evaluate the performance of our approach for two specific machine learning problems, i.e., the estimation of the group-sparsity structure in regression problems and the data distillation problem. The reported results show that our method is competitive with state-of-the-art approaches based on relaxation and rounding.},
  archive      = {J_TMLR},
  author       = {Sara Venturini and Marianna De Santis and Jordan Patracone and Martin Schmidt and Francesco Rinaldi and Saverio Salzo},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Relax and penalize: A new bilevel approach to mixed-binary hyperparameter optimization},
  url          = {https://openreview.net/forum?id=A1R1cQ93Cb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density of states in neural networks: An in-depth
exploration of learning in parameter space. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=BLDtWlFKhn">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning in neural networks critically hinges on the intricate geometry of the loss landscape associated with a given task. Traditionally, most research has focused on finding specific weight configurations that minimize the loss. In this work, born from the cross-fertilization of machine learning and theoretical soft matter physics, we introduce a novel approach to examine the weight space across all loss values. Employing the Wang-Landau enhanced sampling algorithm, we explore the neural network density of states -- the number of network parameter configurations that produce a given loss value -- and analyze how it depends on specific features of the training set. Using both real-world and synthetic data, we quantitatively elucidate the relation between data structure and network density of states across different sizes and depths of binary-state networks. This work presents and illustrates a novel, informative analysis method that aims at paving the way for a better understanding of the interplay between structured data and the networks that process, learn, and generate them.},
  archive      = {J_TMLR},
  author       = {Margherita Mele and Roberto Menichetti and Alessandro Ingrosso and Raffaello Potestio},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Density of states in neural networks: An in-depth exploration of learning in parameter space},
  url          = {https://openreview.net/forum?id=BLDtWlFKhn},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual learning from simulated interactions via multitask
prospective rehearsal for bionic limb behavior modeling. <em>TMLR</em>.
(<a href="https://openreview.net/forum?id=Bmy82p2eez">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lower limb amputations and neuromuscular impairments severely restrict mobility, necessitating advancements beyond conventional prosthetics. While motorized bionic limbs show promise, their effectiveness depends on replicating the dynamic coordination of human movement across diverse environments. In this paper, we introduce a model for human behavior in the context of bionic prosthesis control. Our approach leverages human locomotion demonstrations to learn the synergistic coupling of the lower limbs, enabling the prediction of the kinematic behavior of a missing limb during tasks such as walking, climbing inclines, and stairs. We propose a multitasking, continually adaptive model that anticipates and refines movements over time. At the core of our method is a technique which we call the multitask prospective rehearsal, that anticipates and synthesizes future movements based on the previous prediction and employs a corrective mechanism for subsequent predictions. Our evolving architecture merges lightweight, task-specific modules on a shared backbone, ensuring both specificity and scalability. We validate our model through experiments on real-world human gait datasets, including transtibial amputees, across a wide range of locomotion tasks. Results demonstrate that our approach consistently outperforms baseline models, particularly in scenarios with distributional shifts, adversarial perturbations, and noise.},
  archive      = {J_TMLR},
  author       = {Sharmita Dey and Benjamin Paassen and Sarath Ravindran Nair and Sabri Boughorbel and Arndt F. Schilling},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Continual learning from simulated interactions via multitask prospective rehearsal for bionic limb behavior modeling},
  url          = {https://openreview.net/forum?id=Bmy82p2eez},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-learning for graphs with heterogeneous node attribute
spaces for few-shot edge predictions. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=CAkt3DsAZs">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of edges between nodes in graph data is useful for many applications, such as social network analysis and knowledge graph completion. Existing graph neural network-based approaches have achieved notable advancements, but encounter significant difficulty in building an effective model when there is an insufficient number of known edges in graphs. Although some meta-learning approaches were introduced to solve this problem, having an assumption that the nodes of training graphs and test graphs are in homogeneous attribute spaces, which limits the flexibility of applications. In this paper, we proposed a meta-learning method for edge prediction that can learn from graphs with nodes in heterogeneous attribute spaces. The proposed model consists of attribute-wise message-passing networks that transform information between connected nodes for each attribute, resulting in attribute-specific node embeddings. The node embeddings are obtained by calculating the mean of the attribute-specific node embeddings.The encoding operation can be repeated multiple times to capture complex patterns. The attribute-wise message-passing networks are shared across all graphs, allowing knowledge transfer between different graphs.The probabilities of edges are estimated by the Euclidian distance between node embeddings. Experimental results on 14 real-world data sets demonstrate that the proposed method outperforms existing methods in edge prediction problems with sparse edge information.},
  archive      = {J_TMLR},
  author       = {Zhong Chuang and Yusuke Tanaka and Tomoharu Iwata},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Meta-learning for graphs with heterogeneous node attribute spaces for few-shot edge predictions},
  url          = {https://openreview.net/forum?id=CAkt3DsAZs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On memorization in diffusion models. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=D3DBqvSDbj">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training-data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuration, and training procedure. Besides comprehensive empirical results identifying the influential factors, we surprisingly find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models. Our study holds practical significance for diffusion model users and offers clues to theoretical research in deep generative models.},
  archive      = {J_TMLR},
  author       = {Xiangming Gu and Chao Du and Tianyu Pang and Chongxuan Li and Min Lin and Ye Wang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On memorization in diffusion models},
  url          = {https://openreview.net/forum?id=D3DBqvSDbj},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometry-aware visualization of high dimensional symmetric
positive definite matrices. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=DYCSRf3vby">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetric Positive Definite (SPD) matrices are pervasive in machine learning, from data features (such as covariance matrices) to optimization process.These matrices induce a Riemannian structure, where the curvature plays a critical role in the success of approaches based on those geometries. Yet, for ML practitioners wanting to visualize SPD matrices, the existing (flat) Euclidean approaches will hide the curvature of the manifold. To overcome this lack of expressivity in the existing algorithms, we introduce Riemannian versions of two state-of-the-art techniques, namely t-SNE and Multidimensional Scaling. Therefore, we are able to reduce a set of $c \times c$ SPD matrices into a set of $2 \times 2$ SPD matrices in order to capture the curvature information and avoid any distortion induced by flattening the representation in an Euclidean setup. Moreover, our approaches pave the way for targeting more general dimensionality reduction applications while preserving the geometry of the data. We performed experiments on controlled synthetic dataset to ensure that the low-dimensional representation preserves the geometric properties of both SPD Gaussians and geodesics. We also conduct experiments on various real datasets, such as video, anomaly detection, brain signal and others.},
  archive      = {J_TMLR},
  author       = {Thibault de Surrel and Sylvain Chevallier and Fabien Lotte and Florian Yger},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Geometry-aware visualization of high dimensional symmetric positive definite matrices},
  url          = {https://openreview.net/forum?id=DYCSRf3vby},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wasserstein coreset via sinkhorn loss. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=DrMCDS88IL">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coreset selection, a technique for compressing large datasets while preserving performance, is crucial for modern machine learning. This paper presents a novel method for generating high-quality Wasserstein coresets using the Sinkhorn loss, a powerful tool with computational advantages. However, existing approaches suffer from numerical instability in Sinkhorn&#39;s algorithm. We address this by proposing stable algorithms for the computation and differentiation of the Sinkhorn optimization problem, including an analytical formula for the derivative of the Sinkhorn loss and a rigorous stability analysis of our method. Extensive experiments demonstrate that our approach significantly outperforms existing methods in terms of sample selection quality, computational efficiency, and achieving a smaller Wasserstein distance.},
  archive      = {J_TMLR},
  author       = {Haoyun Yin and Yixuan Qiu and Xiao Wang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Wasserstein coreset via sinkhorn loss},
  url          = {https://openreview.net/forum?id=DrMCDS88IL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust preference optimization through reward model
distillation. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=E2zKNuwNDc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, the empirical evidence suggests that DPO typically assigns implicit rewards that overfit, and trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and use distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM such that its induced implicit reward, i.e., the scaled log-likelihood ratio of the model to the reference model, matches an explicit reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.},
  archive      = {J_TMLR},
  author       = {Adam Fisch and Jacob Eisenstein and Vicky Zayats and Alekh Agarwal and Ahmad Beirami and Chirag Nagpal and Peter Shaw and Jonathan Berant},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Robust preference optimization through reward model distillation},
  url          = {https://openreview.net/forum?id=E2zKNuwNDc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Producers equilibria and dynamics in engagement-driven
recommender systems. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=EWT4GxjGDS">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online platforms such as YouTube, Instagram heavily rely on recommender systems to decide what content to present to users. Producers, in turn, often create content that is likely to be recommended to users and have users engage with it. To do so, producers try to align their content with the preferences of their targeted user base. In this work, we explore the equilibrium behavior of producers who are interested in maximizing user engagement. We study two variants of the content-serving rule for the platform&#39;s recommender system, and provide a structural characterization of producer behavior at equilibrium: namely, each producer chooses to focus on a single embedded feature. We further show that specialization, defined as different producers optimizing for distinct types of content, naturally emerges from the competition among producers trying to maximize user engagement. We provide a heuristic for computing equilibria of our engagement game, and evaluate it experimentally. We highlight i) the performance and convergence of our heuristic, ii) the degree of producer specialization, and iii) the impact of the content-serving rule on producer and user utilities at equilibrium and provide guidance on how to set the content-serving rule.},
  archive      = {J_TMLR},
  author       = {Krishna Acharya and Juba Ziani and Jingyan Wang and Varun Vangala},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Producers equilibria and dynamics in engagement-driven recommender systems},
  url          = {https://openreview.net/forum?id=EWT4GxjGDS},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative risk minimization for out-of-distribution
generalization on graphs. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=EcMVskXo1n">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM.},
  archive      = {J_TMLR},
  author       = {Song Wang and Zhen Tan and Yaochen Zhu and Chuxu Zhang and Jundong Li},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Generative risk minimization for out-of-distribution generalization on graphs},
  url          = {https://openreview.net/forum?id=EcMVskXo1n},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the regularization of learnable embeddings for time
series forecasting. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=F5ALCh3GWG">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In forecasting multiple time series, accounting for the individual features of each sequence can be challenging. To address this, modern deep learning methods for time series analysis combine a shared (global) model with local layers, specific to each time series, often implemented as learnable embeddings. Ideally, these local embeddings should encode meaningful representations of the unique dynamics of each sequence. However, when these are learned end-to-end as parameters of a forecasting model, they may end up acting as mere sequence identifiers. Shared processing blocks may then become reliant on such identifiers, limiting their transferability to new contexts. In this paper, we address this issue by investigating methods to regularize the learning of local learnable embeddings for time series processing. Specifically, we perform the first extensive empirical study on the subject and show how such regularizations consistently improve performance in widely adopted architectures. Furthermore, we show that methods attempting to prevent the co-adaptation of local and global parameters by means of embeddings perturbation are particularly effective in this context. In this regard, we include in the comparison several perturbation-based regularization methods, going as far as periodically resetting the embeddings during training. The obtained results provide an important contribution to understanding the interplay between learnable local parameters and shared processing layers: a key challenge in modern time series processing models and a step toward developing effective foundation models for time series.},
  archive      = {J_TMLR},
  author       = {Luca Butera and Giovanni De Felice and Andrea Cini and Cesare Alippi},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On the regularization of learnable embeddings for time series forecasting},
  url          = {https://openreview.net/forum?id=F5ALCh3GWG},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Necessary and sufficient watermark for large language
models. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=FcyHZ6Q4k0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) can now generate texts that are indistinguishable from those written by humans. Such remarkable performance of LLMs increases their risk of being used for malicious purposes. Thus, it is necessary to develop methods for distinguishing texts written by LLMs from those written by humans. Watermarking is one of the most powerful methods for achieving this. Although existing methods have successfully detected texts generated by LLMs, they inevitably degrade the text quality. In this study, we propose the Necessary and Sufficient Watermark (NS-Watermark) for inserting watermarks into generated texts with minimum text quality degradation. More specifically, we derive minimum constraints required to be imposed on the generated texts to distinguish whether LLMs or humans write the texts, and we formulate the NS-Watermark as a constrained optimization problem. Through the experiments, we demonstrate that the NS-Watermark can generate more natural texts than existing watermarking methods and distinguish more accurately between texts written by LLMs and those written by humans. Especially in machine translation tasks, the NS-Watermark can outperform the existing watermarking method by up to 30 BLEU scores.},
  archive      = {J_TMLR},
  author       = {Yuki Takezawa and Ryoma Sato and Han Bao and Kenta Niwa and Makoto Yamada},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Necessary and sufficient watermark for large language models},
  url          = {https://openreview.net/forum?id=FcyHZ6Q4k0},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics-inspired structure hallucination for
protein-protein interaction modeling. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=GGHk5ukO6t">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein-protein interaction (PPI) represents a central challenge within the biology field, and accurately predicting the consequences of mutations in this context is crucial for drug design and protein engineering. Deep learning (DL) has shown promise in forecasting the effects of such mutations but is hindered by two primary constraints. First, the structures of mutant proteins are often elusive to acquire. Secondly, PPI takes place dynamically, which is rarely integrated into the DL architecture design. To address these obstacles, we present a novel framework named Refine-PPI with two key enhancements. First, we introduce a structure refinement module trained by a mask mutation modeling (MMM) task on available wild-type structures, which is then transferred to hallucinate the inaccessible mutant structures. Second, we employ a new kind of geometric network, called the probability density cloud network (PDC-Net), to capture 3D dynamic variations and encode the atomic uncertainty associated with PPI. Comprehensive experiments on SKEMPI.v2 substantiate the superiority of Refine-PPI over all existing tools for predicting free energy change. These findings underscore the effectiveness of our hallucination strategy and the PDC module in addressing the absence of mutant protein structure and modeling geometric uncertainty.},
  archive      = {J_TMLR},
  author       = {Fang Wu and Stan Z. Li},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Dynamics-inspired structure hallucination for protein-protein interaction modeling},
  url          = {https://openreview.net/forum?id=GGHk5ukO6t},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of best-of-n sampling strategies for language
model alignment. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=H4S4ETc8c9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) with human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Since the reward model is an imperfect proxy for the true objective, an excessive focus on optimizing its value can lead to a compromise of its performance on the true objective. Previous work proposes Regularized BoN sampling (RBoN), a BoN sampling with regularization to the objective, and shows that it outperforms BoN sampling so that it mitigates reward hacking and empirically (Jinnai et al., 2024). However, Jinnai et al. (2024) introduce RBoN based on a heuristic and they lack the analysis of why such regularization strategy improves the performance of BoN sampling. The aim of this study is to analyze the effect of BoN sampling on regularization strategies. Using the regularization strategies corresponds to robust optimization, which maximizes the worst case over a set of possible perturbations in the proxy reward. Although the theoretical guarantees are not directly applicable to RBoN, RBoN corresponds to a practical implementation. This paper proposes an extension of the RBoN framework, called Stochastic RBoN sampling (SRBoN), which is a theoretically guaranteed approach to worst-case RBoN in proxy reward. We then perform an empirical evaluation using the AlpacaFarm and Anthropic’s hh-rlhf datasets to evaluate which factors of the regularization strategies contribute to the improvement of the true proxy reward. In addition, we also propose another simple RBoN method, the Sentence Length Regularized BoN, which has a better performance in the experiment as compared to the previous methods.},
  archive      = {J_TMLR},
  author       = {Yuki Ichihara and Yuu Jinnai and Tetsuro Morimura and Kenshi Abe and Kaito Ariu and Mitsuki Sakamoto and Eiji Uchibe},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Evaluation of best-of-N sampling strategies for language model alignment},
  url          = {https://openreview.net/forum?id=H4S4ETc8c9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking spectral augmentation for contrast-based graph
self-supervised learning. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=HjpD5kpfa3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent surge in contrast-based graph self-supervised learning has prominently featured an intensified exploration of spectral cues. Spectral augmentation, which involves modifying a graph&#39;s spectral properties such as eigenvalues or eigenvectors, is widely believed to enhance model performance. However, an intriguing paradox emerges, as methods grounded in seemingly conflicting assumptions regarding the spectral domain demonstrate notable enhancements in learning performance. Through extensive empirical studies, we find that simple edge perturbations - random edge dropping for node-level and random edge adding for graph-level self-supervised learning - consistently yield comparable or superior performance while being significantly more computationally efficient. This suggests that the computational overhead of sophisticated spectral augmentations may not justify their practical benefits. Our theoretical analysis of the InfoNCE loss bounds for shallow GNNs further supports this observation. The proposed insights represent a significant leap forward in the field, potentially refining the understanding and implementation of graph self-supervised learning.},
  archive      = {J_TMLR},
  author       = {Xiangru Jian and Xinjian Zhao and Wei Pang and Chaolong Ying and Yimu Wang and Yaoyao Xu and Tianshu Yu},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Rethinking spectral augmentation for contrast-based graph self-supervised learning},
  url          = {https://openreview.net/forum?id=HjpD5kpfa3},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the stability of gradient descent with second order
dynamics for time-varying cost functions. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=HlzjI2fn2T">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient based optimization algorithms deployed in Machine Learning (ML) applications are often analyzed and compared by their convergence rates or regret bounds. While these rates and bounds convey valuable information they don’t always directly translate to stability guarantees. Stability and similar concepts, like robustness, will become ever more important as we move towards deploying models in real-time and safety critical systems. In this work we build upon the results in Gaudio et al. 2021 and Moreu &amp; Annaswamy 2022 for gradient descent with second order dynamics when applied to explicitly time varying cost functions and provide more general stability guarantees. These more general results can aid in the design and certification of these optimization schemes so as to help ensure safe and reliable deployment for real-time learning applications. We also hope that the techniques provided here will stimulate and cross-fertilize the analysis that occurs on the same algorithms from the online learning and stochastic optimization communities.},
  archive      = {J_TMLR},
  author       = {Travis E Gibson and Sawal Acharya and Anjali Parashar and Joseph Emilio Gaudio and Anuradha Annaswamy},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On the stability of gradient descent with second order dynamics for time-varying cost functions},
  url          = {https://openreview.net/forum?id=HlzjI2fn2T},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nomic embed: Training a reproducible long context text
embedder. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=IPmzyQSiQE">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on the short-context MTEB benchmark and the long context LoCo benchmark. We release the training code and model weights under an Apache 2.0 license. In contrast with other open-source models, we release the full curated training data and code that allows for full replication of nomic-embed-text-v1. You can find code and data to replicate the model at \href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}},
  archive      = {J_TMLR},
  author       = {Zach Nussbaum and John Xavier Morris and Andriy Mulyar and Brandon Duderstadt},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Nomic embed: Training a reproducible long context text embedder},
  url          = {https://openreview.net/forum?id=IPmzyQSiQE},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metalearning continual learning algorithms. <em>TMLR</em>.
(<a href="https://openreview.net/forum?id=IaUh7CSD3k">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF), i.e., previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to metalearn their own in-context continual (meta)learning algorithms. ACL encodes continual learning (CL) desiderata---good performance on both old and new tasks---into its metalearning objectives. Our experiments demonstrate that ACL effectively resolves &quot;in-context catastrophic forgetting,&quot; a problem that naive in-context learning algorithms suffer from; ACL learned algorithms outperform both hand-crafted learning algorithms and popular meta-continual learning methods on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple standard image classification datasets. We also discuss the current limitations of in-context CL by comparing ACL with state-of-the-art CL methods that leverage pre-trained models. Overall, we bring several novel perspectives into the long-standing problem of CL.},
  archive      = {J_TMLR},
  author       = {Kazuki Irie and Róbert Csordás and Jürgen Schmidhuber},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Metalearning continual learning algorithms},
  url          = {https://openreview.net/forum?id=IaUh7CSD3k},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What makes ImageNet look unlike LAION. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=IrBYuh9W3T">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ImageNet was famously created by querying several image search engines such as Flickr. What if we recreated ImageNet instead by searching the massive LAION dataset based on image captions alone? In this work, we carry out this counterfactual investigation. We find that the resulting ImageNet recreation, which we call LAIONet, looks distinctly unlike the original. Specifically, the intra-class similarity of images in the original ImageNet is dramatically higher than it is for LAIONet. Consequently, models trained on ImageNet perform significantly worse on LAIONet. We propose a rigorous explanation for the discrepancy in terms of a subtle, yet important, difference in two plausible causal data-generating processes for the respective datasets, that we support with systematic experimentation. In a nutshell, searching based on an image caption alone creates an information bottleneck that mitigates the selection bias otherwise present in image-based filtering. Our explanation formalizes a long-held intuition in the community that ImageNet images are stereotypical, unnatural, and overly simple representations of the class category. At the same time, it provides a simple and actionable takeaway for future dataset creation efforts.},
  archive      = {J_TMLR},
  author       = {Ali Shirali and Moritz Hardt},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {What makes ImageNet look unlike LAION},
  url          = {https://openreview.net/forum?id=IrBYuh9W3T},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fused gromov-wasserstein approach to subgraph contrastive
learning. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=J7cY9Jr9WM">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning has become a key method for training deep learning models when labeled data is scarce or unavailable. While graph machine learning holds great promise across various domains, the design of effective pretext tasks for self-supervised graph representation learning remains challenging. Contrastive learning, a popular approach in graph self-supervised learning, leverages positive and negative pairs to compute a contrastive loss function. However, current graph contrastive learning methods often struggle to fully use structural patterns and node similarities. To address these issues, we present a new method called Fused Gromov-Wasserstein Subgraph Contrastive Learning (FOSSIL). Our method integrates node-level and subgraph-level contrastive learning, seamlessly combining a standard node-level contrastive loss with the Fused Gromov-Wasserstein distance. This combination helps our method capture both node features and graph structure together. Importantly, our approach works well with both homophilic and heterophilic graphs and can dynamically create views for generating positive and negative pairs. Through extensive experiments on benchmark graph datasets, we show that FOSSIL outperforms or achieves competitive performance compared to current state-of-the-art methods.},
  archive      = {J_TMLR},
  author       = {Amadou Siaka SANGARE and Nicolas Dunou and Jhony H. Giraldo and Fragkiskos D. Malliaros},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {A fused gromov-wasserstein approach to subgraph contrastive learning},
  url          = {https://openreview.net/forum?id=J7cY9Jr9WM},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JoIN: Joint GANs inversion for intrinsic image
decomposition. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=JEHIVfjmOf">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic Image Decomposition (IID) is a challenging inverse problem that seeks to decompose a natural image into its underlying intrinsic components such as albedo and shading. While recent image decomposition methods rely on learning-based priors on these components, they often suffer from component cross-contamination owing to joint training of priors; or from Sim-to-Real gap since the priors trained on synthetic data are kept frozen during the inference on real images. In this work, we propose to solve the intrinsic image decomposition problem using a bank of Generative Adversarial Networks (GANs) as priors where each GAN is independently trained only on a single intrinsic component, providing stronger and more disentangled priors. At the core of our approach is the idea that the latent space of a GAN is a well-suited optimization domain to solve inverse problems. Given an input image, we propose to jointly invert the latent codes of a set of GANs and combine their outputs to reproduce the input. Contrary to all existing GAN inversion methods that are limited to inverting only a single GAN, our proposed approach, JoIN, is able to jointly invert multiple GANs using only a single image as supervision while still maintaining distribution priors of each intrinsic component. We show that our approach is modular, allowing various forward imaging models, and that it can successfully decompose both synthetic and real images. Further, taking inspiration from existing GAN inversion approaches, we allow for careful fine-tuning of the generator priors during the inference on real images. This way, our method is able to achieve excellent generalization on real images even though it uses only synthetic data to train the GAN priors. We demonstrate the success of our approach through exhaustive qualitative and quantitative evaluations and ablation studies on various datasets.},
  archive      = {J_TMLR},
  author       = {Viraj Shah and Svetlana Lazebnik and Julien Philip},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {JoIN: Joint GANs inversion for intrinsic image decomposition},
  url          = {https://openreview.net/forum?id=JEHIVfjmOf},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A strong baseline for molecular few-shot learning.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=JQ0agisXny">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies. We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts. In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods.},
  archive      = {J_TMLR},
  author       = {Philippe Formont and Hugo Jeannin and Pablo Piantanida and Ismail Ben Ayed},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {A strong baseline for molecular few-shot learning},
  url          = {https://openreview.net/forum?id=JQ0agisXny},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards context and domain-aware algorithms for scene
analysis. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=JQGmbVK4Fr">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpersonal interactions and social situations in multimedia content encompass a rich blend of visual, textual, audio and contextual cues as well. However, contextual data integration in multimodal scene analysis research has often been overlooked, leading to incomplete interpretations. For instance, recognizing that two combatants in a video are positioned within a designated ring with a dedicated referee drastically alters the perception from a simple scuffle to a structured martial arts contest. This paper presents an innovative approach to scene analysis in video content, which not only incorporates contextual data but also emphasizes the most significant features during training. Additionally, we introduce a methodology for integrating domain knowledge into our framework. We evaluate our proposed methodology using two comprehensive datasets, demonstrating promising results compared to a baseline study using one of the datasets. These findings underscore the importance of integrating contextual data into multimodal video analysis, while also recognizing the challenges associated with their utilization.},
  archive      = {J_TMLR},
  author       = {Ibrahim Serouis and Florence Sèdes},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Towards context and domain-aware algorithms for scene analysis},
  url          = {https://openreview.net/forum?id=JQGmbVK4Fr},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Provable quantum algorithm advantage for gaussian process
quadrature. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=K6CvWPtF62">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to develop novel quantum algorithms for Gaussian process quadrature methods. Gaussian process quadratures are numerical integration methods where Gaussian processes are used as functional priors for the integrands to capture the uncertainty arising from the sparse function evaluations. Quantum computers have emerged as potential replacements for classical computers, offering exponential reductions in the computational complexity of machine learning tasks. In this paper, we combine Gaussian process quadrature and quantum computing by proposing a quantum low-rank Gaussian process quadrature method based on a Hilbert space approximation of the Gaussian process kernel and enhancing the quadrature using a quantum circuit. The method combines the quantum phase estimation algorithm with the quantum principal component analysis technique to extract information up to a desired rank. Then, Hadamard and SWAP tests are implemented to find the expected value and variance that determines the quadrature. We use numerical simulations of a quantum computer to demonstrate the effectiveness of the method. Furthermore, we provide a theoretical complexity analysis that shows a polynomial advantage over classical Gaussian process quadrature methods. The code is available at https://github.com/cagalvisf/Quantum_HSGPQ.},
  archive      = {J_TMLR},
  author       = {Cristian A. Galvis-Florez and Ahmad Farooq and Simo Särkkä},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Provable quantum algorithm advantage for gaussian process quadrature},
  url          = {https://openreview.net/forum?id=K6CvWPtF62},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformalized credal regions for classification with
ambiguous ground truth. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=L7sQ8CW2FY">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An open question in Imprecise Probabilistic Machine Learning is how to empirically derive a credal region (i.e., a closed and convex family of probabilities on the output space) from the available data, without any prior knowledge or assumption. In classification problems, credal regions are a tool that is able to provide provable guarantees under realistic assumptions by characterizing the uncertainty about the distribution of the labels. Building on previous work, we show that credal regions can be directly constructed using conformal methods. This allows us to provide a novel extension of classical conformal prediction to problems with ambiguous ground truth, that is, when the exact labels for given inputs are not exactly known. The resulting construction enjoys desirable practical and theoretical properties: (i) conformal coverage guarantees, (ii) smaller prediction sets (compared to classical conformal prediction regions) and (iii) disentanglement of uncertainty sources (epistemic, aleatoric). We empirically verify our findings on both synthetic and real datasets.},
  archive      = {J_TMLR},
  author       = {Michele Caprio and David Stutz and Shuo Li and Arnaud Doucet},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Conformalized credal regions for classification with ambiguous ground truth},
  url          = {https://openreview.net/forum?id=L7sQ8CW2FY},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterated <span
class="math inline"><em>Q</em></span>-network: Beyond one-step bellman
updates in deep reinforcement learning. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=Lt2H8Bd8jF">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast majority of Reinforcement Learning methods is largely impacted by the computation effort and data requirements needed to obtain effective estimates of action-value functions, which in turn determine the quality of the overall performance and the sample-efficiency of the learning procedure. Typically, action-value functions are estimated through an iterative scheme that alternates the application of an empirical approximation of the Bellman operator and a subsequent projection step onto a considered function space. It has been observed that this scheme can be potentially generalized to carry out multiple iterations of the Bellman operator at once, benefiting the underlying learning algorithm. However, until now, it has been challenging to effectively implement this idea, especially in high-dimensional problems. In this paper, we introduce iterated $Q$-Network (i-QN), a novel principled approach that enables multiple consecutive Bellman updates by learning a tailored sequence of action-value functions where each serves as the target for the next. We show that i-QN is theoretically grounded and that it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate the advantages of i-QN in Atari $2600$ games and MuJoCo continuous control problems.},
  archive      = {J_TMLR},
  author       = {Théo Vincent and Daniel Palenicek and Boris Belousov and Jan Peters and Carlo D&#39;Eramo},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Iterated $Q$-network: Beyond one-step bellman updates in deep reinforcement learning},
  url          = {https://openreview.net/forum?id=Lt2H8Bd8jF},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-step refinement network for robust point
cloud registration. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=M3SkSMfWcP">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point Cloud Registration (PCR) estimates the relative rigid transformation between two point clouds of the same scene. Despite significant progress with learning-based approaches, existing methods still face challenges when the overlapping region between the two point clouds is small. In this paper, we propose an adaptive multi-step refinement network that refines the registration quality at each step by leveraging the information from the preceding step. To achieve this, we introduce a training procedure and a refinement network. Firstly, to adapt the network to the current step, we utilize a generalized one-way attention mechanism, which prioritizes the last step&#39;s estimated overlapping region, and we condition the network on step indices. Secondly, instead of training the network to map either random transformations or a fixed pre-trained model&#39;s estimations to the ground truth, we train it on transformations with varying registration qualities, ranging from accurate to inaccurate, thereby enhancing the network&#39;s adaptiveness and robustness. Despite its conceptual simplicity, our method achieves state-of-the-art performance on both the 3DMatch/3DLoMatch and KITTI benchmarks. Notably, on 3DLoMatch, our method reaches 80.4% recall rate, with an absolute improvement of 1.2%.},
  archive      = {J_TMLR},
  author       = {Zhi Chen and Yufan Ren and Tong Zhang and Zheng Dang and Wenbing Tao and Sabine Susstrunk and Mathieu Salzmann},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Adaptive multi-step refinement network for robust point cloud registration},
  url          = {https://openreview.net/forum?id=M3SkSMfWcP},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REX: GPU-accelerated Sim2Real framework with delay and
dynamics estimation. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=O4CQ5AM5yP">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sim2real, the transfer of control policies from simulation to the real world, is crucial for efficiently solving robotic tasks without the risks associated with real-world learning. However, discrepancies between simulated and real environments, especially due to unmodeled dynamics and latencies, significantly impact the performance of these transferred policies. In this paper, we address the challenges of sim2real transfer caused by latency and asynchronous dynamics in real-world robotic systems. Our approach involves developing a novel framework, REX (Robotic Environments with jaX), that uses a graph-based simulation model to incorporate latency effects while optimizing for parallelization on accelerator hardware. Our framework simulates the asynchronous, hierarchical nature of real-world systems, while simultaneously estimating system dynamics and delays from real-world data and implementing delay compensation strategies to minimize the sim2real gap. We validate our approach on two real-world systems, demonstrating its effectiveness in improving sim2real performance by accurately modeling both system dynamics and delays. Our results show that the proposed framework supports both accelerated simulation and real-time processing, making it valuable for robot learning.},
  archive      = {J_TMLR},
  author       = {Bas van der Heijden and Jens Kober and Robert Babuska and Laura Ferranti},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {REX: GPU-accelerated Sim2Real framework with delay and dynamics estimation},
  url          = {https://openreview.net/forum?id=O4CQ5AM5yP},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum mean discrepancy on exponential windows for online
change detection. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=OGaTF9iOxi">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting changes is of fundamental importance when analyzing data streams and has many applications, e.g., in predictive maintenance, fraud detection, or medicine. A principled approach to detect changes is to compare the distributions of observations within the stream to each other via hypothesis testing. Maximum mean discrepancy (MMD), a (semi-)metric on the space of probability distributions, provides powerful non-parametric two-sample tests on kernel-enriched domains. In particular, MMD is able to detect any disparity between distributions under mild conditions. However, classical MMD estimators suffer from a quadratic runtime complexity, which renders their direct use for change detection in data streams impractical. In this article, we propose a new change detection algorithm, called Maximum Mean Discrepancy on Exponential Windows (MMDEW), that combines the benefits of MMD with an efficient computation based on exponential windows. We prove that MMDEW enjoys polylogarithmic runtime and logarithmic memory complexity and show empirically that it outperforms the state of the art on benchmark data streams.},
  archive      = {J_TMLR},
  author       = {Florian Kalinke and Marco Heyden and Georg Gntuni and Edouard Fouché and Klemens Böhm},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Maximum mean discrepancy on exponential windows for online change detection},
  url          = {https://openreview.net/forum?id=OGaTF9iOxi},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DELTA: Dual consistency delving with topological uncertainty
for active graph domain adaptation. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=P5y82LKGbY">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph domain adaptation has recently enabled knowledge transfer across different graphs. However, without the semantic information on target graphs, the performance on target graphs is still far from satisfactory. To address the issue, we study the problem of active graph domain adaptation, which selects a small quantitative of informative nodes on the target graph for extra annotation. This problem is highly challenging due to the complicated topological relationships and the distribution discrepancy across graphs. In this paper, we propose a novel approach named Dual Consistency Delving with Topological Uncertainty (DELTA) for active graph domain adaptation. Our DELTA consists of an edge-oriented graph subnetwork and a path-oriented graph subnetwork, which can explore topological semantics from complementary perspectives. In particular, our edge-oriented graph subnetwork utilizes the message passing mechanism to learn neighborhood information, while our path-oriented graph subnetwork explores high-order relationships from substructures. To jointly learn from two subnetworks, we roughly select informative candidate nodes with the consideration of consistency across two subnetworks. Then, we aggregate local semantics from its K-hop subgraph based on node degrees for topological uncertainty estimation. To overcome potential distribution shifts, we compare target nodes and their corresponding source nodes for discrepancy scores as an additional component for fine selection. Extensive experiments on benchmark datasets demonstrate that DELTA outperforms various state-of-the-art approaches. The code implementation of DELTA is available at https://github.com/goose315/DELTA.},
  archive      = {J_TMLR},
  author       = {Pengyun Wang and Yadi Cao and Chris Russell and Yanxin Shen and Junyu Luo and Ming Zhang and Siyu Heng and Xiao Luo},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {DELTA: Dual consistency delving with topological uncertainty for active graph domain adaptation},
  url          = {https://openreview.net/forum?id=P5y82LKGbY},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the sample complexity of one hidden layer networks with
equivariance, locality and weight sharing. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=Q7aXOnEGgU">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weight sharing, equivariance, and local filters, as in convolutional neural networks, are believed to contribute to the sample efficiency of neural networks. However, it is not clear how each one of these design choices contributes to the generalization error. Through the lens of statistical learning theory, we aim to provide insight into this question by characterizing the relative impact of each choice on the sample complexity. We obtain lower and upper sample complexity bounds for a class of single hidden layer networks. For a large class of activation functions, the bounds depend merely on the norm of filters and are dimension-independent. We also provide bounds for max-pooling and an extension to multi-layer networks, both with mild dimension dependence. We provide a few takeaways from the theoretical results. It can be shown that depending on the weight-sharing mechanism, the non-equivariant weight-sharing can yield a similar generalization bound as the equivariant one. We show that locality has generalization benefits, however the uncertainty principle implies a trade-off between locality and expressivity. We conduct extensive experiments and highlight some consistent trends for these models.},
  archive      = {J_TMLR},
  author       = {Arash Behboodi and Gabriele Cesa},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On the sample complexity of one hidden layer networks with equivariance, locality and weight sharing},
  url          = {https://openreview.net/forum?id=Q7aXOnEGgU},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Where do we stand with implicit neural representations? A
technical and performance survey. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=QTsJXSvAI2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit Neural Representations (INRs) have emerged as a paradigm in knowledge representation, offering exceptional flexibility and performance across a diverse range of applications. INRs leverage multilayer perceptrons (MLPs) to model data as continuous implicit functions, providing critical advantages such as resolution independence, memory efficiency, and generalisation beyond discretised data structures. Their ability to solve complex inverse problems makes them particularly effective for tasks including audio reconstruction, image representation, 3D object reconstruction, and high-dimensional data synthesis. This survey provides a comprehensive review of state-of-the-art INR methods, introducing a clear taxonomy that categorises them into four key areas: activation functions, position encoding, combined strategies, and network structure optimisation. We rigorously analyse their critical properties—such as full differentiability, smoothness, compactness, and adaptability to varying resolutions—while also examining their strengths and limitations in addressing locality biases and capturing fine details. Our experimental comparison offers new insights into the trade-offs between different approaches, showcasing the capabilities and challenges of the latest INR techniques across various tasks. In addition to identifying areas where current methods excel, we highlight key limitations and potential avenues for improvement, such as developing more expressive activation functions, enhancing positional encoding mechanisms, and improving scalability for complex, high-dimensional data. This survey serves as a roadmap for researchers, offering practical guidance for future exploration in the field of INRs. We aim to foster new methodologies by outlining promising research directions for INRs and applications.},
  archive      = {J_TMLR},
  author       = {Amer Essakine and Yanqi Cheng and Chun-Wun Cheng and Lipei Zhang and Zhongying Deng and Lei Zhu and Carola-Bibiane Schönlieb and Angelica I Aviles-Rivero},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Where do we stand with implicit neural representations? a technical and performance survey},
  url          = {https://openreview.net/forum?id=QTsJXSvAI2},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On space folds of ReLU neural networks. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=RfFqBXLDQk">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent findings suggest that the consecutive layers of ReLU neural networks can be understood geometrically as space folding transformations of the input space, revealing patterns of self-similarity. In this paper, we present the first quantitative analysis of this space folding phenomenon in ReLU neural networks. Our approach focuses on examining how straight paths in the Euclidean input space are mapped to their counterparts in the Hamming activation space. In this process, the convexity of straight lines is generally lost, giving rise to non-convex folding behavior. To quantify this effect, we introduce a novel measure based on range metrics, similar to those used in the study of random walks, and provide the proof for the equivalence of convexity notions between the input and activation spaces. Furthermore, we provide empirical analysis on a geometrical analysis benchmark (CantorNet) as well as an image classification benchmark (MNIST). Our work advances the understanding of the activation space in ReLU neural networks by leveraging the phenomena of geometric folding, providing valuable insights on how these models process input information.},
  archive      = {J_TMLR},
  author       = {Michal Lewandowski and Hamid Eghbalzadeh and Bernhard Heinzl and Raphael Pisoni and Bernhard A. Moser},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On space folds of ReLU neural networks},
  url          = {https://openreview.net/forum?id=RfFqBXLDQk},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised discovery of object-centric neural fields.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=ScEv13W2f1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study inferring 3D object-centric scene representations from a single image. While recent methods have shown potential in unsupervised 3D object discovery, they are limited in generalizing to unseen spatial configurations. This limitation stems from the lack of translation invariance in their 3D object representations. Previous 3D object discovery methods entangle objects’ intrinsic attributes like shape and appearance with their 3D locations. This entanglement hinders learning generalizable 3D object representations. To tackle this bottleneck, we propose the unsupervised discovery of Object-Centric neural Fields (uOCF), which integrates translation invariance into the object representation. To allow learning object-centric representations from limited real-world images, we further introduce an object prior learning method that transfers object-centric prior knowledge from a synthetic dataset. To evaluate our approach, we collect four new datasets, including two real kitchen environments. Extensive experiments show that our approach significantly improves generalization and sample efficiency and enables unsupervised 3D object discovery in real scenes. Notably, uOCF demonstrates zero-shot generalization to unseen objects from a single real image. We attach our code in the supplementary file, and the project page is available at https://red-fairy.github.io/uOCF/},
  archive      = {J_TMLR},
  author       = {Rundong Luo and Hong-Xing Yu and Jiajun Wu},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Unsupervised discovery of object-centric neural fields},
  url          = {https://openreview.net/forum?id=ScEv13W2f1},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preserving privacy in large language models: A survey on
current threats and solutions. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=Ss9MTTN7OL">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains. However, their reliance on massive internet-sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare). Moreover, certain application-specific scenarios may require fine-tuning these models on private data. This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadvertently reveal sensitive information. We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms throughout the entire learning pipeline. These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning after training. Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs. This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks.},
  archive      = {J_TMLR},
  author       = {Michele Miranda and Elena Sofia Ruzzetti and Andrea Santilli and Fabio Massimo Zanzotto and Sébastien Bratières and Emanuele Rodolà},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Preserving privacy in large language models: A survey on current threats and solutions},
  url          = {https://openreview.net/forum?id=Ss9MTTN7OL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Over-parameterised shallow neural networks with asymmetrical
node scaling: Global convergence guarantees and feature learning.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=Sx1khIIi95">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider gradient-based optimisation of wide, shallow neural networks, where the output of each hidden node is scaled by a positive parameter. The scaling parameters are non-identical, differing from the classical Neural Tangent Kernel (NTK) parameterisation. We prove that for large such neural networks, with high probability, gradient flow and gradient descent converge to a global minimum and can learn features in some sense, unlike in the NTK parameterisation. We perform experiments illustrating our theoretical results and discuss the benefits of such scaling in terms of prunability and transfer learning.},
  archive      = {J_TMLR},
  author       = {Francois Caron and Fadhel Ayed and Paul Jung and Hoil Lee and Juho Lee and Hongseok Yang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Over-parameterised shallow neural networks with asymmetrical node scaling: Global convergence guarantees and feature learning},
  url          = {https://openreview.net/forum?id=Sx1khIIi95},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARVideo: Autoregressive pretraining for self-supervised
video representation learning. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=TRKwzPnXWQ">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new self-supervised video representation learning framework \textbf{ARVideo}, which \textit{autoregressively} predict the next video token in a tailored sequence order. Two key designs are included. First, we organize autoregressive video tokens into clusters that span both \textit{spatially} and \textit{temporally}, thereby enabling a richer aggregation of contextual information compared to the standard spatial-only or temporal-only clusters. Second, we adopt a randomized spatiotemporal prediction order to facilitate learning from multi-dimensional data, addressing the limitations of a handcrafted spatial-first or temporal-first sequence order. Extensive experiments establish ARVideo as an effective paradigm for self-supervised video representation learning. For example, when trained with the ViT-B backbone, ARVideo competitively attains 81.2\% on Kinetics-400 and 70.9\% on Something-Something V2, which are on par with the strong benchmark set by VideoMAE. Importantly, ARVideo also demonstrates higher training efficiency, \ie, it trains 14\% faster and requires 58\% less GPU memory compared to VideoMAE.},
  archive      = {J_TMLR},
  author       = {Sucheng Ren and Hongru Zhu and Chen Wei and Yijiang Li and Alan Yuille and Cihang Xie},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {ARVideo: Autoregressive pretraining for self-supervised video representation learning},
  url          = {https://openreview.net/forum?id=TRKwzPnXWQ},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting benford’s law for weight regularization of deep
neural networks. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=TnT59yz7lc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic learning of Deep Neural Network (DNN) parameters is highly sensitive to training strategy, hyperparameters, and available training data. Many state-of-the-art solutions use weight regularization to adjust parameter distributions, prevent overfitting, and support generalization of DNNs. None of the existing regularization techniques have ever exploited a typical distribution of numerical datasets with respect to the first non-zero (or significant) digit, called Benford&#39;s Law (BL). In this paper, we show that the deviation of the significant digit distribution of the DNN weights from BL is closely related to the generalization of the DNN. In particular, when the DNN is presented with limited training data. To take advantage of this finding, we use BL to target the weight regularization of DNNs. Extensive experiments are performed on image, table, and speech data, considering convolutional (CNN) and Transformer-based neural network architectures with varying numbers of parameters. We show that the performance of DNNs is improved by minimizing the distance between the significant digit distributions of the DNN weights and the BL distribution along with L2 regularization. The improvements depend on the network architecture and how it deals with limited data. However, the proposed penalty term improves consistently and some CNN-based architectures gain up to $15\%$ test accuracy over the default training scheme with L2 regularization on subsets of CIFAR 100.},
  archive      = {J_TMLR},
  author       = {Julius Ott and Huawei Sun and Enrico Rinaldi and Gianfranco Mauro and Lorenzo Servadei and Robert Wille},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Exploiting benford&#39;s law for weight regularization of deep neural networks},
  url          = {https://openreview.net/forum?id=TnT59yz7lc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using representation balancing to learn conditional-average
dose responses from clustered data. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=U8EMkndyq4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the response to an intervention with an associated dose conditional on a unit&#39;s covariates, the &quot;conditional-average dose response&quot; (CADR), is a relevant task in a variety of domains, from healthcare to business, economics, and beyond. Estimating such a response is challenging for several reasons: Firstly, it typically needs to be estimated from observational data, which can be confounded and negatively affect the performance of intervention response estimators used for counterfactual inference. Secondly, the continuity of the dose prevents the adoption of approaches used to estimate responses to binary-valued interventions. That is why the machine learning (ML) community has proposed several tailored CADR estimators. Yet, the proposal of most of these methods requires strong assumptions on the distribution of data and the assignment of interventions, which go beyond the standard assumptions in causal inference. Whereas previous works have so far focused on smooth shifts in covariate distributions across doses, in this work, we will study estimating CADR from clustered data and where different doses are assigned to different segments of a population. On a novel benchmarking dataset, we show the impacts of clustered data on model performance. Additionally, we propose an estimator, CBRNet, that enables the application of representation balancing for CADR estimation through clustering the covariate space and a novel loss function. CBRNet learns cluster-agnostic and hence dose-agnostic covariate representations through representation balancing for unbiased CADR inference. We run extensive experiments to illustrate the workings of our method and compare it with the state of the art in ML for CADR estimation.},
  archive      = {J_TMLR},
  author       = {Christopher Bockel-Rickermann and Toon Vanderschueren and Jeroen Berrevoets and Tim Verdonck and Wouter Verbeke},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Using representation balancing to learn conditional-average dose responses from clustered data},
  url          = {https://openreview.net/forum?id=U8EMkndyq4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Score-based denoising diffusion models for photon-starved
image restoration problems. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=UYXPt7HUdl">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Score-based denoising diffusion models have recently emerged as a powerful strategy to solve image restoration problems. Early diffusion models required problem-specific training. However, modern approaches can combine a likelihood function that is specified during test-time with a foundational pretrained diffusion model, which is used as an implicit prior in a Plug-and-Play (PnP) manner. This approach has been shown to deliver state-of-the-art performance in a wide range of image restoration problems involving Gaussian and mild Poisson noise. With extreme computer vision applications in mind, this paper presents the first PnP denoising diffusion method for photon-starved imaging problems. These problems arise in new quantum-enhanced imaging systems that exploit the particle nature of light to exceed the limitations of classical imaging. The problems involve highly challenging noise statistics, such as binomial, geometric, and low-intensity Poisson noise, which are difficult because of high uncertainty about the solution and because the models exhibit poor regularity properties (e.g., exploding scores, constraints). The proposed method is demonstrated on a series of challenging photon-starved imaging experiments with as little as 1 photon per pixel, where it delivers remarkably accurate solutions and outperforms alternative strategies from the state-of-the-art.},
  archive      = {J_TMLR},
  author       = {Savvas Melidonis and Yiming Xi and Konstantinos C. Zygalakis and Yoann Altmann and Marcelo Pereyra},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Score-based denoising diffusion models for photon-starved image restoration problems},
  url          = {https://openreview.net/forum?id=UYXPt7HUdl},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CNN interpretability with multivector tucker saliency maps
for self-supervised models. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=VM8bNd5A09">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpreting the decisions of Convolutional Neural Networks (CNNs) is essential for understanding their behavior, yet it remains a significant challenge, particularly for self-supervised models. Most existing methods for generating saliency maps rely on reference labels, restricting their use to supervised tasks. EigenCAM is the only notable label-independent alternative, leveraging Singular Value Decomposition to generate saliency maps applicable across CNN models, but it does not fully exploit the tensorial structure of feature maps. In this work, we introduce the Tucker Saliency Map (TSM) method, which applies Tucker tensor decomposition to better capture the inherent structure of feature maps, producing more accurate singular vectors and values. These are used to generate high-fidelity saliency maps, effectively highlighting objects of interest in the input. We further extend EigenCAM and TSM into multivector variants—Multivec-EigenCAM and Multivector Tucker Saliency Maps (MTSM)—which utilize all singular vectors and values, further improving saliency map quality. Quantitative evaluations on supervised classification models demonstrate that TSM, Multivec-EigenCAM, and MTSM achieve competitive performance with label-dependent methods. Moreover, TSM enhances interpretability by approximately $50\%$ over EigenCAM for both supervised and self-supervised models. Multivec-EigenCAM and MTSM further advance state-of-the-art interpretability performance on self-supervised models, with MTSM achieving the best results.},
  archive      = {J_TMLR},
  author       = {Aymene Mohammed Bouayed and Samuel Deslauriers-gauthier and Adrian IACOVELLI and David Naccache},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {CNN interpretability with multivector tucker saliency maps for self-supervised models},
  url          = {https://openreview.net/forum?id=VM8bNd5A09},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why is constrained neural language generation particularly
challenging? <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=Vwgjk5ysWn">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep neural language models combined with the capacity of large scale datasets have accelerated the development of natural language generation systems that produce fluent and coherent texts (to various degrees of success) in a multitude of tasks and application contexts. However, controlling the output of these models for desired user and task needs is still an open challenge. This is crucial not only to customizing the content and style of the generated language, but also to their safe and reliable deployment in the real world. We present an extensive survey on the emerging topic of constrained neural language generation in which we formally define and categorize the problems of natural language generation by distinguishing between conditions and constraints (the latter being testable conditions on the output text instead of the input), present constrained text generation tasks, and review existing methods and evaluation metrics for constrained text generation. Our aim is to highlight recent progress and trends in this emerging field, informing on the most promising directions and limitations towards advancing the state-of-the-art of constrained neural language generation research.},
  archive      = {J_TMLR},
  author       = {Cristina Garbacea and Qiaozhu Mei},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Why is constrained neural language generation particularly challenging?},
  url          = {https://openreview.net/forum?id=Vwgjk5ysWn},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instance-aware graph prompt learning. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=W50i7r3DHE">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks stand as the predominant technique for graph representation learning owing to their strong expressive power, yet the performance highly depends on the availability of high-quality labels in an end-to-end manner. Thus the pretraining and fine-tuning paradigm has been proposed to mitigate the label cost issue. Subsequently, the gap between the pretext tasks and downstream tasks has spurred the development of graph prompt learning which inserts a set of graph prompts into the original graph data with minimal parameters while preserving competitive performance. However, the current exploratory works are still limited since they all concentrate on learning fixed task-specific prompts which may not generalize well across the diverse instances that the task comprises. To tackle this challenge, we introduce Instance-Aware Graph Prompt Learning (IA-GPL) in this paper, aiming to generate distinct prompts tailored to different input instances. The process involves generating intermediate prompts for each instance using a lightweight architecture, quantizing these prompts through trainable codebook vectors, and employing the exponential moving average technique to ensure stable training. Extensive experiments conducted on multiple datasets and settings showcase the superior performance of IA-GPL compared to state-of-the-art baselines.},
  archive      = {J_TMLR},
  author       = {Jiazheng Li and Jundong Li and Chuxu Zhang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Instance-aware graph prompt learning},
  url          = {https://openreview.net/forum?id=W50i7r3DHE},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data augmentation policy search for long-term forecasting.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=Wnd0XY0twh">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation serves as a popular regularization technique to combat overfitting challenges in neural networks. While automatic augmentation has demonstrated success in image classification tasks, its application to time-series problems, particularly in long-term forecasting, has received comparatively less attention. To address this gap, we introduce a time-series automatic augmentation approach named TSAA, which is both efficient and easy to implement. The solution involves tackling the associated bilevel optimization problem through a two-step process: initially training a non-augmented model for a limited number of epochs, followed by an iterative split procedure. During this iterative process, we alternate between identifying a robust augmentation policy through Bayesian optimization and refining the model while discarding suboptimal runs. Extensive evaluations on challenging univariate and multivariate forecasting benchmark problems demonstrate that TSAA consistently outperforms several robust baselines, suggesting its potential integration into prediction pipelines. Code is available at this repository: \href{https://github.com/azencot-group/TSAA}{https://github.com/azencot-group/TSAA}.},
  archive      = {J_TMLR},
  author       = {Liran Nochumsohn and Omri Azencot},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Data augmentation policy search for long-term forecasting},
  url          = {https://openreview.net/forum?id=Wnd0XY0twh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding LLM embeddings for regression. <em>TMLR</em>.
(<a href="https://openreview.net/forum?id=Wt6Iz5XNIO">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of large language models (LLMs) for flexibly processing information as strings, a natural application is regression, specifically by preprocessing string representations into LLM embeddings as downstream features for metric prediction. In this paper, we provide one of the first comprehensive investigations into embedding-based regression and demonstrate that LLM embeddings as features can be better for high-dimensional regression tasks than using traditional feature engineering. This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space. Furthermore, we quantify the contribution of different model effects, most notably model size and language understanding, which we find surprisingly do not always improve regression performance.},
  archive      = {J_TMLR},
  author       = {Eric Tang and Bangding Yang and Xingyou Song},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Understanding LLM embeddings for regression},
  url          = {https://openreview.net/forum?id=Wt6Iz5XNIO},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-based experience replay for task-agnostic
continual reinforcement learning. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=WxHTSPS2pi">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based reinforcement learning uses a learned dynamics model to imagine actions and select those with the best expected outcomes. An experience replay buffer collects the outcomes of all actions executed in the environment, which is then used to iteratively train the dynamics model. However, as the complexity and scale of tasks increase, training times and memory requirements can grow drastically without necessarily retaining useful experiences. Continual learning proposes a more realistic scenario where tasks are learned in sequence, and the replay buffer can help mitigate catastrophic forgetting. However, it is not realistic to expect the buffer to infinitely grow as the sequence advances. Furthermore, storing every single experience executed in the environment does not necessarily provide a more accurate model. We argue that the replay buffer needs to have the minimal necessary size to retain relevant experiences that cover both common and rare states. Therefore, we propose using an uncertainty-based replay buffer filtering to enable an effective implementation of continual learning agents using model-based reinforcement learning. We show that the combination of the proposed strategies leads to reduced training times, smaller replay buffer size, and less catastrophic forgetting, all while maintaining performance.},
  archive      = {J_TMLR},
  author       = {Adrian Remonda and Cole Corbitt Terrell and Eduardo E. Veas and Marc Masana},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Uncertainty-based experience replay for task-agnostic continual reinforcement learning},
  url          = {https://openreview.net/forum?id=WxHTSPS2pi},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global convergence rate of deep equilibrium models with
general activations. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=XPREcQlAM0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-homogeneous, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion.},
  archive      = {J_TMLR},
  author       = {Lan V. Truong},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Global convergence rate of deep equilibrium models with general activations},
  url          = {https://openreview.net/forum?id=XPREcQlAM0},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test-time adaptation with source based auxiliary tasks.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=XWAXcxNg4n">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work tackles a key challenge in Test Time Adaptation~(TTA): adapting on limited data. This challenge arises naturally from two scenarios. (i) Current TTA methods are limited by the bandwidth with which the stream reveals data, since conducting several adaptation steps on each revealed batch from the stream will lead to overfitting. (ii) In many realistic scenarios, the stream reveals insufficient data for the model to fully adapt to a given distribution shift. We tackle the first scenario problem with auxiliary tasks where we leverage unlabeled data from the training distribution. In particular, we propose distilling the predictions of an originally pretrained model on clean data during adaptation. We found that our proposed auxiliary task significantly accelerates the adaptation to distribution shifts. We report a performance improvement over the state of the art by 1.5\% and 6\% on average across all corruptions on ImageNet-C under episodic and continual evaluation, respectively. To combat the second scenario of limited data, we analyze the effectiveness of combining federated adaptation with our proposed auxiliary task across different models even when different clients observe different distribution shifts. We find that not only federated averaging enhances adaptation, but combining it with our auxiliary task provides a notable 6\% performance gains over previous TTA methods.},
  archive      = {J_TMLR},
  author       = {Motasem Alfarra and Alvaro Correia and Bernard Ghanem and Christos Louizos},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Test-time adaptation with source based auxiliary tasks},
  url          = {https://openreview.net/forum?id=XWAXcxNg4n},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What is the relationship between tensor factorizations and
circuits (and how can we exploit it)? <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=Y7dRmpGiHj">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes a rigorous connection between circuit representations and tensor factorizations, two seemingly distinct yet fundamentally related areas. By connecting these fields, we highlight a series of opportunities that can benefit both communities. Our work generalizes popular tensor factorizations within the circuit language, and unifies various circuit learning algorithms under a single, generalized hierarchical factorization framework. Specifically, we introduce a modular “Lego block” approach to build tensorized circuit architectures. This, in turn, allows us to systematically construct and explore various circuit and tensor factorization models while maintaining tractability. This connection not only clarifies similarities and differences in existing models, but also enables the development of a comprehensive pipeline for building and optimizing new circuit/tensor factorization architectures. We show the effectiveness of our framework through extensive empirical evaluations, and highlight new research opportunities for tensor factorizations in probabilistic modeling.},
  archive      = {J_TMLR},
  author       = {Lorenzo Loconte and Antonio Mari and Gennaro Gala and Robert Peharz and Cassio de Campos and Erik Quaeghebeur and Gennaro Vessio and Antonio Vergari},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {What is the relationship between tensor factorizations and circuits (and how can we exploit it)?},
  url          = {https://openreview.net/forum?id=Y7dRmpGiHj},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural lattice reduction: A self-supervised geometric deep
learning approach. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=YxXyRSlZ4b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice reduction is a combinatorial optimization problem aimed at finding the most orthogonal basis in a given lattice. The Lenstra–Lenstra–Lovász (LLL) algorithm is the best algorithm in the literature for solving this problem. In light of recent research on algorithm discovery, in this work, we would like to answer this question: is it possible to parametrize the algorithm space for lattice reduction problem with neural networks and find an algorithm without supervised data? Our strategy is to use equivariant and invariant parametrizations and train in a self-supervised way. We design a deep neural model outputting factorized unimodular matrices and train it in a self-supervised manner by penalizing non-orthogonal lattice bases. We incorporate the symmetries of lattice reduction into the model by making it invariant to isometries and scaling of the ambient space and equivariant with respect to the hyperocrahedral group permuting and flipping the lattice basis elements. We show that this approach yields an algorithm with comparable complexity and performance to the LLL algorithm on a set of benchmarks. Additionally, motivated by certain applications for wireless communication, we extend our method to a convolutional architecture which performs joint reduction of spatially-correlated lattices arranged in a grid, thereby amortizing its cost over multiple lattices.},
  archive      = {J_TMLR},
  author       = {Giovanni Luca Marchetti and Gabriele Cesa and Kumar Pratik and Arash Behboodi},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Neural lattice reduction: A self-supervised geometric deep learning approach},
  url          = {https://openreview.net/forum?id=YxXyRSlZ4b},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning of probabilistic models: A
PAC-bayesian approach. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=ZMliWjMCor">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) aims to infer a shared model from private and decentralized data stored by multiple clients. Personalized FL (PFL) enhances the model’s fit for each client by adapting the global model to the clients. A significant level of personalization is required for highly heterogeneous clients but can be challenging to achieve, especially when clients’ datasets are small. We introduce PAC-PFL for PFL of probabilistic models. PAC-PFL infers a shared hyper-posterior and treats each client’s posterior inference as the personalization step. Unlike previous PFL algorithms, PAC-PFL does not regularize all personalized models towards a single shared model, thereby greatly enhancing its personalization flexibility. By establishing and minimizing a PAC-Bayesian generalization bound on the average true loss of clients, PAC-PFL effectively mitigates overfitting even in data-poor scenarios. Additionally, PAC-PFL provides generalization bounds for new clients joining later. PAC-PFL achieves accurate and well-calibrated predictions, as supported by our experiments.},
  archive      = {J_TMLR},
  author       = {Mahrokh Ghoddousi Boroujeni and Andreas Krause and Giancarlo Ferrari-Trecate},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Personalized federated learning of probabilistic models: A PAC-bayesian approach},
  url          = {https://openreview.net/forum?id=ZMliWjMCor},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TACO vision models can be efficiently specialized via
few-shot task-aware compression. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=Za9Tm07fig">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent vision architectures and self-supervised training methods have enabled training computer vision models that are extremely accurate, but come with massive computational costs. In settings such as identifying species in camera traps in the field, users have limited resources, and may fine-tune a pretrained model on (often limited) data from a small set of specific categories of interest. Such users may still wish to make use of highly-accurate large models, but are often constrained by the computational cost. To address this, we ask: can we quickly compress generalist models into accurate and efficient specialists given a small amount of data? Towards this goal, we propose a simple and versatile technique, which we call Few-Shot Task-Aware COmpression (TACO). Given a general-purpose model pretrained on a broad task, such as classification on ImageNet or iNaturalist datasets with thousands of categories, TACO produces a much smaller model that is accurate on specialized tasks, such as classifying across vehicle types or animal species, based only on a few examples from each target class. The method is based on two key insights - 1) a powerful specialization effect for data-aware compression, which we showcase for the first time; 2) a dedicated finetuning procedure with knowledge distillation, which prevents overfitting even in scenarios where data is very scarce. Specifically, TACO is applied in few-shot fashion, i.e. only a few task-specific samples are used for compression, and the procedure has low computational overhead. We validate this approach experimentally using highly-accurate ResNet, ViT/DeiT, and ConvNeXt models, originally trained on ImageNet and iNaturalist datasets, which we specialize and compress to a diverse set of ``downstream&#39;&#39; subtasks, with notable computational speedups on both CPU and GPU.},
  archive      = {J_TMLR},
  author       = {Denis Kuznedelev and Soroush Tabesh and Kimia Noorbakhsh and Elias Frantar and Sara Beery and Eldar Kurtic and Dan Alistarh},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {TACO vision models can be efficiently specialized via few-shot task-aware compression},
  url          = {https://openreview.net/forum?id=Za9Tm07fig},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability-aware training of machine learning force fields
with differentiable boltzmann estimators. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=ZckLMG00sO">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning force fields (MLFFs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations, limiting their ability to model phenomena occurring over longer timescales and compromising the quality of estimated observables. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which leverages joint supervision from reference quantum-mechanical calculations and system observables. StABlE Training iteratively runs many MD simulations in parallel to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. We achieve efficient end-to-end automatic differentiation through MD simulations using our Boltzmann Estimator, a generalization of implicit differentiation techniques to a broader class of stochastic algorithms. Unlike existing techniques based on active learning, our approach requires no additional ab-initio energy and forces calculations to correct instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, using three modern MLFF architectures. StABlE-trained models achieve significant improvements in simulation stability, data efficiency, and agreement with reference observables. Crucially, the stability improvements cannot be matched by simply reducing the simulation timestep, meaning that StABlE Training effectively allows for larger timesteps in MD simulations. By incorporating observables into the training process alongside first-principles calculations, StABlE Training can be viewed as a general semi-empirical framework applicable across MLFF architectures and systems. This makes it a powerful tool for training stable and accurate MLFFs, particularly in the absence of large reference datasets. Our code is publicly available at https://github.com/ASK-Berkeley/StABlE-Training.},
  archive      = {J_TMLR},
  author       = {Sanjeev Raja and Ishan Amin and Fabian Pedregosa and Aditi S. Krishnapriyan},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Stability-aware training of machine learning force fields with differentiable boltzmann estimators},
  url          = {https://openreview.net/forum?id=ZckLMG00sO},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shapley values of structured additive regression models and
application to RKHS weightings of functions. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=aWRMvXTvPf">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shapley values are widely used in machine learning to interpret model predictions. However, they have an important drawback in their computational time, which is exponential in the number of variables in the data. Recent work has yielded algorithms that can efficiently and exactly calculate the Shapley values of specific model families, such as Decision Trees and Generalized Additive Models (GAMs). Unfortunately, these model families are fairly restricted. Consequently, we present STAR-SHAP, an algorithm for efficiently calculating the Shapley values of Structured Additive Regression (STAR) models, a generalization of GAMs which allow any number of variable interactions. While the computational cost of STAR-SHAP scales exponentially in the size of these interactions, it is independent of the total number of variables. This allows the interpretation of more complex and flexible models. As long as the variable interactions are moderately-sized, the computation of the Shapley values will be fast, even on high-dimensional datasets. Since STAR models with more than pairwise interactions (e.g. GA2Ms) are seldom used in practice, we also present a new class of STAR models built on the RKHS Weightings of Functions paradigm. More precisely, we introduce a new RKHS Weighting instantiation, and show how to transform it and other RKHS Weightings into STAR models. We therefore introduce a new family of STAR models, as well as the means to interpret their outputs in a timely manner.},
  archive      = {J_TMLR},
  author       = {Gabriel Dubé and Mario Marchand},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Shapley values of structured additive regression models and application to RKHS weightings of functions},
  url          = {https://openreview.net/forum?id=aWRMvXTvPf},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing the information content of probabilistic
representation spaces. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=adhsMqURI1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic representation spaces convey information about a dataset and are shaped by factors such as the training data, network architecture, and loss function. Comparing the information content of such spaces is crucial for understanding the learning process, yet most existing methods assume point-based representations, neglecting the distributional nature of probabilistic spaces. To address this gap, we propose two information-theoretic measures to compare general probabilistic representation spaces by extending classic methods to compare the information content of hard clustering assignments. Additionally, we introduce a lightweight method of estimation that is based on fingerprinting a representation space with a sample of the dataset, designed for scenarios where the communicated information is limited to a few bits. We demonstrate the utility of these measures in three case studies. First, in the context of unsupervised disentanglement, we identify recurring information fragments within individual latent dimensions of VAE and InfoGAN ensembles. Second, we compare the full latent spaces of models and reveal consistent information content across datasets and methods, despite variability during training. Finally, we leverage the differentiability of our measures to perform model fusion, synthesizing the information content of weak learners into a single, coherent representation. Across these applications, the direct comparison of information content offers a natural basis for characterizing the processing of information.},
  archive      = {J_TMLR},
  author       = {Kieran A. Murphy and Sam Dillavou and Danielle Bassett},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Comparing the information content of probabilistic representation spaces},
  url          = {https://openreview.net/forum?id=adhsMqURI1},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving consistency in large language models through chain
of guidance. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=asiBW1bB9b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistency is a fundamental dimension of trustworthiness in Large Language Models (LLMs). For humans to be able to trust LLM-based applications, their outputs should be consistent when prompted with inputs that carry the same meaning or intent. Despite this need, there is no known mechanism to control and guide LLMs to be more consistent at inference time. In this paper, we introduce a novel alignment strategy to maximize semantic consistency in LLM outputs. Our proposal is based on \textbf{Chain of Guidance} (CoG), a multistep prompting technique that generates highly consistent outputs from LLMs. For closed-book question-answering (Q\&amp;A) tasks, when compared to direct prompting, the outputs generated using CoG show improved consistency. While other approaches like template-based responses and majority voting may offer alternative paths to consistency, our work focuses on exploring the potential of guided prompting. We use synthetic data sets comprised of consistent input-output pairs to fine-tune LLMs to produce consistent {\it and} correct outputs. Our fine-tuned models are more than twice as consistent compared to base models and show strong generalization capabilities by producing consistent outputs over datasets not used in the fine-tuning process. Code is available at \url{https://github.com/vijilAI/chain_of_guidance}.},
  archive      = {J_TMLR},
  author       = {Harsh Raj and Vipul Gupta and Domenic Rosati and Subhabrata Majumdar},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Improving consistency in large language models through chain of guidance},
  url          = {https://openreview.net/forum?id=asiBW1bB9b},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QPO: Query-dependent prompt optimization via multi-loop
offline reinforcement learning. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=bqMJToTkvT">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt engineering has demonstrated remarkable success in enhancing the performance of large language models (LLMs) across diverse tasks. However, most existing prompt optimization methods only focus on the task-level performance, overlooking the importance of query-preferred prompts, which leads to suboptimal performances. Additionally, these methods rely heavily on frequent interactions with LLMs to obtain feedback for guiding the optimization process, incurring substantial redundant interaction costs. In this paper, we introduce Query-dependent Prompt Optimization ($\textbf{QPO}$), which leverages multi-loop offline reinforcement learning to iteratively fine-tune a small pretrained language model to generate optimal prompts tailored to the input queries, thus significantly improving the prompting effect on the large target LLM. We derive insights from offline prompting demonstration data, which already exists in large quantities as a by-product of benchmarking diverse prompts on open-sourced tasks, thereby circumventing the expenses of online interactions. Furthermore, we continuously augment the offline dataset with the generated prompts in each loop, as the prompts from the fine-tuned model are supposed to outperform the source prompts in the original dataset. These iterative loops bootstrap the model towards generating optimal prompts. Experiments on various LLM scales and diverse NLP and math tasks demonstrate the efficacy and cost-efficiency of our method in both zero-shot and few-shot scenarios.},
  archive      = {J_TMLR},
  author       = {Yilun Kong and Hangyu Mao and Zhao Qi and Bin Zhang and Jingqing Ruan and Li Shen and Yongzhe Chang and Xueqian Wang and Rui Zhao and Dacheng Tao},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {QPO: Query-dependent prompt optimization via multi-loop offline reinforcement learning},
  url          = {https://openreview.net/forum?id=bqMJToTkvT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label distribution shift-aware prediction refinement for
test-time adaptation. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=c7AAHdEYz5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation (TTA) is an effective approach to mitigate performance degradation of trained models when encountering input distribution shifts at test time. However, existing TTA methods often suffer significant performance drops when facing additional class distribution shifts. We first analyze TTA methods under label distribution shifts and identify the presence of class-wise confusion patterns commonly observed across different covariate shifts. Based on this observation, we introduce label Distribution shift-Aware prediction Refinement for Test-time adaptation (DART), a novel TTA method that refines the predictions by focusing on class-wise confusion patterns. DART trains a prediction refinement module during an intermediate time by exposing it to several batches with diverse class distributions using the training dataset. This module is then used during test time to detect and correct class distribution shifts, significantly improving pseudo-label accuracy for test data. Our method exhibits 5-18% gains in accuracy under label distribution shifts on CIFAR-10C, without any performance degradation when there is no label distribution shift. Extensive experiments on CIFAR, PACS, OfficeHome, and ImageNet benchmarks demonstrate DART&#39;s ability to correct inaccurate predictions caused by test-time distribution shifts. This improvement leads to enhanced performance in existing TTA methods, making DART a valuable plug-in tool.},
  archive      = {J_TMLR},
  author       = {Minguk Jang and Hye Won Chung},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Label distribution shift-aware prediction refinement for test-time adaptation},
  url          = {https://openreview.net/forum?id=c7AAHdEYz5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The sparse matrix-based random projection: A study of binary
and ternary quantization. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=dNJmJ8bh1M">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random projection is a simple yet effective technique for dimension reduction, widely used in various machine learning tasks. Following the projection step, quantization is often applied to further reduce the complexity of projected data. In general, quantized projections are expected to approximately preserve the pairwise distances between the original data points, to avoid significant performance degradation in subsequent tasks. While this distance preservation property has been investigated for Gaussian matrices, our work further extends the analysis to hardware-friendly $\{0,1\}$-binary matrices, particularly focusing on cases where the projections are quantized into two types of low bit-width codes: $\{0,1\}$-binary codes and $\{0,\pm1\}$-ternary codes. It is found that the distance preservation property tends to be better maintained, when the binary projection matrices exhibit sparse structures. This is validated through classification and clustering experiments, where extremely sparse binary matrices, with only one nonzero entry per column, achieve superior or comparable performance to other denser binary matrices and Gaussian matrices. This presents an opportunity to significantly reduce the computational and storage complexity of the quantized random projection model, without compromising, and potentially even improving its performance.},
  archive      = {J_TMLR},
  author       = {Weizhi Lu and Zhongzheng Li and Mingrui Chen and Weiyu Li},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {The sparse matrix-based random projection: A study of binary and ternary quantization},
  url          = {https://openreview.net/forum?id=dNJmJ8bh1M},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced mixed-type tabular data synthesis with diffusion
models. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=dvRysCqmYQ">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have emerged as a robust framework for various generative tasks, including tabular data synthesis. However, current tabular diffusion models tend to inherit bias in the training dataset and generate biased synthetic data, which may influence discriminatory actions. In this research, we introduce a novel tabular diffusion model that incorporates sensitive guidance to generate fair synthetic data with balanced joint distributions of the target label and sensitive attributes, such as sex and race. The empirical results demonstrate that our method effectively mitigates bias in training data while maintaining the quality of the generated samples. Furthermore, we provide evidence that our approach outperforms existing methods for synthesizing tabular data on fairness metrics such as demographic parity ratio and equalized odds ratio, achieving improvements of over $10\%$. Our implementation is available at https://github.com/comp-well-org/fair-tab-diffusion.},
  archive      = {J_TMLR},
  author       = {Zeyu Yang and Han Yu and Peikun Guo and Khadija Zanna and Xiaoxue Yang and Akane Sano},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Balanced mixed-type tabular data synthesis with diffusion models},
  url          = {https://openreview.net/forum?id=dvRysCqmYQ},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative minibatching in graph neural networks.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=f6yMdmrD2g">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training large scale Graph Neural Networks (GNNs) requires significant computational resources, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE, i.e., cores and/or GPUs) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work as batch sizes increase. Hence, it is favorable for processors equipped with a fast interconnect to work on a large minibatch together as a single larger processor, instead of working on separate smaller minibatches, even though global batch size is identical. We also show how to take advantage of the same phenomenon in serial execution by generating dependent consecutive minibatches. Our experimental evaluations show up to 4x bandwidth savings for fetching vertex embeddings, by simply increasing this dependency without harming model convergence. Combining our proposed approaches, we achieve up to 64\% speedup over Independent Minibatching on single-node multi GPU systems, using same resources.},
  archive      = {J_TMLR},
  author       = {Muhammed Fatih Balin and Dominique LaSalle and Umit Catalyurek},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Cooperative minibatching in graph neural networks},
  url          = {https://openreview.net/forum?id=f6yMdmrD2g},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counterfactual learning of stochastic policies with
continuous actions. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=fC4bh1PmZr">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual reasoning from logged data has become increasingly important for many applications such as web advertising or healthcare. In this paper, we address the problem of learning stochastic policies with continuous actions from the viewpoint of counterfactual risk minimization (CRM). While the CRM framework is appealing and well studied for discrete actions, the continuous action case raises new challenges about modelization, optimization, and~offline model selection with real data which turns out to be particularly challenging. Our paper contributes to these three aspects of the CRM estimation pipeline. First, we introduce a modelling strategy based on a joint kernel embedding of contexts and actions, which overcomes the shortcomings of previous discretization approaches. Second, we empirically show that the optimization aspect of counterfactual learning is important, and we demonstrate the benefits of proximal point algorithms and smooth estimators. Finally, we propose an evaluation protocol for offline policies in real-world logged systems, which is challenging since policies cannot be replayed on test data, and we release a new large-scale dataset along with multiple synthetic, yet realistic, evaluation setups.},
  archive      = {J_TMLR},
  author       = {Houssam Zenati and Alberto Bietti and Matthieu Martin and Eustache Diemert and Pierre Gaillard and Julien Mairal},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Counterfactual learning of stochastic policies with continuous actions},
  url          = {https://openreview.net/forum?id=fC4bh1PmZr},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying axiomatic mathematical transformation steps
using tree-structured pointer networks. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=gLQ801ewwp">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of mathematical relations has become a new area of research in deep learning. A major focus lies on determining mathematical equivalence. While previous work has simply approached the task as a binary classification without providing further insight into the underlying decision, we aim to iteratively find a sequence of necessary steps to transform a mathematical expression into an arbitrary equivalent form. Each step in this sequence is specified by an axiom together with its position of application. We denote this task as Stepwise Equation Transformation Identification (SETI) task. To solve the task efficiently, we further propose TreePointerNet, a novel architecture which exploits the inherent tree structure of mathematical equations and consists of three key building blocks: (i) a transformer model tailored to work on hierarchically tree-structured equations, making use of (ii) a copy-pointer mechanism to extract the exact location of a transformation in the tree and finally (iii) custom embeddings that map distinguishable occurrences of the same token type to a common embedding. In addition, we introduce new datasets of equations for the SETI task. We benchmark our model against various baselines and perform an ablation study to quantify the influence of our custom embeddings and the copy-pointer component. Furthermore, we test the robustness of our model on data of unseen complexity. Our results clearly show that incorporating the hierarchical structure, embeddings and copy-pointer into a single model is highly beneficial for solving the SETI task},
  archive      = {J_TMLR},
  author       = {Sebastian Wankerl and Jan Pfister and Andrzej Dulny and Gerhard Götz and Andreas Hotho},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Identifying axiomatic mathematical transformation steps using tree-structured pointer networks},
  url          = {https://openreview.net/forum?id=gLQ801ewwp},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When precision meets position: BFloat16 breaks down RoPE in
long-context training. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=gwXfZ3xkUq">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16&#39;s limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. Despite its limitations, BFloat16 remains desirable for its computational efficiency, particularly given the substantial memory overhead required to extend the context window. To improve long-context training under BFloat16, we develop AnchorAttention, a plug-and-play attention method that enhances long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\% compared to standard full attention mechanisms, while preserving the original LLM&#39;s capabilities on general tasks.},
  archive      = {J_TMLR},
  author       = {Haonan Wang and Qian Liu and Chao Du and Tongyao Zhu and Cunxiao Du and Kenji Kawaguchi and Tianyu Pang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {When precision meets position: BFloat16 breaks down RoPE in long-context training},
  url          = {https://openreview.net/forum?id=gwXfZ3xkUq},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALTA: Compiler-based analysis of transformers.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=h751wl9xiR">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework --- language specification, symbolic interpreter, and weight compiler --- available to the community to enable further applications and insights.},
  archive      = {J_TMLR},
  author       = {Peter Shaw and James Cohan and Jacob Eisenstein and Kenton Lee and Jonathan Berant and Kristina Toutanova},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {ALTA: Compiler-based analysis of transformers},
  url          = {https://openreview.net/forum?id=h751wl9xiR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent space energy-based neural ODEs. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=hCxtlfvL22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces novel deep dynamical models designed to represent continuous-time sequences. Our approach employs a neural emission model to generate each data point in the time series through a non-linear transformation of a latent state vector. The evolution of these latent states is implicitly defined by a neural ordinary differential equation (ODE), with the initial state drawn from an informative prior distribution parameterized by an Energy-based model (EBM). This framework is extended to disentangle dynamic states from underlying static factors of variation, represented as time-invariant variables in the latent space. We train the model using maximum likelihood estimation with Markov chain Monte Carlo (MCMC) in an end-to-end manner. Experimental results on oscillating systems, videos and real-world state sequences (MuJoCo) demonstrate that our model with the learnable energy-based prior outperforms existing counterparts, and can generalize to new dynamic parameterization, enabling long-horizon predictions.},
  archive      = {J_TMLR},
  author       = {Sheng Cheng and Deqian Kong and Jianwen Xie and Kookjin Lee and Ying Nian Wu and Yezhou Yang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Latent space energy-based neural ODEs},
  url          = {https://openreview.net/forum?id=hCxtlfvL22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The RealHumanEval: Evaluating large language models’
abilities to support programmers. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=hGaWq5Buj7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation of large language models for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), or more recently using human preferences of LLM responses. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks or more preferred LLM responses translate to programmer productivity when coding with LLMs, including time spent coding. We introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=243) using RealHumanEval in which users interacted with seven LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional---a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better proxy signals. We open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.},
  archive      = {J_TMLR},
  author       = {Hussein Mozannar and Valerie Chen and Mohammed Alsobay and Subhro Das and Sebastian Zhao and Dennis Wei and Manish Nagireddy and Prasanna Sattigeri and Ameet Talwalkar and David Sontag},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {The RealHumanEval: Evaluating large language models’ abilities to support programmers},
  url          = {https://openreview.net/forum?id=hGaWq5Buj7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapt then unlearn: Exploring parameter space semantics for
unlearning in generative adversarial networks. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=jAHEBivObO">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the growing concerns about privacy and regulatory compliance, it is desirable to regulate the output of generative models. To that end, the objective of this work is to prevent the generation of outputs containing undesired features from a pre-trained Generative Adversarial Network (GAN) where the underlying training data set is inaccessible. Our approach is inspired by the observation that the parameter space of GANs exhibits meaningful directions that can be leveraged to suppress specific undesired features. However, such directions usually result in the degradation of the quality of generated samples. Our proposed two-stage method, known as &#39;Adapt-then-Unlearn,&#39; excels at unlearning such undesirable features while also maintaining the quality of generated samples. In the initial stage, we adapt a pre-trained GAN on a set of negative samples (containing undesired features) provided by the user. Subsequently, we train the original pre-trained GAN using positive samples, along with a repulsion regularizer. This regularizer encourages the learned model parameters to move away from the parameters of the adapted model (first stage) while not degrading the generation quality. We provide theoretical insights into the proposed method. To the best of our knowledge, our approach stands as the first method addressing unlearning within the realm of high-fidelity GANs (such as StyleGAN). We validate the effectiveness of our method through comprehensive experiments, encompassing both class-level unlearning on the MNIST and AFHQ dataset and feature-level unlearning tasks on the CelebA-HQ dataset. Our code and implementation is available at: https://github.com/atriguha/Adapt_Unlearn.},
  archive      = {J_TMLR},
  author       = {Piyush Tiwary and Atri Guha and Subhodip Panda and Prathosh AP},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Adapt then unlearn: Exploring parameter space semantics for unlearning in generative adversarial networks},
  url          = {https://openreview.net/forum?id=jAHEBivObO},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video-language critic: Transferable reward functions for
language-conditioned robotics. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=jJOVpnNrEp">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language is often the easiest and most convenient modality for humans to specify tasks for robots. However, learning to ground language to behavior typically requires impractical amounts of diverse, language-annotated demonstrations collected on each target robot. In this work, we aim to separate the problem of what to accomplish from how to accomplish it, as the former can benefit from substantial amounts of external observation-only data, and only the latter depends on a specific robot embodiment. To this end, we propose Video-Language Critic, a reward model that can be trained on readily available cross-embodiment data using contrastive learning and a temporal ranking objective, and use it to score behavior traces from a separate actor. When trained on Open X-Embodiment data, our reward model enables 2x more sample-efficient policy training on Meta-World tasks than a sparse reward only, despite a significant domain gap. Using in-domain data but in a challenging task generalization setting on Meta-World, we further demonstrate more sample-efficient training than is possible with prior language-conditioned reward models that are either trained with binary classification, use static images, or do not leverage the temporal information present in video data.},
  archive      = {J_TMLR},
  author       = {Minttu Alakuijala and Reginald McLean and Isaac Woungang and Nariman Farsad and Samuel Kaski and Pekka Marttinen and Kai Yuan},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Video-language critic: Transferable reward functions for language-conditioned robotics},
  url          = {https://openreview.net/forum?id=jJOVpnNrEp},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combating inter-task confusion and catastrophic forgetting
by metric learning and re-using a past trained model. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=jRbKsQ3sYO">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the vast research on class-incremental learning (IL), the critical issues have not yet been fully addressed. In this paper, utilizing metric learning, we tackle two fundamental issues of class-incremental learning (class-IL), inter-task confusion and catastrophic forgetting, which have not been fully addressed yet in the literature. To mitigate the inter-task confusion, we propose an innovative loss by utilizing the centroids of previously learned classes as negatives and current data samples as positives in the embedding space, which reduces overlaps between the classes of the current and past tasks in the embedding space. To combat catastrophic forgetting, we also propose that the past trained model is stored and re-used for generating past data samples for only one previous task. Based on this, we further propose a novel knowledge distillation approach utilizing inter-class embedding clusters, intra-class embedding clusters, and mean square embedding distances. Extensive experiments performed on MNIST, CIFAR-10, CIFAR-100, Mini-ImageNet, and TinyImageNet show that our proposed exemplar-free metric class-IL method achieves the state-of-the-art performance, beating all baseline methods by notable margins. We release our codes as the supplementary materials.},
  archive      = {J_TMLR},
  author       = {Sayedmoslem Shokrolahi and IL MIN KIM},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Combating inter-task confusion and catastrophic forgetting by metric learning and re-using a past trained model},
  url          = {https://openreview.net/forum?id=jRbKsQ3sYO},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards measuring predictability: To which extent
data-driven approaches can extract deterministic relations from data
exemplified with time series prediction and classification.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=jZBAVFGUUo">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing loss functions is one important ingredient for machine learning to fit parameters such that the machine learning models extract relations hidden in the data. The smaller the loss function value on various splittings of a dataset, the better the machine learning model is assumed to perform. However, datasets are usually generated by dynamics consisting of deterministic components, where relations are clearly defined and consequently learnable, as well as stochastic parts where outcomes are random and thus not predictable. Depending on the amplitude of the deterministic and stochastic processes, the best achievable loss function value varies and is usually not known in real data science scenarios. In this research, a statistical framework is developed that provides measures to address the predictability of a target given the available input data and, after training a machine learning model, how much of the deterministic relations have been missed by the model. Consequently, the presented framework allows to differentiate model errors into unpredictable parts regarding the given input and a systematic miss of deterministic relations. The work extends the definition of model success or failure as well as the convergence of a training process. Moreover, it is demonstrated how such measures can enrich the procedure of model training. The framework is showcased with time series data on different synthetic and real-world datasets. The code is available at https://github.com/Saleh-Gholam-Zadeh/predictability_measure.},
  archive      = {J_TMLR},
  author       = {Saleh GHOLAM ZADEH and Vaisakh Shaj and Patrick Jahnke and Gerhard Neumann and Tim Breitenbach},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Towards measuring predictability: To which extent data-driven approaches can extract deterministic relations from data exemplified with time series prediction and classification},
  url          = {https://openreview.net/forum?id=jZBAVFGUUo},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized negative reservoir for incremental learning in
recommender systems. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=jrUUk5Fskm">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the incremental learning framework. In this work, we propose a personalized negative reservoir strategy, which is used to obtain negative samples for the standard triplet loss of graph-based recommendation systems. Our technique balances alleviation of forgetting with plasticity by encouraging the model to remember stable user preferences and selectively forget when user interests change. We derive the mathematical formulation of a negative sampler to populate and update the reservoir. We integrate our design in three SOTA and commonly used incremental recommendation models. We show that these concrete realizations of our negative reservoir framework achieve state-of-the-art results for standard benchmarks using multiple top-k evaluation metrics.},
  archive      = {J_TMLR},
  author       = {Antonios Valkanas and Yuening Wang and Yingxue Zhang and Mark Coates},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Personalized negative reservoir for incremental learning in recommender systems},
  url          = {https://openreview.net/forum?id=jrUUk5Fskm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Verbalized machine learning: Revisiting machine learning
with language models. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=k3Ab6RuJE9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the progress of large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning (ML) models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, we revisit classical ML problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why an update is performed. We empirically verify the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability.},
  archive      = {J_TMLR},
  author       = {Tim Z. Xiao and Robert Bamler and Bernhard Schölkopf and Weiyang Liu},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Verbalized machine learning: Revisiting machine learning with language models},
  url          = {https://openreview.net/forum?id=k3Ab6RuJE9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FaAlGrad: Fairness through alignment of gradients across
different subpopulations. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=k4AxEwTaHq">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing deployment of Machine Learning systems has increased interest in systems optimized for other important criteria along with the expected task performance. For instance, machine learning models often exhibit biases that lead to unfair outcomes for certain protected subpopulations. This work aims to handle the bias in machine learning models and enhance their fairness by aligning the loss gradients. Specifically, leveraging the meta-learning technique, we propose a novel training framework that aligns the gradients computed across different subpopulations for learning fair classifiers. Aligning the gradients enables our framework to regularize the training process, thereby prioritizing fairness over predictive accuracy. Our experiments on multiple benchmark datasets demonstrate significant improvements in fairness metrics without having any exclusive regularizers for fairness. Thus our work contributes to developing fairer machine learning models with broader societal benefits.},
  archive      = {J_TMLR},
  author       = {Nikita Malik and Konda Reddy Mopuri},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {FaAlGrad: Fairness through alignment of gradients across different subpopulations},
  url          = {https://openreview.net/forum?id=k4AxEwTaHq},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maxwell’s demon at work: Efficient pruning by leveraging
saturation of neurons. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=nmBleuFzaN">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training neural networks, dying neurons —units becoming inactive or saturated— are traditionally seen as harmful. This paper sheds new light on this phenomenon. By exploring the impact of various hyperparameter configurations on dying neurons during training, we gather insights on how to improve upon sparse training approaches to pruning. We introduce Demon Pruning (DemP), a method that controls the proliferation of dead neurons through a combination of noise injection on active units and a one-cycled schedule regularization strategy, dynamically leading to network sparsity. Experiments on CIFAR-10 and ImageNet datasets demonstrate that DemP outperforms existing dense-to-sparse structured pruning methods, achieving better accuracy-sparsity tradeoffs while speeding up training up to 3.56$\times$. These findings provide a novel perspective on dying neurons as a resource for efficient model compression and optimization.},
  archive      = {J_TMLR},
  author       = {Simon Dufort-Labbé and Pierluca D&#39;Oro and Evgenii Nikishin and Irina Rish and Pierre-Luc Bacon and Razvan Pascanu and Aristide Baratin},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Maxwell&#39;s demon at work: Efficient pruning by leveraging saturation of neurons},
  url          = {https://openreview.net/forum?id=nmBleuFzaN},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private source-target clustering.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=ojeCoOKwWp">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a new private variant of the Source-Target Clustering (STC) setting, which was introduced by de Mathelin et al. (2022). In STC, there is a target dataset that needs to be clustered by selecting centers, in addition to centers that are already provided in a separate source dataset. The goal is to select centers from the target, such that the target clustering cost given the additional source centers is minimized. We consider private STC, in which the source dataset is private and should only be used under the constraint of differential privacy. This is motivated by scenarios in which the existing centers are private, for instance because they represent individuals in a social network. We derive lower bounds for the private STC objective, illustrating the theoretical limitations on worst-case guarantees for this setting. We then present a differentially private algorithm with asymptotically advantageous results under a data-dependent analysis, in which the guarantee depends on properties of the dataset, as well as more practical variants. We demonstrate in experiments the reduction in clustering cost that is obtained by our practical algorithms compared to baseline approaches.},
  archive      = {J_TMLR},
  author       = {Shachar Schnapp and Sivan Sabato},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Differentially private source-target clustering},
  url          = {https://openreview.net/forum?id=ojeCoOKwWp},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture degree-corrected stochastic block model for
multi-group community detection in multiplex graphs. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=p9KSFrTLx0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplex graphs have emerged as a powerful tool for modeling complex data structures due to their ability to handle multiple relational layers. Clustering within a multiplex graph can involve merging vertices into communities that are consistent across all layers, grouping similar layers into clusters, or creating overlapping clusters among vertices and layers. However, a multiplex graph may exhibit distinct vertex communities based on the specific layers to which a vertex is connected. This scenario, termed multi-group community detection, significantly enhances the accuracy of clustering processes and aids in the interpretation of partitions. To date, the current literature on state-of-the-art community detection has not extensively addressed this modeling approach. In this paper, we introduce a novel methodology referred to as the &quot;Mixture Degree-Corrected Stochastic Block Model.&quot; This generative model, an extension of the widely utilized Degree-Corrected Stochastic Block Model (DCSBM), is designed to cluster similar layers by their community structures while simultaneously identifying communities within each layer&#39;s group. We provide a rigorous definition of the model and utilize an iterative technique to perform inference computations. Furthermore, we assess the identifiability of our proposed model and demonstrate the consistency of the maximum likelihood function through analytical analysis. The effectiveness of our method is evaluated using both real-word data sets and synthetic graphs.},
  archive      = {J_TMLR},
  author       = {Noureddine Henka and Mohamad Assaad and Sami Tazi},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Mixture degree-corrected stochastic block model for multi-group community detection in multiplex graphs},
  url          = {https://openreview.net/forum?id=p9KSFrTLx0},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On training-conditional conformal prediction and binomial
proportion confidence intervals. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=pSk5qyt1ob">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the expectation of a Bernoulli random variable based on $N$ independent trials is a classical problem in statistics, typically addressed using Binomial Proportion Confidence Intervals (BPCI). In the control systems community, many critical tasks—such as certifying the statistical safety of dynamical systems—can be formulated as BPCI problems. Conformal Prediction (CP), a distribution-free technique for uncertainty quantification, has gained significant attention in recent years and has been applied to various control systems problems, particularly to address uncertainties in learned dynamics or controllers. A variant known as training-conditional CP was recently employed to tackle the problem of safety certification. In this note, we highlight that the use of training-conditional CP in this context does not provide valid safety guarantees. We demonstrate why CP is unsuitable for BPCI problems and argue that traditional BPCI methods are better suited for statistical safety certification.},
  archive      = {J_TMLR},
  author       = {Rudi Coppola and Manuel Mazo Espinosa},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On training-conditional conformal prediction and binomial proportion confidence intervals},
  url          = {https://openreview.net/forum?id=pSk5qyt1ob},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AttnGCG: Enhancing jailbreaking attacks on LLMs with
attention manipulation. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=prVLANCshF">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, focusing specifically on the optimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe a positive correlation between the effectiveness of attacks and the internal behaviors of the models. For instance, attacks tend to be less effective when models pay more attention to system prompts designed to ensure LLM safety alignment. Building on this discovery, we introduce an enhanced method that manipulates models&#39; attention scores to facilitate LLM jailbreaking, which we term AttnGCG. Empirically, AttnGCG shows consistent improvements in attack efficacy across diverse LLMs, achieving an average increase of ~7% in the Llama-2 series and ~10% in the Gemma series. Our strategy also demonstrates robust attack transferability against both unseen harmful goals and black-box LLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score visualization is more interpretable, allowing us to gain better insights into how our targeted attention manipulation facilitates more effective jailbreaking. We release the code at https://github.com/UCSC-VLAA/AttnGCG-attack.},
  archive      = {J_TMLR},
  author       = {Zijun Wang and Haoqin Tu and Jieru Mei and Bingchen Zhao and Yisen Wang and Cihang Xie},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {AttnGCG: Enhancing jailbreaking attacks on LLMs with attention manipulation},
  url          = {https://openreview.net/forum?id=prVLANCshF},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How does code pretraining affect language model task
performance? <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=pxxmUKKgel">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models are increasingly trained on corpora containing both natural language and non-linguistic data like source code. Aside from aiding programming-related tasks, anecdotal evidence suggests that including code in pretraining corpora may improve performance on other, unrelated tasks, yet to date no work has been able to establish a causal connection by controlling between language and code data. Here we do just this. We pretrain language models on datasets which interleave natural language and code in two different settings: competitive, in which the total volume of data seen during pretraining is held constant; and additive, in which the volume of language data is held constant. We study how the pretraining mixture affects performance on (a) compositionality, measured by generalization accuracy on semantic parsing and syntactic transformation tasks, and more broadly on (b) downstream non-code-related objectives, measured by performance on tasks from the BigBench benchmark. We find that pretraining on higher proportions of code improves performance on compositional tasks involving structured output (like semantic parsing), and mathematics. Conversely, increase code mixture can harm performance on other tasks, including on tasks that requires sensitivity to linguistic structure such as syntax or morphology, and tasks measuring real-world knowledge.},
  archive      = {J_TMLR},
  author       = {Jackson Petty and Sjoerd van Steenkiste and Tal Linzen},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {How does code pretraining affect language model task performance?},
  url          = {https://openreview.net/forum?id=pxxmUKKgel},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Are large language models really robust to word-level
perturbations? <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=rWSiBknwQa">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The swift advancement in the scales and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLMs, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, potentially ignoring the superior generation capabilities of contemporary LLMs. To investigate the robustness of LLMs while using their generation ability, we propose a novel rational evaluation pipeline that leverages reward models as diagnostic tools to evaluate the long conversation generated from more challenging open questions by LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters.Our extensive empirical experiments demonstrate that TREvaL provides an identification for the lack of robustness of nowadays LLMs.Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted, calling for more attention on the robustness during alignment process.},
  archive      = {J_TMLR},
  author       = {Haoyu Wang and Guozheng Ma and Cong Yu and Ning Gui and Linrui Zhang and Zhiqi Huang and Suwei Ma and Yongzhe Chang and Sen Zhang and Li Shen and Xueqian Wang and Peilin Zhao and Dacheng Tao},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Are large language models really robust to word-level perturbations?},
  url          = {https://openreview.net/forum?id=rWSiBknwQa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty representations in state-space layers for deep
reinforcement learning under partial observability. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=rfPns0WJyg">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal decision-making under partial observability requires reasoning about the uncertainty of the environment’s hidden state. However, most reinforcement learning architectures handle partial observability with sequence models that have no internal mechanism to incorporate uncertainty in their hidden state representation, such as recurrent neural networks, deterministic state-space models and transformers. Inspired by advances in probabilistic world models for reinforcement learning, we propose a standalone Kalman filter layer that performs closed-form Gaussian inference in linear state-space models and train it end-to-end within a model-free architecture to maximize returns. Similar to efficient linear recurrent layers, the Kalman filter layer processes sequential data using a parallel scan, which scales logarithmically with the sequence length. By design, Kalman filter layers are a drop-in replacement for other recurrent layers in standard model-free architectures, but importantly they include an explicit mechanism for probabilistic filtering of the latent state representation. Experiments in a wide variety of tasks with partial observability show that Kalman filter layers excel in problems where uncertainty reasoning is key for decision-making, outperforming other stateful models.},
  archive      = {J_TMLR},
  author       = {Carlos E. Luis and Alessandro Giacomo Bottero and Julia Vinogradska and Felix Berkenkamp and Jan Peters},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Uncertainty representations in state-space layers for deep reinforcement learning under partial observability},
  url          = {https://openreview.net/forum?id=rfPns0WJyg},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class incremental learning from first principles: A review.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=sZdtTJInUg">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning systems attempt to efficiently learn over time without forgetting previously acquired knowledge. In recent years, there has been an explosion of work on continual learning, mainly focused on the class-incremental learning (CIL) setting. In this review, we take a step back and reconsider the CIL problem. We reexamine the problem definition and describe its unique challenges, contextualize existing solutions by analyzing non-continual approaches, and investigate the implications of various problem configurations. Our goal is to provide an alternative perspective to existing work on CIL and direct attention toward unexplored aspects of the problem.},
  archive      = {J_TMLR},
  author       = {Neil Ashtekar and Jingxi Zhu and Vasant G Honavar},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Class incremental learning from first principles: A review},
  url          = {https://openreview.net/forum?id=sZdtTJInUg},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Increasing both batch size and learning rate accelerates
stochastic gradient descent. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=sbmp55k6iE">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of mini-batch stochastic gradient descent (SGD) strongly depends on setting the batch size and learning rate to minimize the empirical loss in training the deep neural network. In this paper, we present theoretical analyses of mini-batch SGD with four schedulers: (i) constant batch size and decaying learning rate scheduler, (ii) increasing batch size and decaying learning rate scheduler, (iii) increasing batch size and increasing learning rate scheduler, and (iv) increasing batch size and warm-up decaying learning rate scheduler. We show that mini-batch SGD using scheduler (i) does not always minimize the expectation of the full gradient norm of the empirical loss, whereas it does using any of schedulers (ii), (iii), and (iv). Furthermore, schedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides numerical results of supporting analyses showing that using scheduler (iii) or (iv) minimizes the full gradient norm of the empirical loss faster than using scheduler (i) or (ii).},
  archive      = {J_TMLR},
  author       = {Hikaru Umeda and Hideaki Iiduka},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Increasing both batch size and learning rate accelerates stochastic gradient descent},
  url          = {https://openreview.net/forum?id=sbmp55k6iE},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the robustness of analogical reasoning in large
language models. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=t5cy5v9wph">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, there is debate on the extent to which they are performing general abstract reasoning versus employing shortcuts or other non-robust processes, such as ones that overly rely on similarity to what has been seen in their training data. Here we investigate the robustness of analogy-making abilities previously claimed for LLMs on three of four domains studied by Webb et al. (2023): letter-string analogies, digit matrices, and story analogies. For each of these domains we test humans and GPT models on robustness to variants of the original analogy problems—versions that test the same abstract reasoning abilities but that are likely dissimilar from tasks in the pre-training data. The performance of a system that uses robust abstract reasoning should not decline substantially on these variants. On simple letter-string analogies, we find that while the performance of humans remains high for two types of variants we tested, the GPT models’ performance declines sharply. This pattern is less pronounced as the complexity of these analogy problems is increased, as both humans and GPT models perform poorly on both the original and variant problems requiring more complex analogies. On digit-matrix problems, we find a similar pattern but only on one out of the two types of variants we tested. Lastly, we assess the robustness of humans and GPT models on story-based analogy problems, finding that, unlike humans, the performance of GPT models are susceptible to answer-order effects, and that GPT models also may be more sensitive than humans to paraphrasing. This work provides evidence that, despite previously reported successes of LLMs on zero-shot analogical reasoning, these models often lack the robustness of zero-shot human analogy- making, exhibiting brittleness on most of the variations we tested. More generally, this work points to the importance of carefully evaluating AI systems not only for accuracy but also robustness when testing their cognitive capabilities. Code, data, and results for all experiments is available at https://github.com/marthaflinderslewis/robust-analogy.},
  archive      = {J_TMLR},
  author       = {Martha Lewis and Melanie Mitchell},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Evaluating the robustness of analogical reasoning in large language models},
  url          = {https://openreview.net/forum?id=t5cy5v9wph},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing remaining useful life prediction with ensemble
multi-term fourier graph neural networks. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=tzFjcVqmxw">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remaining useful life (RUL) prediction is crucial in predictive maintenance. Recently, deep learning forecasting methods, especially Spatio-Temporal Graph Neural Networks (ST-GNNs), have achieved remarkable performance in RUL prediction. Most existing ST-GNNs require searching for the graph structure before utilizing GNNs to learn spatial graph representation, and they necessitate a temporal model such as LSTM to leverage the temporal dependencies in a fixed lookback window. However, such an approach has several limitations. Firstly, it demands substantial computational resources to learn graph structures for the time series data. Secondly, independently learning spatial and temporal information disregards their inherent correlation, and thirdly, capturing information within a fixed lookback window ignores long-term dependencies across the entire time series. To mitigate the issues above, instead of treating the data within the lookback window as a sequence of graphs in ST-GNN methods, we regard it as a complete graph and employ a Fourier Graph Neural Network (FGN) to learn the spatiotemporal information within this graph in the frequency space. Additionally, we create training and test graphs with varying sizes of lookback windows, enabling the model to learn both short-term and long-term dependencies and provide multiple predictions for ensemble averaging. We also consider scenarios where sensor signals exhibit multiple operation conditions and design a sequence decomposition plugin to denoise input signals, aiming to enhance the performance of FGN. We evaluate the proposed model on two benchmark datasets, demonstrating its superior performance on the RUL prediction task compared to state-of-the-art approaches.},
  archive      = {J_TMLR},
  author       = {Ya Song and Laurens Bliek and Yaoxin Wu and Yingqian Zhang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Enhancing remaining useful life prediction with ensemble multi-term fourier graph neural networks},
  url          = {https://openreview.net/forum?id=tzFjcVqmxw},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion on graph: Augmentation of graph structure for node
classification. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=tzW948kU6x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph diffusion models have recently been proposed to synthesize entire graphs, such as molecule graphs. Although existing methods have shown great performance in generating entire graphs for graph-level learning tasks, no graph diffusion models have been developed to generate synthetic graph structures, that is, synthetic nodes and associated edges within a given graph, for node-level learning tasks. Inspired by the research in the computer vision literature using synthetic data for enhanced performance, we propose Diffusion on Graph (DoG), which generates synthetic graph structures to boost the performance of GNNs. The synthetic graph structures generated by DoG are combined with the original graph to form an augmented graph for the training of node-level learning tasks, such as node classification and graph contrastive learning (GCL). To improve the efficiency of the generation process, a Bi-Level Neighbor Map Decoder (BLND) is introduced in DoG. To mitigate the adverse effect of the noise introduced by the synthetic graph structures, a low-rank regularization method is proposed for the training of graph neural networks (GNNs) on the augmented graphs. Extensive experiments on various graph datasets for semi-supervised node classification and graph contrastive learning have been conducted to demonstrate the effectiveness of DoG with low-rank regularization. The code of DoG is available at \url{https://github.com/Statistical-Deep-Learning/DoG}.},
  archive      = {J_TMLR},
  author       = {Yancheng Wang and Changyu Liu and Yingzhen Yang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Diffusion on graph: Augmentation of graph structure for node classification},
  url          = {https://openreview.net/forum?id=tzW948kU6x},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PROXI: Challenging the GNNs for link prediction.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=u9EHndbiVw">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, Graph Neural Networks (GNNs) have transformed graph representation learning. In the widely adopted message-passing GNN framework, nodes refine their representations by aggregating information from neighboring nodes iteratively. While GNNs excel in various domains, recent theoretical studies have raised concerns about their capabilities. GNNs aim to address various graph-related tasks by utilizing such node representations, however, this one-size-fits-all approach proves suboptimal for diverse tasks. Motivated by these observations, we conduct empirical tests to compare the performance of current GNN models with more conventional and direct methods in link prediction tasks. Introducing our model, PROXI, which leverages proximity information of node pairs in both graph and attribute spaces, we find that standard machine learning (ML) models perform competitively, even outperforming cutting-edge GNN models when applied to these proximity metrics derived from node neighborhoods and attributes. This holds true across both homophilic and heterophilic networks, as well as small and large benchmark datasets, including those from the Open Graph Benchmark (OGB). Moreover, we show that augmenting traditional GNNs with PROXI significantly boosts their link prediction performance. Our empirical findings corroborate the previously mentioned theoretical observations and imply that there exists ample room for enhancement in current GNN models to reach their potential.},
  archive      = {J_TMLR},
  author       = {Astrit Tola and Jack Myrick and Baris Coskunuzer},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {PROXI: Challenging the GNNs for link prediction},
  url          = {https://openreview.net/forum?id=u9EHndbiVw},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepRRTime: Robust time-series forecasting with a
regularized INR basis. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=uDRzORdPT7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a simple, inexpensive, theoretically motivated regularization term to enhance the robustness of deep time-index models for time-series forecasting. Recently, DeepTime demonstrated that this class of models can rival state-of-the-art deep historical-value models on the long time-series forecasting (LTSF) benchmarks. The DeepTime framework comprises two key components: (1) a time-indexed basis parameterized as an implicit neural representation (INR), and (2) a meta-learning formulation that fits observed data to this basis via ridge regression, then extrapolates the result to generate forecasts. Our regularization term encourages the time-indexed basis elements to be more unit standardized and less mutually correlated, intended to enable more robust ridge regression. The regularized variant matches or outperforms DeepTime on all LTSF benchmarks. Moreover, it is significantly more resilient to missing values in the lookback window at test time, enhances forecast accuracy when applied to higher-frequency data than it was trained on, and boosts performance when trained on smaller datasets. Overall, we conclude that our regularized approach sets a new state-of-the-art for deep time-index models.},
  archive      = {J_TMLR},
  author       = {Chandramouli Shama Sastry and Mahdi Gilany and Kry Yik-Chau Lui and Martin Magill and Alexander Pashevich},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {DeepRRTime: Robust time-series forecasting with a regularized INR basis},
  url          = {https://openreview.net/forum?id=uDRzORdPT7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wonderful team: Zero-shot physical task planning with visual
LLMs. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=udVkqIDYSM">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Wonderful Team, a multi-agent Vision Large Language Model (VLLM) framework for executing high-level robotic planning in a zero-shot regime. In our context, zero-shot high-level planning means that for a novel environment, we provide a VLLM with an image of the robot&#39;s surroundings and a task description, and the VLLM outputs the sequence of actions necessary for the robot to complete the task. Unlike previous methods for high-level visual planning for robotic manipulation, our method uses VLLMs for the entire planning process, enabling a more tightly integrated loop between perception, control, and planning. As a result, Wonderful Team&#39;s performance on real-world semantic and physical planning tasks often exceeds methods that rely on separate vision systems. For example, we see an average 40% success rate improvement on VimaBench over prior methods such as NLaP, an average 30% improvement over Trajectory Generators on tasks from the Trajectory Generator paper, including drawing and wiping a plate, and an average 70% improvement over Trajectory Generators on a new set of semantic reasoning tasks including environment rearrangement with implicit linguistic constraints. We hope these results highlight the rapid improvements of VLLMs in the past year, and motivate the community to consider VLLMs as an option for some high-level robotic planning problems in the future.},
  archive      = {J_TMLR},
  author       = {Zidan Wang and Rui Shen and Bradly C. Stadie},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Wonderful team: Zero-shot physical task planning with visual LLMs},
  url          = {https://openreview.net/forum?id=udVkqIDYSM},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating interpretable methods via geometric alignment of
functional distortions. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=ukLxqA8zXj">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretability researchers face a universal question: without access to ground truth labels, how can the faithfulness of an explanation to its model be determined? Despite immense efforts to develop new evaluation methods, current approaches remain in a pre-paradigmatic state: fragmented, difficult to calibrate, and lacking cohesive theoretical grounding. Observ- ing the lack of a unifying theory, we propose a novel evaluative criterion entitled Generalised Explanation Faithfulness (GEF) which is centered on explanation-to-model alignment, and integrates existing perturbation-based evaluations to eliminate the need for singular, task-specific evaluations. Complementing this unifying perspective, from a geometric point of view, we reveal a prevalent yet critical oversight in current evaluation practice: the failure to account for the learned geometry, and non-linear mapping present in the model, and explanation spaces. To solve this, we propose a general-purpose, threshold-free faithfulness evaluator GEF that incorporates principles from differential geometry, and facilitates evaluation agnostically across tasks, and interpretability approaches. Through extensive cross-domain benchmarks on natural language processing, vision, and tabular tasks, we provide first-of-its-kind insights into the comparative performance of various interpretable methods. This includes local linear approximators, global feature visualisation methods, large language models as post-hoc explainers, and sparse autoencoders. Our contributions are important to the interpretability and AI safety communities, offering a principled, unified approach for evaluation.},
  archive      = {J_TMLR},
  author       = {Anna Hedström and Philine Lou Bommer and Thomas F Burns and Sebastian Lapuschkin and Wojciech Samek and Marina MC Höhne},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Evaluating interpretable methods via geometric alignment of functional distortions},
  url          = {https://openreview.net/forum?id=ukLxqA8zXj},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-level representation learning with joint-embedding
predictive architectures. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=v47f4DwYZb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal y from the latent representation of a context signal x. JEPAs bypass the need for negative and positive samples, traditionally required by contrastive learning while avoiding the overfitting issues associated with generative pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm by proposing a Graph Joint-Embedding Predictive Architecture (Graph-JEPA). In particular, we employ masked modeling and focus on predicting the latent representations of masked subgraphs starting from the latent representation of a context subgraph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative prediction objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2D plane. Through multiple experimental evaluations, we show that Graph-JEPA can learn highly semantic and expressive representations, as shown by the downstream performance in graph classification, regression, and distinguishing non-isomorphic graphs. The code is available at https://github.com/geriskenderi/graph-jepa.},
  archive      = {J_TMLR},
  author       = {Geri Skenderi and Hang Li and Jiliang Tang and Marco Cristani},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Graph-level representation learning with joint-embedding predictive architectures},
  url          = {https://openreview.net/forum?id=v47f4DwYZb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimax lower bounds for estimating distributions on
low-dimensional spaces. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=wIgRV336hC">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent statistical analyses of Generative Adversarial Networks (GAN) suggest that the error in estimating the target distribution in terms of the $\beta$-H\&quot;{o}lder Integral Probability Metric (IPM) scales as $\mathcal{O}\left(n^{-\frac{\beta}{\overline{d}_{\mathbb{M}}+\delta}} \vee n^{-1/2} \log n \right)$. Here $\overline{d}_{\mathbb{M}}$ is the upper Minkowski dimension of the corresponding support $\mathbb{M}$ of the data distribution and $\delta$ is a positive constant. It is, however, unknown as to whether this rate is minimax optimal, i.e. whether there are estimators that achieve a better test-error rate. This paper demonstrates that the minimax rate for estimating unknown distributions in the $\beta$-H\&quot;{o}lder IPM on $\mathbb{M}$ scales as $\Omega\left(n^{-\frac{\beta}{\underline{d}_{\mathbb{M}}-\delta}} \vee n^{-1/2}\right)$, where $\underline{d}_{\mathbb{M}}$ is the lower Minkowski dimension of $\mathbb{M}$. Thus if the low-dimensional structure $\mathbb{M}$ is regular in the Minkowski sense, i.e. $\overline{d}_{\mathbb{M}} = \underline{d}_{\mathbb{M}}$, GANs are roughly minimax optimal in estimating distributions on $\mathbb{M}$. Further, the paper shows that the minimax estimation rate in the $p$-Wasserstein metric scales as $\Omega\left(n^{-\frac{1}{\underline{d}_{\mathbb{M}}-\delta}} \vee n^{-1/(2p)}\right)$.},
  archive      = {J_TMLR},
  author       = {Saptarshi Chakraborty},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Minimax lower bounds for estimating distributions on low-dimensional spaces},
  url          = {https://openreview.net/forum?id=wIgRV336hC},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The elusive pursuit of reproducing PATE-GAN: Benchmarking,
auditing, debugging. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=wcxrJcJ7vq">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic data created by differentially private (DP) generative models is increasingly used in real-world settings. In this context, PATE-GAN has emerged as one of the most popular algorithms, combining Generative Adversarial Networks (GANs) with the private training approach of PATE (Private Aggregation of Teacher Ensembles). In this paper, we set out to reproduce the utility evaluation from the original PATE-GAN paper, compare available implementations, and conduct a privacy audit. More precisely, we analyze and benchmark six open-source PATE-GAN implementations, including three by (a subset of) the original authors. First, we shed light on architecture deviations and empirically demonstrate that none reproduce the utility performance reported in the original paper. We then present an in-depth privacy evaluation, which includes DP auditing, and show that \textit{all implementations leak more privacy than intended}. Furthermore, we uncover \textit{19 privacy violations} and 5 other bugs in these six open-source implementations. Lastly, our codebase is available from: \url{https://github.com/spalabucr/pategan-audit}.},
  archive      = {J_TMLR},
  author       = {Georgi Ganev and Meenatchi Sundaram Muthu Selva Annamalai and Emiliano De Cristofaro},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {The elusive pursuit of reproducing PATE-GAN: Benchmarking, auditing, debugging},
  url          = {https://openreview.net/forum?id=wcxrJcJ7vq},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpreting neurons in deep vision networks with language
models. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=x1dXvvElVd">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Describe-and-Dissect (DnD), a novel method to describe the roles of hidden neurons in vision networks. DnD utilizes recent advancements in multimodal deep learning to produce complex natural language descriptions, without the need for labeled training data or a predefined set of concepts to choose from. Additionally, DnD is training-free, meaning we don’t train any new models and can easily leverage more capable general purpose models in the future. We have conducted extensive qualitative and quantitative analysis to show that DnD outperforms prior work by providing higher quality neuron descriptions. Specifically, our method on average provides the highest quality labels and is more than 2× as likely to be selected as the best explanation for a neuron than the best baseline. Finally, we present a use case providing critical insights into land cover prediction models for sustainability applications.},
  archive      = {J_TMLR},
  author       = {Nicholas Bai and Rahul Ajay Iyer and Tuomas Oikarinen and Akshay R. Kulkarni and Tsui-Wei Weng},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Interpreting neurons in deep vision networks with language models},
  url          = {https://openreview.net/forum?id=x1dXvvElVd},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The 2023 foundation model transparency index. <em>TMLR</em>.
(<a href="https://openreview.net/forum?id=x6fXnsM9Ez">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts. While the societal impact of foundation models is growing, transparency is on the decline, mirroring the opacity that has plagued past digital technologies (e.g. social media). Reversing this trend is essential: transparency is a vital precondition for public accountability, scientific innovation, and effective governance. To assess the transparency of the foundation model ecosystem and help improve transparency over time, we introduce the Foundation Model Transparency Index. The Foundation Model Transparency Index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). We score 10 major foundation model developers (e.g. OpenAI, Google, Meta) against the 100 indicators to assess their transparency. To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. Overall, the Foundation Model Transparency Index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.},
  archive      = {J_TMLR},
  author       = {Rishi Bommasani and Kevin Klyman and Shayne Longpre and Sayash Kapoor and Nestor Maslej and Betty Xiong and Daniel Zhang and Percy Liang},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {The 2023 foundation model transparency index},
  url          = {https://openreview.net/forum?id=x6fXnsM9Ez},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Faster diffusion through temporal attention decomposition.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=xXs2GKXPnH">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the role of the attention mechanism during inference in text-conditional diffusion models. Empirical observations suggest that cross-attention outputs converge to a fixed point after several inference steps. The convergence time naturally divides the entire inference process into two phases: an initial phase for planning text-oriented visual semantics, which are then translated into images in a subsequent fidelity-improving phase. Cross-attention is essential in the initial phase but almost irrelevant thereafter. Self-attention, however, initially plays a minor role but becomes increasingly important in the second phase. These findings yield a simple and training-free method called TGATE which efficiently generates images by caching and reusing attention outputs at scheduled time steps. Experiments show TGATE’s broad applicability to various existing text-conditional diffusion models which it speeds up by 10-50%. The code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.},
  archive      = {J_TMLR},
  author       = {Haozhe Liu and Wentian Zhang and Jinheng Xie and Francesco Faccio and Mengmeng Xu and Tao Xiang and Mike Zheng Shou and Juan-Manuel Perez-Rua and Jürgen Schmidhuber},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Faster diffusion through temporal attention decomposition},
  url          = {https://openreview.net/forum?id=xXs2GKXPnH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially frozen random networks contain compact strong
lottery tickets. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=xpnPYfufhz">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning—strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated that SLTs could also be found within a randomly pruned source network. This phenomenon can be exploited to further compress the small memory size required by SLTs. However, their method is limited to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method for reducing the SLT memory size without restricting the sparsity of the SLTs that can be found. A random subset of the initial weights is frozen by either permanently pruning them or locking them as a fixed part of the SLT, resulting in a smaller model size. Experimental results show that Edge-Popup (Ramanujan et al., 2020; Sreenivasan et al., 2022) finds SLTs with better accuracy-to-model size trade-off within frozen networks than within dense or randomly pruned source networks. In particular, freezing $70\%$ of a ResNet on ImageNet provides $3.3\times$ compression compared to the SLT found within a dense counterpart, raises accuracy by up to $14.12$ points compared to the SLT found within a randomly pruned counterpart, and offers a better accuracy-model size trade-off than both.},
  archive      = {J_TMLR},
  author       = {Hikari Otsuka and Daiki Chijiwa and Ángel López García-Arias and Yasuyuki Okoshi and Kazushi Kawamura and Thiem Van Chu and Daichi Fujiki and Susumu Takeuchi and Masato Motomura},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Partially frozen random networks contain compact strong lottery tickets},
  url          = {https://openreview.net/forum?id=xpnPYfufhz},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLaVA-OneVision: Easy visual task transfer. <em>TMLR</em>.
(<a href="https://openreview.net/forum?id=zKv8qULV6n">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.},
  archive      = {J_TMLR},
  author       = {Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Peiyuan Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {LLaVA-OneVision: Easy visual task transfer},
  url          = {https://openreview.net/forum?id=zKv8qULV6n},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural material point method for particle-based emulation.
<em>TMLR</em>. (<a
href="https://openreview.net/forum?id=zSK81A2hxQ">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh-free Lagrangian methods are widely used for simulating fluids, solids, and their complex interactions due to their ability to handle large deformations and topological changes. These physics simulators, however, require substantial computational resources for accurate simulations. To address these issues, deep learning emulators promise faster and scalable simulations, yet they often remain expensive and difficult to train, limiting their practical use. Inspired by the Material Point Method (MPM), we present NeuralMPM, a neural framework for particle-based emulation. NeuralMPM interpolates Lagrangian particles onto a fixed-size grid, computes updates on grid nodes using image-to-image neural networks, and interpolates back to the particles. Similarly to MPM, NeuralMPM benefits from the regular voxelized representation to simplify the computation of the state dynamics, while avoiding the drawbacks of mesh-based Eulerian methods. We demonstrate the advantages of NeuralMPM on 6 datasets, including fluid dynamics and fluid-solid interactions simulated with MPM and Smoothed Particles Hydrodynamics (SPH). Compared to GNS and DMCF, NeuralMPM reduces training time from 10 days to 15 hours, memory consumption by 10x-100x, and increases inference speed by 5x-10x, while achieving comparable or superior long-term accuracy, making it a promising approach for practical forward and inverse problems. A project page is available at https://neuralmpm.isach.be/.},
  archive      = {J_TMLR},
  author       = {Omer Rochman-Sharabi and Sacha Lewin and Gilles Louppe},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {A neural material point method for particle-based emulation},
  url          = {https://openreview.net/forum?id=zSK81A2hxQ},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On diffusion-based generative models and their error bounds:
The log-concave case with full convergence estimates. <em>TMLR</em>. (<a
href="https://openreview.net/forum?id=zjxKrb4ehr">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions avoiding any Lipschitzness assumption on the score function. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm. Beyond the motivating example and in order to allow for the use of a diverse range of stochastic optimizers, we present our results using an $L^2$-accurate score estimation assumption, which crucially is formed under an expectation with respect to the stochastic optimizer and our novel auxiliary process that uses only known information. This approach yields the best known convergence rate for our sampling algorithm.},
  archive      = {J_TMLR},
  author       = {Stefano Bruno and Ying Zhang and Dongyoung Lim and Omer Deniz Akyildiz and Sotirios Sabanis},
  journal      = {Transactions on Machine Learning Research},
  month        = {2},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates},
  url          = {https://openreview.net/forum?id=zjxKrb4ehr},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
