<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim---65">SIM - 65</h2>
<ul>
<li><details>
<summary>
(2025). Estimating mean viral load trajectory from intermittent
longitudinal data and unknown time origins. <em>SIM</em>,
<em>44</em>(5), e70033. (<a
href="https://doi.org/10.1002/sim.70033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viral load (VL) in the respiratory tract is the leading proxy for assessing infectiousness potential. Understanding the dynamics of disease-related VL within the host is of great importance, as it helps to determine different policies and health recommendations. However, normally the VL is measured on individuals only once, in order to confirm infection, and furthermore, the infection date is unknown. It is therefore necessary to develop statistical approaches to estimate the typical VL trajectory. We show here that, under plausible parametric assumptions, two measures of VL on infected individuals can be used to accurately estimate the VL mean function. Specifically, we consider a discrete-time likelihood-based approach to modeling and estimating partial observed longitudinal samples. We study a multivariate normal model for a function of the VL that accounts for possible correlation between measurements within individuals. We derive an expectation-maximization (EM) algorithm which treats the unknown time origins and the missing measurements as latent variables. Our main motivation is the reconstruction of the daily mean VL, given measurements on patients whose VLs were measured multiple times on different days. Such data should and can be obtained at the beginning of a pandemic with the specific goal of estimating the VL dynamics. For demonstration purposes, the method is applied to SARS-Cov-2 cycle-threshold-value data collected in Israel.},
  archive      = {J_SIM},
  author       = {Yonatan Woodbridge and Micha Mandel and Yair Goldberg and Amit Huppert},
  doi          = {10.1002/sim.70033},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70033},
  shortjournal = {Stat. Med.},
  title        = {Estimating mean viral load trajectory from intermittent longitudinal data and unknown time origins},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling the restricted mean survival time using
pseudo-value random forests. <em>SIM</em>, <em>44</em>(5), e70031. (<a
href="https://doi.org/10.1002/sim.70031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean survival time (RMST) has become a popular measure to summarize event times in longitudinal studies. Defined as the area under the survival function up to a time horizon τ &gt; 0 $$ \tau &gt;0 $$ , the RMST can be interpreted as the life expectancy within the time interval [ 0 , τ ] $$ \left[0,\tau \right] $$ . In addition to its straightforward interpretation, the RMST allows for the definition of valid estimands for the causal analysis of treatment contrasts in medical studies. In this work, we introduce a non-parametric approach to model the RMST conditional on a set of baseline variables (including, e.g., treatment variables and confounders). Our method is based on a direct modeling strategy for the RMST, using leave-one-out jackknife pseudo-values within a random forest regression framework. In this way, it can be employed to obtain precise estimates of both patient-specific RMST values and confounder-adjusted treatment contrasts. Since our method (termed “pseudo-value random forest”, PVRF) is model-free, RMST estimates are not affected by restrictive assumptions like the proportional hazards assumption. Particularly, PVRF offers a high flexibility in detecting relevant covariate effects from higher-dimensional data, thereby expanding the range of existing pseudo-value modeling techniques for RMST estimation. We investigate the properties of our method using simulations and illustrate its use by an application to data from the SUCCESS-A breast cancer trial. Our numerical experiments demonstrate that PVRF yields accurate estimates of both patient-specific RMST values and RMST-based treatment contrasts.},
  archive      = {J_SIM},
  author       = {Alina Schenk and Vanessa Basten and Matthias Schmid},
  doi          = {10.1002/sim.70031},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70031},
  shortjournal = {Stat. Med.},
  title        = {Modeling the restricted mean survival time using pseudo-value random forests},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network meta-analysis with individual participant-level data
of time-to-event outcomes using cox regression. <em>SIM</em>,
<em>44</em>(5), e70027. (<a
href="https://doi.org/10.1002/sim.70027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accessibility of individual participant-level data (IPD) enhances the evaluation of moderation effects of patient covariates. It facilitates the provision of accurate estimation of intervention effects and confidence intervals by incorporating covariate correlations across multiple clinical trials. With a time-to-event outcome, Cox regression can be applied for network meta-analysis (NMA) using IPD. However, there lacks comprehensive reviews and comparisons of the specifications and assumptions of these Cox models and their impact on the interpretation of hazard ratios, effect moderation, and trial heterogeneity in IPD-NMA. In this paper, we examine various Cox models for IPD-NMA and compare different approaches to modeling trial, treatment, and covariate effects. We employ multiple graphical tools and statistical tests to assess proportional hazard assumptions and discuss their implications. Additionally, we explore the application of extended Cox models when the proportional hazard assumption is violated. Practical guidance on interpreting and reporting NMA results is provided. A simulation study is conducted to compare the performance of different models. We illustrate the methods to conduct IPD-NMA through a real data example.},
  archive      = {J_SIM},
  author       = {Kaiyuan Hua and Daniel Wojdyla and Anthony Carnicelli and Christopher Granger and Xiaofei Wang and Hwanhee Hong},
  doi          = {10.1002/sim.70027},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70027},
  shortjournal = {Stat. Med.},
  title        = {Network meta-analysis with individual participant-level data of time-to-event outcomes using cox regression},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to assess the predictiveness of a
continuous biomarker in early phases of drug development. <em>SIM</em>,
<em>44</em>(5), e70026. (<a
href="https://doi.org/10.1002/sim.70026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and quantifying predictive biomarkers is a critical issue of personalized medicine approaches and patient-centric clinical development strategies. In early stages of the development process, significant challenges and numerous uncertainties arise. One of the challenges is the ability to assess the predictive value of a biomarker, i.e., the difference in primary outcomes between experimental and placebo arms above and below a certain threshold of the biomarker. Indeed, when the accumulated information is very limited and the sample size is small, preliminary conclusions about the predictive properties of the biomarker might be misleading. To date, the majority of investigations regarding the predictiveness of biomarkers were in the setting of moderate-to-large sample sizes. In this work, we propose a novel flexible approach inspired by the Kolmogorov-Smirnov Distance in order to assess the predictiveness of a continuous biomarker in a clinical setting where the sample size is small. Via simulations we show that the proposed method allows to achieve a higher power to declare predictiveness compared to the existing methods under a range of scenarios, whilst still maintaining a control of the type I error at a pre-specified level.},
  archive      = {J_SIM},
  author       = {Alessandra Serra and Julia Geronimi and Sandrine Guilleminot and Hugo Hadjur and Marie-Karelle Riviere and Gaëlle Saint-Hilary and Pavel Mozgunov},
  doi          = {10.1002/sim.70026},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70026},
  shortjournal = {Stat. Med.},
  title        = {A novel approach to assess the predictiveness of a continuous biomarker in early phases of drug development},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A double machine learning approach for the evaluation of
COVID-19 vaccine effectiveness under the test-negative design: Analysis
of québec administrative data. <em>SIM</em>, <em>44</em>(5), e70025. (<a
href="https://doi.org/10.1002/sim.70025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The test-negative design (TND), which is routinely used for monitoring seasonal flu vaccine effectiveness (VE), has recently become integral to COVID-19 vaccine surveillance, notably in Québec, Canada. Some studies have addressed the identifiability and estimation of causal parameters under the TND, but efficiency bounds for nonparametric estimators of the target parameter under the unconfoundedness assumption have not yet been investigated. Motivated by the goal of improving adjustment for measured confounders when estimating COVID-19 VE among community-dwelling people aged ≥ 60 $$ \ge 60 $$ years in Québec, we propose a one-step doubly robust and locally efficient estimator called TNDDR (TND doubly robust), which utilizes cross-fitting (sample splitting) and can incorporate machine learning techniques to estimate the nuisance functions and thus improve control for measured confounders. We derive the efficient influence function (EIF) for the marginal expectation of the outcome under a vaccination intervention, explore the von Mises expansion, and establish the conditions for n $$ \sqrt{n} $$ -consistency, asymptotic normality, and double robustness of TNDDR. The proposed estimator is supported by both theoretical and empirical justifications.},
  archive      = {J_SIM},
  author       = {Cong Jiang and Denis Talbot and Sara Carazo and Mireille E. Schnitzer},
  doi          = {10.1002/sim.70025},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70025},
  shortjournal = {Stat. Med.},
  title        = {A double machine learning approach for the evaluation of COVID-19 vaccine effectiveness under the test-negative design: Analysis of québec administrative data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using win odds to improve commit-to-phase-3 decision-making
in oncology. <em>SIM</em>, <em>44</em>(5), e70024. (<a
href="https://doi.org/10.1002/sim.70024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making good decisions about whether to commit-to-phase 3 clinical trials is challenging. This is especially true in oncology because the relationships between the registration endpoint, overall survival, and endpoints such as progression-free survival and confirmed objective response are often poorly understood. We present a framework for decision-making based on a three-endpoint win odds. We discuss properties of the win odds and suggest that it can be interpreted, for decision-making, as the reciprocal of an average hazard ratio for overall survival. We confirm the performance of the decision-making method using simulation studies and a clinical trial case study. As part of this work, we describe the simulation of correlated patient-level oncology endpoints using a multi-state model of disease. This model can provide clinically realistic data for testing the performance of analysis methods. We conclude that the win odds can improve commit-to-phase-3 decision-making compared with other methods.},
  archive      = {J_SIM},
  author       = {Benjamin F. Hartley and Thomas Drury and Brian Di Pace and Helen Zhou and Tai-Tsang Chen and Inna Perevozskaya},
  doi          = {10.1002/sim.70024},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70024},
  shortjournal = {Stat. Med.},
  title        = {Using win odds to improve commit-to-phase-3 decision-making in oncology},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly modeling time-to-event and longitudinal data with
individual-specific change points: A case study in modeling tumor
burden. <em>SIM</em>, <em>44</em>(5), e70021. (<a
href="https://doi.org/10.1002/sim.70021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In oncology clinical trials, tumor burden (TB) stands as a crucial longitudinal biomarker, reflecting the toll a tumor takes on a patient&#39;s prognosis. With certain treatments, the disease&#39;s natural progression shows the tumor burden initially receding before rising once more. Biologically, the point of change may be different between individuals and must have occurred between the baseline measurement and progression time of the patient, implying a random effects model obeying a bound constraint. However, in practice, patients may drop out of the study due to progression or death, presenting a non-ignorable missing data problem. In this paper, we introduce a novel joint model that combines time-to-event data and longitudinal data, where the latter is parameterized by a random change point augmented by random pre-slope and post-slope dynamics. Importantly, the model is equipped to incorporate covariates across the longitudinal and survival models, adding significant flexibility. Adopting a Bayesian approach, we propose an efficient Hamiltonian Monte Carlo (HMC) algorithm for parameter inference. We demonstrate the superiority of our approach compared to a longitudinal-only model via simulations and apply our method to a data set in oncology. The code for implementation is publicly available on https://github.com/quyixiang/chgptModel .},
  archive      = {J_SIM},
  author       = {Ethan M. Alt and Yixiang Qu and Emily Meghan Damone and Jing-ou Liu and Chenguang Wang and Joseph G. Ibrahim},
  doi          = {10.1002/sim.70021},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70021},
  shortjournal = {Stat. Med.},
  title        = {Jointly modeling time-to-event and longitudinal data with individual-specific change points: A case study in modeling tumor burden},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph-theoretic approach to detection of parkinsonian
freezing of gait from videos. <em>SIM</em>, <em>44</em>(5), e70020. (<a
href="https://doi.org/10.1002/sim.70020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of Gait (FOG) is a prevalent symptom in advanced Parkinson&#39;s Disease (PD), characterized by intermittent transitions between normal gait and freezing episodes. This study introduces a novel graph-theoretic approach to detect FOG from video data of PD patients. We construct a sequence of pose graphs that represent the spatial relations and temporal progression of a patient&#39;s posture over time. Each graph node corresponds to an estimated joint position, while the edges reflect the anatomical connections and their proximity. We propose a hypothesis testing procedure that deploys the Fréchet statistics to identify break points in time between regular gait and FOG episodes, where we model the central tendency and dispersion of the pose graphs in the presentation of graph Laplacian matrices by computing their Fréchet mean and variance. We implement binary segmentation and incremental computation in our algorithm for efficient calculation. The proposed framework is validated on two datasets, Kinect3D and AlphaPose, demonstrating its effectiveness in detecting FOG from video data. The proposed approach that extracts matrix features is distinct from the prevailing pixel-based deep learning methods. It provides a new perspective on feature extraction for FOG detection and potentially contributes to improved diagnosis and treatment of PD.},
  archive      = {J_SIM},
  author       = {Qi Liu and Jie Bao and Xu Zhang and Chuan Shi and Catherine Liu and Rui Luo},
  doi          = {10.1002/sim.70020},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70020},
  shortjournal = {Stat. Med.},
  title        = {A graph-theoretic approach to detection of parkinsonian freezing of gait from videos},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of statistical methods for time-to-event
analyses in randomized controlled trials under non-proportional hazards.
<em>SIM</em>, <em>44</em>(5), e70019. (<a
href="https://doi.org/10.1002/sim.70019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While well-established methods for time-to-event data are available when the proportional hazards assumption holds, there is no consensus on the best inferential approach under non-proportional hazards (NPH). However, a wide range of parametric and non-parametric methods for testing and estimation in this scenario have been proposed. To provide recommendations on the statistical analysis of clinical trials where non-proportional hazards are expected, we conducted a simulation study under different scenarios of non-proportional hazards, including delayed onset of treatment effect, crossing hazard curves, subgroups with different treatment effects, and changing hazards after disease progression. We assessed type I error rate control, power, and confidence interval coverage, where applicable, for a wide range of methods, including weighted log-rank tests, the MaxCombo test, summary measures such as the restricted mean survival time (RMST), average hazard ratios, and milestone survival probabilities, as well as accelerated failure time regression models. We found a trade-off between interpretability and power when choosing an analysis strategy under NPH scenarios. While analysis methods based on weighted logrank tests typically were favorable in terms of power, they do not provide an easily interpretable treatment effect estimate. Also, depending on the weight function, they test a narrow null hypothesis of equal hazard functions, and rejection of this null hypothesis may not allow for a direct conclusion of treatment benefit in terms of the survival function. In contrast, non-parametric procedures based on well-interpretable measures like the RMST difference had lower power in most scenarios. Model-based methods based on specific survival distributions had larger power; however, often gave biased estimates and lower than nominal confidence interval coverage. The application of the studied methods is illustrated in a case study with reconstructed data from a phase III oncologic trial.},
  archive      = {J_SIM},
  author       = {Florian Klinglmüller and Tobias Fellinger and Franz König and Tim Friede and Andrew C. Hooker and Harald Heinzl and Martina Mittlböck and Jonas Brugger and Maximilian Bardo and Cynthia Huber and Norbert Benda and Martin Posch and Robin Ristl},
  doi          = {10.1002/sim.70019},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70019},
  shortjournal = {Stat. Med.},
  title        = {A comparison of statistical methods for time-to-event analyses in randomized controlled trials under non-proportional hazards},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power and sample size calculations for cluster randomized
hybrid type 2 effectiveness-implementation studies. <em>SIM</em>,
<em>44</em>(5), e70015. (<a
href="https://doi.org/10.1002/sim.70015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid studies allow investigators to simultaneously study an intervention effectiveness outcome and an implementation research outcome. In particular, type 2 hybrid studies support research that places equal importance on both outcomes rather than focusing on one and secondarily on the other (i.e., type 1 and type 3 studies). Hybrid 2 studies introduce the statistical issue of multiple testing, complicated by the fact that they are typically also cluster randomized trials. Standard statistical methods do not apply in this scenario. Here, we describe the design methodologies available for validly powering hybrid type 2 studies and producing reliable sample size calculations in a cluster-randomized design with a focus on binary outcomes. Through a literature search, 18 publications were identified that included methods relevant to the design of hybrid 2 studies. Five methods were identified, two of which did not account for clustering but are extended in this article to do so, namely the combined outcomes approach and the single 1-degree of freedom combined test. Procedures for powering hybrid 2 studies using these five methods are described and illustrated using input parameters inspired by a study from the Community Intervention to Reduce CardiovascuLar Disease in Chicago (CIRCL-Chicago) Implementation Research Center. In this illustrative example, the intervention effectiveness outcome was controlled blood pressure, and the implementation outcome was reach. The conjunctive test resulted in higher power than the popular p value adjustment methods, and the newly extended combined outcomes and single 1-DF test were found to be the most powerful among all of the tests.},
  archive      = {J_SIM},
  author       = {Melody A. Owen and Geoffrey M. Curran and Justin D. Smith and Yacob Tedla and Chao Cheng and Donna Spiegelman},
  doi          = {10.1002/sim.70015},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70015},
  shortjournal = {Stat. Med.},
  title        = {Power and sample size calculations for cluster randomized hybrid type 2 effectiveness-implementation studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DOD-SSR: An adaptive seamless phase II/III design with dose
optimization decision and sample size re-estimation. <em>SIM</em>,
<em>44</em>(5), e70014. (<a
href="https://doi.org/10.1002/sim.70014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of seamless Phase II/III designs has grown in popularity as a strategy to potentially accelerate the drug development. Making well-informed decisions regarding the drug&#39;s potential and addressing important clinical inquiries at the conclusion of the exploratory phase has become a critical step. In response to the increased emphasis on dose optimization, it becomes logical to integrate treatment arm/dose selections into Phase II and implement corresponding design adjustments. Within this framework, employing a fixed sample size presents challenges due to limited information availability before the trial planning and elevated development risks. Furthermore, practical and feasibility considerations have led to the increased utilization of surrogate endpoints for making interim decisions. In this study, we introduce a novel framework for a seamless Phase II/III design involving multiple treatment arms, leveraging Bayesian predictive probability of success (PPoS) for both treatment arm selection and interim sample size re-estimation (SSR) using surrogate endpoints. The proposed design demonstrates improved performance, including a higher likelihood of selecting favorable treatment arm, increased overall statistical power, and reduced average event sizes and trial durations compared to traditional separate Phase II and III designs, as well as other seamless Phase II/III designs without SSR or of which treatment arm selection is based on conditional power. We also showcase the implementation of the proposed design through a case study in non-small cell lung cancer (NSCLC).},
  archive      = {J_SIM},
  author       = {Meizi Liu and Jianchang Lin and Yefei Zhang and Rachael Liu},
  doi          = {10.1002/sim.70014},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70014},
  shortjournal = {Stat. Med.},
  title        = {DOD-SSR: An adaptive seamless phase II/III design with dose optimization decision and sample size re-estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Challenges for predictive modeling with neural network
techniques using error-prone dietary intake data. <em>SIM</em>,
<em>44</em>(5), e70013. (<a
href="https://doi.org/10.1002/sim.70013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dietary intake data are routinely drawn upon to explore diet-health relationships, and inform clinical practice and public health. However, these data are almost always subject to measurement error, distorting true diet-health relationships. Beyond measurement error, there are likely complex synergistic and sometimes antagonistic interactions between different dietary components, complicating the relationships between diet and health outcomes. Flexible models are required to capture the nuance that these complex interactions introduce. This complexity makes research on diet-health relationships an appealing candidate for the application of modern machine learning techniques, and in particular, neural networks. Neural networks are computational models that can capture highly complex, nonlinear relationships, so long as sufficient data are available. While these models have been applied in many domains, the impacts of measurement error on the performance of predictive modeling have not been widely investigated. In this work, we demonstrate the ways in which measurement error erodes the performance of neural networks and illustrate the care that is required for leveraging these models in the presence of error. We demonstrate the role that sample size and replicate measurements play in model performance, indicate a motivation for the investigation of transformations to additivity, and illustrate the caution required to prevent model overfitting. While the past performance of neural networks across various domains makes them an attractive candidate for examining diet-health relationships, our work demonstrates that substantial care and further methodological development are both required to observe increased predictive performance when applying these techniques compared to more traditional statistical procedures.},
  archive      = {J_SIM},
  author       = {Dylan Spicker and Amir Nazemi and Joy Hutchinson and Paul Fieguth and Sharon Kirkpatrick and Michael Wallace and Kevin W. Dodd},
  doi          = {10.1002/sim.70013},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70013},
  shortjournal = {Stat. Med.},
  title        = {Challenges for predictive modeling with neural network techniques using error-prone dietary intake data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RMST for interval-censored data in oncology clinical trials.
<em>SIM</em>, <em>44</em>(5), e70012. (<a
href="https://doi.org/10.1002/sim.70012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In oncology studies, the assumption of proportional hazards is often questionable due to factors such as the presence of cured patients, a delayed treatment benefit, and possible treatment switching. The restricted mean survival time (RMST) has emerged as a valuable alternative summary measure to the hazard ratio (HR) in this scenario as it provides a clinically meaningful interpretation of treatment benefit without additional assumptions. As a commonly used primary endpoint, progression-free survival (PFS) is defined as the time from randomization to the first occurrence of death or progression of disease (PD). However, PFS involves dual observation processes where, in practice, the exact death time is typically recorded, but PD is interval-censored. This feature is also present in other commonly used primary endpoints, including event-free survival, disease-free survival, and relapse-free survival. The conventional approach imputes the PD time with the right boundary of the time interval during which the PD occurs. This paper presents alternative estimation and inference approaches to estimate RMST with a mixture of right-censored and interval-censored data. Different approaches are explored by simulation under various plausible scenarios for oncology clinical trials with regard to the assessment frequency, randomness in the actual assessment times, and size of treatment effect. The choice of the restricted time point in RMST is also explored. The simulation results indicate that the RMST estimators that take account of the interval censoring inherent in the data are unbiased and more accurate than the conventional estimators, while the performance for two-group comparisons is comparable. Furthermore, the performance of the proposed estimators is contingent on the scheduled assessment plan and patients&#39; visit window.},
  archive      = {J_SIM},
  author       = {Xiyuan Gao and Tianmeng Lyu and Menghao Xu and Lisa V. Hampson and Yan Du and Renxin Lin and Nigel Yateman and Lu Tian and Jianguo Sun},
  doi          = {10.1002/sim.70012},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70012},
  shortjournal = {Stat. Med.},
  title        = {RMST for interval-censored data in oncology clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instability of the AUROC of clinical prediction models.
<em>SIM</em>, <em>44</em>(5), e70011. (<a
href="https://doi.org/10.1002/sim.70011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Florian D. van Leeuwen and Ewout W. Steyerberg and David van Klaveren and Ben Wessler and David M. Kent and Erik W. van Zwet},
  doi          = {10.1002/sim.70011},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70011},
  shortjournal = {Stat. Med.},
  title        = {Instability of the AUROC of clinical prediction models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A calibrated sensitivity analysis for weighted causal
decompositions. <em>SIM</em>, <em>44</em>(5), e70010. (<a
href="https://doi.org/10.1002/sim.70010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disparities in health or well-being experienced by minority groups can be difficult to study using the traditional exposure-outcome paradigm in causal inference, since potential outcomes in variables such as race or sexual minority status are challenging to interpret. Causal decomposition analysis addresses this gap by positing causal effects on disparities under interventions to other intervenable exposures that may play a mediating role in the disparity. While invoking weaker assumptions than causal mediation approaches, decomposition analyses are often conducted in observational settings and require uncheckable assumptions that eliminate unmeasured confounders. Leveraging the marginal sensitivity model, we develop a sensitivity analysis for weighted causal decomposition estimators and use the percentile bootstrap to construct valid confidence intervals for causal effects on disparities. We also propose a two-parameter reformulation that enhances interpretability and facilitates an intuitive understanding of the plausibility of unmeasured confounders and their effects. We illustrate our framework on a study examining the effect of parental support on disparities in suicidal ideation among sexual minority youth. We find that the effect is small and sensitive to unmeasured confounding, suggesting that further screening studies are needed to identify mitigating interventions in this vulnerable population.},
  archive      = {J_SIM},
  author       = {Andy A. Shen and Elina Visoki and Ran Barzilay and Samuel D. Pimentel},
  doi          = {10.1002/sim.70010},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70010},
  shortjournal = {Stat. Med.},
  title        = {A calibrated sensitivity analysis for weighted causal decompositions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse probability of treatment weighting using the
propensity score with competing risks in survival analysis.
<em>SIM</em>, <em>44</em>(5), e70009. (<a
href="https://doi.org/10.1002/sim.70009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability of treatment weighting (IPTW) using the propensity score allows estimation of the effect of treatment in observational studies. We had three objectives: first, to describe methods for using IPTW to estimate the effects of treatments in settings with competing risks; second, to illustrate the application of these methods using empirical analyses; and third, to conduct Monte Carlo simulations to evaluate the relative performance of three methods for estimating time-specific risk differences and time-specific relative risks in settings with competing risks. In doing so, we provide guidance to applied biostatisticians and clinical investigators on the use of IPTW in settings with competing risks. We examined three estimators of time-specific risk differences and relative risks: the weighted Aalen–Johansen estimator, an estimator that combines IPTW with inverse probability of censoring weights (IPTW-IPCWs), and a double-robust augmented IPTW estimator combined with IPCW (AIPTW-IPCW). The design of our simulations reflected clinically realistic scenarios. Our simulations found that all three estimators tended to result in unbiased estimations of time-specific risk differences and time-specific relative risks. However, the weighted Aalen–Johansen estimator and the AIPTW-IPCW estimator tended to result in estimates with greater precision compared to the IPTW-IPCW estimator. In our empirical analyses, we illustrated the application of these methods by estimating the effect of statin prescribing on the risk of subsequent cardiovascular death in patients discharged from the hospital with a diagnosis of acute myocardial infarction.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Jason P. Fine},
  doi          = {10.1002/sim.70009},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70009},
  shortjournal = {Stat. Med.},
  title        = {Inverse probability of treatment weighting using the propensity score with competing risks in survival analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proportional mean residual life model with varying
coefficients for right censored data. <em>SIM</em>, <em>44</em>(5),
e70008. (<a href="https://doi.org/10.1002/sim.70008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual life provides the remaining life expectancy of a subject who has survived to a specific time point. This paper considers a proportional mean residual life model with varying coefficients, which allows one to explore the nonlinear interactions between some covariates and an exposure variable. In a semiparametric setting, we construct local estimating equations to obtain the varying coefficients and establish the asymptotic normality of the proposed estimators. Moreover, the weak convergence property for the local estimator of the baseline mean residual life function is developed. We conduct simulation studies to empirically examine the finite-sample performance of the proposed methods and apply the methodology to a real-life dataset on type 2 diabetic complications.},
  archive      = {J_SIM},
  author       = {Bing Wang and Xinyuan Song and Qian Zhao},
  doi          = {10.1002/sim.70008},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70008},
  shortjournal = {Stat. Med.},
  title        = {Proportional mean residual life model with varying coefficients for right censored data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of the average causal effects
under dietary substitution strategies. <em>SIM</em>, <em>44</em>(5),
e70007. (<a href="https://doi.org/10.1002/sim.70007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2020–2025 Dietary Guidelines suggest that most people can improve their diet by making some changes to what they eat and drink. In many cases, these changes involve simple substitutions. For instance, the Dietary Guidelines recommend choosing chicken instead of processed red meat to reduce sodium intake and switching from refined grains to whole grains to increase dietary fiber intake. The question about such dietary substitution strategies seeks to estimate the average counterfactual outcome under a hypothetical intervention that replaces a food an individual would have consumed in the absence of intervention with a healthier substitute. In this work, we will show the conditions under which the average causal effects of substitution strategies can be non-parametrically identified, and provide efficient estimators for our proposed dietary substitution strategies. We evaluate the performance of our proposed methods via simulation studies and apply them to estimate the effect of substituting processed red meat with chicken on mortality, using data from the Nurses&#39; Health Study.},
  archive      = {J_SIM},
  author       = {Yu-Han Chiu and Lan Wen},
  doi          = {10.1002/sim.70007},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70007},
  shortjournal = {Stat. Med.},
  title        = {Identification and estimation of the average causal effects under dietary substitution strategies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating average treatment effects with support vector
machines. <em>SIM</em>, <em>44</em>(5), e70006. (<a
href="https://doi.org/10.1002/sim.70006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is one of the most popular classification algorithms in the machine learning literature. We demonstrate that SVM can be used to balance covariates and estimate average causal effects under the unconfoundedness assumption. Specifically, we adapt the SVM classifier as a kernel-based weighting procedure that minimizes the maximum mean discrepancy between the treatment and control groups while simultaneously maximizing effective sample size. We also show that SVM is a continuous relaxation of the quadratic integer program for computing the largest balanced subset, establishing its direct relation to the cardinality matching method. Another important feature of SVM is that the regularization parameter controls the trade-off between covariate balance and effective sample size. As a result, the existing SVM path algorithm can be used to compute the balance-sample size frontier. We characterize the bias of causal effect estimation arising from this trade-off, connecting the proposed SVM procedure to the existing kernel balancing methods. Finally, we conduct simulation and empirical studies to evaluate the performance of the proposed methodology and find that SVM is competitive with the state-of-the-art covariate balancing methods.},
  archive      = {J_SIM},
  author       = {Alexander Tarr and Kosuke Imai},
  doi          = {10.1002/sim.70006},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70006},
  shortjournal = {Stat. Med.},
  title        = {Estimating average treatment effects with support vector machines},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individualized time-varying nonparametric model with an
application in mobile health. <em>SIM</em>, <em>44</em>(5), e70005. (<a
href="https://doi.org/10.1002/sim.70005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized modeling has become increasingly popular in recent years with its growing application in fields such as personalized medicine and mobile health studies. With rich longitudinal measurements, it is of great interest to model certain subject-specific time-varying covariate effects. In this paper, we propose an individualized time-varying nonparametric model by leveraging the subgroup information from the population. The proposed method approximates the time-varying covariate effect using nonparametric B-splines and aggregates the estimated nonparametric coefficients that share common patterns. Moreover, the proposed method can effectively handle various missing data patterns that frequently arise in mobile health data. Specifically, our method achieves subgrouping by flexibly accommodating varying dimensions of B-spline coefficients due to missingness. This capability sets it apart from other fusion-type approaches for subgrouping. The subgroup information can also potentially provide meaningful insight into the characteristics of subjects and assist in recommending an effective treatment or intervention. An efficient ADMM algorithm is developed for implementation. Our numerical studies and application to mobile health data on monitoring pregnant women&#39;s deep sleep and physical activities demonstrate that the proposed method achieves better performance compared to other existing methods.},
  archive      = {J_SIM},
  author       = {Jenifer Rim and Qi Xu and Xiwei Tang and Yuqing Guo and Annie Qu},
  doi          = {10.1002/sim.70005},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70005},
  shortjournal = {Stat. Med.},
  title        = {Individualized time-varying nonparametric model with an application in mobile health},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive use of co-data through empirical bayes for bayesian
additive regression trees. <em>SIM</em>, <em>44</em>(5), e70004. (<a
href="https://doi.org/10.1002/sim.70004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For clinical prediction applications, we are often faced with small sample size data compared to the number of covariates. Such data pose problems for variable selection and prediction, especially when the covariate-response relationship is complicated. To address these challenges, we propose to incorporate external information on the covariates into Bayesian additive regression trees (BART), a sum-of-trees prediction model that utilizes priors on the tree parameters to prevent overfitting. To incorporate external information, an empirical Bayes (EB) framework is developed that estimates, assisted by a model, prior covariate weights in the BART model. The proposed EB framework enables the estimation of the other prior parameters of BART as well, rendering an appealing and computationally efficient alternative to cross-validation. We show that the method finds relevant covariates and that it improves prediction compared to default BART in simulations. If the covariate-response relationship is non-linear, the method benefits from the flexibility of BART to outperform regression-based learners. Finally, the benefit of incorporating external information is shown in an application to diffuse large B-cell lymphoma prognosis based on clinical covariates, gene mutations, DNA translocations, and DNA copy number data.},
  archive      = {J_SIM},
  author       = {Jeroen M. Goedhart and Thomas Klausch and Jurriaan Janssen and Mark A. van de Wiel},
  doi          = {10.1002/sim.70004},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70004},
  shortjournal = {Stat. Med.},
  title        = {Adaptive use of co-data through empirical bayes for bayesian additive regression trees},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework to assess complex heterogeneity in the
strength of a surrogate marker. <em>SIM</em>, <em>44</em>(5), e70001.
(<a href="https://doi.org/10.1002/sim.70001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A surrogate marker is a biological measurement in a clinical trial that aims to replace the primary outcome in evaluating the treatment effect, and can be measured earlier, with less cost, or with less patient burden. In theory, once a surrogate is validated, future studies can evaluate treatment efficacy using only the surrogate. While there are many methods to evaluate a surrogate, these methods rarely account for heterogeneity in surrogacy, that is, when a surrogate is valid for only certain people. We propose a general framework for the assessment of complex heterogeneity in the strength of a surrogate marker, as well as corresponding parametric and semiparametric estimation procedures. Our framework defines the proportion of the treatment effect on the primary outcome that is explained by the treatment effect on the surrogate, as a function of multiple baseline covariates, W $$ \mathbf{W} $$ . We additionally propose a formal test of heterogeneity and a method to identify a region of the covariate space where the surrogate is sufficiently strong. We examine the performance of our methods via a simulation study featuring varying levels of heterogeneity and use our methods to examine potential heterogeneity in the strength of a surrogate in an AIDS clinical trial.},
  archive      = {J_SIM},
  author       = {Rebecca Knowlton and Lu Tian and Layla Parast},
  doi          = {10.1002/sim.70001},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70001},
  shortjournal = {Stat. Med.},
  title        = {A general framework to assess complex heterogeneity in the strength of a surrogate marker},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to “DL 101: Basic introduction to deep learning
with its application in biomedical related fields.” <em>SIM</em>,
<em>44</em>(5), e10349. (<a
href="https://doi.org/10.1002/sim.10349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.10349},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10349},
  shortjournal = {Stat. Med.},
  title        = {Correction to “DL 101: Basic introduction to deep learning with its application in biomedical related fields”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying the estimands framework to non-inferiority trials:
Guidance on choice of hypothetical estimands for non-adherence and
comparison of estimation methods. <em>SIM</em>, <em>44</em>(5), e10348.
(<a href="https://doi.org/10.1002/sim.10348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common concern in non-inferiority (NI) trials is that non-adherence due, for example, to poor study conduct can make treatment arms artificially similar. Because intention-to-treat analyses can be anti-conservative in this situation, per-protocol analyses are sometimes recommended. However, such advice does not consider the estimands framework, nor the risk of bias from per-protocol analyses. We therefore sought to update the above guidance using the estimands framework, and compare estimators to improve on the performance of per-protocol analyses. We argue the main threat to validity of NI trials is the occurrence of “trial-specific” intercurrent events (IEs), that is, IEs which occur in a trial setting, but would not occur in practice. To guard against erroneous conclusions of non-inferiority, we suggest an estimand using a hypothetical strategy for trial-specific IEs should be employed, with handling of other non-trial-specific IEs chosen based on clinical considerations. We provide an overview of estimators that could be used to estimate a hypothetical estimand, including inverse probability weighting (IPW), and two instrumental variable approaches (one using an informative Bayesian prior on the effect of standard treatment, and one using a treatment-by-covariate interaction as an instrument). We compare them, using simulation in the setting of all-or-nothing compliance in two active treatment arms, and conclude both IPW and the instrumental variable method using a Bayesian prior are potentially useful approaches, with the choice between them depending on which assumptions are most plausible for a given trial.},
  archive      = {J_SIM},
  author       = {Katy E. Morgan and Ian R. White and Clémence Leyrat and Simon Stanworth and Brennan C. Kahan},
  doi          = {10.1002/sim.10348},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10348},
  shortjournal = {Stat. Med.},
  title        = {Applying the estimands framework to non-inferiority trials: Guidance on choice of hypothetical estimands for non-adherence and comparison of estimation methods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of cohort stepped wedge cluster-randomized trials
with nonignorable dropout via joint modeling. <em>SIM</em>,
<em>44</em>(5), e10347. (<a
href="https://doi.org/10.1002/sim.10347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster-randomized trial (CRTs) designs randomize clusters of individuals to intervention sequences, ensuring that every cluster eventually transitions from a control period to receive the intervention under study by the end of the study period. The analysis of stepped wedge CRTs is usually more complex than parallel-arm CRTs due to more complex intra-cluster correlation structures. A further challenge in the analysis of closed-cohort stepped wedge CRTs, which follow groups of individuals enrolled in each period longitudinally, is the occurrence of dropout. This is particularly problematic in studies of individuals at high risk for mortality, which causes nonignorable missing outcomes. If not appropriately addressed, missing outcomes from death will erode statistical power, at best, and bias treatment effect estimates, at worst. Joint longitudinal-survival models can accommodate informative dropout and missingness patterns in longitudinal studies. Specifically, within the joint longitudinal-survival modeling framework, one directly models the dropout process via a time-to-event submodel together with the longitudinal outcome of interest. The two submodels are then linked using a variety of possible association structures. This work extends linear mixed-effects models by jointly modeling the dropout process to accommodate informative missing outcome data in closed-cohort stepped wedge CRTs. We focus on constant intervention and general time-on-treatment effect parametrizations for the longitudinal submodel and study the performance of the proposed methodology using Monte Carlo simulation under several data-generating scenarios. We illustrate the joint modeling methodology in practice by reanalyzing data from the “Frail Older Adults: Care in Transition” (ACT) trial, a stepped wedge CRT of a multifaceted geriatric care model versus usual care in 35 primary care practices in the Netherlands.},
  archive      = {J_SIM},
  author       = {Alessandro Gasparini and Michael J. Crowther and Emiel O. Hoogendijk and Fan Li and Michael O. Harhay},
  doi          = {10.1002/sim.10347},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10347},
  shortjournal = {Stat. Med.},
  title        = {Analysis of cohort stepped wedge cluster-randomized trials with nonignorable dropout via joint modeling},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random survival forest for censored functional data.
<em>SIM</em>, <em>44</em>(5), e10344. (<a
href="https://doi.org/10.1002/sim.10344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a Random Survival Forest (RSF) method for functional data. The focus is specifically on defining a new functional data structure, the Censored Functional Data (CFD), for addressing the challenge of accurately modelling time-to-event data in the presence of censoring and irregular temporal structures. Traditional survival models struggle to incorporate complex functional patterns, making the proposed approach particularly valuable for improving prediction and interpretation. This approach allows for precise modelling of functional survival trajectories, leading to improved interpretation and prediction of survival dynamics across different groups. A medical survival study on the benchmark Sequential Organ Failure Assessment (SOFA) dataset and an extensive simulation study are presented. Results show good performance of the proposed approach, particularly in ranking the importance of predicting variables.},
  archive      = {J_SIM},
  author       = {Giuseppe Loffredo and Elvira Romano and Fabrizio Maturo},
  doi          = {10.1002/sim.10344},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10344},
  shortjournal = {Stat. Med.},
  title        = {Random survival forest for censored functional data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for the functional form of a continuous covariate in
the shared-parameter joint model. <em>SIM</em>, <em>44</em>(5), e10340.
(<a href="https://doi.org/10.1002/sim.10340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared-parameter joint modeling is a useful technique for properly associating longitudinal and time-to-event data. When the interest is in the survival outcome, the conditional logarithm of the hazard function for an event is conventionally presumed to be linearly related over time to a set of explanatory covariates, among other terms. However, this hypothesis is quite restrictive and may yield misleading results. Our objective here is to easily check such a modeling assumption for any continuous fixed covariate. For this purpose, we examine the appropriateness of a nonparametric test criterion based on a penalty-modified version of the Akaike information criterion. An extensive numerical study is conducted to check the validity of the test within the joint modeling framework, while determining the extent to which the function embedding the continuous covariate deviates from linearity. Furthermore, once a deviation from linearity is detected, the improvement in the model&#39;s predictive performance is examined. The usefulness of the testing procedure is illustrated using a clinical trial with HIV-infected subjects. Specifically, our example focuses on properly accounting for the effect of nadir CD4 cell count within a predictive joint model for the time to immune recovery.},
  archive      = {J_SIM},
  author       = {Xavier Piulachs and Anouar El Ghouch and Ingrid Van Keilegom},
  doi          = {10.1002/sim.10340},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10340},
  shortjournal = {Stat. Med.},
  title        = {Testing for the functional form of a continuous covariate in the shared-parameter joint model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). So many choices: A guide to selecting among methods to
adjust for observed confounders. <em>SIM</em>, <em>44</em>(5), e10336.
(<a href="https://doi.org/10.1002/sim.10336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-randomised studies (NRS) typically assume that there are no differences in unobserved baseline characteristics between the treatment groups under comparison. Traditionally regression models have been deployed to estimate treatment effects adjusting for observed confounders but can lead to biased estimates if the model is missspecified, by making incorrect functional form assumptions. A multitude of alternative methods have been developed which can reduce the risk of bias due to model misspecification. Investigators can now choose between many forms of matching, weighting, doubly robust, and machine learning methods. We review key concepts related to functional form assumptions and how those can contribute to bias from model misspecification. We then categorize the three frameworks for modeling treatment effects and the wide variety of estimation methods that can be applied to each framework. We consider why machine learning methods have been widely proposed for estimation and review the strengths and weaknesses of these approaches. We apply a range of these methods in re-analyzing a landmark case study. In the application, we examine how several widely used methods may be subject to bias from model misspecification. We conclude with a set of recommendations for practice.},
  archive      = {J_SIM},
  author       = {Luke Keele and Richard Grieve},
  doi          = {10.1002/sim.10336},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10336},
  shortjournal = {Stat. Med.},
  title        = {So many choices: A guide to selecting among methods to adjust for observed confounders},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScRecover: Discriminating true and false zeros in
single-cell RNA-seq data for imputation. <em>SIM</em>, <em>44</em>(5),
e10334. (<a href="https://doi.org/10.1002/sim.10334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput single-cell RNA-seq (scRNA-seq) data contains an excess of zero values, which can be contributed by unexpressed genes and detection signal dropouts. Existing imputation methods fail to distinguish between these two types of zeros. In this study, we introduce a statistical framework that effectively differentiates true zeros (lack of expression) from false zeros (dropouts). By focusing only on imputing the dropout zeros, we developed a new imputation tool, scRecover. Our approach utilizes a zero-inflated negative binomial framework to model the gene expression of each gene in each cell, enabling the estimation of zero-dropout probability. Additionally, we employ a modified version of the Good and Toulmin model to identify true zeros for each gene. To achieve imputation, scRecover is combined with other imputation methods such as scImpute, SAVER and MAGIC. Down-sampling experiments show that it recovers dropout zeros with higher accuracy and avoids over-imputing true zero values. Experiments conducted on real world data highlight the ability of scRecover to enhance downstream analysis and visualization.},
  archive      = {J_SIM},
  author       = {Zhun Miao and Xinyi Lin and Jiaqi Li and Joshua Ho and Qiuchen Meng and Xuegong Zhang},
  doi          = {10.1002/sim.10334},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10334},
  shortjournal = {Stat. Med.},
  title        = {ScRecover: Discriminating true and false zeros in single-cell RNA-seq data for imputation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal control of directional false discovery rates in
large-scale testing. <em>SIM</em>, <em>44</em>(5), e10329. (<a
href="https://doi.org/10.1002/sim.10329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-throughput biomedical technology enables measurement of thousands of gene expression levels contemporaneously. A major task in analyzing these gene expression data is to identify both over-expressed and under-expressed genes. The popular two-group models select the non-null genes without further classifying them as overexpression or underexpression. Consequently, two-group decision rules are unable to constrain the numbers of falsely discovered over-expressed or under-expressed genes respectively. We propose a general three-group model that allows dependence between the test statistics and develop a decision rule that separately controls the two types of false discoveries. We show that the optimal decision rule in our three-group model has a special monotonic structure. By making use of this monotonic structure, we can linearize the two-directional false discovery rate constraints. We prove that our decision rule optimizes the expected number of true discoveries while controlling the proportions of falsely discovered over-expressed and under-expressed genes at desired levels simultaneously. The data-driven versions of the proposed procedures are suggested, and their consistency is established. Comparisons with state-of-the-art approaches and applications to genomic studies show that our procedures work well.},
  archive      = {J_SIM},
  author       = {Guozhu Tang and Yicheng Kang and Dongdong Xiang},
  doi          = {10.1002/sim.10329},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10329},
  shortjournal = {Stat. Med.},
  title        = {Optimal control of directional false discovery rates in large-scale testing},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-dependent ROC curve for multiple longitudinal
biomarkers and its application in diagnosing cardiovascular events.
<em>SIM</em>, <em>44</em>(5), e10318. (<a
href="https://doi.org/10.1002/sim.10318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since they can help people detect the early signs of diseases, accurate diagnostic techniques based on biomarkers are crucial in biomedical research. This article proposes a novel bivariate time-varying coefficients logistic regression model for addressing the combined longitudinal biomarkers. Using the B-splines method to estimate the proposed model, we can effectively combine multiple longitudinal biomarkers and improve diagnostic accuracy. We show that the proposed method is theoretically consistent. And it exhibits superior performance compared to the existing method, as presented through numerical results. The proposed method is verified in a study on predicting the probability of onset of future cardiovascular events for type 2 diabetic patients. The longitudinal biomarkers, HbA1c and LDL-C, are considered in this study. We demonstrate that the combined longitudinal biomarkers significantly improved disease diagnostic accuracy over only a combination of the latest measured biomarkers in most cases.},
  archive      = {J_SIM},
  author       = {Lizhe Sun and Pingyuan Wei and Jie Zhou and Xiao-Hua Zhou},
  doi          = {10.1002/sim.10318},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10318},
  shortjournal = {Stat. Med.},
  title        = {Time-dependent ROC curve for multiple longitudinal biomarkers and its application in diagnosing cardiovascular events},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal mediation analysis: A summary-data mendelian
randomization approach. <em>SIM</em>, <em>44</em>(5), e10317. (<a
href="https://doi.org/10.1002/sim.10317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Summary-data Mendelian randomization (MR), a widely used approach in causal inference, has recently attracted attention for improving causal mediation analysis. Two existing methods corresponding to the difference method and product method of linear mediation analysis have been developed to perform MR-based mediation analysis using the inverse-variance weighted method (MR-IVW). Despite these developments, there is still a need for more rigorous, efficient, and precise MR-based mediation methodologies. In this study, we develop summary-data MR-based frameworks for causal mediation analysis. We improve the accuracy, statistical efficiency and robustness of the existing MR-based mediation analysis by implementing novel variance estimators for the mediation effects, deriving rigorous procedures for statistical inference, and accounting for widespread pleiotropic effects. Specifically, we propose Diff-IVW and Prod-IVW to improve upon the existing methods and provide the pleiotropy-robust methods (Diff-Egger, Diff-Median, Prod-Egger, and Prod-Median), adapted from MR-Egger and MR-Median, to enhance the robustness of the MR-based mediation analysis. We conduct comprehensive simulation studies to compare the existing and proposed methods. The results show that the proposed methods, Diff-IVW and Prod-IVW, improve statistical efficiency and type I error control over the existing approaches. Although all IVW-based methods suffer from directional pleiotropy biases, the median-based methods (Diff-Median and Prod-Median) can mitigate such biases. The differences among the methods can lead to discrepant statistical conclusions as demonstrated in real data applications. Based on our simulation results, we recommend the three proposed methods in practice: Diff-IVW, Prod-IVW, and Prod-Median, which are complementary under various scenarios.},
  archive      = {J_SIM},
  author       = {Shu-Chin Lin and Sheng-Hsuan Lin and Tian Ge and Chia-Yen Chen and Yen-Feng Lin},
  doi          = {10.1002/sim.10317},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10317},
  shortjournal = {Stat. Med.},
  title        = {Causal mediation analysis: A summary-data mendelian randomization approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for association studies in the
presence of binary outcome misclassification. <em>SIM</em>,
<em>44</em>(5), e10316. (<a
href="https://doi.org/10.1002/sim.10316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical and public health association studies, binary outcome variables may be subject to misclassification, resulting in substantial bias in effect estimates. The feasibility of addressing binary outcome misclassification in regression models is often hindered by model identifiability issues. In this paper, we characterize the identifiability problems in this class of models as a specific case of “label-switching” and leverage a pattern in the resulting parameter estimates to solve the permutation invariance of the complete data log-likelihood. Our proposed algorithm in binary outcome misclassification models does not require gold standard labels and relies only on the assumption that the sum of the sensitivity and specificity exceeds 1. A label-switching correction is applied within estimation methods to recover unbiased effect estimates and to estimate misclassification rates. Open-source software is provided to implement the proposed methods. We give a detailed simulation study for our proposed methodology and apply these methods to data from the 2020 Medical Expenditure Panel Survey (MEPS).},
  archive      = {J_SIM},
  author       = {Kimberly A. Hochstedler Webb and Martin T. Wells},
  doi          = {10.1002/sim.10316},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10316},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for association studies in the presence of binary outcome misclassification},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dir-GLM: A bayesian GLM with data-driven reference
distribution. <em>SIM</em>, <em>44</em>(5), e10305. (<a
href="https://doi.org/10.1002/sim.10305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently developed semi-parametric generalized linear model (SPGLM) offers more flexibility as compared to the classical GLM by including the baseline or reference distribution of the response as an additional parameter in the model. However, some inference summaries are not easily generated under existing maximum-likelihood-based inference (GLDRM). This includes uncertainty in estimation for model-derived functionals such as exceedance probabilities. The latter are critical in a clinical diagnostic or decision-making setting. In this article, by placing a Dirichlet prior on the baseline distribution, we propose a Bayesian model-based approach for inference to address these important gaps. We establish consistency and asymptotic normality results for the implied canonical parameter. Simulation studies and an illustration with data from an aging research study confirm that the proposed method performs comparably or better in comparison with GLDRM. The proposed Bayesian framework is most attractive for inference with small sample training data or in sparse-data scenarios.},
  archive      = {J_SIM},
  author       = {Entejar Alam and Peter Müller and Paul J. Rathouz},
  doi          = {10.1002/sim.10305},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10305},
  shortjournal = {Stat. Med.},
  title        = {Dir-GLM: A bayesian GLM with data-driven reference distribution},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian hierarchical penalized spline models for immediate
and time-varying intervention effects in stepped wedge cluster
randomized trials. <em>SIM</em>, <em>44</em>(5), e10304. (<a
href="https://doi.org/10.1002/sim.10304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized trials (SWCRTs) often face challenges related to potential confounding by time. Traditional frequentist methods may not provide adequate coverage of an intervention&#39;s true effect using confidence intervals, whereas Bayesian approaches show potential for better coverage of intervention effects. However, Bayesian methods remain underexplored in the context of SWCRTs. To bridge this gap, we propose two innovative Bayesian hierarchical penalized spline models. Our first model accommodates large numbers of clusters and time periods, focusing on immediate intervention effects. To evaluate this approach, we compared this model to traditional frequentist methods. We then extend our approach to account for time-varying intervention effects, conducting a comprehensive comparison with an existing Bayesian monotone effect curve model and alternative frequentist methods. The proposed models were applied in the Primary Palliative Care for Emergency Medicine stepped wedge trial to evaluate the effectiveness of the intervention. Through extensive simulations and real-world application, we demonstrate the robustness of our proposed Bayesian models. Notably, the Bayesian immediate effect model consistently achieves the nominal coverage probability, providing more reliable interval estimations while maintaining high estimation accuracy. Furthermore, our proposed Bayesian time-varying effect model represents a significant advancement over the existing Bayesian monotone effect curve model, offering improved accuracy and reliability in estimation while also achieving higher coverage probability than alternative frequentist methods. To the best of our knowledge, this marks the first development of Bayesian hierarchical spline modeling for SWCRTs. Our proposed models offer promising tools for researchers and practitioners, enabling more precise evaluation of intervention impacts.},
  archive      = {J_SIM},
  author       = {Danni Wu and Hyung G. Park and Corita R. Grudzen and Keith S. Goldfeld},
  doi          = {10.1002/sim.10304},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10304},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical penalized spline models for immediate and time-varying intervention effects in stepped wedge cluster randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian modeling of cancer outcomes using genetic variables
assisted by pathological imaging data. <em>SIM</em>, <em>44</em>(3-4),
e10350. (<a href="https://doi.org/10.1002/sim.10350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing maturity of genetic profiling, an essential and routine task in cancer research is to model disease outcomes/phenotypes using genetic variables. Many methods have been successfully developed. However, oftentimes, empirical performance is unsatisfactory because of a “lack of information.” In cancer research and clinical practice, a source of information that is broadly available and highly cost-effective comes from pathological images, which are routinely collected for definitive diagnosis and staging. In this article, we consider a Bayesian approach for selecting relevant genetic variables and modeling their relationships with a cancer outcome/phenotype. We propose borrowing information from (manually curated, low-dimensional) pathological imaging features via reinforcing the same selection results for the cancer outcome and imaging features. We further develop a weighting strategy to accommodate the scenario where information borrowing may not be equally effective for all subjects. Computation is carefully examined. Simulations demonstrate competitive performance of the proposed approach. We analyze TCGA (The Cancer Genome Atlas) LUAD (lung adenocarcinoma) data, with overall survival and gene expressions being the outcome and genetic variables, respectively. Findings different from the alternatives and with sound properties are made.},
  archive      = {J_SIM},
  author       = {Yunju Im and Rong Li and Shuangge Ma},
  doi          = {10.1002/sim.10350},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10350},
  shortjournal = {Stat. Med.},
  title        = {Bayesian modeling of cancer outcomes using genetic variables assisted by pathological imaging data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing one primary and two secondary endpoints in a
two-stage group sequential trial with extensions. <em>SIM</em>,
<em>44</em>(3-4), e10346. (<a
href="https://doi.org/10.1002/sim.10346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of testing multiple secondary endpoints conditional on a primary endpoint being significant in a two-stage group sequential procedure, focusing on two secondary endpoints. This extends our previous work with one secondary endpoint. The test for the secondary null hypotheses is a closed procedure. Application of the Bonferroni test for testing the intersection of the secondary hypotheses results in the Holm procedure while application of the Simes test results in the Hochberg procedure. The focus of the present paper is on developing normal theory analogs of the abovementioned p $$ p $$ -value based tests that take into account (i) the gatekeeping effect of the test on the primary endpoint and (ii) correlations between the endpoints. The normal theory boundaries are determined by finding the least favorable configuration of the correlations and so their knowledge is not needed to apply these procedures. The p $$ p $$ -value based procedures are easy to apply but they are less powerful than their normal theory analogs because they do not take into account the correlations between the endpoints and the gatekeeping effect referred to above. On the other hand, the normal theory procedures are restricted to two secondary endpoints and two stages mainly because of computational difficulties with more than two secondary endpoints and stages. Comparisons between the two types of procedures are given in terms of secondary powers. The sensitivity of the secondary type I error rate and power to unequal information times is studied. Numerical examples and a real case study illustrate the procedures.},
  archive      = {J_SIM},
  author       = {Ajit C. Tamhane and Dong Xi and Cyrus R. Mehta and Alexander Romanenko and Jiangtao Gou},
  doi          = {10.1002/sim.10346},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10346},
  shortjournal = {Stat. Med.},
  title        = {Testing one primary and two secondary endpoints in a two-stage group sequential trial with extensions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Allocation predictability of individual assignments in
restricted randomization designs for two-arm equal allocation trials.
<em>SIM</em>, <em>44</em>(3-4), e10343. (<a
href="https://doi.org/10.1002/sim.10343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript derives the allocation predictability measured by the correct guess probability and the probability of being deterministic for individual treatment assignments, as well as the averages of a randomization sequence, based on the treatment imbalance transition matrix and the conditional allocation probability. The methods described are applicable to restricted randomization designs that satisfy the following criteria: (1) two-arm equal allocation, (2) restriction of maximum tolerated imbalance, and (3) conditional allocation probability fully determined by the observed current treatment imbalance. Analytical results indicate that, for two-arm equal allocation trials, allocation predictability alternates by the odd/even sequence order of the treatment assignment. Additionally, the sequence average allocation predictability converges to its asymptotic value significantly more slowly than the allocation predictability for individual assignment does. Consequently, comparisons of allocation predictability between different randomization designs based on sequence averages are sensitive to sequence length. Using sequence average allocation predictability may underestimate the risk of selection bias for individual assignment. This discrepancy is particularly pronounced for short sequence lengths, where individual assignment predictability can be substantially higher than the sequence average.},
  archive      = {J_SIM},
  author       = {Wenle Zhao and Sherry Livingston},
  doi          = {10.1002/sim.10343},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10343},
  shortjournal = {Stat. Med.},
  title        = {Allocation predictability of individual assignments in restricted randomization designs for two-arm equal allocation trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matching-assisted power prior for incorporating real-world
data in randomized clinical trial analysis. <em>SIM</em>,
<em>44</em>(3-4), e10342. (<a
href="https://doi.org/10.1002/sim.10342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging external data information to supplement randomized clinical trials has been a popular topic in recent years, especially for medical device and drug discovery. In rare diseases, it is very challenging to recruit patients and run a large-scale randomized trial. To take advantage of real-world data from historical trials on the same disease, we can run a small hybrid trial and borrow historical controls to increase the power. But the borrowing needs to be conducted in a statistically principled manner. Bayesian power prior methods and propensity score adjustments have been discussed in the literature. In this paper, we propose a matching-assisted power prior approach to better mitigate observed bias when incorporating external data. A subset of comparable external subjects is selected by groups through template matching, and different weights are assigned to these groups based on their similarity to the current study population. Power priors are then implemented to incorporate the information into Bayesian inference. Unlike conventional power prior methods, which discount all control patients similarly, matching pre-selects good controls, hence improved the quality of external data being borrowed. We compare its performance with the existing propensity score-integrated power prior approach through simulation studies and illustrate the implementation using data from a real acupuncture clinical trial.},
  archive      = {J_SIM},
  author       = {Ruoyuan Qian and Biqing Yang and Xinyi Xu and Bo Lu},
  doi          = {10.1002/sim.10342},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10342},
  shortjournal = {Stat. Med.},
  title        = {Matching-assisted power prior for incorporating real-world data in randomized clinical trial analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reverse time-to-death as time-scale in time-to-event
analysis for studies of advanced illness and palliative care.
<em>SIM</em>, <em>44</em>(3-4), e10338. (<a
href="https://doi.org/10.1002/sim.10338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incidence of adverse outcome events rises as patients with advanced illness approach end-of-life. Exposures that tend to occur near end-of-life, for example, use of wheelchair, oxygen therapy and palliative care, may therefore be found associated with the incidence of the adverse outcomes. We propose a concept of reverse time-to-death (rTTD) and its use for the time-scale in time-to-event analysis based on partial likelihood to mitigate the time-varying confounding. We used data on community-based palliative care uptake (exposure) and emergency department visits (outcome) among patients with advanced cancer in Singapore to illustrate. We compare the results against that of the common practice of using time-on-study (TOS) as time-scale. Graphical analysis demonstrated that cancer patients receiving palliative care had higher rate of emergency department visits than non-recipients mainly because they were closer to end-of-life, and that rTTD analysis made comparison between patients at the same time-to-death. In analysis of a decedent cohort, emergency department visits in relation to palliative care using TOS time-scale showed significant increase in hazard ratio estimate when observed time-varying covariates were omitted from statistical adjustment (% change-in-estimate = 16.2%; 95% CI 6.4% to 25.6%). There was no such change in otherwise the same analysis using rTTD (% change-in-estimate = 3.1%; 95% CI -1.0% to 8.5%), demonstrating the ability of rTTD time-scale to mitigate confounding that intensifies in relation to time-to-death. A similar pattern was found in the full cohort. Simulations demonstrated that the proposed method had smaller relative bias and root mean square error than TOS-based analysis. In conclusion, use of rTTD as time-scale in time-to-event analysis provides a simple and robust approach to control time-varying confounding in studies of advanced illness, even if the confounders are unmeasured.},
  archive      = {J_SIM},
  author       = {Yin Bun Cheung and Xiangmei Ma and Isha Chaudhry and Nan Liu and Qingyuan Zhuang and Grace Meijuan Yang and Chetna Malhotra and Eric Andrew Finkelstein},
  doi          = {10.1002/sim.10338},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10338},
  shortjournal = {Stat. Med.},
  title        = {Reverse time-to-death as time-scale in time-to-event analysis for studies of advanced illness and palliative care},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On inclusion of covariates in model based dose finding
clinical trial designs. <em>SIM</em>, <em>44</em>(3-4), e10337. (<a
href="https://doi.org/10.1002/sim.10337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing number of Phase I dose-finding studies that use a model-based approach, such as the CRM or the EWOC method to estimate the dose-toxicity relationship. It is common to assume that all patients will have similar toxicity risk given the dose regardless of patients&#39; individual characteristics. In many trials, however, some patients&#39; covariates (e.g., a concomitant drug assigned by a clinician) might have an impact on the dose-toxicity relationship. In this work, motivated by a real trial, we evaluate an impact of taking into account (or omitting) some patients&#39; covariates on the individual target dose recommendations and patients&#39; safety in Phase I model-based dose-finding study. We investigate several variable penalisation criteria and found that, for continuous and binary covariates, omitting a prognostic covariate leads to a drastically low proportion of correct selections and an increase of overdosing. At the same time, including a covariate can lead to good operating characteristics in all scenarios but can sometimes slightly decrease the proportion of good selections and increase the overdosing. To tackle this, we propose to use a Bayesian Lasso Bayesian Logistic Regression Model (BLRM) and Spike-and-Slab BLRM. We have found that the BLRM coupled to the Bayesian LASSO and the BLRM with Spike-and-Slab are on average better appropriate to consider variable inclusion.},
  archive      = {J_SIM},
  author       = {Adrien Ollier and Pavel Mozgunov},
  doi          = {10.1002/sim.10337},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10337},
  shortjournal = {Stat. Med.},
  title        = {On inclusion of covariates in model based dose finding clinical trial designs},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating meta-learners to analyze treatment heterogeneity
in survival data: Application to electronic health records of pediatric
asthma care in COVID-19 pandemic. <em>SIM</em>, <em>44</em>(3-4),
e10333. (<a href="https://doi.org/10.1002/sim.10333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important aspect of precision medicine focuses on characterizing diverse responses to treatment due to unique patient characteristics, also known as heterogeneous treatment effects (HTE) or individualized treatment effects (ITE), and identifying beneficial subgroups with enhanced treatment effects. Estimating HTE with right-censored data in observational studies remains challenging. In this paper, we propose a pseudo-ITE-based framework for analyzing HTE in survival data, which includes a group of meta-learners for estimating HTE, a variable importance metric for identifying predictive variables to HTE, and a data-adaptive procedure to select subgroups with enhanced treatment effects. We evaluate the finite sample performance of the framework under various observational study settings. Furthermore, we applied the proposed methods to analyze the treatment heterogeneity of a written asthma action plan (WAAP) on time-to-ED (Emergency Department) return due to asthma exacerbation using a large asthma electronic health records dataset with visit records expanded from pre- to post-COVID-19 pandemic. We identified vulnerable subgroups of patients with poorer asthma outcomes but enhanced benefits from WAAP and characterized patient profiles. Our research provides valuable insights for healthcare providers on the strategic distribution of WAAP, particularly during disruptive public health crises, ultimately improving the management and control of pediatric asthma.},
  archive      = {J_SIM},
  author       = {Na Bo and Jong-Hyeon Jeong and Erick Forno and Ying Ding},
  doi          = {10.1002/sim.10333},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10333},
  shortjournal = {Stat. Med.},
  title        = {Evaluating meta-learners to analyze treatment heterogeneity in survival data: Application to electronic health records of pediatric asthma care in COVID-19 pandemic},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-parametric estimation for semi-competing risks data with
event misascertainment. <em>SIM</em>, <em>44</em>(3-4), e10332. (<a
href="https://doi.org/10.1002/sim.10332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semi-competing risks data model is a special type of disease-state model that focuses on studying the association between an intermediate event and a terminal event and proves to be a useful tool in modeling disease progression. The study of the semi-competing risk data model not only allows us to evaluate whether a disease episode is related to death but also provides a toolkit to predict death, given that the episode occurred at a certain time. However, the computation of the semi-competing risk models is a numerically challenging task. The Gamma-Frailty conditional Markov model has been shown to be an efficient computation model for studying semi-competing risks data. Building on recent advances in studying semi-competing risks data, this work proposes a non-parametric pseudo-likelihood method equipped with an EM-like algorithm to study semi-competing risks data with event misascertainment under the restricted Gamma-Frailty conditional Markov model. A thorough simulation study is conducted to demonstrate the inference validity of the proposed method and its numerical stability. The proposed method is applied to a large HIV cohort study, EA-IeDEA, that has a severe death under-reporting issue to assess the degree of adverse impact of the interruption of ART care on HIV mortality.},
  archive      = {J_SIM},
  author       = {Ruiqian Wu and Ying Zhang and Giorgos Bakoyannis},
  doi          = {10.1002/sim.10332},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10332},
  shortjournal = {Stat. Med.},
  title        = {Non-parametric estimation for semi-competing risks data with event misascertainment},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-label classification with
gene-environment interactions in disease modeling. <em>SIM</em>,
<em>44</em>(3-4), e10330. (<a
href="https://doi.org/10.1002/sim.10330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, gene-environment (G-E) interactions have been demonstrated to have important implications for analyzing disease outcomes beyond the main G and main E effects. Many approaches have been developed for G-E interaction analysis, yielding important findings. However, hierarchical multi-label classification, which provides insightful information on disease outcomes, remains unexplored in G-E analysis literature. Moreover, unlabeled data are commonly observed in practical settings but omitted by many existing methods of hierarchical multi-label classification. In this study, we consider a semi-supervised scenario and develop a novel approach for the two-layer hierarchical response with G-E interactions. A two-step penalized estimation is then proposed using an efficient expectation-maximization (EM) algorithm. Simulation shows that it has superior performance in classification and feature selection. The analysis of The Cancer Genome Atlas (TCGA) data on lung cancer demonstrates the practical utility of the proposed method. Overall, this study can fill the important knowledge gap in G-E interaction analysis by providing a widely applicable framework for hierarchical multi-label classification of complex disease outcomes.},
  archive      = {J_SIM},
  author       = {Jingmao Li and Qingzhao Zhang and Shuangge Ma and Kuangnan Fang and Yaqing Xu},
  doi          = {10.1002/sim.10330},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10330},
  shortjournal = {Stat. Med.},
  title        = {Hierarchical multi-label classification with gene-environment interactions in disease modeling},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample size adjustment in sequential multiple assignment
randomized trials. <em>SIM</em>, <em>44</em>(3-4), e10328. (<a
href="https://doi.org/10.1002/sim.10328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are often designed based on limited information about effect sizes and precision parameters with risks of underpowered studies. This is more problematic for SMARTs where strategy effects are based on sequences of treatments. Sample size adjustment offers flexibility through re-estimating sample size during the trial to ensure adequate power at the final analysis. While this adaptation is common for standard clinical trials, corresponding methods to perform sample size adjustment have not been adapted to SMARTs. In this paper, we propose a sample size adjustment procedure for SMARTs. Sample sizes are re-calculated at the interim analysis based on the conditional power derived from a bivariate non-central chi-square distribution. We demonstrate through simulation studies that even with an underpowered initial sample size due to miss-specified parameters at the design stage, the proposed method can maintain desirable power at the end of the study, and additional resources are only invested in trials that show promising conditional power at the interim analysis.},
  archive      = {J_SIM},
  author       = {Liwen Wu and Junyao Wang and Abdus S. Wahed},
  doi          = {10.1002/sim.10328},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10328},
  shortjournal = {Stat. Med.},
  title        = {Sample size adjustment in sequential multiple assignment randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric path-specific effects on a survival outcome
through multiple time-to-event mediators. <em>SIM</em>,
<em>44</em>(3-4), e10327. (<a
href="https://doi.org/10.1002/sim.10327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A causal mediation model with multiple time-to-event mediators is exemplified by the natural course of human disease marked by sequential milestones with a time-to-event nature. For example, from hepatitis B infection to death, patients may experience intermediate events such as liver cirrhosis and liver cancer. The sequential events of hepatitis, cirrhosis, cancer, and death are susceptible to right censoring; moreover, the latter events may preclude the former events. Casting the natural course of human diseases in the framework of causal mediation modeling, we establish a model with intermediate and terminal events as the mediators and outcomes, respectively. We define the interventional analog of path-specific effects (iPSEs) as the effect of an exposure on a terminal event mediated (or not mediated) by any combination of intermediate events without parametric models. The expression of a counting process-based counterfactual hazard is derived under the sequential ignorability assumption. We employ composite nonparametric likelihood estimation to obtain maximum likelihood estimators for the counterfactual hazard and iPSEs. Our proposed estimators achieve asymptotic unbiasedness, uniform consistency, and weak convergence. Applying the proposed method, we show that hepatitis B induced mortality is mostly mediated through liver cancer and/or cirrhosis whereas hepatitis C induced mortality may be through extrahepatic diseases.},
  archive      = {J_SIM},
  author       = {Yen-Tsung Huang and Ju-Sheng Hong},
  doi          = {10.1002/sim.10327},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10327},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric path-specific effects on a survival outcome through multiple time-to-event mediators},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Between- and within-cluster spearman rank correlations.
<em>SIM</em>, <em>44</em>(3-4), e10326. (<a
href="https://doi.org/10.1002/sim.10326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered data are common in practice. Clustering arises when subjects are measured repeatedly, or subjects are nested in groups (e.g., households, schools). It is often of interest to evaluate the correlation between two variables with clustered data. There are three commonly used Pearson correlation coefficients (total, between-, and within-cluster), which together provide an enriched perspective of the correlation. However, these Pearson correlation coefficients are sensitive to extreme values and skewed distributions. They also vary with data transformation, which is arbitrary and often difficult to choose, and they are not applicable to ordered categorical data. Current nonparametric correlation measures for clustered data are only for the total correlation. Here we define population parameters for the between- and within-cluster Spearman rank correlations. The definitions are natural extensions of the Pearson between- and within-cluster correlations to the rank scale. We show that the total Spearman rank correlation approximates a linear combination of the between- and within-cluster Spearman rank correlations, where the weights are functions of rank intraclass correlations of the two random variables. We also discuss the equivalence between the within-cluster Spearman rank correlation and the covariate-adjusted partial Spearman rank correlation. Furthermore, we describe estimation and inference for the three Spearman rank correlations, conduct simulations to evaluate the performance of our estimators, and illustrate their use with data from a longitudinal biomarker study and a clustered randomized trial.},
  archive      = {J_SIM},
  author       = {Shengxin Tu and Chun Li and Bryan E. Shepherd},
  doi          = {10.1002/sim.10326},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10326},
  shortjournal = {Stat. Med.},
  title        = {Between- and within-cluster spearman rank correlations},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian multivariate model with temporal dependence on
random partition of areal data for mosquito-borne diseases.
<em>SIM</em>, <em>44</em>(3-4), e10325. (<a
href="https://doi.org/10.1002/sim.10325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More than half of the world&#39;s population is exposed to mosquito-borne diseases, leading to millions of cases and hundreds of thousands of deaths every year. Analyzing this type of data is complex and poses several interesting challenges, mainly due to the usually vast geographic area involved, the peculiar temporal behavior, and the potential correlation between infections. Motivation for this work stems from the analysis of tropical disease data, namely, the number of cases of dengue and chikungunya, for the 145 microregions in Southeast Brazil from 2018 to 2022. As a contribution to the literature on multivariate disease data, we develop a flexible Bayesian multivariate spatio-temporal model where temporal dependence is defined for areal clusters. The model features a prior distribution for the random partition of areal data that incorporates neighboring information. It also incorporates an autoregressive structure and terms related to seasonal patterns into temporal components that are disease- and cluster-specific. Furthermore, it considers a multivariate directed acyclic graph autoregressive structure to accommodate spatial and inter-disease dependence. We explore the properties of the model through simulation studies and show results that prove our proposal compares well to competing alternatives. Finally, we apply the model to the motivating dataset with a twofold goal: finding clusters of areas with similar temporal trends for some of the diseases and exploring the existence of correlation between two diseases transmitted by the same mosquito.},
  archive      = {J_SIM},
  author       = {Jessica Pavani and Fernando Andrés Quintana},
  doi          = {10.1002/sim.10325},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10325},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multivariate model with temporal dependence on random partition of areal data for mosquito-borne diseases},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised functional principal component analysis under the
mixture cure rate model: An application to alzheimer’s disease.
<em>SIM</em>, <em>44</em>(3-4), e10324. (<a
href="https://doi.org/10.1002/sim.10324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain imaging data is one of the primary predictors for assessing the risk of Alzheimer&#39;s disease (AD). This study aims to extract image-based features associated with the possibly right-censored time-to-event outcomes and to improve predictive performance. While the functional proportional hazards model is well-studied in the literature, these studies often do not consider the existence of patients who have a very low risk and are approximately insusceptible to AD. We introduce a functional mixture cure rate model that extends the proportional hazards model by allowing a proportion of event-free patients. We propose a novel supervised functional principal component analysis (sFPCA) method to extract image features associated with AD risk while accounting for the complexity arising from right censoring. The proposed method accommodates the irregular boundary issue inherent in brain images with bivariate splines over triangulations. We demonstrate the advantages of the proposed method through extensive simulation studies and provide an application to the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) study.},
  archive      = {J_SIM},
  author       = {Jiahui Feng and Haolun Shi and Da Ma and Mirza Faisal Beg and Jiguo Cao},
  doi          = {10.1002/sim.10324},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10324},
  shortjournal = {Stat. Med.},
  title        = {Supervised functional principal component analysis under the mixture cure rate model: An application to Alzheimer&#39;S disease},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusting for ascertainment bias in meta-analysis of
penetrance for cancer risk. <em>SIM</em>, <em>44</em>(3-4), e10323. (<a
href="https://doi.org/10.1002/sim.10323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-gene panel testing allows efficient detection of pathogenic variants in cancer susceptibility genes including moderate-risk genes such as ATM and PALB2. A growing number of studies examine the risk of breast cancer (BC) conferred by pathogenic variants of these genes. A meta-analysis combining the reported risk estimates can provide an overall estimate of age-specific risk of developing BC, that is, penetrance for a gene. However, estimates reported by case–control studies often suffer from ascertainment bias. Currently, there is no method available to adjust for such bias in this setting. We consider a Bayesian random effect meta-analysis method that can synthesize different types of risk measures and extend it to incorporate studies with ascertainment bias. This is achieved by introducing a bias term in the model and assigning appropriate priors. We validate the method through a simulation study and apply it to estimate BC penetrance for carriers of pathogenic variants in the ATM and PALB2 genes. Our simulations show that the proposed method results in more accurate and precise penetrance estimates compared to when no adjustment is made for ascertainment bias or when such biased studies are discarded from the analysis. The overall estimated BC risk for individuals with pathogenic variants are (1) 5.77% (3.22%–9.67%) by age 50 and 26.13% (20.31%–32.94%) by age 80 for ATM; (2) 12.99% (6.48%–22.23%) by age 50, and 44.69% (34.40%–55.80%) by age 80 for PALB2. The proposed method allows meta-analyses to include studies with ascertainment bias, resulting in inclusion of more studies and thereby more accurate estimates.},
  archive      = {J_SIM},
  author       = {Thanthirige Lakshika M. Ruberu and Danielle Braun and Giovanni Parmigiani and Swati Biswas},
  doi          = {10.1002/sim.10323},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10323},
  shortjournal = {Stat. Med.},
  title        = {Adjusting for ascertainment bias in meta-analysis of penetrance for cancer risk},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian joint model of multiple nonlinear longitudinal
and competing risks outcomes for dynamic prediction in multiple myeloma:
Joint estimation and corrected two-stage approaches. <em>SIM</em>,
<em>44</em>(3-4), e10322. (<a
href="https://doi.org/10.1002/sim.10322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting cancer-associated clinical events is challenging in oncology. In Multiple Myeloma (MM), a cancer of plasma cells, disease progression is determined by changes in biomarkers, such as serum concentration of the paraprotein secreted by plasma cells (M-protein). Therefore, the time-dependent behavior of M-protein and the transition across lines of therapy (LoT), which may be a consequence of disease progression, should be accounted for in statistical models to predict relevant clinical outcomes. Furthermore, it is important to understand the contribution of the patterns of longitudinal biomarkers, upon each LoT initiation, to time-to-death or time-to-next-LoT. Motivated by these challenges, we propose a Bayesian joint model for trajectories of multiple longitudinal biomarkers, such as M-protein, and the competing risks of death and transition to the next LoT. Additionally, we explore two estimation approaches for our joint model: simultaneous estimation of all parameters (joint estimation) and sequential estimation of parameters using a corrected two-stage strategy aiming to reduce computational time. Our proposed model and estimation methods are applied to a retrospective cohort study from a real-world database of patients diagnosed with MM in the US from January 2015 to February 2022. We split the data into training and test sets in order to validate the joint model using both estimation approaches and make dynamic predictions of times until clinical events of interest, informed by longitudinally measured biomarkers and baseline variables available up to the time of prediction.},
  archive      = {J_SIM},
  author       = {Danilo Alvares and Jessica K. Barrett and François Mercier and Spyros Roumpanis and Sean Yiu and Felipe Castro and Jochen Schulze and Yajing Zhu},
  doi          = {10.1002/sim.10322},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10322},
  shortjournal = {Stat. Med.},
  title        = {A bayesian joint model of multiple nonlinear longitudinal and competing risks outcomes for dynamic prediction in multiple myeloma: Joint estimation and corrected two-stage approaches},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bioequivalence design with sampling distribution segments.
<em>SIM</em>, <em>44</em>(3-4), e10321. (<a
href="https://doi.org/10.1002/sim.10321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In bioequivalence design, power analyses dictate how much data must be collected to detect the absence of clinically important effects. Power is computed as a tail probability in the sampling distribution of the pertinent test statistics. When these test statistics cannot be constructed from pivotal quantities, their sampling distributions are approximated via repetitive, time-intensive computer simulation. We propose a novel simulation-based method to quickly approximate the power curve for many such bioequivalence tests by efficiently exploring segments (as opposed to the entirety) of the relevant sampling distributions. Despite not estimating the entire sampling distribution, this approach prompts unbiased sample size recommendations. We illustrate this method using two-group bioequivalence tests with unequal variances and overview its broader applicability in clinical design. All methods proposed in this work can be implemented using the developed dent package in R.},
  archive      = {J_SIM},
  author       = {Luke Hagar and Nathaniel T. Stevens},
  doi          = {10.1002/sim.10321},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10321},
  shortjournal = {Stat. Med.},
  title        = {Bioequivalence design with sampling distribution segments},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The harms of class imbalance corrections for machine
learning based prediction models: A simulation study. <em>SIM</em>,
<em>44</em>(3-4), e10320. (<a
href="https://doi.org/10.1002/sim.10320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Alex Carriero and Kim Luijken and Anne de Hond and Karel G. M. Moons and Ben van Calster and Maarten van Smeden},
  doi          = {10.1002/sim.10320},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10320},
  shortjournal = {Stat. Med.},
  title        = {The harms of class imbalance corrections for machine learning based prediction models: A simulation study},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A varying-coefficient additive hazard model for recurrent
events data. <em>SIM</em>, <em>44</em>(3-4), e10319. (<a
href="https://doi.org/10.1002/sim.10319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The additive hazard model, which focuses on risk differences rather than risk ratios, has been widely applied in practice. In this paper, we consider an additive hazard model with varying coefficients to analyze recurrent events data. The model allows for both varying and constant coefficients. We first propose an estimating equation-based approach with spline basis smoothing for all functional coefficients. Then, we provide theoretical justifications for the resulting estimates, including consistency, rate of convergence, and asymptotic distribution. Furthermore, we construct a Cramér–von Mises test procedure to investigate whether the functional coefficients should be treated as constant, and its asymptotic null distribution is also derived. Extensive simulation experiments are conducted to evaluate the finite-sample performance of the proposed approaches. A Chronic Granulotamous Disease data set was analyzed to illustrate our methodology.},
  archive      = {J_SIM},
  author       = {Zhao Da and Xia Xiaochao and Li Jialiang},
  doi          = {10.1002/sim.10319},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10319},
  shortjournal = {Stat. Med.},
  title        = {A varying-coefficient additive hazard model for recurrent events data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of diagnostic test accuracy without gold
standards. <em>SIM</em>, <em>44</em>(3-4), e10315. (<a
href="https://doi.org/10.1002/sim.10315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ideal evaluation of diagnostic test performance requires a reference test that is free of errors. However, for many diseases, obtaining such a “gold standard” reference is either impossible or prohibitively expensive. Estimating test accuracy in the absence of a gold standard is therefore a significant challenge. In this article, we introduce and categorize existing methods for evaluating diagnostic tests without a gold standard, considering factors such as the type and number of tests, as well as the structure of the observed data. For each method, we provide a comprehensive introduction and analysis of its underlying assumptions, model architecture, identifiability, estimation techniques, and inference procedures. We use R to conduct simulations for widely applicable models, validating assumptions, comparing models, and assessing their reliability. Additionally, we present real-world examples along with the corresponding R code for these models, enabling readers to better understand how to apply them effectively. Beyond diagnostic medicine, we underscore that the issue of imperfect gold standards affects other fields, drawing parallels to the noisy label problem in machine learning. By highlighting similarities and differences across these domains, we open pathways for further research. The primary aim of this article is to consolidate existing methods for assessing test accuracy in the absence of a gold standard and to provide practical guidance for researchers seeking to apply these methods effectively.},
  archive      = {J_SIM},
  author       = {Ao Sun and Xiao-Hua Zhou},
  doi          = {10.1002/sim.10315},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10315},
  shortjournal = {Stat. Med.},
  title        = {Estimation of diagnostic test accuracy without gold standards},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the effectiveness of sample size re-estimation: An
operating characteristic focused, hybrid frequentist-bayesian approach.
<em>SIM</em>, <em>44</em>(3-4), e10310. (<a
href="https://doi.org/10.1002/sim.10310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size re-estimation (SSR) is perhaps the most used adaptive procedure in both frequentist and Bayesian adaptive designs for clinical trials. The primary focus of all current frequentist and Bayesian SSR procedures is type I error control. We propose a hybrid frequentist-Bayesian SSR approach that focuses on optimizing operating characteristics (OC), which uses simulations to investigate the associated OC and adjusts accordingly. The hybrid approach incorporates the Bayesian predictive power into the frequentist framework of SSR. Simulations show that the hybrid approach can substantially outperform popular frequentist type error-focused SSR procedure. The hybrid approach can substantially improve the effectiveness of SSR using Bayesian predictive power.},
  archive      = {J_SIM},
  author       = {Ping Gao},
  doi          = {10.1002/sim.10310},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10310},
  shortjournal = {Stat. Med.},
  title        = {Improving the effectiveness of sample size re-estimation: An operating characteristic focused, hybrid frequentist-bayesian approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing covariates effects on bivariate reference regions.
<em>SIM</em>, <em>44</em>(3-4), e10308. (<a
href="https://doi.org/10.1002/sim.10308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated clinical measurements are routinely interpreted via comparisons with univariate reference intervals examined side by side. Multivariate reference regions (MVRs), i.e., regions that characterize the distribution of multivariate results, have been proposed as a more adequate interpretation tool in such situations. However, MVR estimation methods have not yet been fully developed and are rarely used by physicians. The multivariate distribution of correlated measurements might change with certain patient characteristics (e.g., age or gender), and their effect on the shape of an MVR can be complex, involving interaction terms. For instance, the reference region shape for a given set of continuous covariates might vary across groups with respect to the value of a categorical variable. This paper examines the use of a bootstrap-based hypothesis test for examining the effect of covariates on bivariate reference regions, testing the effect of factor-by-region interactions. An estimation algorithm based on smoothing splines was used to construct the bivariate reference region for a pediatric anthropometric dataset, and the bootstrapping procedure was used to determine the effect of age and gender on the shape of the reference region. (Height, weight) bivariate distribution was shown to depend on the interaction between age and gender. The bootstrapping procedure confirmed that a bivariate growth chart is desirable over univariate age-gender body mass index (BMI) percentile curves. Whereas the well-known BMI criterion detects only two atypical situations (i.e., underweight, overweight), the bootstrap-tested bivariate reference region detected abnormally large or small body frames for different ages and genders.},
  archive      = {J_SIM},
  author       = {Óscar Lado-Baleato and Javier Roca-Pardiñas and Carmen Cadarso-Suárez and Francisco Gude},
  doi          = {10.1002/sim.10308},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10308},
  shortjournal = {Stat. Med.},
  title        = {Testing covariates effects on bivariate reference regions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian borrowing with multiple heterogeneous historical
studies using order restricted normalized power prior. <em>SIM</em>,
<em>44</em>(3-4), e10302. (<a
href="https://doi.org/10.1002/sim.10302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent U.S. Food and Drug Administration guidance on complex innovative trial designs acknowledges the use of Bayesian strategies to incorporate historical information based on clinical expertise and data similarity. Also, data from multiple previous studies with similar settings often qualify for historical borrowing. Although several classes of informative priors can semi-automatically leverage historical information based on data compatibility, it is common that some exogenous factors, such as the year of patient enrollment, can also influence the relevance of each historical study to the current trial. Consequently, a natural a priori ordering among historical trials often arises, a constraint that many current informative priors fail to accommodate. Motivated by a pediatric lupus clinical study and an oncology trial, we introduce a variant of the power prior, named the ordered normalized power prior , which ensures a targeted order restriction on the power parameters and maintains data-adaptive borrowing. We further explore and compare two distinct normalization strategies and outline computational details with efficient sampling algorithms. The clinical datasets mentioned are analyzed, and extensive simulations are conducted for comparison. An efficient implementation is provided in our updated package NPP available on the Comprehensive R Archive Network.},
  archive      = {J_SIM},
  author       = {Zifei Han and Qiang Zhang and Ram Tiwari and Tianyu Bai},
  doi          = {10.1002/sim.10302},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10302},
  shortjournal = {Stat. Med.},
  title        = {Bayesian borrowing with multiple heterogeneous historical studies using order restricted normalized power prior},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reference-based multiple imputation for longitudinal binary
data. <em>SIM</em>, <em>44</em>(3-4), e10301. (<a
href="https://doi.org/10.1002/sim.10301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Suzie Cro and Matteo Quartagno and Ian R. White and James R. Carpenter},
  doi          = {10.1002/sim.10301},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10301},
  shortjournal = {Stat. Med.},
  title        = {Reference-based multiple imputation for longitudinal binary data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian analysis of censored linear mixed-effects models
for heavy-tailed irregularly observed repeated measures. <em>SIM</em>,
<em>44</em>(3-4), e10295. (<a
href="https://doi.org/10.1002/sim.10295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of mixed-effect models to understand the evolution of the human immunodeficiency virus (HIV) and the progression of acquired immune deficiency syndrome (AIDS) has been the cornerstone of longitudinal data analysis in recent years. However, data from HIV/AIDS clinical trials have several complexities. Some of the most common recurrences are related to the situation where the HIV viral load can be undetectable, and the measures of the patient can be registered irregularly due to some problems in the data collection. Although censored mixed-effects models assuming conditionally independent normal random errors are commonly used to analyze this data type, this model may need to be more appropriate for accommodating outlying observations and responses recorded at irregular intervals. Consequently, in this paper, we propose a Bayesian analysis of censored linear mixed-effects models that replace Gaussian assumptions with a flexible class of distributions, such as the scale mixture of normal family distributions, considering a damped exponential correlation structure that was employed to account for within-subject autocorrelation among irregularly observed measures. For this complex structure, Stan &#39;s default No-U-Turn sampler is utilized to obtain posterior simulations. The feasibility of the proposed methods was demonstrated through several simulation studies and their application to two AIDS case studies.},
  archive      = {J_SIM},
  author       = {Kelin Zhong and Fernanda L. Schumacher and Luis M. Castro and Víctor H. Lachos},
  doi          = {10.1002/sim.10295},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10295},
  shortjournal = {Stat. Med.},
  title        = {Bayesian analysis of censored linear mixed-effects models for heavy-tailed irregularly observed repeated measures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comments on “sample size adaptation designs and efficiency
comparison with group sequential designs.” <em>SIM</em>,
<em>44</em>(3-4), e10294. (<a
href="https://doi.org/10.1002/sim.10294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Meinhard Kieser and Maximilian Pilz},
  doi          = {10.1002/sim.10294},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10294},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Sample size adaptation designs and efficiency comparison with group sequential designs”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized bayesian stochastic block model for microbiome
community detection. <em>SIM</em>, <em>44</em>(3-4), e10291. (<a
href="https://doi.org/10.1002/sim.10291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in next-generation sequencing technology have enabled the high-throughput profiling of metagenomes and accelerated microbiome studies. Recently, there has been a rise in quantitative studies that aim to decipher the microbiome co-occurrence network and its underlying community structure based on metagenomic sequence data. Uncovering the complex microbiome community structure is essential to understanding the role of the microbiome in disease progression and susceptibility. Taxonomic abundance data generated from metagenomic sequencing technologies are high-dimensional and compositional, suffering from uneven sampling depth, over-dispersion, and zero-inflation. These characteristics often challenge the reliability of the current methods for microbiome community detection. To study the microbiome co-occurrence network and perform community detection, we propose a generalized Bayesian stochastic block model that is tailored for microbiome data analysis where the data are transformed using the recently developed modified centered-log ratio transformation. Our model also allows us to leverage taxonomic tree information using a Markov random field prior. The model parameters are jointly inferred by using Markov chain Monte Carlo sampling techniques. Our simulation study showed that the proposed approach performs better than competing methods even when taxonomic tree information is non-informative. We applied our approach to a real urinary microbiome dataset from postmenopausal women. To the best of our knowledge, this is the first time the urinary microbiome co-occurrence network structure in postmenopausal women has been studied. In summary, this statistical methodology provides a new tool for facilitating advanced microbiome studies.},
  archive      = {J_SIM},
  author       = {Kevin C. Lutz and Michael L. Neugent and Tejasv Bedi and Nicole J. De Nisco and Qiwei Li},
  doi          = {10.1002/sim.10291},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10291},
  shortjournal = {Stat. Med.},
  title        = {A generalized bayesian stochastic block model for microbiome community detection},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple sensitivity analysis method for unmeasured
confounders via linear programming with estimating equation constraints.
<em>SIM</em>, <em>44</em>(3-4), e10288. (<a
href="https://doi.org/10.1002/sim.10288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In estimating the average treatment effect in observational studies, the influence of confounders should be appropriately addressed. To this end, the propensity score is widely used. If the propensity scores are known for all the subjects, bias due to confounders can be adjusted by using the inverse probability weighting (IPW) by the propensity score. Since the propensity score is unknown in general, it is usually estimated by the parametric logistic regression model with unknown parameters estimated by solving the score equation under the strongly ignorable treatment assignment (SITA) assumption. Violation of the SITA assumption and/or misspecification of the propensity score model can cause serious bias in estimating the average treatment effect (ATE). To relax the SITA assumption, the IPW estimator based on the outcome-dependent propensity score has been successfully introduced. However, it still depends on the correctly specified parametric model and its identification. In this paper, we propose a simple sensitivity analysis method for unmeasured confounders. In the standard practice, the estimating equation is used to estimate the unknown parameters in the parametric propensity score model. Our idea is to make inferences on the (ATE) by removing restrictive parametric model assumptions while still utilizing the estimating equation. Using estimating equations as constraints, which the true propensity scores asymptotically satisfy, we construct the worst-case bounds for the ATE with linear programming. Differently from the existing sensitivity analysis methods, we construct the worst-case bounds with minimal assumptions. We illustrate our proposal by simulation studies and a real-world example.},
  archive      = {J_SIM},
  author       = {Chengyao Tang and Yi Zhou and Ao Huang and Satoshi Hattori},
  doi          = {10.1002/sim.10288},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10288},
  shortjournal = {Stat. Med.},
  title        = {A simple sensitivity analysis method for unmeasured confounders via linear programming with estimating equation constraints},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian spatial relative survival model to estimate the
loss in life expectancy and crude probability of death for cancer
patients. <em>SIM</em>, <em>44</em>(3-4), e10287. (<a
href="https://doi.org/10.1002/sim.10287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date, there have not been any population-based cancer studies quantifying geographical patterns of the loss in life expectancy (LLE) and crude probability of death due to cancer ( C r C $$ C{r}_C $$ ). These absolute measures of survival are complementary to the more typically used relative measures of excess mortality and relative survival, and, together, they provide a fuller understanding of geographical disparities in survival outcomes for cancer patients. We propose using a spatially flexible parametric relative survival model in the Bayesian framework, which allows for the inclusion of spatial effects in hazard-level model components. The relative survival framework is the preferred approach to analyze cancer survival data because it does not require information on the cause of death, and the Bayesian spatial modeling approach allows complex and robust small-area estimation. The calculation of spatial estimates for LLE and C r C $$ C{r}_C $$ are demonstrated using publicly available simulated datasets. The associated computer program scripts are available to support the understanding and implementation of our methodology in other spatial cancer modelling applications.},
  archive      = {J_SIM},
  author       = {Yuliya Leontyeva and Yuxin Huang and Susanna Cramb and Jessica Cameron and Peter Baade and Kerrie Mengersen and Helen Thompson},
  doi          = {10.1002/sim.10287},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10287},
  shortjournal = {Stat. Med.},
  title        = {Bayesian spatial relative survival model to estimate the loss in life expectancy and crude probability of death for cancer patients},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple imputation for longitudinal data: A tutorial.
<em>SIM</em>, <em>44</em>(3-4), e10274. (<a
href="https://doi.org/10.1002/sim.10274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal studies are frequently used in medical research and involve collecting repeated measures on individuals over time. Observations from the same individual are invariably correlated and thus an analytic approach that accounts for this clustering by individual is required. While almost all research suffers from missing data, this can be particularly problematic in longitudinal studies as participation often becomes harder to maintain over time. Multiple imputation (MI) is widely used to handle missing data in such studies. When using MI, it is important that the imputation model is compatible with the proposed analysis model. In a longitudinal analysis, this implies that the clustering considered in the analysis model should be reflected in the imputation process. Several MI approaches have been proposed to impute incomplete longitudinal data, such as treating repeated measurements of the same variable as distinct variables or using generalized linear mixed imputation models. However, the uptake of these methods has been limited, as they require additional data manipulation and use of advanced imputation procedures. In this tutorial, we review the available MI approaches that can be used for handling incomplete longitudinal data, including where individuals are clustered within higher-level clusters. We illustrate implementation with replicable R and Stata code using a case study from the Childhood to Adolescence Transition Study.},
  archive      = {J_SIM},
  author       = {Rushani Wijesuriya and Margarita Moreno-Betancur and John B. Carlin and Ian R. White and Matteo Quartagno and Katherine J. Lee},
  doi          = {10.1002/sim.10274},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10274},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation for longitudinal data: A tutorial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
