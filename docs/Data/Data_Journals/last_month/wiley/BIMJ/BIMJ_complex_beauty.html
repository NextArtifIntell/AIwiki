<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIMJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="bimj---21">BIMJ - 21</h2>
<ul>
<li><details>
<summary>
(2025). Unscaled indices for assessing agreement of functional data.
<em>BIMJ</em>, <em>67</em>(1), e70039. (<a
href="https://doi.org/10.1002/bimj.70039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A decision to adopt a new medical device requires a rigorous assessment of the reliability and reproducibility of its clinical measurements. In this paper, with the goal of establishing the validity and acceptability of modern high-tech medical devices that generate functional data, we focus on the problem of assessing agreement of multiple functional data that are measured on the same subjects by different methods/technologies/raters. Specifically, we introduce a series of unscaled indices, total deviation index (TDI) and coverage probability (CP), that themselves are functions of time and can delineate the trends of intramethod, intermethod, and total (intra+inter) agreement of functional data across time in terms of the original measurement scale. We also develop scalar-valued TDI and CP indices that summarize the degree of agreement over the entire domain based on the weighted average idea. We advocate an experimental design under which each of the two methods generates replicated functional data measurements for each subject, and express each index using a mean function and variance components of a bivariate multilevel functional linear mixed effects model. Such a formulation allows us to smoothly estimate the indices based on our bivariate multilevel functional principal component analysis approach that only requires eigenanalyses of univariate covariance functions for better efficiency and scalability. Comprehensive simulation studies are conducted to examine the finite-sample properties of the estimators. The proposed method is applied to assess the reliability and reproducibility of renogram curves generated by diuresis renography, a high-tech medical imaging device widely used to detect kidney obstruction.},
  archive      = {J_BIMJ},
  author       = {Kaeum Choi and Jeong Hoon Jang},
  doi          = {10.1002/bimj.70039},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70039},
  shortjournal = {Bio. J.},
  title        = {Unscaled indices for assessing agreement of functional data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parametric estimation of the mean number of events in the
presence of competing risks. <em>BIMJ</em>, <em>67</em>(1), e70038. (<a
href="https://doi.org/10.1002/bimj.70038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events, for example, hospitalizations or drug prescriptions, are common in time-to-event research. One useful summary measure of the recurrent event process is the mean number of events. Methods for estimating the mean number of events exist and are readily implemented for situations in which the recurrent event is the only possible outcome. However, estimation gets more challenging in the competing risk setting, in which methods are so far limited to nonparametric approaches. To this end, we propose a postestimation command for estimating the mean number of events in the presence of competing risks by jointly modeling the intensity function of the recurrent event and the survival function for the competing events. The proposed method is implemented in the R-package JointFPM which is available on CRAN. Simulations demonstrate low bias and good coverage in scenarios where the intensity of the recurrent event does not depend on the number of previous events. We illustrate our method using data on readmissions after colorectal cancer surgery included in the frailtypack package for R. Estimates of the mean number of events can be used to augment time-to-event analyses when both recurrent and competing events exist. The proposed parametric approach offers estimation of a smooth function across time as well as easy estimation of different contrasts which is not available using a nonparametric approach.},
  archive      = {J_BIMJ},
  author       = {Joshua P. Entrop and Lasse H. Jakobsen and Michael J. Crowther and Mark Clements and Sandra Eloranta and Caroline E. Dietrich},
  doi          = {10.1002/bimj.70038},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70038},
  shortjournal = {Bio. J.},
  title        = {Parametric estimation of the mean number of events in the presence of competing risks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network meta-analysis of time-to-event endpoints with
individual participant data using restricted mean survival time
regression. <em>BIMJ</em>, <em>67</em>(1), e70037. (<a
href="https://doi.org/10.1002/bimj.70037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) extends pairwise meta-analysis to compare multiple treatments simultaneously by combining “direct” and “indirect” comparisons of treatments. The availability of individual participant data (IPD) makes it possible to evaluate treatment effect moderation and to draw inferences about treatment effects by taking the full utilization of individual covariates from multiple clinical trials. In IPD-NMA, restricted mean survival time (RMST) models have gained popularity when analyzing time-to-event outcomes because RMST models offer more straightforward interpretations of treatment effects with fewer assumptions than hazard ratios commonly estimated from Cox models. Existing approaches estimate RMST within each study and then combine by using aggregate-level NMA methods. However, these methods cannot incorporate individual covariates to evaluate the effect moderation. In this paper, we propose advanced RMST NMA models when IPD are available. Our models allow us to study treatment effect moderation and provide a comprehensive understanding about comparative effectiveness of treatments and subgroup effects. The methods are evaluated by an extensive simulation study and illustrated using a real NMA example about treatments for patients with atrial fibrillation.},
  archive      = {J_BIMJ},
  author       = {Kaiyuan Hua and Xiaofei Wang and Hwanhee Hong},
  doi          = {10.1002/bimj.70037},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70037},
  shortjournal = {Bio. J.},
  title        = {Network meta-analysis of time-to-event endpoints with individual participant data using restricted mean survival time regression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional variable selection with competing events
using cooperative penalized regression. <em>BIMJ</em>, <em>67</em>(1),
e70036. (<a href="https://doi.org/10.1002/bimj.70036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection is an important step in the analysis of high-dimensional data, yet there are limited options for survival outcomes in the presence of competing risks. Commonly employed penalized Cox regression considers each event type separately through cause-specific models, neglecting possibly shared information between them. We adapt the feature-weighted elastic net (fwelnet), an elastic net generalization, to survival outcomes and competing risks. For two causes, our proposed algorithm fits two alternating cause-specific models, where each model receives the coefficient vector of the complementary model as prior information. We dub this “cooperative penalized regression,” as it enables the modeling of competing risk data with cause-specific models while accounting for shared effects between causes. Coefficients that are shrunken toward zero in the model for the first cause will receive larger penalization weights in the model for the second cause and vice versa. Through multiple iterations, this process ensures stronger penalization of uninformative predictors in both models. We demonstrate our method&#39;s variable selection capabilities on simulated genomics data and apply it to bladder cancer microarray data. We evaluate selection performance using the positive predictive value for the correct selection of informative features and the false positive rate for the selection of uninformative variables. The benchmark compares results with cause-specific penalized Cox regression, random survival forests, and likelihood-boosted Cox regression. Results indicate that our approach is more effective at selecting informative features and removing uninformative features. In settings without shared effects, variable selection performance is similar to cause-specific penalized Cox regression.},
  archive      = {J_BIMJ},
  author       = {Lukas Burk and Andreas Bender and Marvin N. Wright},
  doi          = {10.1002/bimj.70036},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70036},
  shortjournal = {Bio. J.},
  title        = {High-dimensional variable selection with competing events using cooperative penalized regression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation analysis with exposure–mediator interaction and
covariate measurement error under the additive hazards model.
<em>BIMJ</em>, <em>67</em>(1), e70035. (<a
href="https://doi.org/10.1002/bimj.70035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis is a useful tool to examine how an exposure variable causally affects an outcome variable through an intermediate variable. In recent years, there is increasing research interest in mediation analysis with survival data. The existing literature usually requires accurate measurements of the mediator and the confounders, which is infeasible in many biomedical and social science studies. Ignoring measurement errors may lead to misleading inference results. Furthermore, the current identification results of causal effects under the additive hazards model are limited to the scenario with no exposure–mediator interaction, which can be unappealing in mediation analysis. In this paper, we derive the identification results of direct and indirect effects under the additive hazards model in the presence of exposure–mediator interaction. Furthermore, we propose a corrected approach to adjust for the impact of measurement error in the mediator and the confounders and obtain consistent estimations of the direct and indirect effects. The performance of the proposed method is studied in simulation studies and a real data study.},
  archive      = {J_BIMJ},
  author       = {Ying Yan and Lingzhu Shen},
  doi          = {10.1002/bimj.70035},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70035},
  shortjournal = {Bio. J.},
  title        = {Mediation analysis with Exposure–Mediator interaction and covariate measurement error under the additive hazards model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bias-corrected bayesian nonparametric model for combining
studies with varying quality in meta-analysis. <em>BIMJ</em>,
<em>67</em>(1), e70034. (<a
href="https://doi.org/10.1002/bimj.70034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian nonparametric (BNP) approaches for meta-analysis have been developed to relax distributional assumptions and handle the heterogeneity of random effects distributions. These models account for possible clustering and multimodality of the random effects distribution. However, when we combine studies of varying quality, the resulting posterior is not only a combination of the results of interest but also factors threatening the integrity of the studies&#39; results. We refer to these factors as the studies&#39; internal validity biases (e.g., reporting bias, data quality, and patient selection bias). In this paper, we introduce a new meta-analysis model called the bias-corrected Bayesian nonparametric (BC-BNP) model, which aims to automatically correct for internal validity bias in meta-analysis by only using the reported effects and their standard errors. The BC-BNP model is based on a mixture of a parametric random effects distribution, which represents the model of interest, and a BNP model for the bias component. This model relaxes the parametric assumptions of the bias distribution of the model introduced by Verde. Using simulated data sets, we evaluate the BC-BNP model and illustrate its applications with two real case studies. Our results show several potential advantages of the BC-BNP model: (1) It can detect bias when present while producing results similar to a simple normal–normal random effects model when bias is absent. (2) Relaxing the parametric assumptions of the bias component does not affect the model of interest and yields consistent results with the model of Verde. (3) In some applications, a BNP model of bias offers a better understanding of the studies&#39; biases by clustering studies with similar biases. We implemented the BC-BNP model in the R package jarbes, facilitating its practical application.},
  archive      = {J_BIMJ},
  author       = {Pablo Emilio Verde and Gary L. Rosner},
  doi          = {10.1002/bimj.70034},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70034},
  shortjournal = {Bio. J.},
  title        = {A bias-corrected bayesian nonparametric model for combining studies with varying quality in meta-analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis for effects of multiple exposures in
the presence of unmeasured confounding. <em>BIMJ</em>, <em>67</em>(1),
e70033. (<a href="https://doi.org/10.1002/bimj.70033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiological research aims to investigate how multiple exposures affect health outcomes of interest, but observational studies often suffer from biases caused by unmeasured confounders. In this study, we develop a novel sensitivity model to investigate the effect of correlated multiple exposures on the continuous health outcomes of interest. The proposed sensitivity analysis is model-agnostic and can be applied to any machine learning algorithm. The interval of single- or joint-exposure effects is efficiently obtained by solving a linear programming problem with a quadratic constraint. Some strategies for reducing the input burden in the sensitivity analysis are discussed. We demonstrate the usefulness of sensitivity analysis via numerical studies and real data application.},
  archive      = {J_BIMJ},
  author       = {Boram Jeong and Seungjae Lee and Shinhee Ye and Donghwan Lee and Woojoo Lee},
  doi          = {10.1002/bimj.70033},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70033},
  shortjournal = {Bio. J.},
  title        = {Sensitivity analysis for effects of multiple exposures in the presence of unmeasured confounding},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification of difference in nonselectivity between in
vitro diagnostic medical devices. <em>BIMJ</em>, <em>67</em>(1), e70032.
(<a href="https://doi.org/10.1002/bimj.70032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correct measurement results from in vitro diagnostic (IVD) medical devices (MD) are crucial for optimal patient care. The performance of IVD-MDs is often assessed through method comparison studies. Such studies can be compromised by the influence of various factors. The effect of these factors must be examined in every method comparison study, for example, nonselectivity differences between compared IVD-MDs are examined. Historically, selectivity or nonselectivity has been defined as a qualitative term. However, a quantification of nonselectivity differences between IVD-MDs is needed. This paper fills this need by introducing a novel measure for quantifying differences in nonselectivity (DINS) between a pair of IVD-MDs. Assuming one of the IVD-MDs involved in the comparison exhibits high selectivity for the analyte, it becomes feasible to quantify nonselectivity in the other IVD-MD by employing this DINS measure. Our approach leverages elements from univariate ordinary least squares regression and incorporates repeatability IVD-MD variances, resulting in a normalized measure. We also introduce a plug-in estimator for this measure, which is notably linked to the average relative increase in prediction interval widths attributable to DINS. This connection is exploited to establish a criterion for identifying excessive DINS utilizing a proof-of-hazard approach. Utilizing Monte Carlo simulations, we investigate how the estimator relates to population characteristics like DINS and heteroskedasticity. We find that DINS impacts the mean, variance, and 99th percentile of the estimator, while heteroskedasticity affects only the latter two, and to a considerably smaller extent compared to DINS. Importantly, the size of the study design modulates these effects. We also confirm, when using clinical data, that DINS between pairs of IVD-MDs influence the estimator correspondingly to those of simulated data. Thus, the proposed estimator serves as an effective metric for quantifying DINS between IVD-MDs and helping to determine the quality of a method comparison study.},
  archive      = {J_BIMJ},
  author       = {Pernille Kjeilen Fauskanger and Sverre Sandberg and Jesper Johansen and Thomas Keller and Jeffrey Budd and W. Greg Miller and Anne Stavelin and Vincent Delatour and Mauro Panteghini and Bård Støve},
  doi          = {10.1002/bimj.70032},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70032},
  shortjournal = {Bio. J.},
  title        = {Quantification of difference in nonselectivity between in vitro diagnostic medical devices},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing and comparing four families of bayesian network
autocorrelation models for binary outcomes: Estimating peer effects
involving adoption of medical technologies. <em>BIMJ</em>,
<em>67</em>(1), e70030. (<a
href="https://doi.org/10.1002/bimj.70030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the extensive use of network autocorrelation models in social network analysis, network autocorrelation models for binary dependent variables have received surprisingly scant attention. In this paper, we develop four network autocorrelation models for a binary random variable defined by whether the peer effect (also termed social influence or contagion) acts on latent continuous outcomes leading to an indirect effect under a normal or a logistic distribution or on the probability of the observed outcome itself under a probit or a logit link function defining a direct effect to account for interdependence between outcomes. For all models, we use a Bayesian approach for model estimation under a uniform prior on a transformed peer effect parameter ( ) designed to enhance model computation and compare results to those under the uniform prior for . We use simulation to assess the performance of Bayesian point and interval estimators for each of the four models when the model that generated the data is used for estimation (precision assessment) and when each of the other three models instead generated the data (robustness assessment). We construct a United States New England region patient-sharing hospital network and apply the four network autocorrelation models to study the adoption of robotic surgery, a new medical technology, among hospitals using a cohort of United States Medicare beneficiaries in 2016 and 2017. Finally, we develop a deviance information criterion for each of the four models to compare their fit to the observed data and use posterior predictive p -values to assess the models&#39; ability to recover specified features of the data. The results find that although the indirect peer effect of the propensity of peer hospital adoption on that of the focal hospital is positive under both latent response autocorrelation models, the direct peer effect of the peer hospital&#39;s probability of adopting robotic surgery on the probability of the focal hospital adopting robotic surgery decreases under both mean autocorrelation data models. However, neither of these associations is statistically significant.},
  archive      = {J_BIMJ},
  author       = {Guanqing Chen and A. James O&#39;Malley},
  doi          = {10.1002/bimj.70030},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70030},
  shortjournal = {Bio. J.},
  title        = {Developing and comparing four families of bayesian network autocorrelation models for binary outcomes: Estimating peer effects involving adoption of medical technologies},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The progression-free-survival ratio in molecularly aided
tumor trials: A critical examination of current practice and suggestions
for alternative methods. <em>BIMJ</em>, <em>67</em>(1), e70028. (<a
href="https://doi.org/10.1002/bimj.70028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progression-free-survival ratio is a popular endpoint in oncology trials, which is frequently applied to evaluate the efficacy of molecularly targeted treatments in late-stage patients. Using elementary calculations and simulations, numerous shortcomings of the current methodology are pointed out. As a remedy to these shortcomings, an alternative methodology is proposed, using a marginal Cox model or a marginal accelerated failure time model for clustered time-to-event data. Using comprehensive simulations, it is shown that this methodology outperforms existing methods in settings where the intrapatient correlation is low to moderate. The performance of the model is further demonstrated in a real data example from a molecularly aided tumor trial. Sample size considerations are discussed.},
  archive      = {J_BIMJ},
  author       = {Dominic Edelmann and Tobias Terzer and Peter Horak and Richard Schlenk and Axel Benner},
  doi          = {10.1002/bimj.70028},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70028},
  shortjournal = {Bio. J.},
  title        = {The progression-free-survival ratio in molecularly aided tumor trials: A critical examination of current practice and suggestions for alternative methods},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A principled approach to adjust for unmeasured time-stable
confounding of supervised treatment. <em>BIMJ</em>, <em>67</em>(1),
e70026. (<a href="https://doi.org/10.1002/bimj.70026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to adjust for unmeasured time-stable confounding when the time between consecutive treatment administrations is fixed. We achieve this by focusing on a new-user cohort. Furthermore, we envisage that all time-stable confounding goes through the potential time on treatment as dictated by the disease condition at the initiation of treatment. Following this logic, we may eliminate all unmeasured time-stable confounding by adjusting for the potential time on treatment. A challenge with this approach is that right censoring of the potential time on treatment occurs when treatment is terminated at the time of the event of interest, for example, if the event of interest is death. We show how this challenge may be solved by means of the expectation-maximization algorithm without imposing any further assumptions on the distribution of the potential time on treatment. The usefulness of the methodology is illustrated in a simulation study. We also apply the methodology to investigate the effect of depression/anxiety drugs on subsequent poisoning by other medications in the Danish population by means of national registries. We find a protective effect of treatment with selective serotonin reuptake inhibitors on the risk of poisoning by various medications (1- year risk difference of approximately ) and a standard Cox model analysis shows a harming effect (1-year risk difference of approximately ), which is consistent with what we would expect due to confounding by indication. Unmeasured time-stable confounding can be entirely adjusted for when the time between consecutive treatment administrations is fixed.},
  archive      = {J_BIMJ},
  author       = {Jeppe Ekstrand Halkjær Madsen and Thomas Delvin and Thomas Scheike and Christian Pipper},
  doi          = {10.1002/bimj.70026},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70026},
  shortjournal = {Bio. J.},
  title        = {A principled approach to adjust for unmeasured time-stable confounding of supervised treatment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A preplanned multi-stage platform trial for discovering
multiple superior treatments with control of FWER and power.
<em>BIMJ</em>, <em>67</em>(1), e70025. (<a
href="https://doi.org/10.1002/bimj.70025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in the implementation of platform trials, which provide the flexibility to incorporate new treatment arms during the trial and the ability to halt treatments early based on lack of benefit or observed superiority. In such trials, it can be important to ensure that error rates are controlled. This paper introduces a multi-stage design that enables the addition of new treatment arms, at any point, in a preplanned manner within a platform trial, while still maintaining control over the family-wise error rate. This paper focuses on finding the required sample size to achieve a desired level of statistical power when treatments are continued to be tested even after a superior treatment has already been found. This may be of interest if there are treatments from different sponsors which are also superior to the current control or multiple doses being tested. The calculations to determine the expected sample size is given. A motivating trial is presented in which the sample size of different configurations is studied. In addition, the approach is compared to running multiple separate trials and it is shown that in many scenarios if family-wise error rate control is needed there may not be benefit in using a platform trial when comparing the sample size of the trial.},
  archive      = {J_BIMJ},
  author       = {Peter Greenstreet and Thomas Jaki and Alun Bedding and Pavel Mozgunov},
  doi          = {10.1002/bimj.70025},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70025},
  shortjournal = {Bio. J.},
  title        = {A preplanned multi-stage platform trial for discovering multiple superior treatments with control of FWER and power},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing balance of baseline time-dependent covariates via
the fréchet distance. <em>BIMJ</em>, <em>67</em>(1), e70024. (<a
href="https://doi.org/10.1002/bimj.70024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment of covariate balance is a key step when performing comparisons between groups particularly in real-world data. We generally evaluate it on baseline covariates, but rarely on longitudinal ones prior to a management decision. We could use pointwise standardized mean differences, standardized differences of slopes, or weights from the model for such purpose. Pointwise differences could be cumbersome for densely sampled longitudinal markers and/or measured at different points. Slopes are suitable for linear or transformable models but not for more complex curves. Weights do not identify the specific covariate(s) responsible for imbalances. This work presents the Fréchet distance as a viable alternative to assess balance of time-dependent covariates. A set of linear and nonlinear curves for which their standardized difference or differences in functional parameters were within 10% sought to identify the Fréchet distance equivalent to this threshold. This threshold is dependent on the level of noise present and thus within group heterogeneity and error variance are needed for its interpretation. Applied to a set of real curves representing the monthly trajectory of hemoglobin A1c from diabetic patients showed that the curves in the two groups were not balanced at the 10% mark. A Beta distribution represents the Fréchet distance distribution reasonably well in most scenarios. This assessment of covariate balance provides the following advantages: It can handle curves of different lengths, shapes, and arbitrary time points. Future work includes examining the utility of this measure under within-series missingness, within-group heterogeneity, its comparison with other approaches, and asymptotics.},
  archive      = {J_BIMJ},
  author       = {Mireya Díaz},
  doi          = {10.1002/bimj.70024},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70024},
  shortjournal = {Bio. J.},
  title        = {Assessing balance of baseline time-dependent covariates via the fréchet distance},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating a domain adaptation approach for integrating
different measurement instruments in a longitudinal clinical registry.
<em>BIMJ</em>, <em>67</em>(1), e70023. (<a
href="https://doi.org/10.1002/bimj.70023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a longitudinal clinical registry, different measurement instruments might have been used for assessing individuals at different time points. To combine them, we investigate deep learning techniques for obtaining a joint latent representation, to which the items of different measurement instruments are mapped. This corresponds to domain adaptation, an established concept in computer science for image data. Using the proposed approach as an example, we evaluate the potential of domain adaptation in a longitudinal cohort setting with a rather small number of time points, motivated by an application with different motor function measurement instruments in a registry of spinal muscular atrophy (SMA) patients. There, we model trajectories in the latent representation by ordinary differential equations (ODEs), where person-specific ODE parameters are inferred from baseline characteristics. The goodness of fit and complexity of the ODE solutions then allow to judge the measurement instrument mappings. We subsequently explore how alignment can be improved by incorporating corresponding penalty terms into model fitting. To systematically investigate the effect of differences between measurement instruments, we consider several scenarios based on modified SMA data, including scenarios where a mapping should be feasible in principle and scenarios where no perfect mapping is available. While misalignment increases in more complex scenarios, some structure is still recovered, even if the availability of measurement instruments depends on patient state. A reasonable mapping is feasible also in the more complex real SMA data set. These results indicate that domain adaptation might be more generally useful in statistical modeling for longitudinal registry data.},
  archive      = {J_BIMJ},
  author       = {Maren Hackenberg and Michelle Pfaffenlehner and Max Behrens and Astrid Pechmann and Janbernd Kirschner and Harald Binder},
  doi          = {10.1002/bimj.70023},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70023},
  shortjournal = {Bio. J.},
  title        = {Investigating a domain adaptation approach for integrating different measurement instruments in a longitudinal clinical registry},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Goodness-of-fit testing for a regression model with a doubly
truncated response. <em>BIMJ</em>, <em>67</em>(1), e70022. (<a
href="https://doi.org/10.1002/bimj.70022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis and epidemiology, among other fields, interval sampling is often employed. With interval sampling, the individuals undergoing the event of interest within a calendar time interval are recruited. This results in doubly truncated event times. Double truncation, which may appear with other sampling designs too, induces a selection bias, so ordinary statistical methods are generally inconsistent. In this paper, we introduce goodness-of-fit procedures for a regression model when the response variable is doubly truncated. With this purpose, a marked empirical process based on weighted residuals is constructed and its weak convergence is established. Kolmogorov–Smirnov– and Cramér–von Mises–type tests are consequently derived from such core process, and a bootstrap approximation for their practical implementation is given. The performance of the proposed tests is investigated through simulations. An application to model selection for AIDS incubation time as depending on age at infection is provided.},
  archive      = {J_BIMJ},
  author       = {Jacobo de Uña-Álvarez},
  doi          = {10.1002/bimj.70022},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70022},
  shortjournal = {Bio. J.},
  title        = {Goodness-of-fit testing for a regression model with a doubly truncated response},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test statistics and statistical inference for data with
informative cluster sizes. <em>BIMJ</em>, <em>67</em>(1), e70021. (<a
href="https://doi.org/10.1002/bimj.70021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, investigators often encounter clustered data. The cluster sizes are said to be informative if the outcome depends on the cluster size. Ignoring informative cluster sizes in the analysis leads to biased parameter estimation in marginal and mixed-effect regression models. Several methods to analyze data with informative cluster sizes have been proposed; however, methods to test the informativeness of the cluster sizes are limited, particularly for the marginal model. In this paper, we propose a score test and a Wald test to examine the informativeness of the cluster sizes for a generalized linear model, a Cox model, and a proportional subdistribution hazards model. Statistical inference can be conducted through weighted estimating equations. The simulation results show that both tests control Type I error rates well, but the score test has higher power than the Wald test for right-censored data while the power of the Wald test is generally higher than the score test for the binary outcome. We apply the Wald and score tests to hematopoietic cell transplant data and compare regression analysis results with/without adjusting for informative cluster sizes.},
  archive      = {J_BIMJ},
  author       = {Soyoung Kim and Michael J. Martens and Kwang Woo Ahn},
  doi          = {10.1002/bimj.70021},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70021},
  shortjournal = {Bio. J.},
  title        = {Test statistics and statistical inference for data with informative cluster sizes},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusted inference for multiple testing procedure in
group-sequential designs. <em>BIMJ</em>, <em>67</em>(1), e70020. (<a
href="https://doi.org/10.1002/bimj.70020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adjustment of statistical significance levels for repeated analysis in group-sequential trials has been understood for some time. Adjustment accounting for testing multiple hypotheses is also well understood. There is limited research on simultaneously adjusting for both multiple hypothesis testing and repeated analyses of one or more hypotheses. We address this gap by proposing adjusted-sequential p-values that reject when they are less than or equal to the family-wise Type I error rate (FWER). We also propose sequential p $p$ -values for intersection hypotheses to compute adjusted-sequential p $p$ -values for elementary hypotheses. We demonstrate the application using weighted Bonferroni tests and weighted parametric tests for inference on each elementary hypothesis tested.},
  archive      = {J_BIMJ},
  author       = {Yujie Zhao and Qi Liu and Linda Z. Sun and Keaven M. Anderson},
  doi          = {10.1002/bimj.70020},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70020},
  shortjournal = {Bio. J.},
  title        = {Adjusted inference for multiple testing procedure in group-sequential designs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple contrast tests in the presence of partial
heteroskedasticity. <em>BIMJ</em>, <em>67</em>(1), e70019. (<a
href="https://doi.org/10.1002/bimj.70019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a general approach for handling multiple contrast tests for normally distributed data in the presence of partial heteroskedasticity. In contrast to the usual case of complete heteroskedasticity, the treatments belong to subgroups according to their variances. Treatments within these subgroups are homoskedastic, whereas treatments of different subgroups are heteroskedastic. New candidate as well as already existing approaches are described and compared by α $\alpha$ -simulations. Power simulations show that a gain in power is achieved when the partial heteroskedasticity is taken into account compared to procedures which wrongly assume complete heteroskedasticity. The new approaches will be applied to a phytopathological experiment.},
  archive      = {J_BIMJ},
  author       = {Mario Hasler and Tim Birr and Ludwig A. Hothorn},
  doi          = {10.1002/bimj.70019},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70019},
  shortjournal = {Bio. J.},
  title        = {Multiple contrast tests in the presence of partial heteroskedasticity},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oncology clinical trial design planning based on a
multistate model that jointly models progression-free and overall
survival endpoints. <em>BIMJ</em>, <em>67</em>(1), e70017. (<a
href="https://doi.org/10.1002/bimj.70017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When planning an oncology clinical trial, the usual approach is to assume proportional hazards and even an exponential distribution for time-to-event endpoints. Often, besides the gold-standard endpoint overall survival (OS), progression-free survival (PFS) is considered as a second confirmatory endpoint. We use a survival multistate model to jointly model these two endpoints and find that neither exponential distribution nor proportional hazards will typically hold for both endpoints simultaneously. The multistate model provides a stochastic process approach to model the dependency of such endpoints neither requiring latent failure times nor explicit dependency modeling such as copulae. We use the multistate model framework to simulate clinical trials with endpoints OS and PFS and show how design planning questions can be answered using this approach. In particular, nonproportional hazards for at least one of the endpoints are a consequence of OS and PFS being dependent and are naturally modeled to improve planning. We then illustrate how clinical trial design can be based on simulations from a multistate model. Key applications are coprimary endpoints and group-sequential designs. Simulations for these applications show that the standard simplifying approach may very well lead to underpowered or overpowered clinical trials. Our approach is quite general and can be extended to more complex trial designs, further endpoints, and other therapeutic areas. An R package is available on CRAN.},
  archive      = {J_BIMJ},
  author       = {Alexandra Erdmann and Jan Beyersmann and Kaspar Rufibach},
  doi          = {10.1002/bimj.70017},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70017},
  shortjournal = {Bio. J.},
  title        = {Oncology clinical trial design planning based on a multistate model that jointly models progression-free and overall survival endpoints},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To tweak or not to tweak. How exploiting flexibilities in
gene set analysis leads to overoptimism. <em>BIMJ</em>, <em>67</em>(1),
e70016. (<a href="https://doi.org/10.1002/bimj.70016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene set analysis, a popular approach for analyzing high-throughput gene expression data, aims to identify sets of genes that show enriched expression patterns between two conditions. In addition to the multitude of methods available for this task, users are typically left with many options when creating the required input and specifying the internal parameters of the chosen method. This flexibility can lead to uncertainty about the “right” choice, further reinforced by a lack of evidence-based guidance. Especially when their statistical experience is scarce, this uncertainty might entice users to produce preferable results using a “trial-and-error” approach. While it may seem unproblematic at first glance, this practice can be viewed as a form of “cherry-picking” and cause an optimistic bias, rendering the results nonreplicable on independent data. After this problem has attracted a lot of attention in the context of classical hypothesis testing, we now aim to raise awareness of such overoptimism in the different and more complex context of gene set analyses. We mimic a hypothetical researcher who systematically selects the analysis variants yielding their preferred results, thereby considering three distinct goals they might pursue. Using a selection of popular gene set analysis methods, we tweak the results in this way for two frequently used benchmark gene expression data sets. Our study indicates that the potential for overoptimism is particularly high for a group of methods frequently used despite being commonly criticized. We conclude by providing practical recommendations to counter overoptimism in research findings in gene set analysis and beyond.},
  archive      = {J_BIMJ},
  author       = {Milena Wünsch and Christina Sauer and Moritz Herrmann and Ludwig Christian Hinske and Anne-Laure Boulesteix},
  doi          = {10.1002/bimj.70016},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70016},
  shortjournal = {Bio. J.},
  title        = {To tweak or not to tweak. how exploiting flexibilities in gene set analysis leads to overoptimism},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Best subset solution path for linear dimension reduction
models using continuous optimization. <em>BIMJ</em>, <em>67</em>(1),
e70015. (<a href="https://doi.org/10.1002/bimj.70015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of best variables is a challenging problem in supervised and unsupervised learning, especially in high-dimensional contexts where the number of variables is usually much larger than the number of observations. In this paper, we focus on two multivariate statistical methods: principal components analysis and partial least squares. Both approaches are popular linear dimension-reduction methods with numerous applications in several fields including in genomics, biology, environmental science, and engineering. In particular, these approaches build principal components, new variables that are combinations of all the original variables. A main drawback of principal components is the difficulty to interpret them when the number of variables is large. To define principal components from the most relevant variables, we propose to cast the best subset solution path method into principal component analysis and partial least square frameworks. We offer a new alternative by exploiting a continuous optimization algorithm for best subset solution path. Empirical studies show the efficacy of our approach for providing the best subset solution path. The usage of our algorithm is further exposed through the analysis of two real data sets. The first data set is analyzed using the principle component analysis while the analysis of the second data set is based on partial least square framework.},
  archive      = {J_BIMJ},
  author       = {Benoit Liquet and Sarat Moka and Samuel Muller},
  doi          = {10.1002/bimj.70015},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70015},
  shortjournal = {Bio. J.},
  title        = {Best subset solution path for linear dimension reduction models using continuous optimization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
