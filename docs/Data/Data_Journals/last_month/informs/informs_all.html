<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>informs_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="informs">INFORMS</h1>
<h2 id="ijaa---6">IJAA - 6</h2>
<ul>
<li><details>
<summary>
(2025). Optimizing mobility for elderly and disabled dutch citizens
using taxis. <em>INFORMS Journal on Applied Analytics</em>,
<em>55</em>(1), 66–82. (<a
href="https://doi.org/10.1287/inte.2024.0180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the Netherlands, 200,000 elderly and disabled citizens annually use subsidized taxi rides executed by Transvision. The day-to-day planning of up to 15,000 long-distance rides was previously a complex and daunting task split over dozens of subcontractors. Transvision, CQM, and Geodan developed an optimization solution that combines the rides into efficient taxi routes. Starting in January 2020, this solution significantly improved the mobility challenge for elderly and disabled citizens, including (1) increased punctuality and a 50% improvement in passenger satisfaction, (2) savings of 15 million driving kilometers per year, and (3) combined financial savings for all stakeholders of 60 million euros over the years 2019 to 2023 and another total of 30 million euros projected for 2024 and 2025, according to conservative estimates. Daily planning in a single batch can range from 1,000 to 15,000 rides. To construct high-quality ride plans in reasonable time for this massive-scale operations research problem, we applied classical operations research techniques viewed through a modern lens. In this paper, we explain how practical large-scale dial-a-ride problems can be solved using high-quality heuristics that exploit the power of parallel processing. Furthermore, we present new and efficient techniques to perform the required millions to billions of calculations to determine distances and driving times on the Dutch road network. We overcome several practical challenges such as (1) aligning the interests of a vulnerable passenger group and over 60 different taxi operators, (2) aligning the software that interfaces with the various companies, and (3) adapting to changing regulations and ad hoc COVID-19 measures.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0180},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {66-82},
  title    = {Optimizing mobility for elderly and disabled dutch citizens using taxis},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of continuous steel annealing operations using
model predictive control at tata steel, india. <em>INFORMS Journal on
Applied Analytics</em>, <em>55</em>(1), 48–65. (<a
href="https://doi.org/10.1287/inte.2024.0183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In steel manufacturing, continuous annealing is a crucial heat treatment applied to cold-rolled steel strips to achieve a prescribed temperature, which will ensure quality in terms of mechanical properties. However, controlling this process is challenging because of slow furnace temperature dynamics, significant time delays, frequent changes in the steel mass flow rate, target annealing temperature changes induced by steel grade transitions, and multivariable interactions within the furnace zones. To address these challenges, Tata Steel, India, with consultancy support from the Indian Institute of Technology Bombay, developed a novel model predictive control (MPC) technology-based solution for a continuous annealing furnace, which produces automotive grade steels. The dynamic model was developed using data from both perturbation trials and scraping historical processes. We then converted the model to a discrete-time state-space form and used it to formulate an optimal control problem over a moving time window. The solution generates optimal furnace setpoints by solving this finite-horizon optimal control problem each minute, ensuring smooth temperature transitions during steel grade changes while avoiding operational constraint violations. Tata Steel successfully implemented an MPC-based real-time supervisory optimal control solution, which became fully operational in January 2023. The implementation of the solution has led to a significant improvement in the proportion of annealed products meeting the premium quality band (±5°C of the target temperature), increasing from 30% (manually operated) to 50% (MPC operated), thereby ensuring better uniformity of properties. Furthermore, an 8% reduction in products outside the widest band (±15°C) has prevented the reprocessing of 13,000 tons of material from one line alone, annually. We have seen a consistent 8% reduction in specific fuel consumption per ton of steel. When considering Tata Steel’s current installations and those under commissioning, these improvements translate to savings of US$2.5 million and a reduction of 10,000 tons of CO 2 emissions annually.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0183},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {48-65},
  title    = {Optimization of continuous steel annealing operations using model predictive control at tata steel, india},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). McDonald’s china adopts operations research for network
design. <em>INFORMS Journal on Applied Analytics</em>, <em>55</em>(1),
36–47. (<a href="https://doi.org/10.1287/inte.2024.0179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The supply chain network design (SCND) problem is a typical optimization problem that determines the structure of a supply chain and affects its costs and operational performance. SCND deals with various decisions, such as determining the number, size, and location of facilities and the optimal material and product flows of the entire supply chain network. Therefore, SCND is one of the most crucial planning problems in supply chain management. In this paper, we present a practical approach in which we adopt a mixed-integer programming (MIP) mathematical model to solve a real industry SCND problem for McDonald’s China. As a result of this project, McDonald’s China has saved millions of dollars in logistics costs and reduced CO 2 emissions by more than 10%. In our approach, size-reduction techniques were successfully applied to deal with a large-scale model, making it possible to analyze hundreds of scenarios before coming to a consensus.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0179},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {36-47},
  title    = {McDonald’s china adopts operations research for network design},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven inventory control and integrated employee
involvement for special buys at ALDI SÜD germany. <em>INFORMS Journal on
Applied Analytics</em>, <em>55</em>(1), 22–35. (<a
href="https://doi.org/10.1287/inte.2024.0182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a subsidiary of the ALDI SOUTH Group, ALDI SÜD Germany launched the data-driven special buys project in response to pandemic-related supply chain disruptions, market shifts, and growing competition. This initiative combines advanced analytics and operations research with employee engagement to optimize product life cycles of special buys products. The solution components include (1) multiperiod mixed-integer optimization (MIP) models for product order decisions to warehouses; (2) XGBoost classification for store product allocation (SAM); (3) a proprietary algorithm (ANA) for just-in-time reallocations between stores; (4) a multiperiod dynamic programming model (DAVE) developed in 2021 for nationwide inventory clearance; (5) an advanced version for inventory clearance, the DAVE stochastic dynamic programming model (DAVE SDP), introduced in 2023; (6) a multivariate regression model for budget allocations for markdowns on small product leftover quantities for incorporating store employee involvement; and (7) a smartphone application (Market-Whispering) to engage 50,000 employees in product selection. This paper focuses on the MIP, which optimizes the decision on stock order quantities to warehouses. We examine this in conjunction with ANA and DAVE because of their significant influence on operational efficiency. For DAVE, we examine two variants: the reference model DAVE and DAVE SDP. The project faced coordination challenges and required a strategic shift from decentralized to centralized approaches. Proprietary software for the special buys product range improved operational efficiency, positively impacting the daily operations of 40,000 store employees, and led to annual savings of several million euros in Germany.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0182},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {22-35},
  title    = {Data-driven inventory control and integrated employee involvement for special buys at ALDI SÜD germany},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven at sea: Forecasting and revenue management at
molslinjen. <em>INFORMS Journal on Applied Analytics</em>,
<em>55</em>(1), 5–21. (<a
href="https://doi.org/10.1287/inte.2024.0177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Molslinjen, one of the world’s largest operators of fast-moving catamaran ferries, based in Denmark, adopted a focus on digitalization to profoundly change its operations and business practices. Molslinjen partnered with Halfspace, a data, analytics, and artificial intelligence (AI) company based in Copenhagen, Denmark, to support that transition. Halfspace and Molslinjen jointly developed and deployed a successful forecasting and revenue management toolbox for the data-driven operation of ferries in Denmark since 2020. This has resulted in $2.6–3.2 million yearly savings (and a total of $5 million savings as of December 2023), a significant reduction in the number of delayed departures and average delays, and a 3% reduction in fuel costs and emissions. This toolbox relies on some of the latest advances in machine learning for forecasting and in analytics approaches to revenue management. The potential for generalizing our toolbox to the global ferry industry is significant, with an impact on both revenues and environmental, societal, and governance criteria.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0177},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {5-21},
  title    = {Data-driven at sea: Forecasting and revenue management at molslinjen},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction: 2024 franz edelman award for achievement in
advanced analytics, operations research, and management science.
<em>INFORMS Journal on Applied Analytics</em>, <em>55</em>(1), 1–4. (<a
href="https://doi.org/10.1287/inte.2024.intro.v55.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This special issue of the INFORMS Journal on Applied Analytics (formerly Interfaces ) is devoted to the finalists of the 2024 annual competition for the Franz Edelman Award for Achievement in Advanced Analytics, Operations Research, and Management Science, the profession’s most prestigious award for deployed work. As in previous years, the finalists this year cover a wide range of industries and functions.},
  archive  = {J},
  doi      = {10.1287/inte.2024.intro.v55.n1},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {1-4},
  title    = {Introduction: 2024 franz edelman award for achievement in advanced analytics, operations research, and management science},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mksc---14">MKSC - 14</h2>
<ul>
<li><details>
<summary>
(2025). Focus on authors. <em>MKSC</em>, <em>44</em>(1), 243–246.
(<a href="https://doi.org/10.1287/mksc.2025.focusonaus.v44.1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2025.focusonaus.v44.1},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {243-246},
  shortjournal = {Market. Sci.},
  title        = {Focus on authors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rejoinder: “Consumer uncertainty and purchase decision
reversals: Theory and evidence.” <em>MKSC</em>, <em>44</em>(1), 242. (<a
href="https://doi.org/10.1287/mksc.2024.1022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior research found that providing uncertainty-reducing information can increase product returns or service cancellations when judgments are reference-dependent and losses loom larger than gains. This paper revisits the analytical proof to demonstrate that the results hold when an implicit assumption is made explicit. History: Olivier Toubia served as the senior editor for this article.},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2024.1022},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {242},
  shortjournal = {Market. Sci.},
  title        = {Rejoinder: “Consumer uncertainty and purchase decision reversals: theory and evidence”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Erratum on “purchase decision reversals” model by shulman et
al. (2015). <em>MKSC</em>, <em>44</em>(1), 240–241. (<a
href="https://doi.org/10.1287/mksc.2024.0837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shulman et al. (2015) [Consumer uncertainty and purchase decision reversals: Theory and evidence. Marketing Sci . 34(4): 590–605.] provide theory and evidence that uncertainty-reducing information provided before the purchase decision can actually increase the number of decision reversals. In its current form, readers may get the impression that the main result of the model holds for all distributions of a relevant parameter in the model. However, we show that the main result may not hold for all distributions of the parameter. We then provide two potential resolutions that can serve to preserve the main result. History: Olivier Toubia served as the senior editor for this article. Funding: C. Li’s work was supported by the National Natural Science Foundation of China [Grant 72072051].},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2024.0837},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {240-241},
  shortjournal = {Market. Sci.},
  title        = {Erratum on “Purchase decision reversals” model by shulman et al. (2015)},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling behavioral dynamics in digital content consumption:
An attention-based neural point process approach with applications in
video games. <em>MKSC</em>, <em>44</em>(1), 220–239. (<a
href="https://doi.org/10.1287/mksc.2020.0180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The consumption of digital content products (e.g., video games and live streaming) is often associated with multifaceted, dynamically interacting consumer behavior that is subject to influence from pertinent external events. Inspired by these characteristics, we develop a novel attention-based neural point process approach to holistically capture the richness and complexity of consumer behavioral dynamics in modern digital content consumption. Our model features a new multirepresentational, continuous-time attention mechanism that can flexibly model dynamic interactions between different types of behavior under external influence. Using learned representations as sufficient statistics of past events, we build a marked point process to efficiently characterize the occurrence time, behavior combination, and consumption quantity of consumers’ future activities. We illustrate our model development and applications in the empirical context of a sports video game, showing its superior predictive performance over a wide range of baseline methods. Leveraging individual-level parameter estimates, we further demonstrate our model’s utility for conducting segmentation analysis and evaluating the effects of past events on consumers’ future engagement. Our model provides managers and practitioners with a powerful tool for developing more effective and targeted marketing strategies and gaining insights into consumer behavioral dynamics in digital content consumption. History: Yuxin Chen served as the senior editor. Funding: J. Yin was partly supported by the Adobe Digital Experience Research Award and the Amazon AWS Machine Learning Research Award. Y. (K.) Feng was supported by the Research Grants Council of Hong Kong [ECS Grant 25508819]. Y. Liu gratefully acknowledges the support from Bob Eckert, chairman of the board at Levi Struss and former CEO and chairman at Mattel, through the Robert A. Eckert endowed chair in marketing at the University of Arizona. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mksc.2020.0180 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2020.0180},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {220-239},
  shortjournal = {Market. Sci.},
  title        = {Modeling behavioral dynamics in digital content consumption: An attention-based neural point process approach with applications in video games},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A theory-based explainable deep learning architecture for
music emotion. <em>MKSC</em>, <em>44</em>(1), 196–219. (<a
href="https://doi.org/10.1287/mksc.2022.0323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a theory-based, explainable deep learning convolutional neural network (CNN) classifier to predict the time-varying emotional response to music. We design novel CNN filters that leverage the frequency harmonics structure from acoustic physics known to impact the perception of musical features. Our theory-based model is more parsimonious, but it provides comparable predictive performance with atheoretical deep learning models while performing better than models using handcrafted features. Our model can be complemented with handcrafted features, but the performance improvement is marginal. Importantly, the harmonics-based structure placed on the CNN filters provides better explainability for how the model predicts emotional response (valence and arousal) because emotion is closely related to consonance—a perceptual feature defined by the alignment of harmonics. Finally, we illustrate the utility of our model with an application involving digital advertising. Motivated by YouTube’s midroll ads, we conduct a laboratory experiment in which we exogenously insert ads at different times within videos. We find that ads placed in emotionally similar contexts increase ad engagement (lower skip rates and higher brand recall rates). Ad insertion based on emotional similarity metrics predicted by our theory-based, explainable model produces comparable or better engagement relative to atheoretical models. History: Tat Chan served as the senior editor. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0323 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0323},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {196-219},
  shortjournal = {Market. Sci.},
  title        = {A theory-based explainable deep learning architecture for music emotion},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online causal inference for advertising in real-time bidding
auctions. <em>MKSC</em>, <em>44</em>(1), 176–195. (<a
href="https://doi.org/10.1287/mksc.2022.0406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time bidding systems, which utilize auctions to allocate user impressions to competing advertisers, continue to enjoy success in digital advertising. Assessing the effectiveness of such advertising remains a challenge in research and practice. This paper proposes a new approach to perform causal inference on advertising bought through such mechanisms. Leveraging the economic structure of first- and second-price auctions, we establish novel results that show how the effects of advertising are connected to and, hence, identified from optimal bids. Importantly, we also outline the precise conditions under which these relationships hold. Because these optimal bids are required to estimate the effects of advertising, we present an adapted Thompson Sampling algorithm to solve a multiarmed bandit problem that succeeds in recovering such bids and, consequently, the effects of advertising, while minimizing the costs of experimentation. We also show that a greedy variant of this algorithm can perform just as well, if not better, when exploiting the structure of the model we consider. We use data from real-time bidding auctions to show that it outperforms commonly used methods to estimate the effects of advertising. History: Olivier Toubia served as the senior editor for this article. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0406 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0406},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {176-195},
  shortjournal = {Market. Sci.},
  title        = {Online causal inference for advertising in real-time bidding auctions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The interactions of customer reviews and price and their
dual roles in conveying quality information. <em>MKSC</em>,
<em>44</em>(1), 155–175. (<a
href="https://doi.org/10.1287/mksc.2022.0380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer reviews help communicate product information, but their effectiveness may suffer from selection bias (i.e., depending on factors, such as the individual experience and price, not all consumers may voluntarily write reviews). Consequently, a seller may have to resort to additional means (e.g., signaling through price in the context of an experience good) to convey its quality. This paper develops an analytical model to investigate the interaction of customer reviews and price with the presence of selection bias in marketing an experience good with uncertain quality to consumers. Our analysis reveals the dual roles played by both customer reviews and price in communicating quality information. On one hand, customer reviews may either directly convey product information with unbiased distribution of reviews or facilitate price signaling when reviews are biased because of selection. On the other hand, price may be adjusted to mitigate the selection bias of reviews to make them more informative, and it may also signal quality directly in the presence of review bias. As a result, we show that bias in reviews may actually benefit consumers without compromising information communication as the incentive to reduce review selection bias makes it credible and profitable for the high-quality seller to signal its type by undercutting the price that would be set if it is of low quality. We then extend our analysis to examine the information, profits, and welfare impacts of several important design elements of a review system as well as the impact of consumers’ aversion to risk. Finally, the implications of our findings on the management of user-generated content and pricing are discussed. History: Anthony Dukes served as the senior editor. Funding: J. Du is grateful for financial support from the Research Grants Council (RGC) of Hong Kong [Grant GRF/17501021]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2022.0380 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0380},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {155-175},
  shortjournal = {Market. Sci.},
  title        = {The interactions of customer reviews and price and their dual roles in conveying quality information},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Call me maybe: Does customer feedback seeking impact
nonsolicited customers? <em>MKSC</em>, <em>44</em>(1), 129–154. (<a
href="https://doi.org/10.1287/mksc.2023.0324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Academics and practitioners acknowledge the value of customer feedback in improving firm performance. Companies routinely solicit feedback from different customer subsets. However, the extent to which this feedback impacts nonsolicited customers depends on whether firms implement meaningful business-level changes that resonate with customers. This paper assesses customer feedback’s impact on firm learning and business improvements as well as its spillover effects on nonsolicited customers using a randomized, controlled field experiment conducted in Rwanda over two years. We hypothesize that private feedback seeking could operate through two broad mechanisms: (a) directly influencing solicited customers and/or (b) prompting firms to improve their offerings, leading to spillover effects on other customers. Our results demonstrate a 38.2% increase in recall and a 77.4% increase in purchases for customers not engaged in the feedback process. The analysis further suggests that business-level changes driven by customer feedback fuel these spillovers. Additionally, customer feedback seeking significantly improves treatment firm performance, resulting in a 62.0% revenue increase and 54.5% profit increase compared with control firms. Our study also introduces a basic customer feedback-seeking technology for small businesses to improve performance. These findings can guide firms in leveraging customer feedback to undertake business changes and generate greater revenues/profits. History: Olivier Toubia served as the senior editor for this article. Funding: This research was supported by grants from the Stanford King Center on Global Development, the Polsky Center for Entrepreneurship and Innovation (Chicago Booth), the Rustandy Center for Social Sector Innovation (Chicago Booth), the London School of Economics and Political Science, the Stanford Graduate School of Business, the John A. and Cynthia Fry Gunn Faculty Scholar award (Stanford), the UK Department for International Development (DFID) and Economic and Social Research Council’s (ESRC) joint Growth Research Program, Private Enterprise Development in Low-Income Countries (PEDL), and the Fama-Miller Center for Research in Finance. Further support is acknowledged from the Chicago Booth Kilts Center for Marketing and the Leonard L. Berry Chair in Services Marketing at Mays Business School. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0324 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0324},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {129-154},
  shortjournal = {Market. Sci.},
  title        = {Call me maybe: Does customer feedback seeking impact nonsolicited customers?},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating parameters of structural models using neural
networks. <em>MKSC</em>, <em>44</em>(1), 102–128. (<a
href="https://doi.org/10.1287/mksc.2022.0360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io . History: Manchanda Puneet served as the senior editor. Supplemental Material: The data files are available at https://doi.org/10.1287/mksc.2022.0360 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0360},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {102-128},
  shortjournal = {Market. Sci.},
  title        = {Estimating parameters of structural models using neural networks},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The value of platform endorsement. <em>MKSC</em>,
<em>44</em>(1), 84–101. (<a
href="https://doi.org/10.1287/mksc.2022.0226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many digital platforms with large product assortments endorse a select group of items to facilitate user choice. However, although it seems intuitive that such endorsement may increase the sales of endorsed items, little is known about its effect on unendorsed items and on the platform. Using data from a field experiment conducted by an online freelance platform, we examine the effect of exposure to platform endorsement on user search and purchase behavior. We find that exposure to platform endorsement increases user search and purchases not only for endorsed services but also, for unendorsed services. We link the increase in search and purchases to an increase in the perception of the quality of services offered on the platform. We further explore heterogeneity in the effect of platform endorsement and find that the effect of exposure to platform endorsement on purchase is more pronounced for users with a higher propensity to purchase. We discuss implications for platforms, merchants, and regulators. History: Olivier Toubia served as the senior editor. Funding: The authors acknowledge the support of London Business School’s Research and Materials Development Fund for this research. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0226 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0226},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {84-101},
  shortjournal = {Market. Sci.},
  title        = {The value of platform endorsement},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymmetric impact of matching technology on influencer
marketing: Implications for platform revenue. <em>MKSC</em>,
<em>44</em>(1), 65–83. (<a
href="https://doi.org/10.1287/mksc.2023.0211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the impact of using advanced technology such as artificial intelligence (AI) to match marketers with social media influencers. We develop a theoretical model to examine how matching accuracy affects the competition between influencers and the profitability of a social media platform. Our findings show that improving matching accuracy may not always benefit the platform, especially for platforms with intermediate follower density. Two opposing effects of technology improvement affect the prices of influencer marketing campaigns: advanced technology, such as AI, enhances the matching between influencers and marketers and also intensifies competition between different types of influencers. The overall effect on prices can be negative for some influencers because of the asymmetric nature of such matching technology: the matching outcome for influencers with a narrower audience (niche influencers) is more sensitive to matching accuracy than that for those with a broader audience (general influencers). As a result, more niche influencers begin to participate in marketing campaigns when matching accuracy improves, which reduces the prices offered by sufficiently general influencers and may lead to a decline in platform revenue. Additionally, we find that adjusting commission rates in response to technology improvements could help mitigate the negative impact although it may not eliminate it entirely. Our findings offer valuable insights for social media platforms seeking to remain competitive in the influencer marketing landscape. History: Anthony Dukes served as the senior editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.0211 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0211},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {65-83},
  shortjournal = {Market. Sci.},
  title        = {Asymmetric impact of matching technology on influencer marketing: Implications for platform revenue},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What drives demand for playlists on spotify? <em>MKSC</em>,
<em>44</em>(1), 54–64. (<a
href="https://doi.org/10.1287/mksc.2022.0273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide estimates of the drivers of playlist followers on Spotify. We base our analysis on a unique panel data set for 30,000+ popular playlists and combine it with data on how prominently these playlists are featured in the Spotify app. Using two-way fixed effects and staggered synthetic difference in difference models, we compare the short-term effect of two important demand factors in our data—featuring playlists on Spotify’s Search Page and adding songs by exceptionally popular major label artists to playlists. We find that users prefer to follow playlists featured in the app. According to our estimates, being featured on the Search Page raises daily playlist followers by 0.95%—which is about two times larger than the effect on followers of including a song by an exceptionally popular major label artist (0.45%). Our examination of playlist demand has two important implications. First, Spotify can effectively guide user attention to certain playlists, supporting industry executives’ and artists’ concerns that the platform has the potential to favor some producers by selectively promoting their content. Second, popular artists signed with major labels play an important role in attracting followers to playlists on Spotify. History: Catherine Tucker served as the senior editor for this article. Funding: This work was supported by the Dutch Research Council [NWO 451-17-028] and the Tilburg Science Hub. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mksc.2022.0273 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0273},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {54-64},
  shortjournal = {Market. Sci.},
  title        = {What drives demand for playlists on spotify?},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can AI and AI-hybrids detect persuasion skills? Salesforce
hiring with conversational video interviews. <em>MKSC</em>,
<em>44</em>(1), 30–53. (<a
href="https://doi.org/10.1287/mksc.2023.0149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an AI and AI-human-based model for salesforce hiring using recordings of conversational video interviews that involve two-sided, back-and-forth interactions with messages conveyed through multiple modalities (text, voice, and body language). We derive objective, theory-backed measures of sales performance from these modalities, leveraging recent advances in body language analysis, conversational analysis, and large language models (LLMs). These measures serve as explanatory variables in our AI model. Our key contribution to the broader research on persuasion and influence is that we show how to use conversational videos to capture features related to (i) two-way conversational interactivity; (ii) real-time adaptation; and (iii) human body language, with minimal measurement error relative to extant survey-based approaches that suffer from recall biases. We use rubric-based scores by panels of sales professionals (correlated with hiring decisions) to isolate a candidate’s “latent sales ability” and use these as outcome variables to be predicted by the AI model. The AI model achieves reasonable predictive accuracy, yet incorporating human judgments into an AI-human hybrid model enhances its effectiveness—improving workforce quality by 67% over random selection. Although the content of what is spoken is most important in prediction, conversational interactivity, sellers’ real-time adaptation to the buyer, and body language also have good explanatory power. Finally, in terms of performance-cost trade-offs, the addition of just one human professional evaluation in the hiring loop in combination with AI is optimal. Further, using human input based on only the two early stages of the interview in a task-based hybrid model is the most cost-effective in improving performance. History: Tat Chan served as the senior editor. Funding: K. Chiong and H. Dover received financial support for this work from the NEC Foundation of America. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0149 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0149},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {30-53},
  shortjournal = {Market. Sci.},
  title        = {Can AI and AI-hybrids detect persuasion skills? salesforce hiring with conversational video interviews},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommending for a multi-sided marketplace: A
multi-objective hierarchical approach. <em>MKSC</em>, <em>44</em>(1),
1–29. (<a href="https://doi.org/10.1287/mksc.2022.0238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems play a vital role in driving the long-term values for online platforms. However, developing recommender systems for multi-sided platforms faces two prominent challenges. First, recommending for multi-sided platforms typically involves a joint optimization of multiple, potentially conflicting objectives. Second, many platforms adopt hierarchical homepages, where items can either be individual products or groups of products. Off-the-shelf recommendation algorithms are not applicable in these settings. To address these challenges, we propose MOHR, a novel multi-objective hierarchical recommender. By combining machine learning, probabilistic hierarchical aggregation, and multi-objective optimization, MOHR efficiently solves the multi-objective ranking problem in a hierarchical setting through an innovative formulation of probabilistic consumer behavior modeling and constrained optimization. We implemented MOHR at Uber Eats, one of the world’s largest food delivery platforms. Online experiments showed significant improvements in consumer conversion, retention, and gross bookings, resulting in a $1.5 million weekly increase in revenue. Moreover, MOHR offers managers a mathematically principled tool to make quantifiable and interpretable trade-offs across multiple objectives. As a result, it has been deployed globally as the recommender system for Uber Eats’ app homepage. History: Puneet Manchanda served as the senior editor for this article. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mksc.2022.0238 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0238},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Market. Sci.},
  title        = {Recommending for a multi-sided marketplace: A multi-objective hierarchical approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ms---41">MS - 41</h2>
<ul>
<li><details>
<summary>
(2025). Publicly traded debt restructuring methods, corporate
investment, and debt contracting. <em>MS</em>, <em>71</em>(2),
1846–1863. (<a href="https://doi.org/10.1287/mnsc.2022.01831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I use an important ruling, Marblegate Asset Management v. Education Management Corporation (EMC), to study the economic role of two publicly traded debt restructuring methods: coercive bond exchange offers and Chapter 11. This ruling restricted firms from employing coercive bond exchange offers to facilitate out-of-court restructurings, thereby increasing the likelihood of restructuring publicly traded debt under Chapter 11. Following the ruling, investment in affected distressed firms decreased substantially, but investment efficiency improved. The changes in covenant, maturity, and offering yield of newly issued bonds suggested that existing bondholders with covenants gained more bargaining power than shareholders and new bondholders. The paper provides causal evidence from a large sample analysis, demonstrating the divergent effects of these two publicly traded debt restructuring methods on investment policies. This paper was accepted by Victoria Ivashina, finance. Funding: Funding for this research was provided by University of Pittsburgh (Doctoral Fellowship) and Vanderbilt University (standard research funds). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01831 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01831},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1846-1863},
  shortjournal = {Manag. Sci.},
  title        = {Publicly traded debt restructuring methods, corporate investment, and debt contracting},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The early exercise risk premium. <em>MS</em>,
<em>71</em>(2), 1824–1845. (<a
href="https://doi.org/10.1287/mnsc.2023.00440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the asset pricing implications of being able to optimally early exercise plain vanilla puts, contrasting expected raw and delta-hedged returns across equivalent American and European puts. Our theory suggests that American puts yield less negative raw but more negative delta-hedged expected returns than equivalent European puts. The raw (delta-hedged) spread widens with a higher early exercise probability as induced through, for example, moneyness, time to maturity, and underlying asset volatility (variance and jump risk premiums). An empirical comparison of single-stock American puts with equivalent synthetic European puts formed from put–call parity supports our theory if and only if we allow for optimal early exercises in our return calculations. More strikingly, allowing for optimal early exercises significantly alters the profitability of 14 out of 15 well-known option anomalies with the average absolute change equal to 33% and five anomalies becoming insignificant. This paper was accepted by Lukas Schmid, finance. Supplemental Material: The internet appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00440 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00440},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1824-1845},
  shortjournal = {Manag. Sci.},
  title        = {The early exercise risk premium},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk management with variable capital utilization and
time-varying collateral capacity. <em>MS</em>, <em>71</em>(2),
1803–1823. (<a href="https://doi.org/10.1287/mnsc.2022.00415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build a risk management model that incorporates variable capital utilization and time-varying collateral capacity. The former lets firms optimally choose capital utilization, and hence production, which affects capital depreciation and risk exposure. The latter means firms’ ability to borrow and hedge increases with expected earnings and thus utilization. Calibrated solutions show both ingredients matter for firm value. We test the novel model predictions using a new data set of oil and gas producers. Consistent with model predictions, we find utilization is negatively correlated with firm liquidity, while hedging is positively correlated with liquidity and expected profitability. This paper was accepted by Lukas Schmid, finance. Funding: Lu and Vij acknowledge financial support from the Terry-Sanford Research Award, University of Georgia. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2022.00415 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00415},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1803-1823},
  shortjournal = {Manag. Sci.},
  title        = {Risk management with variable capital utilization and time-varying collateral capacity},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-adapting network relaxations for weakly coupled markov
decision processes. <em>MS</em>, <em>71</em>(2), 1779–1802. (<a
href="https://doi.org/10.1287/mnsc.2022.01108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional weakly coupled Markov decision processes (WDPs) arise in dynamic decision making and reinforcement learning, decomposing into smaller Markov decision processes (MDPs) when linking constraints are relaxed. The Lagrangian relaxation of WDPs (LAG) exploits this property to compute policies and (optimistic) bounds efficiently; however, dualizing linking constraints averages away combinatorial information. We introduce feasibility network relaxations (FNRs), a new class of linear programming relaxations that exactly represents the linking constraints. We develop a procedure to obtain the unique minimally sized relaxation, which we refer to as self-adapting FNR, as its size automatically adjusts to the structure of the linking constraints. Our analysis informs model selection: (i) the self-adapting FNR provides (weakly) stronger bounds than LAG, is polynomially sized when linking constraints admit a tractable network representation, and can even be smaller than LAG, and (ii) self-adapting FNR provides bounds and policies that match the approximate linear programming (ALP) approach but is substantially smaller in size than the ALP formulation and a recent alternative Lagrangian that is equivalent to ALP. We perform numerical experiments on constrained dynamic assortment and preemptive maintenance applications. Our results show that self-adapting FNR significantly improves upon LAG in terms of policy performance and/or bounds, while being an order of magnitude faster than an alternative Lagrangian and ALP, which are unsolvable in several instances. This paper was accepted by Baris Ata, stochastic models and simulation. Supplemental Material: The electronic companion and data files are available at https://doi.org/10.1287/mnsc.2022.01108 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01108},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1779-1802},
  shortjournal = {Manag. Sci.},
  title        = {Self-adapting network relaxations for weakly coupled markov decision processes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intangible capital in factor models. <em>MS</em>,
<em>71</em>(2), 1756–1778. (<a
href="https://doi.org/10.1287/mnsc.2022.01261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition from a traditional manufacturing-based economy to a knowledge- and service-based economy over recent decades resulted in a considerable rise in intangible capital, most of which is not reported on companies’ balance sheets. As a result, balance sheet-based valuation ratios, investment measures, and other firm characteristics that do not incorporate off-balance sheet (OBS) intangible capital suffer from significant measurement error problems. We incorporate a new measure of OBS intangible capital into firm characteristics, such as book to market, investment, and profitability, to address these measurement errors. These OBS intangible adjustments improve the performance of the Fama–French three- and five-factor models and the q -factor model, especially during recent decades. We further find that the value factor is no longer redundant in these empirical factor models. This paper was accepted by Lukas Schmid, finance. Funding: The authors are grateful for financial support from the 2019 EDHEC Scientific Beta “Advanced ESG &amp; Factor Investing” Research Chair. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01261 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01261},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1756-1778},
  shortjournal = {Manag. Sci.},
  title        = {Intangible capital in factor models},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A procedure for revising data-based priors in a group.
<em>MS</em>, <em>71</em>(2), 1737–1755. (<a
href="https://doi.org/10.1287/mnsc.2022.02912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A group of individuals is interested in predicting the outcome of a current problem. Each individual has access to private data that are used to form a prior probability over possible outcomes. While individuals may be reluctant or unable to disclose their private data, they are willing to publish their priors. We characterize a procedure for revising the individuals’ prior probabilities based on the published priors of others and discuss its implications regarding the formation of a common prior in the group, both in the short and long run. This paper was accepted by Manel Baucells, behavioral economics and decision analysis. Funding: G. Gayer gratefully acknowledges support from ISF [Grant 1443/20].},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02912},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1737-1755},
  shortjournal = {Manag. Sci.},
  title        = {A procedure for revising data-based priors in a group},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detours in shared rides. <em>MS</em>, <em>71</em>(2),
1716–1736. (<a href="https://doi.org/10.1287/mnsc.2020.03125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detours are considered key for the efficient operation of a shared rides service, but they are also a major pain point for consumers of such services. This paper studies the relationship between the value generated by shared rides and the detours they create for riders. We establish a limit on the sum of value and detour, and we prove that this leads to a tight bound on the Pareto frontier of values and detours in a general setting with an arbitrary number of requests. We explicitly compute the Pareto frontier for one family of city topologies and construct it via simulation for several more networks, including one based on ride-sharing data from commute hours in Manhattan. We find that average detours are usually small, even in low-demand-density settings. We also find that by carefully choosing the match objective, detours can be reduced with a relatively small impact on values and that the density of ride requests is far more important than detours for the effective operations of a shared rides service. In response, we propose that platforms implement a two-product version of shared rides and limit the worst-case detours of its users. This paper was accepted by Hamid Nazerzadeh, data science. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2020.03125 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2020.03125},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1716-1736},
  shortjournal = {Manag. Sci.},
  title        = {Detours in shared rides},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Industry peer information and the equity valuation accuracy
of firms emerging from chapter 11. <em>MS</em>, <em>71</em>(2),
1692–1715. (<a href="https://doi.org/10.1287/mnsc.2022.01233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Valuation plays a central role in determining Chapter 11 reorganization outcomes. However, obtaining accurate valuation estimates of reorganized firms is challenging because of limited firm-specific market-based information and the oft-conflicting incentives of claimholders. We examine the role of industry peer information in reducing misvaluations and its implications for unintended interclaimant wealth transfers and postreorganization performance. First, we find that the availability of relevant industry peer information is negatively associated with equity valuation errors for firms emerging from Chapter 11. Cross-sectional results suggest that the relation between industry peer information and valuation errors varies substantially with debtors’ information environment and case characteristics. Second, we find that industry peer information quality is associated with better ex post financial performance of emerged firms because of lower overvaluation. Finally, we document the role of industry peer information in substantially reducing the frequency and magnitude of unintended wealth transfers between claimants arising from equity valuation errors. This paper was accepted by Suraj Srinivasan, accounting. Funding: The authors appreciate financial support from the Social Sciences and Humanities Research Council of Canada [Grant 435-2020-0583] and the Canadian Academic Accounting Association. B. Fang acknowledges financial support from the Della Suantio Fellowship. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01233 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01233},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1692-1715},
  shortjournal = {Manag. Sci.},
  title        = {Industry peer information and the equity valuation accuracy of firms emerging from chapter 11},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The pitfalls of review solicitation: Evidence from a natural
experiment on TripAdvisor. <em>MS</em>, <em>71</em>(2), 1671–1691. (<a
href="https://doi.org/10.1287/mnsc.2023.01006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the effect of firms’ participation in platform-endorsed review solicitation programs on consumers’ online review generation. We leverage a natural experiment on TripAdvisor, which launched a review solicitation program that allows hotels to collect reviews directly from guests after their stays with the aid of certified connectivity partners. Applying a two-stage difference-in-differences approach to a panel data set of online reviews for a matched set of hotels across TripAdvisor and Expedia, we find that hotels’ participation in the review solicitation program results in a 34.3% increase in review volume, a 0.151 increase in review rating, but a 16.9% decrease in review length. Review solicitation, however, generates a notable negative spillover effect on the volume of organic reviews. Specifically, the volume of organic reviews is reduced by 15.5% after hotels start soliciting reviews. We provide evidence that the motivational crowding-out effect plays an important role in driving this negative spillover. Further analyses reveal that the effects of review solicitation are heterogeneous with respect to hotels of different types and consumers with different demographic and behavioral characteristics. Finally, using a novel structural topic model, we detect a significant shift in review content from specific and concrete topics to general and abstract topics. Our findings suggest that review platforms and firms should be cautious about the unintended negative consequences of review solicitation on consumers’ review generation. This paper was accepted by Hemant Bhargava, information systems. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72371192, 72132008, 71872061, and 72061127002] and the Humanities and Social Science Fund of Ministry of Education of China [Grant 22YJA630021]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01006 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01006},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1671-1691},
  shortjournal = {Manag. Sci.},
  title        = {The pitfalls of review solicitation: Evidence from a natural experiment on TripAdvisor},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tech-enabled financial data access, retail investors, and
gambling-like behavior in the stock market. <em>MS</em>, <em>71</em>(2),
1646–1670. (<a href="https://doi.org/10.1287/mnsc.2021.01379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in technology have reduced information acquisition costs, creating an improved information environment for retail investors. Specifically, new technologies, such as application programming interface (API), deliver high-volume, institutional-like raw data directly to Main Street investors. Although greater availability of information can be beneficial, it may also exacerbate retail investors’ existing trading deficiencies. Exploiting the sudden shutdown of Yahoo! Finance API, the largest free API for retail investors, this study examines how access to tech-enabled raw financial data affects retail investment. We find that retail trading volumes in stocks favored by active retail investors dropped by 8.6%–10.5% within one month of the API shutdown. The remaining retail trades collectively became more predictive of future returns, suggesting less gambling-like behavior after the API shutdown. Moreover, our randomized controlled experiment affirms the underlying mechanism: tech-enabled access to high-volume historical price data increases individuals’ overconfidence, which further leads them to engage in excessive trading. The study reveals an unintended consequence of technology-led, wider data access for retail investors. This paper was accepted by D. J. Wu, information systems. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.01379 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01379},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1646-1670},
  shortjournal = {Manag. Sci.},
  title        = {Tech-enabled financial data access, retail investors, and gambling-like behavior in the stock market},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investor sentiment and the pricing of macro risks for hedge
funds. <em>MS</em>, <em>71</em>(2), 1623–1645. (<a
href="https://doi.org/10.1287/mnsc.2022.02792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hedge funds with larger macroeconomic-risk betas do not earn higher returns, in contrast to the theoretically predicted risk-return trade-off. Meanwhile, high macro-beta funds deliver higher returns than low macro-beta funds following a low-sentiment period, whereas the risk-return relation is flat following a high-sentiment period. We show that the sophisticated management of hedge funds explains this pattern. The relation between funds’ macro-risk betas and the timing abilities/investor flows is sentiment dependent, and such variation likely drives the contrasting beta-return trade-offs after high- and low-sentiment periods. A similar pattern is also observed in mutual funds. This paper was accepted by Lin William Cong, finance. Funding: X. Zhu acknowledges financial support from the National Natural Science Foundation of China [Grant 72203035] and the Ministry of Education Project of Humanities and Social Sciences [Grant 22YJC790194]. Z. Chen acknowledges financial support from the National Natural Science Foundation of China [Grant 72222004] and Tsinghua University [Grant 20225080020]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02792 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02792},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1623-1645},
  shortjournal = {Manag. Sci.},
  title        = {Investor sentiment and the pricing of macro risks for hedge funds},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explicit and implicit belief-based gender discrimination: A
hiring experiment. <em>MS</em>, <em>71</em>(2), 1600–1622. (<a
href="https://doi.org/10.1287/mnsc.2022.01229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a key element of discrimination, namely, when stereotypes translate into discriminatory actions. Using a hiring experiment, we rule out taste-based discrimination by design and test for the presence of two types of belief-based gender discrimination. We document evidence of explicit discriminators —individuals who are willing to discriminate even when their hiring choices are highly revealing of their gender-biased beliefs. Crucially, we also identify implicit discriminators —individuals who do not discriminate against women when taking a discriminatory action is highly revealing of their biased beliefs, but do discriminate against women when their biased motive is obscured. Our analysis highlights the central role played by features of the choice environment in determining whether and how discrimination will manifest. We conclude by discussing the implications for policy design. This paper was accepted by Marie Claire Villeval, behavioral economics and decision analysis. Funding: K. Barron and S. Schweighofer-Kodritsch gratefully acknowledge financial support from the Deutsche Forschungsgemeinschaft through CRC TRR 190 [Grant 280092119]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01229 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01229},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1600-1622},
  shortjournal = {Manag. Sci.},
  title        = {Explicit and implicit belief-based gender discrimination: A hiring experiment},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinated inattention and disclosure complexity.
<em>MS</em>, <em>71</em>(2), 1581–1599. (<a
href="https://doi.org/10.1287/mnsc.2021.01029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine a beauty contest game with an option to analyze an additional disclosure. We analytically prove that in some scenarios, coordination incentives cause sophisticated players who can comprehend disclosures to choose not to analyze them to match unsophisticated players’ actions, a phenomenon we call “coordinated inattention.” Laboratory experiments provide support for the coordinated inattention mechanism: Coordination incentives reduce sophisticated subjects’ propensity to analyze disclosures, especially when they believe others are unlikely to comprehend them. We further find that psychological biases help reduce coordinated inattention. Subjects are overconfident, sophisticated subjects overestimate others’ ability to comprehend disclosures, and both biases are associated with a higher tendency to analyze disclosures. Our analysis suggests that unsophisticated decision makers’ inability to comprehend complex disclosures has a negative spillover effect by reducing sophisticated decision makers’ attention to disclosures. Our results highlight the importance of the recent efforts of the Securities and Exchange Commission (SEC) and the Financial Accounting Standards Board (FASB) to make disclosures easier to comprehend. This paper was accepted by Brian Bushee, accounting. Funding: This study involved no funding except the payments made to the experimental subjects; these funds were provided by Penn State University. Supplemental Material: The online appendix and electronic companion are available at https://doi.org/10.1287/mnsc.2021.01029 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01029},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1581-1599},
  shortjournal = {Manag. Sci.},
  title        = {Coordinated inattention and disclosure complexity},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free nonstationary reinforcement learning:
Near-optimal regret and applications in multiagent reinforcement
learning and inventory control. <em>MS</em>, <em>71</em>(2), 1564–1580.
(<a href="https://doi.org/10.1287/mnsc.2022.02533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider model-free reinforcement learning (RL) in nonstationary Markov decision processes. Both the reward functions and the state transition functions are allowed to vary arbitrarily over time as long as their cumulative variations do not exceed certain variation budgets. We propose Restarted Q-Learning with Upper Confidence Bounds (RestartQ-UCB), the first model-free algorithm for nonstationary RL, and show that it outperforms existing solutions in terms of dynamic regret. Specifically, RestartQ-UCB with Freedman-type bonus terms achieves a dynamic regret bound of O ˜ ( S 1 3 A 1 3 Δ 1 3 H T 2 3 ) , where S and A are the numbers of states and actions, respectively, Δ &gt; 0 is the variation budget, H is the number of time steps per episode, and T is the total number of time steps. We further present a parameter-free algorithm named Double-Restart Q-UCB that does not require prior knowledge of the variation budget. We show that our algorithms are nearly optimal by establishing an information-theoretical lower bound of Ω ( S 1 3 A 1 3 Δ 1 3 H 2 3 T 2 3 ) , the first lower bound in nonstationary RL. Numerical experiments validate the advantages of RestartQ-UCB in terms of both cumulative rewards and computational efficiency. We demonstrate the power of our results in examples of multiagent RL and inventory control across related products. This paper was accepted by Omar Besbes, revenue management and market analytics. Funding: The research of D. Simchi-Levi and R. Zhu was supported by the MIT Data Science Laboratory. The research of W. Mao, K. Zhang, and T. Başar was supported in part by the U.S. Army Research Laboratory (ARL) Cooperative Agreement W911NF-17-2-0196, in part by the Office of Naval Research (ONR) [MURI Grant N00014-16-1-2710], and in part by the Air Force Office of Scientific Research (AFOSR) [Grant FA9550-19-1-0353]. K. Zhang also acknowledges support from U.S. Army Research Laboratory (ARL) [Grant W911NF-24-1-0085]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02533 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02533},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1564-1580},
  shortjournal = {Manag. Sci.},
  title        = {Model-free nonstationary reinforcement learning: Near-optimal regret and applications in multiagent reinforcement learning and inventory control},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The algorithmic assignment of incentive schemes.
<em>MS</em>, <em>71</em>(2), 1546–1563. (<a
href="https://doi.org/10.1287/mnsc.2022.03362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assignment of individuals with different observable characteristics to different treatments is a central question in designing optimal policies. We study this question in the context of increasing workers’ performance via targeted incentives using machine learning algorithms with worker demographics, personality traits, and preferences as input. Running two large-scale experiments, we show that (i) performance can be predicted by accurately measured worker characteristics, (ii) a machine learning algorithm can detect heterogeneity in responses to different schemes, (iii) a targeted assignment of schemes to individuals increases performance significantly above the level of the single best scheme, and (iv) algorithmic assignment is more effective for workers who have a high likelihood to repeatedly interact with the employer or who provide more consistent survey answers. This paper was accepted by Yan Chen, behavioral economics and decision analysis. Funding: Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy [Grant EXC 2126/1-390838866]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.03362 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.03362},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1546-1563},
  shortjournal = {Manag. Sci.},
  title        = {The algorithmic assignment of incentive schemes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Employment protection and venture capital investment: The
impact of wrongful discharge laws. <em>MS</em>, <em>71</em>(2),
1523–1545. (<a href="https://doi.org/10.1287/mnsc.2023.01936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wrongful discharge laws (WDLs) provide limits to the employment-at-will doctrine, and thus impair operating flexibility, increasing expected financial distress costs by making it costly to fire employees. This impairment is detrimental to start-ups, leading to a decline in venture capital (VC) investment. Using a difference-in-differences framework enabled by the staggered adoption of WDLs across the U.S. states, we show VC investment declines after a state adopts the good faith exception (the strongest form of WDL). This decline is most pronounced in sectors with high labor dependency. This paper was accepted by Victoria Ivashina, finance. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2023.01936 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01936},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1523-1545},
  shortjournal = {Manag. Sci.},
  title        = {Employment protection and venture capital investment: The impact of wrongful discharge laws},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using machine learning to measure conservatism. <em>MS</em>,
<em>71</em>(2), 1504–1522. (<a
href="https://doi.org/10.1287/mnsc.2024.4983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an approach to measure conservatism using machine learning techniques that are not constrained by functional form restrictions. We extend the differential timeliness model to allow for observable characteristics related to conservatism to follow nonlinear relationships. By developing machine learning measures of conservatism, we draw attention to potential benefits and drawbacks and show how its insights complement conventional measures. Our broader goal is to investigate the effectiveness of machine learning algorithms for filtering noise in traditional archival studies and uncovering more complex empirical patterns. This paper was accepted by Suraj Srinivasan, accounting. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2024.4983 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.4983},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1504-1522},
  shortjournal = {Manag. Sci.},
  title        = {Using machine learning to measure conservatism},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dueling contests and platform’s coordinating role.
<em>MS</em>, <em>71</em>(2), 1488–1503. (<a
href="https://doi.org/10.1287/mnsc.2021.03973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing platforms typically take a passive approach, and they let the competing firms freely design their own contests and allow every solver to self-select and join any of the concurrently running contests. In a model of competing noise-driven contests, we show that the duopoly prize allocation has fewer (but larger) prizes compared with a monopolist contest designer. We also find that contests with firm-chosen budgets and solvers’ endogenous participation create coordination inefficiencies. Thus, platform policies that constrain the competing firms from freely choosing their budgets and offer solvers non-enforceable recommendations toward specific noise-driven contests strictly enhance total welfare. Extending our framework to include arbitrarily correlated ability-driven contests, we highlight the critical role of inter-contest dependence on the efficacy of a platform’s interventions. Specifically, platform nudges to improve solver-contest (mis)matches are welfare enhancing only when the contests are sufficiently related, and allowing solvers to self-sort is appropriate otherwise. This paper was accepted by Gabriel Weintraub, revenue management and market analytics. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2021.03973 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03973},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1488-1503},
  shortjournal = {Manag. Sci.},
  title        = {Dueling contests and platform’s coordinating role},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Congressional apportionment: A multiobjective optimization
approach. <em>MS</em>, <em>71</em>(2), 1464–1487. (<a
href="https://doi.org/10.1287/mnsc.2023.02472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two events, with major implications for U.S. voters, occur after each decennial census. First, congressional “apportionment” takes place, followed by congressional “districting.” Apportionment determines how to allocate the 435 seats in the House of Representatives across the 50 states, whereas districting determines the geographic boundaries assigned to representatives within each state. Although districting and the practice of gerrymandering often receive great attention in the media and courts, the best way to apportion representatives across states has been debated for nearly 250 years. Historical methods (including the current method) each satisfy some desirable optimality criteria that the others are not guaranteed to satisfy. Moreover, none are guaranteed to optimize certain reasonable fairness measures (e.g., minimum range, minimum bias). To our knowledge, we are the first to formulate and analyze a multiobjective optimization approach to apportionment, allowing policymakers to identify Pareto-optimal allocations and quantify their trade-offs between several competing criteria. Some of these models can be formulated and solved as mixed-integer linear programs, whereas others require the solution of mixed-integer, nonconvex, quadratically constrained quadratic programs. We take advantage of recent software advances that allow one to solve these problems with optimality guarantees. Policy implications of our work include Pareto curves from historical censuses and simulations, which suggest opportunities for improvement in some objectives at little sacrifice to others. This paper was accepted by David Simchi-Levi, operations management. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02472 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02472},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1464-1487},
  shortjournal = {Manag. Sci.},
  title        = {Congressional apportionment: A multiobjective optimization approach},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Target firm advertising and firm value. <em>MS</em>,
<em>71</em>(2), 1438–1463. (<a
href="https://doi.org/10.1287/mnsc.2022.01534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistent with hypotheses underlying firm advertising, we find that targets with pretakeover advertising obtain higher premiums, whereas their acquirers earn lower announcement returns. These economically significant effects suggest that through advertising, targets increase their profile and negotiating power. Further, targets that advertise are more likely to initiate their takeovers, attract multiple bidders, receive enhanced bids, capture more merger rents, and even in failed acquisitions, experience a 1% permanent revaluation. The latter result differentiates between information asymmetry and behavioral explanations for the target advertising. Overall, the results support the hypothesis that management advertises to transmit information to investors and potential acquirers. This paper was accepted by Victoria Ivashina, finance. Funding: A. L. Tran acknowledges financial support from the Mergers and Acquisitions Research Centre at Bayes Business School. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01534 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01534},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1438-1463},
  shortjournal = {Manag. Sci.},
  title        = {Target firm advertising and firm value},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentivizing organ donation under different priority rules:
The role of information. <em>MS</em>, <em>71</em>(2), 1418–1437. (<a
href="https://doi.org/10.1287/mnsc.2022.01530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the incentive to register for deceased organ donation under alternative organ allocation priority rules, which may prioritize registered donors and/or patients with higher valuations for organ transplantation. Specifically, the donor priority rule grants higher priority on the organ waiting list to those who have previously registered as donors. The dual-incentive priority rules allocate organs based on donor status, followed by individual valuations within the same donor status, or vice versa. Both theoretical and experimental results suggest that the efficacy of the donor priority rule and the dual-incentive priority rules critically depends on the information environment. When organ transplantation valuations are unobservable prior to making donation decisions, the hybrid dual-incentive rules generate higher donation rates. In contrast, if valuations are observable, the dual-incentive priority rules create unbalanced incentives between high- and low-value agents, potentially undermining the efficacy of the hybrid dual-incentive rules in increasing overall donation rates. This paper was accepted by Marie Claire Villeval, behavioral economics and decision analysis. Funding: This research is supported by the National Natural Science Foundation of China [Grants 72173103, 72373127, and 71988101], the Singapore Ministry of Education (MOE) Academic Research Fund Tier 1 [RG57/20], and the Open Foundation of Key Laboratory of Interdisciplinary Research of Computation and Economics (Shanghai University of Finance and Economics), Ministry of Education of China. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2022.01530 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01530},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1418-1437},
  shortjournal = {Manag. Sci.},
  title        = {Incentivizing organ donation under different priority rules: The role of information},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidder-specific synergies and the evolution of acquirer
returns. <em>MS</em>, <em>71</em>(2), 1391–1417. (<a
href="https://doi.org/10.1287/mnsc.2022.02208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Largely constant average acquirer returns over the past four decades mask fundamental changes in the takeover market. Controlling for bidder composition, the common component of acquirer returns has increased by five percentage points relative to the 1980s. Offsetting this increase, the average bidder-specific component has declined. We propose a theory of bidder-specific synergies to help interpret these opposing trends. In our theory and in the data, acquirer returns increase with the extent to which synergies are unique to that bidder. The composition effect reflects bidder uniqueness. Overall, the evidence is consistent with rising merger synergies that have become less bidder specific. This paper was accepted by Victoria Ivashina, finance. Funding: A. Golubov acknowledges financial support from the Bank of Canada [Governor’s Award]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02208 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02208},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1391-1417},
  shortjournal = {Manag. Sci.},
  title        = {Bidder-specific synergies and the evolution of acquirer returns},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is blockholder diversity detrimental? <em>MS</em>,
<em>71</em>(2), 1356–1390. (<a
href="https://doi.org/10.1287/mnsc.2023.00528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We find that, overall, blockholder diversity, i.e., the firm shareholder base including several different types of blocks, is detrimental to firm performance. We show that lagged disclosure, on exogenous predetermined dates, that reveals an increase in block diversity is followed by a negative market reaction. Firms held by heterogeneous blockholders consistently perform worse than firms held by homogeneous blockholders. Block diversity is particularly detrimental when uncertainty is high. Disagreement among shareholders (e.g., as reflected in the frequency of lawsuits being filed) increases when the blockholder base is diverse. We make our blockholder data set public for the benefit of other researchers. This paper was accepted by Victoria Ivashina, finance. Funding: This work was supported by the Israel Science Foundation [Grant 264/20], the Faculty of Business and Economics, University of Melbourne [Grant ECR00009FNL], and the Accounting and Finance Association of Australia and New Zealand [Grant 2020-033]. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2023.00528 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00528},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1356-1390},
  shortjournal = {Manag. Sci.},
  title        = {Is blockholder diversity detrimental?},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving customer compatibility with tradeoff transparency.
<em>MS</em>, <em>71</em>(2), 1335–1355. (<a
href="https://doi.org/10.1287/mnsc.2024.4985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through a large-scale field experiment with 393,036 customers considering opening a credit card account with a nationwide retail bank, we investigate how providing transparency into an offering’s tradeoffs affects subsequent rates of customer acquisition and long-run engagement. Although we find tradeoff transparency to have an insignificant effect on acquisition rates, customers who were shown each offering’s tradeoffs selected different products than those who were not. Moreover, prospective customers who experienced transparency and subsequently chose to open an account went on to exhibit higher-quality service relationships over time. Monthly spending was 9.9% higher and cancellation rates were 20.5% lower among those who experienced transparency into each offering’s tradeoffs. Increased product use and retention accrued disproportionately to customers with prior category experience: more-experienced customers who were provided transparency spent 19.2% more on a monthly basis and were 33.7% less likely to defect after nine months. Importantly, we find that these gains in engagement and retention do not come at the expense of customers’ financial well-being: the probability of making late payments was reduced among customers who experienced transparency. We further find that the positive effects of tradeoff transparency on engagement and retention were attenuated in the presence of a promotion that provided financial incentives to choose particular offerings. Taken together, these results suggest that providing transparency into an offering’s tradeoffs may be an effective strategy for informing customer choices, leading to better outcomes for customers and firms alike. This paper was accepted by Vishal Gaur, operations management. Funding: Funding for this research, apart from the costs Commonwealth Bank of Australia internally incurred to support the experiment, was provided by Harvard Business School. R. W. Buell has received compensation from Commonwealth Bank of Australia in the past for executive education teaching. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2024.4985 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.4985},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1335-1355},
  shortjournal = {Manag. Sci.},
  title        = {Improving customer compatibility with tradeoff transparency},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Patents, freedom to operate, and follow-on innovation:
Evidence from post-grant opposition. <em>MS</em>, <em>71</em>(2),
1315–1334. (<a href="https://doi.org/10.1287/mnsc.2019.02294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the blocking effect of patents on follow-on innovation by others. We posit that follow-on innovation requires freedom to operate (FTO), which firms typically obtain through a license from the patentee holding the original innovation. Where licensing fails, follow-on innovation is blocked unless firms gain FTO through patent invalidation. Using large-scale data from post-grant oppositions at the European Patent Office, we find that patent invalidation increases follow-on innovation, measured in citations, by 16% on average. This effect exhibits a U-shape in the value of the original innovation. For patents on low-value original innovations, invalidation predominantly increases low-value follow-on innovation outside the patentee’s product market. Here, transaction costs likely exceed the joint surplus of licensing, causing licensing failure. In contrast, for patents on high-value original innovations, invalidation mainly increases high-value follow-on innovation in the patentee’s product market. We attribute this latter result to rent dissipation, which renders patentees unwilling to license out valuable technologies to (potential) competitors. This paper was accepted by Ashish Arora, entrepreneurship and innovation. Funding: This work was supported by the Deutsche Forschungsgemeinschaft [Collaborative Research Center TRR 190]. F. Gaessler acknowledges financial support from the Spanish Agencia Estatal de Investigación through the Severo Ochoa Programme for Centres of Excellence in R&amp;D [Barcelona School of Economics CEX2019-000915-S]. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2019.02294 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2019.02294},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1315-1334},
  shortjournal = {Manag. Sci.},
  title        = {Patents, freedom to operate, and follow-on innovation: Evidence from post-grant opposition},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Workplace automation and corporate liquidity policy.
<em>MS</em>, <em>71</em>(2), 1287–1314. (<a
href="https://doi.org/10.1287/mnsc.2021.03902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using an occupational probability of computerization, we measure a firm’s ability to replace labor with automated capital. Our evidence suggests that the potential to automate a workforce enhances operating flexibility, allowing firms to hold less precautionary cash. To provide evidence for this mechanism, we exploit the 2011–2012 Thailand hard drive crisis as an exogenous shock to the cost of automation. In addition, the negative relation between prospective automation and cash holdings is greater for firms with a lower expected cost of worker displacement and greater labor-induced operating leverage. This paper was accepted by Lukas Schmid, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03902 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03902},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1287-1314},
  shortjournal = {Manag. Sci.},
  title        = {Workplace automation and corporate liquidity policy},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bank specialization and zombie lending. <em>MS</em>,
<em>71</em>(2), 1260–1286. (<a
href="https://doi.org/10.1287/mnsc.2023.01437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study whether banks internalize congestion externalities when lending to zombie firms. We conjecture that banks should be better informed about the presence of zombie firms and the congestion externalities that such firms exert on healthy borrowers in industries where banks are specialized and show that banks’ credit supply to zombie firms relates negatively to their industry specialization. This relation is stronger when congestion externalities are likely to have stronger adverse effects, namely when zombie firms take a higher fraction of resources in the industry or when the industry is geographically more concentrated. Additionally, this relation is weaker in industries with higher asset specificity as zombie firms’ default (and potential asset fire sales) could reduce healthy borrowers’ collateral value. This paper was accepted by Victoria Ivashina, finance. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2023.01437 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01437},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1260-1286},
  shortjournal = {Manag. Sci.},
  title        = {Bank specialization and zombie lending},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The (surprising) sample optimality of greedy procedures for
large-scale ranking and selection. <em>MS</em>, <em>71</em>(2),
1238–1259. (<a href="https://doi.org/10.1287/mnsc.2023.00694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking and selection (R&amp;S) aims to select the best alternative with the largest mean performance from a finite set of alternatives. Recently, considerable attention has turned toward the large-scale R&amp;S problem which involves a large number of alternatives. Ideal large-scale R&amp;S procedures should be sample optimal; that is, the total sample size required to deliver an asymptotically nonzero probability of correct selection (PCS) grows at the minimal order (linear order) in the number of alternatives, k . Surprisingly, we discover that the naïve greedy procedure, which keeps sampling the alternative with the largest running average, performs strikingly well and appears sample optimal. To understand this discovery, we develop a new boundary-crossing perspective and prove that the greedy procedure is sample optimal for the scenarios where the best mean maintains at least a positive constant away from all other means as k increases. We further show that the derived PCS lower bound is asymptotically tight for the slippage configuration of means with a common variance. For other scenarios, we consider the probability of good selection and find that the result depends on the growth behavior of the number of good alternatives: if it remains bounded as k increases, the sample optimality still holds; otherwise, the result may change. Moreover, we propose the explore-first greedy procedures by adding an exploration phase to the greedy procedure. The procedures are proven to be sample optimal and consistent under the same assumptions. Last, we numerically investigate the performance of our greedy procedures in solving large-scale R&amp;S problems. This paper was accepted by Baris Ata, stochastic models and simulation. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72091211, 72071146, 72161160340]. Supplemental Material: The e-companion and data files are available at https://doi.org/10.1287/mnsc.2023.00694 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00694},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1238-1259},
  shortjournal = {Manag. Sci.},
  title        = {The (Surprising) sample optimality of greedy procedures for large-scale ranking and selection},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trade secret protection and the integration of information
within firms. <em>MS</em>, <em>71</em>(2), 1213–1237. (<a
href="https://doi.org/10.1287/mnsc.2021.03484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the effect of trade secret protection laws on internal information integration (i.e., the extent to which economic agents are provided with access to decision-relevant information from other economic agents within a firm). We argue that stronger trade secret protection laws increase firms’ internal information integration because they reduce the proprietary costs of information leakage. To test our prediction, we measure a firm’s internal information integration by the share of its sites integrated into its enterprise management system. Exploiting the staggered adoption of trade secret protection laws via the Uniform Trade Secrets Act (UTSA), we find that these laws increase firms’ internal information integration. This effect is stronger (weaker) for firms with higher proprietary costs (coordination benefits). Further, we provide evidence that the UTSA-induced increase in internal information integration translates into improvements of firms’ internal information quality and decision-making quality. Taken together, our results enhance the understanding of the economic trade-offs shaping firms’ internal information environment. This paper was accepted by Ranjani Krishnan, accounting. Funding: S. Bormann and K. Hombach gratefully acknowledge funding from Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) [Grant Project-ID 403041268–TRR 266 Accounting for Transparency]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03484 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03484},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1213-1237},
  shortjournal = {Manag. Sci.},
  title        = {Trade secret protection and the integration of information within firms},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unconventional monetary policy transmission and bank lending
relationships. <em>MS</em>, <em>71</em>(2), 1187–1212. (<a
href="https://doi.org/10.1287/mnsc.2022.01871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms with only one bank relationship make up the majority of firms in many economies. This paper explores whether policy-driven lending is differentially transmitted to single-bank firms in comparison with the multibank firms that are the focus of the literature. Using unique variation in the ECB’s very long-term refinancing operations (VLTROs), which affected lending to firms discontinuously across credit ratings but within banks, we find selective transmission of VLTRO liquidity to single-bank firms. Banks apply higher lending standards to single-bank firms, with banking relationships determining both new lending and lending maturity. By contrast, banks appear to transmit policy lending near-uniformly across multibank firms. This paper was accepted by David Sraer, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01871 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01871},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1187-1212},
  shortjournal = {Manag. Sci.},
  title        = {Unconventional monetary policy transmission and bank lending relationships},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to optimize contextually constrained problems for
real-time decision generation. <em>MS</em>, <em>71</em>(2), 1165–1186.
(<a href="https://doi.org/10.1287/mnsc.2020.03565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of learning to solve optimization problems has received interest from both the operations research and machine learning communities. In this paper, we combine ideas from both fields to address the problem of learning to generate decisions to instances of optimization problems with potentially nonlinear or nonconvex constraints where the feasible set varies with contextual features. We propose a novel framework for training a generative model to produce provably optimal decisions by combining interior point methods and adversarial learning, which we further embed within an iterative data generation algorithm. To this end, we first train a classifier to learn feasibility and then train the generative model to produce optimal decisions to an optimization problem using the classifier as a regularizer. We prove that decisions generated by our model satisfy in-sample and out-of-sample optimality guarantees. Furthermore, the learning models are embedded in an active learning loop in which synthetic instances are iteratively added to the training data; this allows us to progressively generate provably tighter optimal decisions. We investigate case studies in portfolio optimization and personalized treatment design, demonstrating that our approach yields advantages over predict-then-optimize and supervised deep learning techniques, respectively. In particular, our framework is more robust to parameter estimation error compared with the predict-then-optimize paradigm and can better adapt to domain shift as compared with supervised learning models. This paper was accepted by Chung Piaw Teo, optimization. Funding: This work was supported in part by the Natural Sciences and Engineering Research Council of Canada. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2020.03565 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2020.03565},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1165-1186},
  shortjournal = {Manag. Sci.},
  title        = {Learning to optimize contextually constrained problems for real-time decision generation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost structure and the usefulness of earnings. <em>MS</em>,
<em>71</em>(2), 1138–1164. (<a
href="https://doi.org/10.1287/mnsc.2021.03023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the relation between the usefulness of earnings and firm cost structure. We document that the usefulness of earnings to external users decreases in the degree of operating leverage, which measures the proportion of fixed to variable costs, and that this decline extends at least partially from the relation between the degree of operating leverage and two earnings properties: aggressive revenue recognition to meet an earnings target and earnings volatility. Our results illustrate how firm fundamentals influence a firm’s information environment and thereby the usefulness of earnings to external users. This paper was accepted by Ranjani Krishnan, accounting. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2021.03023 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03023},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1138-1164},
  shortjournal = {Manag. Sci.},
  title        = {Cost structure and the usefulness of earnings},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis of the cost coefficients in
multiobjective integer linear optimization. <em>MS</em>, <em>71</em>(2),
1120–1137. (<a href="https://doi.org/10.1287/mnsc.2021.01406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers sensitivity analysis of the cost coefficients in multiobjective integer linear programming problems. We define the sensitivity region as the set of simultaneous changes to the coefficients for which the efficient set and its structure remain the same. In particular, we require that the component-wise relation between efficient solutions is preserved and that inefficient solutions remain inefficient, and we show that this is sufficient for the efficient set to be the same upon changes. For a single coefficient, we show that a subset of the inefficient solutions can be excluded from consideration. More specifically, we prove that it suffices to inspect the inefficient solutions of a q -objective problem that are efficient in one of two related q + 1-objective problems. Finally, we show that the sensitivity region is a convex set (an interval). Our approach generalizes to simultaneous changes in multiple coefficients. For illustration, we consider mean-variance capital budgeting and determine the region for which the set of efficient portfolios remains the same, despite a misspecification or a change in the net present values of the projects. Further computational experience with multiobjective binary and integer knapsack problems demonstrates the general applicability of our technique. For instance, we obtain all sensitivity intervals for changes to single coefficients of biobjective problems with 500 binary variables in less than half an hour of CPU time by excluding a large number of inefficient solutions. In fact, the number of excluded solutions is above 100 orders of magnitude larger than the number of remaining solutions. This paper was accepted by Chung Piaw Teo, optimization. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.01406 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01406},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1120-1137},
  shortjournal = {Manag. Sci.},
  title        = {Sensitivity analysis of the cost coefficients in multiobjective integer linear optimization},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bargaining with voluntary disclosure and endogenous
matching. <em>MS</em>, <em>71</em>(2), 1102–1119. (<a
href="https://doi.org/10.1287/mnsc.2022.03566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a bargaining setting between an “informed” player, who has private information, and an “uninformed” player. The informed player has the option to truthfully disclose private information in two unique environments. In the first environment, the informed player is randomly matched with an uninformed player and, after matching, can voluntarily disclose private information prior to a negotiation taking place. In the second environment, the informed player can voluntarily disclose private information before any endogenous matching between players (and subsequent negotiation) takes place. We begin by examining these environments theoretically. When disclosure occurs after matching, we show that low informed types should disclose more often than high informed types to avoid disagreement. However, when disclosure occurs before matching, for a range of parameter values, we show that high informed types should disclose more than low informed types to secure a better match. We test these predictions in a controlled human-subjects experiment and verify many of the theoretical predictions. Among other results, we find that low informed types do indeed disclose their private information to avoid disagreement and that, when it is in their interest to do so, high informed types disclose their private information to secure a better match. Another key insight is that high uninformed types benefit from disclosure regardless of whether it occurs before or after matching. This paper was accepted by Axel Ockenfels, behavioral economics and decision analysis. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.03566 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.03566},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1102-1119},
  shortjournal = {Manag. Sci.},
  title        = {Bargaining with voluntary disclosure and endogenous matching},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Politically affiliated analysts. <em>MS</em>,
<em>71</em>(2), 1074–1101. (<a
href="https://doi.org/10.1287/mnsc.2022.00579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Government ownership of financial intermediaries is pervasive around the world. In this study, we examine the impact of common government ownership between the brokerage and listed firms on the information production role of brokerage firms. We show that affiliated analysts tend to issue more optimistic recommendations for stocks of firms controlled by the same government entity that controls their brokerage firms. This optimistic bias is particularly pronounced during periods of economic shocks. Our study demonstrates this by utilizing additional tariff impositions and tariff exemptions during the U.S.–China trade war as exogenous negative and positive shocks, respectively. Additionally, our study indicates that stocks recommended by politically affiliated analysts tend to underperform those recommended by independent analysts, implying that the optimism stems from conflicts of interest rather than superior information. Furthermore, our research highlights that sophisticated investors perceive the potential bias and incorporate it into their trading. Consistent with an exchange of favors story, politically affiliated brokerage firms receive a larger allocation during the issuance of local government debt, whereas governments subscribe for more shares during seasoned equity offerings by these affiliated brokerage firms. This paper was accepted by Kay Giesecke, finance. Funding: The authors gratefully acknowledge the financial support from the National Natural Science Foundation of China [Grants 71972088, 72132002, and 71991473], the National Social Science Foundation of China [Grants 21ZDA010 and 22VRC145], and the Innovation and Talent Base for Digital Technology and Finance [Grant B21038]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00579 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00579},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1074-1101},
  shortjournal = {Manag. Sci.},
  title        = {Politically affiliated analysts},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online advertising as passive search. <em>MS</em>,
<em>71</em>(2), 1050–1073. (<a
href="https://doi.org/10.1287/mnsc.2022.02154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard search models assume that consumers actively decide on the order, identity, and number of products they search. We document that online, a large fraction of searches happen in a more passive manner, with consumers merely reacting to online advertisements that do not allow them to choose the timing or the identity of products to which they will be exposed. Using a clickstream panel data set capturing full URL addresses of websites that consumers visit, we show how to detect whether a click is ad initiated. We then report that in the apparel category, ad-initiated clicks account for more than half of all website arrivals, are more concentrated early on in the consumer search process, and lead to less in-depth searches and fewer transactions, consistent with the passive nature of these searches. To account for these systematic differences between active and passive searches, we propose and estimate a simple model that accommodates both types of searches. Our results show that incorrectly treating all searches as active inflates the estimated value of brands that advertise frequently. Our model can more accurately recover data patterns, especially for advertising brands. We finish with model extensions and a discussion of the managerial implications. This paper was accepted by Dmitry Kuksov, marketing. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02154 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02154},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1050-1073},
  shortjournal = {Manag. Sci.},
  title        = {Online advertising as passive search},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Production chain organization in the digital age:
Information technology use and vertical integration in u.s.
manufacturing. <em>MS</em>, <em>71</em>(2), 1027–1049. (<a
href="https://doi.org/10.1287/mnsc.2019.01586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in information technology (IT) may affect the organizational design of production. Exploiting the rapid diffusion of the internet in the United States, we assess the sensitivity of production chain organization to this innovation in IT access and use. Extending theories of the firm that recognize the importance of downstream transfers (selling as opposed to sourcing) and plural governance in organizational design, we predict IT-driven shifts in downstream vertical integration. In a detailed panel of Census Bureau data for over 5,600 manufacturing plants, we observe the extent of a production unit’s downstream transactions within the firm alongside concurrent sales to external customers—a mix we refer to as plural selling . Our main finding is that the use of the internet for external coordination precipitated a significant decline in downstream vertical integration across the manufacturing sector. Instrumental variables estimation points to a causal relationship but also heterogeneous treatment effects. Key drivers of plural organization, such as complementarities and constraints across differently governed transactions, help explain such heterogeneity, as does concurrent use of internal production management IT. Our study is the first study to leverage a plural governance framework and large-scale microdata to understand how U.S. production chain organization shifted in response to this rapid and far-reaching technological change. This paper was accepted by David Simchi-Levi, information systems. Funding: This research was performed at the Atlanta, Boston, and Cornell (supported by the Cornell Center for the Social Sciences) Federal Statistical Research Data Centers [Project 1069 (CBDRB-FY22-279)]. Support for the Research Data Centers network from the National Science Foundation [Grant ITR-0427889] is gratefully acknowledged, as is support from the Social Sciences and Humanities Research Council of Canada. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2019.01586 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2019.01586},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1027-1049},
  shortjournal = {Manag. Sci.},
  title        = {Production chain organization in the digital age: Information technology use and vertical integration in U.S. manufacturing},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adwords with unknown budgets and beyond. <em>MS</em>,
<em>71</em>(2), 1009–1026. (<a
href="https://doi.org/10.1287/mnsc.2021.03243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the classic Adwords problem introduced by [Mehta A, Saberi A, Vazirani U, Vazirani V (2007) Adwords and generalized online matching. J. ACM 54(5):22-es.], we have a bipartite graph between advertisers and queries. Each advertiser has a maximum budget that is known a priori. Queries are unknown a priori and arrive sequentially. When a query arrives, advertisers make bids, and we (immediately and irrevocably) decide which (if any) Ad to display based on the bids and advertiser budgets. The winning advertiser for each query pays their bid up to their remaining budget. Our goal is to maximize total budget used without any foreknowledge of the arrival sequence (which could be adversarial). We consider the setting where the online algorithm does not know the advertisers’ budgets a priori and the budget of an advertiser is revealed to the algorithm only when it is exceeded. A naïve greedy algorithm is 0.5 competitive for this setting, and finding an algorithm with better performance remained an open problem. We show that no deterministic algorithm has competitive ratio better than 0.5 and give the first (randomized) algorithm with strictly better performance guarantee. We show that the competitive ratio of our algorithm is at least 0.522 but also strictly less than ( 1 − 1 / e ) . We present novel applications of budget oblivious algorithms in search ads and beyond. In particular, we show that our algorithm achieves the best possible performance guarantee for deterministic online matching in the presence of multichannel traffic [Manshadi V, Rodilitz S, Saban D, Suresh A (2022) Online algorithms for matching platforms with multi-channel traffic. Proc. 23rd ACMConf. Econom. Comput. (ACM, NewYork), 986–987.]. This paper was accepted by Omar Besbes, revenue management and market analytics. Funding: NSF Division of Civil, Mechanical, and Manufacturing Innovation [Grant 2340306], and Google Research Scholar Program. Supplemental Material: The online appendices are available at https://doi.org/10.1287/mnsc.2021.03243 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03243},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1009-1026},
  shortjournal = {Manag. Sci.},
  title        = {Adwords with unknown budgets and beyond},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling the impact of community first responders.
<em>MS</em>, <em>71</em>(2), 992–1008. (<a
href="https://doi.org/10.1287/mnsc.2022.04024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In community first responder (CFR) systems, traditional emergency service response is augmented by a network of trained volunteers who are dispatched via an app. A central application of such systems is out-of-hospital cardiac arrest (OHCA), where a very fast response is crucial. For a target performance level, how many volunteers are needed, and from which locations should they be recruited? We model the presence of volunteers throughout a region as a Poisson point process, which permits the computation of the response-time distribution of the first-arriving volunteer. Combining this with known survival-rate functions, we deduce survival probabilities in the cardiac arrest setting. We then use convex optimization to compute a location distribution of volunteers across the region that optimizes either the fraction of incidents with a fast response (a common measure in the industry) or patient survival in the case of OHCA. The optimal location distribution provides a bound on the best possible performance with a given number of volunteers. This can be used to determine whether introducing a CFR system in a new region is worthwhile or can serve as a guide for additional recruitment in existing systems. Effective target areas for recruitment are not always obvious because volunteers recruited from one area may be found in various areas across the city depending on the time of day; we explicitly capture this issue. We demonstrate these methods through an extended case study of Auckland, New Zealand. This paper was accepted by Carri Chan, healthcare management. Funding: This research was financed in part by the Netherlands Organization for Scientific Research (NWO) in the form of a Rubicon grant (019.172EN.016) to C. J. Jagtenberg and a Veni grant (VI.Veni.191E.005) to P. L. van den Berg. S. G. Henderson and H. Li were supported in part by National Science Foundation [Grant CMMI-2035086]. Vrije Universiteit Amsterdam and Erasmus University received funding from TKI Dinalog [Grant 2023-1-307TKI]. Some of this work has been executed under the TKI Dinalog [Grant 2023-1-307TKI]. Part of this work was completed during a research visit made possible by a [Distinguished Visitor Award] from the University of Auckland. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.04024 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.04024},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {992-1008},
  shortjournal = {Manag. Sci.},
  title        = {Modeling the impact of community first responders},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk taking under limited liability and moral hazard:
Quantifying the role of motivated beliefs. <em>MS</em>, <em>71</em>(2),
976–991. (<a href="https://doi.org/10.1287/mnsc.2021.03947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates whether limited liability and moral hazard affect risk taking through motivated beliefs. On the one hand, limited liability encourages investors to take excessive risks. On the other, excessive risk taking makes it hard for investors to maintain a positive self-image when moral hazard is present. Using a novel experimental design, we show that subjects form motivated beliefs to self-justify their excessive risk taking. For the same investment opportunity, subjects invest more and are significantly more optimistic about the success of the investment if its failure can harm others. We show that more than one third of the investment increases under limited liability, and moral hazard can be explained through motivated beliefs. Moreover, through a treatment with limited liability but no moral hazard, we show that motivated beliefs are formed subconsciously and can lead to the paradoxical result of investors taking larger risks when their investment can harm a third party compared with when it cannot. These results underscore the importance of motivated beliefs in regulatory policy, emphasizing that policymakers must not only address bad incentives, but also address the role of bad beliefs. This paper was accepted by Bruno Biais, finance. Funding: Financial support by Deutsche Forschungsgemeinschaft [Grant CRC TRR 190, project number 280092119] is gratefully acknowledged. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03947 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03947},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {976-991},
  shortjournal = {Manag. Sci.},
  title        = {Risk taking under limited liability and moral hazard: Quantifying the role of motivated beliefs},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rise of the machines: The impact of automated underwriting.
<em>MS</em>, <em>71</em>(2), 955–975. (<a
href="https://doi.org/10.1287/mnsc.2024.4986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a randomized experiment in auto lending, we find that algorithmic underwriting outperforms the human underwriting process, resulting in 10.2% higher loan profits and 6.8% lower default rates. The human and machine underwriters show similar performance for low-risk, less complex loans. However, the performance of human underwritten loans largely declines for riskier and more complex loans, whereas the machine performance stays relatively stable across various risk dimensions and loan characteristics. The performance difference is more pronounced at underwriting thresholds with a high potential for agency conflict. These results are consistent with algorithmic underwriting mitigating agency conflicts and humans’ limited capacity for analyzing complex problems. This paper was accepted by Will Cong, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2024.4986 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.4986},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {955-975},
  shortjournal = {Manag. Sci.},
  title        = {Rise of the machines: The impact of automated underwriting},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="msom---19">MSOM - 19</h2>
<ul>
<li><details>
<summary>
(2025). Optimal prototyping with noisy measurements. <em>MSOM</em>,
<em>27</em>(1), 322–338. (<a
href="https://doi.org/10.1287/msom.2024.1133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Prototyping and testing are an integral part of almost any new product development process, helping firms navigate the inherent uncertainties of creating new products. Recent developments in rapid prototyping, including technologies that enable cheaper low-fidelity tests, have opened up the possibilities for firms in reconfiguring their product development processes. Firms can, by choosing the level of evaluation fidelity, alter the traditional cost-quality trade-offs inherent in sequential prototyping. Methodology/results : The current article formulates a general model of sequential search where firms can proceed by obtaining noisy low-fidelity evaluations of their prototypes. Our results demonstrate that the imperfect fidelity of evaluations alters the firm’s optimal experimentation, with the starkest difference being that it may make it optimal for the firm to select and launch a prototype that did not yield the best evaluation. In addition, our analysis of optimal measurement technology reveals that the focal firm should demand the most precise measurements when their ex-ante uncertainty is moderate (not too high or low). We also consider extensions analyzing how the optimal choice of evaluation fidelity is affected by the number of available prototypes, by operational flexibility (to dynamically change measurement technology), and by the ability to outsource evaluations to an experimentation platform. Managerial implications : We develop managerial insights for how the optimal choice of fidelity and the optimal length of the evaluation cycle should be planned depending on the evaluation costs and the firm’s ex-ante uncertainty. The resulting framework offers guidance to product and software development firms to successfully leverage imperfect fidelity experiments. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2024.1133 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.1133},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {322-338},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Optimal prototyping with noisy measurements},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service operations for justice-on-time: A data-driven
queueing approach. <em>MSOM</em>, <em>27</em>(1), 305–321. (<a
href="https://doi.org/10.1287/msom.2023.0530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Limited resources in the judicial system can lead to costly delays, stunted economic development, and even failure to deliver justice. Using the Supreme Court of India as an exemplar for such resource-constrained settings, we apply ideas from service operations to study delay. Specifically, court dynamics constitute a case-management queue , whereby each case may experience multiple service encounters spread across time, but all are necessarily with the same server. Our goal is to elucidate the drivers of congestion, focusing on metrics such as the expected case-disposition time (delay) and expected number of cases awaiting adjudication (pendency), and leverage this understanding to recommend operational interventions. Methodology/results : We employ data-driven calibrated simulations to model the analytically intractable case-management queue. The life cycle of a case comprises two stages: preadmission (before determining its merit for detailed hearings) and postadmission. Our methodology allows us to capture the queueing dynamics in which the judges are shared resources across the two stages. It also permits modeling of holiday capacity, which is flexibly tailored to address any surplus work that spills over from the regular year. We find that the second stage of this judicial queue is overloaded, but holiday capacity creates a perception of stability by steadying performance metrics. Managerial implications : The sources of inefficiency that drive congestion include a misalignment between scheduling guidelines and judicial capacity, coupled with the requirement to schedule hearings in advance. Together, these factors inhibit utilization of shared capacity across the two-stage judicial queue. We demonstrate how interventions that account for these inefficiencies can successfully tackle judicial delay. In particular, scheduling to improve the allocation of time across preadmission and postadmission cases can cut down the expected delay by as much as 65%. Funding: This study is (partially) supported by a Korea University Business School Research Grant. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0530 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0530},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {305-321},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Service operations for justice-on-time: A data-driven queueing approach},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Not all lines are skipped equally: An experimental
investigation of line-sitting and express lines. <em>MSOM</em>,
<em>27</em>(1), 287–304. (<a
href="https://doi.org/10.1287/msom.2022.0338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : In this paper, we investigate how line-sitting and express lines affect customers’ satisfaction and fairness perception about queues. In line-sitting, customers skip the wait by hiring others to stand in line on their behalf. In express lines, customers skip the wait by purchasing priority. Because both schemes disrupt the first-in-first-out (FIFO) rule, they may be perceived to be unfair and lead to customer dissatisfaction. Methodology/results : In three experiments, we find that customers are more satisfied with the overall queueing experience when they encounter a line-sitter than when they encounter an express-line customer, even when the wait time is held constant. We also find that customers perceive express lines as less fair than line-sitting, which has a significant influence on their satisfaction. A key operational difference between line-sitting and express lines is that line-sitting involves a one-to-one swap between a line-sitter doing a fair share of waiting and their client, whereas express lines insert a new priority customer into the queue without an existing surrogate in the queue. We show that if line-sitting includes an insertion by letting a line-sitter hold a spot for more than one customer, then line-sitting is perceived to be just as unfair and unsatisfying as express lines. Managerial implications : Our results imply that express lines can engender a lower fairness perception, thus harming future business through both more negative word of mouth and a higher customer churn. Therefore, service providers should beware of customer backlash against express lines and yet be more open-minded about line-sitting, provided that one-to-one swaps can be enforced. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0338 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0338},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {287-304},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Not all lines are skipped equally: An experimental investigation of line-sitting and express lines},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information provision from a platform to competing sellers:
The role of strategic ambiguity. <em>MSOM</em>, <em>27</em>(1), 269–286.
(<a href="https://doi.org/10.1287/msom.2023.0262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : With the rapid growth of e-commerce, platforms are able to gather large quantities of data, which can result in high-precision predictions regarding consumer purchasing patterns and future demand. A fundamental question is whether a platform has the incentives and ability to share such nonverifiable information with its sellers. Methodology/results : We show that when information is nonverifiable, sharing the precise value of the platform’s private information by means of cheap-talk cannot result in an equilibrium. This outcome is due to the incentive of the platform to portray a more favorable market condition than that predicted, in order to encourage the sellers to raise their prices and improve market efficiency. In spite of this negative result, we demonstrate that the level of incentives misalignment between the platform and its sellers is bounded; consequently, a region-forecast information-sharing equilibrium can emerge. In this equilibrium, the support of the platform’s private information is divided into several intervals, and the platform strategically chooses to report truthfully the interval containing its private information. The structure of the partition is influenced by two main factors: the incentive of the platform to reduce market uncertainty for the sellers, and the motivation of the platform to soften competition among the sellers. Although both the sellers and the platform benefit from the ability to share some level of information, such an outcome hurts the consumers. Managerial implications : This work explains the observed practice of a platform providing nonverifiable information to its sellers via cheap-talk. The main advantage of this equilibrium is the ability to share information in a costless manner; however, the amount of information that can be shared is limited and is influenced by the level of market competition between the sellers. Funding: N. Shamir gratefully acknowledges financial support from the Israel Science Foundation [Grant 2358/22] and The Henry Crown Institute of Business Research in Israel. T. Avinadav and T. Chernonog gratefully acknowledge financial support from the Israel Science Foundation [Grant 1571/20]. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/msom.2023.0262 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0262},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {269-286},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Information provision from a platform to competing sellers: The role of strategic ambiguity},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The choice overload effect in online recommender systems.
<em>MSOM</em>, <em>27</em>(1), 249–268. (<a
href="https://doi.org/10.1287/msom.2022.0659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Online retailing platforms are increasingly relying on personalized recommender systems to help guide consumer choice. An important but understudied question in such settings is how many products to include in a recommendation set. In this work, we study how the number of recommended products influences consumers’ search and purchase behavior in an online personalized recommender system within a retargeting setting. Methodology/results : Via a field experiment involving 1.6 million consumers on an online retailing platform, we causally demonstrate that consumers’ likelihood of purchasing any product from the recommendation set first increases then decreases as the number of recommended products increases. Importantly, as much as 64% of the decrease in purchase probability (i.e., the choice overload effect) can be attributed to a decrease in consumers’ likelihood of starting a search (i.e., clicking on any recommended product). We discuss the possible behavioral mechanisms driving these results and analyze how these effects could be heterogeneous across different product categories, price ranges, and timing. Managerial implications : This work presents real-world experimental evidence for the choice overload effect in online retailing platforms, highlights the important role of consumer search behavior in driving this effect, and sheds light on when and how limiting the number of options in a recommender system may be beneficial to online retailers. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0659 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0659},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {249-268},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {The choice overload effect in online recommender systems},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian ensembles of exponentially smoothed life-cycle
forecasts. <em>MSOM</em>, <em>27</em>(1), 230–248. (<a
href="https://doi.org/10.1287/msom.2022.0359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We study the problem of forecasting an entire demand distribution for a new product before and after its launch. Firms need accurate distributional forecasts of demand to make operational decisions about capacity, inventory, and marketing expenditures. We introduce a unified, robust, and interpretable approach to producing these pre- and postlaunch distributional forecasts. Methodology/results : Our approach is inspired by Bayesian model averaging. Each candidate model in our ensemble is a life-cycle model fitted to the completed life cycle of a comparable product. A prelaunch forecast is an ensemble with equal weights on the candidate models’ forecasts, whereas a postlaunch forecast is an ensemble with weights that evolve according to Bayesian updating. Our approach is part frequentist and part Bayesian, resulting in a novel approach tailored to the demand forecasting challenge. We also introduce a new type of life-cycle or product diffusion model with states that can be updated using exponential smoothing. The trend in this model follows the density of an exponentially tilted Gompertz random variable. For postlaunch forecasting, this model is attractive because it can adapt itself to the most recent changes in a product’s life cycle. We provide closed-form distributional forecasts from our model. In two empirical studies, we show that when the ensemble’s candidate models are all in our new type of exponential smoothing model, this version of the ensemble outperforms several leading approaches in both point and quantile forecasting. Managerial implications : In a data-driven operations environment, our model can produce accurate forecasts frequently and at scale. When quantile forecasts are needed, our model has the potential to provide meaningful economic benefits. In addition, our model’s interpretability should be attractive to managers who already use exponential smoothing and ensemble methods for other forecasting purposes. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0359 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0359},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {230-248},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Bayesian ensembles of exponentially smoothed life-cycle forecasts},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in repeated multiunit pay-as-bid auctions.
<em>MSOM</em>, <em>27</em>(1), 200–229. (<a
href="https://doi.org/10.1287/msom.2023.0403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Motivated by carbon emissions trading schemes (ETSs), Treasury auctions, procurement auctions, and wholesale electricity markets, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multiunit pay-as-bid (PAB) auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. In this work, we study the problem of optimizing bidding strategies from the perspective of a single bidder. Methodology/results : Effective bidding in PAB auctions is complex due to the combinatorial nature of the action space. We show that a utility decoupling trick enables a polynomial time algorithm to solve the offline problem where competing bids are known in advance. Leveraging this structure, we design efficient algorithms for the online problem under both full information and bandit feedback settings that achieve an upper bound on regret of O ( M T log T ) and O ( M T 2 3 log T ) , respectively, where M is the number of units demanded by the bidder, and T is the total number of auctions. We accompany these results with a regret lower bound of Ω ( M T ) for the full information setting and Ω ( M 2 / 3 T 2 / 3 ) for the bandit setting. We also present additional findings on the characterization of PAB equilibria. Managerial implications : Although the Nash equilibria of PAB auctions possess nice properties such as winning bid uniformity and high welfare and revenue, they are not guaranteed under no-regret learning dynamics. Nevertheless, our simulations suggest that these properties hold anyways, regardless of Nash equilibrium existence. Compared with its uniform price counterpart, the PAB dynamics converge faster and achieve higher revenue, making PAB appealing whenever revenue holds significant social value—for example, ETSs and Treasury auctions. Funding: R. Galgana and N. Golrezaei were supported in part by the Young Investigator Program Award from the Office of Naval Research [Grant N00014-21-1-2776] and the Massachusetts Institute of Technology Research Support Award. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0403 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0403},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {200-229},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Learning in repeated multiunit pay-as-bid auctions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling regulatory operations: A data set of the
determinants, process, and outcomes of product defect investigations by
the u.s. Automotive safety regulator. <em>MSOM</em>, <em>27</em>(1),
181–199. (<a href="https://doi.org/10.1287/msom.2023.0705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The paucity of data on governmental regulatory agencies’ product safety defect investigations has restricted our knowledge about (1) the determinants of a regulator’s decisions to open or close an investigation, (2) the process it follows between opening and closing of an investigation, and (3) the outcomes of the investigation when it is closed. Methodology/results : The authors view a safety regulator’s opening and closing of a product defect investigation as a decision of interest to the operations management discipline. This data paper describes a rich, novel, and hand-collected data set of all investigations that the National Highway Traffic Safety Administration—the U.S. regulator for automobile safety—opened and closed against 187 manufacturers between 2009 and 2021. The authors provide two Microsoft Excel data files, one capturing data for the investigations opened and the other for the investigations closed. The data files enable researchers to address three sets of research questions. First, researchers can use the “Data on Investigations Opened” file to model the determinants of a regulator’s opening of a product defect investigation. Second, researchers can mine the textual variables from both files to identify the steps involved in the investigation process. They can also use the process variables included in the data to investigate the regulator’s efficiency in opening and closing investigations. Third, researchers can use the “Data on Investigations Closed” file to better understand when and why a regulator closes an investigation and the outcomes of the closed investigations. Managerial implications : The data files can also be valuable to nonacademic stakeholders (e.g., governmental organizations and regulators, journalists, liability lawyers, politicians, and safety advocates). The authors provide an open-access website that simplifies the use of the data for a nonacademic audience and allows them to draw insights from the data via graphs and tables. Supplemental Material: The online supplement is available at https://doi.org/10.1287/msom.2023.0705 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0705},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {181-199},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Unveiling regulatory operations: A data set of the determinants, process, and outcomes of product defect investigations by the U.S. automotive safety regulator},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating demand with unobserved no-purchases on
revenue-managed data. <em>MSOM</em>, <em>27</em>(1), 161–180. (<a
href="https://doi.org/10.1287/msom.2021.0291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper studies the joint estimation of the consumer arrival rate and choice model parameters when “no-purchasers” (customers who considered the product but did not purchase) are not observable. Estimating this unconstrained demand even with the simplest discrete-choice model such as the multinomial logit (MNL) becomes challenging as we do not know the fraction that have chosen the outside option (i.e., not purchased). Methods have been proposed to use market share to pin down the parameter associated with the outside option. However, market share data are difficult to obtain in many situations, and in some industries, such as fashion retail, have little meaning as the items are difficult to compare. In this paper, we point out an additional difficulty that can arise in practice: Many firms monitor sales and optimize their prices and assortments within the sale period as part of their revenue management (RM) process, based on partially observed demand. This can potentially cause a revenue management induced endogeneity as the data used for estimation is the result of optimization (in turn based on prior data) to set controls. As we demonstrate, methods that work well on randomly generated assortments may do badly on optimized assortment data. Methodology/results : In this paper, we propose a robust method when the firm cannot observe no-purchases and has no market share information, and the data have been revenue-managed. We develop a two-step generalized method-of-moments (GMM) procedure that is based on a modified moment condition, and importantly, does not require instrumental variables (IVs), a significant advantage in practice. Managerial implications : In Monte Carlo simulations, the performance of our method matches existing methods when the controls are generated randomly, and is robust under all conditions, whether RM-induced endogeneity is present or not. On a large real-world data set from the fashion industry, subject to stock-outs and markdown pricing along with unknown management controls, our method provides robust estimates compared with existing methods without requiring any input on market shares, which is especially difficult to pin down at a category and season/collection level. Funding: This work was supported by the Hong Kong Research Grants Council’s General Research Fund [Grant 14506423]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2021.0291 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0291},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {161-180},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Estimating demand with unobserved no-purchases on revenue-managed data},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of temporary store closures on online sales: Evidence
from a natural experiment. <em>MSOM</em>, <em>27</em>(1), 147–160. (<a
href="https://doi.org/10.1287/msom.2022.0527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper examines the impact of retail store closures on omnichannel sales and consumer shopping behavior in the context of the coronavirus disease 2019 pandemic. To explain the likelihood of store closure, we develop a novel instrumental variable motivated by varying geopolitical responses across the United States to the pandemic. Methodology/results : Using data from a luxury fashion retailer, we find that when a store is closed, the volume of online orders originating from its location increases by 24%. Furthermore, when the retailer closes 10% of its stores, the omnichannel total sales (offline + online) decrease by 5.5%. Notably, our findings indicate that the online channel enables the retailer to recover 11% of offline sales that would have otherwise been lost because of store closures. We also show that compared with existing e-shoppers, new e-shoppers are more likely to order popular product models in an effort to mitigate the heightened mismatch risk associated with online transactions. For new e-shoppers, the likelihood of ordering a popular model stands at 70%, whereas it is 45% for existing online consumers. Additionally, the conservative behavior of favoring popular models reduces the likelihood of returns by new e-shoppers. Managerial implications : Even for luxury apparel, which is often associated with in-store purchases requiring “touch and feel” and customer tryout, the option to purchase online proves immensely valuable. The tendency of new e-shoppers to limit product mismatch risk by choosing popular products may create an opportunity for retailers to strategically target these inexperienced online customers with advertisements, product promotions, or virtual fitting rooms, all geared toward reducing online shopping risk of product mismatch. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0527 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0527},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {147-160},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Impact of temporary store closures on online sales: Evidence from a natural experiment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recycling standards, green inventions, and spillover:
Evidence from california’s electronic waste recycling act (EWRA).
<em>MSOM</em>, <em>27</em>(1), 127–146. (<a
href="https://doi.org/10.1287/msom.2023.0444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The improper disposal of electronic and electrical goods at the end of their useful lives (i.e., e-waste) can have adverse effects on the environment and human health. The increased awareness of the negative consequences of e-waste has prompted many regulators to enact recycling standards that promote the proper disposal and recycling of e-waste. A large body of research has used analytical models to explore how recycling standards affect firms that make electronic and electrical goods. A key insight from this research is that e-waste recycling standards would induce firms to design products that have reduced environmental impact and are easier to recycle. In other words, e-waste recycling standards would enhance inventive activity at firms to better comply with regulatory requirements. But hardly any empirical work has validated the insights developed with analytical models. Methodology/results : We empirically examine whether California’s Electronic Waste Recycling Act (EWRA) affects the inventive output (i.e., measured as patents) of firms that manufacture electronic and electrical goods. We leverage a quasi-experimental setup that arises when California enacted the EWRA and use multiple identification strategies to isolate the law’s effect on the inventive output of firms. We disentangle two causal pathways, industry and headquarter location , by which EWRA affects manufacturers. We find that EWRA increased the environmentally focused inventive output (i.e., “green” patents) of affected firms in California by nearly 14% and by nearly 8% for firms in other states. Interestingly, we also observe spillover effects—EWRA increased other inventive output (i.e., patents other than green patents) of affected firms in California by nearly 41% and by nearly 24% for firms in other states. Managerial implications : Our study provides important insights for managers and policy makers by empirically quantifying the impact of recycling standards on environmentally focused inventions and by identifying spillover effects for other inventions. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0444 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0444},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {127-146},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Recycling standards, green inventions, and spillover: Evidence from california’s electronic waste recycling act (EWRA)},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interplay between servicizing and remanufacturing: Economic
and environmental implications. <em>MSOM</em>, <em>27</em>(1), 114–126.
(<a href="https://doi.org/10.1287/msom.2022.0379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : To investigate the interplay between servicizing and remanufacturing, two widely used strategies in the circular economy, and its impact on firms’ economic and environmental performance, we consider a profit-maximizing manufacturer who explores the possibility of jointly adopting the two strategies. Methodology and results : We develop optimization models that capture the main attributes for servicizing (pay-per-use, demand pooling) and remanufacturing (lower production costs, higher operating costs). We show that the presence of remanufacturing always makes servicizing less attractive to adopt, and having servicizing may discourage the adoption of remanufacturing if the remanufacturing cost reduction effect is not sufficiently large. The joint adoption of servicizing and remanufacturing is preferred when the remanufacturing cost is neither too high nor too low and the range of the remanufacturing cost that favors the joint adoption is further moderated by the firm’s pooling level and the operating cost of remanufactured products. Lastly, we find that when the firm adopts servicizing in the presence of remanufacturing with relatively high remanufacturing costs, it can reduce the number of remanufactured products with low environmental impact in the market and, thus, harm the environment, resulting in a misalignment between economic and environmental goals. Managerial implications : Our findings suggest that managers considering jointly adopting servicizing and remanufacturing must balance their competition on the low-usage end of the market with the potential complementarity due to remanufacturing cost reduction. Further, by showing the win-win conditions under which the joint adoption of servicizing and remanufacturing improves firms’ economic and environmental performance, we demonstrate that there are no one-size-fits-all circular economy strategies and call for caution when promoting these strategies across industries. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/msom.2022.0379 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0379},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {114-126},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Interplay between servicizing and remanufacturing: Economic and environmental implications},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Green disposable packaging and communication: The
implications of bring-your-own-container. <em>MSOM</em>, <em>27</em>(1),
94–113. (<a href="https://doi.org/10.1287/msom.2021.0605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : A growing number of firms are encouraging consumers to participate in “bring-your-own-container” (BYOC) behavior in which consumers bring their own reusable packaging to purchase and consume products, thus reducing single-use packaging waste. In this paper, we study the environmental implications of a firm’s BYOC implementation when considering its disposable packaging choice and communication strategy. Methodology/results : We build a stylized model to study a firm’s joint decisions on BYOC, disposable packaging choice, and communication and their implications on the environment. Our main results follow. First, allowing BYOC reduces the firm’s incentive to make fraudulent green claims about its disposable product packaging; however, BYOC implementation may harm the overall environment while improving the firm’s profit, thereby creating a new form of greenwashing. Second, the adoption of third-party certification for green disposable packaging is an effective remedy to mitigate the negative environmental impact of BYOC. In addition, the environmental implications of adopting third-party certification (either voluntarily or because of government mandates) depend on the relationship between the environmental qualities of green disposable packaging and reusable packaging. Whereas it always benefits the environment when the firm’s green disposable packaging has better environmental performance, adopting certification may negatively impact the environment if consumers’ reusable packaging is greener. Furthermore, we find numerically that offering a price discount for BYOC may encourage the firm to adopt certification because of increased profitability, thereby leading to the aforementioned environmental implications. Managerial implications : We offer operational insights on how firms should make joint decisions on BYOC, disposable packaging choice, and communication. We also generate insights on how governments should regulate firms’ green claims when firms start to allow BYOC. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2021.0605 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0605},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {94-113},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Green disposable packaging and communication: The implications of bring-your-own-container},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emergency care efficiency vs. Quality: Uncovering hidden
consequences of fast-track routing decisions. <em>MSOM</em>,
<em>27</em>(1), 75–93. (<a
href="https://doi.org/10.1287/msom.2022.0440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This work aims to examine the role of emergency department (ED) operational status related to congestion in fast-track (FT) routing decisions and the subsequent effects on patient outcomes. Methodology/results : In this paper, we utilize a two-year data set from two hospital EDs in Alberta, Canada, and adopt an instrumental variable approach to examine the effects of FT routing decisions on patient outcomes. Based on the empirical findings, we utilize a data-calibrated simulation to compare the performance of different routing policies. First, our study reveals that FT routing decisions are not purely clinically driven, and ED operational status is also associated with FT routing decisions. Second, being routed to FT can improve ED efficiency by reducing the average length of stay and left without being seen rates. However, this efficiency improvement comes at the cost of potential quality decline. In particular, being routed to the FT leads to an 8.2% increase in the 48-hour revisit rate for the high-complexity group and a 2.3% increase for the medium-complexity group. Third, we delve into the mechanisms behind observed patient outcomes and find that physicians in the FT area may prioritize expediting patient flow by simplifying patient diagnosis and treatment procedures. Consequently, the quality of care may be compromised for high- and medium-complexity patients. Finally, our simulation findings highlight the importance of selecting the “right” patients to be routed to the FT unit. To this end, the complexity-based classification method and dynamic routing policies emerge as promising avenues. Managerial implications : Our findings call for immediate attention from healthcare practitioners to carefully balance the trade-off between emergency care efficiency and quality, emphasizing the necessity of selecting the right patients for routing. Funding: This study is partially supported by the Hong Kong Research Grants Council [Grants GRF 11508921 and CRF C7162-20G]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0440 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0440},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {75-93},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Emergency care efficiency vs. quality: Uncovering hidden consequences of fast-track routing decisions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multichannel healthcare: Impact of asynchronous telemedicine
adoption on patient flow. <em>MSOM</em>, <em>27</em>(1), 59–74. (<a
href="https://doi.org/10.1287/msom.2022.0235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper studies a multichannel healthcare system where physicians diagnose patients and prescribe treatment in-person or through asynchronous telemedicine (AT), a widely adopted yet relatively under-explored form of telemedicine. In collaboration with physicians at the Veterans Healthcare Administration (VHA), we examine the impact of introducing an AT channel on the existing in-person channel and on overall system performance. Methodology/results : VHA implemented AT at select clinics in the state of Georgia in 2012. Using a difference-in-differences design, we find that the introduction of the AT channel led to a sorting process whereby more complex patients were seen in the in-person channel. AT implementation led to a 20% increase in recommended visit time and an 8.5% increase in required clinical resources for in-person consultations. In addition, the adoption of AT resulted in higher throughput—more patients seen by the specialists per month across both channels. Using a fixed-effects model we find a reduction in average wait time for in-person referrals (37.5%), and for the most common medically necessary procedure (43%) despite an increase in the total number of consultations at the specialist clinic. We attribute the improved efficiency to early patient triage, better match between patient needs and treatment modality, and reduction of setup and switching costs in physicians’ workflow. Managerial implications : This paper contributes to our understanding of a rapidly expanding form of healthcare delivery: multichannel healthcare with in-person and AT channels. Our results suggest that healthcare managers and physicians can adopt AT to improve overall system efficiency. At the same time, they should take into account the additional impact of AT on the in-person channel when making capacity decisions and developing guidance on patient referrals. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0235 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0235},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {59-74},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Multichannel healthcare: Impact of asynchronous telemedicine adoption on patient flow},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Telehealth in acute care: Pay parity and patient access.
<em>MSOM</em>, <em>27</em>(1), 40–58. (<a
href="https://doi.org/10.1287/msom.2022.0345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : In response to the increased use of telehealth to replace traditional office visits with a physician, several U.S. states have recently adopted telehealth pay-parity policies. Such policies state that payers must reimburse healthcare providers for telehealth services at the same rate that would apply if those services had been provided in a traditional office setting. But health policy researchers have pointed out that telehealth may not be as effective as a traditional office visit for acute care. Specifically, telehealth is associated with increased probability of a subsequent office visit (a “duplicate visit”). We examine whether telehealth pay-parity policies are effective at improving access to acute care, and under what conditions. Methodology/results : We use a three-stage game-theoretic model to study the impact of telehealth pay parity. In the first stage, the payer sets a reimbursement policy for telehealth visits. In the second stage, a healthcare provider commits a portion of its capacity to telehealth, and in the third stage, patients arrive and choose between telehealth and office visits according to an equilibrium queueing network. We find structural results for the equilibria and characterize the equilibria in closed-form. When the chance of a duplicate visit is moderate (neither too high nor too low), pay parity leads providers to allocate too much capacity to telehealth, resulting in lower overall patient access than could be otherwise achieved. We characterize a reimbursement level that avoids this misalignment and maximizes patient access, which we show is less than parity. Managerial implications : The literature shows that patients receiving acute care via telehealth may be more likely to require a duplicate, in-person visit to resolve their health concern. In the fee-for-service environment that is common in the United States for acute care, duplicate visits resulting from telehealth lead to an incentive alignment problem because they generate extra work and provider revenue, without any corresponding increase in patient access. Legislating pay parity for telehealth can lead to providers committing more capacity to telehealth, which may not always be good. However, there is good news in that all parties (payers, providers, and patients) would be better off if duplicate visits could be decreased. Policy makers should understand these implications before enacting policies that affect reimbursements for telehealth. Funding: Support for this project was provided by a PSC-CUNY Award, jointly funded by The Professional Staff Congress and The City University of New York. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0345 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0345},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {40-58},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Telehealth in acute care: Pay parity and patient access},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated approach to improving itinerary completion in
coordinated care networks. <em>MSOM</em>, <em>27</em>(1), 21–39. (<a
href="https://doi.org/10.1287/msom.2022.0649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Coordinated care network (CCN) is a burgeoning paradigm where patients’ diagnosis and treatment plans are developed based on collaboration between multiple, colocated medical specialties to holistically address patients’ health needs. A primary performance metric for CCNs is how quickly patients can complete their itinerary of appointments at multiple medical services in the network. Rapid completion is critical to care delivery but also presents a major operational challenge. Because information about a patient’s condition and treatment options evolves over the course of the itinerary, care paths are not known a priori. Thus, appointments (except for the first one) cannot be reserved in advance, which may result in significant delays if capacity is not allocated properly. Methodology/results : We study capacity allocation for the patient’s first (root) appointment as the primary operational lever to achieve rapid itinerary completion in CCNs. We develop a novel queueing-based analytical framework to optimize this root appointment allocation, maximizing the proportion of patients completing care by prespecified deadlines. Our framework accounts for the complex interactions among all patients in the network through the blocking process, which contrasts with conventional siloed planning. We provide an exact characterization of the itinerary time and develop a mean-field approximation with convergence guarantees that permits tractable solutions for large-scale network problems. In a simulation case study of Mayo Clinic, our solution improves on-time completion from 60% under the current plan to more than 93%. Managerial implications : We demonstrate that root appointment allocation is a multifaceted problem and that ignoring any of those facets can lead to poor performance. Simultaneously accounting for all of these complexities makes manual template design or traditional optimization methods inadequate, highlighting the significance of our integrated approach. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0649 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0649},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {21-39},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {An integrated approach to improving itinerary completion in coordinated care networks},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OM forum—asset-level perspectives on the clean energy
transition. <em>MSOM</em>, <em>27</em>(1), 8–20. (<a
href="https://doi.org/10.1287/msom.2024.1421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The clean energy transition is an important component of the net zero transformation, which aims at averting potentially disastrous climate change related to excessive global warming by practically eliminating new global greenhouse gas emissions within 2050. The resulting move toward decarbonized energy systems is projected to encompass massive investments in assets that employ technologies with low environmental impact, some of which are innovative or yet unproven or even undeveloped, an area known as CleanTech. The typical analyses that are informing the dialogue on this process in practice provide insights into decarbonization paths at the technology level. In contrast, managers need to make decisions about assets. Methodology/results : Taking an asset-level view of the clean energy transition that complements its technology outlook, this essay provides a concise entry point into this topic based on both the practitioner literature and personal reflections, highlighting CleanTech innovations and related operational considerations in carbon capture, use, and storage; sustainable fuels; hydrogen; electricity; and concomitant government support activities. Managerial implications : Further, this work advocates for potential CleanTech research or teaching pursuits, especially ones aimed at directly supporting managerial decision making, for which it provides an example from the literature.},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.1421},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {8-20},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {OM Forum—Asset-level perspectives on the clean energy transition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OM forum—operations management research: Relevance and
impact. <em>MSOM</em>, <em>27</em>(1), 1–7. (<a
href="https://doi.org/10.1287/msom.2023.0691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : A fundamental issue faced by operations management researcher relates to striking the right balance between rigor and relevance in their work. Another important aspect of operations management research relates to influencing and positively impacting businesses and society at large. We constantly struggle to achieve these objectives. Methodology/results : This MSOM Fellow forum article discusses key opportunities for increasing the relevance and impact of Operations Management research. In particular, it highlights two major areas: technology enabled operations and society and operations where unique opportunities exist for the field to make lasting contributions to business and society. Managerial implications : It concludes with a menu of approaches to enhance practical impact of our research.},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0691},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {1-7},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {OM Forum—Operations management research: Relevance and impact},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="or---29">OR - 29</h2>
<ul>
<li><details>
<summary>
(2025). On the optimality of greedy policies in dynamic matching.
<em>OR</em>, <em>73</em>(1), 560–582. (<a
href="https://doi.org/10.1287/opre.2021.0596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study centralized dynamic matching markets with finitely many agent types and heterogeneous match values. A network topology describes the pairs of agent types that can form a match and the value generated from each match. A matching policy is hindsight optimal if the policy can (nearly) maximize the total value simultaneously at all times. We find that suitably designed greedy policies are hindsight optimal in two-way matching networks. This implies that there is essentially no positive externality from having agents waiting to form future matches. We first show that the greedy longest-queue policy with a minor variation is hindsight optimal. Importantly, the policy is greedy relative to a residual network, which includes only nonredundant matches with respect to the static optimal matching rates. Moreover, when the residual network is acyclic (e.g., as in two-sided networks), we prescribe a greedy static priority policy that is also hindsight optimal. The priority order of this policy is robust to arrival rate perturbations that do not alter the residual network. Hindsight optimality is closely related to the lengths of type-specific queues. Queue lengths cannot be smaller (in expectation) than of the order of ϵ − 1 , where ϵ is the general position gap that quantifies the stability in the network. The greedy longest-queue policy achieves this lower bound. Funding: This work was supported by National Science Foundation (CMMI-2010940) and U.S. Department of Defense (STTR A18B-T007).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0596},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {560-582},
  shortjournal = {Oper. Res.},
  title        = {On the optimality of greedy policies in dynamic matching},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shape-constrained regression using sum of squares
polynomials. <em>OR</em>, <em>73</em>(1), 543–559. (<a
href="https://doi.org/10.1287/opre.2021.0383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hierarchy of semidefinite programs (SDPs) for the problem of fitting a shape-constrained (multivariate) polynomial to noisy evaluations of an unknown shape-constrained function. These shape constraints include convexity or monotonicity over a box. We show that polynomial functions that are optimal to any fixed level of our hierarchy form a consistent estimator of the underlying shape-constrained function. As a by-product of the proof, we establish that sum of squares-convex polynomials are dense in the set of polynomials that are convex over an arbitrary box. A similar sum-of-squares-type density result is established for monotone polynomials. In addition, we classify the complexity of convex and monotone polynomial regression as a function of the degree of the polynomial regressor. Whereas our results show NP-hardness of these problems for degree three or larger, we can check numerically that our SDP-based regressors often achieve a similar training error at low levels of the hierarchy. Finally, on the computational side, we present an empirical comparison of our SDP-based convex regressors with the convex least squares estimator introduced in Hildreth [ Hildreth C (1954) Point estimates of ordinates of concave functions. J. Amer. Statist. Assoc. 49(267):598–619] and Holloway [ Holloway CA (1979) On the estimation of convex functions. Oper. Res. 27(2):401–407] and show that our regressor is valuable in settings in which the number of data points is large and the dimension is relatively small. We demonstrate the performance of our regressor for the problem of computing optimal transport maps in a color transfer task and that of estimating the optimal value function of a conic program. A real-time application of the latter problem to inventory management contract negotiation is presented. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0383 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0383},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {543-559},
  shortjournal = {Oper. Res.},
  title        = {Shape-constrained regression using sum of squares polynomials},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal routing under demand surges: The value of future
arrival rates. <em>OR</em>, <em>73</em>(1), 510–542. (<a
href="https://doi.org/10.1287/opre.2022.0282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the growing availability of advanced demand forecast tools, we study how to use future demand information in designing routing strategies in queueing systems under demand surges. We consider a parallel server system operating in a nonstationary environment with general time-varying arrival rates. Servers are cross-trained to help nonprimary customer classes during demand surges. However, such flexibility comes with various operational costs, such as a loss of efficiency and inconvenience in coordination. We characterize how to incorporate the future arrival information into the routing policy to balance the tradeoff between various costs and quantify the benefit of doing so. Based on transient fluid control analysis, we develop a two-stage index-based look-ahead policy that explicitly takes the overflow costs and future arrival rates into account. The policy has an interpretable structure, is easy to implement and is adaptive when the future arrival information is inaccurate. In the special case of the N-model, we prove that this policy is asymptotically optimal even in the presence of certain prediction errors in the demand forecast. We substantiate our theoretical analysis with extensive numerical experiments, showing that our policy achieves superior performance compared with other benchmark policies (i) in complicated parallel server systems and (ii) when the demand forecast is imperfect with various forms of prediction errors. Funding: This work was supported by the National Science Foundation Civil, Mechanical, and Manufacturing Innovation [Grant 1944209]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0282 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0282},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {510-542},
  shortjournal = {Oper. Res.},
  title        = {Optimal routing under demand surges: The value of future arrival rates},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal routing to parallel servers in heavy traffic.
<em>OR</em>, <em>73</em>(1), 483–509. (<a
href="https://doi.org/10.1287/opre.2022.0055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a system with heterogeneous parallel servers, each with an infinite waiting room. Upon arrival, a job is routed to the queue of one of the servers, possibly depending on the dynamic state information such as the real-time queue lengths, the arrival, and service history of jobs. The objective is to find the routing policy that best uses the available state information to minimize the expected stationary queue length. In this paper, we establish the diffusion limit for the round-robin policy (respectively, arrival-chasing policy, service-chasing policy), and show that with properly chosen parameters, it achieves the optimal performance asymptotically within the class of admissible policies that require no state information (respectively, require arrival history, service history). Like the jointhe-shortest-queue and the balanced routing policies that use real-time queue length information, the optimal service-chasing policy is also asymptotically optimal over all admissible policies. Further analysis of the diffusion limits yields a number of insights into the performance of these routing policies and reveals the value of various state information. We numerically demonstrate the effectiveness of the estimators derived from the diffusion limits for the policies being studied and obtain interesting observations. We also address the problem of interchange of limits under the aforementioned policies, which justifies the stationary performance of the diffusion limit as a valid approximation to that of the original system under respective policies. Methodologically, this study contributes to the application of the BIGSTEP method for constructing control policy to optimize stationary performance and the recipe for justifying the interchange of limits in the heavy traffic analysis of stochastic processing networks. Funding: This work was supported by the Research Grants Council, University Grants Committee [GRF Grant 15501421 and NSFC/RGC Grant N_PolyU590/22]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0055 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0055},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {483-509},
  shortjournal = {Oper. Res.},
  title        = {Optimal routing to parallel servers in heavy traffic},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The when and how of delegated search. <em>OR</em>,
<em>73</em>(1), 461–482. (<a
href="https://doi.org/10.1287/opre.2019.0498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms often outsource search processes, such as the acquisition of real estate, new technologies, or talent. To ensure the efficacy of such delegated search, firms need to carefully design incentive contracts to attenuate the ill effects of agency issues. We model this problem using a dynamic principal-agent framework, embedding the standard sequential search model. The optimal contract pays the agent a fixed per-period fee plus a bonus for finding a suitable alternative. The bonus size is defined a priori and decreases over time, whereas the range of values deemed suitable expands over time. If the principal is unable to contract on the value of the delivered alternatives, the optimal contract consists of two parts. Early in the search process, the agent is granted a small bonus for every alternative brought to the principal, irrespective of whether the principal accepts it; late in the search process, the agent is awarded a comparatively larger bonus, which is decreasing in time, but only if the principal accepts the alternative. We also consider situations where the principal chooses between searching in house and outsourcing. This decision is shown to hinge on the principal’s trade-off between speed and quality. The age-old aphorism “if you want it done right, do it yourself” holds, as in-house search is optimal for a principal who prioritizes quality. Yet, in the context of our model, we also establish an addendum: “If you want it done fast, hire someone else to do it.” Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2019.0498 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.0498},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {461-482},
  shortjournal = {Oper. Res.},
  title        = {The when and how of delegated search},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic pricing in volatile markets. <em>OR</em>,
<em>73</em>(1), 444–460. (<a
href="https://doi.org/10.1287/opre.2021.0550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study dynamic entry deterrence through limit pricing in markets subject to persistent demand shocks. An incumbent is privately informed about its costs, high or low, and can deter a Bayesian potential entrant by setting its prices strategically. The entrant can irreversibly enter the market at any time for a fixed cost, earning a payoff that depends on the market conditions and the incumbent’s unobserved type. Market demand evolves as a geometric Brownian motion. When market demand is low, entry becomes a distant threat, so there is little benefit to further deterrence, and, in equilibrium, a weak incumbent becomes tempted to reveal itself by raising its prices. We characterize a unique equilibrium in which the entrant enters when market demand is sufficiently high (relative to the incumbent’s current reputation), and the weak incumbent mixes over revealing itself when market demand is sufficiently low. In this equilibrium, pricing and entry decisions exhibit path dependence, depending not only on the market’s current size, but also its historical minimum. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2021.0550 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0550},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {444-460},
  shortjournal = {Oper. Res.},
  title        = {Strategic pricing in volatile markets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual inverse optimization: Offline and online
learning. <em>OR</em>, <em>73</em>(1), 424–443. (<a
href="https://doi.org/10.1287/opre.2021.0369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problems of offline and online contextual optimization with feedback information, where instead of observing the loss, we observe, after the fact, the optimal action an oracle with full knowledge of the objective function would have taken. We aim to minimize regret, which is defined as the difference between our losses and the ones incurred by an all-knowing oracle. In the offline setting, the decision maker has information available from past periods and needs to make one decision, whereas in the online setting, the decision maker optimizes decisions dynamically over time based a new set of feasible actions and contextual functions in each period. For the offline setting, we characterize the optimal minimax policy, establishing the performance that can be achieved as a function of the underlying geometry of the information induced by the data. In the online setting, we leverage this geometric characterization to optimize the cumulative regret. We develop an algorithm that yields the first regret bound for this problem that is logarithmic in the time horizon. Finally, we show via simulation that our proposed algorithms outperform previous methods from the literature. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.0369 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0369},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {424-443},
  shortjournal = {Oper. Res.},
  title        = {Contextual inverse optimization: Offline and online learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Competitive algorithms for the online minimum peak job
scheduling. <em>OR</em>, <em>73</em>(1), 408–423. (<a
href="https://doi.org/10.1287/opre.2021.0080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a fundamental online scheduling problem called the minimum peak job scheduling (MPJS) problem. In this problem, there is a sequence of arriving jobs, each with a specified required scheduled time for one unit of a scarce and reusable resource. The goal is to schedule each job upon arrival within a scheduling interval to minimize the resulting peak utilization (i.e., the maximum number of units used simultaneously throughout the entire scheduling interval). The MPJS problem captures many practical settings of real-time appointment scheduling. Its offline version where all jobs are known in advance is equivalent to the well-known bin-packing problem, where jobs correspond to items and the unit resource is a bin. However, the online variant of MPJS allows additional flexibility in that initially, one only commits to the scheduling time, but the allocation to the resources can be done later. In the bin-packing problem, this corresponds to the ability to move items across bins. Some relaxed versions of online bin-packing problems have already been studied, but none fundamentally capture the MPJS model studied in this paper. The paper describes the first competitive online algorithm to the MPJS problem called the harmonic rematching (HR) algorithm. The analysis shows that the HR algorithm has an asymptotic competitive ratio below 1.5. The fact that the current best lower bound on randomized online algorithms for the bin-packing problem is 1.536 highlights the fundamental difference between these two related models. Funding: The work of C. Escribe was partially supported by the Centre for Humane Innovations in Clinical Care [Grant 4000093665]. The work of M. Hu was partially funded by an MGH-MIT Fellowship. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0080 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0080},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {408-423},
  shortjournal = {Oper. Res.},
  title        = {Competitive algorithms for the online minimum peak job scheduling},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian process-based random search for continuous
optimization via simulation. <em>OR</em>, <em>73</em>(1), 385–407. (<a
href="https://doi.org/10.1287/opre.2021.0303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random search is an important category of algorithms to solve continuous optimization via simulation problems. To design an efficient random search algorithm, the handling of the triple “E” (i.e., exploration, exploitation and estimation) is critical. The first two E’s refer to the design of sampling distribution to balance explorative and exploitative searches, whereas the third E refers to the estimation of objective function values based on noisy simulation observations. In this paper, we propose a class of Gaussian process-based random search (GPRS) algorithms, which provide a new framework to handle the triple “E.” In each iteration, algorithms under the framework build a Gaussian process surrogate model to estimate the objective function based on single observation of each sampled solution and randomly sample solutions from a lower-bounded sampling distribution. Under the assumption of heteroscedastic and known simulation noise, we prove the global convergence of GPRS algorithms. Moreover, for Gaussian processes having continuously differentiable sample paths, we show that the rate of convergence of GPRS algorithms can be no slower than O p ( n − 1 / ( d + 2 ) ) . Then, we introduce a specific GPRS algorithm to show how to design an integrated GPRS algorithm with adaptive sampling distributions and how to implement the algorithm efficiently. Numerical experiments show that the algorithm has good performances, even for problems where the variances of simulation noises are unknown. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72031007, 72091211, 71931007]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0303 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0303},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {385-407},
  shortjournal = {Oper. Res.},
  title        = {Gaussian process-based random search for continuous optimization via simulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expanding service capabilities through an on-demand
workforce. <em>OR</em>, <em>73</em>(1), 363–384. (<a
href="https://doi.org/10.1287/opre.2021.0651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An on-demand workforce can greatly benefit a traditional call center by allowing it to adjust its service capacity on demand quickly. Despite its conceptual elegance, the operationalization of this process is challenging due to the various sources of randomness involved. The purpose of this paper is to help call centers enhance service levels while keeping operating expenses low by taking advantage of an on-call pool of temporary agents in day-to-day operations. For that purpose, we develop a two-stage decision model in which the first stage seeks the optimal mix of permanent and on-call staff, and the second stage seeks a joint on-demand staffing and call scheduling policy to minimize the associated cost given the base staffing level and the size of the on-call pool. Because the exact analysis of the two-stage decision model seems analytically intractable, we resort to an approximation in a suitable asymptotic regime. In that regime, we characterize the system dynamics of the service operation and derive an optimal joint on-demand staffing and call scheduling policy for the second-stage problem, which in turn is used to find an approximate solution to the first-stage problem. In particular, the derived policy for the second-stage problem involves tapping into the on-call pool to procure a team of on-demand agents when the number of calls to be processed exceeds a certain threshold and dismissing them when it falls below another threshold; additionally, the call scheduling rule shows an unusual pattern due to the interplay between staffing and scheduling decisions. Extensive numerical studies under realistic parameter settings show that the solution approach we propose can achieve significant cost savings. Funding: W. Liu has been supported by the President’s Graduate Fellowship at the National University of Singapore. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0651 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0651},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {363-384},
  shortjournal = {Oper. Res.},
  title        = {Expanding service capabilities through an on-demand workforce},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scores for multivariate distributions and level sets.
<em>OR</em>, <em>73</em>(1), 344–362. (<a
href="https://doi.org/10.1287/opre.2020.0365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasts of multivariate probability distributions are required for a variety of applications. Scoring rules enable the evaluation of forecast accuracy and comparison between forecasting methods. We propose a theoretical framework for scoring rules for multivariate distributions that encompasses the existing quadratic score and multivariate continuous ranked probability score. We demonstrate how this framework can be used to generate new scoring rules. In some multivariate contexts, it is a forecast of a level set that is needed, such as a density level set for anomaly detection or the level set of the cumulative distribution as a measure of risk. This motivates consideration of scoring functions for such level sets. For univariate distributions, it is well established that the continuous ranked probability score can be expressed as the integral over a quantile score. We show that, in a similar way, scoring rules for multivariate distributions can be decomposed to obtain scoring functions for level sets. Using this, we present scoring functions for different types of level sets, including density level sets and level sets for cumulative distributions. To compute the scores, we propose a simple numerical algorithm. We perform a simulation study to support our proposals, and we use real data to illustrate usefulness for forecast combining and conditional value at risk estimation. Funding: The work of S. Li was supported by the National Natural Science Foundation of China [Grant 12201399] and the Shanghai Frontier Research Institute for Modern Analysis.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0365},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {344-362},
  shortjournal = {Oper. Res.},
  title        = {Scores for multivariate distributions and level sets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving efficiency in black-box simulation of distribution
tails with self-structuring importance samplers. <em>OR</em>,
<em>73</em>(1), 325–343. (<a
href="https://doi.org/10.1287/opre.2021.0331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel importance sampling (IS) scheme for estimating distribution tails of performance measures modeled with a rich set of tools, such as linear programs, integer linear programs, piecewise linear/quadratic objectives, feature maps specified with deep neural networks, etc. The conventional approach of explicitly identifying efficient changes of measure suffers from feasibility and scalability concerns beyond highly stylized models because of their need to be tailored intricately to the objective and the underlying probability distribution. This bottleneck is overcome in the proposed scheme with an elementary transformation that is capable of implicitly inducing an effective IS distribution in a variety of models by replicating the concentration properties observed in less rare samples. This novel approach is guided by developing a large deviations principle that brings out the phenomenon of self-similarity of optimal IS distributions. The proposed sampler is the first to attain asymptotically optimal variance reduction across a spectrum of multivariate distributions despite being oblivious to the specifics of the underlying model. Its applicability is illustrated with contextual shortest-path and portfolio credit risk models informed by neural networks. Funding: This work was supported by the Singapore Ministry of Education Academic Research Fund [Grant MOE2019-T2-2-163]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0331 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0331},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {325-343},
  shortjournal = {Oper. Res.},
  title        = {Achieving efficiency in black-box simulation of distribution tails with self-structuring importance samplers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projective hedging algorithms for multistage stochastic
programming, supporting distributed and asynchronous implementation.
<em>OR</em>, <em>73</em>(1), 311–324. (<a
href="https://doi.org/10.1287/opre.2022.0228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a decomposition algorithm for multistage stochastic programming that resembles the progressive hedging method of Rockafellar and Wets but is provably capable of several forms of asynchronous operation. We derive the method from a class of projective operator splitting methods fairly recently proposed by Combettes and Eckstein, significantly expanding the known applications of those methods. Our derivation assures convergence for convex problems whose feasible set is compact, subject to some standard regularity conditions and a mild “fairness” condition on subproblem selection. The method’s convergence guarantees are deterministic and do not require randomization, in contrast to other proposed asynchronous variations of progressive hedging. Computational experiments described in an online appendix show the method to outperform progressive hedging on large-scale problems in a highly parallel computing environment. Funding: This work was supported by the National Science Foundation Directorate of Computer and Information Science and Engineering [Grant CCF-1617617] and U.S. Department of Energy [Office of Electricity’s Advanced Grid Modeling program]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0228 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0228},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {311-324},
  shortjournal = {Oper. Res.},
  title        = {Projective hedging algorithms for multistage stochastic programming, supporting distributed and asynchronous implementation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavior-aware queueing: The finite-buffer setting with many
strategic servers. <em>OR</em>, <em>73</em>(1), 290–310. (<a
href="https://doi.org/10.1287/opre.2023.2487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service system design is often informed by queueing theory. Traditional queueing theory assumes that servers work at constant speeds. That is reasonable in computer science and manufacturing contexts. However, servers in service systems are people, and in contrast to machines, the incentives created by design decisions influence their work speeds. We study how server work speed is affected by managerial decisions concerning (i) how many servers to staff and how much to pay them and (ii) whether and when to turn away customers in the context of many-server queues with finite or infinite buffers ( M / M / N / k with k ∈ Z + ∪ { ∞ } ) in which the work speeds emerge as the solution to a noncooperative game. We show that a symmetric equilibrium always exists in a loss system ( N = k ) and provide conditions for equilibrium existence in a single-server system ( N = 1). For the general M / M / N / k system, we provide a sufficient condition for the existence of a solution to the first-order condition and bounds on such a solution; however, showing that it is an equilibrium is challenging because of the existence of multiple local maxima in the utility function. Nevertheless, in an asymptotic regime in which demand becomes large, the utility function becomes concave, allowing us to characterize underloaded, critically loaded, and overloaded equilibria. Funding: This work was supported in part by funding from the Social Sciences and Humanities Research Council of Canada [Grant 430-2020-00334] and the Charles M. Harper Faculty Fellowship at the University of Chicago Booth School of Business. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2487 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2487},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {290-310},
  shortjournal = {Oper. Res.},
  title        = {Behavior-aware queueing: The finite-buffer setting with many strategic servers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—online matching with bayesian rewards.
<em>OR</em>, <em>73</em>(1), 278–289. (<a
href="https://doi.org/10.1287/opre.2021.0499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study in this paper an online matching problem where a central platform needs to match a number of limited resources to different groups of users that arrive sequentially over time. The reward of each matching option depends on both the type of resource and the time period the user arrives. The matching rewards are assumed to be unknown but drawn from probability distributions that are known a priori. The platform then needs to learn the true rewards online based on real-time observations of the matching results. The goal of the central platform is to maximize the total reward from all of the matchings without violating the resource capacity constraints. We formulate this matching problem with Bayesian rewards as a Markovian multiarmed bandit problem with budget constraints, where each arm corresponds to a pair of a resources and a time period. We devise our algorithm by first finding policies for each single arm separately via a relaxed linear program and then “assembling” these policies together through judicious selection criteria and well-designed pulling orders. We prove that the expected reward of our algorithm is at least 1 2 ( 2 − 1 ) of the expected reward of an optimal algorithm. Funding: The authors thank the Massachusetts Institute of Technology (MIT)-IBM partnership in Artificial Intelligence and the MIT Data Science Laboratory for support. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0499 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0499},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {278-289},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Online matching with bayesian rewards},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—a stationary infinite-horizon supply contract
under asymmetric inventory information. <em>OR</em>, <em>73</em>(1),
270–277. (<a href="https://doi.org/10.1287/opre.2020.0495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a decentralized supply chain in which a supplier sells goods to a retailer facing general random demand over an infinite horizon. The retailer satisfies the demand to the extent of the inventory on hand. The retailer has private information about the retailer’s stock in each period, and the supplier offers the retailer a supply contract menu to account for the information asymmetry. We obtain a necessary condition for optimizing a long-term stationary truth-telling contract under general demand and belief distributions. We apply it to a batch-order contract, which replenishes a prespecified inventory quantity for a fixed payment in each period only when the retailer’s beginning inventory becomes zero. Methodologically, we formulate the supplier’s contract design as a calculus of variations problem and apply the concept of Gâteaux derivative to obtain these results. This methodology can potentially be applied to other dynamic contracting problems. Funding: A. Bensoussan acknowledges support from the National Science Foundation [Grant NSF-DMS 220 47 95]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2020.0495 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0495},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {270-277},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A stationary infinite-horizon supply contract under asymmetric inventory information},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—conic mixed-binary sets: Convex hull
characterizations and applications. <em>OR</em>, <em>73</em>(1),
251–269. (<a href="https://doi.org/10.1287/opre.2020.0827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a general conic mixed-binary set where each homogeneous conic constraint j involves an affine function of independent continuous variables and an epigraph variable associated with a nonnegative function, f j , of common binary variables. Sets of this form naturally arise as substructures in a number of applications, including mean-risk optimization, chance-constrained problems, portfolio optimization, lot sizing and scheduling, fractional programming, variants of the best subset selection problem, a class of sparse semidefinite programs, and distributionally robust chance-constrained programs. We give a convex hull description of this set that relies on simultaneous characterization of the epigraphs of f j ’s, which is easy to do when all functions f j ’s are submodular. Our result unifies and generalizes an existing result in two important directions. First, it considers multiple general convex cone constraints instead of a single second-order cone type constraint. Second, it takes arbitrary nonnegative functions instead of a specific submodular function obtained from the square root of an affine function. We close by demonstrating the applicability of our results in the context of a number of problem classes. Funding: The research is supported, in part, by ONR [Grants N00014-19-1-2321 and N00014-22-1-2602], AFOSR [Grant FA9550-22-1-0365], the Institute for Basic Science [IBS-R029-C1, Y2], the FOUR Brain Korea 21 Program [NRF-5199990113928], the National Research Foundation of Korea [NRF-2022M3J6A1063021], the KAIST Starting Fund [KAIST-G04220016], NSF [Grant CMMI 1454548], and Early Postdoc Mobility Fellowship SNSF [Grant P2ELP2_195149].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0827},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {251-269},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Conic mixed-binary sets: Convex hull characterizations and applications},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—a new rate-optimal sampling allocation for
linear belief models. <em>OR</em>, <em>73</em>(1), 239–250. (<a
href="https://doi.org/10.1287/opre.2022.2337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a new optimal sampling budget allocation for belief models based on linear regression with continuous covariates, where the expected response is interpreted as the value of the covariate vector, and an “error” occurs if a lower-valued vector is falsely identified as being better than a higher-valued one. Our allocation optimizes the rate at which the probability of error converges to zero using a large deviations theoretic characterization. This is the first large deviations-based optimal allocation for continuous decision spaces, and it turns out to be considerably simpler and easier to implement than allocations that use discretization. We give a practicable sequential implementation and illustrate its empirical potential. Funding: This work was supported by the National Science Foundation [Grant CMMI-2112828].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2337},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {239-250},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A new rate-optimal sampling allocation for linear belief models},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal no-regret learning in repeated first-price auctions.
<em>OR</em>, <em>73</em>(1), 209–238. (<a
href="https://doi.org/10.1287/opre.2020.0282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study online learning in repeated first-price auctions where a bidder, only observing the winning bid at the end of each auction, learns to adaptively bid to maximize the cumulative payoff. To achieve this goal, the bidder faces censored feedback: If the bidder wins the bid, then the bidder is not able to observe the highest bid of the other bidders, which we assume is i.i.d. drawn from an unknown distribution. In this paper, we develop the first learning algorithm that achieves a near-optimal O ˜ ( T ) regret bound, by exploiting two structural properties of first-price auctions, that is, the specific feedback structure and payoff function. We first formulate the feedback structure in first-price auctions as partially ordered contextual bandits, a combination of the graph feedback across actions (bids), the cross-learning across contexts (private values), and a partial order over the contexts. We establish both strengths and weaknesses of this framework by showing a curious separation that a regret nearly independent of the action/context sizes is possible under stochastic contexts but is impossible under adversarial contexts. In particular, this framework leads to an O ( T log 2.5 T ) regret for first-price auctions when the bidder’s private values are independent and identically distributed. Despite the limitation of this framework, we further exploit the special payoff function of first-price auctions to develop a sample-efficient algorithm even in the presence of adversarially generated private values. We establish an O ( T log 3 T ) regret bound for this algorithm, hence providing a complete characterization of optimal learning guarantees for first-price auctions. Funding: This project was supported in part by the National Science Foundation [Awards CCF-2106467 and CCF-2106508]. Y. Han and T. Weissman were partially supported by the Yahoo Faculty Research and Engagement Program.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0282},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {209-238},
  shortjournal = {Oper. Res.},
  title        = {Optimal no-regret learning in repeated first-price auctions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to persuade on the fly: Robustness against
ignorance. <em>OR</em>, <em>73</em>(1), 194–208. (<a
href="https://doi.org/10.1287/opre.2021.0529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by information sharing in online platforms, we study repeated persuasion between a sender and a stream of receivers, where, at each time, the sender observes a payoff-relevant state drawn independently and identically from an unknown distribution and shares state information with the receivers, who each choose an action. The sender seeks to persuade the receivers into taking actions aligned with the sender’s preference by selectively sharing state information. However, in contrast to the standard models, neither the sender nor the receivers know the distribution, and the sender has to persuade while learning the distribution on the fly. We study the sender’s learning problem of making persuasive action recommendations to achieve low regret against the optimal persuasion mechanism with the knowledge of the distribution. To do this, we first propose and motivate a persuasiveness criterion for the unknown distribution setting that centers robustness as a requirement in the face of uncertainty. Our main result is an algorithm that, with high probability, is robustly persuasive and achieves O ( T log T ) regret, where T is the horizon length. Intuitively, at each time, our algorithm maintains a set of candidate distribution and chooses a signaling mechanism that is simultaneously persuasive for all of them. Core to our proof is a tight analysis about the cost of robust persuasion, which may be of independent interest. We further prove that this regret order is optimal (up to logarithmic terms) by showing that no algorithm can achieve regret better than Ω ( T ) . Funding: Y. Zu and K. Iyer gratefully acknowledge partial support from the National Science Foundation (NSF) Division of Civil, Mechanical, and Manufacturing Innovation [Grant CMMI-2002156]. H. Xu is supported by the NSF Division of Computing and Communication Foundations [Award CCF-2303372], the Army Research Office [Award W911NF-23-1-0030], and a Google Faculty Research Award. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0529 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0529},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {194-208},
  shortjournal = {Oper. Res.},
  title        = {Learning to persuade on the fly: Robustness against ignorance},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pricing optimal outcomes in coupled and non-convex markets:
Theory and applications to electricity markets. <em>OR</em>,
<em>73</em>(1), 178–193. (<a
href="https://doi.org/10.1287/opre.2023.0401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world markets, participants have non-convex preferences, and the allocation problem needs to consider complex constraints. Electricity markets are a prime example, but similar problems appear in many markets, which has led to a growing literature on market design. Competitive equilibrium does not generally exist in such markets. Today, power markets use heuristic pricing rules based on the dual of a relaxed allocation problem. With increasing levels of renewables, these rules have come under scrutiny as they lead to high out-of-market side payments to some participants and inadequate congestion signals. We show that existing pricing heuristics optimize specific design goals that can be conflicting. The tradeoffs can be substantial, and we establish that the design of pricing rules is fundamentally a multiobjective optimization problem addressing different incentives. In addition to traditional multiobjective optimization techniques that involve weighting individual objectives, we introduce a novel parameter-free pricing rule that minimizes incentives for market participants to deviate locally. Our theoretical and experimental findings show how the new pricing rule capitalizes on the upsides of existing pricing rules under scrutiny today. It leads to prices that incur low make-whole payments while providing adequate congestion signals and low lost opportunity costs. Our suggested pricing rule does not require weighing objectives, it is computationally scalable, and balances tradeoffs in a principled manner, addressing a critical policy issue in electricity markets. Funding: The financial support from the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) [Grant BI 1057/9-1] is gratefully acknowledged. Supplemental Material: The computer code and data that supports the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0401 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0401},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {178-193},
  shortjournal = {Oper. Res.},
  title        = {Pricing optimal outcomes in coupled and non-convex markets: Theory and applications to electricity markets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Price interpretability of prediction markets: A convergence
analysis. <em>OR</em>, <em>73</em>(1), 157–177. (<a
href="https://doi.org/10.1287/opre.2022.0417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction markets are long known for prediction accuracy. This study systematically explores the fundamental properties of prediction markets, addressing questions about their information aggregation process and the factors contributing to their remarkable efficacy. We propose a novel multivariate utility–based mechanism that unifies several existing automated market-making schemes. Using this mechanism, we establish the convergence results for markets comprised of risk-averse traders who have heterogeneous beliefs and repeatedly interact with the market maker. We demonstrate that the resulting limiting wealth distribution aligns with the Pareto efficient frontier defined by the utilities of all market participants. With the help of this result, we establish analytical and numerical results for the limiting price in different market models. Specifically, we show that the limiting price converges to the geometric mean of agent beliefs in exponential utility-based markets. In risk measure-based markets, we construct a family of risk measures that satisfy the convergence criteria and prove that the price converges to a unique level represented by the weighted power mean of agent beliefs. In broader markets with constant relative risk aversion utilities, we reveal that the limiting price can be characterized by systems of equations that encapsulate agent beliefs, risk parameters, and wealth. Despite the impact of traders’ trading sequences on the limiting price, we establish a price invariance result for markets with a large trader population. Using this result, we propose an efficient approximation scheme for the limiting price. Numerical experiments demonstrate that the accuracy of this approximation scheme outperforms existing approximation methods across various scenarios. Our findings serve to aid market designers in better tailoring and adjusting the market-making mechanism for more effective opinion elicitation. Funding: This work was supported by the National Natural Science Foundation of China [Grants 71671045, 71971132, 72150002, 72201067, and 72394361], the InnoHK initiative of the Government of the HKSAR, Laboratory for AI-Powered Financial Technologies, the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [Grant 2023B1212010001], the Shanghai Research Center for Data Science and Decision Technology, and the Key Laboratory of Interdisciplinary Research of Computation and Economics, Ministry of Education, Shanghai University of Finance and Economics. Supplemental Material: The computer code and data that supports the findings of this study is available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0417 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0417},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {157-177},
  shortjournal = {Oper. Res.},
  title        = {Price interpretability of prediction markets: A convergence analysis},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of lookahead and approximate policy evaluation in
reinforcement learning with linear value function approximation.
<em>OR</em>, <em>73</em>(1), 139–156. (<a
href="https://doi.org/10.1287/opre.2022.0357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function approximation is widely used in reinforcement learning to handle the computational difficulties associated with very large state spaces. However, function approximation introduces errors that may lead to instabilities when using approximate dynamic programming techniques to obtain the optimal policy. Therefore, techniques such as lookahead for policy improvement and m -step rollout for policy evaluation are used in practice to improve the performance of approximate dynamic programming with function approximation. We quantitatively characterize the impact of lookahead and m -step rollout on the performance of approximate dynamic programming (DP) with function approximation. (i) Without a sufficient combination of lookahead and m -step rollout, approximate DP may not converge. (ii) Both lookahead and m -step rollout improve the convergence rate of approximate DP. (iii) Lookahead helps mitigate the effect of function approximation and the discount factor on the asymptotic performance of the algorithm. Our results are presented for two approximate DP methods: one that uses least-squares regression to perform function approximation and another that performs several steps of gradient descent of the least-squares objective in each iteration. Funding: The research presented here was supported in part by a grant from Sandia National Labs and the NSF [Grants CCF 1934986, CCF 2207547, CNS 2106801], ONR [Grant N00014-19-1-2566], and ARO [Grant W911NF-19-1-0379].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0357},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {139-156},
  shortjournal = {Oper. Res.},
  title        = {The role of lookahead and approximate policy evaluation in reinforcement learning with linear value function approximation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online learning for constrained assortment optimization
under markov chain choice model. <em>OR</em>, <em>73</em>(1), 109–138.
(<a href="https://doi.org/10.1287/opre.2022.0693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a dynamic assortment selection problem where arriving customers make purchase decisions among offered products from a universe of products under a Markov chain choice (MCC) model. The retailer only observes the assortment and the customer’s single choice per period. Given limited display capacity, resource constraints, and no a priori knowledge of problem parameters, the retailer’s objective is to sequentially learn the choice model and optimize cumulative revenues over a finite selling horizon. We develop a fast linear system based explore-then-commit (FastLinETC for short) learning algorithm that balances the tradeoff between exploration and exploitation. The algorithm can simultaneously estimate the arrival and transition probabilities in the MCC model by solving a linear system of equations and determining the near-optimal assortment based on these estimates. Furthermore, our consistent estimators offer superior computational times compared with existing heuristic estimation methods, which often suffer from inconsistency or a significant computational burden. Funding: The research of Q. Luo is partially supported by the National Science Foundation [Grant CMMI-2308750]. The research of Z. Huang is partially supported by the Shanghai Sailing Program [Grant 22YF1451100 and the Fundamental Research Funds for the Central Universities]. The research of C. Shi is partially supported by Amazon [Research Award].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0693},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {109-138},
  shortjournal = {Oper. Res.},
  title        = {Online learning for constrained assortment optimization under markov chain choice model},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drone-delivery network for opioid overdose: Nonlinear
integer queueing-optimization models and methods. <em>OR</em>,
<em>73</em>(1), 86–108. (<a
href="https://doi.org/10.1287/opre.2022.0489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new stochastic emergency network design model that uses a fleet of drones to quickly deliver naloxone in response to opioid overdoses. The network is represented as a collection of M / G / K queueing systems in which the capacity K of each system is a decision variable, and the service time is modeled as a decision-dependent random variable. The model is a queuing-based optimization problem which locates fixed (drone bases) and mobile (drones) servers and determines the drone dispatching decisions and takes the form of a nonlinear integer problem intractable in its original form. We develop an efficient reformulation and algorithmic framework. Our approach reformulates the multiple nonlinearities (fractional, polynomial, exponential, factorial terms) to give a mixed-integer linear programming (MILP) formulation. We demonstrate its generalizability and show that the problem of minimizing the average response time of a collection of M / G / K queueing systems with unknown capacity K is always MILP-representable. We design an outer approximation branch-and-cut algorithmic framework that is computationally efficient and scales well. The analysis based on real-life data reveals that drones can in Virginia Beach: (1) decrease the response time by 82%, (2) increase the survival chance by more than 273%, (3) save up to 33 additional lives per year, and (4) provide annually up to 279 additional quality-adjusted life years. Funding: M. A. Lejeune acknowledges the support of the National Science Foundation [Grant ECCS-2114100] and the Office of Naval Research [Grant N00014-22-1-2649]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0489 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0489},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {86-108},
  shortjournal = {Oper. Res.},
  title        = {Drone-delivery network for opioid overdose: Nonlinear integer queueing-optimization models and methods},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the security of united states elections with
robust optimization. <em>OR</em>, <em>73</em>(1), 61–85. (<a
href="https://doi.org/10.1287/opre.2023.0422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For more than a century, election officials across the United States have inspected voting machines before elections using a procedure called logic and accuracy testing (LAT). This procedure consists of election officials casting a test deck of ballots into each voting machine and confirming the machine produces the expected vote total for each candidate. We bring a scientific perspective to LAT by introducing the first formal approach to designing test decks with rigorous security guarantees. Specifically, our approach employs robust optimization to find test decks that are guaranteed to detect any voting machine misconfiguration that would cause votes to be swapped across candidates. Of all the test decks with this security guarantee, our robust optimization problem yields the test deck with the minimum number of ballots, thereby minimizing implementation costs for election officials. To facilitate deployment at scale, we develop a practically efficient exact algorithm for solving our robust optimization problems based on the cutting plane method. In partnership with the Michigan Bureau of Elections, we retrospectively applied our approach to all 6,928 ballot styles from Michigan’s November 2022 general election; this retrospective study reveals that the test decks with rigorous security guarantees obtained by our approach require, on average, only 1.2% more ballots than current practice. Our approach has since been piloted in real-world elections by the Michigan Bureau of Elections as a low-cost way to improve election security and increase public trust in democratic institutions. Funding: This research was supported by the U.S. National Science Foundation [Grant CNS-1518888]. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study are available at https://doi.org/10.1287/opre.2023.0422 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0422},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {61-85},
  shortjournal = {Oper. Res.},
  title        = {Improving the security of united states elections with robust optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preventing price-mediated contagion due to fire sales
externalities: Strategic foundations of macroprudential regulation.
<em>OR</em>, <em>73</em>(1), 40–60. (<a
href="https://doi.org/10.1287/opre.2023.0237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We offer a stress test framework in which interaction between regulated banks occurs through the impact they may have on asset prices when they deleverage. Because banks are constrained to maintain their risk-based capital ratio higher than a threshold, the deleveraging problem yields a generalized game in which the solvency constraint of each bank depends on the decisions of the others. We analyze the game under microprudential but also under macroprudential regulation. Microprudential regulation corresponds to the standard situation in which each bank considers its own solvency constraint, whereas macroprudential regulation is defined as the situation in which each bank faces a systemic constraint in that it must consider the solvency constraints of all the banks. When bankruptcies can be avoided, we show that a Nash equilibrium generically exists under macroprudential regulation, contagion of failures due to fire sales externalities is prevented, whereas it may not exist under microprudential regulation. We eventually analyze the deleveraging problem when bankruptcies cannot be avoided and present additional results. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0237 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0237},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {40-60},
  shortjournal = {Oper. Res.},
  title        = {Preventing price-mediated contagion due to fire sales externalities: Strategic foundations of macroprudential regulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application-driven learning: A closed-loop prediction and
optimization approach applied to dynamic reserves and demand
forecasting. <em>OR</em>, <em>73</em>(1), 22–39. (<a
href="https://doi.org/10.1287/opre.2023.0565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision making is generally modeled as sequential forecast-decision steps with no feedback, following an open-loop approach. For instance, in the electricity sector, system operators use the forecast-decision approach followed by ad hoc rules to determine reserve requirements and biased net load forecasts to guard the system against renewable generation and demand uncertainty. Such procedures lack technical formalism to minimize operating and reliability costs. We present a new closed-loop framework, named application-driven learning, in which the best forecasting model is defined according to a given application cost function. We consider applications in which the decision-making process is driven by two-stage optimization schemes fed by multivariate point forecasts. We present our estimation method as a bilevel optimization problem and prove convergence to the best estimator regarding the expected application cost. We propose two solution methods: an exact method based on the KKT conditions of the second-level problems and a scalable heuristic suitable for decomposition. Thus, we offer an alternative scientifically grounded approach to current ad hoc procedures implemented in industry practices. We test the proposed methodology with real data and large-scale systems with thousands of buses. Results show that the proposed methodology is scalable and consistently performs better than the standard open-loop approach. Funding: J. Dias Garcia was partially supported by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) [Finance Code 001]. A. Street was partially supported by Fundação de Amparo à Pesquisa do Estado do Rio de Janeiro (FAPERJ) and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq). T. Homem-de-Mello acknowledges the support of Grant FONDECYT 1221770 from ANID, Chile. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0565 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0565},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {22-39},
  shortjournal = {Oper. Res.},
  title        = {Application-driven learning: A closed-loop prediction and optimization approach applied to dynamic reserves and demand forecasting},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error propagation in asymptotic analysis of the data-driven
(s, s) inventory policy. <em>OR</em>, <em>73</em>(1), 1–21. (<a
href="https://doi.org/10.1287/opre.2020.0568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study periodic review stochastic inventory control in the data-driven setting where the retailer makes ordering decisions based only on historical demand observations without any knowledge of the probability distribution of the demand. Because an ( s , S )-policy is optimal when the demand distribution is known, we investigate the statistical properties of the data-driven ( s , S )-policy obtained by recursively computing the empirical cost-to-go functions. This policy is inherently challenging to analyze because the recursion induces propagation of the estimation error backward in time. In this work, we establish the asymptotic properties of this data-driven policy by fully accounting for the error propagation. In this setting, the empirical cost-to-go functions for the estimated parameters are not i.i.d. sums because of the error propagation. Our main methodological innovation comes from an asymptotic representation for multi-sample U -processes in terms of i.i.d. sums. This representation enables us to apply empirical process theory to derive the influence functions of the estimated parameters and to establish joint asymptotic normality. Based on these results, we also propose an entirely data-driven estimator of the optimal expected cost, and we derive its asymptotic distribution. We demonstrate some useful applications of our asymptotic results, including sample size determination and interval estimation. Funding: This work was supported by Singapore MOE AcRF Tier 2 [A-8001052-00-00] and the National Natural Science Foundation of China [72071138]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2020.0568 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0568},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Oper. Res.},
  title        = {Error propagation in asymptotic analysis of the data-driven (s, s) inventory policy},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="trsc---12">TRSC - 12</h2>
<ul>
<li><details>
<summary>
(2025). A 0,1 linear programming approach to deadlock detection and
management in railways. <em>TRSC</em>, <em>59</em>(1), 187–205. (<a
href="https://doi.org/10.1287/trsc.2024.0521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In railway systems, a deadlock occurs when trains accidentally occupy positions that prevent each other from moving forward. Although deadlocks are rare events, they do occur from time to time, requiring costly recourse actions and generating significant knock-on delays. In this paper, we present a noncompact 0,1 linear programming formulation and a methodology for discovering (possibly future) deadlocks and the subsequent implementation of optimal recovery measures. The approach is implemented in a tool to dispatch trains in real time developed in cooperation with Union Pacific (UP) and currently in operations on the entire UP network. Funding: This work was partially funded by Europe’s Rail, Flagship Project MOTIONAL [Action Horizon JU Innovation, Project 101101973]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2024.0521 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0521},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {187-205},
  shortjournal = {Trans. Sci.},
  title        = {A 0,1 linear programming approach to deadlock detection and management in railways},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-dock trailer scheduling with workforce constraints: A
dynamic discretization discovery approach. <em>TRSC</em>,
<em>59</em>(1), 165–186. (<a
href="https://doi.org/10.1287/trsc.2023.0406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Less-than-truckload (LTL) freight carriers operate consolidation networks that utilize cross-docking terminals to facilitate the transfer of freight between trailers and enhance trailer utilization. This research addresses the problem of determining an optimal schedule for unloading inbound trailers at specific unloading doors using teams of dock workers. The optimization objective is chosen to ensure that outbound trailers are loaded with minimal delay with respect to their target loading due dates. Formulating this problem, which is known to be NP-hard, using a typical time-expanded network often results in an excessively large mixed-integer programming (MIP) model. To overcome this challenge, we propose an exact dynamic discretization discovery (DDD) algorithm that iteratively solves MIPs formulated over partial networks. The algorithm employs a combination of a simple time discretization refinement strategy to progressively refine the partial network until a provably optimal solution is obtained. We demonstrate the effectiveness of the algorithm in solving problem instances representative of a large L-shaped cross-dock in Atlanta. The DDD algorithm outperforms solving the model formulated over a complete time-expanded network with a commercial solver in terms of both computational time and solution quality for practical instances with 180 trailers, 44 unloading doors, and 57 loading doors. Additionally, we compare the DDD algorithm with a state-of-the-art interval scheduling approach using instances from a previous study with a different objective function and additional constraints. The DDD algorithm is computationally faster for most of the small and medium instances and achieves competitive bounds for the larger instances. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0406 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0406},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {165-186},
  shortjournal = {Trans. Sci.},
  title        = {Cross-dock trailer scheduling with workforce constraints: A dynamic discretization discovery approach},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiperiod, multicommodity, capacitated international
agricultural trade network equilibrium model with applications to
ukraine in wartime. <em>TRSC</em>, <em>59</em>(1), 143–164. (<a
href="https://doi.org/10.1287/trsc.2023.0294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world is facing immense challenges because of increasing strife and the impacts of climate change with accompanying disasters, both sudden-onset as well as slow-onset ones, which have affected the trade of agricultural commodities needed for food security. In this paper, a multiperiod, multicommodity, international, agricultural trade network equilibrium model is constructed with capacity constraints on the production, transportation, and storage of agricultural commodities. The model allows for multiple routes between supply and demand country markets, different modes of transport, and storage in the producing and consuming countries as well as in the intermediate countries. The generality of the underlying functions, coupled with the capacity constraints, allow for the modeling of competition among agricultural commodities for production, transportation, and storage. The capacity constraints also enable the quantification of various disaster-related disruptions to production, transportation, and storage on the volumes of commodity flows as well as on the prices. A series of numerical examples inspired by the effects of Russia’s full-scale invasion of Ukraine on agricultural trade is presented, and the results are analyzed to provide insights into food insecurity issues caused by the war.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0294},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {143-164},
  shortjournal = {Trans. Sci.},
  title        = {A multiperiod, multicommodity, capacitated international agricultural trade network equilibrium model with applications to ukraine in wartime},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the concept of opportunity cost in integrated demand
management and vehicle routing. <em>TRSC</em>, <em>59</em>(1), 125–142.
(<a href="https://doi.org/10.1287/trsc.2024.0644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated demand management and vehicle routing problems are characterized by a stream of customers arriving dynamically over a booking horizon and requesting logistical services, fulfilled by a given fleet of vehicles during a service horizon. Prominent examples are attended home delivery and same-day delivery problems, where customers commonly have heterogeneous preferences regarding service fulfillment and requests differ in profitability. Thus, demand management methods are applied to steer the booking process to maximize total profit considering the cost of the routing decisions for the resulting orders. To measure the requests’ profitability for any demand management method, it is common to estimate their opportunity cost. In the context of integrated demand management and vehicle routing problems, this estimation differs substantially from the estimation in the well-examined demand management problems of traditional revenue management applications as, for example, found in the airline or car rental industry. This is because of the unique interrelation of demand control decisions and vehicle routing decisions as it inhibits a clear quantification and attribution of cost, and of displaced revenue, to certain customer requests. In this paper, we extend the theoretical foundation of opportunity cost in integrated demand management and vehicle routing problems. By defining and analyzing a generic Markov decision process model, we formally derive a definition of opportunity cost and prove opportunity cost properties on a general level. Hence, our findings are valid for a wide range of specific problems. Further, based on these theoretical findings, we propose approximation approaches that have not yet been applied in the existing literature, and evaluate their potential in a computational study. Thereby, we provide evidence that the theoretical results can be practically exploited in the development of solution algorithms. Funding: This work was supported by the University of the Bundeswehr Munich. Supplemental Material: The online appendices are available at https://doi.org/10.1287/trsc.2024.0644 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0644},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {125-142},
  shortjournal = {Trans. Sci.},
  title        = {On the concept of opportunity cost in integrated demand management and vehicle routing},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal multi-agent pickup and delivery using
branch-and-cut-and-price algorithms. <em>TRSC</em>, <em>59</em>(1),
104–124. (<a href="https://doi.org/10.1287/trsc.2023.0268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of agents and a set of pickup-delivery requests located on a two-dimensional grid map, the multi-agent pickup and delivery problem assigns the requests to the agents such that every agent moves from its start location to the locations of its assigned requests and finally, to its end location without colliding into other agents and that the sum of arrival times is minimized. This paper proposes two exact branch-and-cut-and-price algorithms for the problem. The first algorithm performs a three-level search. A high-level master problem selects an optimal sequence of requests and a path for every agent from a large collection. A mid-level sequencing problem and a low-level navigation problem are solved simultaneously to incrementally enlarge the collection of request sequences and paths. The second algorithm first solves the sequencing problem to find a set of request sequences and then solves the navigation problem to determine if paths compatible with the request sequences exist. Experimental results indicate that the integrated algorithm solves more instances with higher congestion, and the deferred algorithm solves more instances with lower congestion and could scale to 100 agents and 100 requests, significantly higher than a state-of-the-art suboptimal approach. Funding: This research was supported by the Australian Research Council [Discovery Early Career Researcher Award DE240100042 and Discovery Projects DP190100013 and DP200100025] and by Amazon. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0268 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0268},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {104-124},
  shortjournal = {Trans. Sci.},
  title        = {Optimal multi-agent pickup and delivery using branch-and-cut-and-price algorithms},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heatmap design for probabilistic driver repositioning in
crowdsourced delivery. <em>TRSC</em>, <em>59</em>(1), 81–103. (<a
href="https://doi.org/10.1287/trsc.2022.0418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the use of heatmaps as a control lever to manage the probabilistic repositioning of independent drivers in crowdsourced delivery platforms. The platform aims to maximize order fulfillment by dynamically matching drivers and orders and selecting heatmaps that trigger the probabilistic flow of unmatched drivers to balance driver supply and delivery requests across the service region. We develop a Markov decision process (MDP) model to sequentially select matching and heatmap decisions in which the repositioning behavior of drivers is captured by a multinomial logit discrete choice model. Because of the curse of dimensionality and the endogenous uncertainty of driver repositioning, the MDP model is solved using a rolling-horizon stochastic lookahead policy. This policy decomposes matching and heatmap decisions into two optimization problems: a two-stage stochastic programming upper bounding problem for matching decisions and a mixed-integer programming problem for heatmap decisions. We also propose a simple policy for efficiently solving large-scale problems. An extensive computational study on instances derived from the Chicago ride-hailing data set is conducted. Computational experiments demonstrate the value of heatmaps in improving order fulfillment beyond the level achieved by matching alone (up to 25%) and identify conditions that affect the benefit of using heatmaps to guide driver repositioning. Funding: The authors gratefully acknowledge the support of the Natural Sciences and Engineering Research Council of Canada through Discovery Grants [Grants RGPIN-2024-04881, RGPIN-2020-04498, and RGPIN-2019-06207] awarded to the first, second, and third authors, respectively. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2022.0418 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2022.0418},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {81-103},
  shortjournal = {Trans. Sci.},
  title        = {Heatmap design for probabilistic driver repositioning in crowdsourced delivery},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact solution of the vehicle routing problem with drones.
<em>TRSC</em>, <em>59</em>(1), 60–80. (<a
href="https://doi.org/10.1287/trsc.2024.0544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem with drones (VRP-D) that we consider is an extension of the capacitated vehicle routing problem in which the fleet consists of trucks equipped with one drone each. A truck and its drone can either move together or separately. A truck can release its drone at the depot or at a customer location and must pick it up later at another customer or the depot location. In this way, both trucks and drones deliver goods to customers working together as synchronized working units. A feasible route has to satisfy the capacity constraints of both the truck and the drone. A feasible solution to the VRP-D is a set of feasible routes such that each customer is served exactly once by either a truck or a drone. We investigate two standard objectives considered in the literature, that is, the minimization of the total routing cost and the sum of the routes’ durations. To solve the VRP-D exactly, we develop a branch-price-and-cut (BPC) algorithm. In particular, we present a new forward and implicit bidirectional labeling algorithm defined over an artificial network to solve the column-generation subproblems. The new bidirectional labeling algorithm substantially accelerates the solution process compared with its monodirectional counterpart. The time needed to solve the pricing problems is reduced by 55% on average when minimizing routing costs and by 30% when minimizing the sum of the routes’ durations. In further computational experiments, we analyze algorithmic components of the BPC algorithm, compare the cost and duration objectives, and highlight the impact of the drones’ speed on the structure of VRP-D solutions. For the routing-cost minimization objective, our BPC algorithm is able to solve several VRP-D instances with 50 vertices to proven optimality within one hour of computation time. The same instances with duration minimization are more difficult, and the BPC algorithm provides only heuristic solutions with an average gap not exceeding 3%. Funding: This work was supported by Deutsche Forschungsgemeinschaft [Project 418727865, Grant IR 122/10-1]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2024.0544 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0544},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {60-80},
  shortjournal = {Trans. Sci.},
  title        = {Exact solution of the vehicle routing problem with drones},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The online shortest path problem: Learning travel times
using a multiarmed bandit framework. <em>TRSC</em>, <em>59</em>(1),
28–59. (<a href="https://doi.org/10.1287/trsc.2023.0196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of e-commerce, logistics companies often operate within extensive road networks without accurate knowledge of travel times for their specific fleet of vehicles. Moreover, millions of dollars are spent on routing services that fail to accurately capture the unique characteristics of the drivers and vehicles of the companies. In this work, we address the challenge faced by a logistic operator with limited travel time information, aiming to find the optimal expected shortest path between origin-destination pairs. We model this problem as an online shortest path problem, common to many last-mile routing settings; given a graph whose arcs’ travel times are stochastic and follow an unknown distribution, the objective is to find a vehicle route of minimum travel time from an origin to a destination. The planner progressively collects travel condition data as drivers complete their routes. Inspired by the combinatorial multiarmed bandit and kriging literature, we propose three methods with distinct features to effectively learn the optimal shortest path, highlighting the practical advantages of incorporating spatial correlation in the learning process. Our approach balances exploration (improving estimates for unexplored arcs) and exploitation (executing the minimum expected time path) using the Thompson sampling algorithm. In each iteration, our algorithm executes the path that minimizes the expected travel time based on data from a posterior distribution of the speeds of the arcs. We conduct a computational study comprising two settings: a set of four artificial instances and a real-life case study. The case study uses empirical data of taxis in the 17-km-radius area of the center of Beijing, encompassing Beijing’s “5th Ring Road.” In both settings, our algorithms demonstrate efficient and effective balancing of the exploration-exploitation trade-off.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0196},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {28-59},
  shortjournal = {Trans. Sci.},
  title        = {The online shortest path problem: Learning travel times using a multiarmed bandit framework},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plan your system and price for free: Fast algorithms for
multimodal transit operations. <em>TRSC</em>, <em>59</em>(1), 13–27. (<a
href="https://doi.org/10.1287/trsc.2022.0452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of jointly pricing and designing a smart transit system, where a transit agency (the platform ) controls a fleet of demand-responsive vehicles (cars) and a fixed line service (buses). The platform offers commuters a menu of options ( modes ) to travel between origin and destination (e.g., direct car trip, a bus ride, or a combination of the two), and commuters make a utility-maximizing choice within this menu, given the price of each mode. The goal of the platform is to determine an optimal set of modes to display to commuters, prices for these modes, and the design of the transit network in order to maximize the social welfare of the system. In this work, we tackle the commuter choice aspect of this problem, traditionally approached via computationally intensive bilevel programming techniques. In particular, we develop a framework that efficiently decouples the pricing and network design problem: Given an efficient (approximation) algorithm for centralized network design without prices , there exists an efficient (approximation) algorithm for decentralized network design with prices and commuter choice . We demonstrate the practicality of our framework via extensive numerical experiments on a real-world data set. We moreover explore the dependence of metrics such as welfare, revenue, and mode usage on (i) transfer costs and (ii) cost of contracting with on-demand service providers and exhibit the welfare gains of a fully integrated mobility system. Funding: This work was supported by the National Science Foundation [Awards CMMI-2308750, CNS-1952011, and CMMI-2144127]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2022.0452 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2022.0452},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {13-27},
  shortjournal = {Trans. Sci.},
  title        = {Plan your system and price for free: Fast algorithms for multimodal transit operations},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transportation science and logistics society best
dissertation award competition: Abstracts of 2024 winners.
<em>TRSC</em>, <em>59</em>(1), 5–12. (<a
href="https://doi.org/10.1287/trsc.2024_dissertation_award">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The journal is pleased to bring back the tradition of publishing the abstracts of the winners of the TSL Best Dissertation Award. The 2024 dissertation prize committee was chaired by Margaretha Gansterer. The other committee members were Shadi Sharif Azadeh, Justin Goodson, Tal Raviv, and Hai Wang.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024_dissertation_award},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {5-12},
  shortjournal = {Trans. Sci.},
  title        = {Transportation science and logistics society best dissertation award competition: Abstracts of 2024 winners},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2024 transportation science meritorious service awards.
<em>TRSC</em>, <em>59</em>(1), 3–4. (<a
href="https://doi.org/10.1287/trsc.2023.servawards.v59.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are pleased to announce the recipients of the Transportation Science Meritorious Service Awards. These awards recognize associate editors, special issue guest editors, and reviewers who have offered exceptional service in the review process. We truly appreciate all the efforts of the many volunteers who provide invaluable service to the journal. The 2024 recipients have distinguished themselves by the number of papers handled, their efficiency in handling papers, and the quality of their reviews.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.servawards.v59.n1},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {3-4},
  shortjournal = {Trans. Sci.},
  title        = {2024 transportation science meritorious service awards},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial statement. <em>TRSC</em>, <em>59</em>(1), 1–2. (<a
href="https://doi.org/10.1287/trsc.2025.ed.v59.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2025.ed.v59.n1},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Trans. Sci.},
  title        = {Editorial statement},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
