<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---21">TVCG - 21</h2>
<ul>
<li><details>
<summary>
(2025). The census-stub graph invariant descriptor. <em>TVCG</em>,
<em>31</em>(3), 1945–1961. (<a
href="https://doi.org/10.1109/TVCG.2024.3513275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ‘invariant descriptor’ captures meaningful structural features of networks, useful where traditional visualizations, like node-link views, face challenges like the ’hairball phenomenon’ (inscrutable overlap of points and lines). Designing invariant descriptors involves balancing abstraction and information retention, as richer data summaries demand more storage and computational resources. Building on prior work, chiefly the BMatrix—a matrix descriptor visualized as the invariant ’network portrait’ heatmap—we introduce BFS-Census, a new algorithm computing our Census data structures: Census-Node, Census-Edge, and Census-Stub. Our experiments show Census-Stub, which focuses on ’stubs’ (half-edges), has orders of magnitude greater discerning power (ability to tell non-isomorphic graphs apart) than any other descriptor in this study, without a difficult trade-off: the substantial increase in resolution doesn&#39;t come at a commensurate cost in storage space or computation power. We also present new visualizations—our Hop-Census polylines and Census-Census trajectories—and evaluate them using real-world graphs, including a sensitivity analysis that shows graph topology change maps to visual Census change.},
  archive      = {J_TVCG},
  author       = {Matt I. B. Oddo and Stephen Kobourov and Tamara Munzner},
  doi          = {10.1109/TVCG.2024.3513275},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1945-1961},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The census-stub graph invariant descriptor},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TimeLighting: Guided exploration of 2D temporal network
projections. <em>TVCG</em>, <em>31</em>(3), 1932–1944. (<a
href="https://doi.org/10.1109/TVCG.2024.3514858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In temporal (event-based) networks, time is a continuous axis, with real-valued time coordinates for each node and edge. Computing a layout for such graphs means embedding the node trajectories and edge surfaces over time in a $2D + t$ space, known as the space-time cube. Currently, these space-time cube layouts are visualized through animation or by slicing the cube at regular intervals. However, both techniques present problems such as below-average performance on tasks as well as loss of precision and difficulties in selecting timeslice intervals. In this article, we present TimeLighting, a novel visual analytics approach to visualize and explore temporal graphs embedded in the space-time cube. Our interactive approach highlights node trajectories and their movement over time, visualizes node “aging”, and provides guidance to support users during exploration by indicating interesting time intervals (“when”) and network elements (“where”) are located for a detail-oriented investigation. This combined focus helps to gain deeper insights into the temporal network&#39;s underlying behavior. We assess the utility and efficacy of our approach through two case studies and qualitative expert evaluation. The results demonstrate how TimeLighting supports identifying temporal patterns, extracting insights from nodes with high activity, and guiding the exploration and analysis process.},
  archive      = {J_TVCG},
  author       = {Velitchko Filipov and Davide Ceneda and Daniel Archambault and Alessio Arleo},
  doi          = {10.1109/TVCG.2024.3514858},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1932-1944},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TimeLighting: Guided exploration of 2D temporal network projections},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualization of finite-time separation in multiphase flow.
<em>TVCG</em>, <em>31</em>(3), 1918–1931. (<a
href="https://doi.org/10.1109/TVCG.2024.3493607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a particle-based visualization approach for finite-time analysis of the connectivity of fluid portions in multiphase flow, i.e., the evolution of the droplets in volume of fluid simulations. We address the Lagrangian inconsistency between the interpolated flow field and the interpolated volume of fluid field by a correction approach, and complement that with an uncertainty measure that provides an estimate of the involved inconsistency. We demonstrate the utility and versatility of our approach using different multiphase flow simulations, exemplify its application in physics-based assessment of droplet formation processes, and discuss its limitations and benefits.},
  archive      = {J_TVCG},
  author       = {Moritz Heinemann and Johanna Potyka and Kathrin Schulte and Filip Sadlo and Thomas Ertl},
  doi          = {10.1109/TVCG.2024.3493607},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1918-1931},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization of finite-time separation in multiphase flow},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agnostic visual recommendation systems: Open challenges and
future directions. <em>TVCG</em>, <em>31</em>(3), 1902–1917. (<a
href="https://doi.org/10.1109/TVCG.2024.3374571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization Recommendation Systems (VRSs) are a novel and challenging field of study aiming to help generate insightful visualizations from data and support non-expert users in information discovery. Among the many contributions proposed in this area, some systems embrace the ambitious objective of imitating human analysts to identify relevant relationships in data and make appropriate design choices to represent these relationships with insightful charts. We denote these systems as “agnostic” VRSs since they do not rely on human-provided constraints and rules but try to learn the task autonomously. Despite the high application potential of agnostic VRSs, their progress is hindered by several obstacles, including the absence of standardized datasets to train recommendation algorithms, the difficulty of learning design rules, and defining quantitative criteria for evaluating the perceptual effectiveness of generated plots. This article summarizes the literature on agnostic VRSs and outlines promising future research directions.},
  archive      = {J_TVCG},
  author       = {Luca Podo and Bardh Prenkaj and Paola Velardi},
  doi          = {10.1109/TVCG.2024.3374571},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1902-1917},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Agnostic visual recommendation systems: Open challenges and future directions},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ProbIBR: Fast image-based rendering with learned
probability-guided sampling. <em>TVCG</em>, <em>31</em>(3), 1888–1901.
(<a href="https://doi.org/10.1109/TVCG.2024.3372152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general, fast, and practical solution for interpolating novel views of diverse real-world scenes given a sparse set of nearby views. Existing generic novel view synthesis methods rely on time-consuming scene geometry pre-computation or redundant sampling of the entire space for neural volumetric rendering, limiting the overall efficiency. Instead, we incorporate learned MVS priors into the neural volume rendering pipeline while improving the rendering efficiency by reducing sampling points under the guidance of depth probability distributions. Specifically, fewer but important points are sampled under the guidance of depth probability distributions extracted from the learned MVS architecture. Based on the learned probability-guided sampling, we develop a sophisticated neural volume rendering module that effectively integrates source view information with the learned scene structures. We further propose confidence-aware refinement to improve the rendering results in uncertain, occluded, and unreferenced regions. Moreover, we build a four-view camera system for holographic display and provide a real-time version of our framework for free-viewpoint experience, where novel view images of a spatial resolution of 512×512 can be rendered at around 20 fps on a single GTX 3090 GPU. Experiments show that our method achieves 15 to 40 times faster rendering compared to state-of-the-art baselines, with strong generalization capacity and comparable high-quality novel view synthesis performance.},
  archive      = {J_TVCG},
  author       = {Yuemei Zhou and Tao Yu and Zerong Zheng and Gaochang Wu and Guihua Zhao and Wenbo Jiang and Ying Fu and Yebin Liu},
  doi          = {10.1109/TVCG.2024.3372152},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1888-1901},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ProbIBR: Fast image-based rendering with learned probability-guided sampling},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Examining limits of small multiples: Frame quantity impacts
judgments with line graphs. <em>TVCG</em>, <em>31</em>(3), 1875–1887.
(<a href="https://doi.org/10.1109/TVCG.2024.3372620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small multiples are a popular visualization method, displaying different views of a dataset using multiple frames, often with the same scale and axes. However, there is a need to address their potential constraints, especially in the context of human cognitive capacity limits. These limits dictate the maximum information our mind can process at once. We explore the issue of capacity limitation by testing competing theories that describe how the number of frames shown in a display, the scale of the frames, and time constraints impact user performance with small multiples of line charts in an energy grid scenario. In two online studies (Experiment 1 n = 141 and Experiment 2 n = 360) and a follow-up eye-tracking analysis (n = 5), we found a linear decline in accuracy with increasing frames across seven tasks, which was not fully explained by differences in frame size, suggesting visual search challenges. Moreover, the studies demonstrate that highlighting specific frames can mitigate some visual search difficulties but, surprisingly, not eliminate them. This research offers insights into optimizing the utility of small multiples by aligning them with human limitations.},
  archive      = {J_TVCG},
  author       = {Helia Hosseinpour and Laura E. Matzen and Kristin M. Divis and Spencer C. Castro and Lace Padilla},
  doi          = {10.1109/TVCG.2024.3372620},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1875-1887},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Examining limits of small multiples: Frame quantity impacts judgments with line graphs},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VisTellAR: Embedding data visualization to short-form videos
using mobile augmented reality. <em>TVCG</em>, <em>31</em>(3),
1862–1874. (<a href="https://doi.org/10.1109/TVCG.2024.3372104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of short-form video platforms and the increasing availability of data, we see the potential for people to share short-form videos embedded with data in situ (e.g., daily steps when running) to increase the credibility and expressiveness of their stories. However, creating and sharing such videos in situ is challenging since it involves multiple steps and skills (e.g., data visualization creation and video editing), especially for amateurs. By conducting a formative study (N=10) using three design probes, we collected the motivations and design requirements. We then built VisTellAR, a mobile AR authoring tool, to help amateur video creators embed data visualizations in short-form videos in situ. A two-day user study shows that participants (N=12) successfully created various videos with data visualizations in situ and they confirmed the ease of use and learning. AR pre-stage authoring was useful to assist people in setting up data visualizations in reality with more designs in camera movements and interaction with gestures and physical objects to storytelling.},
  archive      = {J_TVCG},
  author       = {Wai Tong and Kento Shigyo and Lin-Ping Yuan and Mingming Fan and Ting-Chuen Pong and Huamin Qu and Meng Xia},
  doi          = {10.1109/TVCG.2024.3372104},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1862-1874},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisTellAR: Embedding data visualization to short-form videos using mobile augmented reality},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic motion transition: A hybrid data-driven and
model-driven method for human pose transitions. <em>TVCG</em>,
<em>31</em>(3), 1848–1861. (<a
href="https://doi.org/10.1109/TVCG.2024.3372421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid, accurate, and robust computation of virtual human figures’ “in-between” pose transitions from available and sometimes sparse inputs is of fundamental significance to 3D interactive graphics and computer animation. Various methods have been proposed to produce natural lifelike transitions of human pose automatically in recent decades. Nevertheless, conventional pure model-driven methods require heuristic knowledge (e.g., least motion guided by physics laws) and ad-hoc clues (e.g., splines with non-uniform time warp) that are difficult to obtain, learn, and infer. With the fast emergence of large-scale datasets readily available to animators in the most recent years, deep models afford a powerful alternative to tackle the aforementioned challenges. However, pure data-driven methods still suffer from the remaining challenges such as unseen data in practice and less generative power in model/domain/data transfer, and the measurement of the generative power has always been omitted in these works. In essence, data-driven methods solely rely on the qualities and quantities of training datasets. In this paper, we propose a hybrid approach built upon the seamless integration of data-driven and model-driven methods, called Dynamic Motion Transition (DMT), with the following salient modeling advantages: (1) The data augmentation capability based on the limited human locomotion data capture and the concept of force-derived directly from physical laws; (2) Force learning by which skeleton joints are driven to move, and the Conditional Temporal Transformer (CTT) being trained to learn the force change in the local range, both at the fine level; and (3) At the coarse level, the effective and flexible creation of the subsequent step motion using Dynamic Movement Primitives (DMP) until the target is reached. Our extensive experiments have confirmed that our model can outperform the state-of-the-art methods under the newly devised metric by virtue of the least action loss function. In addition, our novel method and system are of immediate benefit to many other animation tasks such as motion synthesis and control, and motion tracking and prediction in this bigdata graphics era.},
  archive      = {J_TVCG},
  author       = {Zhi Chai and Hong Qin},
  doi          = {10.1109/TVCG.2024.3372421},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1848-1861},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic motion transition: A hybrid data-driven and model-driven method for human pose transitions},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEVA: Using large language models to enhance visual
analytics. <em>TVCG</em>, <em>31</em>(3), 1830–1847. (<a
href="https://doi.org/10.1109/TVCG.2024.3368060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual analytics supports data analysis tasks within complex domain problems. However, due to the richness of data types, visual designs, and interaction designs, users need to recall and process a significant amount of information when they visually analyze data. These challenges emphasize the need for more intelligent visual analytics methods. Large language models have demonstrated the ability to interpret various forms of textual data, offering the potential to facilitate intelligent support for visual analytics. We propose LEVA, a framework that uses large language models to enhance users’ VA workflows at multiple stages: onboarding, exploration, and summarization. To support onboarding, we use large language models to interpret visualization designs and view relationships based on system specifications. For exploration, we use large language models to recommend insights based on the analysis of system status and data to facilitate mixed-initiative exploration. For summarization, we present a selective reporting strategy to retrace analysis history through a stream visualization and generate insight reports with the help of large language models. We demonstrate how LEVA can be integrated into existing visual analytics systems. Two usage scenarios and a user study suggest that LEVA effectively aids users in conducting visual analytics.},
  archive      = {J_TVCG},
  author       = {Yuheng Zhao and Yixing Zhang and Yu Zhang and Xinyi Zhao and Junjie Wang and Zekai Shao and Cagatay Turkay and Siming Chen},
  doi          = {10.1109/TVCG.2024.3368060},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1830-1847},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LEVA: Using large language models to enhance visual analytics},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid base complex: Extract and visualize structure of
hex-dominant meshes. <em>TVCG</em>, <em>31</em>(3), 1818–1829. (<a
href="https://doi.org/10.1109/TVCG.2024.3372333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hex-dominant mesh generation has received significant attention in recent research due to its superior robustness compared to pure hex-mesh generation techniques. In this work, we introduce the first structure for analyzing hex-dominant meshes. This structure builds on the base complex of pure hex-meshes but incorporates the non-hex elements for a more comprehensive and complete representation. We provide its definition and describe its construction steps. Based on this structure, we present an extraction and categorization of sheets using advanced graph matching techniques to handle the non-hex elements. This enables us to develop an enhanced visual analysis of the structure for any hex-dominant meshes. We apply this structure-based visual analysis to compare hex-dominant meshes generated by different methods to study their advantages and disadvantages. This complements the standard quality metric based on the non-hex element percentage for hex-dominant meshes. Moreover, we propose a strategy to extract a cleaned (optimized) valence-based singularity graph wireframe to analyze the structure for both mesh and sheets. Our results demonstrate that the proposed hybrid base complex provides a coarse representation for mesh element, and the proposed valence singularity graph wireframe provides a better internal visualization of hex-dominant meshes.},
  archive      = {J_TVCG},
  author       = {Lei Si and Haowei Cao and Guoning Chen},
  doi          = {10.1109/TVCG.2024.3372333},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1818-1829},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hybrid base complex: Extract and visualize structure of hex-dominant meshes},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EvoVis: A visual analytics method to understand the labeling
iterations in data programming. <em>TVCG</em>, <em>31</em>(3),
1802–1817. (<a href="https://doi.org/10.1109/TVCG.2024.3370654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining high-quality labeled training data poses a significant bottleneck in the domain of machine learning. Data programming has emerged as a new paradigm to address this issue by converting human knowledge into labeling functions (LFs) to quickly produce low-cost probabilistic labels. To ensure the quality of labeled data, data programmers commonly iterate LFs for many rounds until satisfactory performance is achieved. However, the challenge in understanding the labeling iterations stems from interpreting the intricate relationships between data programming elements, exacerbated by their many-to-many and directed characteristics, inconsistent formats, and the large scale of data typically involved in labeling tasks. These complexities may impede the evaluation of label quality, identification of areas for improvement, and the effective optimization of LFs for acquiring high-quality labeled data. In this article, we introduce EvoVis, a visual analytics method for multi-class text labeling tasks. It seamlessly integrates relationship analysis and temporal overview to display contextual and historical information on a single screen, aiding in explaining the labeling iterations in data programming. We assessed its utility and effectiveness through case studies and user studies. The results indicate that EvoVis can effectively assist data programmers in understanding labeling iterations and improving the quality of labeled data, as evidenced by an increase of 0.16 in the average F1 score when compared to the default analysis tool.},
  archive      = {J_TVCG},
  author       = {Sisi Li and Guanzhong Liu and Tianxiang Wei and Shichao Jia and Jiawan Zhang},
  doi          = {10.1109/TVCG.2024.3370654},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1802-1817},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EvoVis: A visual analytics method to understand the labeling iterations in data programming},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluid inverse volumetric modeling and applications from
surface motion. <em>TVCG</em>, <em>31</em>(3), 1785–1801. (<a
href="https://doi.org/10.1109/TVCG.2024.3370551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we devise a framework for volumetrically reconstructing fluid from observable, measurable free surface motion. Our innovative method amalgamates the benefits of deep learning and conventional simulation to preserve the guiding motion and temporal coherence of the reproduced fluid. We infer surface velocities by encoding and decoding spatiotemporal features of surface sequences, and a 3D CNN is used to generate the volumetric velocity field, which is then combined with 3D labels of obstacles and boundaries. Concurrently, we employ a network to estimate the fluid&#39;s physical properties. To progressively evolve the flow field over time, we input the reconstructed velocity field and estimated parameters into the physical simulator as the initial state. Our approach yields promising results for both synthetic fluid generated by different fluid solvers and captured real fluid. The developed framework naturally lends itself to a variety of graphics applications, such as 1) effective reproductions of fluid behaviors visually congruent with the observed surface motion, and 2) physics-guided re-editing of fluid scenes. Extensive experiments affirm that our novel method surpasses state-of-the-art approaches for 3D fluid inverse modeling and animation in graphics.},
  archive      = {J_TVCG},
  author       = {Xueguang Xie and Yang Gao and Fei Hou and Tianwei Cheng and Aimin Hao and Hong Qin},
  doi          = {10.1109/TVCG.2024.3370551},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1785-1801},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fluid inverse volumetric modeling and applications from surface motion},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IntiVisor: A visual analytics system for interaction log
analysis. <em>TVCG</em>, <em>31</em>(3), 1772–1784. (<a
href="https://doi.org/10.1109/TVCG.2024.3370637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application developers frequently augment their code to produce event logs of specific operations performed by their users. Subsequent analysis of these event logs can help provide insight about the users’ behavior relative to its intended use. The analysis process typically includes both event organization and pattern discovery activities. However, most existing visual analytics systems for interaction log analysis excel at supporting pattern discovery and overlook the importance of flexible event organization. This omission limits the practical application of these systems. Therefore, we developed a novel visual analytics system called IntiVisor that implements the entire end-to-end interaction analysis approach. An evaluation of the system with interaction data from four visualization applications showed the value and importance of supporting event organization in interaction log analysis.},
  archive      = {J_TVCG},
  author       = {Yi Han and Gregory D. Abowd and John Stasko},
  doi          = {10.1109/TVCG.2024.3370637},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1772-1784},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IntiVisor: A visual analytics system for interaction log analysis},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pose-aware 3D talking face synthesis using geometry-guided
audio-vertices attention. <em>TVCG</em>, <em>31</em>(3), 1758–1771. (<a
href="https://doi.org/10.1109/TVCG.2024.3371064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing 3D talking face synthesis methods suffer from the lack of detailed facial expressions and realistic head poses, resulting in unsatisfactory experiences for users. In this article, we propose a novel pose-aware 3D talking face synthesis method with a novel geometry-guided audio-vertices attention. To capture more detailed expression, such as the subtle nuances of mouth shape and eye movement, we propose to build hierarchical audio features including a global attribute feature and a series of vertex-wise local latent movement features. Then, in order to fully exploit the topology of facial models, we further propose a novel geometry-guided audio-vertices attention module to predict the displacement of each vertex by using vertex connectivity relations to take full advantage of the corresponding hierarchical audio features. Finally, to accomplish pose-aware animation, we expand the existing database with an additional pose attribute, and a novel pose estimation module is proposed by paying attention to the whole head model. Numerical experiments demonstrate the effectiveness of the proposed method on realistic expression and head movements against state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Bo Li and Xiaolin Wei and Bin Liu and Zhifen He and Junjie Cao and Yu-Kun Lai},
  doi          = {10.1109/TVCG.2024.3371064},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1758-1771},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pose-aware 3D talking face synthesis using geometry-guided audio-vertices attention},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuNeRF: Robust makeup transfer in neural radiance fields.
<em>TVCG</em>, <em>31</em>(3), 1746–1757. (<a
href="https://doi.org/10.1109/TVCG.2024.3368443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a high demand for facial makeup transfer tools in fashion e-commerce and virtual avatar generation. Most of the existing makeup transfer methods are based on the generative adversarial networks. Despite their success in makeup transfer for a single image, they struggle to maintain the consistency of makeup under different poses and expressions of the same person. In this article, we propose a robust makeup transfer method which consistently transfers the makeup style of a reference image to facial images in any poses and expressions. Our method introduces the implicit 3D representation, neural radiance fields (NeRFs), to ensure the geometric and appearance consistency. It has two separate stages, including one basic NeRF module to reconstruct the geometry from the input facial image sequence, and a makeup module to learn how to transfer the reference makeup style consistently. We propose a novel hybrid makeup loss which is specially designed based on the makeup characteristics to supervise the training of the makeup module. The proposed loss significantly improves the visual quality and faithfulness of the makeup transfer effects. To better align the distribution between the transferred makeup and the reference makeup, a patch-based discriminator that works in the pose-independent UV texture space is proposed to provide more accurate control of the synthesized makeup. Extensive experiments and a user study demonstrate the superiority of our network for a variety of different makeup styles.},
  archive      = {J_TVCG},
  author       = {Yu-Jie Yuan and Xinyang Han and Yue He and Fang-Lue Zhang and Lin Gao},
  doi          = {10.1109/TVCG.2024.3368443},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1746-1757},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MuNeRF: Robust makeup transfer in neural radiance fields},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChartGPT: Leveraging LLMs to generate charts from abstract
natural language. <em>TVCG</em>, <em>31</em>(3), 1731–1745. (<a
href="https://doi.org/10.1109/TVCG.2024.3368621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of natural language interfaces (NLIs) to create charts is becoming increasingly popular due to the intuitiveness of natural language interactions. One key challenge in this approach is to accurately capture user intents and transform them to proper chart specifications. This obstructs the wide use of NLI in chart generation, as users’ natural language inputs are generally abstract (i.e., ambiguous or under-specified), without a clear specification of visual encodings. Recently, pre-trained large language models (LLMs) have exhibited superior performance in understanding and generating natural language, demonstrating great potential for downstream tasks. Inspired by this major trend, we propose ChartGPT, generating charts from abstract natural language inputs. However, LLMs are struggling to address complex logic problems. To enable the model to accurately specify the complex parameters and perform operations in chart generation, we decompose the generation process into a step-by-step reasoning pipeline, so that the model only needs to reason a single and specific sub-task during each run. Moreover, LLMs are pre-trained on general datasets, which might be biased for the task of chart generation. To provide adequate visualization knowledge, we create a dataset consisting of abstract utterances and charts and improve model performance through fine-tuning. We further design an interactive interface for ChartGPT that allows users to check and modify the intermediate outputs of each step. The effectiveness of the proposed system is evaluated through quantitative evaluations and a user study.},
  archive      = {J_TVCG},
  author       = {Yuan Tian and Weiwei Cui and Dazhen Deng and Xinjing Yi and Yurun Yang and Haidong Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2024.3368621},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1731-1745},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ChartGPT: Leveraging LLMs to generate charts from abstract natural language},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple monitors or single canvas? Evaluating window
management and layout strategies on virtual displays. <em>TVCG</em>,
<em>31</em>(3), 1713–1730. (<a
href="https://doi.org/10.1109/TVCG.2024.3368930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual displays enabled through head-worn augmented reality have unique characteristics that can yield extensive amounts of screen space. Existing research has shown that increasing the space on a computer screen can enhance usability. Since virtual displays offer the unique ability to present content without rigid physical space constraints, they provide various new design possibilities. Therefore, we must understand the trade-offs of layout choices when structuring that space. We propose a single Canvas approach that eliminates boundaries from traditional multi-monitor approaches and instead places windows in one large, unified space. Our user study compared this approach against a multi-monitor setup, and we considered both purely virtual systems and hybrid systems that included a physical monitor. We looked into usability factors such as performance, accuracy, and overall window management. Results show that Canvas displays can cause users to compact window layouts more than multiple monitors with snapping behavior, even though such optimizations may not lead to longer window management times. We did not find conclusive evidence of either setup providing a better user experience. Multi-Monitor displays offer quick window management with snapping and a structured layout through subdivisions. However, Canvas displays allow for more control in placement and size, lowering the amount of space used and, thus, head rotation. Multi-Monitor benefits were more prominent in the hybrid configuration, while the Canvas display was more beneficial in the purely virtual configuration.},
  archive      = {J_TVCG},
  author       = {Leonardo Pavanatto and Feiyu Lu and Chris North and Doug A. Bowman},
  doi          = {10.1109/TVCG.2024.3368930},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1713-1730},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multiple monitors or single canvas? evaluating window management and layout strategies on virtual displays},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-modal attention-based approach for points of
interest detection on 3D shapes. <em>TVCG</em>, <em>31</em>(3),
1698–1712. (<a href="https://doi.org/10.1109/TVCG.2024.3368767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying points of interest (POIs) on the surface of 3D shapes is a significant challenge in geometric processing research. The complex connection between POIs and their geometric descriptors, combined with the small percentage of POIs on the shape, makes detecting POIs on any given 3D shape a highly challenging task. Existing methods directly detect POIs from the entire 3D shape, resulting in low efficiency and accuracy. Therefore, we propose a novel multi-modal POI detection method using a coarse-to-fine approach, with the key idea of reducing data complexity and enabling more efficient and accurate subsequent POI detection by first identifying and processing important regions on the 3D shape. It first obtains important areas on the 3D shape through 2D projected images, then processes points within these regions using attention mechanisms. Extensive experiments demonstrate that our method outperforms existing POI detection techniques.},
  archive      = {J_TVCG},
  author       = {Zhenyu Shu and Junlong Yu and Kai Chao and Shiqing Xin and Ligang Liu},
  doi          = {10.1109/TVCG.2024.3368767},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1698-1712},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A multi-modal attention-based approach for points of interest detection on 3D shapes},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral descriptors for 3D deformable shape matching: A
comparative survey. <em>TVCG</em>, <em>31</em>(3), 1677–1697. (<a
href="https://doi.org/10.1109/TVCG.2024.3368083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of 3D spectral descriptors have been proposed in the literature, which act as an essential component for 3D deformable shape matching and related applications. An outstanding descriptor should have desirable natures including high-level descriptive capacity, cheap storage, and robustness to a set of nuisances. It is, however, unclear which descriptors are more suitable for a particular application. This paper fills the gap by comprehensively evaluating nine state-of-the-art spectral descriptors on ten popular deformable shape datasets as well as perturbations such as mesh discretization, geometric noise, scale transformation, non-isometric setting, partiality, and topological noise. Our evaluated terms for a spectral descriptor cover four major concerns, i.e., distinctiveness, robustness, compactness, and computational efficiency. In the end, we present a summary of the overall performance and several interesting findings that can serve as guidance for the following researchers to construct a new spectral descriptor and choose an appropriate spectral feature in a particular application.},
  archive      = {J_TVCG},
  author       = {Shengjun Liu and Haibo Wang and Dong-Ming Yan and Qinsong Li and Feifan Luo and Zi Teng and Xinru Liu},
  doi          = {10.1109/TVCG.2024.3368083},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1677-1697},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spectral descriptors for 3D deformable shape matching: A comparative survey},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MARR: A multi-agent reinforcement resetter for redirected
walking. <em>TVCG</em>, <em>31</em>(3), 1664–1676. (<a
href="https://doi.org/10.1109/TVCG.2024.3368043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reset technique of Redirected Walking (RDW) forcibly reorients the user&#39;s direction overtly to avoid collisions with boundaries, obstacles, or other users in the physical space. However, excessive resetting can decrease the user&#39;s sense of immersion and presence. Several RDW studies have been conducted to address this issue. Among them, much research has been done on reset techniques that reduce the number of resets by devising reset direction rules or optimizing them for a given environment. However, existing optimization studies on reset techniques have mainly focused on a single-user environment. In a multi-user environment, the dynamic movement of other users and static obstacles in the physical space increase the possibility of resetting. In this study, we propose Multi-Agent Reinforcement Resetter (MARR), which resets the user taking into account both physical obstacles and multi-user movement to minimize the number of resets. MARR is trained using multi-agent reinforcement learning to determine the optimal reset direction in different environments. This approach allows MARR to effectively account for different environmental contexts, including arbitrary physical obstacles and the dynamic movements of other users in the same physical space. We compared MARR to other reset technologies through simulation tests and user studies, and found that MARR outperformed the existing methods. MARR improved performance by learning the optimal reset direction for each subtle technique used in training. MARR has the potential to be applied to new subtle techniques proposed in the future. Overall, our study confirmed that MARR is an effective reset technique in multi-user environments.},
  archive      = {J_TVCG},
  author       = {Ho Jung Lee and Sang-Bin Jeon and Yong-Hun Cho and In-Kwon Lee},
  doi          = {10.1109/TVCG.2024.3368043},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1664-1676},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MARR: A multi-agent reinforcement resetter for redirected walking},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Isomorphic mesh generation from point clouds with multilayer
perceptrons. <em>TVCG</em>, <em>31</em>(3), 1647–1663. (<a
href="https://doi.org/10.1109/TVCG.2024.3367855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel neural network called the isomorphic mesh generator (iMG) is proposed to generate isomorphic meshes from point clouds containing noise and missing parts. Isomorphic meshes of arbitrary objects exhibit a unified mesh structure, despite objects belonging to different classes. This unified representation enables various modern deep neural networks (DNNs) to easily handle surface models without requiring additional pre-processing. Additionally, the unified mesh structure of isomorphic meshes enables the application of the same process to all isomorphic meshes, unlike general mesh models, where processes need to be tailored depending on their mesh structures. Therefore, the use of isomorphic meshes can ensure efficient memory usage and reduce calculation time. Apart from the point cloud of the target object used as input for the iMG, point clouds and mesh models need not be prepared in advance as training data because the iMG is a data-free method. Furthermore, the iMG outputs an isomorphic mesh obtained by mapping a reference mesh to a given input point cloud. To stably estimate the mapping function, a step-by-step mapping strategy is introduced. This strategy enables flexible deformation while simultaneously maintaining the structure of the reference mesh. Simulations and experiments conducted using a mobile phone have confirmed that the iMG reliably generates isomorphic meshes of given objects, even when the input point cloud includes noise and missing parts.},
  archive      = {J_TVCG},
  author       = {Shoko Miyauchi and Ken&#39;ichi Morooka and Ryo Kurazume},
  doi          = {10.1109/TVCG.2024.3367855},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  number       = {3},
  pages        = {1647-1663},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Isomorphic mesh generation from point clouds with multilayer perceptrons},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
