<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami---57">TPAMI - 57</h2>
<ul>
<li><details>
<summary>
(2025). Unveiling and mitigating generalized biases of DNNs through
the intrinsic dimensions of perceptual manifolds. <em>TPAMI</em>,
<em>47</em>(3), 2237–2244. (<a
href="https://doi.org/10.1109/TPAMI.2024.3510048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building fair deep neural networks (DNNs) is a crucial step towards achieving trustworthy artificial intelligence. Delving into deeper factors that affect the fairness of DNNs is paramount and serves as the foundation for mitigating model biases. However, current methods are limited in accurately predicting DNN biases, relying solely on the number of training samples and lacking more precise measurement tools. Here, we establish a geometric perspective for analyzing the fairness of DNNs, comprehensively exploring how DNNs internally shape the intrinsic geometric characteristics of datasets—the intrinsic dimensions (IDs) of perceptual manifolds, and the impact of IDs on the fairness of DNNs. Based on multiple findings, we propose Intrinsic Dimension Regularization (IDR), which enhances the fairness and performance of models by promoting the learning of concise and ID-balanced class perceptual manifolds. In various image recognition benchmark tests, IDR significantly mitigates model bias while improving its performance.},
  archive      = {J_TPAMI},
  author       = {Yanbiao Ma and Licheng Jiao and Fang Liu and Lingling Li and Wenping Ma and Shuyuan Yang and Xu Liu and Puhua Chen},
  doi          = {10.1109/TPAMI.2024.3510048},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2237-2244},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unveiling and mitigating generalized biases of DNNs through the intrinsic dimensions of perceptual manifolds},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Insights on “complex-valued iris recognition network.”
<em>TPAMI</em>, <em>47</em>(3), 2232–2236. (<a
href="https://doi.org/10.1109/TPAMI.2024.3489775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We comment on a recently published TPAMI paper presenting an iris recognition algorithm. While the approach is intriguing, we have identified several inconsistencies and errors in this paper. Additionally, their comparison with the state-of-the-art methods lacks fairness. We take this opportunity to clarify and underline these errors, aiming to assist fellow researchers like us who are interested in advancing biometrics research.},
  archive      = {J_TPAMI},
  author       = {Ajay Kumar},
  doi          = {10.1109/TPAMI.2024.3489775},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2232-2236},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Insights on ‘Complex-valued iris recognition network’},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RenAIssance: A survey into AI text-to-image generation in
the era of large model. <em>TPAMI</em>, <em>47</em>(3), 2212–2231. (<a
href="https://doi.org/10.1109/TPAMI.2024.3522305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image generation (TTI) refers to the usage of models that could process text input and generate high fidelity images based on text descriptions. Text-to-image generation using neural networks could be traced back to the emergence of Generative Adversial Network (GAN), followed by the autoregressive Transformer. Diffusion models are one prominent type of generative model used for the generation of images through the systematic introduction of noises with repeating steps. As an effect of the impressive results of diffusion models on image synthesis, it has been cemented as the major image decoder used by text-to-image models and brought text-to-image generation to the forefront of machine-learning (ML) research. In the era of large models, scaling up model size and the integration with large language models have further improved the performance of TTI models, resulting the generation result nearly indistinguishable from real-world images, revolutionizing the way we retrieval images. Our explorative study has incentivised us to think that there are further ways of scaling text-to-image models with the combination of innovative model architectures and prediction enhancement techniques. We have divided the work of this survey into five main sections wherein we detail the frameworks of major literature in order to delve into the different types of text-to-image generation methods. Following this we provide a detailed comparison and critique of these methods and offer possible pathways of improvement for future work. In the future work, we argue that TTI development could yield impressive productivity improvements for creation, particularly in the context of the AIGC era, and could be extended to more complex tasks such as video generation and 3D generation.},
  archive      = {J_TPAMI},
  author       = {Fengxiang Bie and Yibo Yang and Zhongzhu Zhou and Adam Ghanem and Minjia Zhang and Zhewei Yao and Xiaoxia Wu and Connor Holmes and Pareesa Golnari and David A. Clifton and Yuxiong He and Dacheng Tao and Shuaiwen Leon Song},
  doi          = {10.1109/TPAMI.2024.3522305},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2212-2231},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RenAIssance: A survey into AI text-to-image generation in the era of large model},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-head encoding for extreme label classification.
<em>TPAMI</em>, <em>47</em>(3), 2199–2211. (<a
href="https://doi.org/10.1109/TPAMI.2024.3522298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of categories of instances in the real world is normally huge, and each instance may contain multiple labels. To distinguish these massive labels utilizing machine learning, eXtreme Label Classification (XLC) has been established. However, as the number of categories increases, the number of parameters and nonlinear operations in the classifier also rises. This results in a Classifier Computational Overload Problem (CCOP). To address this, we propose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla classifier with a multi-head classifier. During the training process, MHE decomposes extreme labels into the product of multiple short local labels, with each head trained on these local labels. During testing, the predicted labels can be directly calculated from the local predictions of each head. This reduces the computational load geometrically. Then, according to the characteristics of different XLC tasks, e.g., single-label, multi-label, and model pretraining tasks, three MHE-based implementations, i.e., Multi-Head Product, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more effectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can achieve performance approximately equivalent to that of the vanilla classifier by generalizing the low-rank approximation problem from Frobenius-norm to Cross-Entropy. Experimental results show that the proposed methods achieve state-of-the-art performance while significantly streamlining the training and inference processes of XLC tasks.},
  archive      = {J_TPAMI},
  author       = {Daojun Liang and Haixia Zhang and Dongfeng Yuan and Minggao Zhang},
  doi          = {10.1109/TPAMI.2024.3522298},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2199-2211},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-head encoding for extreme label classification},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal guided visual representation learning for social
image retrieval. <em>TPAMI</em>, <em>47</em>(3), 2186–2198. (<a
href="https://doi.org/10.1109/TPAMI.2024.3519112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social images are often associated with rich but noisy tags from community contributions. Although social tags can potentially provide valuable semantic training information for image retrieval, existing studies all fail to effectively filter noises by exploiting the cross-modal correlation between image content and tags. The current cross-modal vision-and-language representation learning methods, which selectively attend to the relevant parts of the image and text, show a promising direction. However, they are not suitable for social image retrieval since: (1) they deal with natural text sequences where the relationships between words can be easily captured by language models for cross-modal relevance estimation, while the tags are isolated and noisy; (2) they take (image, text) pair as input, and consequently cannot be employed directly for unimodal social image retrieval. This paper tackles the challenge of utilizing cross-modal interactions to learn precise representations for unimodal retrieval. The proposed framework, dubbed CGVR (Cross-modal Guided Visual Representation), extracts accurate semantic representations of images from noisy tags and transfers this ability to image-only hashing subnetwork by a carefully designed training scheme. To well capture correlated semantics and filter noises, it embeds a priori common-sense relationship among tags into attention computation for joint awareness of textual and visual context. Experiments show that CGVR achieves approximately 8.82 and 5.45 points improvement in MAP over the state-of-the-art on two widely used social image benchmarks. CGVR can serve as a new baseline for the image retrieval community.},
  archive      = {J_TPAMI},
  author       = {Ziyu Guan and Wanqing Zhao and Hongmin Liu and Yuta Nakashima and Noboru Babaguchi and Xiaofei He},
  doi          = {10.1109/TPAMI.2024.3519112},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2186-2198},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross-modal guided visual representation learning for social image retrieval},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised anomaly detection with neural
transformations. <em>TPAMI</em>, <em>47</em>(3), 2170–2185. (<a
href="https://doi.org/10.1109/TPAMI.2024.3519543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation plays a critical role in self-supervised learning, including anomaly detection. While hand-crafted transformations such as image rotations can achieve impressive performance on image data, effective transformations of non-image data are lacking. In this work, we study learning such transformations for end-to-end anomaly detection on arbitrary data. We find that a contrastive loss–which encourages learning diverse data transformations while preserving the relevant semantic content of the data–is more suitable than previously proposed losses for transformation learning, a fact that we prove theoretically and empirically. We demonstrate that anomaly detection using neural transformation learning can achieve state-of-the-art results for time series data, tabular data, text data and graph data. Furthermore, our approach can make image anomaly detection more interpretable by learning transformations at different levels of abstraction.},
  archive      = {J_TPAMI},
  author       = {Chen Qiu and Marius Kloft and Stephan Mandt and Maja Rudolph},
  doi          = {10.1109/TPAMI.2024.3519543},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2170-2185},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised anomaly detection with neural transformations},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trajectory of fifths based on chroma subbands extraction–a
new approach to music representation, analysis, and classification.
<em>TPAMI</em>, <em>47</em>(3), 2157–2169. (<a
href="https://doi.org/10.1109/TPAMI.2024.3519420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new method of representing and analyzing music audio records. The method is based on the concept of the trajectory of fifths, which was initially developed for the analysis of music represented in MIDI format. To adapt this concept to the needs of audio signal processing, we implement a short-term spectral analysis of a musical piece, followed by a mapping of its subsequent spectral timeframes onto signatures of fifths reflecting relative intensities of sounds associated with each of the 12 pitch classes. Subsequently, the calculation of the characteristic points of the consecutive signatures of fifths enables the creation of the trajectory of fifths. The results of the experiments and statistical analysis conducted in a set of 8996 audio music pieces belonging to 10 genres indicate that this kind of trajectory, just as its MIDI-compliant precursor, is a source of valuable information (i.e., feature coefficients) concerning the harmonic structure of music, which may find use in audio music classification processes.},
  archive      = {J_TPAMI},
  author       = {Tomasz Łukaszewicz and Dariusz Kania},
  doi          = {10.1109/TPAMI.2024.3519420},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2157-2169},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Trajectory of fifths based on chroma subbands Extraction–A new approach to music representation, analysis, and classification},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual learning: Forget-free winning subnetworks for
video representations. <em>TPAMI</em>, <em>47</em>(3), 2140–2156. (<a
href="https://doi.org/10.1109/TPAMI.2024.3518588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) and Task-agnostic Incremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our comprehensive experiments demonstrate FSO&#39;s effectiveness, significantly improving task performance at various convolutional representational levels. Specifically, FSO enhances higher-layer performance in TIL and FSCIL and lower-layer performance in VIL.},
  archive      = {J_TPAMI},
  author       = {Haeyong Kang and Jaehong Yoon and Sung Ju Hwang and Chang D. Yoo},
  doi          = {10.1109/TPAMI.2024.3518588},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2140-2156},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Continual learning: Forget-free winning subnetworks for video representations},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical banzhaf interaction for general video-language
representation learning. <em>TPAMI</em>, <em>47</em>(3), 2125–2139. (<a
href="https://doi.org/10.1109/TPAMI.2024.3522124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal representation learning, with contrastive learning, plays an important role in the artificial intelligence domain. As an important subfield, video-language representation learning focuses on learning representations using global semantic interactions between pre-defined video-text pairs. However, to enhance and refine such coarse-grained global interactions, more detailed interactions are necessary for fine-grained multimodal learning. In this study, we introduce a new approach that models video-text as game players using multivariate cooperative game theory to handle uncertainty during fine-grained semantic interactions with diverse granularity, flexible combination, and vague intensity. Specifically, we design the Hierarchical Banzhaf Interaction to simulate the fine-grained correspondence between video clips and textual words from hierarchical perspectives. Furthermore, to mitigate the bias in calculations within Banzhaf Interaction, we propose reconstructing the representation through a fusion of single-modal and cross-modal components. This reconstructed representation ensures fine granularity comparable to that of the single-modal representation, while also preserving the adaptive encoding characteristics of cross-modal representation. Additionally, we extend our original structure into a flexible encoder-decoder framework, enabling the model to adapt to various downstream tasks. Extensive experiments on commonly used text-video retrieval, video-question answering, and video captioning benchmarks, with superior performance, validate the effectiveness and generalization of our method.},
  archive      = {J_TPAMI},
  author       = {Peng Jin and Hao Li and Li Yuan and Shuicheng Yan and Jie Chen},
  doi          = {10.1109/TPAMI.2024.3522124},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2125-2139},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hierarchical banzhaf interaction for general video-language representation learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDPP: Projected diffusion for procedure planning in
instructional videos. <em>TPAMI</em>, <em>47</em>(3), 2107–2124. (<a
href="https://doi.org/10.1109/TPAMI.2024.3518762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of procedure planning in instructional videos, which aims to make a plan (i.e. a sequence of actions) given the current visual observation and the desired goal. Previous works cast this as a sequence modeling problem and leverage either intermediate visual observations or language instructions as supervision to make autoregressive planning, resulting in complex learning schemes and expensive annotation costs. To avoid intermediate supervision annotation and error accumulation caused by planning autoregressively, we propose a diffusion-based framework, coined as PDPP (Projected Diffusion model for Procedure Planning), to directly model the whole action sequence distribution with task label as supervision instead. Our core idea is to treat procedure planning as a distribution fitting problem under the given observations, thus transform the planning problem to a sampling process from this distribution during inference. The diffusion-based modeling approach also effectively addresses the uncertainty issue in procedure planning. Based on PDPP, we further apply joint training to our framework to generate plans with varying horizon lengths using a single model and reduce the number of training parameters required. We instantiate our PDPP with three popular diffusion models and investigate a serious of condition-introducing methods in our framework, including condition embeddings, Mixture-of-Experts (MoEs), two-stage prediction and Classifier-Free Guidance strategy. Finally, we apply our PDPP to the Visual Planners for human Assistance (VPA) problem which requires the goal specified in natural language rather than visual observation. We conduct experiments on challenging datasets of different scales and our PDPP model achieves the state-of-the-art performance on multiple metrics, even compared with those strongly-supervised counterparts. These results further demonstratethe effectiveness and generalization ability of our model.},
  archive      = {J_TPAMI},
  author       = {Hanlin Wang and Yilu Wu and Sheng Guo and Limin Wang},
  doi          = {10.1109/TPAMI.2024.3518762},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2107-2124},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PDPP: Projected diffusion for procedure planning in instructional videos},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural adversarial mask for face identity protection in
physical world. <em>TPAMI</em>, <em>47</em>(3), 2089–2106. (<a
href="https://doi.org/10.1109/TPAMI.2024.3522994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial recognition (FR) technology offers convenience in our daily lives, but it also raises serious privacy issues due to unauthorized FR applications. To protect facial privacy, existing methods have proposed adversarial face examples that can fool FR systems. However, most of these methods work only in the digital domain and do not consider natural physical protections. In this paper, we present NatMask, a 3D-based method for creating natural and realistic adversarial face masks that can preserve facial identity in the physical world. Our method utilizes 3D face reconstruction and differentiable rendering to generate 2D face images with natural-looking facial masks. Moreover, we propose an identity-aware style injection (IASI) method to improve the naturalness and transferability of the mask texture. We evaluate our method on two face datasets to verify its effectiveness in protecting face identity against four state-of-the-art (SOTA) FR models and three commercial FR APIs in both digital and physical domains under black-box impersonation and dodging strategies. Experiments show that our method can generate adversarial masks with superior naturalness and physical realizability to safeguard face identity, outperforming SOTA methods by a large margin.},
  archive      = {J_TPAMI},
  author       = {Tianxin Xie and Hu Han and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TPAMI.2024.3522994},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2089-2106},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Natural adversarial mask for face identity protection in physical world},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully-connected transformer for multi-source image fusion.
<em>TPAMI</em>, <em>47</em>(3), 2071–2088. (<a
href="https://doi.org/10.1109/TPAMI.2024.3523364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source image fusion combines the information coming from multiple images into one data, thus improving imaging quality. This topic has aroused great interest in the community. How to integrate information from different sources is still a big challenge, although the existing self-attention based transformer methods can capture spatial and channel similarities. In this paper, we first discuss the mathematical concepts behind the proposed generalized self-attention mechanism, where the existing self-attentions are considered basic forms. The proposed mechanism employs multilinear algebra to drive the development of a novel fully-connected self-attention (FCSA) method to fully exploit local and non-local domain-specific correlations among multi-source images. Moreover, we propose a multi-source image representation embedding it into the FCSA framework as a non-local prior within an optimization problem. Some different fusion problems are unfolded into the proposed fully-connected transformer fusion network (FC-Former). More specifically, the concept of generalized self-attention can promote the potential development of self-attention. Hence, the FC-Former can be viewed as a network model unifying different fusion tasks. Compared with state-of-the-art methods, the proposed FC-Former method exhibits robust and superior performance, showing its capability of faithfully preserving information.},
  archive      = {J_TPAMI},
  author       = {Xiao Wu and Zi-Han Cao and Ting-Zhu Huang and Liang-Jian Deng and Jocelyn Chanussot and Gemine Vivone},
  doi          = {10.1109/TPAMI.2024.3523364},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2071-2088},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fully-connected transformer for multi-source image fusion},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast semi-supervised learning on large graphs: An improved
green-function method. <em>TPAMI</em>, <em>47</em>(3), 2055–2070. (<a
href="https://doi.org/10.1109/TPAMI.2024.3518595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the graph-based semi-supervised learning, the Green-function method is a classical method that works by computing the Green&#39;s function in the graph space. However, when applied to large graphs, especially those sparse ones, this method performs unstably and unsatisfactorily. We make a detailed analysis on it and propose a novel method from the perspective of optimization. On fully connected graphs, the method is equivalent to the Green-function method and can be seen as another interpretation with physical meanings, while on non-fully connected graphs, it helps to explain why the Green-function method causes a mess on large sparse graphs. To solve this dilemma, we propose a workable approach to improve our proposed method. Unlike the original method, our improved method can also apply two accelerating techniques, Gaussian Elimination, and Anchored Graphs to become more efficient on large graphs. Finally, the extensive experiments prove our conclusions and the efficiency, accuracy, and stability of our improved Green&#39;s function method.},
  archive      = {J_TPAMI},
  author       = {Feiping Nie and Yitao Song and Wei Chang and Rong Wang and Xuelong Li},
  doi          = {10.1109/TPAMI.2024.3518595},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2055-2070},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast semi-supervised learning on large graphs: An improved green-function method},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Saliency-free and aesthetic-aware panoramic video
navigation. <em>TPAMI</em>, <em>47</em>(3), 2037–2054. (<a
href="https://doi.org/10.1109/TPAMI.2024.3516874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing panoramic video navigation approaches are saliency-driven, whereby off-the-shelf saliency detection tools are directly employed to aid the navigation approaches in localizing video content that should be incorporated into the navigation path. In view of the dilemma faced by our research community, we rethink if the “saliency clues” are really appropriate to serve the panoramic video navigation task. According to our in-depth investigation, we argue that using “saliency clues” cannot generate a satisfying navigation path, failing to well represent the given panoramic video, and the views in the navigation path are also low aesthetics. In this paper, we present a brand-new navigation paradigm. Although our model is still trained on eye-fixations, our methodology can additionally enable the trained model to perceive the “meaningful” degree of the given panoramic video content. Outwardly, the proposed new approach is saliency-free, but inwardly, it is developed from saliency but biasing more to be “meaningful-driven”; thus, it can generate a navigation path with more appropriate content coverage. Besides, this paper is the first attempt to devise an unsupervised learning scheme to ensure all localized meaningful views in the navigation path have high aesthetics. Thus, the navigation path generated by our approach can also bring users an enjoyable watching experience. As a new topic in its infancy, we have devised a series of quantitative evaluation schemes, including objective verifications and subjective user studies. All these innovative attempts would have great potential to inspire and promote this research field in the near future.},
  archive      = {J_TPAMI},
  author       = {Chenglizhao Chen and Guangxiao Ma and Wenfeng Song and Shuai Li and Aimin Hao and Hong Qin},
  doi          = {10.1109/TPAMI.2024.3516874},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2037-2054},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Saliency-free and aesthetic-aware panoramic video navigation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BEVFormer: Learning bird’s-eye-view representation from
LiDAR-camera via spatiotemporal transformers. <em>TPAMI</em>,
<em>47</em>(3), 2020–2036. (<a
href="https://doi.org/10.1109/TPAMI.2024.3515454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modality fusion strategy is currently the de-facto most competitive solution for 3D perception tasks. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations from multi-modality data with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from both point cloud and camera input, thus completing multi-modality information fusion under BEV space. For temporal information, we propose temporal self-attention to fuse the history BEV information recurrently. By comparing with other fusion paradigms, we demonstrate that the fusion method proposed in this work is both succinct and effective. Our approach achieves the new state-of-the-art 74.1% in terms of NDS metric on the nuScenes test set. In addition, we extend BEVFormer to encompass a wide range of autonomous driving tasks, including object tracking, vectorized mapping, occupancy prediction, and end-to-end autonomous driving, achieving outstanding results across these tasks.},
  archive      = {J_TPAMI},
  author       = {Zhiqi Li and Wenhai Wang and Hongyang Li and Enze Xie and Chonghao Sima and Tong Lu and Qiao Yu and Jifeng Dai},
  doi          = {10.1109/TPAMI.2024.3515454},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2020-2036},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BEVFormer: Learning bird’s-eye-view representation from LiDAR-camera via spatiotemporal transformers},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey and benchmark of automatic surface reconstruction
from point clouds. <em>TPAMI</em>, <em>47</em>(3), 2000–2019. (<a
href="https://doi.org/10.1109/TPAMI.2024.3510932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive survey and benchmark of both traditional and learning-based methods for surface reconstruction from point clouds. This task is particularly challenging for real-world acquisitions due to factors, such as noise, outliers, non-uniform sampling, and missing data. Traditional approaches often simplify the problem by imposing handcrafted priors on either the input point clouds or the resulting surface, a process that can require tedious hyperparameter tuning. In contrast, deep learning models have the capability to directly learn the properties of input point clouds and desired surfaces from data. We study the influence of handcrafted and learned priors on the precision and robustness of surface reconstruction techniques. We evaluate various time-tested and contemporary methods in a standardized manner. When both trained and evaluated on point clouds with identical characteristics, the learning-based models consistently produce higher-quality surfaces compared to their traditional counterparts—even in scenarios involving novel shape categories. However, traditional methods demonstrate greater resilience to the diverse anomalies commonly found in real-world 3D acquisitions. For the benefit of the research community, we make our code and datasets available, inviting further enhancements to learning-based surface reconstruction.},
  archive      = {J_TPAMI},
  author       = {Raphael Sulzer and Renaud Marlet and Bruno Vallet and Loic Landrieu},
  doi          = {10.1109/TPAMI.2024.3510932},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {2000-2019},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A survey and benchmark of automatic surface reconstruction from point clouds},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectrally-corrected and regularized LDA for spiked model.
<em>TPAMI</em>, <em>47</em>(3), 1991–1999. (<a
href="https://doi.org/10.1109/TPAMI.2024.3511080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an improved linear discriminant analysis called spectrally-corrected and regularized LDA (SRLDA). This approach incorporates design principles from both the spectrally-corrected covariance matrix and the regularized discriminant analysis. With the support of a large-dimensional random matrix theory, it is demonstrated that SRLDA achieves a globally optimal linear classification solution under the spiked model assumption. According to simulation data analysis, the SRLDA classifier exhibits better performance compared to RLDA and ILDA, closely to the theoretical classifier. Empirical experiments across diverse datasets further reflect that the SRLDA algorithm excels in both classification accuracy and dimensionality reduction, outperforming currently employed tools.},
  archive      = {J_TPAMI},
  author       = {Hua Li and Wenya Luo and Zhidong Bai and Huanchao Zhou and Zhangni Pu},
  doi          = {10.1109/TPAMI.2024.3511080},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1991-1999},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spectrally-corrected and regularized LDA for spiked model},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiking variational policy gradient for brain inspired
reinforcement learning. <em>TPAMI</em>, <em>47</em>(3), 1975–1990. (<a
href="https://doi.org/10.1109/TPAMI.2024.3511936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies in reinforcement learning have explored brain-inspired function approximators and learning algorithms to simulate brain intelligence and adapt to neuromorphic hardware. Among these approaches, reward-modulated spike-timing-dependent plasticity (R-STDP) is biologically plausible and energy-efficient, but suffers from a gap between its local learning rules and the global learning objectives, which limits its performance and applicability. In this paper, we design a recurrent winner-take-all network and propose the spiking variational policy gradient (SVPG), a new R-STDP learning method derived theoretically from the global policy gradient. Specifically, the policy inference is derived from an energy-based policy function using mean-field inference, and the policy optimization is based on a last-step approximation of the global policy gradient. These fill the gap between the local learning rules and the global target. In experiments including a challenging ViZDoom vision-based navigation task and two realistic robot control tasks, SVPG successfully solves all the tasks. In addition, SVPG exhibits better inherent robustness to various kinds of input, network parameters, and environmental perturbations than compared methods.},
  archive      = {J_TPAMI},
  author       = {Zhile Yang and Shangqi Guo and Ying Fang and Zhaofei Yu and Jian K. Liu},
  doi          = {10.1109/TPAMI.2024.3511936},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1975-1990},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spiking variational policy gradient for brain inspired reinforcement learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Divide-and-conquer: Confluent triple-flow network for RGB-t
salient object detection. <em>TPAMI</em>, <em>47</em>(3), 1958–1974. (<a
href="https://doi.org/10.1109/TPAMI.2024.3511621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-Thermal Salient Object Detection (RGB-T SOD) aims to pinpoint prominent objects within aligned pairs of visible and thermal infrared images. A key challenge lies in bridging the inherent disparities between RGB and Thermal modalities for effective saliency map prediction. Traditional encoder-decoder architectures, while designed for cross-modality feature interactions, may not have adequately considered the robustness against noise originating from defective modalities, thereby leading to suboptimal performance in complex scenarios. Inspired by hierarchical human visual systems, we propose the ConTriNet, a robust Confluent Triple-Flow Network employing a “Divide-and-Conquer” strategy. This framework utilizes a unified encoder with specialized decoders, each addressing different subtasks of exploring modality-specific and modality-complementary information for RGB-T SOD, thereby enhancing the final saliency map prediction. Specifically, ConTriNet comprises three flows: two modality-specific flows explore cues from RGB and Thermal modalities, and a third modality-complementary flow integrates cues from both modalities. ConTriNet presents several notable advantages. It incorporates a Modality-induced Feature Modulator (MFM) in the modality-shared union encoder to minimize inter-modality discrepancies and mitigate the impact of defective samples. Additionally, a foundational Residual Atrous Spatial Pyramid Module (RASPM) in the separated flows enlarges the receptive field, allowing for the capture of multi-scale contextual information. Furthermore, a Modality-aware Dynamic Aggregation Module (MDAM) in the modality-complementary flow dynamically aggregates saliency-related cues from both modality-specific flows. Leveraging the proposed parallel triple-flow framework, we further refine saliency maps derived from different flows through a flow-cooperative fusion strategy, yielding a high-quality, full-resolution saliency map for the final prediction. To evaluate the robustness and stability of our approach, we collect a comprehensive RGB-T SOD benchmark, VT-IMAG, covering various real-world challenging scenarios. Extensive experiments on public benchmarks and our VT-IMAG dataset demonstrate that ConTriNet consistently outperforms state-of-the-art competitors in both common and challenging scenarios, even when dealing with incomplete modality data.},
  archive      = {J_TPAMI},
  author       = {Hao Tang and Zechao Li and Dong Zhang and Shengfeng He and Jinhui Tang},
  doi          = {10.1109/TPAMI.2024.3511621},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1958-1974},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Divide-and-conquer: Confluent triple-flow network for RGB-T salient object detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Remembering what is important: A factorised multi-head
retrieval and auxiliary memory stabilisation scheme for human motion
prediction. <em>TPAMI</em>, <em>47</em>(3), 1941–1957. (<a
href="https://doi.org/10.1109/TPAMI.2024.3511393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans exhibit complex motions that vary depending on the activity they are performing, the interactions they engage in, as well as subject-specific preferences. Therefore, forecasting a human’s future pose based on the history of his or her previous motion is a challenging task. This paper presents an innovative auxiliary-memory-powered deep neural network framework to improve the modelling of historical knowledge. Specifically, we disentangle subject-specific, action-specific, and other auxiliary information from the observed pose sequences and utilise these factorised features to query the memory. A novel Multi-Head knowledge retrieval scheme leverages these factorised feature embeddings to perform multiple querying operations over the historical observations captured within the auxiliary memory. Moreover, we propose a dynamic masking strategy to make this feature disentanglement process adaptive. Two novel loss functions are introduced to encourage diversity within the auxiliary memory, while ensuring the stability of the memory content such that it can locate and store salient information that aids the long-term prediction of future motion, irrespective of any data imbalances or the diversity of the input data distribution. Extensive experiments conducted on two public benchmarks, Human3.6M and CMU-Mocap, demonstrate that these design choices collectively allow the proposed approach to outperform the current state-of-the-art methods by significant margins: $&amp;gt; $ 17% on the Human3.6M dataset and $&amp;gt; $ 9% on the CMU-Mocap dataset.},
  archive      = {J_TPAMI},
  author       = {Tharindu Fernando and Harshala Gammulle and Sridha Sridharan and Simon Denman and Clinton Fookes},
  doi          = {10.1109/TPAMI.2024.3511393},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1941-1957},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Remembering what is important: A factorised multi-head retrieval and auxiliary memory stabilisation scheme for human motion prediction},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iteratively capped reweighting norm minimization with global
convergence guarantee for low-rank matrix learning. <em>TPAMI</em>,
<em>47</em>(3), 1923–1940. (<a
href="https://doi.org/10.1109/TPAMI.2024.3512458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a large number of studies have shown that low rank matrix learning (LRML) has become a popular approach in machine learning and computer vision with many important applications, such as image inpainting, subspace clustering, and recommendation system. The latest LRML methods resort to using some surrogate functions as convex or nonconvex relaxation of the rank function. However, most of these methods ignore the difference between different rank components and can only yield suboptimal solutions. To alleviate this problem, in this paper we propose a novel nonconvex regularizer called capped reweighting norm minimization (CRNM), which not only considers the different contributions of different rank components, but also adaptively truncates sequential singular values. With it, a general LRML model is obtained. Meanwhile, under some mild conditions, the global optimum of CRNM regularized least squares subproblem can be easily obtained in closed-form. Through the analysis of the theoretical properties of CRNM, we develop a high computational efficiency optimization method with convergence guarantee to solve the general LRML model. More importantly, by using the Kurdyka-Łojasiewicz (KŁ) inequality, its local and global convergence properties are established. Finally, we show that the proposed nonconvex regularizer, as well as the optimization approach are suitable for different low rank tasks, such as matrix completion and subspace clustering. Extensive experimental results demonstrate that the constructed models and methods provide significant advantages over several state-of-the-art low rank matrix leaning models and methods.},
  archive      = {J_TPAMI},
  author       = {Zhi Wang and Dong Hu and Zhuo Liu and Chao Gao and Zhen Wang},
  doi          = {10.1109/TPAMI.2024.3512458},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1923-1940},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Iteratively capped reweighting norm minimization with global convergence guarantee for low-rank matrix learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Scale propagation network for generalizable depth
completion. <em>TPAMI</em>, <em>47</em>(3), 1908–1922. (<a
href="https://doi.org/10.1109/TPAMI.2024.3513440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion, inferring dense depth maps from sparse measurements, is crucial for robust 3D perception. Although deep learning based methods have made tremendous progress in this problem, these models cannot generalize well across different scenes that are unobserved in training, posing a fundamental limitation that yet to be overcome. A careful analysis of existing deep neural network architectures for depth completion, which are largely borrowing from successful backbones for image analysis tasks, reveals that a key design bottleneck actually resides in the conventional normalization layers. These normalization layers are designed, on one hand, to make training more stable, on the other hand, to build more visual invariance across scene scales. However, in depth completion, the scale is actually what we want to robustly estimate in order to better generalize to unseen scenes. To mitigate, we propose a novel scale propagation normalization (SP-Norm) method to propagate scales from input to output, and simultaneously preserve the normalization operator for easy convergence. More specifically, we rescale the input using learned features of a single-layer perceptron from the normalized input, rather than directly normalizing the input as conventional normalization layers. We then develop a new network architecture based on SP-Norm and the ConvNeXt V2 backbone. We explore the composition of various basic blocks and architectures to achieve superior performance and efficient inference for generalizable depth completion. Extensive experiments are conducted on six unseen datasets with various types of sparse depth maps, i.e., randomly sampled 0.1%/1%/10% valid pixels, 4/8/16/32/64-line LiDAR points, and holes from Structured-Light. Our model consistently achieves the best accuracy with faster speed and lower memory when compared to state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Haotian Wang and Meng Yang and Xinhu Zheng and Gang Hua},
  doi          = {10.1109/TPAMI.2024.3513440},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1908-1922},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Scale propagation network for generalizable depth completion},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JARVIS-1: Open-world multi-task agents with memory-augmented
multimodal language models. <em>TPAMI</em>, <em>47</em>(3), 1894–1907.
(<a href="https://doi.org/10.1109/TPAMI.2024.3511593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable of completing over 200 different tasks using control and observation space similar to humans. These tasks range from short-horizon tasks, e.g., “chopping trees” to long-horizon ones, e.g., “obtaining a diamond pickaxe”. JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly perfect performance. In the classic long-term task of ObtainDiamondPickaxe, JARVIS-1 surpasses the reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon and more challenging tasks. Furthermore, we show that JARVIS-1 is able to self-improve following a life-long learning paradigm thanks to multimodal memory, sparking a more general intelligence and improved autonomy.},
  archive      = {J_TPAMI},
  author       = {Zihao Wang and Shaofei Cai and Anji Liu and Yonggang Jin and Jinbing Hou and Bowei Zhang and Haowei Lin and Zhaofeng He and Zilong Zheng and Yaodong Yang and Xiaojian Ma and Yitao Liang},
  doi          = {10.1109/TPAMI.2024.3511593},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1894-1907},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {JARVIS-1: Open-world multi-task agents with memory-augmented multimodal language models},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LVLM-EHub: A comprehensive evaluation benchmark for large
vision-language models. <em>TPAMI</em>, <em>47</em>(3), 1877–1893. (<a
href="https://doi.org/10.1109/TPAMI.2024.3507000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building an LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of 13 representative LVLMs such as InstructBLIP and LLaVA, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates five categories of multimodal capabilities of LVLMs such as visual question answering and object hallucination on 42 in-domain text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study investigates how specific features of LVLMs such as model configurations, modality alignment mechanisms, and training data affect the multimodal understanding. By conducting a comprehensive comparison of these features on quantitative and arena evaluation, our study uncovers several innovative findings, which establish a fundamental framework for the development and evaluation of innovative strategies aimed at enhancing multimodal techniques.},
  archive      = {J_TPAMI},
  author       = {Peng Xu and Wenqi Shao and Kaipeng Zhang and Peng Gao and Shuo Liu and Meng Lei and Fanqing Meng and Siyuan Huang and Yu Qiao and Ping Luo},
  doi          = {10.1109/TPAMI.2024.3507000},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1877-1893},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LVLM-EHub: A comprehensive evaluation benchmark for large vision-language models},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised global and local homography estimation with
coplanarity-aware GAN. <em>TPAMI</em>, <em>47</em>(3), 1863–1876. (<a
href="https://doi.org/10.1109/TPAMI.2024.3509614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised methods have received increasing attention in homography learning due to their promising performance and label-free training. However, existing methods do not explicitly consider the plane-induced parallax, making the prediction compromised on multiple planes. In this work, we propose a novel method HomoGAN to guide unsupervised homography estimation to focus on the dominant plane. First, a multi-scale transformer is designed to predict homography from the feature pyramids of input images in a coarse-to-fine fashion. Moreover, we propose an unsupervised GAN to impose coplanarity constraint on the predicted homography, which is realized by using a generator to predict a mask of aligned regions, and then a discriminator to check if two masked feature maps are induced by a single homography. Based on the global homography framework, we extend it to the local mesh-grid homography estimation, namely, MeshHomoGAN, where plane constraints can be enforced on each mesh cell to go beyond a single dominant plane, such that scenes with multiple depth planes can be better aligned. To validate the effectiveness of our method and its components, we conduct extensive experiments on large-scale datasets. Results show that our matching error is 22% lower than previous SOTA methods.},
  archive      = {J_TPAMI},
  author       = {Shuaicheng Liu and Mingbo Hong and Yuhang Lu and Nianjin Ye and Chunyu Lin and Bing Zeng},
  doi          = {10.1109/TPAMI.2024.3509614},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1863-1876},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unsupervised global and local homography estimation with coplanarity-aware GAN},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ELDP: Enhanced label distribution propagation for
crowdsourcing. <em>TPAMI</em>, <em>47</em>(3), 1850–1862. (<a
href="https://doi.org/10.1109/TPAMI.2024.3507774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowdsourcing scenarios, we can obtain multiple noisy labels for an instance from crowd workers and then aggregate these labels to infer the unknown true label of this instance. Due to the lack of expertise of workers, obtained labels usually contain a degree of noise. Existing studies usually focus on the crowdsourcing scenarios with low noise ratios but rarely focus on the crowdsourcing scenarios with high noise ratios. In this paper, we focus on the crowdsourcing scenarios with high noise ratios and propose a novel label aggregation algorithm called enhanced label distribution propagation (ELDP). First, ELDP harnesses an internal worker weighting method to estimate the weights of workers and then performs the first label distribution enhancement. Then, for instances not covered in the first enhancement, ELDP performs the second enhancement using a class membership estimation method based on the intra-cluster distance. Finally, ELDP propagates enhanced label distributions from accurately enhanced instances to inaccurately enhanced instances. Experimental results on both simulated and real-world crowdsourced datasets show that ELDP significantly outperforms all the other state-of-the-art label aggregation algorithms.},
  archive      = {J_TPAMI},
  author       = {Wenjun Zhang and Liangxiao Jiang and Chaoqun Li},
  doi          = {10.1109/TPAMI.2024.3507774},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1850-1862},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ELDP: Enhanced label distribution propagation for crowdsourcing},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STAR: A first-ever dataset and a large-scale benchmark for
scene graph generation in large-size satellite imagery. <em>TPAMI</em>,
<em>47</em>(3), 1832–1849. (<a
href="https://doi.org/10.1109/TPAMI.2024.3508072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting understanding of geospatial scenarios from perception to cognition. In SAI, objects exhibit great variations in scales and aspect ratios, and there exist rich relationships between objects (even between spatially disjoint objects), which makes it attractive to holistically conduct SGG in large-size very-high-resolution (VHR) SAI. However, there lack such SGG datasets. Due to the complexity of large-size SAI, mining triplets $&amp;lt; $&lt;mml:math&gt;&lt;mml:mo&gt;&amp;lt;&lt;/mml:mo&gt;&lt;/mml:math&gt;subject, relationship, object$&amp;gt; $&lt;mml:math&gt;&lt;mml:mo&gt;&amp;gt;&lt;/mml:mo&gt;&lt;/mml:math&gt; heavily relies on long-range contextual reasoning. Consequently, SGG models designed for small-size natural imagery are not directly applicable to large-size SAI. This paper constructs a large-scale dataset for SGG in large-size VHR SAI with image sizes ranging from 512 × 768 to 27 860 × 31 096 pixels, named STAR (Scene graph generaTion in lArge-size satellite imageRy), encompassing over 210K objects and over 400K triplets. To realize SGG in large-size SAI, we propose a context-aware cascade cognition (CAC) framework to understand SAI regarding object detection (OBD), pair pruning and relationship prediction for SGG. We also release a SAI-oriented SGG toolkit with about 30 OBD and 10 SGG methods which need further adaptation by our devised modules on our challenging STAR dataset.},
  archive      = {J_TPAMI},
  author       = {Yansheng Li and Linlin Wang and Tingzhu Wang and Xue Yang and Junwei Luo and Qi Wang and Youming Deng and Wenbin Wang and Xian Sun and Haifeng Li and Bo Dang and Yongjun Zhang and Yi Yu and Junchi Yan},
  doi          = {10.1109/TPAMI.2024.3508072},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1832-1849},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {STAR: A first-ever dataset and a large-scale benchmark for scene graph generation in large-size satellite imagery},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized face liveness detection via de-fake face
generator. <em>TPAMI</em>, <em>47</em>(3), 1818–1831. (<a
href="https://doi.org/10.1109/TPAMI.2024.3507101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous Face Anti-spoofing (FAS) methods face the challenge of generalizing to unseen domains, mainly because most existing FAS datasets are relatively small and lack data diversity. Thanks to the development of face recognition in the past decade, numerous real face images are available publicly, which are however neglected previously by the existing literature. In this paper, we propose an Anomalous cue Guided FAS (AG-FAS) method, which can effectively leverage large-scale additional real faces for improving model generalization via a De-fake Face Generator (DFG). Specifically, by training on a large-scale real face only dataset, the generator obtains the knowledge of what a real face should be like, and thus has the capability of generating a “real” version of any input face image. Consequently, the difference between the input face and the generated “real” face can be treated as cues of attention for the fake feature learning. With the above ideas, an Off-real Attention Network (OA-Net) is proposed which allocates its attention to the spoof region of the input according to the anomalous cue. Extensive experiments on a total of nine public datasets show our method achieves state-of-the-art results under cross-domain evaluations with unseen scenarios and unknown presentation attacks. Besides, we provide theoretical analysis demonstrating the effectiveness of the proposed anomalous cues.},
  archive      = {J_TPAMI},
  author       = {Xingming Long and Jie Zhang and Shiguang Shan},
  doi          = {10.1109/TPAMI.2024.3507101},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1818-1831},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generalized face liveness detection via de-fake face generator},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NAS-PED: Neural architecture search for pedestrian
detection. <em>TPAMI</em>, <em>47</em>(3), 1800–1817. (<a
href="https://doi.org/10.1109/TPAMI.2024.3507918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection currently suffers from two issues in crowded scenes: occlusion and dense boundary prediction, making it still challenging in complex real-world scenarios. In recent years, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have shown their superiorities in addressing these issues, where ViTs capture global feature dependency to infer occlusion parts and CNNs make accurate dense predictions by local detailed features. Nevertheless, limited by the narrow receptive field, CNNs fail to infer occlusion parts, while ViTs tend to ignore local features that are vital to distinguish different pedestrians in the crowd. Therefore, it is essential to combine the advantages of CNN and ViT for pedestrian detection. However, manually designing a specific CNN and ViT hybrid network requires enormous time and resources for trial and error. To address this issue, we propose the first Neural Architecture Search (NAS) framework specifically designed for pedestrian detection named NAS-PED, which automatically designs an appropriate CNNs and ViTs hybrid backbone for the crowded pedestrian detection task. Specifically, we formulate transformers and convolutions with various kernel sizes in the same format, which provides an unconstrained space for diverse hybrid network search. Furthermore, to search for a suitable backbone, we propose an information bottleneck based NAS objective function, which treats the process of NAS as an information extraction process, preserving relevant information and suppressing redundant information from the dense pedestrians in crowd scenes Extensive experiments on CrowdHuman, CityPersons and EuroCity Persons datasets demonstrate the effectiveness of the proposed method. Our NAS-PED obtains absolute gains of 4.0% MR$^{-2}$ and 1.9% AP over the state-of-the-art (SOTA) pedestrian detection framework on CrowdHuman datasets. For the CityPersons and EuroCity Persons datasets, the searched backbone achieves stable improvement across all three subsets, outperforming some large language-image pre-trained models.},
  archive      = {J_TPAMI},
  author       = {Yi Tang and Min Liu and Baopu Li and Yaonan Wang and Wanli Ouyang},
  doi          = {10.1109/TPAMI.2024.3507918},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1800-1817},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NAS-PED: Neural architecture search for pedestrian detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uni-AdaFocus: Spatial-temporal dynamic computation for video
recognition. <em>TPAMI</em>, <em>47</em>(3), 1782–1799. (<a
href="https://doi.org/10.1109/TPAMI.2024.3514654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive exploration of the phenomenon of data redundancy in video understanding, with the aim to improve computational efficiency. Our investigation commences with an examination of spatial redundancy, which refers to the observation that the most informative region in each video frame usually corresponds to a small image patch, whose shape, size and location shift smoothly across frames. Motivated by this phenomenon, we formulate the patch localization problem as a dynamic decision task, and introduce a spatially adaptive video recognition approach, termed AdaFocus. In specific, a lightweight encoder is first employed to quickly process the full video sequence, whose features are then utilized by a policy network to identify the most task-relevant regions. Subsequently, the selected patches are inferred by a high-capacity deep network for the final prediction. The complete model can be trained conveniently in an end-to-end manner. During inference, once the informative patch sequence has been generated, the bulk of computation can be executed in parallel, rendering it efficient on modern GPU devices. Furthermore, we demonstrate that AdaFocus can be easily extended by further considering the temporal and sample-wise redundancies, i.e., allocating the majority of computation to the most task-relevant video frames, and minimizing the computation spent on relatively “easier” videos. Our resulting algorithm, Uni-AdaFocus, establishes a comprehensive framework that seamlessly integrates spatial, temporal, and sample-wise dynamic computation, while it preserves the merits of AdaFocus in terms of efficient end-to-end training and hardware friendliness. In addition, Uni-AdaFocus is general and flexible as it is compatible with off-the-shelf backbone models (e.g., TSM and X3D), which can be readily deployed as our feature extractor, yielding a significantly improved computational efficiency. Empirically, extensive experiments based on seven widely-used benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&amp;V2, Jester, and Kinetics-400) and three real-world application scenarios (i.e., fine-grained diving action classification, Alzheimer’s and Parkinson’s diseases diagnosis with brain magnetic resonance images (MRI), and violence recognition for online videos) substantiate that Uni-AdaFocus is considerably more efficient than the competitive baselines.},
  archive      = {J_TPAMI},
  author       = {Yulin Wang and Haoji Zhang and Yang Yue and Shiji Song and Chao Deng and Junlan Feng and Gao Huang},
  doi          = {10.1109/TPAMI.2024.3514654},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1782-1799},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Uni-AdaFocus: Spatial-temporal dynamic computation for video recognition},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaEarth: A generative foundation model for global-scale
remote sensing image generation. <em>TPAMI</em>, <em>47</em>(3),
1764–1781. (<a
href="https://doi.org/10.1109/TPAMI.2024.3507010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancement of generative foundational models has ushered in a new era of image generation in the realm of natural images, revolutionizing art design, entertainment, environment simulation, and beyond. Despite producing high-quality samples, existing methods are constrained to generating images of scenes at a limited scale. In this paper, we present MetaEarth - a generative foundation model that breaks the barrier by scaling image generation to a global level, exploring the creation of worldwide, multi-resolution, unbounded, and virtually limitless remote sensing images. In MetaEarth, we propose a resolution-guided self-cascading generative framework, which enables the generating of images at any region with a wide range of geographical resolutions. To achieve unbounded and arbitrary-sized image generation, we design a novel noise sampling strategy for denoising diffusion models by analyzing the generation conditions and initial noise. To train MetaEarth, we construct a large dataset comprising multi-resolution optical remote sensing images with geographical information. Experiments have demonstrated the powerful capabilities of our method in generating global-scale images. Additionally, the MetaEarth serves as a data engine that can provide high-quality and rich training data for downstream tasks. Our model opens up new possibilities for constructing generative world models by simulating Earthâs visuals from an innovative overhead perspective.},
  archive      = {J_TPAMI},
  author       = {Zhiping Yu and Chenyang Liu and Liqin Liu and Zhenwei Shi and Zhengxia Zou},
  doi          = {10.1109/TPAMI.2024.3507010},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1764-1781},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MetaEarth: A generative foundation model for global-scale remote sensing image generation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive graph learning with semantic promotability for
domain adaptation. <em>TPAMI</em>, <em>47</em>(3), 1747–1763. (<a
href="https://doi.org/10.1109/TPAMI.2024.3507534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Adaptation (DA) is used to reduce cross-domain differences between the labeled source and unlabeled target domains. As the existing semantic-based DA approaches mainly focus on extracting consistent knowledge under semantic guidance, they may fail in acquiring: (a) personalized knowledge between intra-class samples and (b) local knowledge of neighbor samples from different categories. Hence, a multi-semantic-granularity and target-sample oriented approach, called Adaptive Graph Learning with Semantic Promotability (AGLSP), is proposed, which consists of three parts: (a) Adaptive Graph Embedding with Semantic Guidance (AGE-SG) that adaptively estimates the promotability of target samples and learns variant semantic and geometrical components from the source and those semantically promotable target samples; (b) Semantically Promotable Sample Enhancement (SPSE) that further increases the discriminability and adaptability of tag granularity by mining the features of intra-class source and semantically promotable target samples with multi-granularities; and (c) Adaptive Graph Learning with Implicit Semantic Preservation (AGL-ISP) that forms the tag granularity by extracting commonalities between the source and those semantically non-promotable target samples. As AGLSP learns more semantics from the two domains, more cross-domain knowledge is transferred. Mathematical proofs and extensive experiments on seven datasets demonstrate the performance of AGLSP.},
  archive      = {J_TPAMI},
  author       = {Zefeng Zheng and Shaohua Teng and Luyao Teng and Wei Zhang and NaiQi Wu},
  doi          = {10.1109/TPAMI.2024.3507534},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1747-1763},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive graph learning with semantic promotability for domain adaptation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair clustering ensemble with equal cluster capacity.
<em>TPAMI</em>, <em>47</em>(3), 1729–1746. (<a
href="https://doi.org/10.1109/TPAMI.2024.3507857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble has been widely studied in data mining and machine learning. However, the existing clustering ensemble methods do not pay attention to fairness, which is important in real-world applications, especially in applications involving humans. To address this issue, this paper proposes a novel fair clustering ensemble method, which takes multiple base clustering results as inputs and learns a fair consensus clustering result. When designing the algorithm, we observe that one of the widely used definitions of fairness may cause a cluster imbalance problem. To tackle this problem, we give a new definition of fairness that can simultaneously characterize fairness and cluster capacity equality. Based on this new definition, we design an extremely simple yet effective regularized term to achieve fairness and cluster capacity equality. We plug this regularized term into our clustering ensemble framework, finally leading to our new fair clustering ensemble method. The extensive experiments show that, compared with the state-of-the-art clustering ensemble methods, our method can not only achieve a comparable or even better clustering performance, but also obtain a much fairer and better capacity equality result, which well demonstrates the effectiveness and superiority of our method.},
  archive      = {J_TPAMI},
  author       = {Peng Zhou and Rongwen Li and Zhaolong Ling and Liang Du and Xinwang Liu},
  doi          = {10.1109/TPAMI.2024.3507857},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1729-1746},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fair clustering ensemble with equal cluster capacity},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIGC++: Advanced multi-instance generation controller for
image synthesis. <em>TPAMI</em>, <em>47</em>(3), 1714–1728. (<a
href="https://doi.org/10.1109/TPAMI.2024.3510752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Multi-Instance Generation (MIG) task, which focuses on generating multiple instances within a single image, each accurately placed at predefined positions with attributes such as category, color, and shape, strictly following user specifications. MIG faces three main challenges: avoiding attribute leakage between instances, supporting diverse instance descriptions, and maintaining consistency in iterative generation. To address attribute leakage, we propose the Multi-Instance Generation Controller (MIGC). MIGC generates multiple instances through a divide-and-conquer strategy, breaking down multi-instance shading into single-instance tasks with singular attributes, later integrated. To provide more types of instance descriptions, we developed MIGC++. MIGC++ allows attribute control through text &amp; images and position control through boxes &amp; masks. Lastly, we introduced the Consistent-MIG algorithm to enhance the iterative MIG ability of MIGC and MIGC++. This algorithm ensures consistency in unmodified regions during the addition, deletion, or modification of instances, and preserves the identity of instances when their attributes are changed. We introduce the COCO-MIG and Multimodal-MIG benchmarks to evaluate these methods. Extensive experiments on these benchmarks, along with the COCO-Position benchmark and DrawBench, demonstrate that our methods substantially outperform existing techniques, maintaining precise control over aspects including position, attribute, and quantity.},
  archive      = {J_TPAMI},
  author       = {Dewei Zhou and You Li and Fan Ma and Zongxin Yang and Yi Yang},
  doi          = {10.1109/TPAMI.2024.3510752},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1714-1728},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MIGC++: Advanced multi-instance generation controller for image synthesis},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WinDB: HMD-free and distortion-free panoptic video fixation
learning. <em>TPAMI</em>, <em>47</em>(3), 1694–1713. (<a
href="https://doi.org/10.1109/TPAMI.2024.3510793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date, the widely adopted way to perform fixation collection in panoptic video is based on a head-mounted display (HMD), where users’ fixations are collected while wearing a HMD to explore the given panoptic scene freely. However, this widely-used data collection method is insufficient for training deep models to accurately predict which regions in a given panoptic are most important when it contains intermittent salient events. The main reason is that there always exist “blind zooms” when using HMD to collect fixations since the users cannot keep spinning their heads to explore the entire panoptic scene all the time. Consequently, the collected fixations tend to be trapped in some local views, leaving the remaining areas to be the “blind zooms”. Therefore, fixation data collected using HMD-based methods that accumulate local views cannot accurately represent the overall global importance—the main purpose of fixations—of complex panoptic scenes. To conquer, this paper introduces the auxiliary window with a dynamic blurring (WinDB) fixation collection approach for panoptic video, which doesn&#39;t need HMD and is able to well reflect the regional-wise importance degree. Using our WinDB approach, we have released a new PanopticVideo-300 dataset, containing 300 panoptic clips covering over 225 categories. Specifically, since using WinDB to collect fixations is blind zoom free, there exists frequent and intensive “fixation shifting”—a very special phenomenon that has long been overlooked by the previous research—in our new set. Thus, we present an effective fixation shifting network (FishNet) to conquer it. All these new fixation collection tool, dataset, and network could be very potential to open a new age for fixation-related research and applications in $360^\mathrm{o}$ environments.},
  archive      = {J_TPAMI},
  author       = {Guotao Wang and Chenglizhao Chen and Aimin Hao and Hong Qin and Deng-Ping Fan},
  doi          = {10.1109/TPAMI.2024.3510793},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1694-1713},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {WinDB: HMD-free and distortion-free panoptic video fixation learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and transferable backdoor attacks against deep image
compression with selective frequency prior. <em>TPAMI</em>,
<em>47</em>(3), 1674–1693. (<a
href="https://doi.org/10.1109/TPAMI.2024.3507873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning-based compression techniques have demonstrated remarkable performance surpassing traditional methods. Nevertheless, deep neural networks have been observed to be vulnerable to backdoor attacks, where an added pre-defined trigger pattern can induce the malicious behavior of the models. In this paper, we propose a novel approach to launch a backdoor attack with multiple triggers against learned image compression models. Drawing inspiration from the widely used discrete cosine transform (DCT) in existing compression codecs and standards, we propose a frequency-based trigger injection model that adds triggers in the DCT domain. In particular, we design several attack objectives that are adapted for a series of diverse scenarios, including: 1) attacking compression quality in terms of bit-rate and reconstruction quality; 2) attacking task-driven measures, such as face recognition and semantic segmentation in downstream applications. To facilitate more efficient training, we develop a dynamic loss function that dynamically balances the impact of different loss terms with fewer hyper-parameters, which also results in more effective optimization of the attack objectives with improved performance. Furthermore, we consider several advanced scenarios. We evaluate the resistance of the proposed backdoor attack to the defensive pre-processing methods and then propose a two-stage training schedule along with the design of robust frequency selection to further improve resistance. To strengthen both the cross-model and cross-domain transferability on attacking downstream CV tasks, we propose to shift the classification boundary in the attack loss during training. Extensive experiments also demonstrate that by employing our trained trigger injection models and making slight modifications to the encoder parameters of the compression model, our proposed attack can successfully inject multiple backdoors accompanied by their corresponding triggers into a single image compression model.},
  archive      = {J_TPAMI},
  author       = {Yi Yu and Yufei Wang and Wenhan Yang and Lanqing Guo and Shijian Lu and Ling-Yu Duan and Yap-Peng Tan and Alex C. Kot},
  doi          = {10.1109/TPAMI.2024.3507873},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1674-1693},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust and transferable backdoor attacks against deep image compression with selective frequency prior},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anchor3DLane++: 3D lane detection via sample-adaptive sparse
3D anchor regression. <em>TPAMI</em>, <em>47</em>(3), 1660–1673. (<a
href="https://doi.org/10.1109/TPAMI.2024.3508798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the challenging task of monocular 3D lane detection. Previous methods typically adopt inverse perspective mapping (IPM) to transform the Front-Viewed (FV) images or features into the Bird-Eye-Viewed (BEV) space for lane detection. However, IPM&#39;s dependence on flat ground assumption and context information loss in BEV representations lead to inaccurate 3D information estimation. Though efforts have been made to bypass BEV and directly predict 3D lanes from FV representations, their performances still fall behind BEV-based methods due to a lack of structured modeling of 3D lanes. In this paper, we propose a novel BEV-free method named Anchor3DLane++ which defines 3D lane anchors as structural representations and makes predictions directly from FV features. We also design a Prototype-based Adaptive Anchor Generation (PAAG) module to generate sample-adaptive sparse 3D anchors dynamically. In addition, an Equal-Width (EW) loss is developed to leverage the parallel property of lanes for regularization. Furthermore, camera-LiDAR fusion is also explored based on Anchor3DLane++ to leverage complementary information. Extensive experiments on three popular 3D lane detection benchmarks show that our Anchor3DLane++ outperforms previous state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Shaofei Huang and Zhenwei Shen and Zehao Huang and Yue Liao and Jizhong Han and Naiyan Wang and Si Liu},
  doi          = {10.1109/TPAMI.2024.3508798},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1660-1673},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Anchor3DLane++: 3D lane detection via sample-adaptive sparse 3D anchor regression},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffAct++: Diffusion action segmentation. <em>TPAMI</em>,
<em>47</em>(3), 1644–1659. (<a
href="https://doi.org/10.1109/TPAMI.2024.3509434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding long-form videos requires precise temporal action segmentation. While existing studies typically employ multi-stage models that follow an iterative refinement process, we present a novel framework based on the denoising diffusion model that retains this core iterative principle. Within this framework, the model iteratively produces action predictions starting with random noise, conditioned on the features of the input video. To effectively capture three key characteristics of human actions, namely the position prior, the boundary ambiguity, and the relational dependency, we propose a cohesive masking strategy for the conditioning features. Moreover, a consistency gradient guidance technique is proposed, which maximizes the similarity between outputs with or without the masking, thereby enriching conditional information during the inference process. Extensive experiments are performed on four datasets, i.e., GTEA, 50Salads, Breakfast, and Assembly101. The results indicate that our proposed method outperforms or is on par with existing state-of-the-art techniques, underscoring the potential of generative approaches for action segmentation.},
  archive      = {J_TPAMI},
  author       = {Daochang Liu and Qiyue Li and Anh-Dung Dinh and Tingting Jiang and Mubarak Shah and Chang Xu},
  doi          = {10.1109/TPAMI.2024.3509434},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1644-1659},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DiffAct++: Diffusion action segmentation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IBCS: Learning information bottleneck-constrained denoised
causal subgraph for graph classification. <em>TPAMI</em>,
<em>47</em>(3), 1627–1643. (<a
href="https://doi.org/10.1109/TPAMI.2024.3508766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significant success of graph learning has provoked a meaningful but challenging task of extracting the precise causal subgraphs that can interpret and improve the predictions. Unfortunately, current works merely center on partially eliminating either the spurious or the noisy parts, while overlook the fact that in more practical and general situations, both the spurious and noisy subgraph coexist with the causal one. This brings great challenges and makes previous methods fail to extract the true causal substructure. Unlike existing studies, in this paper, we propose a more reasonable problem formulation that hypothesizes the graph is a mixture of causal, spurious, and noisy subgraphs. With this regard, an Information Bottleneck-constrained denoised Causal Subgraph (IBCS) learning model is developed, which is capable of simultaneously excluding the spurious and noisy parts. Specifically, for the spurious correlation, we design a novel causal learning objective, in which beyond minimizing the empirical risks of causal and spurious subgraph classification, the intervention is further conducted on spurious features to cut off its correlation with the causal part. On this basis, we further impose the information bottleneck constraint to filter out label-irrelevant noise information. Theoretically, we prove that the causal subgraph extracted by our IBCS can approximate the ground-truth. Empirically, extensive evaluations on nine benchmark datasets demonstrate our superiority over state-of-the-art baselines.},
  archive      = {J_TPAMI},
  author       = {Ruiwen Yuan and Yongqiang Tang and Yanghao Xiao and Wensheng Zhang},
  doi          = {10.1109/TPAMI.2024.3508766},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1627-1643},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {IBCS: Learning information bottleneck-constrained denoised causal subgraph for graph classification},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical compact deep compressed sensing. <em>TPAMI</em>,
<em>47</em>(3), 1610–1626. (<a
href="https://doi.org/10.1109/TPAMI.2024.3504490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the success of deep networks in compressed sensing (CS), which allows for a significant reduction in sampling cost and has gained growing attention since its inception. In this paper, we propose a new practical and compact network dubbed PCNet for general image CS. Specifically, in PCNet, a novel collaborative sampling operator is designed, which consists of a deep conditional filtering step and a dual-branch fast sampling step. The former learns an implicit representation of a linear transformation matrix into a few convolutions and first performs adaptive local filtering on the input image, while the latter then uses a discrete cosine transform and a scrambled block-diagonal Gaussian matrix to generate under-sampled measurements. Our PCNet is equipped with an enhanced proximal gradient descent algorithm-unrolled network for reconstruction. It offers flexibility, interpretability, and strong recovery performance for arbitrary sampling rates once trained. Additionally, we provide a deployment-oriented extraction scheme for single-pixel CS imaging systems, which allows for the convenient conversion of any linear sampling operator to its matrix form to be loaded onto hardware like digital micro-mirror devices. Extensive experiments on natural image CS, quantized CS, and self-supervised CS demonstrate the superior reconstruction accuracy and generalization ability of PCNet compared to existing state-of-the-art methods, particularly for high-resolution images.},
  archive      = {J_TPAMI},
  author       = {Bin Chen and Jian Zhang},
  doi          = {10.1109/TPAMI.2024.3504490},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1610-1626},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Practical compact deep compressed sensing},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained visual text prompting. <em>TPAMI</em>,
<em>47</em>(3), 1594–1609. (<a
href="https://doi.org/10.1109/TPAMI.2024.3504568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-Language Models (VLMs), such as CLIP, excel in zero-shot image-level visual understanding but struggle with object-based tasks requiring precise localization and recognition. Visual prompts, like colorful boxes or circles, are suggested to enhance local perception. However, these methods often include irrelevant and noisy pixels, leading to suboptimal performance. The design of better visual prompts and their collaboration with text prompting remains underexplored. This paper introduces Fine-Grained Visual Text Prompting (FGVTP), a new zero-shot framework for object-based tasks using precise semantic masks and reinforced image-text alignment. FGVTP comprises Fine-Grained Visual Prompting (FGVP) and Consistency-Enhanced Text Prompting (CETP). Specifically, we carefully study visual prompting designs by exploring more visual markings that vary in shape and form. FGVP uses semantic masks from a segmenter like the Segment Anything Model (SAM) and employs background blurring (Blur Reverse Mask) to highlight targets while maintaining spatial coherence. Further, CETP enhances image-text alignment by prompting captions based on FGVP-processed images. As a result, FGVTP achieves superior zero-shot referring expression comprehension on RefCOCO/+/g benchmarks, outperforming previous SOTA methods by 5.8% on average. Part detection experiments conducted on the PACO dataset further validate the preponderance of FGVTP over existing works.},
  archive      = {J_TPAMI},
  author       = {Lingfeng Yang and Xiang Li and Yueze Wang and Xinlong Wang and Jian Yang},
  doi          = {10.1109/TPAMI.2024.3504568},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1594-1609},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fine-grained visual text prompting},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffI2I: Efficient diffusion model for image-to-image
translation. <em>TPAMI</em>, <em>47</em>(3), 1578–1593. (<a
href="https://doi.org/10.1109/TPAMI.2024.3498003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Diffusion Model (DM) has emerged as the SOTA approach for image synthesis. However, the existing DM cannot perform well on some image-to-image translation (I2I) tasks. Different from image synthesis, some I2I tasks, such as super-resolution, require generating results in accordance with GT images. Traditional DMs for image synthesis require extensive iterations and large denoising models to estimate entire images, which gives their strong generative ability but also leads to artifacts and inefficiency for I2I. To tackle this challenge, we propose a simple, efficient, and powerful DM framework for I2I, called DiffI2I. Specifically, DiffI2I comprises three key components: a compact I2I prior extraction network (CPEN), a dynamic I2I transformer (DI2Iformer), and a denoising network. We train DiffI2I in two stages: pretraining and DM training. For pretraining, GT and input images are fed into CPEN$_{S1}$ to capture a compact I2I prior representation (IPR) guiding DI2Iformer. In the second stage, the DM is trained to only use the input images to estimate the same IRP as CPEN$_{S1}$. Compared to traditional DMs, the compact IPR enables DiffI2I to obtain more accurate outcomes and employ a lighter denoising network and fewer iterations. Through extensive experiments on various I2I tasks, we demonstrate that DiffI2I achieves SOTA performance while significantly reducing computational burdens.},
  archive      = {J_TPAMI},
  author       = {Bin Xia and Yulun Zhang and Shiyin Wang and Yitong Wang and Xinglong Wu and Yapeng Tian and Wenming Yang and Radu Timotfe and Luc Van Gool},
  doi          = {10.1109/TPAMI.2024.3498003},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1578-1593},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DiffI2I: Efficient diffusion model for image-to-image translation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Streaming quanta sensors for online, high-performance
imaging and vision. <em>TPAMI</em>, <em>47</em>(3), 1564–1577. (<a
href="https://doi.org/10.1109/TPAMI.2024.3501154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently quanta image sensors (QIS) – ultra-fast, zero-read-noise binary image sensors– have demonstrated remarkable imaging capabilities in many challenging scenarios. Despite their potential, the adoption of these sensors is severely hampered by (a) high data rates and (b) the need for new computational pipelines to handle the unconventional raw data. We introduce a simple, low-bandwidth computational pipeline to address these challenges. Our approach is based on a novel streaming representation with a small memory footprint, efficiently capturing intensity information at multiple temporal scales. Updating the representation requires only 24floating-point operations/pixel, which can be efficiently computed online at the native frame rate of the binary frames. We use a neural network operating on this representation to reconstruct videos in real-time (10-30 fps). We illustrate why such representation is well-suited for these emerging sensors, and how it offers low latency and high frame rate while retaining flexibility for downstream computer vision. Our approach results in significant data bandwidth reductions ($\sim 100\times$) and real-time image reconstruction and computer vision $-10^{4}\text{-}10^{5} \times$ reduction in computation than existing state-of-the-art approach (Ma et al. 2020), while maintaining comparable quality. To the best of our knowledge, our approach is the first to achieve online, real-time image reconstruction on QIS.},
  archive      = {J_TPAMI},
  author       = {Tianyi Zhang and Matthew Dutson and Vivek Boominathan and Mohit Gupta and Ashok Veeraraghavan},
  doi          = {10.1109/TPAMI.2024.3501154},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1564-1577},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Streaming quanta sensors for online, high-performance imaging and vision},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial scene text retrieval. <em>TPAMI</em>,
<em>47</em>(3), 1548–1563. (<a
href="https://doi.org/10.1109/TPAMI.2024.3496576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of partial scene text retrieval involves localizing and searching for text instances that are the same or similar to a given query text from an image gallery. However, existing methods can only handle text-line instances, leaving the problem of searching for partial patches within these text-line instances unsolved due to a lack of patch annotations in the training data. To address this issue, we propose a network that can simultaneously retrieve both text-line instances and their partial patches. Our method embeds the two types of data (query text and scene text instances) into a shared feature space and measures their cross-modal similarities. To handle partial patches, our proposed approach adopts a Multiple Instance Learning (MIL) approach to learn their similarities with query text, without requiring extra annotations. However, constructing bags, which is a standard step of conventional MIL approaches, can introduce numerous noisy samples for training, and lower inference speed. To address this issue, we propose a Ranking MIL (RankMIL) approach to adaptively filter those noisy samples. Additionally, we present a Dynamic Partial Match Algorithm (DPMA) that can directly search for the target partial patch from a text-line instance during the inference stage, without requiring bags. This greatly improves the search efficiency and the performance of retrieving partial patches. We evaluate the proposed method on both English and Chinese datasets in two tasks: retrieving text-line instances and partial patches. For English text retrieval, our method outperforms state-of-the-art approaches by 8.04% mAP and 12.71% mAP on average, respectively, among three datasets for the two tasks. For Chinese text retrieval, our approach surpasses state-of-the-art approaches by 24.45% mAP and 38.06% mAP on average, respectively, among three datasets for the two tasks.},
  archive      = {J_TPAMI},
  author       = {Hao Wang and Minghui Liao and Zhouyi Xie and Wenyu Liu and Xiang Bai},
  doi          = {10.1109/TPAMI.2024.3496576},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1548-1563},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Partial scene text retrieval},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BokehMe++: Harmonious fusion of classical and neural
rendering for versatile bokeh creation. <em>TPAMI</em>, <em>47</em>(3),
1530–1547. (<a
href="https://doi.org/10.1109/TPAMI.2024.3501739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advancements in simulating the bokeh effect of Digital Single Lens Reflex Camera (DSLR) from an all-in-focus image, challenges remain in processing highlight points, preserving boundary details for in-focus objects and processing high-resolution images efficiently. To tackle these issues, we first develop a ray-tracing-based bokeh simulator. An innovative pipeline with weight redistribution is introduced to handle highlight rendering. By considering the front length of lens barrel, we can simulate realistic cat-eye effect. This bokeh simulator serves as the foundation for creating our training dataset. Building on this dataset, we introduce a hybrid framework BokehMe++, combining a classical renderer and a neural renderer. The classical renderer is implemented by a hierarchical scattering-based method, which suffers from boundary inaccuracies. These erroneous areas will be identified by an error map generator and be corrected by a two-stage neural renderer. Adaptive resizing and iterative upsampling are introduced in the neural renderer to process arbitrary blur size efficiently. Extensive experiments demonstrate that BokehMe++ outperforms existing methods and provides highly customizable rendering features, such as adjustable blur amount, focal plane, highlight mode and cat-eye effect. Furthermore, BokehMe++ can maintain the sharpness of hair details in portraits through an auxiliary alpha map input.},
  archive      = {J_TPAMI},
  author       = {Juewen Peng and Zhiguo Cao and Xianrui Luo and Ke Xian and Wenfeng Tang and Jianming Zhang and Guosheng Lin},
  doi          = {10.1109/TPAMI.2024.3501739},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1530-1547},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BokehMe++: Harmonious fusion of classical and neural rendering for versatile bokeh creation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlation verification for image retrieval and its memory
footprint optimization. <em>TPAMI</em>, <em>47</em>(3), 1514–1529. (<a
href="https://doi.org/10.1109/TPAMI.2024.3504274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel image retrieval network named Correlation Verification Network (CVNet) to replace the conventional geometric re-ranking with a 4D convolutional neural network that learns diverse geometric matching possibilities. To enable efficient cross-scale matching, we construct feature pyramids and establish cross-scale feature correlations in a single inference, thereby replacing the costly multi-scale inference. Additionally, we employ curriculum learning with the Hide-and-Seek strategy to handle challenging samples. Our proposed CVNet demonstrates state-of-the-art performance on several image retrieval benchmarks by a large margin. From an implementation perspective, however, CVNet has one drawback: it requires high memory usage because it needs to store dense features of all database images. This high memory requirement can be a significant limitation in practical applications. To address this issue, we introduce an extension of CVNet called Dense-to-Sparse CVNet (CVNet$^{DS}$), which can significantly reduce memory usage by sparsifying the features of the database images. The sparsification module in CVNet$^{DS}$ learns to select the relevant parts of image features end-to-end using a Gumbel estimator. Since the sparsification is performed offline, CVNet$^{DS}$ does not increase online extraction and matching times. CVNet$^{DS}$ dramatically reduces the memory footprint while preserving performance levels nearly identical to CVNet.},
  archive      = {J_TPAMI},
  author       = {Seongwon Lee and Hongje Seong and Suhyeon Lee and Euntai Kim},
  doi          = {10.1109/TPAMI.2024.3504274},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1514-1529},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Correlation verification for image retrieval and its memory footprint optimization},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep loss convexification for learning iterative models.
<em>TPAMI</em>, <em>47</em>(3), 1501–1513. (<a
href="https://doi.org/10.1109/TPAMI.2024.3509860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iterative methods such as iterative closest point (ICP) for point cloud registration often suffer from bad local optimality (e.g., saddle points), due to the nature of nonconvex optimization. To address this fundamental challenge, in this paper we propose learning to form the loss landscape of a deep iterative method w.r.t. predictions at test time into a convex-like shape locally around each ground truth given data, namely Deep Loss Convexification (DLC), thanks to the overparametrization in neural networks. To this end, we formulate our learning objective based on adversarial training by manipulating the ground-truth predictions, rather than input data. In particular, we propose using star-convexity, a family of structured nonconvex functions that are unimodal on all lines that pass through a global minimizer, as our geometric constraint for reshaping loss landscapes, leading to (1) extra novel hinge losses appended to the original loss and (2) near-optimal predictions. We demonstrate the state-of-the-art performance using DLC with existing network architectures for the tasks of training recurrent neural networks (RNNs), 3D point cloud registration, and multimodel image alignment.},
  archive      = {J_TPAMI},
  author       = {Ziming Zhang and Yuping Shao and Yiqing Zhang and Fangzhou Lin and Haichong Zhang and Elke Rundensteiner},
  doi          = {10.1109/TPAMI.2024.3509860},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1501-1513},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep loss convexification for learning iterative models},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PATNAS: A path-based training-free neural architecture
search. <em>TPAMI</em>, <em>47</em>(3), 1484–1500. (<a
href="https://doi.org/10.1109/TPAMI.2024.3498035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of Neural Architecture Search (NAS) is hindered by high costs associated with evaluating network architectures. Recently, several zero-cost proxies have been proposed as a promising method to reduce the evaluation cost of network architectures in NAS. They can quickly estimate the final performance of the network in a few seconds during the initial phase. However, existing zero-cost proxies either ignore the network structure&#39;s impact on performance or are limited to specific tasks. To address these issues, we propose a novel zero-cost proxy called Skeleton Path Kernel Trace (SPKT) that leverages the whole network architecture&#39;s skeleton path structure information. We then integrate it into an effective Bayesian optimization for NAS framework called PATNAS, and demonstrate its efficacy on different datasets. The results show that our proposed SPKT zero-cost proxy can achieve a high correlation with the final performance of the network across multiple tasks. Furthermore, it can significantly accelerate the search process for finding the best-performing network architectures.},
  archive      = {J_TPAMI},
  author       = {Jiechao Yang and Yong Liu and Wei Wang and Haoran Wu and Zhiyuan Chen and Xibo Ma},
  doi          = {10.1109/TPAMI.2024.3498035},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1484-1500},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PATNAS: A path-based training-free neural architecture search},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). A comprehensive survey of forgetting in deep learning
beyond continual learning. <em>TPAMI</em>, <em>47</em>(3), 1464–1483.
(<a href="https://doi.org/10.1109/TPAMI.2024.3498346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forgetting refers to the loss or deterioration of previously acquired knowledge. While existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new task, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context, we present a more nuanced understanding of this phenomenon and highlight its potential advantages. Through this comprehensive survey, we aspire to uncover potential solutions by drawing upon ideas and approaches from various fields that have dealt with forgetting. By examining forgetting beyond its conventional boundaries, we hope to encourage the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications.},
  archive      = {J_TPAMI},
  author       = {Zhenyi Wang and Enneng Yang and Li Shen and Heng Huang},
  doi          = {10.1109/TPAMI.2024.3498346},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1464-1483},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A comprehensive survey of forgetting in deep learning beyond continual learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-oriented channel attention for fine-grained few-shot
classification. <em>TPAMI</em>, <em>47</em>(3), 1448–1463. (<a
href="https://doi.org/10.1109/TPAMI.2024.3504537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difficulty of fine-grained image classification mainly comes from a shared overall appearance across classes. Thus, recognizing discriminative details, such as the eyes and beaks of birds, is a key to the task. However, this is particularly challenging when training data is limited. To address this, we propose Task Discrepancy Maximization (TDM), a task-oriented channel attention method tailored for fine-grained few-shot classification with two novel modules Support Attention Module (SAM) and Query Attention Module (QAM). SAM highlights channels encoding class-wise discriminative features, while QAM assigns higher weights to object-relevant channels of the query. Based on these submodules, TDM produces task-adaptive features by focusing on channels encoding class-discriminative details and possessed by the query at the same time, for accurate class-sensitive similarity measure between support and query instances. While TDM influences high-level feature maps by task-adaptive calibration of channel-wise importance, we further introduce Instance Attention Module (IAM) operating in intermediate layers of feature extractors to instance-wisely highlight object-relevant channels, by extending QAM. The merits of TDM and IAM and their complementary benefits are experimentally validated in fine-grained few-shot classification tasks. Moreover, IAM is also effective in coarse-grained and cross-domain few-shot classifications.},
  archive      = {J_TPAMI},
  author       = {SuBeen Lee and WonJun Moon and Hyun Seok Seong and Jae-Pil Heo},
  doi          = {10.1109/TPAMI.2024.3504537},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1448-1463},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Task-oriented channel attention for fine-grained few-shot classification},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hi-SAM: Marrying segment anything model for hierarchical
text segmentation. <em>TPAMI</em>, <em>47</em>(3), 1431–1447. (<a
href="https://doi.org/10.1109/TPAMI.2024.3495831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Segment Anything Model (SAM), a profound vision foundation model pretrained on a large-scale dataset, breaks the boundaries of general segmentation and sparks various downstream applications. This paper introduces Hi-SAM, a unified model leveraging SAM for hierarchical text segmentation. Hi-SAM excels in segmentation across four hierarchies, including pixel-level text, word, text-line, and paragraph, while realizing layout analysis as well. Specifically, we first turn SAM into a high-quality pixel-level text segmentation (TS) model through a parameter-efficient fine-tuning approach. We use this TS model to iteratively generate the pixel-level text labels in a semi-automatical manner, unifying labels across the four text hierarchies in the HierText dataset. Subsequently, with these complete labels, we launch the end-to-end trainable Hi-SAM based on the TS architecture with a customized hierarchical mask decoder. During inference, Hi-SAM offers both automatic mask generation (AMG) mode and promptable segmentation (PS) mode. In the AMG mode, Hi-SAM segments pixel-level text foreground masks initially, then samples foreground points for hierarchical text mask generation and achieves layout analysis in passing. As for the PS mode, Hi-SAM provides word, text-line, and paragraph masks with a single point click. Experimental results show the state-of-the-art performance of our TS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU on TextSeg for pixel-level text segmentation. Moreover, compared to the previous specialist for joint hierarchical detection and layout analysis on HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on the text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis, requiring $20\times$ fewer training epochs.},
  archive      = {J_TPAMI},
  author       = {Maoyuan Ye and Jing Zhang and Juhua Liu and Chenyu Liu and Baocai Yin and Cong Liu and Bo Du and Dacheng Tao},
  doi          = {10.1109/TPAMI.2024.3495831},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1431-1447},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hi-SAM: Marrying segment anything model for hierarchical text segmentation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale part-based feature representation for 3D domain
generalization and adaptation. <em>TPAMI</em>, <em>47</em>(3),
1414–1430. (<a
href="https://doi.org/10.1109/TPAMI.2024.3496670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep networks for 3D point clouds have achieved remarkable success in classification task but remain vulnerable to geometric variations resulting from inconsistent data acquisition procedures. This leads to significant performance degradation when models trained on a source domain are tested on out-of-distribution target domains, highlighting the challenges of 3D domain generalization and adaptation. In this paper, we introduce a novel Multi-Scale Part-based feature Representation, dubbed MSPR, as a generalizable representation for point cloud domain generalization and adaptation. Rather than relying on global shape feature, we align the part-level features of shapes at different scales to a set of learnable part-template features that encode local geometric structures shared between the source and the target domains. Specifically, shapes from different domains are organized into part-level features at various scales and then aligned to the part-template features. To balance the generalization and discrimination abilities of parts at different scales, we further design a cross-scale feature fusion module to exchange information between aligned part-based features at different scales. The fused part-based representations are finally aggregated by a part-based feature aggregation module. To improve the robustness of the aligned part-based representations and global shape representation to geometry variations, we further propose a Contrastive Learning framework on Shape Representation (CLSR). Experiments are conducted on 3D domain generalization and adaptation benchmarks for point cloud classification. Extensive experiments on 3D domain generalization and adaptation benchmarks demonstrate that proposed approach outperforms previous state-of-the-art methods in both tasks. Ablation studies confirm the effectiveness of the components in our model.},
  archive      = {J_TPAMI},
  author       = {Xin Wei and Xiang Gu and Jian Sun},
  doi          = {10.1109/TPAMI.2024.3496670},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1414-1430},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-scale part-based feature representation for 3D domain generalization and adaptation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imaginary-connected embedding in complex space for unseen
attribute-object discrimination. <em>TPAMI</em>, <em>47</em>(3),
1395–1413. (<a
href="https://doi.org/10.1109/TPAMI.2024.3487631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions of seen primitives. Prior studies have attempted to either learn primitives individually (non-connected) or establish dependencies among them in the composition (fully-connected). In contrast, human comprehension of composition diverges from the aforementioned methods as humans possess the ability to make composition-aware adaptation for these primitives, instead of inferring them rigidly through the aforementioned methods. However, developing a comprehension of compositions akin to human cognition proves challenging within the confines of real space. This arises from the limitation of real-space-based methods, which often categorize attributes, objects, and compositions using three independent measures, without establishing a direct dynamic connection. To tackle this challenge, we expand the CZSL distance metric scheme to encompass complex spaces to unify the independent measures, and we establish an imaginary-connected embedding in complex space to model human understanding of attributes. To achieve this representation, we introduce an innovative visual bias-based attribute extraction module that selectively extracts attributes based on object prototypes. As a result, we are able to incorporate phase information in training and inference, serving as a metric for attribute-object dependencies while preserving the independent acquisition of primitives. We evaluate the effectiveness of our proposed approach on three benchmark datasets, illustrating its superiority compared to baseline methods.},
  archive      = {J_TPAMI},
  author       = {Chenyi Jiang and Shidong Wang and Yang Long and Zechao Li and Haofeng Zhang and Ling Shao},
  doi          = {10.1109/TPAMI.2024.3487631},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1395-1413},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Imaginary-connected embedding in complex space for unseen attribute-object discrimination},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast window-based event denoising with spatiotemporal
correlation enhancement. <em>TPAMI</em>, <em>47</em>(3), 1381–1394. (<a
href="https://doi.org/10.1109/TPAMI.2024.3467709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous deep learning-based event denoising methods mostly suffer from poor interpretability and difficulty in real-time processing due to their complex architecture designs. In this paper, we propose window-based event denoising, which simultaneously deals with a stack of events while existing element-based denoising focuses on one event each time. Besides, we give the theoretical analysis based on probability distributions in both temporal and spatial domains to improve interpretability. In temporal domain, we use timestamp deviations between processing events and central event to judge the temporal correlation and filter out temporal-irrelevant events. In spatial domain, we choose maximum a posteriori (MAP) to discriminate real-world event and noise and use the learned convolutional sparse coding to optimize the objective function. Based on the theoretical analysis, we build Temporal Window (TW) module and Soft Spatial Feature Embedding (SSFE) module to process temporal and spatial information separately, and construct a novel multi-scale window-based event denoising network, named WedNet. The high denoising accuracy and fast running speed of our WedNet enables us to achieve real-time denoising in complex scenes. Extensive experimental results verify the effectiveness and robustness of our WedNet. Our algorithm can remove event noise effectively and efficiently and improve the performance of downstream tasks.},
  archive      = {J_TPAMI},
  author       = {Huachen Fang and Jinjian Wu and Qibin Hou and Weisheng Dong and Guangming Shi},
  doi          = {10.1109/TPAMI.2024.3467709},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1381-1394},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast window-based event denoising with spatiotemporal correlation enhancement},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A versatile framework for multi-scene person
re-identification. <em>TPAMI</em>, <em>47</em>(3), 1362–1380. (<a
href="https://doi.org/10.1109/TPAMI.2024.3381184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-identification (ReID) has been extensively developed for a decade in order to learn the association of images of the same person across non-overlapping camera views. To overcome significant variations between images across camera views, mountains of variants of ReID models were developed for solving a number of challenges, such as resolution change, clothing change, occlusion, modality change, and so on. Despite the impressive performance of many ReID variants, these variants typically function distinctly and cannot be applied to other challenges. To our best knowledge, there is no versatile ReID model that can handle various ReID challenges at the same time. This work contributes to the first attempt at learning a versatile ReID model to solve such a problem. Our main idea is to form a two-stage prompt-based twin modeling framework called VersReID. Our VersReID firstly leverages the scene label to train a ReID Bank that contains abundant knowledge for handling various scenes, where several groups of scene-specific prompts are used to encode different scene-specific knowledge. In the second stage, we distill a V-Branch model with versatile prompts from the ReID Bank for adaptively solving the ReID of different scenes, eliminating the demand for scene labels during the inference stage. To facilitate training VersReID, we further introduce the multi-scene properties into self-supervised learning of ReID via a multi-scene prioris data augmentation (MPDA) strategy. Through extensive experiments, we demonstrate the success of learning an effective and versatile ReID model for handling ReID tasks under multi-scene conditions without manual assignment of scene labels in the inference stage, including general, low-resolution, clothing change, occlusion, and cross-modality scenes.},
  archive      = {J_TPAMI},
  author       = {Wei-Shi Zheng and Junkai Yan and Yi-Xing Peng},
  doi          = {10.1109/TPAMI.2024.3381184},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1362-1380},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A versatile framework for multi-scene person re-identification},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for real-world super-resolution
from dual and multiple zoomed observations. <em>TPAMI</em>,
<em>47</em>(3), 1348–1361. (<a
href="https://doi.org/10.1109/TPAMI.2024.3379736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we propose a two-stage alignment method, including patch-based optical flow alignment and auxiliary-LR guiding alignment. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss. Furthermore, we take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Zhilu Zhang and Ruohao Wang and Hongzhi Zhang and Wangmeng Zuo},
  doi          = {10.1109/TPAMI.2024.3379736},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1348-1361},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised learning for real-world super-resolution from dual and multiple zoomed observations},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global model selection via solution paths for robust support
vector machine. <em>TPAMI</em>, <em>47</em>(3), 1331–1347. (<a
href="https://doi.org/10.1109/TPAMI.2023.3346765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust support vector machine (RSVM) using ramp loss provides a better generalization performance than traditional support vector machine (SVM) using hinge loss. However, the good performance of RSVM heavily depends on the proper values of regularization parameter and ramp parameter. Traditional model selection technique with gird search has extremely high computational cost especially for fine-grained search. To address this challenging problem, in this paper, we first propose solution paths of RSVM (SPRSVM) based on the concave-convex procedure (CCCP) which can track the solutions of the non-convex RSVM with respect to regularization parameter and ramp parameter respectively. Specifically, we use incremental and decremental learning algorithms to deal with the Karush-Khun-Tucker violating samples in the process of tracking the solutions. Based on the solution paths of RSVM and the piecewise linearity of model function, we can compute the error paths of RSVM and find the values of regularization parameter and ramp parameter, respectively, which corresponds to the minimum cross validation error. We prove the finite convergence of SPRSVM and analyze the computational complexity of SPRSVM. Experimental results on a variety of benchmark datasets not only verify that our SPRSVM can globally search the regularization and ramp parameters respectively, but also show a huge reduction of computational time compared with the grid search approach.},
  archive      = {J_TPAMI},
  author       = {Zhou Zhai and Bin Gu and Cheng Deng and Heng Huang},
  doi          = {10.1109/TPAMI.2023.3346765},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1331-1347},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Global model selection via solution paths for robust support vector machine},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
