<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tit---43">TIT - 43</h2>
<ul>
<li><details>
<summary>
(2025). Convolution type of metaplectic cohen’s distribution
time-frequency analysis theory, method and technology. <em>TIT</em>,
<em>71</em>(3), 2292–2314. (<a
href="https://doi.org/10.1109/TIT.2024.3522079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional Cohen’s distribution can’t meet the requirement of additive noises jamming signals high-performance denoising under the condition of low signal-to-noise ratio, it is necessary to integrate the metaplectic transform for non-stationary signal fractional domain time-frequency analysis. In this paper, we embark on blending time-frequency operators and coordinate operator fractionizations to formulate the definition of the joint fractionizations metaplectic Wigner distribution (JFMWD), based on which we integrate the generalized metaplectic convolution to address the unified representation issue of the convolution type of metaplectic Cohen’s distribution (CMCD), whose special cases and essential properties are also derived. We blend Wiener filter principle and fractional domain filter mechanism of the metaplectic transform to design the least-squares adaptive filter method in the JFMWD domain, giving birth to the least-squares adaptive filter-based CMCD whose kernel function can be adjusted with the input signal automatically to achieve the minimum mean-square error (MSE) denoising in Wigner distribution domain. We discuss the optimal symplectic matrices selection strategy of the proposed adaptive CMCD through the minimum MSE minimization modeling and solving. Some examples are also carried out to demonstrate that the proposed filtering method outperforms some state-of-the-arts including the classical Gaussian filter, the Cohen’s distribution filtering methods with fixed kernel functions, the classical Wiener filter, the adaptive generalized linear canonical convolution-based filtering method, the adaptive Cohen’s distribution filtering method, the adaptive JFMWD-based Cohen’s distribution filtering method, and the adaptive generalized metaplectic convolution-based Cohen’s distribution filtering method in noise suppression.},
  archive      = {J_TIT},
  author       = {Manjun Cui and Zhichao Zhang and Jie Han and Yunjie Chen and Chunzheng Cao},
  doi          = {10.1109/TIT.2024.3522079},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2292-2314},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Convolution type of metaplectic cohen’s distribution time-frequency analysis theory, method and technology},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi complementary sequence sets: New bounds and optimal
constructions via quasi-florentine rectangles. <em>TIT</em>,
<em>71</em>(3), 2271–2291. (<a
href="https://doi.org/10.1109/TIT.2025.3528056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi complementary sequence sets (QCSSs) are important in modern communication systems as they are capable of supporting more users, which is desired in applications like MC-CDMA nowadays. In this paper, we first derive a tighter bound on the maximum aperiodic correlation among all constituent complementary sequence sets in QCSSs. By proposing a new combinatorial structure called quasi-Florentine rectangles, we obtain a new construction of QCSSs with large set sizes. Using Butson-type Hadamard matrices and quasi-Florentine rectangles, we propose another construction which can construct QCSSs with flexible parameters over any given alphabet size, including small alphabets. All the proposed sequences are optimal with respect to the newly proposed bound. Also, through some of the constructions, the column sequence PMEPR of the proposed QCSSs are upper bounded by 2.},
  archive      = {J_TIT},
  author       = {Avik Ranjan Adhikary and Hui Zhang and Zhengchun Zhou and Qi Wang and Sihem Mesnager},
  doi          = {10.1109/TIT.2025.3528056},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2271-2291},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Quasi complementary sequence sets: New bounds and optimal constructions via quasi-florentine rectangles},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal constant-weight and mixed-weight conflict-avoiding
codes. <em>TIT</em>, <em>71</em>(3), 2257–2270. (<a
href="https://doi.org/10.1109/TIT.2025.3528132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A conflict-avoiding code (CAC) is a deterministic transmission scheme for asynchronous multiple access without feedback. When the number of simultaneously active users is less than or equal to w, a CAC of length L with weight w can provide a hard guarantee that each active user has at least one successful transmission within every consecutive L slots. In this paper, we generalize some previously known constructions of constant-weight CACs, and then derive several classes of optimal CACs by the help of Kneser’s Theorem and some techniques in Additive Combinatorics. Another spotlight of this paper is to relax the identical-weight constraint in prior studies to study mixed-weight CACs for the first time, for the purpose of increasing the throughput and reducing the access delay of some potential users with higher priority. As applications of those obtained optimal CACs, we derive some classes of optimal mixed-weight CACs.},
  archive      = {J_TIT},
  author       = {Yuan-Hsun Lo and Tsai-Lien Wong and Kangkang Xu and Yijin Zhang},
  doi          = {10.1109/TIT.2025.3528132},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2257-2270},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Optimal constant-weight and mixed-weight conflict-avoiding codes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximizing weighted energy efficiency over parallel gaussian
broadcast channels. <em>TIT</em>, <em>71</em>(3), 2245–2256. (<a
href="https://doi.org/10.1109/TIT.2025.3527887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A power assignment over parallel Gaussian broadcast channels splits a power budget at the access point among all channel-user pairs subject to per-channel upper-bounds on the sum-power, and yields a rate allocation to all channel-user pairs. Its weighted energy efficiency (WEE) is the ratio of its weighted sum-rate over its sum-power plus a fixed positive overhead. The problem Max-WEE seeks a power assignment maximizing the WEE. Special variants of Max-WEE with unit weights or two users per channel have been extensively studied in the literature. But none of the existing algorithms for those special variants have known bounds on running time, mainly because they follow the general-purposed methods for fractional programming. In this paper, we first derive fundamental properties and closed-form expressions of maximum WEE. Then we devise a simple water-filling algorithm for Max-WEE. Assuming all users are presorted by weight, the water-filling algorithm has linear complexity in the number of channel-user pairs. Under a mild presorting condition, we further develop a linear-complexity algorithm for Max-WEE subject to rate demand.},
  archive      = {J_TIT},
  author       = {Peng-Jun Wan and Pengpeng Chen},
  doi          = {10.1109/TIT.2025.3527887},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2245-2256},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Maximizing weighted energy efficiency over parallel gaussian broadcast channels},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grouping-based cyclic scheduling under age of correlated
information constraints. <em>TIT</em>, <em>71</em>(3), 2218–2244. (<a
href="https://doi.org/10.1109/TIT.2025.3529497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies an internet of things (IoT) network where a fusion center relies on multi-view and correlated information generated by multiple sources to monitor various regions. Each region possesses hard age of correlated information (AoCI) constraints for information update, and accordingly we propose a scheduling policy to satisfy such needs and minimize the required wireless resources. We first approximate the problem to a dual bin-packing problem. Secondly, efficient scheduling policies are identified when the age constraints possess special mathematical properties, where the number of channels at most required is analyzed. Optimality conditions of the proposed policies are presented. For general constraints, a two-step grouping algorithm for multi-view (TGAM) is proposed to establish scheduling policies. Under TGAM, the constraints are mapped into a combination of the special constraints. To quickly identify an optimized mapping from a vast solution space, TGAM heuristically groups the regions according to their constraints and then searches for the optimal mapping for each group. Numerical results demonstrate that, compared to a derived lower bound, the proposed TGAM requires only 1.07% more channels. Additionally, the number of regions that can be served by TGAM is significantly larger than the state-of-the art algorithm, given the number of channels.},
  archive      = {J_TIT},
  author       = {Lehan Wang and Jingzhou Sun and Yuxuan Sun and Sheng Zhou and Zhisheng Niu and Miao Jiang and Lu Geng},
  doi          = {10.1109/TIT.2025.3529497},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2218-2244},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Grouping-based cyclic scheduling under age of correlated information constraints},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coded distributed computing with pre-set data placement and
output functions assignment. <em>TIT</em>, <em>71</em>(3), 2195–2217.
(<a href="https://doi.org/10.1109/TIT.2025.3528083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coded distributed computing can reduce the communication load for distributed computing systems by introducing redundant computation and creating multicasting opportunities. However, the existing schemes require delicate data placement and output function assignment, which is not feasible when distributed nodes fetch data without the orchestration of a central server. In this paper, we consider the general systems where the data placement and output function assignment are arbitrary but pre-set. We propose two coded computing schemes, One-shot Coded Transmission (OSCT) and Few-shot Coded Transmission (FSCT), to reduce the communication load. Both schemes first group the nodes into clusters and divide the transmission of each cluster into multiple rounds, and then design coded transmission in each round to maximize the multicast gain. The key difference between OSCT and FSCT is that the former uses a one-shot transmission where each encoded message can be decoded independently by the intended nodes, while the latter allows each node to jointly decode multiple received symbols to achieve potentially larger multicast gains. Furthermore, based on the lower bound proposed by Yu et al., we derive sufficient conditions for the optimality of OSCT and FSCT, respectively. This not only recovers the existing optimality results but also includes some cases where our schemes are optimal while others are not.},
  archive      = {J_TIT},
  author       = {Yuhan Wang and Youlong Wu},
  doi          = {10.1109/TIT.2025.3528083},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2195-2217},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Coded distributed computing with pre-set data placement and output functions assignment},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized fractional repetition codes for binary coded
computations. <em>TIT</em>, <em>71</em>(3), 2170–2194. (<a
href="https://doi.org/10.1109/TIT.2025.3529680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the gradient coding and coded matrix multiplication problems in distributed optimization and coded computing. We present a computationally efficient coding method which overcomes the drawbacks of the Fractional Repetition Coding gradient coding method proposed by Tandon et al., and can also be leveraged by coded computing networks whose servers are of heterogeneous nature. Specifically, we propose a construction for fractional repetition gradient coding; while ensuring that the generator matrix remains close to perfectly balanced for any set of coding parameters, as well as a low complexity decoding step. The proposed binary encoding avoids operations over the real and complex numbers which inherently introduce numerical and rounding errors, thereby enabling accurate distributed encodings of the partial gradients. We then make connections between gradient coding and coded matrix multiplication. Specifically, we show that any gradient coding scheme can be extended to coded matrix multiplication. Furthermore, we show how the proposed binary gradient coding scheme can be used to construct two different coded matrix multiplication schemes, each achieving different trade-offs.},
  archive      = {J_TIT},
  author       = {Neophytos Charalambides and Hessam Mahdavifar and Alfred O. Hero},
  doi          = {10.1109/TIT.2025.3529680},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2170-2194},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Generalized fractional repetition codes for binary coded computations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covert communication over additive-noise channels.
<em>TIT</em>, <em>71</em>(3), 2157–2169. (<a
href="https://doi.org/10.1109/TIT.2024.3522151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the fundamental limits of covert communications over general memoryless additive-noise channels. We assume that the legitimate receiver and the eavesdropper share the same channel and therefore see the same outputs. Under mild integrability assumptions, we find a general upper bound on the square-root scaling constant, which only involves the variance of the logarithm of the probability density function of the noise. Furthermore, we show that, under some additional assumptions, this upper bound is tight. We also provide upper bounds on the length of the secret key required to achieve the optimal scaling.},
  archive      = {J_TIT},
  author       = {Cécile Bouette and Laura Luzzi and Ligong Wang},
  doi          = {10.1109/TIT.2024.3522151},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2157-2169},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Covert communication over additive-noise channels},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Private noisy side information helps to increase the
capacity of SPIR. <em>TIT</em>, <em>71</em>(3), 2140–2156. (<a
href="https://doi.org/10.1109/TIT.2025.3530400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noiseless private side information does not reduce the download cost in Symmetric Private Information Retrieval (SPIR) unless the client knows all but one file. While this is a pessimistic result, we explore in this paper whether noisy client side information available at the client helps decrease the download cost in the context of SPIR with colluding and replicated servers. Specifically, we assume that the client possesses noisy side information about each stored file, which is obtained by passing each file through one of D possible discrete memoryless test channels. The statistics of the test channels are known by the client and by all the servers, but the mapping $\boldsymbol {\mathcal {M}}$ between the files and the test channels is unknown to the servers. We study this problem under two privacy metrics. Under the first metric, the client wants to preserve the privacy of its file selection and the mapping $\boldsymbol {\mathcal {M}}$ , and the servers want to preserve the privacy of all the non-selected files. Under the second metric, the client is willing to reveal the index of the test channel that is associated with its desired file. For both privacy metrics, we derive the optimal common randomness and download cost. Our setup generalizes SPIR with colluding servers and SPIR with private noiseless side information. Unlike noiseless side information, our results demonstrate that noisy side information can reduce the download cost, even when the client does not have noiseless knowledge of all but one file.},
  archive      = {J_TIT},
  author       = {Hassan ZivariFard and Rémi A. Chou and Xiaodong Wang},
  doi          = {10.1109/TIT.2025.3530400},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2140-2156},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Private noisy side information helps to increase the capacity of SPIR},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework for clustering and distribution matching
with bandit feedback. <em>TIT</em>, <em>71</em>(3), 2116–2139. (<a
href="https://doi.org/10.1109/TIT.2025.3528655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a general framework for clustering and distribution matching problems with bandit feedback. We consider a K-armed bandit model where some subset of K arms is partitioned into M groups. Within each group, the random variable associated to each arm follows the same distribution on a finite alphabet. At each time step, the decision maker pulls an arm and observes its outcome from the random variable associated to that arm. Subsequent arm pulls depend on the history of arm pulls and their outcomes. The decision maker has no knowledge of the distributions of the arms or the underlying partitions. The task is to devise an online algorithm to learn the underlying partition of arms with the least number of arm pulls on average and with an error probability not exceeding a pre-determined value $\delta $ . Several existing problems fall under our general framework, including finding M pairs of arms, odd arm identification, and N-ary clustering of K arms belong to our general framework. We derive a non-asymptotic lower bound on the average number of arm pulls for any online algorithm with an error probability not exceeding $\delta $ . Furthermore, we develop a computationally-efficient online algorithm based on the Track-and-Stop method and Frank-Wolfe algorithm, and show that the average number of arm pulls of our algorithm asymptotically matches that of the lower bound. Our refined analysis also uncovers a novel bound on the speed at which the average number of arm pulls of our algorithm converges to the fundamental limit as $\delta $ vanishes.},
  archive      = {J_TIT},
  author       = {Recep Can Yavas and Yuqi Huang and Vincent Y. F. Tan and Jonathan Scarlett},
  doi          = {10.1109/TIT.2025.3528655},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2116-2139},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {A general framework for clustering and distribution matching with bandit feedback},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Divergence maximizing linear projection for supervised
dimension reduction. <em>TIT</em>, <em>71</em>(3), 2104–2115. (<a
href="https://doi.org/10.1109/TIT.2025.3528340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes two linear projection methods for supervised dimension reduction using only first- and second-order statistics. The methods, each catering to a different parameter regime, are derived under the general Gaussian model by maximizing the Kullback-Leibler divergence between the two classes in the projected sample for a binary classification problem. They subsume existing linear projection approaches developed under simplifying assumptions of Gaussian distributions, such as these distributions might share an equal mean or covariance matrix. As a by-product, we establish that the multi-class linear discriminant analysis, a celebrated method for classification and supervised dimension reduction, is provably optimal for maximizing pairwise Kullback-Leibler divergence when the Gaussian populations share an identical covariance matrix. For the case when the Gaussian distributions share an equal mean, we establish conditions under which the optimal subspace remains invariant regardless of how the Kullback-Leibler divergence is defined, despite the asymmetry of the divergence measure itself. Such conditions encompass the classical case of signal plus noise, where both signal and noise have zero mean and arbitrary covariance matrices. Experiments are conducted to validate the proposed solutions, demonstrate their superior performance over existing alternatives, and illustrate the procedure for selecting the appropriate linear projection solution.},
  archive      = {J_TIT},
  author       = {Biao Chen and Joshua Kortje},
  doi          = {10.1109/TIT.2025.3528340},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2104-2115},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Divergence maximizing linear projection for supervised dimension reduction},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal estimation of the null distribution in large-scale
inference. <em>TIT</em>, <em>71</em>(3), 2075–2103. (<a
href="https://doi.org/10.1109/TIT.2025.3529457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of large-scale inference has spurred reexamination of conventional statistical thinking. In a series of highly original articles, Efron persuasively illustrated the danger for downstream inference in assuming the veracity of a posited null distribution. In a Gaussian model for n many z-scores with at most $k \lt \frac {n}{2}$ nonnulls, Efron suggests estimating the parameters of an empirical null $N(\theta , \sigma ^{2})$ instead of assuming the theoretical null $N(0, 1)$ . Looking to the robust statistics literature by viewing the nonnulls as outliers is unsatisfactory as the question of optimal rates is still open; even consistency is not known in the regime $k \asymp n$ which is especially relevant to many large-scale inference applications. However, provably rate-optimal robust estimators have been developed in other models (e.g. Huber contamination) which appear quite close to Efron’s proposal. Notably, the impossibility of consistency when $k \asymp n$ in these other models may suggest the same major weakness afflicts Efron’s popularly adopted recommendation. A sound evaluation thus requires a complete understanding of information-theoretic limits. We characterize the regime of k for which consistent estimation is possible, notably without imposing any assumptions at all on the nonnull effects. Unlike in other robust models, it is shown consistent estimation of the location parameter is possible if and only if $\frac {n}{2} {-} k = \omega (\sqrt {n})$ , and of the scale parameter in the entire regime $k \lt \frac {n}{2}$ . Furthermore, we establish sharp minimax rates and show estimators based on the empirical characteristic function are optimal by exploiting the Gaussian character of the data.},
  archive      = {J_TIT},
  author       = {Subhodh Kotekal and Chao Gao},
  doi          = {10.1109/TIT.2025.3529457},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2075-2103},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Optimal estimation of the null distribution in large-scale inference},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient solvers for wyner common information with
application to multi-modal clustering. <em>TIT</em>, <em>71</em>(3),
2054–2074. (<a href="https://doi.org/10.1109/TIT.2025.3532280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose computationally efficient solvers for novel extensions of Wyner common information. By separating information sources into bipartite, the proposed Bipartite common information framework has difference-of-convex structure for efficient non-convex optimization. In known joint distribution cases, our difference-of-convex algorithm(DCA)-based solver has a provable convergence guarantee to local stationary points. As for unknown distribution settings, the insights from DCA combined with the exponential family of distributions for parameterization allows for closed-form expressions for efficient estimation. Furthermore, we show that the Bipartite common information applies to multi-modal clustering without employing ad-hoc clustering algorithms. Empirically, our solvers outperform state-of-the-art methods in clustering accuracy and running time over a range of non-trivial multi-modal clustering datasets with different number of data modalities.},
  archive      = {J_TIT},
  author       = {Teng-Hui Huang and Hesham El Gamal},
  doi          = {10.1109/TIT.2025.3532280},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2054-2074},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Efficient solvers for wyner common information with application to multi-modal clustering},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How does distribution matching help domain generalization:
An information-theoretic analysis. <em>TIT</em>, <em>71</em>(3),
2028–2053. (<a href="https://doi.org/10.1109/TIT.2025.3531136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization aims to learn invariance across multiple source domains, thereby enhancing generalization against out-of-distribution data. While gradient or representation matching algorithms have achieved remarkable success in domain generalization, these methods generally lack generalization guarantees or depend on strong assumptions, leaving a gap in understanding the underlying mechanism of distribution matching. In this work, we formulate domain generalization from a novel probabilistic perspective, ensuring robustness while avoiding overly conservative solutions. Through comprehensive information-theoretic analysis, we provide key insights into the roles of gradient and representation matching in promoting generalization. Our results reveal the complementary relationship between these two components, indicating that existing works focusing solely on either gradient or representation alignment are insufficient to solve the domain generalization problem. In light of these theoretical findings, we introduce IDM to simultaneously align the inter-domain gradients and representations. Integrated with the proposed PDM method for complex distribution matching, IDM achieves superior performance over various baseline methods.},
  archive      = {J_TIT},
  author       = {Yuxin Dong and Tieliang Gong and Hong Chen and Shuangyong Song and Weizhan Zhang and Chen Li},
  doi          = {10.1109/TIT.2025.3531136},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2028-2053},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {How does distribution matching help domain generalization: An information-theoretic analysis},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian cramér-rao bound estimation with score-based
models. <em>TIT</em>, <em>71</em>(3), 2007–2027. (<a
href="https://doi.org/10.1109/TIT.2024.3447552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bayesian Cramér-Rao bound (CRB) provides a lower bound on the mean square error of any Bayesian estimator under mild regularity conditions. It can be used to benchmark the performance of statistical estimators, and provides a principled metric for system design and optimization. However, the Bayesian CRB depends on the underlying prior distribution, which is often unknown for many problems of interest. This work introduces a new data-driven estimator for the Bayesian CRB using score matching, i.e., a statistical estimation technique that models the gradient of a probability distribution from a given set of training data. The performance of the proposed estimator is analyzed in both the classical parametric modeling regime and the neural network modeling regime. In both settings, we develop novel non-asymptotic bounds on the score matching error and our Bayesian CRB estimator based on the results from empirical process theory, including classical bounds and recently introduced techniques for characterizing neural networks. We illustrate the performance of the proposed estimator with two application examples: a signal denoising problem and a dynamic phase offset estimation problem with applications in communication systems.},
  archive      = {J_TIT},
  author       = {Evan Scope Crafts and Xianyang Zhang and Bo Zhao},
  doi          = {10.1109/TIT.2024.3447552},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {2007-2027},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Bayesian cramér-rao bound estimation with score-based models},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist model averaging for global fréchet regression.
<em>TIT</em>, <em>71</em>(3), 1994–2006. (<a
href="https://doi.org/10.1109/TIT.2024.3520979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To consider model uncertainty in global Fréchet regression and improve density response prediction, we propose a frequentist model averaging method. The weights are chosen by minimizing a cross-validation criterion based on Wasserstein distance. In the cases where all candidate models are misspecified, we prove that the corresponding model averaging estimator has asymptotic optimality, achieving the lowest possible Wasserstein distance. When there are correctly specified candidate models, we prove that our method asymptotically assigns all weights to the correctly specified models. Numerical results of extensive simulations and a real data analysis on intracerebral hemorrhage data strongly favour our method.},
  archive      = {J_TIT},
  author       = {Xingyu Yan and Xinyu Zhang and Peng Zhao},
  doi          = {10.1109/TIT.2024.3520979},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1994-2006},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Frequentist model averaging for global fréchet regression},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization performance of empirical risk minimization on
over-parameterized deep ReLU nets. <em>TIT</em>, <em>71</em>(3),
1978–1993. (<a href="https://doi.org/10.1109/TIT.2025.3531048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the generalization performance of global minima of empirical risk minimization (ERM) on over-parameterized deep ReLU nets. Using a novel deepening scheme for deep ReLU nets, we rigorously prove that there exist perfect global minima achieving optimal generalization error rates for numerous types of data under mild conditions. Since over-parameterization of deep ReLU nets is crucial to guarantee that the global minima of ERM can be realized by the widely used stochastic gradient descent (SGD) algorithm, our results present a potential way to fill the gap between optimization and generalization of deep learning.},
  archive      = {J_TIT},
  author       = {Shao-Bo Lin and Yao Wang and Ding-Xuan Zhou},
  doi          = {10.1109/TIT.2025.3531048},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1978-1993},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Generalization performance of empirical risk minimization on over-parameterized deep ReLU nets},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overparameterized ReLU neural networks learn the simplest
model: Neural isometry and phase transitions. <em>TIT</em>,
<em>71</em>(3), 1926–1977. (<a
href="https://doi.org/10.1109/TIT.2025.3530355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practice of deep learning has shown that neural networks generalize remarkably well even with an extreme number of learned parameters. This appears to contradict traditional statistical wisdom, in which a trade-off between model complexity and fit to the data is essential. We aim to address this discrepancy by adopting a convex optimization and sparse recovery perspective. We consider the training and generalization properties of two-layer ReLU networks with standard weight decay regularization. Under certain regularity assumptions on the data, we show that ReLU networks with an arbitrary number of parameters learn only simple models that explain the data. This is analogous to the recovery of the sparsest linear model in compressed sensing. For ReLU networks and their variants with skip connections or normalization layers, we present isometry conditions that ensure the exact recovery of planted neurons. For randomly generated data, we show the existence of a phase transition in recovering planted neural network models, which is easy to describe: whenever the ratio between the number of samples and the dimension exceeds a numerical threshold, the recovery succeeds with high probability; otherwise, it fails with high probability. Surprisingly, ReLU networks learn simple and sparse models that generalize well even when the labels are noisy. The phase transition phenomenon is confirmed through numerical experiments.},
  archive      = {J_TIT},
  author       = {Yifei Wang and Yixuan Hua and Emmanuel J. Candès and Mert Pilanci},
  doi          = {10.1109/TIT.2025.3530355},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1926-1977},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Overparameterized ReLU neural networks learn the simplest model: Neural isometry and phase transitions},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On penalization in stochastic multi-armed bandits.
<em>TIT</em>, <em>71</em>(3), 1909–1925. (<a
href="https://doi.org/10.1109/TIT.2025.3525666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an important variant of the stochastic multi-armed bandit (MAB) problem, which takes penalization into consideration. Instead of directly maximizing cumulative expected reward, we need to balance between the total reward and fairness level. In this paper, we present some new insights into MAB and formulate the problem in the penalization framework, where a rigorous penalized regret can be well defined and a more sophisticated regret analysis is possible. Under such a framework, we propose a hard-threshold UCB-like algorithm, which enjoys many merits including the asymptotic fairness, nearly optimal regret, good tradeoff between reward and fairness. Both gap-dependent and gap-independent regret bounds have been established. Multiple insightful comments are given to illustrate the soundness of our theoretical analysis. Numerous experimental results corroborate the theory and show the usefulness of our formulation of the problem and our method to solve it.},
  archive      = {J_TIT},
  author       = {Guanhua Fang and Ping Li and Gennady Samorodnitsky},
  doi          = {10.1109/TIT.2025.3525666},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1909-1925},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {On penalization in stochastic multi-armed bandits},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuity of entropies via integral representations.
<em>TIT</em>, <em>71</em>(3), 1896–1908. (<a
href="https://doi.org/10.1109/TIT.2025.3527858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that Frenkel’s integral representation of the quantum relative entropy provides a natural framework to derive continuity bounds for quantum information measures. Our main general result is a dimension-independent semi-continuity relation for the quantum relative entropy with respect to the first argument. Using it, we obtain a number of results: 1) a tight continuity relation for the conditional entropy in the case where the two states have equal marginals on the conditioning system, resolving a conjecture by Wilde in this special case; 2) a stronger version of the Fannes-Audenaert inequality on quantum entropy; 3) better estimates on the quantum capacity of approximately degradable channels; 4) an improved continuity relation for the entanglement cost; 5) general upper bounds on asymptotic transformation rates in infinite-dimensional entanglement theory; and 6) a proof of a conjecture due to Christandl, Ferrara, and Lancien on the continuity of ‘filtered’ relative entropy distances.},
  archive      = {J_TIT},
  author       = {Mario Berta and Ludovico Lami and Marco Tomamichel},
  doi          = {10.1109/TIT.2025.3527858},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1896-1908},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Continuity of entropies via integral representations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error exponents for entanglement transformations from
degenerations. <em>TIT</em>, <em>71</em>(3), 1874–1895. (<a
href="https://doi.org/10.1109/TIT.2025.3534327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the trade-off relation between the rate and the strong converse exponent for asymptotic LOCC transformations between pure multipartite states. Any single-copy probabilistic transformation between a pair of states implies that an asymptotic transformation at rate 1 is possible with an exponentially decreasing success probability. However, it is possible that an asymptotic transformation is feasible with nonzero probability, but there is no transformation between any finite number of copies with the same rate, even probabilistically. In such cases it is not known if the optimal success probability decreases exponentially or faster. A fundamental tool for showing the feasibility of an asymptotic transformation is degeneration. Any degeneration gives rise to a sequence of stochastic LOCC transformations from copies of the initial state plus a sublinear number of GHZ states to the same number of copies of the target state. These protocols involve parameters that can be freely chosen, but the choice affects the success probability. In this paper, we characterize an asymptotically optimal choice of the parameters and derive a single-letter expression for the error exponent of the resulting protocol. In particular, this implies an exponential lower bound on the success probability when the stochastic transformation arises from a degeneration.},
  archive      = {J_TIT},
  author       = {Dávid Bugár and Péter Vrana},
  doi          = {10.1109/TIT.2025.3534327},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1874-1895},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Error exponents for entanglement transformations from degenerations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contraction of private quantum channels and private quantum
hypothesis testing. <em>TIT</em>, <em>71</em>(3), 1851–1873. (<a
href="https://doi.org/10.1109/TIT.2025.3527859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A quantum generalized divergence by definition satisfies the data-processing inequality; as such, the relative decrease in such a divergence under the action of a quantum channel is at most one. This relative decrease is formally known as the contraction coefficient of the channel and the divergence. Interestingly, there exist combinations of channels and divergences for which the contraction coefficient is strictly less than one. Furthermore, understanding the contraction coefficient is fundamental for the study of statistical tasks under privacy constraints. To this end, here we establish upper bounds on contraction coefficients for the hockey-stick divergence under privacy constraints, where privacy is quantified with respect to the quantum local differential privacy (QLDP) framework, and we fully characterize the contraction coefficient for the trace distance under privacy constraints. Using the machinery developed, we also determine an upper bound on the contraction of both the Bures distance and quantum relative entropy relative to the normalized trace distance, under QLDP constraints. Next, we apply our findings to establish bounds on the sample complexity of quantum hypothesis testing under privacy constraints. Furthermore, we study various scenarios in which the sample complexity bounds are tight while providing order-optimal quantum channels that achieve those bounds. Lastly, we show how private quantum channels provide fairness and Holevo information stability in quantum learning settings.},
  archive      = {J_TIT},
  author       = {Theshani Nuradha and Mark M. Wilde},
  doi          = {10.1109/TIT.2025.3527859},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1851-1873},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Contraction of private quantum channels and private quantum hypothesis testing},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multipartite entanglement theory with
entanglement-nonincreasing operations. <em>TIT</em>, <em>71</em>(3),
1841–1850. (<a href="https://doi.org/10.1109/TIT.2025.3526419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key problem in quantum information science is to determine optimal protocols for the interconversion of entangled states shared between remote parties. While for two parties a large number of results in this direction is available, the multipartite setting still remains a major challenge. In this article, this problem is addressed by extending the resource theory of entanglement for multipartite systems beyond the standard framework of local operations and classical communication. Specifically, we consider transformations capable of introducing a small, controllable increase of entanglement of a state, with the requirement that the increase can be made arbitrarily small. We demonstrate that in this adjusted framework, the transformation rates between multipartite states are fundamentally dictated by the bipartite entanglement entropies of the respective quantum states. Remarkably, this approach allows the reduction of tripartite entanglement to its bipartite analog, indicating that every pure tripartite state can be reversibly synthesized from a suitable number of singlets distributed between pairs of parties.},
  archive      = {J_TIT},
  author       = {Alexander Streltsov},
  doi          = {10.1109/TIT.2025.3526419},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1841-1850},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Multipartite entanglement theory with entanglement-nonincreasing operations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized quantum data-syndrome codes and belief
propagation decoding for phenomenological noise. <em>TIT</em>,
<em>71</em>(3), 1824–1840. (<a
href="https://doi.org/10.1109/TIT.2025.3529773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum stabilizer codes often struggle with syndrome errors due to measurement imperfections. Typically, multiple rounds of syndrome extraction are employed to ensure reliable error information. In this paper, we consider phenomenological decoding problems, where data qubit errors may occur between extractions, and each measurement can be faulty. We introduce generalized quantum data-syndrome codes along with a generalized check matrix that integrates both quaternary and binary alphabets to represent diverse error sources. This results in a Tanner graph with mixed variable nodes, enabling the design of belief propagation (BP) decoding algorithms that effectively handle phenomenological errors. Importantly, our BP decoders are applicable to general sparse quantum codes. Through simulations, we achieve an error threshold of more than 3% for quantum memory protected by rotated toric codes, using solely BP without post-processing. Our results indicate that d rounds of syndrome extraction are sufficient for a toric code of distance d. We observe that at high error rates, fewer rounds of syndrome extraction tend to perform better, while more rounds improve performance at lower error rates. Additionally, we propose a method to construct effective redundant stabilizer checks for single-shot error correction. Our simulations show that BP decoding remains highly effective even with a high syndrome error rate.},
  archive      = {J_TIT},
  author       = {Kao-Yueh Kuo and Ching-Yi Lai},
  doi          = {10.1109/TIT.2025.3529773},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1824-1840},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Generalized quantum data-syndrome codes and belief propagation decoding for phenomenological noise},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage solution to quantum process tomography: Error
analysis and optimal design. <em>TIT</em>, <em>71</em>(3), 1803–1823.
(<a href="https://doi.org/10.1109/TIT.2024.3522005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum process tomography is a critical task for characterizing the dynamics of quantum systems and achieving precise quantum control. In this paper, we propose a two-stage solution for both trace-preserving and non-trace-preserving quantum process tomography. Utilizing a tensor structure, our algorithm exhibits a computational complexity of $O(MLd^{2})$ where d is the dimension of the quantum system and $M, L~(M\geq d^{2}, L\geq d^{2})$ represent the numbers of different input states and measurement operators, respectively. We establish an analytical error upper bound and then design the optimal input states and the optimal measurement operators, which are both based on minimizing the error upper bound and maximizing the robustness characterized by the condition number. Numerical examples and testing on IBM quantum devices are presented to demonstrate the performance and efficiency of our algorithm.},
  archive      = {J_TIT},
  author       = {Shuixin Xiao and Yuanlong Wang and Jun Zhang and Daoyi Dong and Gary J. Mooney and Ian R. Petersen and Hidehiro Yonezawa},
  doi          = {10.1109/TIT.2024.3522005},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1803-1823},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {A two-stage solution to quantum process tomography: Error analysis and optimal design},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bounds and constructions of quantum locally recoverable
codes from quantum CSS codes. <em>TIT</em>, <em>71</em>(3), 1794–1802.
(<a href="https://doi.org/10.1109/TIT.2025.3533494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical locally recoverable codes (LRCs) have become indispensable in distributed storage systems. They provide efficient recovery in terms of localized errors. Quantum LRCs have very recently been introduced for their potential application in quantum data storage. In this paper, we use classical LRCs to investigate quantum LRCs. We prove that the parameters of quantum LRCs are bounded by their classical counterparts. We deduce bounds on the parameters of quantum LRCs from bounds on the parameters of the classical ones. We establish a characterization of optimal pure quantum LRCs based on classical codes with specific properties. Using well-crafted classical LRCs as ingredients in the construction of quantum CSS codes, we offer the first construction of several families of optimal pure quantum LRCs.},
  archive      = {J_TIT},
  author       = {Gaojun Luo and Bocong Chen and Martianus Frederic Ezerman and San Ling},
  doi          = {10.1109/TIT.2025.3533494},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1794-1802},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Bounds and constructions of quantum locally recoverable codes from quantum CSS codes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electromagnetic information theory-based statistical channel
model for improved channel estimation. <em>TIT</em>, <em>71</em>(3),
1777–1793. (<a href="https://doi.org/10.1109/TIT.2025.3526689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electromagnetic information theory (EIT) is an emerging interdisciplinary subject that integrates classical Maxwell electromagnetics and Shannon information theory. The goal of EIT is to uncover the information transmission mechanisms from an electromagnetic (EM) perspective in wireless systems. Existing works on EIT are mainly focused on the analysis of EM channel characteristics, degrees-of-freedom, and system capacity. However, these works do not clarify how to integrate EIT knowledge into the design and optimization of wireless systems. To fill in this gap, in this paper, we propose an EIT-based statistical channel model with simplified parameterization. Thanks to the simplified closed-form expression of the EMCF, it can be readily applied to various channel modeling and inference tasks. Specifically, by averaging the solutions of Maxwell’s equations over a tunable von Mises distribution, we obtain a spatio-temporal correlation function (STCF) model of the EM channel, which we name as the EMCF. Furthermore, by tuning the parameters of the EMCF, we propose an EIT-based covariance estimator (EIT-Cov) to accurately capture the channel covariance. Since classical MMSE estimators can exploit prior information contained in the channel covariance matrix, we further propose the EIT-MMSE channel estimator by substituting EMCF for the covariance matrix. Simulation results show that both the proposed EIT-Cov covariance estimator and the EIT-MMSE channel estimator outperform their baseline algorithms, thus proving that EIT is beneficial to wireless communication systems.},
  archive      = {J_TIT},
  author       = {Jieao Zhu and Zhongzhichao Wan and Linglong Dai and Tie Jun Cui},
  doi          = {10.1109/TIT.2025.3526689},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1777-1793},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Electromagnetic information theory-based statistical channel model for improved channel estimation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Broadcast channels with heterogeneous arrival and decoding
deadlines: Second-order achievability. <em>TIT</em>, <em>71</em>(3),
1758–1776. (<a href="https://doi.org/10.1109/TIT.2025.3532649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A standard assumption in the design of ultra-reliable low-latency communication systems is that the duration between message arrivals is larger than the number of channel uses before the decoding deadline. Nevertheless, this assumption fails when messages arrive rapidly and reliability constraints require that the number of channel uses exceed the time between arrivals. In this paper, we consider a broadcast setting in which a transmitter wishes to send two different messages to two receivers over Gaussian channels. Messages have different arrival times and decoding deadlines such that their transmission windows overlap. For this setting, we propose a coding scheme that exploits Marton’s coding strategy. We derive rigorous bounds on the achievable rate regions. Those bounds can be easily employed in point-to-point settings with one or multiple parallel channels. In the point-to-point setting with one or multiple parallel channels, the proposed achievability scheme is consistent with the normal approximation. In the broadcast setting, our scheme agrees with Marton’s strategy for sufficiently large numbers of channel uses and shows significant performance improvements over standard approaches based on time sharing for transmission of short packets.},
  archive      = {J_TIT},
  author       = {Homa Nikbakht and Malcolm Egan and Jean-Marie Gorce and H. Vincent Poor},
  doi          = {10.1109/TIT.2025.3532649},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1758-1776},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Broadcast channels with heterogeneous arrival and decoding deadlines: Second-order achievability},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Universal polarization for processes with memory.
<em>TIT</em>, <em>71</em>(3), 1705–1757. (<a
href="https://doi.org/10.1109/TIT.2025.3528241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A transform that is universally polarizing over a set of channels with memory is presented. Memory may be present in both the input to the channel and the channel itself. Both the encoder and the decoder are aware of the input distribution, which is fixed. However, only the decoder is aware of the actual channel being used. The transform can be used to design a universal code for this scenario. The code is to have vanishing error probability when used over any channel in the set, and achieve the infimal information rate over the set. The setting considered is, in fact, more general: we consider a set of processes with memory. Universal polarization is established for the case where each process in the set: 1) has memory in the form of an underlying hidden Markov state sequence that is aperiodic and irreducible; and 2) satisfies a ‘forgetfulness’ property. Forgetfulness, which we believe to be of independent interest, occurs when two hidden Markov states become approximately independent of each other given a sufficiently long sequence of observations between them. We show that aperiodicity and irreducibility of the underlying Markov chain is not sufficient for forgetfulness, and develop a sufficient condition for a hidden Markov process to be forgetful.},
  archive      = {J_TIT},
  author       = {Boaz Shuval and Ido Tal},
  doi          = {10.1109/TIT.2025.3528241},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1705-1757},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Universal polarization for processes with memory},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The generating idempotent is a minimum-weight codeword for
some binary BCH codes. <em>TIT</em>, <em>71</em>(3), 1700–1704. (<a
href="https://doi.org/10.1109/TIT.2024.3522185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a paper from 2015, Ding et al. (IEEE Trans. IT, May 2015) conjectured that for odd m, the minimum distance of the binary BCH code of length $2^{m}-1$ and designed distance $2^{m-2}+1$ is equal to the Bose distance calculated in the same paper. In this paper, we prove the conjecture. In fact, we prove a stronger result suggested by Ding et al.: the weight of the generating idempotent is equal to the Bose distance for both odd and even m. Our main tools are some new properties of the so-called fibbinary integers, in particular, the splitting field of related polynomials, and the relation of these polynomials to the idempotent of the BCH code.},
  archive      = {J_TIT},
  author       = {Yaron Shany and Amit Berman},
  doi          = {10.1109/TIT.2024.3522185},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1700-1704},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {The generating idempotent is a minimum-weight codeword for some binary BCH codes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal 5-seq LRCs with availability from golomb rulers.
<em>TIT</em>, <em>71</em>(3), 1689–1699. (<a
href="https://doi.org/10.1109/TIT.2025.3525668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a simple construction for binary $(n,k)$ linear codes using s-mark Golomb rulers. We prove that these codes are sequential-recovery locally repairable codes (LRCs) with availability 2, which can sequentially recover 5 erased symbols. We prove the necessary and sufficient condition for the proposed codes to be rate-optimal. We also prove the necessary and sufficient condition for the proposed codes to be dimension-optimal. Finally, we propose some variations of this constructions to obtain some 5-sequential recovery LRCs with availability 3. The proposed codes have higher rates and have more flexible choice for the lengths than other previously reported constructions.},
  archive      = {J_TIT},
  author       = {Hyojeong Choi and Hong-Yeop Song},
  doi          = {10.1109/TIT.2025.3525668},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1689-1699},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Optimal 5-seq LRCs with availability from golomb rulers},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear complementary dual codes and linear complementary
pairs of AG codes in function fields. <em>TIT</em>, <em>71</em>(3),
1676–1688. (<a href="https://doi.org/10.1109/TIT.2024.3521094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, linear complementary pairs (LCPs) of codes and linear complementary dual (LCD) codes have gained significant attention due to their applications in coding theory and cryptography. In this work, we construct explicit LCPs of codes and LCD codes from function fields of genus $g \geq 1$ . To accomplish this, we present pairs of suitable divisors that give rise to non-special divisors of degree $g-1$ in the function field. The results are applied in constructing LCPs of algebraic geometry codes and LCD algebraic geometry (AG) codes in Kummer extensions, hyperelliptic function fields, and elliptic curves.},
  archive      = {J_TIT},
  author       = {Alonso S. Castellanos and Adler V. Marques and Luciane Quoos},
  doi          = {10.1109/TIT.2024.3521094},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1676-1688},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Linear complementary dual codes and linear complementary pairs of AG codes in function fields},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One code fits all: Strong stuck-at codes for versatile
memory encoding. <em>TIT</em>, <em>71</em>(3), 1666–1675. (<a
href="https://doi.org/10.1109/TIT.2025.3526747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we consider a generalization of the well-studied problem of coding for “stuck-at” errors, which we refer to as “strong stuck-at” codes. In the traditional framework of stuck-at codes, the task involves encoding a message into a one-dimensional binary vector. However, a certain number of the bits in this vector are ‘frozen’, meaning they are fixed at a predetermined value and cannot be altered by the encoder. The decoder, aware of the proportion of frozen bits but not their specific positions, is responsible for deciphering the intended message. We consider a more challenging version of this problem where the decoder does not know also the fraction of frozen bits. We construct explicit and efficient encoding and decoding algorithms that get arbitrarily close to capacity in this scenario. Furthermore, to the best of our knowledge, our construction is the first, fully explicit construction of stuck-at codes that approach capacity.},
  archive      = {J_TIT},
  author       = {Roni Con and Ryan Gabrys and Eitan Yaakobi},
  doi          = {10.1109/TIT.2025.3526747},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1666-1675},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {One code fits all: Strong stuck-at codes for versatile memory encoding},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust gray codes approaching the optimal rate.
<em>TIT</em>, <em>71</em>(3), 1647–1665. (<a
href="https://doi.org/10.1109/TIT.2024.3522986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust Gray codes were introduced by (Lolck and Pagh, SODA 2024). Informally, a robust Gray code is a (binary) Gray code $\mathcal {G}$ so that, given a noisy version of the encoding $\mathcal {G}(j)$ of an integer j, one can recover $\hat {j}$ that is close to j (with high probability over the noise). Such codes have found applications in differential privacy. In this work, we present near-optimal constructions of robust Gray codes. In more detail, we construct a Gray code $\mathcal {G}$ of rate $1 - H_{2}(p) - \varepsilon $ that is efficiently encodable, and that is robust in the following sense. Supposed that $\mathcal {G}(j)$ is passed through the binary symmetric channel ${\text {BSC}}_{p}$ with cross-over probability p, to obtain x. We present an efficient decoding algorithm that, given x, returns an estimate $\hat {j}$ so that $| j - \hat {j}|$ is small with high probability.},
  archive      = {J_TIT},
  author       = {Roni Con and Dorsa Fathollahi and Ryan Gabrys and Mary Wootters and Eitan Yaakobi},
  doi          = {10.1109/TIT.2024.3522986},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1647-1665},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Robust gray codes approaching the optimal rate},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Codes for correcting a burst of edits using
weighted-summation VT sketch. <em>TIT</em>, <em>71</em>(3), 1631–1646.
(<a href="https://doi.org/10.1109/TIT.2025.3530506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bursts of errors are a class of errors that can be found in a variety of applications. A burst of t edits refers to a burst of t deletions, or a burst of t insertions, or a burst of t substitutions. This paper focuses on studying codes that can correct a burst of t edits. Our primary approach involves the use of the tool called weighted-summation VT sketch. The $(t,k)$ -weighted-summation VT sketch of a length-n sequence is defined as the weighted summation of the VT sketch of each row of its $t\times \lceil n/t \rceil $ array representation, with weights in the i-th row set as $k^{i-1}$ for $i=1,2,\ldots,t$ . By employing the weighted-summation VT sketch alongside multiple weight sketches, we introduce a construction for q-ary t-burst-substitution correcting codes with a redundancy of $\log n+O(1)$ , where the logarithm base is 2. Subsequently, we improve the redundancy to address specific types of burst-substitution errors, such as inversion errors, adjacent-block-transposition errors, and absorption errors. Moreover, by utilizing the method developed in the construction of burst-substitution correcting codes and imposing additional run-length-limited constraints, locally-bounded constraints, and strong-locally-balanced constraints, respectively, we introduce three constructions of t-burst-deletion correcting codes, each requiring a redundancy of $\log n+O(\log \log n)$ . Any t-burst-deletion-correcting code is also a t-burst-insertion correcting code, allowing us to intersect the t-burst-substitution-correcting codes and t-burst-deletion-correcting codes designed above to derive three constructions of q-ary t-burst-edit-correcting codes. The first two constructions have a redundancy of $\log n+(t\log q-1)\log \log n+O(1)$ , while the third construction has a redundancy of $\log n+\log \log n+O(1)$ . Most of the proposed codes demonstrate superior performance compared to previous results, with the exception of burst-deletion correcting codes. Furthermore, in cases of single-edit errors (t-burst-edit error with $t=1$ ), the redundancy of the first two constructions of quaternary single-edit correcting codes outperforms the results of Gabrys et al. (IEEE Trans. Inf. Theory 2023). We also provide efficient encoding and decoding algorithms for our codes to enhance their practical usability.},
  archive      = {J_TIT},
  author       = {Yubo Sun and Gennian Ge},
  doi          = {10.1109/TIT.2025.3530506},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1631-1646},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Codes for correcting a burst of edits using weighted-summation VT sketch},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized regenerating codes and node repair on graphs.
<em>TIT</em>, <em>71</em>(3), 1613–1630. (<a
href="https://doi.org/10.1109/TIT.2025.3532625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider regenerating codes in distributed storage systems where connections between the nodes are constrained by a graph. In this problem, the failed node downloads the information stored at a subset of vertices of the graph for the purpose of recovering the lost data. Compared to the standard setting, regenerating codes on graphs address two additional features. The repair information is moved across the network, and the cost of node repair is determined by the graphical distance from the helper nodes to the failed node. Accordingly, the helpers far away from the failed node may be expected to contribute less data for repair than the nodes in the neighborhood of that node. We analyze regenerating codes with nonuniform download for repair on graphs. Moreover, in the process of repair, the information moved from the helpers to the failed node may be combined through intermediate processing, reducing the repair bandwidth. We derive lower bounds for communication complexity of node repair on graphs, including repair schemes with nonuniform download and intermediate processing, and construct codes that attain these bounds. Additionally, some of the nodes may act as adversaries, introducing errors into the data moved in the network. For repair on graphs in the presence of adversarial nodes, we construct codes that support node repair and error correction in systematic nodes.},
  archive      = {J_TIT},
  author       = {Adway Patra and Alexander Barg},
  doi          = {10.1109/TIT.2025.3532625},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1613-1630},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Generalized regenerating codes and node repair on graphs},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bounded-degree low-rank parity-check codes. <em>TIT</em>,
<em>71</em>(3), 1593–1612. (<a
href="https://doi.org/10.1109/TIT.2025.3532811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank parity-check (LRPC) codes are the rank-metric analogue of low-density parity-check codes and they found important applications in code-based cryptography. In this paper we investigate a sub-family of LRPC codes, which have a parity-check matrix defined over a subspace ${\mathcal {V}}_{\alpha,d}=\langle 1,\alpha, \ldots, \alpha ^{d-1} \rangle _{\mathbb {F}_{q}}\subsetneq \mathbb {F}_{q^{m}} $ , where $\mathbb {F}_{q^{m}}$ is the finite field of $q^{m}$ elements, $\alpha \in \mathbb {F}_{q^{m}}$ is an element not in any proper subfield of $\mathbb {F}_{q^{m}}$ , and d is a positive integer significantly smaller than m. These codes are termed bounded-degree LRPC (BD-LRPC) codes. BD-LRPC codes are the same as the standard LRPC codes of density 2 when the degree $d=2$ , while for degree $d\gt 2$ they constitute a proper subset of LRPC codes of density d. Exploiting the structure of ${\mathcal {V}}_{\alpha,d}$ , the BD-LRPC codes of degree d can uniquely correct errors of rank weight r when $n-k \geq r + u$ for certain $u \geq 1$ , in contrast to the condition $n-k\geq dr$ required for the standard LRPC codes. This underscores the superior decoding capability of the BD-LRPC codes. Moreover, as the code length $n\rightarrow \infty $ , when $n/m\rightarrow 0$ , the BD-LRPC codes with a code rate of $R=k/n$ can be uniquely decodable with radius $\rho =r/n$ approaching the Singleton bound $1-R$ by letting $\epsilon =u/n\rightarrow 0$ ; and when $n/m$ is a constant, the BD-LRPC codes can have unique decoding radius $\rho = 1-R-\epsilon $ for a small $\epsilon $ , allowing for $\rho \gt (1-R)/2$ with properly chosen parameters. This superior decoding capability is theoretically proved for the case $d=2$ and confirmed by experimental results for $d\gt 2$ .},
  archive      = {J_TIT},
  author       = {Ermes Franch and Chunlei Li},
  doi          = {10.1109/TIT.2025.3532811},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1593-1612},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Bounded-degree low-rank parity-check codes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covering b-symbol metric codes and the generalized singleton
bound. <em>TIT</em>, <em>71</em>(3), 1585–1592. (<a
href="https://doi.org/10.1109/TIT.2024.3521328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbol-pair codes were proposed for the application in high density storage systems, where it is not possible to read individual symbols. Yaakobi, Bruck and Siegel proved that the minimum pair-distance $d_{2}$ of binary linear cyclic codes satisfies $d_{2} \geq \lceil 3d_{H}/2 \rceil $ and introduced b-symbol metric codes in 2016. In this paper, covering codes in b-symbol metrics are considered. Some examples are given to show that the Delsarte bound and the Norse bound for covering codes in the Hamming metric do not hold true for covering codes in the pair metric. We give the redundancy bound on covering radius of linear codes in the b-symbol metric and give some optimal codes attaining this bound. Then we prove that there is no perfect linear symbol-pair code with the minimum pair-distance 7 and there is no perfect b-symbol metric code if $b\geq \frac {n+4}{2}$ . Moreover a lot of cyclic and algebraic-geometric codes are proved non-perfect in the b-symbol metric. The covering radius of the Reed-Solomon code in the b-symbol metric is determined. As an application, the generalized Singleton bound on the sizes of list-decodable b-symbol metric codes is also presented. Then an upper bound on lengths of general MDS symbol-pair codes is proved.},
  archive      = {J_TIT},
  author       = {Hao Chen},
  doi          = {10.1109/TIT.2024.3521328},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1585-1592},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Covering b-symbol metric codes and the generalized singleton bound},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal codes for (t, s)-burst error.
<em>TIT</em>, <em>71</em>(3), 1570–1584. (<a
href="https://doi.org/10.1109/TIT.2025.3531915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, codes for correcting a burst of errors have attracted significant attention. One of the most important reasons is that bursts of errors occur in certain emerging techniques, such as DNA storage. In this paper, we investigate a type of error, called a $(t,s)$ -burst, which deletes t consecutive symbols and inserts s arbitrary symbols at the same coordinate. Note that a $(t,s)$ -burst error can be seen as a generalization of a burst of insertions ( $t=0$ ), a burst of deletions ( $s=0$ ), and a burst of substitutions ( $t=s$ ). Our main contribution is to give explicit constructions of q-ary $(t,s)$ -burst correcting codes with $\log n + O(1)$ bits of redundancy for any given constant non-negative integers t, s, and $q \geq 2$ . These codes have optimal redundancy up to an additive constant. Furthermore, we apply our $(t,s)$ -burst correcting codes to combat other various types of errors and improve the corresponding results. In particular, one of our byproducts is a permutation code capable of correcting a burst of t stable deletions with $\log n + O(1)$ bits of redundancy, which is optimal up to an additive constant.},
  archive      = {J_TIT},
  author       = {Yubo Sun and Ziyang Lu and Yiwei Zhang and Gennian Ge},
  doi          = {10.1109/TIT.2025.3531915},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1570-1584},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Asymptotically optimal codes for (t, s)-burst error},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified study on sequentiality in universal classification
with empirically observed statistics. <em>TIT</em>, <em>71</em>(3),
1546–1569. (<a href="https://doi.org/10.1109/TIT.2024.3525012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the binary hypothesis testing problem, it is well known that sequentiality in taking samples eradicates the trade-off between two error exponents, yet implementing the optimal test requires the knowledge of the underlying distributions, say $P_{0}$ and $P_{1}$ . In the scenario where the knowledge of distributions is replaced by empirically observed statistics from the respective distributions, the gain of sequentiality is less understood when subject to universality constraints over all possible $P_{0},P_{1}$ . In this work, the gap is mended by a unified study on sequentiality in the universal binary classification problem, where the universality constraints are set on the expected stopping time as well as the type-I error exponent. The type-I error exponent is required to achieve a pre-set distribution-dependent constraint $\lambda (P_{0},P_{1})$ for all $P_{0},P_{1}$ . Under the proposed framework, different sequential setups are investigated so that fair comparisons can be made with the fixed-length counterpart. By viewing these sequential classification problems as special cases of a general sequential composite hypothesis testing problem, the optimal type-II error exponents are characterized. Specifically, in the general sequential composite hypothesis testing problem subject to universality constraints, upper and lower bounds on the type-II error exponent are proved, and a sufficient condition for which the bounds coincide is given. The results for sequential classification problems are then obtained accordingly. With the characterization of the optimal error exponents, the benefit of sequentiality is shown both analytically and numerically by comparing the sequential and the fixed-length cases in representative examples of type-I exponent constraint $\lambda $ .},
  archive      = {J_TIT},
  author       = {Ching-Fang Li and I-Hsiang Wang},
  doi          = {10.1109/TIT.2024.3525012},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1546-1569},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {A unified study on sequentiality in universal classification with empirically observed statistics},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tight exponential strong converse for source coding problem
with encoded side information. <em>TIT</em>, <em>71</em>(3), 1533–1545.
(<a href="https://doi.org/10.1109/TIT.2025.3529612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The source coding problem with encoded side information is considered. A lower bound on the strong converse exponent has been derived by Oohama, but its tightness has not been clarified. In this paper, we derive a tight strong converse exponent. For the special case where the side-information does not exist, we demonstrate that our tight exponent of the Wyner-Ahlswede-Körner (WAK) problem reduces to the known tight expression of that special case while Oohama’s lower bound is strictly loose. The converse part is proved by a judicious use of the change-of-measure argument, which was introduced by Gu and Effros and further developed by Tyagi and Watanabe. A key component of the methodology by Tyagi and Watanabe is the use of soft Markov constraint, which was originally introduced by Oohama, as a penalty term to prove the Markov constraint at the end. A technical innovation of this paper compared to Tyagi and Watanabe is recognizing that the soft Markov constraint is a part of the exponent, rather than a penalty term that should vanish at the end; this recognition enables us to derive the matching achievability bound. In fact, via numerical experiment, we provide evidence that the soft Markov constraint is strictly positive. Compared to Oohama’s derivation of the lower bound, which relies on the single-letterization of a certain moment-generating function, the derivation of our tight exponent only involves manipulations of the Kullback-Leibrer divergence and Shannon entropies. The achievability part is derived by a careful analysis of the type argument; however, unlike the conventional analysis for the achievable rate region, we need to derive the soft Markov constraint in the analysis of the correct probability. Furthermore, we present an application of our derivation of the strong converse exponent to the privacy amplification.},
  archive      = {J_TIT},
  author       = {Daisuke Takeuchi and Shun Watanabe},
  doi          = {10.1109/TIT.2025.3529612},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1533-1545},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Tight exponential strong converse for source coding problem with encoded side information},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel coding with mean and variance cost constraints.
<em>TIT</em>, <em>71</em>(3), 1504–1532. (<a
href="https://doi.org/10.1109/TIT.2025.3533947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider channel coding for discrete memoryless channels (DMCs) with a novel cost constraint that constrains both the mean and the variance of the cost of the codewords. We show that the maximum (asymptotically) achievable rate under the new cost formulation is equal to the capacity-cost function; in particular, the strong converse holds. We further characterize the optimal second-order coding rate of these cost-constrained codes; in particular, the optimal second-order coding rate is finite. We then show that the second-order coding performance is strictly improved with feedback using a new variation of timid/bold coding, significantly broadening the applicability of timid/bold coding schemes from unconstrained compound-dispersion channels to all cost-constrained channels. Equivalent results on the minimum average probability of error are also given.},
  archive      = {J_TIT},
  author       = {Adeel Mahmood and Aaron B. Wagner},
  doi          = {10.1109/TIT.2025.3533947},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1504-1532},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Channel coding with mean and variance cost constraints},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Successive refinement of shannon cipher system under maximal
leakage. <em>TIT</em>, <em>71</em>(3), 1487–1503. (<a
href="https://doi.org/10.1109/TIT.2024.3523130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the successive refinement setting of Shannon cipher system (SCS) under the maximal leakage secrecy metric for discrete memoryless sources under bounded distortion measures. Specifically, we generalize the threat model for the point-to-point rate-distortion setting of Issa, Wagner and Kamath (T-IT 2020) to the multiterminal successive refinement setting. Under mild conditions that correspond to partial secrecy, we characterize the asymptotically optimal normalized maximal leakage region for both the joint excess-distortion probability (JEP) and the expected distortion reliability constraints. Under JEP, in the achievability part, we propose a type-based coding scheme, analyze the reliability guarantee for JEP and bound the leakage of the information source through compressed messages. In the converse part, by analyzing a guessing scheme of the eavesdropper, we prove the optimality of our achievability result. Under expected distortion, the achievability part is established similarly to the JEP counterpart. The converse proof proceeds by generalizing the corresponding results for the rate-distortion setting of SCS by Schieler and Cuff (T-IT 2014) to the successive refinement setting. Somewhat surprisingly, the normalized maximal leakage regions under both JEP and expected distortion constraints are identical under certain conditions, although JEP appears to be a stronger reliability constraint.},
  archive      = {J_TIT},
  author       = {Zhuangfei Wu and Lin Bai and Lin Zhou},
  doi          = {10.1109/TIT.2024.3523130},
  journal      = {IEEE Transactions on Information Theory},
  month        = {3},
  number       = {3},
  pages        = {1487-1503},
  shortjournal = {IEEE Trans. Inf. Theory},
  title        = {Successive refinement of shannon cipher system under maximal leakage},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
