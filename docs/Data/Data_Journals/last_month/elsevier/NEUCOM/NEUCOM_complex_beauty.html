<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NEUCOM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neucom---158">NEUCOM - 158</h2>
<ul>
<li><details>
<summary>
(2025). A reformulation neurodynamic algorithm for distributed
nonconvex optimization. <em>NEUCOM</em>, <em>635</em>, 130023. (<a
href="https://doi.org/10.1016/j.neucom.2025.130023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a reformulation neurodynamic algorithm for solving distributed nonconvex optimization problems. A class of general Lagrangian functions is introduced to eliminate the dual gap in nonconvex problems. This algorithm extends the application of neurodynamic algorithms based on the p -power reformulation transformation of Lagrangian functions. Under mild conditions, the initial point of the decision vector can be arbitrarily chosen. It is proven that the output trajectories will eventually converge to a strict local minimum point of the distributed nonconvex optimization problem. Finally, numerical experiments demonstrate the effectiveness of the proposed algorithm, which is also applied to solve the oblique throwing problem and the distributed source localization problem.},
  archive      = {J_NEUCOM},
  author       = {Xin Yu and Qingzhou Huang and Rixin Lin},
  doi          = {10.1016/j.neucom.2025.130023},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {130023},
  shortjournal = {Neurocomputing},
  title        = {A reformulation neurodynamic algorithm for distributed nonconvex optimization},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASTR: Transformer-based alert-to-stage translator for
multi-stage attack detection. <em>NEUCOM</em>, <em>635</em>, 130016. (<a
href="https://doi.org/10.1016/j.neucom.2025.130016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-stage attacks have complex patterns and intricate inter-stage dependencies, making them difficult to detect using conventional methods. Traditional detection techniques struggle to capture long-term dependencies in alert sequences and often fail to map individual alerts to their specific attack stages. Aiming to develop an advanced detection method enhancing the accuracy and granularity of multi-stage attack detection, this paper proposes ASTR (Alert Stage TRanslator for MSA), a Transformer-based detection architecture. ASTR utilizes a Doc2Vec-based frontend to embed IDS alerts into numerical vectors and applies positional encoding to preserve sequential information. The core Transformer backbone, with its attention mechanism, is employed to extract deep contextual features and long-term dependencies from alert sequences. Finally, a refined backend maps these features to specific attack stages. ASTR is evaluated against state-of-the-art methods using three public datasets: DARPA2000, ISCXIDS2012, and CIC-IDS2017. Experimental results demonstrate that ASTR achieves detection accuracy of over 97% across all datasets. Compared to existing methods, ASTR not only improves precision, recall, and F1-scores but also provides detailed, one-to-one mappings from alerts to attack stages, thereby offering a more fine-grained detection capability.},
  archive      = {J_NEUCOM},
  author       = {Wei Ma and Yunyun Hou and Aina Sui and Pengpeng Jian},
  doi          = {10.1016/j.neucom.2025.130016},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {130016},
  shortjournal = {Neurocomputing},
  title        = {ASTR: Transformer-based alert-to-stage translator for multi-stage attack detection},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRG-net: Point relationship-guided network for 3D human
action recognition. <em>NEUCOM</em>, <em>635</em>, 130015. (<a
href="https://doi.org/10.1016/j.neucom.2025.130015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds contain rich spatial information, providing important supplementary clues for human action recognition. Recent methods for action recognition based on point cloud sequences primarily rely on complex spatiotemporal local encoding. However, these methods often utilize max-pooling operations to select features when extracting local features, restricting feature updates to local neighborhoods and failing to fully exploit the relationships between regions. Moreover, cross-frame encoding can also lead to the loss of spatiotemporal information. In this study, we propose PRG-Net, a Point Relation Guided Network, to further improve the learning of spatiotemporal features in point clouds. First, we designed two core modules: the Spatial Feature Aggregation (SFA) and the Spatial Feature Descriptor (SFD) modules. The SFA module expands the spatial structure between regions using dynamic aggregation techniques, while the SFD module guides the region aggregation process by Attention-Weighted Descriptors. They enhance the modeling of human spatial structure by expanding the relationships between regions. Second, we introduce inter-frame motion encoding techniques that can obtain the final spatiotemporal representation of the human body through the aggregation of cross-frame vectors, without relying on complex spatiotemporal local encoding. We evaluate PRG-Net on publicly available human action recognition datasets, including NTU RGB+D 60, NTU RGB+D 120, UTD-MHAD, and MSR Action 3D. Experimental results demonstrate that our method outperforms state-of-the-art point-based 3D action recognition methods significantly. Furthermore, we conduct extended experiments on the SHREC 2017 dataset for gesture recognition, and the results show that our method maintains competitive performance on that dataset as well.},
  archive      = {J_NEUCOM},
  author       = {Yao Du and Zhenjie Hou and En Lin and Xing Li and Jiuzhen Liang and Xinwen Zhou},
  doi          = {10.1016/j.neucom.2025.130015},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {130015},
  shortjournal = {Neurocomputing},
  title        = {PRG-net: Point relationship-guided network for 3D human action recognition},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep consistent penalizing hashing with noise-robust
representation for large-scale image retrieval. <em>NEUCOM</em>,
<em>635</em>, 130014. (<a
href="https://doi.org/10.1016/j.neucom.2025.130014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the powerful representational capacity of deep learning and the attractive computational efficiency of binary codes, deep hashing frameworks have made large progress for large-scale image retrieval applications. By calculating the priori labels, most existing deep supervised hashing usually introduces the effective margin-based objective loss to generate label-level penalizing boundaries for training samples during the model optimization. However, the decision boundaries from label-level penalizing may be inconsistent with semantic relations hidden in raw samples, compromising the performance. Besides, for classes with low intra-class variances or inter-class correlations, the force field of the margin-based methods might be too weak to learn the discriminant embedding space. In this paper, we solve this dilemma with a novel unified deep hashing framework, termed Deep Consistent Penalizing Hashing with noise-robust representation (DCPH), to generate compact yet discriminative binary codes for efficient and accurate image retrieval. Specifically, by learning the unbalanced correlations of training samples, the semantic consistency penalizing loss, consisting of pulling penalizing elements and pushing penalizing elements, is proposed to generate the semantic decision boundaries across classes. For parameter optimization, the dice-like optimization strategy is introduced to balance the pulling and pushing field, facilitating the generation of highly separable Hamming space. Besides, to mitigate the negative influence caused by objective-unrelated information or noise, by introducing patch-wise attention strategy and depth-wise convolution operation, the noise-robust representation module is developed to capture the robust feature descriptor with abundant fine-grained information. Comprehensive evaluations are performed on several benchmark datasets, and the experimental results consistently validate the effectiveness of our proposed DCPH framework, which significantly outperforms the state-of-the-art deep hashing methods.},
  archive      = {J_NEUCOM},
  author       = {Qibing Qin and Hong Wang and Wenfeng Zhang and Lei Huang and Jie Nie},
  doi          = {10.1016/j.neucom.2025.130014},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {130014},
  shortjournal = {Neurocomputing},
  title        = {Deep consistent penalizing hashing with noise-robust representation for large-scale image retrieval},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-wise knowledge decoupling for personalized federated
learning via radon transform. <em>NEUCOM</em>, <em>635</em>, 130013. (<a
href="https://doi.org/10.1016/j.neucom.2025.130013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (pFL) customizes local models to address heterogeneous data across clients. One prominent research direction in pFL is model decoupling, where the knowledge of a global model is selectively utilized to assist local model personalization. Prior studies primarily use decoupled global-model parameters to convey this selected knowledge. However, due to the task-related knowledge-mixing nature of deep learning models, using these parameters may introduce irrelevant knowledge to specific clients, impeding personalization. To address this, we propose a domain-wise knowledge decoupling approach (pFedDKD), which decouples global-model knowledge into diverse projection segments in the representation space, meeting the specific needs of clients on heterogeneous local domains. A Radon transform-based method is provided to facilitate this decoupling, enabling clients to extract relevant knowledge segments for personalization. Besides, we provide a distillation-based back-projection learning method to fuse local-model knowledge into the global model, ensuring the updated global-model knowledge remains decouplable by projection. A theoretical analysis confirms that our approach improves generalization. Extensive experiments on four datasets demonstrate that pFedDKD consistently outperforms eleven state-of-the-art baselines, achieving an average improvement of 1.21% in test accuracy over the best-performing baseline.},
  archive      = {J_NEUCOM},
  author       = {Zihao Lu and Junli Wang and Changjun Jiang},
  doi          = {10.1016/j.neucom.2025.130013},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {130013},
  shortjournal = {Neurocomputing},
  title        = {Domain-wise knowledge decoupling for personalized federated learning via radon transform},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective backbone network architecture search based on
transfer learning in steel defect detection. <em>NEUCOM</em>,
<em>635</em>, 130012. (<a
href="https://doi.org/10.1016/j.neucom.2025.130012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, steel defect detection methods based on deep learning have been widely used. However, due to the shape specificity of steel defects and data scarcity, using existing convolutional neural network architectures for training requires significant expertise and time to fine-tune the hyperparameters. Transfer learning effectively tackles the challenges of data scarcity or limited computing resources by transferring domain knowledge from source tasks to related target tasks, reducing the resource consumption of model training from scratch. In this paper, we propose a transfer learning-based multiobjective backbone network architecture search method (TMBNAS). First, TMBNAS formulates defect detection network search as a multiobjective problem while optimizing its detection accuracy and model complexity. Second, an effective variable-length encoding strategy is designed to represent different building blocks and unpredictable optimal depths in convolutional neural networks, and targeted improvements are made to the crossover and mutation operators. For the specificity of the steel defect detection task, a transfer learning strategy based on similar knowledge is used to transfer the architecture and weight parameters obtained from the search in the source task to the target task, and adjust and optimize them. Finally, a dynamic adjustment mechanism based on actual constraints is designed during the search process to gradually approximate the optimal non-dominated solution set with higher detection accuracy without losing its population diversity. The proposed method is tested on the continuous casting slab and workpiece defect datasets. The experimental results show that the model searched by the proposed method can achieve better detection performance compared with manually designed deep learning algorithms and classical network architecture search methods.},
  archive      = {J_NEUCOM},
  author       = {Tianchen Zhao and Xianpeng Wang and Xiangman Song},
  doi          = {10.1016/j.neucom.2025.130012},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {130012},
  shortjournal = {Neurocomputing},
  title        = {Multiobjective backbone network architecture search based on transfer learning in steel defect detection},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pool-mamba: Pooling state space model for low-light image
enhancement. <em>NEUCOM</em>, <em>635</em>, 130005. (<a
href="https://doi.org/10.1016/j.neucom.2025.130005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mamba, with its advantages in long-distance modeling and computational efficiency, has been promptly applied in low-light image enhancement (LLIE). Nevertheless, Mamba faces two key issues when processing low-light images: (1) overexposure in scenes with uneven illumination due to the lack of multi-scale modeling; (2) insufficient local detail recovery, as its sequential operation weakens the perception of local lighting changes. To alleviate these problems, we propose a Pooling State Space Model (Pool-Mamba) by integrating the state space model with pooling techniques. First, we devise the Pyramid-pooling Mamba (PMamba) module, which leverages pyramid pooling to capture multi-scale information, effectively mitigating uneven exposure under varying lighting conditions. Next, the Axis-pooling Mamba (AMamba) module is proposed to model local correlations along specific spatial dimensions (height and width), generating more refined local representations and enhancing the model’s ability to adapt to local lighting variations and intricate details. Finally, we incorporate a Dual Gated Enhancement Module (DGEM) to strengthen the channel correlations between PMamba and AMamba, facilitating the integration of multi-scale and local features. Benchmark assessments demonstrate that Pool-Mamba surpasses current state-of-the-art (SOTA) methods, achieving superior quantitative evaluations and less distorted visual results.},
  archive      = {J_NEUCOM},
  author       = {Qiao Zhang and Mingwen Shao and Xinyuan Chen},
  doi          = {10.1016/j.neucom.2025.130005},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {130005},
  shortjournal = {Neurocomputing},
  title        = {Pool-mamba: Pooling state space model for low-light image enhancement},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MKGF: A multi-modal knowledge graph based RAG framework to
enhance LVLMs for medical visual question answering. <em>NEUCOM</em>,
<em>635</em>, 129999. (<a
href="https://doi.org/10.1016/j.neucom.2025.129999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical visual question answering (MedVQA) is a challenging task that requires models to understand medical images and return accurate responses for the given questions. Most recent methods focus on transferring general-domain large vision–language models (LVLMs) to the medical domain by constructing medical instruction datasets and in-context learning. However, the performance of these methods are limited due to the hallucination issue of LVLMs. In addition, fine-tuning the abundant parameters of LVLMs on medical instruction datasets is high time and economic cost. Hence, we propose a MKGF framework that leverages a multi-modal medical knowledge graph (MMKG) to relieve the hallucination issue without fine-tuning the abundant parameters of LVLMs. Firstly, we employ a pre-trained text retriever to build question–knowledge relations on training set. Secondly, we train a multi-modal retriever with these relations. Finally, we use it to retrieve question-relevant knowledge and enhance the performance of LVLMs on the test set. To evaluate the effectiveness of MKGF, we conduct extensive experiments on two public datasets Slake and VQA-RAD. Our method improves the pre-trained SOTA LVLMs by 10.15% and 9.32%, respectively. The source codes are available at https://github.com/ehnal/MKGF .},
  archive      = {J_NEUCOM},
  author       = {Yinan Wu and Yuming Lu and Yan Zhou and Yifan Ding and Jingping Liu and Tong Ruan},
  doi          = {10.1016/j.neucom.2025.129999},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129999},
  shortjournal = {Neurocomputing},
  title        = {MKGF: A multi-modal knowledge graph based RAG framework to enhance LVLMs for medical visual question answering},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label self-correction intelligent diagnosis method and
embedded system for axle box bearings of high-speed trains with noisy
labels. <em>NEUCOM</em>, <em>635</em>, 129998. (<a
href="https://doi.org/10.1016/j.neucom.2025.129998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to annotation errors, delayed labeling, and noise interference, data label noise is a common issue in high-speed train datasets, leading to overfitting of existing intelligent diagnostic methods on noisy-label samples and a decline in the accuracy of fault diagnosis, which affects the correct assessment of high-speed train bearing health. To tackle this issue, this article presents an adaptive label self-correction intelligent diagnostic method. The method consists of three main parts: First, it employs dynamic thresholds and multi-network interactive training to separate clean from noisy labels. Second, it corrects noisy labels using classifiers trained on clean data, with two designed correction methods for high-accuracy label correction. Third, it retrains the model by reweighting loss to ensure that it fully captures information from noisy label data. Additionally, based on the proposed method, an AI microprocessor diagnosis system is developed for real-world health monitoring of axle box bearings. Both the method and the system have been validated through diagnostic cases of axle box bearings. Validation through diagnostic cases demonstrates that the method can train high-accuracy diagnostic models under label noise conditions and the system can rapidly diagnose data in real-time.},
  archive      = {J_NEUCOM},
  author       = {Yaning Li and Yang Gao and Bin Yang and Yaguo Lei and Xiang Li and Yue Shu and Ke Feng},
  doi          = {10.1016/j.neucom.2025.129998},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129998},
  shortjournal = {Neurocomputing},
  title        = {Label self-correction intelligent diagnosis method and embedded system for axle box bearings of high-speed trains with noisy labels},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SR-CIBN: Semantic relationship-based consistency and
inconsistency balancing network for multimodal fake news detection.
<em>NEUCOM</em>, <em>635</em>, 129997. (<a
href="https://doi.org/10.1016/j.neucom.2025.129997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast dissemination of online information has facilitated the evolution of multimodal fake news, thereby rendering the trustworthiness of its content difficult to identify. Some existing studies, although capturing the consistency and inconsistency features between different modalities, neglect to dynamically balance these two types of features based on their contributions during the features fusion. Thus, we propose a S emantic R elationship-based C onsistency and I nconsistency B alancing N etwork for multimodal fake news detection (SR-CIBN). Specifically, the global features are thoroughly investigated by hierarchically penetrating and interacting between multimodal features that are aligned by contrastive learning at both the intra- and inter-modal views. Then, the global consistency and inconsistency features are obtained through the interaction between the selected key image patches features and the global features. Additionally, the fusion intensity of the global consistency and inconsistency features is adjusted based on the image–text matching degree, resulting in the final optimized features. Under the joint learning framework we proposed, confusion between semantically similar real and fake news is effectively avoided by training with triplet loss based on the image–text semantic relationship. Our model surpasses comparable approaches, as shown by comprehensive experiments on the Twitter and Weibo datasets.},
  archive      = {J_NEUCOM},
  author       = {Hongzhu Yu and Hongchen Wu and Xiaochang Fang and Meng Li and Huaxiang Zhang},
  doi          = {10.1016/j.neucom.2025.129997},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129997},
  shortjournal = {Neurocomputing},
  title        = {SR-CIBN: Semantic relationship-based consistency and inconsistency balancing network for multimodal fake news detection},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory-augmented prototypical meta-learning method for
bearing fault identification under few-sample conditions.
<em>NEUCOM</em>, <em>635</em>, 129996. (<a
href="https://doi.org/10.1016/j.neucom.2025.129996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in deep learning have enhanced the performance of rolling bearing fault diagnosis. However, two challenges persist: 1) These models typically require large amounts of labeled data, which limits their effectiveness when such data is scarce; 2) The decision-making process of these models is often opaque, making it difficult to understand the key factors they focus on. In this paper, the memory-augmented prototypical meta-learning (MAPML) is proposed for few-shot fault identification. In MAPML, the dynamic prototype adjustment module is proposed to dynamically and adaptively refine class prototypes through the memory-augmented mechanism, enhancing the model’s ability to accurately differentiate between classes. Additionally, the multi-scale convolutional architecture is employed to extract hidden features from signals at multiple scales in parallel, providing comprehensive feature representation across different scales. This proposed method is verified through two datasets of bearings, demonstrating robust generalization capabilities across different working conditions. By introducing Gaussian white noise to simulate real industrial environments, the robustness and practicality of MAPML in handling noisy data are further substantiated. The interpretability analysis indicates that MAPML effectively captures impact characteristics in the time domain, as well as resonance bands and fault characteristic frequencies in the frequency domain.},
  archive      = {J_NEUCOM},
  author       = {Xianze Li and Zhitai Xing and Ling Xiang and Yang Chen and Aijun Hu},
  doi          = {10.1016/j.neucom.2025.129996},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129996},
  shortjournal = {Neurocomputing},
  title        = {Memory-augmented prototypical meta-learning method for bearing fault identification under few-sample conditions},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot SAR image classification via multiple prototypes
ensemble. <em>NEUCOM</em>, <em>635</em>, 129989. (<a
href="https://doi.org/10.1016/j.neucom.2025.129989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) image classification has benefited significantly from deep learning techniques, which excel at automatically learning semantic features from data. However, compared with optical remote sensing images, SAR images face a more pronounced sample limitation problem due to sparsely distributed training samples and complex backgrounds. As a result, the performance of data-driven SAR image classification significantly degrades due to overfitting, especially when deep learning algorithms operate under data scarcity. To address these challenges, we present a novel few-shot SAR classification approach using a multiple-prototype ensemble network within the meta-learning paradigm. Specifically, to better capture the scattering feature distribution and enhance intra-category aggregation in SAR images, multiple prototypes are learned to fully leverage the semantic information embedded in the limited support samples. Furthermore, we introduce a subspace discriminative loss to improve the representational power of the learned prototypes by ensuring consistency in SAR feature representation while maintaining inter-class divergence. Extensive experiments conducted on three real-world datasets demonstrate the superiority of the proposed method compared to several state-of-the-art approaches.},
  archive      = {J_NEUCOM},
  author       = {Zhiqiang Zhao and Yuhui Tong and Meng Jia and Yuan Qiu and Xiaofan Wang and Xinhong Hei},
  doi          = {10.1016/j.neucom.2025.129989},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129989},
  shortjournal = {Neurocomputing},
  title        = {Few-shot SAR image classification via multiple prototypes ensemble},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSKN-MFIF: Large selective kernel network for multi-focus
image fusion. <em>NEUCOM</em>, <em>635</em>, 129984. (<a
href="https://doi.org/10.1016/j.neucom.2025.129984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion (MFIF) is an image enhancement technique that compensates for the limited depth of field of optical lenses. It has broad application prospects across multiple disciplines such as digital photography, biomedical imaging, and machine vision, similar to other advanced visual tasks. This study proposes a large selective kernel network, referred to as LSKN-MFIF, which is designed to improve multi-focus image fusion. This network has the capability to dynamically adjust its extensive spatial receptive field, effectively expanding the receptive field and capturing multi-scale global information, fully extracting and integrating multi-scale context to better identify various focused regions in the image. More specifically, LSKN-MFIF extracts multi-scale global features through a large selective kernel module (LKSB) that decomposes large kernel convolutions, utilizes spatial feature selection for feature aggregation, and then enhances representation capability by extracting local information through a gated differential convolution block (GDCB), thereby generating accurate decision maps. Experimental results show that the proposed methodology surpasses current multi-focus image fusion techniques in terms of both subjective visual quality and objective performance metrics.},
  archive      = {J_NEUCOM},
  author       = {Hao Zhai and Guochao Zhang and Zhi Zeng and Zhendong Xu and Aiqing Fang},
  doi          = {10.1016/j.neucom.2025.129984},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129984},
  shortjournal = {Neurocomputing},
  title        = {LSKN-MFIF: Large selective kernel network for multi-focus image fusion},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double-activation neural network for solving parabolic
equations with time delay. <em>NEUCOM</em>, <em>635</em>, 129978. (<a
href="https://doi.org/10.1016/j.neucom.2025.129978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While physics-informed neural network (PINN) has shown promise in solving partial differential equations (PDEs), their performance is limited by the expressive capacity of standard neural network architectures, particularly when dealing with complex, nonlinear problems. To improve the nonlinear expressive capacity of the network and broaden the applications of PINN, we propose a double-activation neural network (DANN) for solving parabolic equations with time delay, where each neuron is equipped with two activation functions and a new parameter is introduced in one of the functions to form quadratic terms. To address the issue of low fitting accuracy caused by the discontinuity of solution’s derivative, a piecewise fitting approach is proposed by dividing the global solving domain into several subdomains according to the discontinuous points. The convergence of the loss function is proved. We conduct a series of numerical experiments to show the efficiency of the proposed DANN technique, including solving delay partial differential equations (DPDEs) with constant delay, time-dependent delay, and delay differential equations (DDEs) with state-dependent delay. The robustness of DANN is assessed by varying the number of training points, hidden layers, neurons per layer, and random seeds. We also evaluate the extended application of DANN by simulating a second-order rogue wave in the derivative nonlinear Schrödinger (DNLS) equation, to show its applicability beyond DPDEs.},
  archive      = {J_NEUCOM},
  author       = {Qiumei Huang and Qiao Zhu},
  doi          = {10.1016/j.neucom.2025.129978},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129978},
  shortjournal = {Neurocomputing},
  title        = {Double-activation neural network for solving parabolic equations with time delay},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy and synchronization of multifunctional loop neural
networks. <em>NEUCOM</em>, <em>635</em>, 129973. (<a
href="https://doi.org/10.1016/j.neucom.2025.129973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifunctional thermo-photoelectric neurons are newly proposed intelligent artificial neurons with expanded functionalities. They are capable of detecting and encoding multimodal thermo-photoelectric signals. This study constructs three loop neural networks by bridging photosensitive, thermosensitive, and thermo-photoelectric neurons with linear resistors, inductive coils, and memristors, respectively. These networks are designated as Voltage, Magnetic, and Memory Coupling Functional Networks (VCFN, MACFN, and MECFN). In the adaptive VCFN model, the system&#39;s diverse energy cannot be effectively balanced by voltage coupling, resulting in an inability to achieve a stable synchronization state. MACFN and MECFN models perform energy pumping and extraction among neurons, resulting in a self-organizing behavior. This behavior spontaneously adjusts the membrane sequences of neurons within the model, ultimately achieving a stable equilibrium of the system&#39;s intrinsic field energy. The acceleration of this process can be facilitated by an increase in the coupling gain ratio. The MECFN model quickly achieves phase synchronization, and due to its adaptive mechanism, the phase synchronization state is further stabilized. This indicates memristors can serve as bridges for communication and encoding between heterogeneous functional neural networks. The effects of chaotic currents on isolated neurons and models were also considered. High-intensity chaotic currents will cause resonance phenomena in neurons under single-peaking oscillation mode, exhibiting chaotic geometric characteristics but without changing their periodic characteristics. In neurons with multi-spiking oscillation modes, chaotic currents may induce extremely narrow chaotic windows. VCFN, MACFN, and MECFN models demonstrate good robustness in spike periodic modes but are more susceptible to external chaotic current interference in chaotic modes. Appropriate intensities of chaotic current stimulation can promote synchronization in models, while excessive intensities may suppress it. This research provides insights into the synchronization processes of multifunctional neural networks and the design of multimodal and high-robustness intelligent sensor systems, providing a theoretical foundation for advancements in artificial intelligence.},
  archive      = {J_NEUCOM},
  author       = {Zebang Cheng and Shu Zhou and Jiajun Jiang and Shunwei Yao and Lin Peng and Tingting Shi and Xiaolin Liu and Jia Lin},
  doi          = {10.1016/j.neucom.2025.129973},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129973},
  shortjournal = {Neurocomputing},
  title        = {Energy and synchronization of multifunctional loop neural networks},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Student behavior detection model based on multilevel
residual networks and hybrid attention mechanisms. <em>NEUCOM</em>,
<em>635</em>, 129965. (<a
href="https://doi.org/10.1016/j.neucom.2025.129965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately analyzing student behaviors allows for better evaluation of student engagement, which in turn can improve teaching quality. To address challenges such as multi-scale scenes, occluded targets, and subtle fine features in classroom environments, while also considering model implementability, we propose an efficient student behavior detection model, RSAY. This model leverages multi-scale information extraction and a hybrid attention mechanism to support teaching. Both the backbone and feature fusion networks of the model integrate our designed Rep_SC_Atten module, which incorporates our novel multi-level residual network architecture and a lightweight hybrid attention mechanism. This hybrid architecture enhances the model’s sensitivity and ability to extract multi-scale information, while ensuring effective extraction of fine-grained features via the attention mechanism. Additionally, the DDetect strategy is introduced in the detection head to reduce model size without sacrificing accuracy. We evaluated our model using the SCB-Dataset and a custom student behavior dataset, demonstrating a 6.3% improvement in accuracy over the baseline model.},
  archive      = {J_NEUCOM},
  author       = {Wenbin Lu and Songyan Liu and Boyang Ding and Peng Chen and Fangpeng Lu},
  doi          = {10.1016/j.neucom.2025.129965},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129965},
  shortjournal = {Neurocomputing},
  title        = {Student behavior detection model based on multilevel residual networks and hybrid attention mechanisms},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable ℓ2−ℓ∞ state estimation for delayed neural networks
under weighted try-one-discard protocol. <em>NEUCOM</em>, <em>635</em>,
129923. (<a href="https://doi.org/10.1016/j.neucom.2025.129923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the problem of reliable ℓ 2 - ℓ ∞ state estimation is addressed for discrete-time artificial neural networks with switched time-delays under weighted try-once-discard (WTOD) protocol. To mitigate data congestion and transmission burdens, the WTOD protocol is implemented in the transmission channels to optimize data communication, where the transmission priority is dynamically determined based on mission importance. A Bernoulli-distributed stochastic variable with known statistical properties is introduced to model the switching behavior between the presence and absence of time-delays and a failure matrix is constructed to characterize potential failures affecting the received measurement data. The primary objective of this paper is to develop a state estimator that effectively performs the desired estimation task by thoroughly accounting for the combined effects of switched time-delays and the WTOD protocol. Specifically, by utilizing Lyapunov theory and matrix inequality techniques, the estimator parameters are meticulously derived to ensure exponentially mean-square stability and ℓ 2 - ℓ ∞ performance. Finally, the efficacy and validity of the proposed algorithm are demonstrated through an illustrative example.},
  archive      = {J_NEUCOM},
  author       = {Yuqiang Luo and Siyu Guo and Di Zhao and Hong Lin},
  doi          = {10.1016/j.neucom.2025.129923},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129923},
  shortjournal = {Neurocomputing},
  title        = {Reliable ℓ2−ℓ∞ state estimation for delayed neural networks under weighted try-one-discard protocol},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-paced learning for anchor-based multi-view clustering:
A progressive approach. <em>NEUCOM</em>, <em>635</em>, 129921. (<a
href="https://doi.org/10.1016/j.neucom.2025.129921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-view clustering (MVC), the surge in data has led to a significant increase in both the number of samples and the complexity of feature spaces, posing considerable challenges to the domain. Traditional anchor-based clustering methods effectively reduce the time and space complexity of algorithms by selecting representative samples to reconstruct similarity matrices. However, these methods are susceptible to the influence of low-quality anchors. To address this issue, we propose a novel self-paced learning for anchor-based MVC method, termed MSPA. This approach begins by constructing an anchor alternative pool, a novel strategy for selecting anchors that captures both intra-view and inter-view structural information. Subsequently, the concept of self-paced learning (SPL) is employed to progressively integrate anchors of varying quality into the model learning process, thereby constructing an anchor graph. Finally, the K-Means algorithm is applied to the resulting feature matrix to infer the final clustering results. Comprehensive comparative analyses conducted on eight benchmark datasets demonstrate that our proposed method outperforms existing state-of-the-art MVC algorithms in terms of efficiency.},
  archive      = {J_NEUCOM},
  author       = {Xia Ji and Xinran Cheng and Peng Zhou},
  doi          = {10.1016/j.neucom.2025.129921},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129921},
  shortjournal = {Neurocomputing},
  title        = {Self-paced learning for anchor-based multi-view clustering: A progressive approach},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LGKGR: A knowledge graph reasoning model using LLMs
augmented GNNs. <em>NEUCOM</em>, <em>635</em>, 129919. (<a
href="https://doi.org/10.1016/j.neucom.2025.129919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph reasoning (KGR) aims to infer new factual knowledge based on existing structured factual data, and plays a vital role in various applications. Graph neural networks (GNNs)-based methods have garnered attention due to their exceptional capabilities in learning graph structures. However, they cannot effectively leverage rich text semantics within KG for reasoning. Given the remarkable semantic understanding capabilities of large language models (LLMs), this paper proposes a novel KGR model using LLMs augmented GNNs (LGKGR), which aims to utilize LLMs to enhance the graph structure learning of GNNs. Each round of reasoning includes three stages: path search, path pruning, and path decision. The first stage adopts an incremental path search strategy to identify adjacent entities of current query entity and extract features. The second stage adopts GNNs as a pruning tool to filter out semantically irrelevant reasoning paths. The third stage exploits LLMs for semantic analysis of candidate reasoning paths, and then selects the most possible reasoning paths. In the end, LLMs are further exploited to analyze the semantic information of reasoning paths and generate final reasoning results. Experimental results on public datasets demonstrate that the proposed method achieves an average improvement of 2.1% in the MRR metric and 2.68% in the Hits@1 metric compared to existing SOTA methods. Explainable reasoning justifications are also generated during the reasoning process.},
  archive      = {J_NEUCOM},
  author       = {Yuanming Zhang and Wenbo Zheng and Jiacheng Huang and Gang Xiao},
  doi          = {10.1016/j.neucom.2025.129919},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129919},
  shortjournal = {Neurocomputing},
  title        = {LGKGR: A knowledge graph reasoning model using LLMs augmented GNNs},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SARFormer: Segmenting anything guided transformer for
semantic segmentation. <em>NEUCOM</em>, <em>635</em>, 129915. (<a
href="https://doi.org/10.1016/j.neucom.2025.129915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation plays a crucial role in robotic systems. Despite advances, we find that current state-of-the-art methods are hard to apply in practice due to their weak generalization ability. Especially, diffusion-based segmentation methods struggle with over-reliance on noisy Ground Truth (GT) annotations, which are corrupted with noise and directly fed into the model’s forward propagation process during training, limiting the model’s ability to generalize. While the Segment Anything Model (SAM) excels at instance segmentation, it faces challenges in controlling granularity and lacks semantic information. To address these issues, we propose SARFormer, a semantic segmentation algorithm guided by SAM. Unlike conventional methods, SARFormer uses GT solely for supervision and replaces noisy GT with SAM guidance, enabling better generalization. The key innovations include a region-based SAM optimizer to refine granularity and a feature aggregation method for enhanced deep feature extraction. Experimental results show SARFormer achieves competitive accuracy, demonstrating the effectiveness of SAM in improving segmentation performance},
  archive      = {J_NEUCOM},
  author       = {Lixin Zhang and Wenteng Huang and Bin Fan},
  doi          = {10.1016/j.neucom.2025.129915},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129915},
  shortjournal = {Neurocomputing},
  title        = {SARFormer: Segmenting anything guided transformer for semantic segmentation},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DACFusion: Dual asymmetric cross-attention guided feature
fusion for multispectral object detection. <em>NEUCOM</em>,
<em>635</em>, 129913. (<a
href="https://doi.org/10.1016/j.neucom.2025.129913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective fusion of unique features from different spectra plays a crucial role in multispectral object detection. Recent research has focused on transplanting advanced methods from other multimodal fusion fields to multispectral object detection tasks. These fusion methods focus on the fusion of features and ignore the spatial correspondence between multispectral images. This lack of correspondence in turn limits the full utilization of the complementarities between the different modalities, which affects the accuracy of object detection. To address this problem, we creatively propose a dual asymmetric cross-attention multispectral fusion (DACFusion) method, which is able to process features interactively based on the positional correspondence between two spectra, and then asymmetrically fuses the multispectral data according to the characteristics of each spectrum to take advantage of their complementary strengths. Meanwhile, we introduce a large selective kernel network to expand the receptive field for object detection, which further improves the detection accuracy. Experimental results on the VEDAI and LLVIP datasets validate the significant performance advantages of our proposed method and show its applicability to a variety of practical application scenarios. Code will be available at https://github.com/wood-fish/DACFusion .},
  archive      = {J_NEUCOM},
  author       = {Jingchen Qian and Baiyou Qiao and Yuekai Zhang and Tongyan Liu and Shuo Wang and Gang Wu and Donghong Han},
  doi          = {10.1016/j.neucom.2025.129913},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129913},
  shortjournal = {Neurocomputing},
  title        = {DACFusion: Dual asymmetric cross-attention guided feature fusion for multispectral object detection},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From separation to fusion: Screening-assisted bilevel
collaborative evolutionary optimization for railway freight allocation.
<em>NEUCOM</em>, <em>635</em>, 129910. (<a
href="https://doi.org/10.1016/j.neucom.2025.129910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient freight space allocation and stowage planning are critical for optimizing transportation efficiency and minimizing operational costs in railway transportation systems of large-scale enterprises. Traditional methods typically handle freight space allocation and stowage decisions in isolation or by simply layering these processes, leading to suboptimal results in terms of transportation efficiency and operational costs. To address this issue, this paper proposes a novel screening-assisted bilevel collaborative evolutionary optimization (Sa-BCEO) algorithm to explore and fusion the interdependencies between freight space allocation and stowage problems, thereby improving transportation efficiency. First, a screening-assisted mechanism (SAM) is designed to alleviate the complexity of the nested structure of bilevel optimization. This mechanism narrows the search space by retaining individuals with higher potential in the upper-level optimization, thereby enhancing efficiency in solving the lower-level optimization problem. Then, a bilevel framework is constructed to optimize the freight allocation and stowage. The effectiveness of the Sa-BCEO algorithm is validated through extensive experiments on a real-world enterprise dataset and two random datasets. Extensive results demonstrate significant improvements in transportation efficiency and cost reduction compared to traditional optimization methods.},
  archive      = {J_NEUCOM},
  author       = {Yiyin Tang and Yalin Wang and Chenliang Liu and Yong Wang and Weihua Gui},
  doi          = {10.1016/j.neucom.2025.129910},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129910},
  shortjournal = {Neurocomputing},
  title        = {From separation to fusion: Screening-assisted bilevel collaborative evolutionary optimization for railway freight allocation},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RA2SP: Transform readability assessment to sequence
prediction with a loss function incorporating penalty mechanism.
<em>NEUCOM</em>, <em>635</em>, 129909. (<a
href="https://doi.org/10.1016/j.neucom.2025.129909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of text readability assessment, traditional methods often rely on complex linguistic feature engineering, which increases the complexity of feature extraction and analysis while not necessarily improving model performance. To address these challenges, this work proposes a novel sequence prediction framework named RA2SP, which transforms the text readability classification task into a sequence prediction task. By introducing a penalty mechanism in the loss function, RA2SP reduces the reliance on burdensome linguistic feature engineering. The framework solely depends on deep features extracted from large pre-trained language models as input. Experimental results across various public datasets demonstrate that RA2SP achieves state-of-the-art or competitive performance across various evaluation metrics compared to existing baseline models. Additionally, this work provides a theoretical analysis of the role of the proposed penalty mechanism during model training, offering theoretical support and validating its effectiveness through experimental results. Finally, the potential applicability and superiority of the RA2SP framework are showcased in other text classification tasks, highlighting its adaptability and scalability.},
  archive      = {J_NEUCOM},
  author       = {Yijun Chen and Yurui Zheng and Jianhui Xu and Shaohong Zhang},
  doi          = {10.1016/j.neucom.2025.129909},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129909},
  shortjournal = {Neurocomputing},
  title        = {RA2SP: Transform readability assessment to sequence prediction with a loss function incorporating penalty mechanism},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handling language prior and compositional reasoning issues
in visual question answering system. <em>NEUCOM</em>, <em>635</em>,
129906. (<a href="https://doi.org/10.1016/j.neucom.2025.129906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) models often suffer from language bias, favoring common but incorrect answers, and struggle with compositional reasoning in complex queries. This paper proposes a unified approach using a multimodal large language model enhanced with adaptive prompts designed for specific tasks. Our method directly addresses these issues by reducing language bias and improving compositional reasoning. Extensive evaluations on benchmark datasets, including VQA v2.0, VQACP, TDIUC, GQA, Visual7 W, TextVQA, and STVQA show that our approach outperforms state-of-the-art models, achieving accuracy improvements of 8% to 9%. These results demonstrate the effectiveness of our method in enhancing VQA accuracy, making it a significant advancement for more reliable and robust applications in real-world scenarios.},
  archive      = {J_NEUCOM},
  author       = {Souvik Chowdhury and Badal Soni},
  doi          = {10.1016/j.neucom.2025.129906},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129906},
  shortjournal = {Neurocomputing},
  title        = {Handling language prior and compositional reasoning issues in visual question answering system},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-based optical flow: Method categorisation and review
of techniques that leverage deep learning. <em>NEUCOM</em>,
<em>635</em>, 129899. (<a
href="https://doi.org/10.1016/j.neucom.2025.129899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing new convolutional neural network architectures and event-based camera representations could play a crucial role in autonomous navigation, pose estimation, and visual odometry applications. This study explores the potential of event cameras in optical flow estimation using convolutional neural networks. We provide a detailed description of the principles of operation and the software available for extracting and processing information from event cameras, along with the various event representation methods offered by this technology. Likewise, we identify four method categories to estimate optical flow using event cameras: gradient-based, frequency-based, correlation-based and neural network models. We report on these categories, including their latest developments, current status and challenges. We provide information on existing datasets and identify the appropriate dataset to evaluate deep learning-based optical flow estimation methods. We evaluate the accuracy of the implemented methods using the average endpoint error metric; meanwhile, the efficiency of the algorithms is evaluated as a function of execution time. Finally, we discuss research directions that promise future advances in this field.},
  archive      = {J_NEUCOM},
  author       = {Robert Guamán-Rivera and Jose Delpiano and Rodrigo Verschae},
  doi          = {10.1016/j.neucom.2025.129899},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129899},
  shortjournal = {Neurocomputing},
  title        = {Event-based optical flow: Method categorisation and review of techniques that leverage deep learning},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level sparse network lasso: Locally sparse learning
with flexible sample clusters. <em>NEUCOM</em>, <em>635</em>, 129898.
(<a href="https://doi.org/10.1016/j.neucom.2025.129898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional learning usually assumes that all samples share the same global model, which fails to preserve critical local information for heterogeneous data. It can be tackled by detecting sample clusters and learning sample-specific models but is limited to sample-level clustering and sample-specific feature selection. In this paper, we propose multi-level sparse network lasso (MSN Lasso) for flexible local learning. It multiplicatively decomposes model parameters into two components: One component is for coarse-grained group-level, and another is for fine-grained entry-level. At the clustering stage, MSN Lasso simultaneously groups samples (group-level) and clusters specific features across samples (entry-level). At the feature selection stage, it enables both across-sample (group-level) and sample-specific (entry-level) feature selection. Theoretical analysis reveals a potential equivalence to a jointly regularized local model, which informs the development of an efficient algorithm. A divide-and-conquer optimization strategy is further introduced to enhance the algorithm’s efficiency. Extensive experiments across diverse datasets demonstrate that MSN Lasso outperforms existing methods and exhibits greater flexibility.},
  archive      = {J_NEUCOM},
  author       = {Luhuan Fei and Xinyi Wang and Jiankun Wang and Lu Sun and Yuyao Zhang},
  doi          = {10.1016/j.neucom.2025.129898},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129898},
  shortjournal = {Neurocomputing},
  title        = {Multi-level sparse network lasso: Locally sparse learning with flexible sample clusters},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in few-shot nested named entity recognition:
The efficacy of meta-learning convolutional approaches. <em>NEUCOM</em>,
<em>635</em>, 129893. (<a
href="https://doi.org/10.1016/j.neucom.2025.129893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Named Entity Recognition (NER) involves the identification of new entities using a limited amount of labeled data, which may contain nested entities. Currently, mainstream few-shot NER methods are not designed to handle nested entities. This study introduces a novel span-based meta-learning framework that uses meta-learning convolution to address the challenges of few-shot nested NER. Our proposed method, called M eta-Learning C onvolution for F ew- S hot N ested NER ( MCFSN ), is the first to integrate meta-learning with convolutional neural networks, effectively handling nested entities with limited training examples. This study presents a two-stage processing approach: extracting span features using CNN combined with the Biaffine attention mechanism, followed by entity span classification utilizing ProtoNet and the Biaffine classifier. Our experiments demonstrate consistently superior performance across three diverse language datasets, outperforming several competing baseline models in terms of F1 scores. Specifically, our approach achieves 6.9% F1 score improvement on the Genia, 5.2% F1 value improvement on the GermEval, and 4.5% F1 value enhancement on the NEREL, thus validating the effectiveness of our proposed approach.},
  archive      = {J_NEUCOM},
  author       = {Shuaichen Zhu and Yang Yuan and Lin Shi and Shoukun Xu},
  doi          = {10.1016/j.neucom.2025.129893},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129893},
  shortjournal = {Neurocomputing},
  title        = {Advancements in few-shot nested named entity recognition: The efficacy of meta-learning convolutional approaches},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCMVC: Dual contrastive multi-view clustering.
<em>NEUCOM</em>, <em>635</em>, 129889. (<a
href="https://doi.org/10.1016/j.neucom.2025.129889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which aims to divide data into different categories that are unsupervised in respect to information from different views, plays an important role in the field of computer vision. Contrastive learning is widely used in deep multi-view clustering methods to learn more discriminative representations. However, most existing multi-view clustering methods based on contrastive learning use only a single positive sample and do not fully utilize the category information in the learning process. To address the above issues, we propose a novel dual contrastive multi-view clustering (DCMVC) method, which uses pseudo-labels to refine the embedded features to make them more suitable for clustering tasks. Specifically, an inter-view correlation contrastive module is designed to learn more compact clustering assignments through a shared clustering prediction layer. Then, on the basis of the clustering predictions, we propose an intra-view consistency contrastive module, which dynamically selects the samples with the same pseudo label as positive samples and sets the other samples as negative samples to construct contrastive learning. The proposed model can alleviate the constraints of a single positive sample on contrastive learning by fully considering the latent category information to regularize the representation structure. Extensive experiments conducted on nine real datasets demonstrate the superiority of the proposed method.},
  archive      = {J_NEUCOM},
  author       = {Pengyuan Li and Dongxia Chang and Zisen Kong and Yiming Wang and Yao Zhao},
  doi          = {10.1016/j.neucom.2025.129889},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129889},
  shortjournal = {Neurocomputing},
  title        = {DCMVC: Dual contrastive multi-view clustering},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAD-DGTD: Multivariate time series anomaly detection based
on dynamic graph structure learning with time delay. <em>NEUCOM</em>,
<em>635</em>, 129887. (<a
href="https://doi.org/10.1016/j.neucom.2025.129887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection of multivariate time series data is extremely important in the industrial operation maintenance of Internet of Things (IoT). Researchers have found that the relationship between multiple sensors can be modeled as graph structure, and most researchers expresses this relationship by learning static graph structures which only contains the information of single modal. However, in actual IoT, the relationship between sensors will change with the changes of operating conditions, and this fixed graph structure cannot capture the relationship between sensors when working mode changes. To compensate the shortage of static graph, we propose a Multivariate time series Anomaly Detection framework based on Dynamic Graph learning with Time Delay (MAD-DGTD). Firstly, time-delay dynamic graph learning module (TDDG) is proposed to learn the changed mutual relationship between sensors over time and model it as a dynamic graph structure. In TDDG, a delay impact learning mechanism was designed to reconfigure the similarity calculation of node embeddings, which is designed to handle the temporal asynchrony of interactions between sensors in IoT. Secondly, we designed a stacked time dimension information extraction module (TDIE) and graph convolution information propagation module (GCIP) to capture information of different fine-grained sizes through multi-scale feature extraction. Finally, experimental research on three real-world datasets shows that our method outperforms the existing 10 competitive baselines in terms of overall performance.},
  archive      = {J_NEUCOM},
  author       = {Kang Wang and Jun Kong and Meicheng Zhang and Min Jiang and Tianshan Liu},
  doi          = {10.1016/j.neucom.2025.129887},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129887},
  shortjournal = {Neurocomputing},
  title        = {MAD-DGTD: Multivariate time series anomaly detection based on dynamic graph structure learning with time delay},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint subspace learning and subspace clustering based
unsupervised feature selection. <em>NEUCOM</em>, <em>635</em>, 129885.
(<a href="https://doi.org/10.1016/j.neucom.2025.129885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection (UFS) has become a focal point of extensive research due to its ability to reduce the dimensionality of unlabeled data. Currently, many UFS methods based on subspace learning embed multiple graph regularization terms to preserve the local similarity structure of samples or features and rarely consider exploring global structure simultaneously, such as the self-representation structure between features and the potential clustering structure of samples. We propose a novel UFS model based on subspace learning and subspace orthogonal basis clustering (JSLSC) to address this problem. First, through robust subspace learning, JSLSC explores the self-representation information between the selected features and the original feature space. Features’ local and global structures are learned through feature selection and self-representation structure learning. Secondly, orthogonal basis clustering is introduced to learn the potential clustering structure in the low-dimensional sample space, thus enabling subspace clustering. Thirdly, hard-constrained graph structure learning is introduced to adaptively maintain the local structural consistency between low-dimensional samples and original samples. Finally, an optimization algorithm and convergence proof are proposed, and the superiority of the JSLSC is demonstrated through comparative experiments on nine real datasets.},
  archive      = {J_NEUCOM},
  author       = {Zijian Xiao and Hongmei Chen and Yong Mi and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
  doi          = {10.1016/j.neucom.2025.129885},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129885},
  shortjournal = {Neurocomputing},
  title        = {Joint subspace learning and subspace clustering based unsupervised feature selection},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The consistency analysis of gradient learning under
independent covariate shift. <em>NEUCOM</em>, <em>635</em>, 129883. (<a
href="https://doi.org/10.1016/j.neucom.2025.129883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many issues have drawn significant attention to gradient learning (GL), which seeks to approximate the gradient of the target function. Despite rapid progress, the existing methods on GL are almost based on the strict assumption that the samples are independent and identically distributed (i.i.d.) drawn. In this paper, we go beyond the classical i.i.d. framework and propose to investigate the GL under the independent covariate shift (i.c.s.) assumption. To be specific, we establish the upper bound of generalization error from the viewpoint of function approximation and show its theoretical consistency under a mild regularity condition on the bounded density-ratio, which generalizes the classical GL results under the i.i.d. framework. In addition, we have discovered a real-world example which meets the i.c.s. assumption. The numerical studies on the synthetic and real-world examples validate the effectiveness of proposed approach on the i.c.s. setting.},
  archive      = {J_NEUCOM},
  author       = {Liyuan Liu and Hong Chen and Chi Xiao and Weifu Li},
  doi          = {10.1016/j.neucom.2025.129883},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129883},
  shortjournal = {Neurocomputing},
  title        = {The consistency analysis of gradient learning under independent covariate shift},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EKCA-cap: Improving dense captioning via external knowledge
and context awareness. <em>NEUCOM</em>, <em>635</em>, 129867. (<a
href="https://doi.org/10.1016/j.neucom.2025.129867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense captioning is a image-to-text task that aims to locate the key semantic areas of an image and describe them in natural language. Previous researches have made great progresses, but this task remains challenging as it often generates ambiguous sentences due to weak perception of the attributes and semantic relationships of visual objects ( e.g. , holding something green in hands rather than cutting green vegetables). Furthermore, there is a lack of coherence between the sentences generated by each Region of Interest (RoI) and the context in the input image. Due to dense captioning being a multimodal task, the lack of interaction between text and visual modalities in the network can also lead to low accuracy. To tackle these challenges, we propose a dense captioning architecture with External Knowledge and Context Awareness, namely EKCA-Cap. Specifically, we construct a novel Common-Sense Knowledge Graph (CSKG) to provide attribute information of visual objects and semantic relationships with other visual cues. In addition, a Contextual Extractor (CE) is designed to extract contextual features of the target region from both spatial and semantic levels. Finally, a Dual-Stream Interactive Module (DSIM) is introduced to maximize the interaction between visual and text knowledge modalities. Extensive comparison and ablation experiments on Visual Genome (VG) and RefCOCOg datasets demonstrate the effectiveness of our method.},
  archive      = {J_NEUCOM},
  author       = {Zhenwei Zhu and Fengyu Zhou and Saike Huang and Teng Li},
  doi          = {10.1016/j.neucom.2025.129867},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129867},
  shortjournal = {Neurocomputing},
  title        = {EKCA-cap: Improving dense captioning via external knowledge and context awareness},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSNet: A new small object attention based deep classifier
for presence of gun in complex scenes. <em>NEUCOM</em>, <em>635</em>,
129855. (<a href="https://doi.org/10.1016/j.neucom.2025.129855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motivation for focusing on weapon-based scene classification stems from the critical need to enhance public safety by enabling automated systems to quickly and accurately detect firearms in various environments. In contrast to common surveillance scenario classification based on intruders, weapon-based scenario classification often involves small weapons distributed throughout the scene or image. This requires more discriminative features and local semantics for effective classification. However, when deep convolutional neural networks (CNNs) are applied to scene classification, the loss of low- and mid-level features cannot be avoided. Furthermore, most existing networks tend to emphasize the global semantics of images. The low inter-class variability and high intra-class variability present specific challenges in weapon-based scene classification. To address these challenges, we propose a small object attention-based architecture in this work, with DenseNet serving as the backbone of our classification model. We modified the original DenseNet architecture to obtain more structured features. Additionally, we introduce a Small Object Attention (SAN) module after each dense block and an enhancement layer after each transition layer. Furthermore, we propose an enhanced classification layer in place of the traditional softmax layer, which helps retain relevant semantic features during classification. Consequently, the proposed classification model processes small patches of the image, preserving the relevant features of weapons. Experiments on six widely used benchmark datasets for weapon-based scenes demonstrate that our GSNet outperforms state-of-the-art methods by a significant margin while utilizing considerably fewer parameters. On average, the DenseNet model achieved an accuracy of 94.2%, whereas the proposed network attained an average accuracy of 98% across the six datasets.},
  archive      = {J_NEUCOM},
  author       = {Rajib Debnath and Kakali Das and Mrinal Kanti Bhowmik},
  doi          = {10.1016/j.neucom.2025.129855},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129855},
  shortjournal = {Neurocomputing},
  title        = {GSNet: A new small object attention based deep classifier for presence of gun in complex scenes},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on dynamic scene understanding using temporal
knowledge graphs: From scene knowledge representation to extrapolation.
<em>NEUCOM</em>, <em>635</em>, 129854. (<a
href="https://doi.org/10.1016/j.neucom.2025.129854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic scene understanding is the process of extracting information from video, identifying and inferring entities and relations within the scene, with the aim of thoroughly analyzing complex scenes that evolve over time. This process leverages temporal knowledge graphs to achieve a deep and comprehensive understanding of dynamic environments and is widely applied in areas such as autonomous driving, surveillance, and video analysis. Initially, scene knowledge representation is explored as the foundational step in dynamic scene understanding, achieved through the generation of temporal knowledge graphs. These graphs are categorized based on temporal granularity. Temporal knowledge graphs are divided into multiple-frame dynamic graphs and single-frame dynamic graphs. The generation methods for multiple-frame dynamic graphs are categorized into fragment-based and sliding-window approaches, while single-frame dynamic graphs primarily utilize transformer-based methods. This section provides an overview of the generation models for temporal knowledge graphs. Subsequently, dynamic scenes are further analyzed using extrapolation methods, which are classified into entity-based and relation-based modeling approaches. Entity-based modeling methods mainly include temporal point processes and graph neural network techniques, while relation-based modeling focuses on reinforcement learning and meta-learning techniques. This section summarizes various existing extrapolation techniques within these categories. Finally, the paper discusses the challenges associated with temporal knowledge graphs and explores potential research directions, offering insights into future advancements in dynamic scene understanding.},
  archive      = {J_NEUCOM},
  author       = {Linnan Lu and Guannan Si and Xinyu Liang and Mingshen Li and Fengyu Zhou},
  doi          = {10.1016/j.neucom.2025.129854},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129854},
  shortjournal = {Neurocomputing},
  title        = {A survey on dynamic scene understanding using temporal knowledge graphs: From scene knowledge representation to extrapolation},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFP-DETR: Marine UAV target detection based on multi-scale
fuzzy perception. <em>NEUCOM</em>, <em>635</em>, 129843. (<a
href="https://doi.org/10.1016/j.neucom.2025.129843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An end-to-end object detection algorithm based on multi-scale fuzzy perception and feature enhancement, MFP-DETR, is proposed to solve the problem of image blur caused by the fast motion of Unmanned Aerial Vehicle (UAV) and pixel limitation of cameras in Unmanned Surface Vehicle (USV) images. Firstly, a learnable image mapping network — LPNet is designed to improve the representation of the target region and the quality of the input image. Secondly, the features of S3 and S4 and the features of S5 in the backbone Network were extracted, and they were input into the Squeeze and Excitation Network V2 (SENetV2) feature fusion module for feature fusion to enhance the detection ability of small targets. Finally, replace the Attention on the Intermediate Feature Interaction (AIFI) module in the Real-Time DEtection TRansformer (RT-DETR) with a Contextual Transformer(CoT) module. Context information is fully used to guide the learning of dynamic attention matrix and enhance the ability of global visual representation, to improve the accuracy of the model for different targets. The experimental results show that this method can enhance the fuzzy image effectively and detect the UAV accurately under background interference in the natural environment.},
  archive      = {J_NEUCOM},
  author       = {Ting Zou and Quanbo Ge and Yanjun Huang},
  doi          = {10.1016/j.neucom.2025.129843},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129843},
  shortjournal = {Neurocomputing},
  title        = {MFP-DETR: Marine UAV target detection based on multi-scale fuzzy perception},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RagNet3D: Learning distinguishable representation for pooled
grids in 3D object detection. <em>NEUCOM</em>, <em>635</em>, 129841. (<a
href="https://doi.org/10.1016/j.neucom.2025.129841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection plays a crucial role in autonomous driving. Previous two-stage 3D detectors have developed Grid-based Region-of-Interest (RoI) Pooling techniques, such as RoI-Grid Pooling and Voxel Pooling, both of which quantify RoI into grids. However, the grids are usually ambiguous since parts of them pertain to multiple RoIs that identify a single object. To address this issue, we propose a RoI-Aware Grids Pooling Network (RagNet3D), which introduces RoI-View context to create distinguishable grid representations. Specifically, we present a RoI-View Prediction module that predicts RoI-View context via the guidance of the distance between RoIs and objects. Meanwhile, we propose a Couple-View Fusion module that propagates the probabilistic distribution, calculated from the RoI-View context, into the RoI-irrelevant grid features for further box refinement. Extensive experiments on KITTI and Waymo Open Dataset show that our method achieves remarkable improvements against the baselines.},
  archive      = {J_NEUCOM},
  author       = {Jiaxin Chen and Yuehui Han and Zhiqiang Yan and Jianjun Qian and Jun Li and Jian Yang},
  doi          = {10.1016/j.neucom.2025.129841},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129841},
  shortjournal = {Neurocomputing},
  title        = {RagNet3D: Learning distinguishable representation for pooled grids in 3D object detection},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning for drone detection from images: A review
of techniques and challenges. <em>NEUCOM</em>, <em>635</em>, 129823. (<a
href="https://doi.org/10.1016/j.neucom.2025.129823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs), popularly known as drones, have revolutionized many sectors. In precision agriculture, they are used to effectively sprinkle water, fertilizers, and pesticides. In cinematography, UAVs snap aerial images that were difficult or impossible to obtain in the past. However, just as they can be used for good, they also have the potential for malicious uses. For example, drug smugglers use drones to evade border surveillance and push their goods across countries. Additionally, in the hands of militants, drones can be used to launch ballistics on targets, leading to the loss of lives and properties. Thus, researchers have recently focused on designing automated tools to detect friendly from unfriendly drones. One effective tool for such is Machine Learning (ML). This paper reviews works that use ML to detect drones from images. The images include visible light, infrared, and thermal. After studying the papers, we present the taxonomy and trends in the field. In addition, we also provide open research issues: the development of lightweight models, the use of synthetic data, the adoption of auto-annotation models, and the employment of transformer-based models.},
  archive      = {J_NEUCOM},
  author       = {Abubakar Bala and Ali H. Muqaibel and Naveed Iqbal and Mudassir Masood and Diego Oliva and Mujaheed Abdullahi},
  doi          = {10.1016/j.neucom.2025.129823},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129823},
  shortjournal = {Neurocomputing},
  title        = {Machine learning for drone detection from images: A review of techniques and challenges},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified concept-based system for local, global, and
misclassification explanations. <em>NEUCOM</em>, <em>635</em>, 129761.
(<a href="https://doi.org/10.1016/j.neucom.2025.129761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability of Deep Neural Networks (DNNs) has garnered increasing attention in recent years. Of the various explainability approaches, concept-based techniques stand out for their ability to utilize human-meaningful concepts instead of focusing solely on individual pixels. However, there is a scarcity of methods that consistently provide both local and global explanations. Moreover, most of the methods have no offer to explain misclassification cases. Considering these challenges, we present a unified concept-based system for unsupervised learning of local and global concepts. Our primary objective is to uncover the intrinsic concepts underlying each data category by training a surrogate explainer network to estimate the concepts’ importance. Our experimental results substantiated the efficacy of the discovered concepts through diverse quantitative and qualitative assessments, encompassing faithfulness, completeness, and generality. Furthermore, our approach facilitates the explanation of both accurate and erroneous predictions, rendering it a valuable tool for comprehending the characteristics of the target objects of different classes.},
  archive      = {J_NEUCOM},
  author       = {Fatemeh Aghaeipoor and Dorsa Asgarian and Mohammad Sabokrou},
  doi          = {10.1016/j.neucom.2025.129761},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129761},
  shortjournal = {Neurocomputing},
  title        = {A unified concept-based system for local, global, and misclassification explanations},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural headline generation: A comprehensive survey.
<em>NEUCOM</em>, <em>635</em>, 129633. (<a
href="https://doi.org/10.1016/j.neucom.2025.129633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic headline generation (HG) is an important natural language processing (NLP) task that aims to obtain a highly compressed text snippet from a document, to exhibit the core concept. Traditional headline generation (HG) techniques predominantly employ text summarization methods to generate short texts, by selecting important information from original documents. In recent years, with the rapid development of deep learning techniques, research on HG has leaned toward neural network-based end-to-end modeling approaches. Pretrained schemes and large language models (LLMs) demonstrate superior capability in generating natural language texts, thereby promoting further exploration on HG studies. However, a quality gap remains between machine-generated and human-written texts, making the generation of attractive and faithful headlines worthy of in-depth research. Therefore, this study presents a review of the most recent technologies on HG, including methods, datasets, and evaluation strategies. Future research directions are outlined, which provide a valuable reference point for HG and other summarization tasks. A collection of reference papers and code sources is available at: https://github.com/xiaona-chang/HGSurvey .},
  archive      = {J_NEUCOM},
  author       = {Han Ren and Xiaona Chang and Xia Li},
  doi          = {10.1016/j.neucom.2025.129633},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129633},
  shortjournal = {Neurocomputing},
  title        = {Neural headline generation: A comprehensive survey},
  volume       = {635},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked graph autoencoder-based multi-agent dynamic
relational inference model for trajectory prediction. <em>NEUCOM</em>,
<em>634</em>, 129922. (<a
href="https://doi.org/10.1016/j.neucom.2025.129922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic relational inference models uncover potential complex system interactions, enabling trajectory prediction and improving the interpretability of underlying system dynamics. However, the existing models cannot accurately infer the structural evolution trends and complete dynamic processes of temporal networks. Additionally, when uncertain noisy data are input, more serious graph noise problems, including redundant and noisy edges, occur, undermining the stability of interaction inference and reducing the accuracy of trajectory prediction. Therefore, a masked graph autoencoder-based multi-agent dynamic relational inference (MGAE-MDRI) trajectory prediction model is proposed herein. The mask reconstruction module is integrated into MDRI, where the partial edges of the interaction graph, representing multi-agent dynamic evolution, are masked through sampling. The reconstruction strategy leverages path and degree considerations to mitigate the impact of graph noise on the network topology. Furthermore, a graph attention network-based path sampler with a preference random walk is introduced, effectively combining network topology and node attribute features to construct a topologically weighted degree matrix and assign optimal mask sampling weights to neighboring nodes. Experiments conducted on four standard public datasets demonstrate that MGAE-MDRI outperforms the state-of-the-art models, achieving better trajectory prediction robustness and for complex multi-agent systems.},
  archive      = {J_NEUCOM},
  author       = {Fuyuan Zhao and Xiangang Cao and Jiangbin Zhao and Yong Duan and Xin Yang and Xinyuan Zhang},
  doi          = {10.1016/j.neucom.2025.129922},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129922},
  shortjournal = {Neurocomputing},
  title        = {Masked graph autoencoder-based multi-agent dynamic relational inference model for trajectory prediction},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain state model: A novel method to represent the
rhythmicity of object-specific selective attention from
magnetoencephalography data. <em>NEUCOM</em>, <em>634</em>, 129920. (<a
href="https://doi.org/10.1016/j.neucom.2025.129920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object-specific selective attention can be achieved either by simultaneously splitting attention to multiple objects, or by sequentially shifting spatial attention among objects. A growing body of research show that object-specific selective attention can be implemented using the second way and that the sequential movement of attention exhibits specific rhythmicity. However, the neurocomputing mechanisms underlying this phenomenon are still not fully understood. To clarify this issue, we conducted magnetoencephalography experiments on healthy participants and subsequently proposed a computational framework based on time-series decomposition and rhythmic analysis to delve into the neural mechanisms of object-specific selective attention. Our investigation reveals that the four single-object attention states are decodable on the level of magnetoencephalography (MEG) sensor signals. Furthermore, these states manifest dynamically and rhythmically during object-specific selective attention. These findings suggest that the attentional rhythm exhibited by neural activity during object-specific selective attention is fundamentally characterized by a set of basic attentional units. This research provides valuable information for future investigations into the brain model of object-specific selective attention.},
  archive      = {J_NEUCOM},
  author       = {Chunyu Liu and Xin-Yue Yang and Xueyuan Xu},
  doi          = {10.1016/j.neucom.2025.129920},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129920},
  shortjournal = {Neurocomputing},
  title        = {Brain state model: A novel method to represent the rhythmicity of object-specific selective attention from magnetoencephalography data},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GDoT: A gated dual domain transformer for enhanced MRI
off-resonance correction. <em>NEUCOM</em>, <em>634</em>, 129918. (<a
href="https://doi.org/10.1016/j.neucom.2025.129918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based MRI reconstruction methods have gained significant attention recently due to the need for accelerated MRI scans. However, existing deep learning-based methods for off-resonance correction rely on simple CNNs, resulting in suboptimal solutions. In this paper, we propose a gated dual domain transformer with gated spatial projection and gated frequency projection to effectively handle complex-valued MRI, as the first attempt to utilize transformer-based model for off-resonance correction. Additionally, we introduce a selective perceptual loss with a novel test-time translation-merger to reconstruct perceptually high-quality images without checkerboard artifacts. Experiments on both simulated and real off-resonance MRI datasets demonstrate the effectiveness of our approach. Furthermore, we also present ablation studies to determine the optimal design choices.},
  archive      = {J_NEUCOM},
  author       = {Jaesin Ahn and Heechul Jung},
  doi          = {10.1016/j.neucom.2025.129918},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129918},
  shortjournal = {Neurocomputing},
  title        = {GDoT: A gated dual domain transformer for enhanced MRI off-resonance correction},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving spatial-temporal PDEs with arbitrary boundary
conditions using physics-constrained convolutional recurrent neural
networks. <em>NEUCOM</em>, <em>634</em>, 129917. (<a
href="https://doi.org/10.1016/j.neucom.2025.129917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inception of physics-constrained or physics-informed machine learning represents a paradigm shift, addressing the challenges associated with data scarcity and enhancing model interpretability. This innovative approach incorporates the fundamental laws of physics as constraints, guiding the training process of machine learning models. In this work, the physics-constrained convolutional recurrent neural network is further extended for solving spatial-temporal partial differential equations with arbitrary boundary conditions. Two notable advancements are introduced: the implementation of boundary conditions as soft constraints through finite difference-based differentiation, and the establishment of an adaptive weighting mechanism for the optimal allocation of weights to various losses. These enhancements significantly augment the network&#39;s ability to manage intricate boundary conditions and expedite the training process. The efficacy of the proposed model is validated through its application to two-dimensional phase transition, fluid dynamics, and reaction-diffusion problems, which are pivotal in materials modeling. Compared to traditional physics-constrained neural networks, the physics-constrained convolutional recurrent neural network demonstrates a tenfold increase in prediction accuracy within a similar computational budget. Moreover, the model&#39;s exceptional performance in extrapolating solutions for the Burgers&#39; equation underscores its utility. Therefore, this research establishes the physics-constrained recurrent neural network as a viable surrogate model for sophisticated spatial-temporal PDE systems, particularly beneficial in scenarios plagued by sparse and noisy datasets.},
  archive      = {J_NEUCOM},
  author       = {Guangfa Li and Yanglong Lu and Dehao Liu},
  doi          = {10.1016/j.neucom.2025.129917},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129917},
  shortjournal = {Neurocomputing},
  title        = {Solving spatial-temporal PDEs with arbitrary boundary conditions using physics-constrained convolutional recurrent neural networks},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSQN: Robust path planning of mobile robot based on deep
spiking q-network. <em>NEUCOM</em>, <em>634</em>, 129916. (<a
href="https://doi.org/10.1016/j.neucom.2025.129916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of science and technology, the field of mobile robot applications continues to expand, with path planning emerging as a fundamental yet challenging task. While traditional path planning techniques have developed into a relatively complete theoretical system, their performance in uncertain environments remains a critical area of research. To address this, we propose a novel Deep Spiking Q-Network (DSQN) algorithm that significantly enhances path planning performance by leveraging the unique advantages of spiking neural networks (SNNs). Unlike classic Q-learning and its contemporary variants, the DSQN algorithm integrates global and local information simultaneously, resulting in superior overall performance. As the third generation of neural networks, SNNs offer unparalleled robustness and energy efficiency by mimicking biological neural systems. By introducing spiking neurons into the conventional Deep Q-learning (DQN) framework, the DSQN algorithm overcomes key challenges in deep reinforcement learning (DRL), such as limited robustness and high energy consumption. The DSQN training process incorporates both surrogate gradient learning (SGL) and ANN-to-SNN conversion techniques, with SGL demonstrating remarkable effectiveness in mobile robot path planning tasks. Experimental results validate the practicality and efficiency of DSQN, showcasing improved performance across diverse test scenarios compared to the original DQN algorithm. These findings highlight the potential of DSQN to advance path planning in complex and uncertain environments, establishing it as a robust and energy-efficient solution for mobile robotics.},
  archive      = {J_NEUCOM},
  author       = {Aakash Kumar and Lei Zhang and Hazrat Bilal and Shifeng Wang and Ali Muhammad Shaikh and Lu Bo and Avinash Rohra and Alisha Khalid},
  doi          = {10.1016/j.neucom.2025.129916},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129916},
  shortjournal = {Neurocomputing},
  title        = {DSQN: Robust path planning of mobile robot based on deep spiking Q-network},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent progress in digital twin-driven fault diagnosis of
rotating machinery: A comprehensive review. <em>NEUCOM</em>,
<em>634</em>, 129914. (<a
href="https://doi.org/10.1016/j.neucom.2025.129914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of the Internet of Things (IoT) and industrial automation has propelled diagnostics into the age of intelligence and digitization. Equipment fault diagnosis is predicted to shift from &quot;preventive&quot; to &quot;predictive&quot; through digital twin (DT) technology, which creates a virtual mirror of the physical entity and simulates its operating state in a genuine operating environment. The operation, development history, and current state of application of DT technology are investigated through a literature review, highlighting the technology&#39;s enormous promise in intelligent fault diagnostics. Then, from the four fields of aerospace, transportation, industrial machinery, and energy equipment, the current status of domestic and international research and the latest research developments on the application of DT technology to implement intelligent fault diagnosis (IFD) of equipment are reviewed. On this basis, the limitations of the current application of DT technology in the field of fault diagnosis are analyzed, and the challenges and development trends of the future application of DT technology in the implementation of IFD are discussed and pointed out, providing a clear direction for intelligent fault diagnosis of machinery.},
  archive      = {J_NEUCOM},
  author       = {Pengbo Zhang and Renxiang Chen and Lixia Yang and Ye Zou and Liang Gao},
  doi          = {10.1016/j.neucom.2025.129914},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129914},
  shortjournal = {Neurocomputing},
  title        = {Recent progress in digital twin-driven fault diagnosis of rotating machinery: A comprehensive review},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid safe reinforcement learning: Tackling distribution
shift and outliers with the student-t’s process. <em>NEUCOM</em>,
<em>634</em>, 129912. (<a
href="https://doi.org/10.1016/j.neucom.2025.129912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safe reinforcement learning (SRL) aims to optimize control policies that maximize long-term reward, while adhering to safety constraints. SRL has many real-world applications such as, autonomous vehicles, industrial robotics, and healthcare. Recent advances in offline reinforcement learning (RL) — where agents learn policies from static datasets without interacting with the environment — have made it a promising approach to derive safe control policies. However, offline RL faces significant challenges, such as covariate shift and outliers in the data, which can lead to suboptimal policies. Similarly, online SRL, which derives safe policies through real-time environment interaction, struggles with outliers and often relies on unrealistic regularity assumptions, limiting its practicality. This paper addresses these challenges by proposing a hybrid-offline–online approach. First, prior knowledge from offline learning guides online exploration. Then, during online learning, we replace the popular Gaussian Process (GP) with the Student-t’s Process (TP) to enhance robustness to covariate shift and outliers.},
  archive      = {J_NEUCOM},
  author       = {Xavier Hickman and Yang Lu and Daniel Prince},
  doi          = {10.1016/j.neucom.2025.129912},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129912},
  shortjournal = {Neurocomputing},
  title        = {Hybrid safe reinforcement learning: Tackling distribution shift and outliers with the student-t’s process},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient motor imagery electroencephalogram classification
via cross tensor coupling decomposition based on augmented covariance
networks. <em>NEUCOM</em>, <em>634</em>, 129911. (<a
href="https://doi.org/10.1016/j.neucom.2025.129911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge in fully harnessing the potential of motor imagery-based brain-computer interfaces (MI-BCIs) is the accurate classification of MI electroencephalography (MI-EEG) signals. Traditional methods of analyzing MI-EEG signals, such as spatial pattern analysis, focus solely on the average linear correlation within a specific time frame of the signals. To address these limitations, we introduce an effective classification approach named &quot;Cross Tensor Coupling Decomposition (CTCD) based on Augmented Covariance Networks (ACNs)&quot;. Based on a specific embedding of the original system, we introduce the ACN, which can be conceptualized as a standard covariance network computed within a high-dimensional space. It not only integrates the spatiotemporal characteristics of the signals but also enhances the representation of nonlinear information. By constructing ACN tensors and applying CTCD, we rapidly and accurately extract multidimensional deep features from different categories of MI tasks, significantly enhancing decoding accuracy. Validating on public datasets, we achieved an average accuracy of 91.75 % ± 3.25 % on Dataset 1 and 85.86 % ± 7.93 % on Dataset 2 in subject-independent binary classification experiments, and for cross-subject binary classification experiments, we obtained an average accuracy of 93.91 % ± 8.71 % on Dataset 1 and 84.36 % ± 16.63 % on Dataset 2. These results highlight the robustness and superiority of our approach. Furthermore, our method enables rapid decoding and real-time feedback, enhancing the practical application of MI-BCIs.},
  archive      = {J_NEUCOM},
  author       = {Hechong Su and Jieren Xie and Zengyao Yang and Yuncheng Ge and Jingya Fu and Chengxi Xie and Kai Zhang and Xinyi Hu and Sicong Zhang and Guanghua Xu},
  doi          = {10.1016/j.neucom.2025.129911},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129911},
  shortjournal = {Neurocomputing},
  title        = {Efficient motor imagery electroencephalogram classification via cross tensor coupling decomposition based on augmented covariance networks},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-shot phase-shifting composition fringe projection
profilometry by multi-attention fringe restoration network.
<em>NEUCOM</em>, <em>634</em>, 129908. (<a
href="https://doi.org/10.1016/j.neucom.2025.129908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D measurement techniques have permeated sectors like intelligent manufacturing and online inspection, demanding higher real-time performance in industrial applications. Recently, deep-learning-based profilometry (DLP) has gained significant attention due to the powerful feature extraction capabilities of convolutional neural networks, enabling single-shot projection to meet both real-time and robustness requirements in dynamic measurement scenarios. However, while these methods yield promising results, they predominantly emphasize model improvements (e.g., optimizing module functionality or expanding network parameters) often neglecting the parallel integration with traditional physical models and the optimization of input features. To overcome these limitations, a novel DLP that combines parallel phase-shifting theory with a fringe restoration network has been proposed and leverages a high-quality dataset, balancing the trade-offs between projection efficiency and reconstruction accuracy: (1) A learning-based spatial composite phase-shifting profilometry (LSCPP) is proposed, marking the first application of equal thickness interference theory from digital holography to DLP. This mitigates pixel loss caused by frequency-domain filtering in FTP while ensuring high-precision fringe reconstruction without auxiliary templates; (2) A multi-exposure dataset is constructed, accompanied by the introduction of a novel evaluation metric, modulation error rate (MER), for performance assessment; (3) Extensive experiments are conducted across both static and dynamic scenarios, utilizing multi-exposure and public datasets, and are compared with state-of-the-art DLPs. It demonstrates the proposed LSCPP’s real-time capability and generalization performance in dynamic measurement environments.},
  archive      = {J_NEUCOM},
  author       = {Jiayi Qin and Yansong Jiang and Yiping Cao and Haitao Wu},
  doi          = {10.1016/j.neucom.2025.129908},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129908},
  shortjournal = {Neurocomputing},
  title        = {Single-shot phase-shifting composition fringe projection profilometry by multi-attention fringe restoration network},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parallel neural network based structural anomaly
detection: Leveraging time-frequency domain features. <em>NEUCOM</em>,
<em>634</em>, 129907. (<a
href="https://doi.org/10.1016/j.neucom.2025.129907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural anomaly detection is essential for ensuring the safety, reliability, and longevity of building engineering. By identifying deviations from normal patterns, it enables early intervention and prevents potential failures. However, most existing methods rely on a single feature extracted from the time domain or the frequency domain. Time-domain features alone are insufficient to capture variations in the frequency components, while frequency-domain features fail to account for transient behaviours in the time domain. This limitation significantly reduces detection performance, particularly when dealing with nonlinear and non-stationary signals. To address the issues, this study proposes a new framework for anomaly detection using parallel time convolutional networks (TCN) and wavelet decomposition based convolutional neural networks (WD-CNN), termed PTWC. In this framework, time-frequency domain features are achieved by utilizing TCN to capture time-domain features and WD-CNN to capture frequency-domain features, distinguishing structural anomaly patterns. The proposed PTWC framework is validated on an actual three-story frame structure. Compared with ten baseline methods, the experimental results demonstrate that PTWC has high accuracy and achieves at least 7 % improvement in area under the curve (AUC) scores, thus confirming its superior performance in structural anomaly detection.},
  archive      = {J_NEUCOM},
  author       = {Yingying He and Bo Yang and Weihong Jin and Likai Zhang and Hongyang Chen},
  doi          = {10.1016/j.neucom.2025.129907},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129907},
  shortjournal = {Neurocomputing},
  title        = {A parallel neural network based structural anomaly detection: Leveraging time-frequency domain features},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing fourier neural operators with CNNs architectures:
Pooling, groupwise convolution and inverted block. <em>NEUCOM</em>,
<em>634</em>, 129905. (<a
href="https://doi.org/10.1016/j.neucom.2025.129905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we hypothesize that effective architectural configurations from Convolutional Neural Networks (CNNs) can significantly enhance the performance of the Fourier Neural Operator (FNO). Pooling layers, such as max-pooling and average-pooling in CNNs, play a key role in improving the learnability of nonlocal features in images. However, despite its resemblance to CNNs, FNO lacks such mechanisms due to two primary challenges: (1) applying traditional pooling layers violates discretization invariance, and (2) the fixed size of pooling windows cannot be pre-defined because of variable data resolutions. To address these issues, we propose a novel “pooling operator” for the FNO architecture that preserves discretization invariance and is differentiable with respect to window size. Additionally, we adapt groupwise convolution and the inverted residual block from CNNs to FNOs. Together, the pooling operator, groupwise convolution, and inverted residual block play a pivotal role in achieving superior performance. Benchmark experiments on the 2D Navier–Stokes and 1D Burger’s equation datasets demonstrate the effectiveness of our proposed architecture.},
  archive      = {J_NEUCOM},
  author       = {Seungtae Park and Heejoon Jeon and Hyung Ju Hwang},
  doi          = {10.1016/j.neucom.2025.129905},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129905},
  shortjournal = {Neurocomputing},
  title        = {Enhancing fourier neural operators with CNNs architectures: Pooling, groupwise convolution and inverted block},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label-only model inversion attacks: Adaptive boundary
exclusion for limited queries. <em>NEUCOM</em>, <em>634</em>, 129902.
(<a href="https://doi.org/10.1016/j.neucom.2025.129902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that privacy data in deep learning models is vulnerable to attacks by adversaries who can restore private images even with access only to the labels of DNN models. However, existing similar attacks rely heavily on extensive access to the target model, making them easily detectable by defenders. This paper introduces an Adaptive Boundary Exclusion method, called ABE-MI, that achieves a high success rate with minimal access attempts. The core idea of our algorithm is to estimate the gradient of the target point through Gaussian sampling that progressively expands outward, dynamically adjusting the movement step of the data point to reach the centroid of the decision area quickly. Experiments show that our algorithm can attack both CNN and ViT models, and its performance on ViT models far surpasses that of the state-of-the-art (SOTA) algorithms. Moreover, compared to the best current label-only algorithms, our approach requires significantly fewer queries and even exceeds the performance of white-box attacks on other metrics. Furthermore, our algorithm remains effective on tabular data.},
  archive      = {J_NEUCOM},
  author       = {Jiayuan Wu and Chang Wan and Hao Chen and Zhonglong Zheng and Yaxin Sun},
  doi          = {10.1016/j.neucom.2025.129902},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129902},
  shortjournal = {Neurocomputing},
  title        = {Label-only model inversion attacks: Adaptive boundary exclusion for limited queries},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature representation fidelity preservation during neural
network pruning for enhanced compression efficiency. <em>NEUCOM</em>,
<em>634</em>, 129901. (<a
href="https://doi.org/10.1016/j.neucom.2025.129901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning is a widely used technique for compressing convolutional neural networks (CNNs), primarily by identifying and eliminating redundant filters or channels. However, existing pruning criteria often fail to consider inter-channel interactions, leading to inaccurate importance scores. Furthermore, many criteria focus solely on the immediate influence of channels, neglecting their contributions to higher-level feature representations. These limitations hinder CNNs&#39; ability to retain critical information after pruning. In this paper, we propose a novel criterion to address these issues. Our method highlights how feature information loss occurs due to inter-channel dependencies during forward propagation, which affects the accuracy of feature-based importance scores. To mitigate this, we introduce a per-layer channel contribution analysis that captures channel-layer interactions, ensuring more precise pruning decisions. Extensive experiments on image classification (CIFAR-10, ImageNet), object detection (PASCAL VOC, COCO 2017), and 3D point cloud analysis demonstrate the effectiveness of our approach. For instance, our method achieves a 52.6 % reduction in floating-point operations (FLOPs) in ResNet-56 on CIFAR-10 while improving accuracy by 0.4 %, outperforming existing pruning techniques. On ImageNet, pruning ResNet-50 results in a 55.5 % FLOPs reduction with only a 0.72 % top-1 accuracy drop, surpassing prior methods. Additionally, pruning MobileNet-V2 achieves a 31.5 % FLOPs reduction with a higher top-1 accuracy than alternative approaches. These results confirm that our method enhances compression efficiency while preserving feature representation fidelity, making it a robust solution for CNN model compression.},
  archive      = {J_NEUCOM},
  author       = {Ambuj},
  doi          = {10.1016/j.neucom.2025.129901},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129901},
  shortjournal = {Neurocomputing},
  title        = {Feature representation fidelity preservation during neural network pruning for enhanced compression efficiency},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic neighborhood selection for regularized local linear
embedding with elastic networks. <em>NEUCOM</em>, <em>634</em>, 129900.
(<a href="https://doi.org/10.1016/j.neucom.2025.129900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The processing of high-dimensional data is of paramount importance in the fields of machine learning and pattern recognition. Local linear embedding (LLE), a popular nonlinear dimensionality reduction method, has gained attention for maintaining the local structure of data in low-dimensional space. However, traditional LLE algorithms have limitations in dealing with datasets that are non-uniformly distributed or noisy. To address this issue, we propose an elastic network regularized LLE algorithm with dynamic neighborhood selection (DNS-ENRLLE). The objective is to enhance the adaptability and robustness of the algorithm through a dynamic neighborhood selection mechanism and a regularization strategy. A series of experiments on synthetic and real datasets demonstrate that our method outperforms existing LLE and other nonlinear dimension reduction methods. In particular, the improved method demonstrates significant performance improvements in the analysis of non-uniformly distributed data, effectively enhancing the accuracy and reliability of data analysis results.},
  archive      = {J_NEUCOM},
  author       = {Jiaqi Cheng and Chaojie Wang},
  doi          = {10.1016/j.neucom.2025.129900},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129900},
  shortjournal = {Neurocomputing},
  title        = {Dynamic neighborhood selection for regularized local linear embedding with elastic networks},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving GBDT performance on imbalanced datasets: An
empirical study of class-balanced loss functions. <em>NEUCOM</em>,
<em>634</em>, 129896. (<a
href="https://doi.org/10.1016/j.neucom.2025.129896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance poses a persistent challenge in machine learning, particularly for tabular data classification tasks. While Gradient Boosting Decision Trees (GBDT) models are widely regarded as state-of-the-art for these tasks, their effectiveness diminishes in the presence of imbalanced datasets. This paper is the first to comprehensively explore the integration of class-balanced loss functions into three popular GBDT algorithms, addressing binary, multi-class, and multi-label classification. We present a novel benchmark, derived from extensive experiments across diverse datasets, to evaluate the performance gains from class-balanced losses in GBDT models. Our findings establish the efficacy of these loss functions in enhancing model performance under class imbalance, providing actionable insights for practitioners tackling real-world imbalanced data challenges. To bridge the gap between research and practice, we introduce an open-source Python package that simplifies the application of class-balanced loss functions within GBDT workflows, democratizing access to these advanced methodologies. The code is available at https://github.com/Luojiaqimath/ClassbalancedLoss4GBDT .},
  archive      = {J_NEUCOM},
  author       = {Jiaqi Luo and Yuan Yuan and Shixin Xu},
  doi          = {10.1016/j.neucom.2025.129896},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129896},
  shortjournal = {Neurocomputing},
  title        = {Improving GBDT performance on imbalanced datasets: An empirical study of class-balanced loss functions},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Unsupervised infrared–visible person re-identification by
multi-level dual-stream contrastive learning. <em>NEUCOM</em>,
<em>634</em>, 129895. (<a
href="https://doi.org/10.1016/j.neucom.2025.129895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Visible–Infrared Person Re-identification (USL-VI-ReID) aims to learn a label-free cross-modal retrieval model to reduce reliance on expensive manual annotations for cross-modal data. Previous works on USL-VI-ReID tend to generate sample labels from the perspective of inter-modality contrastive learning, while neglecting the information differences between cameras, resulting in low credibility of the generated sample labels. Additionally, traditional inter-modality contrastive learning methods use shared feature encoders, which cannot learn personalized modality features. To address these issues, this paper proposes a Dual-Stream Contrastive Learning Method (DCM) that jointly learns personalized feature encoders from both inter-modality and inter-camera perspectives, generating more reliable sample labels, which we refer to as a Multi-level Dual-Stream Contrastive Learning Method. We first establish a Dual-Stream Contrastive Learning (DCL) pre-training framework, which includes three different feature encoders for learning pedestrian representations within specific modalities, and optimize these encoders through contrastive learning using all training data. Then, based on these three feature encoders, we construct a Multi-level Dual-Stream Contrastive Learning framework (MLA) for cross-camera to cross-modal feature clustering and pseudo-label generation. This enables MLA to focus more on learning hierarchical domain information during subsequent model training, combining multi-level feature information aggregation to obtain more accurate cross-modal pseudo-labels. Finally, to further ensure the semantic consistency of the two modal labels, we insert a Cross-Modality Alignment Module (CMAM) between modalities to further improve the recognition accuracy of USL-VI-ReID. Numerous experiments demonstrate that the proposed method stands out among various USL-VI-ReID approaches, exhibiting promising performance.},
  archive      = {J_NEUCOM},
  author       = {Yifeng Zhang and Canlong Zhang and Haifei Ma and Zhixin Li and Zhiwen Wang and Chunrong Wei},
  doi          = {10.1016/j.neucom.2025.129895},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129895},
  shortjournal = {Neurocomputing},
  title        = {Unsupervised infrared–visible person re-identification by multi-level dual-stream contrastive learning},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSPL: Multi-granularity semantic prototype learning for
occluded person re-identification. <em>NEUCOM</em>, <em>634</em>,
129894. (<a href="https://doi.org/10.1016/j.neucom.2025.129894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although person re-identification has made remarkable progress in recent years, it remains a challenging problem in occluded scenes. Some existing popular methods attempt to decouple the human local features directly by clustering semantics to counter occlusion. However, due to the lack of good semantic learning strategies, the network will inevitably acquire non-discriminative local features. To effectively separate body parts and eliminate occlusion interference, we propose a Multi-granularity Semantic Prototype Learning (MSPL) network based on a superior semantic learning strategy. MSPL mainly consists of a prototype embedding encoder and a multi-granularity prototype decoder. With the help of our proposed Dual Parallel Attention (DPA) module and easy-to-hard learning strategy, the encoder can gradually aggregate semantics and effectively separate discriminative fine-grained features. We also propose a Pose-guided Feature Enhancement (PFE) module and a Prototype Cyclic Grouping (PCG) module in the decoder to resist background clutter and enhance network robustness for complex occlusion scenes. Finally, we design a Prototype Discriminability (PD) loss to reduce the focused redundancy to increase the discrepancy between the focused areas of prototypes. Extensive experimental results on several challenging benchmark datasets show that our MSPL achieves excellent performance and good generalization ability.},
  archive      = {J_NEUCOM},
  author       = {Zhihao Li and Huaxiang Zhang and Lei Zhu and Jiande Sun and Li Liu},
  doi          = {10.1016/j.neucom.2025.129894},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129894},
  shortjournal = {Neurocomputing},
  title        = {MSPL: Multi-granularity semantic prototype learning for occluded person re-identification},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with noisy labels for classifying biological echoes
in polarimetric weather radar observations using artificial neural
networks. <em>NEUCOM</em>, <em>634</em>, 129892. (<a
href="https://doi.org/10.1016/j.neucom.2025.129892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of biological echoes in radar data has revolutionized research into airborne migratory species. Deep learning applied to polarimetric weather radar observations can reveal signature patterns of mass movement by bio-scatterers such as birds, bats, and insects. However, due to the difficulties in labelling bio-scatterers in these data, threshold approaches have been proposed in the literature. In this research, we used the depolarization ratio (DR) based on differential reflectivity (zDR) and the cross-correlation coefficient (pHV), along with citizen scientist-reported data, to label bio-scatterers for deep learning. This method of labelling biological echoes in radar signatures is prone to noise, which impacts the accuracy of any model that relies on it. We introduce a novel semi-supervised co-training approach that uses a bootstrap ensemble with a confidence threshold. Our ensemble consists of the newly proposed STNet and two modified FNet models, which incorporate co-learning through bootstrap sampling for label correction. This innovative method significantly improves classification accuracy across all three multivariate numerical datasets compared to baseline models that lack co-learning with bootstrap-based label correction.},
  archive      = {J_NEUCOM},
  author       = {John Atanbori and Christos A. Frantzidis and Mohammed Al-Khafajiy and Aliyu Aliyu and Behnaz Sohani and Kofi Appiah and Harriet Moore and Catherine Sanders and Alastair I. Ward},
  doi          = {10.1016/j.neucom.2025.129892},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129892},
  shortjournal = {Neurocomputing},
  title        = {Learning with noisy labels for classifying biological echoes in polarimetric weather radar observations using artificial neural networks},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilingual neural machine translation for low-resource
languages by twinning important nodes. <em>NEUCOM</em>, <em>634</em>,
129890. (<a href="https://doi.org/10.1016/j.neucom.2025.129890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilingual neural machine translation (MNMT) is a novel machine translation approach that benefits from large multilingual resources. However, its performance drops significantly when training with low-resource languages due to the reliance on parameter sharing and data size. In this paper, a new method is proposed to improve the performance of MNMT for a pair of languages where the target language is low-resource. The main idea of this study is to find important nodes that have parameters connected to them that negatively affect an MNMT model and then split those nodes into two sub nodes. Then, the model selects the important sub node that has an effect on the specific language pair to create a twin sub node. This twin sub node helps to strengthen the translation quality of the specific language pair without having a negative effect on other languages. The proposed method works in four steps as: 1) training an MNMT model with parameter sharing over multiple languages, 2) selecting important nodes which negatively affect the MNMT, 3) splitting important nodes into sub nodes, and 4) Twining important sub nodes. The proposed method has been evaluated using several multilingual datasets, including TED 2013, TED 2020, and BIBLE, by examining English-Persian language as a case study. The obtained results show that the proposed method yields the best results for one-to-many and many-to-many models according to the average BLEU value and semantic similarity . The results also show that the proposed method has given better results than other well-known large language models, such as ChatGPT, BING GPT4, and the Google Neural Machine Translation (GNMT) model, when applied to a low-resource language.},
  archive      = {J_NEUCOM},
  author       = {Abouzar Qorbani and Reza Ramezani and Ahmad Baraani and Arefeh Kazemi},
  doi          = {10.1016/j.neucom.2025.129890},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129890},
  shortjournal = {Neurocomputing},
  title        = {Multilingual neural machine translation for low-resource languages by twinning important nodes},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal and local correlations based network for multivariate
time series classification. <em>NEUCOM</em>, <em>634</em>, 129884. (<a
href="https://doi.org/10.1016/j.neucom.2025.129884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, time series classification has attracted the attention of a large number of researchers, and hundreds of methods have been proposed. However, these methods often ignore the spatial correlations among dimensions and the local correlations among features. To address this issue, the causal and local correlations based network (CaLoNet) is proposed in this study for multivariate time series classification. First, pairwise spatial correlations between dimensions are modeled using causality modeling to obtain the graph structure. Then, a relationship extraction network is used to fuse local correlations to obtain long-term dependency features. Finally, the graph structure and long-term dependency features are integrated into the graph neural network. Experiments on the UEA datasets show that CaLoNet can obtain competitive performance compared with state-of-the-art methods.},
  archive      = {J_NEUCOM},
  author       = {Mingsen Du and Yanxuan Wei and Xiangwei Zheng and Cun Ji},
  doi          = {10.1016/j.neucom.2025.129884},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129884},
  shortjournal = {Neurocomputing},
  title        = {Causal and local correlations based network for multivariate time series classification},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSSFN: A multi-scale sequence fusion network for CT-based
diagnosis of pulmonary complications. <em>NEUCOM</em>, <em>634</em>,
129878. (<a href="https://doi.org/10.1016/j.neucom.2025.129878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulmonary complications exhibit similar symptoms, such as ground-glass opacities and patchy consolidations, posing significant challenges for manual diagnosis. In this paper, a novel Multi-Scale Sequence Fusion Network (MSSFN) designed for a small-scale Computed Tomography (CT) database is proposed for computer-aided identification of pulmonary complications. The MSSFN first applies a transfer learning mechanism that utilizes the convolutional neural network weights pre-trained on a large dataset, enabling optimization of both feature extraction efficiency and model accuracy. Next, the multi-scale feature fusion module uses different types of convolutional layers in parallel to extract and integrate high-level features, enabling the network to capture complex lung lesion characteristics from multiple perspectives and enhance information interaction across different layers. Finally, a sequence feature fusion module establishes three-dimensional connections within the CT data, ensuring comprehensive data fusion by effectively integrating spatial features across the sequence. The comprehensive experimental results demonstrate that, under the same experimental conditions, the MSSFN achieves an accuracy of 82.54% for identifying pulmonary complications in CT images, which exhibits superior performance compared to nine similar network structures as well as other four state-of-the-art deep learning models. Consequently, the proposed MSSFN demonstrates practical significance by providing radiologists with reliable tools for accurately distinguishing various pulmonary complications in CT images. It also holds theoretical value by advancing methods for constructing robust features with enhanced representational capabilities.},
  archive      = {J_NEUCOM},
  author       = {Hongfu Zeng and Xinyu Li and Haipeng Xu and Keyi Yu and Huihua Hu and Peishu Wu and Nianyin Zeng},
  doi          = {10.1016/j.neucom.2025.129878},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129878},
  shortjournal = {Neurocomputing},
  title        = {MSSFN: A multi-scale sequence fusion network for CT-based diagnosis of pulmonary complications},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance-oriented fault detection and fault-tolerant
control for nonlinear uncertain systems: Improved stochastic
configuration network-based methods. <em>NEUCOM</em>, <em>634</em>,
129869. (<a href="https://doi.org/10.1016/j.neucom.2025.129869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper primarily focuses on performance-oriented fault detection (FD) and fault-tolerant control (FTC) for nonlinear uncertain systems. To achieve this, we initially develop an improved stochastic configuration network-based (ISCN-based) method for modeling nonlinear uncertain systems, leveraging the supervision mechanism and incremental construction techniques. It is followed by a performance indicator to enhance the robustness against the uncertainties and the sensitivity to the faults, which is further implemented for performance-oriented FD purpose. Subsequently, an adaptive FTC method is proposed to recover the nonlinear system performance, and associated with it, the configuration of the fault-tolerant controller parameters is investigated. Finally, a case study on the continuous stirred-tank reactor (CSTR) system is presented to showcase the efficacy and advantage of the proposed methods.},
  archive      = {J_NEUCOM},
  author       = {Jingjing Gao and Feng Gao and Zhengxuan Zhang and Xu Yang and Jian Huang and Kaixiang Peng},
  doi          = {10.1016/j.neucom.2025.129869},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129869},
  shortjournal = {Neurocomputing},
  title        = {Performance-oriented fault detection and fault-tolerant control for nonlinear uncertain systems: Improved stochastic configuration network-based methods},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPRInT: Scaling programmatic reasoning for INstruction
tuning in mathematics. <em>NEUCOM</em>, <em>634</em>, 129868. (<a
href="https://doi.org/10.1016/j.neucom.2025.129868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present SPRInT , a novel approach for large-scale, cost-effective synthesis of instruction-tuning datasets, leveraging Program-of-Thoughts (PoT) to enhance mathematical reasoning capabilities. Through the SPRInT framework, we synthesized data from seven high-quality open-source math datasets (including GSM8K, MATH, AQuA), and developed InfinityMATH -a dataset containing over 100,000 samples generated from QA pairs, offering extensive coverage across various mathematical domains. The SPRInT model series, fine-tuned on InfinityMATH using open-source language and code models such as Llama2-7B, Mistral-7B, and CodeLlama-7B, achieved remarkable improvements in mathematical reasoning, with performance gains between 184.7% and 514.3%. In zero-shot settings, our SPRInT -CodeLlama-7B model surpassed MAmmoTH-Coder on widely-used benchmarks, including GSM8K (65.80% vs. 56.86%) and MATH (34.06% vs. 29.88%). To assess logical consistency in numerical transformations, we created the GSM8K+ and MATH＋ test sets by modifying the numerical values in the original datasets. While traditional models struggled with these alterations, the SPRInT models exhibited superior robustness. The InfinityMATH dataset is publicly available at https://huggingface.co/datasets/BAAI/InfinityMATH .},
  archive      = {J_NEUCOM},
  author       = {Yan Yan and Lin Li and Bo-Wen Zhang},
  doi          = {10.1016/j.neucom.2025.129868},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129868},
  shortjournal = {Neurocomputing},
  title        = {SPRInT: Scaling programmatic reasoning for INstruction tuning in mathematics},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCSA: Exploring the synergistic effects between spatial and
channel attention. <em>NEUCOM</em>, <em>634</em>, 129866. (<a
href="https://doi.org/10.1016/j.neucom.2025.129866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel and spatial attentions have respectively brought significant improvements in extracting feature dependencies and spatial structure relations for various downstream vision tasks. The combined use of both channel and spatial attentions is widely considered beneficial for further performance improvement; however, the synergistic effects between channel and spatial attentions, especially in terms of spatial guidance and mitigating semantic disparities, have not yet been thoroughly studied. This motivates us to propose a novel Spatial and Channel Synergistic Attention module (SCSA), entailing our investigation toward the synergistic relationship between spatial and channel attentions at multiple semantic levels. Our SCSA consists of two parts: the Shareable Multi-Semantic Spatial Attention (SMSA) and the Progressive Channel-wise Self-Attention (PCSA). SMSA integrates multi-semantic information and utilizes a progressive compression strategy to inject discriminative spatial priors into PCSA’s channel self-attention, effectively guiding channel recalibration. Additionally, the robust feature interactions based on the Channel-wise single-head self-attention mechanism in PCSA further mitigate the disparities in multi-semantic information among different sub-features within SMSA. We conduct extensive experiments on seven benchmark datasets, including classification on ImageNet-1K, object detection on MSCOCO, segmentation on ADE20K, and four other complex scene detection datasets. Our results demonstrate that our proposed SCSA not only surpasses the current plug-and-play state-of-the-art attention but also exhibits enhanced generalization capabilities across various task scenarios. The code and models are available at: https://github.com/HZAI-ZJNU/SCSA .},
  archive      = {J_NEUCOM},
  author       = {Yunzhong Si and Huiying Xu and Xinzhong Zhu and Wenhao Zhang and Yao Dong and Yuxing Chen and Hongbo Li},
  doi          = {10.1016/j.neucom.2025.129866},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129866},
  shortjournal = {Neurocomputing},
  title        = {SCSA: Exploring the synergistic effects between spatial and channel attention},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment analysis applications using deep learning
advancements in social networks: A systematic review. <em>NEUCOM</em>,
<em>634</em>, 129862. (<a
href="https://doi.org/10.1016/j.neucom.2025.129862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is required to extract insights from social media content affecting decision-making and personalized services. The enormous volume of social network information has to be technically processed to extract relevant knowledge. Sentiment analysis is the most widely used method for this purpose. The current techniques of sentiment analysis have made significant progress in various fields. However, the potential of social networks to better understand human emotions and the recent advancements in deep learning necessitate the review and use of advanced sentiment analysis techniques that still require more attention from researchers in this field. In this regard, this review presents a systematic literature review (SLR) on the advancements of sentiment analysis using deep learning techniques in social networks from 2019 to May 2024. Furthermore, this review emphasizes that sentiment analysis can provide meaningful insights into information extracted from large and diverse datasets such as social media, which is extremely important for decision-making and personalized services. It also highlights mental health concerns as one of the windows into the emotional atmosphere of social networks. In addition, this SLR provides a technical taxonomy and comparison of various deep learning approaches. This SLR not only provides a comprehensive overview of the most advanced techniques and methodologies now used in sentiment analysis but also highlights forthcoming challenges and open issues that need to be addressed in the future. This study helps researchers and practitioners use deep learning to improve sentiment analysis applications and digital social well-being.},
  archive      = {J_NEUCOM},
  author       = {Erfan Bakhtiari Ramezani},
  doi          = {10.1016/j.neucom.2025.129862},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129862},
  shortjournal = {Neurocomputing},
  title        = {Sentiment analysis applications using deep learning advancements in social networks: A systematic review},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised visual similarity-based medical image retrieval
via dual-encoder and metric learning. <em>NEUCOM</em>, <em>634</em>,
129861. (<a href="https://doi.org/10.1016/j.neucom.2025.129861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer stands as one of the most lethal cancers, with its prognosis heavily reliant upon timely diagnosis. Medical image retrieval (MIR) techniques aim to retrieve images similar to the query one from a dataset, assisting doctors in disease diagnosis and clinical treatment programming. However, a significant challenge in dermatoscopic image interpretation stems from the heterogeneity of lesion visual appearance, which hinders existing methods from effectively balancing visual similarity with disease category. To address this issue, we propose an unsupervised approach for medical image retrieval to enhance the visual congruity of retrieval while ensuring disease accuracy.Dual-Encoder is trained to extract image features via a self-distilling dual network, and followed by subsequent refinement of the feature representation utilizing unsupervised metric learning methodologies. Our method is tested on the skin cancer dataset ISIC2019, evaluated using dual-dimensional metrics of visual and disease similarity. To assess visual similarity performance, we constructed a test visual dataset and categorize into 27 visual types we fine-tuned the pre-trained model on multiple datasets encompassing ultrasound, MRI and CT images to evaluate its applicability, which outperforms other advanced generalized retrieval methods. The experiments demostrated the effectiveness and generalization of our proposed method.},
  archive      = {J_NEUCOM},
  author       = {Xiya Weng and Yan Zhuang and Rui Wang and Ke Chen and Lin Han and Zhan Hua and Jiangli Lin},
  doi          = {10.1016/j.neucom.2025.129861},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129861},
  shortjournal = {Neurocomputing},
  title        = {Unsupervised visual similarity-based medical image retrieval via dual-encoder and metric learning},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based interactive knowledge distillation for social
relation continual learning. <em>NEUCOM</em>, <em>634</em>, 129860. (<a
href="https://doi.org/10.1016/j.neucom.2025.129860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multimedia advances, there is a growing need for machines to adeptly understand diverse social relations. Traditional methods for recognizing these relations, which are limited to a fixed number of classes, are ill-equipped for continual learning as new social interactions emerge. To address this prob-lem, we propose a pioneering Graph-based Interactive Knowledge Distillation (GI-KD) method for social relation continual learning. GI-KD, embedded in a class incremental learning structure, creates a balanced system where previously learned social relations and new knowledge are positioned at either end of the scale. The old and new knowledge is learned dynamically by adjusting the tilt of the balance. To achieve this balance, we propose a novel Libra loss function, which evaluate the relative contribution of old and new information and thus guides the adaptive fine-tuning of the model. We evaluate the GI-KD on three public social relation recognition (SRR) datasets, under different data distribution strategies. Our method shows a remarkable average 3.6% increase in incremental accuracy over current CIL techniques, effectively reducing catastrophic forgetting. Furthermore, GI-KD improves mAP and Acc by 4.6%, 5.4%, and 4.5%, respectively, compared to current CIL techniques, highlighting its strength in both continual learning and SRR.},
  archive      = {J_NEUCOM},
  author       = {Wang Tang and Linbo Qing and Pingyu Wang and Lindong Li and Yonghong Peng},
  doi          = {10.1016/j.neucom.2025.129860},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129860},
  shortjournal = {Neurocomputing},
  title        = {Graph-based interactive knowledge distillation for social relation continual learning},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample-pair learning network for extremely imbalanced
classification. <em>NEUCOM</em>, <em>634</em>, 129859. (<a
href="https://doi.org/10.1016/j.neucom.2025.129859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data classification, class-balanced data is ideal, but real datasets are often imbalanced, necessitating rebalancing through methods like resampling. In recent years, some new generative model-based resampling methods have been proposed. However, when facing extreme class imbalance, where the minority class is strongly underrepresented and on its own does not contain enough information to conduct the generative process. Some deep learning methods have been proposed to solve extremely imbalanced classification problems, but some of them are only used for specific datasets. Therefore, we proposed a novel deep learning method that combines a generative strategy with multi-task joint learning, termed sample-pair learning network (SPLN), for extremely imbalanced classification. The network consists of data preprocessing and multi-task joint learning modules. During data preprocessing, the training set is expanded by constructing positive and negative sample-pairs, then rebalanced using a strategy combining attention and resampling, termed undersampling based on attention power values (APVUS). The multi-task joint learning module employs a Siamese convolutional subnetwork to measure the similarity between sample-pairs and a multi-layer perceptron to recognize the category of single samples. The module can reduce the risk of overfitting caused by excessive noise in the training set. Finally, we designed a voting model based on the Siamese convolutional subnetwork to infer the categories of test samples. Experimental results demonstrate that our approach outperforms state-of-the-art generative model-based methods and is effective and general for extremely imbalanced classification.},
  archive      = {J_NEUCOM},
  author       = {Linjun Chen and Xiao-Yuan Jing and Runhang Chen and Fei Wu and Yongchang Ding and Changhui Hu and Ziyun Cai},
  doi          = {10.1016/j.neucom.2025.129859},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129859},
  shortjournal = {Neurocomputing},
  title        = {Sample-pair learning network for extremely imbalanced classification},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distantly supervised relation extraction with multi-level
contextual information integration. <em>NEUCOM</em>, <em>634</em>,
129858. (<a href="https://doi.org/10.1016/j.neucom.2025.129858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core objective of distantly Supervised relation extraction (DSRE) is to automatically and at scale extract relational information between two entities from unstructured text. However, the generation of noisy data in the automated labeling process is inevitable. To address the limitations of DSRE in terms of contextual information, current mainstream methods primarily focus on enriching entity representations through external knowledge bases. Nevertheless, there remains a wealth of untapped and rich information within the text itself, which plays a crucial role in helping the model to better understand the relationships between entities. In response to this, we propose a novel strategy that does not require the introduction of additional external knowledge. Instead, it cleverly leverages the principle of semantic similarity to construct contextually rich information that is highly semantically aligned with individual sentences. Furthermore, we incorporate information from Knowledge Graphs (KG) to achieve a deep integration of both textual and KG information. Specifically, we share contextual information at any level of the text with the KG information, merging them as the final bag-level representation for relation extraction. Experimental results demonstrate that updating the KG encoder using only the text encoder yields better performance. Furthermore, experiments conducted on two different domain-specific relation extraction datasets confirmed that our model achieved superior performance.},
  archive      = {J_NEUCOM},
  author       = {Danjie Han and Heyan Huang and Shumin Shi and Changsen Yuan and Cunhan Guo},
  doi          = {10.1016/j.neucom.2025.129858},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129858},
  shortjournal = {Neurocomputing},
  title        = {Distantly supervised relation extraction with multi-level contextual information integration},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal information fusion for multi-task end-to-end
behavior prediction in autonomous driving. <em>NEUCOM</em>,
<em>634</em>, 129857. (<a
href="https://doi.org/10.1016/j.neucom.2025.129857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Behavior prediction in autonomous driving is increasingly achieved through end-to-end frameworks that predict vehicle states from multi-modal information, streamlining decision-making and enhancing robustness in time-varying road conditions. This study proposes a novel multi-modal information fusion-based, multi-task end-to-end model that integrates RGB images, depth maps, and semantic segmentation data, enhancing situational awareness and predictive precision. Utilizing a Vision Transformer (ViT) for comprehensive spatial feature extraction and a Residual-CNN-BiGRU structure for capturing temporal dependencies, the model fuses spatiotemporal features to predict vehicle speed and steering angle with high precision. Through comparative, ablation, and generalization tests on the Udacity and self-collected datasets, the proposed model achieves steering angle prediction errors of MSE 0.012 rad, RMSE 0.109 rad, and MAE 0.074 rad, and speed prediction errors of MSE 0.321 km/h, RMSE 0.567 km/h, and MAE 0.373 km/h, outperforming existing driving behavior prediction models. Key contributions of this study include the development of a channel difference attention mechanism and advanced spatiotemporal feature fusion techniques, which improve predictive accuracy and robustness. These methods effectively balance computational efficiency and predictive performance, contributing to practical advancements in driving behavior prediction.},
  archive      = {J_NEUCOM},
  author       = {Guo Baicang and Liu Hao and Yang Xiao and Cao Yuan and Jin Lisheng and Wang Yinlin},
  doi          = {10.1016/j.neucom.2025.129857},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129857},
  shortjournal = {Neurocomputing},
  title        = {Multi-modal information fusion for multi-task end-to-end behavior prediction in autonomous driving},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free extended q-learning method for h∞ output tracking
control of networked control systems with network delays and packet
loss. <em>NEUCOM</em>, <em>634</em>, 129846. (<a
href="https://doi.org/10.1016/j.neucom.2025.129846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the extended Q-learning method is used to study the H ∞ output tracking control (HOTC) problem of networked control systems with state delay and data loss. Compared with the existing results, the network control system in this paper contains both network delays and packet loss, as well as external disturbances. To deal with the disturbances, the H ∞ control problem is transformed into the maximum and minimum value problem, which is solved by the method of zero-sum game. The packet loss and delay of the state make it difficult to obtain accurate current state information. Therefore, it is necessary to design a new smith predictor that contains delay and packet loss to predict the current state. Using the predicted state, the extended Q-learning algorithm is implemented to solve the H ∞ output tracking problem with unknown dynamics of the system. Then, the convergence of the extended Q-learning algorithm is proved. Moreover, the stability and optimality of the proposed method are analyzed in the theorems. Finally, numerical simulation is performed to verify the effectiveness of the proposed algorithm.},
  archive      = {J_NEUCOM},
  author       = {Longyan Hao and Chaoli Wang and Dong Liang and Shihua Li},
  doi          = {10.1016/j.neucom.2025.129846},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129846},
  shortjournal = {Neurocomputing},
  title        = {Model-free extended Q-learning method for h∞ output tracking control of networked control systems with network delays and packet loss},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H∞ human-assistance fuzzy control of discrete-time nonlinear
human-in-the-loop systems. <em>NEUCOM</em>, <em>634</em>, 129845. (<a
href="https://doi.org/10.1016/j.neucom.2025.129845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reflecting the change of human behavior on control performance of the human–machine collaboration is a difficult task. In this paper, an H ∞ human-assistance fuzzy control is developed for discrete-time nonlinear Markov jump human-in-the-loop (HiTL) systems. More specifically, to improve the human internal state, giving control input to human is considered in a controlled hidden Markov model utilized to describe human behavior. Then, a Takagi–Sugeno fuzzy model is employed to represent the nonlinear HiTL system. Based on this model and by using a stochastic Lyapunov functional, sufficient conditions for the existence of an H ∞ human-assistance fuzzy controller are provided in terms of linear matrix inequalities guaranteeing closed-loop stochastic stability of HiTL fuzzy system with an H ∞ control performance. Finally, simulation results on a single-degree-of-freedom vehicle with HiTL demonstrate that both adding the control input to the driver via the warning system and applying the suggested human-assistance fuzzy control to vehicle via automation are effective.},
  archive      = {J_NEUCOM},
  author       = {Xiu-Mei Zhang and Huai-Ning Wu},
  doi          = {10.1016/j.neucom.2025.129845},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129845},
  shortjournal = {Neurocomputing},
  title        = {H∞ human-assistance fuzzy control of discrete-time nonlinear human-in-the-loop systems},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved YOLOv7 for small object detection in airports:
Task-oriented feature learning with gaussian wasserstein loss and
attention mechanisms. <em>NEUCOM</em>, <em>634</em>, 129844. (<a
href="https://doi.org/10.1016/j.neucom.2025.129844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small objects in the airport scene, such as Persons and Vehicles, can lead to low accuracy and robustness of the object detection task. To address the above problems, an improved YOLOv7 model is proposed to detect certain objects on the airport scene. Considering the perspective distortion of the monitoring camera on the airport surface, the deformable convolutional feature extractor (DCFE) is first designed to adaptively extract features from input images for irregular occlusion objects. To learn task-oriented features from different channels, the attention mechanism is incorporated into the backbone to focus on informative concepts in a data-driven manner, formulating an attention feature extractor (AttFE). During the model training, the Normalized Gaussian Wasserstein distance (NWD) is considered as the loss function to measure the prediction errors after converting the bounding boxes into Gaussian distribution, thereby enhancing the ability to fit the small objects. A real-world airport surface dataset (ASD) is constructed to validate the proposed model. Extensive experimental results demonstrate that the proposed model outperforms selective baselines, achieving a 1.2% absolute improvement in mAP over the original YOLOv7 network. Experiments conducted on multiple common datasets and the results demonstrate that the proposed model exhibits superior performance in terms of mAP. All proposed technical modules contribute to expected performance improvement. Most importantly, the proposed model achieves higher performance for small objects and has the desired robustness over occluded objects.},
  archive      = {J_NEUCOM},
  author       = {Ruijie Peng and Chuanlin Liao and Weijun Pan and Xiaolin Gou and Jianwei Zhang and Yi Lin},
  doi          = {10.1016/j.neucom.2025.129844},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129844},
  shortjournal = {Neurocomputing},
  title        = {Improved YOLOv7 for small object detection in airports: Task-oriented feature learning with gaussian wasserstein loss and attention mechanisms},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conv-enhanced transformer and robust optimization network
for robust multimodal sentiment analysis. <em>NEUCOM</em>, <em>634</em>,
129842. (<a href="https://doi.org/10.1016/j.neucom.2025.129842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis seeks to utilize data from multiple modalities to enhance the overall performance of sentiment analysis. However, in real-world applications, missing modality data is an inevitable phenomenon. The semantic sparsity associated with missing data hinders existing methods from effectively capturing local dependencies among modalities, resulting in inefficiencies during cross-modal interactions. Furthermore, simplistic feature reconstruction strategies lead to suboptimal representation learning, rendering models susceptible to noise interference. To address these issues, we propose a novel Conv-Enhanced Transformer and Robust Optimization Network (CTRN) for robust multimodal sentiment analysis. Firstly, this method effectively models local dependencies by incorporating convolutional layers within the cross-modal Transformer, while exploring the intrinsic correlations between global multimodal context and local unimodal feature, thereby facilitating cross-modal fusion. Additionally, to enable effective representation learning and reduce the impact of noisy data, we implements Auxiliary Robust Optimization (ARO), integrating feature reconstruction and adversarial training to encourage the model to learn supplementary semantic information from the differences between complete and missing data. Finally, the introduction of momentum distillation further mitigates the influence of noisy data on model performance. Experimental results on three benchmark datasets demonstrate that our approach significantly enhances the performance of robust multimodal sentiment analysis.},
  archive      = {J_NEUCOM},
  author       = {Bin Sun and Li Jia and Yiming Cui and Na Wang and Tao Jiang},
  doi          = {10.1016/j.neucom.2025.129842},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129842},
  shortjournal = {Neurocomputing},
  title        = {Conv-enhanced transformer and robust optimization network for robust multimodal sentiment analysis},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memo-UNet: Leveraging historical information for enhanced
wave height prediction. <em>NEUCOM</em>, <em>634</em>, 129840. (<a
href="https://doi.org/10.1016/j.neucom.2025.129840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wave height prediction is a challenging spatiotemporal modeling task. It requires accurately capturing the dynamic evolution patterns of historical data. However, current methods show significant limitations in storing, retrieving, and utilizing historical information. These limitations hinder the learning ability of neural networks in critical temporal dynamics utilization of wave height prediction. Therefore, we propose a novel neural network Meme-Unet for wave height prediction. Specifically, to address the challenges in storing and retrieving historical information, we design a Memo module that adaptively stores historical data and retrieves key historical information through a minimum distance constraint. Additionally, to enhance the utilization effectiveness of historical information, we designed a TimeEncoding module with attention mechanisms to guide the model in better capturing temporal relationships. We conducted wave height prediction experiments in two different ocean regions, achieving the best MSE of 0.041 and a 6.8% improvement compared to state-of-the-art methods. This shows the advantage of the Memo-UNet’s design in capturing the dynamics of spatiotemporal modeling.},
  archive      = {J_NEUCOM},
  author       = {Teng Fang and Xiaojie Li and Canghong Shi and Xian Zhang and Wei Xiao and Yi Kou and Imran Mumtaz and Zhan ao Huang},
  doi          = {10.1016/j.neucom.2025.129840},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129840},
  shortjournal = {Neurocomputing},
  title        = {Memo-UNet: Leveraging historical information for enhanced wave height prediction},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard-label adversarial attack with dual-granularity
optimization on texts. <em>NEUCOM</em>, <em>634</em>, 129839. (<a
href="https://doi.org/10.1016/j.neucom.2025.129839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of artificial intelligence security research has led to the emergence of adversarial attack technology as a critical approach for identifying potential security vulnerabilities in artificial intelligence models. When targeting natural language processing models, conducting adversarial attacks in the hard-label setting presents a more practical and challenging black-box scenario due to the difficulty in computing gradients directly from discrete word sequences. Current textual adversarial attack methods are inefficient due to the lack of consideration for the limited number of queries available during the adversarial text generation process, creating a disparity between these approaches and real-world adversarial attack scenarios. To this end, this work proposes a dual-granularity optimization strategy that consists of a single-word semantic optimization and a multi-word joint semantic optimization procedure, and presents a query-efficient hard-label attack method called DualAttack by incorporating the proposed dual-granularity optimization strategy into the mutation and crossover process of the Genetic Algorithm framework. Extensive experimental results demonstrate that DualAttack can effectively produce high-quality adversarial texts with superior semantic similarity and minimal perturbation rates within fewer queries compared to existing methods in the hard-label setting.},
  archive      = {J_NEUCOM},
  author       = {Shilin Qiu and Qihe Liu and Shijie Zhou and Min Gou and Zhun Zhang and Zhewei Wu},
  doi          = {10.1016/j.neucom.2025.129839},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129839},
  shortjournal = {Neurocomputing},
  title        = {Hard-label adversarial attack with dual-granularity optimization on texts},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Momentum gradient-based untargeted poisoning attack on
hypergraph neural networks. <em>NEUCOM</em>, <em>634</em>, 129835. (<a
href="https://doi.org/10.1016/j.neucom.2025.129835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph Neural Networks (HGNNs) have been successfully applied in various hypergraph-related tasks due to their excellent higher-order representation capabilities. Unfortunately, recent works have shown deep learning models vulnerable to diverse attacks. Most studies of attacks on graphs have focused on Graph Neural Networks (GNNs), and the study of attacks on HGNNs remains largely unexplored. In this paper, we try to bridge this gap. We design a new untargeted poisoning attack for HGNNs, MGHGA, which focuses on modifying node features. We consider the process of HGNNs training and use a surrogate model to implement the attack before hypergraph modeling. Precisely, MGHGA consists of two parts: feature selection and feature modification. We use a momentum gradient mechanism to choose the attack node features in the feature selection module. In the feature modification module, we use two feature generation approaches (direct modification and sign gradient) to enable MGHGA to be employed on discrete and continuous datasets. We conduct extensive experiments on seven benchmark datasets to validate the attack performance of MGHGA in the node and the visual object classification tasks. The results show that MGHGA improves performance by an average of 2% compared to the baselines.},
  archive      = {J_NEUCOM},
  author       = {Yang Chen and Stjepan Picek and Zhonglin Ye and Zhaoyang Wang and Haixing Zhao},
  doi          = {10.1016/j.neucom.2025.129835},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129835},
  shortjournal = {Neurocomputing},
  title        = {Momentum gradient-based untargeted poisoning attack on hypergraph neural networks},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple-level enhanced graph convolutional network for
aspect sentiment triplet extraction. <em>NEUCOM</em>, <em>634</em>,
129834. (<a href="https://doi.org/10.1016/j.neucom.2025.129834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect Sentiment Triplet Extraction (ASTE) is a method for extracting aspect terms, opinion terms, and their corresponding sentiment polarities from a given sentence. Most of the existing studies use joint extraction methods to extract the triplets directly in a unified framework. However, most joint extraction methods only consider the semantic and syntactic dependency information of the sentence. Due to a lack of sentiment information and positional information, they are unable to accurately and completely express the aspect and opinion in the sentence. In order to solve the above problems, we introduce a Multiple-level Enhanced Graph Convolutional Network (MEGCN) for ASTE, which utilizes sentiment scores and sentiment polarity nodes alongside syntactic dependency information. This approach not only enriches contextual understanding by integrating sentiment data but also improves positional analysis of aspect and opinion terms through polarity nodes. Moreover, our dual-aware fusion module, combining semantic with sentiment-enhanced syntactic features through a biaffine attention mechanism and matrix construction, enables a deeper representation of aspect sentiment triplets. Our model demonstrates superior performance over existing methods on two widely recognized ASTE datasets.},
  archive      = {J_NEUCOM},
  author       = {Haowen Xu and Mingwei Tang and Tao Cai and Jie Hu and Zhongyuan Jiang and Deng Bian and Shixuan Lv},
  doi          = {10.1016/j.neucom.2025.129834},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129834},
  shortjournal = {Neurocomputing},
  title        = {Multiple-level enhanced graph convolutional network for aspect sentiment triplet extraction},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dense attention networks for texture classification.
<em>NEUCOM</em>, <em>634</em>, 129833. (<a
href="https://doi.org/10.1016/j.neucom.2025.129833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture plays a fundamental role in visual perception and is a key feature for classifying materials and surfaces in images. However, due to the limited number of training samples for each texture class, current deep learning-based methods struggle to capture diverse texture patterns effectively. To address this challenge, we propose a Dense Attention Networks (DANets), a novel architecture for texture classification that leverages both attention mechanisms and dense connectivity. DANet aims to enhance the ability to capture various texture patterns by constructing a mixed-scale, densely connected squeeze-and-excitation attention network. Additionally, we introduce a new Texture Loss function that incorporates dynamic regularization to improve robustness and generalization. To further boost performance, we propose Trident Feature Fusion (TFF), a simple yet effective method for fusing feature vectors from shallow, middle, and deep layers, enabling more comprehensive texture representation. Experimental results on six benchmark texture datasets demonstrate that DANets outperform 26 representative methods, showing their effectiveness in improving texture classification accuracy.},
  archive      = {J_NEUCOM},
  author       = {Yongsheng Dong and Naixin Lu and Xuelong Li},
  doi          = {10.1016/j.neucom.2025.129833},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129833},
  shortjournal = {Neurocomputing},
  title        = {Dense attention networks for texture classification},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical incremental learning: Striving for better
performance-efficiency trade-off. <em>NEUCOM</em>, <em>634</em>, 129831.
(<a href="https://doi.org/10.1016/j.neucom.2025.129831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the storage of historical data and buffer management in replay-based Incremental Learning (IL), where the buffer is usually a complete copy of the storage. Storage or buffer size constraints present significant challenges in laboratory settings but may not be practical in real-world applications. Since storing historical data is generally feasible and affordable in most realistic scenarios, we propose a generalization to IL by decoupling the concepts of storage and buffer, dubbed Practical Incremental Learning (PIL). We argue that, in practice, storage size should be determined by cost considerations, while buffer size should be limited for efficiency. Under PIL, we establish a strong baseline by resampling the buffer for each training iteration, ensuring better historical data coverage. We then evaluate popular strategies within the IL community, revealing their ineffectiveness against this baseline. We identify key limitations through in-depth analysis and derive insightful design principles for PIL methods. Based on these principles, Cross-Task Distance Calibration with Distribution Annealing (DCDA) is proposed. It achieves significant improvements over state-of-the-art methods across multiple benchmarks and settings, with performance gains of up to 15%, demonstrating its industrial-level performance and usability.},
  archive      = {J_NEUCOM},
  author       = {Shixiong Xu and Bolin Ni and Xing Nie and Fei Zhu and Yin Li and Jianlong Chang and Gaofeng Meng},
  doi          = {10.1016/j.neucom.2025.129831},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129831},
  shortjournal = {Neurocomputing},
  title        = {Practical incremental learning: Striving for better performance-efficiency trade-off},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing generalization in camera trap image recognition:
Fine-tuning visual language models. <em>NEUCOM</em>, <em>634</em>,
129826. (<a href="https://doi.org/10.1016/j.neucom.2025.129826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel fine-tuning approach for enhancing the generalization capabilities of visual language models in the context of wildlife monitoring, particularly for camera trap image recognition. In this paper, we introduce Ecological Visual Language Models (Eco-VLMs), a model fine-tuned using an ecological subset of the ImageNet1K dataset (ImageNet1K-E), aimed at reducing the reliance on spurious correlations that affect the performance of models like CLIP when applied to specialized domains. By employing text augmentation techniques and expanding species names with rich descriptors, Eco-VLM is optimized to extract more distinctive features from images, thereby improving its discriminative capabilities for wildlife features. Meanwhile, random contrastive loss is proposed to improve the diversity of training data and the generalization of Eco-VLMs. The proposed Eco-CLIP and Eco-SigLIP model are rigorously evaluated against various camera trap datasets and demonstrates superior performance, with average F1 scores improved by 4.44 % and 3.79 % compared to the standard CLIP and SigLIP model. Intrinsic evaluations further confirm that Eco-VLMs have acquired a broader ecological knowledge base, highlighting its enhanced generalization abilities. This research contributes to the field by addressing the limitations of current visual language models in specialized ecological applications and underscores the potential of Eco-VLMs for improving wildlife monitoring efforts.},
  archive      = {J_NEUCOM},
  author       = {Zihe Yang and Ye Tian and Lifeng Wang and Junguo Zhang},
  doi          = {10.1016/j.neucom.2025.129826},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129826},
  shortjournal = {Neurocomputing},
  title        = {Enhancing generalization in camera trap image recognition: Fine-tuning visual language models},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Training multi-bit spiking neural network with virtual
neurons. <em>NEUCOM</em>, <em>634</em>, 129825. (<a
href="https://doi.org/10.1016/j.neucom.2025.129825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Back-Propagation Through Time (BPTT) with surrogate gradient has been widely applied for directly training the Spiking Neural Network (SNN), a biologically realistic type of Artificial Neural Network. The Multi-Level Firing model (MLF) improves SNN performance by introducing more sub-neurons in a unit and extending the output of spiking neurons from single-bit binary impulse to multiple-bit integers. However, it also worsens the excessive computational resources demanded by the BPTT process. This paper presents a novel neuron model called Virtual Neurons(VN) and its training method. VN can output multi-bit spikes as MLF does but avoid its usually heavy training cost. The VN has been proven numerically equivalent to a special case of several single-bit sub-neurons combined in the MLF way, making it theoretically compatible with 1-bit neuromorphic chips. Experimental results on various vision tasks demonstrate that VN-based SNNs not only have much lower computation costs compared to the MLF model during off-chip training but also achieve advanced performance in fewer time steps.},
  archive      = {J_NEUCOM},
  author       = {Haoran Xu and Zonghua Gu and Ruimin Sun and De Ma},
  doi          = {10.1016/j.neucom.2025.129825},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129825},
  shortjournal = {Neurocomputing},
  title        = {Training multi-bit spiking neural network with virtual neurons},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing sentiment analysis with distributional emotion
embeddings. <em>NEUCOM</em>, <em>634</em>, 129822. (<a
href="https://doi.org/10.1016/j.neucom.2025.129822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment classification tasks, such as emotion detection and sentiment analysis, are essential in modern natural language processing (NLP). Moreover, vector representation frameworks modeling semantic content underlie each state-of-the-art NLP algorithmic scheme. In sentiment classification, traditional methods often rely on such embedding vectors for semantic representation, yet they typically overlook the dynamic and sequential nature of emotions within textual data. In this work, we present a novel methodology that leverages the distributional patterns of emotions. An embedding framework that captures the inherent serial structure of emotional occurrences in text is introduced, modeling the interdependencies between emotion states as they unfold within a document. Our approach treats each sentence as an observation in a multivariate series of emotions, transforming the emotional flow of a text into a sequence of emotion strings. By applying distributional logic, emotion-based embeddings that represent both emotional and semantic information are derived. Through a comprehensive experimental framework, we demonstrate the effectiveness of these embeddings across various sentiment-related tasks, including emotion detection, irony identification, and hate speech classification, evaluated on multiple datasets. The results show that our distributional emotion embeddings significantly enhance the performance of sentiment classification models, offering improved generalization across diverse domains such as financial news and climate change discourse. Hence, this work highlights the potential of using distributional emotion embeddings to advance sentiment analysis, offering a more nuanced understanding of emotional language and its structured, context-dependent manifestations.},
  archive      = {J_NEUCOM},
  author       = {Charalampos M. Liapis and Aikaterini Karanikola and Sotiris Kotsiantis},
  doi          = {10.1016/j.neucom.2025.129822},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129822},
  shortjournal = {Neurocomputing},
  title        = {Enhancing sentiment analysis with distributional emotion embeddings},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-scale salient object detection framework
utilizing nonlinear spiking neural p systems. <em>NEUCOM</em>,
<em>634</em>, 129821. (<a
href="https://doi.org/10.1016/j.neucom.2025.129821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) is fundamental to computer vision applications ranging from autonomous driving and surveillance to medical imaging. Despite significant progress, existing methods struggle to effectively model multi-scale features and their complex interdependencies, particularly in challenging real-world scenarios with complex backgrounds and varying scales. To address these limitations, this paper proposes a novel detection framework that leverages the hierarchical processing capabilities of nonlinear spiking neural P (NSNP) systems. The proposed framework introduces three key innovations: a bio-inspired convolution mechanism that captures fine-grained local features with neural dynamics; a semantic learning module enhanced by Contextual Transformer Attention for comprehensive global context understanding; and an adaptive mixed attention-based fusion strategy that optimizes cross-scale feature integration. The experimental results on four challenging benchmark datasets demonstrate that the proposed method outperforms fourteen other state-of-the-art methods, achieving average improvements of 1.02%, 1.3%, 2.3%, and 0.1% on the four evaluation metrics ( S m , E ξ m , F β w , and M A E ), respectively. These advances validate the potential of spiking neural P systems in salient object detection, while opening new possibilities for bio-inspired approaches in visual computing.},
  archive      = {J_NEUCOM},
  author       = {Nan Zhou and Minglong He and Hong Peng and Zhicai Liu},
  doi          = {10.1016/j.neucom.2025.129821},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129821},
  shortjournal = {Neurocomputing},
  title        = {A novel multi-scale salient object detection framework utilizing nonlinear spiking neural p systems},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local and global spatial–temporal transformer for
skeleton-based action recognition. <em>NEUCOM</em>, <em>634</em>,
129820. (<a href="https://doi.org/10.1016/j.neucom.2025.129820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition represents a dynamic and expanding research domain in computer vision. Currently, GCN-based methods primarily rely on the graph topology to capture dependencies between joints, however, they are limited in the ability to capture long-distance dependencies. On the other hand, transformer-based methods have also been applied to skeleton-based action recognition, as transformers prove effective in capturing long-distance dependencies. Nevertheless, most transformer-based methods directly calculate pairwise global self-attention of all nodes in both spatial and temporal dimensions, making it challenging to distinguish the correlation between short-distance joints and underestimate the impact of short-term temporal dynamics. Additionally, some existing methods often utilize multi-stream fusion to combine features from different modalities, neglecting the fusion of low-level information from these modalities. In this work, we propose a novel L ocal and G lobal S patial– T emporal Trans former network (LG-STFormer) containing two key components: (1) LGA-module: local and global attention module. The LGA-module enables the model to capture richer temporal and spatial information. It consists of two parts: skeleton topology constraint spatial transformer (STC-SFormer) and attention-enhanced multiscale TCN (AM-TCN). The STC-SFormer focuses on the correlation between local joints and distant joints in the spatial dimension, while the AM-TCN integrates the global self-attention mechanism into multi-scale temporal convolution to capture local and global temporal motion patterns of joints effectively. (2) JBC-Fusion-module: The module consists of joint-bone cross fusion transformer (JBC-Former) and AM-TCN. Utilizing dynamic generation method for complementary features, the JBC-Former facilitates the fusion of low-level information between complementary features. Finally, we make extensive experiments on the NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA datasets to show the competitive performance of the proposed LG-STFormer in the field of skeleton-based action recognition.},
  archive      = {J_NEUCOM},
  author       = {Ruyi Liu and Yu Chen and Feiyu Gai and Yi Liu and Qiguang Miao and Shuai Wu},
  doi          = {10.1016/j.neucom.2025.129820},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129820},
  shortjournal = {Neurocomputing},
  title        = {Local and global Spatial–Temporal transformer for skeleton-based action recognition},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GKA: Graph-guided knowledge association for fine-grained
visual categorization. <em>NEUCOM</em>, <em>634</em>, 129819. (<a
href="https://doi.org/10.1016/j.neucom.2025.129819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual categorization aims to distinguish highly similar subclasses by exploiting subtle differences. However, existing methods predominantly emphasize the extraction of visual cues from individual images, overlooking the exploration of semantic relationships within and across classes. To this end, we introduce a novel approach termed Graph-based Knowledge Association (GKA). Specifically, we employ a positional embedding to model the relationship between instances in the feature space, and adaptively mine the connections between features of different instances via a graph neural network. The framework effectively aggregates features from neighboring nodes to enhance the understanding of discriminative features by exploiting complementary information between instances. Furthermore, a plain knowledge-guided module embeds this relational knowledge into the training of the backbone network for discriminative feature extraction, thus improving fine-grained classification performance. Empirical evaluations on four benchmark datasets for Fine-grained Visual Categorization (FGVC) demonstrate that our method achieves state-of-the-art (SOTA) performance.},
  archive      = {J_NEUCOM},
  author       = {Yuetian Wang and Shuo Ye and Wenjin Hou and Duanquan Xu and Xinge You},
  doi          = {10.1016/j.neucom.2025.129819},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129819},
  shortjournal = {Neurocomputing},
  title        = {GKA: Graph-guided knowledge association for fine-grained visual categorization},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STAD-AI: Spatio-temporal anomaly detection in videos with
attentive dual-stage integration. <em>NEUCOM</em>, <em>634</em>, 129817.
(<a href="https://doi.org/10.1016/j.neucom.2025.129817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel and comprehensive framework for video anomaly detection, distinguished by its specialized spatio-temporal feature extraction and precise anomaly prediction capabilities. The proposed system employs an advanced spatio-temporal attention-based framework designed for effective video frame reconstruction. By isolating and amplifying critical feature regions within the frames, it enables the extraction of fine-grained spatial and temporal representations, which are crucial for detecting subtle anomalies. Complementing this, an attentive U-Net architecture is employed to predict anomalies with high precision, incorporating motion features to enhance temporal coherence and anomaly localization. The attention mechanism in both components is strategically designed to focus on critical areas within each frame and sequence, where abnormal activities are likely to occur, improving detection accuracy and reducing false positives. The two components are seamlessly integrated using a fusion strategy, combining their complementary strengths to enhance the system’s overall robustness and effectiveness. Extensive evaluations on benchmark datasets, including UCSD Peds1, UCSD Peds2, CUHK Avenue, and ShanghaiTech, demonstrate that STAD-AI achieves superior performance with AUC scores of 86.6%, 99.1%, 91.4%, and 77.7%, respectively. These results highlight the framework’s ability to effectively leverage spatial and temporal features for detecting anomalies with high precision, advancing the state-of-the-art in video anomaly detection.},
  archive      = {J_NEUCOM},
  author       = {Rangachary Kommanduri and Mrinmoy Ghorai},
  doi          = {10.1016/j.neucom.2025.129817},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129817},
  shortjournal = {Neurocomputing},
  title        = {STAD-AI: Spatio-temporal anomaly detection in videos with attentive dual-stage integration},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fractional gannet humming optimization enabled deep
convolutional neural network for detection and segmentation of skin
cancer. <em>NEUCOM</em>, <em>634</em>, 129816. (<a
href="https://doi.org/10.1016/j.neucom.2025.129816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is a dangerous disorder that is caused by an unchecked proliferation of aberrant skin cells that produce genetic mutations on the skin. When ultraviolet (UV) radiation from sunshine or tanning beds damages the skin cells in a way that leads to their rapid multiplication and formation of malignant tumours. Certain forms of skin cancer metastasize along nerves. This results in tingling, pain, itching, numbness, or a sensation like ants crawling under the skin. Skin cancer spreads to deeper tissues, including cartilage, muscle, and bone. The earlier prediction of skin lesions increases the possibility of survival rate. However, during diagnosis, an anomalous finding is made, and the condition is diagnosed as cancer. To overcome these challenges, a novel deep learning (DL) technique is developed in this research article for categorizing skin cancer by employing the proposed Fractional Gannet Humming Optimization_Deep Convolutional Neural Network (FGHO_DeepCNN). Initially, the input skin cancer image is subjected to the image pre-processing phase. The image pre-processing is done by the bilateral filter. Afterwards, skin lesion segmentation is carried out using an encoder-decoder with Dense-Residual block (DRB), which is trained by the Fractional Gannet optimization algorithm (FGOA). Here, the FGOA is formed by the integration of the Fractional Calculus (FC) concept with the Gannet Optimization Algorithm (GOA). Thereafter, image augmentation is done to enlarge the segmented image using geometric and colour space transformation. After that, a feature extraction process is conducted to obtain the significant features, like Completed Local Binary Pattern (CLBP), Gray Level Co-occurrence Matrix (GLCM), Local Vector Pattern (LVP), Significant Local Binary Pattern (SLBP) and CNN features. Finally, skin cancer detection is done using DeepCNN, which is tuned by the proposed FGHO. Here, the proposed FGHO is formed by the combination of FGOA with the artificial hummingbird algorithm (AHA). The experimental outcomes of the proposed FGHO_DeepCNN approach attained a better Positive Predictive Value (PPV), Negative Predictive Value (NPV), True Positive Rate (TPR), and True Negative Rate (TNR), and accuracy with values of 89.80 %, 89.40 %, 94.50 %, 94.00 % and 93.40 % respectively. The employed FGHO_DeepCNN has acquired excellent performance, thus achieving a PPV of 91.68 %, NPV of 88.46 %, TPR of 91.68 %, TNR of 91.23 % and accuracy of 90.67 % for dataset 2. In dataset 3, the FGHO_DeepCNN obtained superior performance than other techniques with a PPV of 90.56 %, NPV of 90.36 %, TPR of 90.95 %, TNR of 90.87 %, and accuracy of 90.15 %. The practical significance of the proposed FGHO_DeepCNN approach is that it is widely used in dermatology clinics and hospitals to detect skin cancer.},
  archive      = {J_NEUCOM},
  author       = {Aravapalli Rama Satish and Balajee Maram and Varaprasada Rao Perumalla and Mallikharjuna Rao K},
  doi          = {10.1016/j.neucom.2025.129816},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129816},
  shortjournal = {Neurocomputing},
  title        = {Fractional gannet humming optimization enabled deep convolutional neural network for detection and segmentation of skin cancer},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness in constrained spectral clustering.
<em>NEUCOM</em>, <em>634</em>, 129815. (<a
href="https://doi.org/10.1016/j.neucom.2025.129815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised clustering methods have gained significant attention in both theoretical research and real-world applications, including economics, finance, marketing, and healthcare. Among these methods, constrained spectral clustering enhances clustering quality by incorporating pairwise constraints, namely, must-link and cannot-link constraints, which guide the clustering process by specifying whether certain data points should or should not belong to the same cluster. However, traditional constrained spectral clustering methods may inadvertently propagate biases present in the data or constraints, leading to unequal representation of sensitive groups, such as different genders or racial groups, across clusters. This imbalance raises concerns about fairness, an issue that remains largely unexplored in constrained spectral clustering. To address this gap, this paper proposes a novel method named fair-constrained Spectral Clustering (fair-cSC). The proposed method integrates fairness into the must-link and cannot-link constraints by defining a fair constraint matrix, ensuring that pairwise relationships do not introduce bias against any particular group. Additionally, a balance constraint is incorporated to enforce fairness across input data points, promoting equal representation of sensitive groups within clusters. Comprehensive experiments on six benchmarked datasets, including ablation studies, demonstrate that the proposed fair-cSC method effectively enhances fairness while preserving clustering quality. Furthermore, the ablation study provides insights into the method’s performance under different settings, reinforcing its robustness and applicability in real-world scenarios.},
  archive      = {J_NEUCOM},
  author       = {Laxita Agrawal and V. Vijaya Saradhi and Teena Sharma},
  doi          = {10.1016/j.neucom.2025.129815},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129815},
  shortjournal = {Neurocomputing},
  title        = {Fairness in constrained spectral clustering},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-driven human image generation with texture and pose
control. <em>NEUCOM</em>, <em>634</em>, 129813. (<a
href="https://doi.org/10.1016/j.neucom.2025.129813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating high-quality and diverse human images solely based on textual prompts is a challenging task in computer vision and computer graphics. Existing models do not have stable generation capabilities, and their control over image generation is insufficient, requiring an additional densepose map to provide pose information. In this work, we present a text-driven human image generation model, which enables control over texture and pose through textual input. The framework consists of three stages: (1) generating a pose map from the pose description which achieves control over the pose information of the generated human image. (2) converting the pose map to a parsing map based on the shape description, and (3) generating the final image by using textual descriptions of clothes texture along with a quality enhancement network. For the texture learning stage, we construct a texture-aware codebook using FSQ, learning vector representations for different clothes fabrics and patterns. Furthermore, we fine-tune the diffusion model to equip it with the capability of image denoising and image refining, thus improving the quality of the generated images. Extensive experimental results demonstrate our model’s ability of generating realistic human body images that align with the semantics of the input text.},
  archive      = {J_NEUCOM},
  author       = {Zhedong Jin and Guiyu Xia and Paike Yang and Mengxiang Wang and Yubao Sun and Qingshan Liu},
  doi          = {10.1016/j.neucom.2025.129813},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129813},
  shortjournal = {Neurocomputing},
  title        = {Text-driven human image generation with texture and pose control},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing DoA assessment through federated learning: A
one-shot pseudo data approach. <em>NEUCOM</em>, <em>634</em>, 129812.
(<a href="https://doi.org/10.1016/j.neucom.2025.129812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately measuring the Depth of Anaesthesia (DoA) during surgical procedures is crucial for patient safety. A significant challenge in developing effective machine learning models for DoA assessment is the lack of data from single organisations and preserving data privacy between institutions. Federated learning offers a solution by enabling multiple parties to collaboratively train models without exchanging data. However, traditional federated learning algorithms perform poorly in data heterogeneous, non-identically distributed data distribution scenarios. To address these challenges, we propose a one-shot federated learning framework, DoAFedP-NN, which facilitates federated learning with heterogeneous model development. The framework is tested in a range of model and data heterogeneity environments. This method enables the training of a global DoA prediction model across different medical facilities without sharing local data. The DoAFedP-NN model, utilising neural network design with entropy and spectral feature extraction, is compared to benchmark federated learning architectures, demonstrating its advantage in handling heterogeneous medical data. Experimental results show that DoAFedP-NN achieves robust DoA estimation when compared to the Bispectral (BIS) index, with high correlation coefficients of 0.8472 and 0.8542 across independent databases. The proposed model outperforms locally developed models, showing significant improvements when validated against external datasets from different medical facilities. This paper makes the key contributions: (1) introduces a one-shot pseudo-data method for federated learning; (2) demonstrates the effectiveness of this approach for EEG-based DoA using real-world databases; (3) showcases the model’s ability to achieve high correlation with the BIS index while preserving patient privacy in a range of client distribution scenarios and under cross-validation.},
  archive      = {J_NEUCOM},
  author       = {Thomas Schmierer and Tianning Li and Di Wu and Yan Li},
  doi          = {10.1016/j.neucom.2025.129812},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129812},
  shortjournal = {Neurocomputing},
  title        = {Advancing DoA assessment through federated learning: A one-shot pseudo data approach},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCKGL: Hyperbolic collaborative knowledge graph learning for
recommendation. <em>NEUCOM</em>, <em>634</em>, 129808. (<a
href="https://doi.org/10.1016/j.neucom.2025.129808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the integration of knowledge graph and recommendation system has become a hot topic. Its popular solution is firstly combining the knowledge graph and user–item interaction graph to generate a unified Collaborative Knowledge Graph (CKG), and then learn the representations of users and items by applying graph convolutional networks to aggregate high-order neighbor information between entities in CKG. However, existing related methods mainly focus on learning representations in the Euclidean space, posing challenges in capturing the hierarchical structure and intricate relational logic between users and items. In view of this, we propose a novel hyperbolic CKG learning model HCKGL for recommendation, which leverages relation-specific curvature and attention-based geometric transformations to preserve the inherent features of CKG. Additionally, we address two significant challenges that existing methods have often overlooked. Firstly, in order to capture the relationship dependencies between neighbors and accurately calculate the contribution of neighbor information, we propose a hyperbolic graph attention network (HGAT), which combines the curvature of the relationship to assign weights. Secondly, we present a new graph contrastive learning technique (HMCL) that utilizes the hyperbolic embedding propagation and multi-level contrastive learning to improve the representations of users and items. Comprehensive experimental results on two widely used datasets demonstrate that HCKGL outperforms state-of-the-art baselines. The source code for our model is publicly available at: https://github.com/GDM-SCNU/HCKGL .},
  archive      = {J_NEUCOM},
  author       = {Huijuan Hu and Chaobo He and Xinran Chen and Quanlong Guan},
  doi          = {10.1016/j.neucom.2025.129808},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129808},
  shortjournal = {Neurocomputing},
  title        = {HCKGL: Hyperbolic collaborative knowledge graph learning for recommendation},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature expansion and enhanced compression for class
incremental learning. <em>NEUCOM</em>, <em>634</em>, 129807. (<a
href="https://doi.org/10.1016/j.neucom.2025.129807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class incremental learning consists in training discriminative models to classify an increasing number of classes over time. However, doing so using only the newly added class data leads to the known problem of catastrophic forgetting of the previous classes. Recently, dynamic deep learning architectures have been shown to exhibit a better stability-plasticity trade-off by dynamically adding new feature extractors to the model in order to learn new classes followed by a compression step to scale the model back to its original size, thus avoiding a growing number of parameters. In this context, we propose a new algorithm that enhances the compression of previous class knowledge by cutting and mixing patches of previous class samples with the new images during compression using our Rehearsal-CutMix method. We show that this new data augmentation reduces catastrophic forgetting by specifically targeting past class information and improving its compression. Extensive experiments performed on the CIFAR and ImageNet datasets under diverse incremental learning evaluation protocols demonstrate that our approach consistently outperforms the state-of-the-art . The code will be made available upon publication of our work 1 .},
  archive      = {J_NEUCOM},
  author       = {Quentin Ferdinand and Benoit Clement and Panagiotis Papadakis and Quentin Oliveau and Gilles Le Chenadec},
  doi          = {10.1016/j.neucom.2025.129807},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129807},
  shortjournal = {Neurocomputing},
  title        = {Feature expansion and enhanced compression for class incremental learning},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bioinspired model of decision making guided by reward
dimensions and a motivational state. <em>NEUCOM</em>, <em>634</em>,
129806. (<a href="https://doi.org/10.1016/j.neucom.2025.129806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decision-making process is a critical component of computational systems, whose processing involves the evaluation of various alternatives presented as possible solutions to a given problem, depending on the current context. This paper seeks to show how a neuroscience-based decision-making mechanism (DMM) integrating decision criteria, knowledge of reward stimuli, and motivational information helps to contribute to producing human-like adaptive behavior. To fulfill this objective, a computational model on DMM is proposed. The alternatives in this proposed model are constructed based on preferences, and the selection of the best alternative is guided by a goal-directed control scheme influenced by a motivational state (MS). The formation of preferences considers some dimensions of the reward, e.g., magnitude, probability of receiving the reward, incentive salience, and affective value. To validate the model exhibits a behavior considering parameters human being uses to compute its behavior, a case study was proposed. The case study’s objective is to gain the maximum reward (food) from the choice of a 4-choice card (a variation of Iowa Gambling Test), each card has a reward and a contingency probability associated with it. The analysis of the results of the case study shows that the model presents a short exploitation stage to find the contingency rule and choose the best option frequently according to some studies, also observed that the utility value of the card influenced the MS of hunger and other factors play a critical role in the DMM.},
  archive      = {J_NEUCOM},
  author       = {Diana G. Gómez-Martínez and Alison Muñoz-Capote and Oscar Hernández and Francisco Robles and Félix Ramos},
  doi          = {10.1016/j.neucom.2025.129806},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129806},
  shortjournal = {Neurocomputing},
  title        = {A bioinspired model of decision making guided by reward dimensions and a motivational state},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SSDM: Generated image interaction method based on spatial
sparsity for diffusion models. <em>NEUCOM</em>, <em>634</em>, 129805.
(<a href="https://doi.org/10.1016/j.neucom.2025.129805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image interaction methods based on diffusion models are significantly superior to traditional methods. However, due to its slow sampling speed and high computational complexity, it could be more conducive to image editing applications. To address these issues, we propose a generated image interaction method based on spatial sparsity diffusion models (SSDM), which utilizes the spatial sparsity of the differences between the edited and original images to reduce computational complexity and accelerate image generation. It takes sparse block data as a constraint, uses difference masks, converts it into an index, and learns the spatial sparse features of differences to describe the image, thereby reducing the network parameters and computational complexity during the training process. In addition, the “overlap-add” and “overlap-save” mechanisms are used to ensure coherence and consistency between different boundaries. It also compensates and trains the sampled feature maps by reusing low-frequency information and introduces L p -norm to replace Euclidean distance to calculate the loss function, thereby enhancing the reconstruction effect of high-frequency image features. Compared to the original methods, experiments on the LSUN and CelebA-HQ datasets show that the proposed method has achieved better performance of PSNR by improving 1.2 dB, reduced computational complexity by 4.1 × , and increased processing speed by 3.6 × .},
  archive      = {J_NEUCOM},
  author       = {Zhuochao Yang and Jingjing Liu and Haozhe Zhu and Jianhua Zhang and Wanquan Liu},
  doi          = {10.1016/j.neucom.2025.129805},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129805},
  shortjournal = {Neurocomputing},
  title        = {SSDM: Generated image interaction method based on spatial sparsity for diffusion models},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Backpropagation-based learning with local derivative
approximation and memory replay in biologically plausible neural
systems. <em>NEUCOM</em>, <em>634</em>, 129804. (<a
href="https://doi.org/10.1016/j.neucom.2025.129804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When learning, the brain modifies individual synaptic connections to reach a desired behavior. Animal and human brains have been shown to be incredibly capable of learning complex and varied functions across a wide variety of tasks. In recent years, artificial neural networks, inspired by human and animal brains, have shown great capabilities in learning a wide variety of difficult tasks. However, artificial neural networks primarily teach themselves through the use of backpropagation, a learning method which has no clear analogue within the brain. Additionally, Artificial Neural Networks primarily use continuous activation functions, which differ significantly from the spiking neuronal behavior present in the brain. In this paper, we discuss and demonstrate a biologically plausible learning method that approximates backpropagation through two techniques on Spiking Neural Networks. First, we show that the local temporal derivatives that are necessary for backpropagation can be approximately recovered through reconstruction using spike timings. Second, we show that through learning during a sleep phase, inspired by neuroscience research into memory replay, the localized parallel feedback path can learn to approximate the derivative through the forward path weight matrix, thus solving the weight transport problem. Finally, we demonstrate that the combination of these two methods can approach or exceed the accuracy of backpropagation-based methods for a variety of neuromorphic vision tasks while maintaining biological plausibility.},
  archive      = {J_NEUCOM},
  author       = {Richard Boone and Peng Li},
  doi          = {10.1016/j.neucom.2025.129804},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129804},
  shortjournal = {Neurocomputing},
  title        = {Backpropagation-based learning with local derivative approximation and memory replay in biologically plausible neural systems},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven stabilization for linear sampled-data systems
with unknown parameters: A pure data analytics perspective.
<em>NEUCOM</em>, <em>634</em>, 129798. (<a
href="https://doi.org/10.1016/j.neucom.2025.129798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the sampled-data stabilization problem for a class of linear systems with unknown parameters, incorporating a data analytics perspective. Traditional sampled-data controller design methods are rendered ineffective due to the lack of knowledge about the system parameters. To address this challenge, a data-driven approach is proposed for designing a sampled-data stabilization controller using an offline-collected data set, eliminating the need for prior knowledge of the plant parameters. Specifically, a novel sampled-data control strategy with a matrix exponential gain is introduced to stabilize the system. The controller gain is designed based on data insights derived from offline information, without requiring access to the system matrices. Leveraging data-driven techniques and Lyapunov stability theory, sufficient conditions are established to guarantee the exponential stability of the system. The proposed sampled-data controller with matrix exponential gain significantly extends the maximum allowable sampling intervals compared to conventional methods. To optimize performance, a convex optimization approach is utilized to maximize the bounds of these sampling intervals. The efficacy of the proposed approach is validated through a practical example involving an operational amplifier circuit, demonstrating the power of integrating data analytics into control design for unknown-parameter systems.},
  archive      = {J_NEUCOM},
  author       = {Luyang Yu and Jiayi Ding and Ying Cui and Yurong Liu and Yamin Wang},
  doi          = {10.1016/j.neucom.2025.129798},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129798},
  shortjournal = {Neurocomputing},
  title        = {Data-driven stabilization for linear sampled-data systems with unknown parameters: A pure data analytics perspective},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new minimalist time series reduction technique.
<em>NEUCOM</em>, <em>634</em>, 129772. (<a
href="https://doi.org/10.1016/j.neucom.2025.129772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose in this paper a minimalist time series reduction technique that comes with a new distance. The proposed technique relies on the extraction of a minimal number of salient features of a time series, which are leveraged by the new distance to perform time series classification. We prove in this paper that the new proposed distance lower bounds the Euclidean distance and has a tighter lower bound than Piecewise Aggregation Approximation. Furthermore, we conduct experiments using standard UCR univariate time series datasets and include a comparative study of the new proposed technique with related SAX and deep learning based time series reduction and classification techniques. The experiments show that the new distance based time series classification enjoys better accuracy results than classifications based on well-known distances and is competitive to deep learning based time series classification techniques.},
  archive      = {J_NEUCOM},
  author       = {Hamdi Yahyaoui and Hosam AboElFotoh and Yanjun Shu and Tianrun Gao},
  doi          = {10.1016/j.neucom.2025.129772},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129772},
  shortjournal = {Neurocomputing},
  title        = {A new minimalist time series reduction technique},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deformable symmetry attention for nuclear medicine image
segmentation. <em>NEUCOM</em>, <em>634</em>, 129757. (<a
href="https://doi.org/10.1016/j.neucom.2025.129757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior knowledge of the medical domain has consistently enriched medical image analysis, yet its full potential remains to be explored. Our Deformable Symmetry Attention (DSA) aims to leverage anatomical symmetry prior, commonly used by clinicians, to enhance the automated analysis of medical images, especially in inherently grainy nuclear medicine images. DSA processes an explicit feature alignment attention. It starts with a prior alignment by reflecting the input image to leverage symmetry, followed by a fine-grained alignment using deformable image registration. Our novel Rotational Spatial Transformation Function (RSTF) enhances the registration by estimating local displacement with rotation in relative coordinates, enforcing local smoothness under anatomical variations. Additionally, the Attention Redistribution (AR) module addresses pixel-level mismatch in grainy nuclear medicine images by a set of learnable Gaussian kernels. By translating the anatomical symmetry prior into a locality prior, DSA seamlessly integrates with and enhances sophisticated convolutional neural networks (CNNs) and transformer-based segmentation models. In our study, the DSA framework, when combined with robust baseline models, demonstrated significant performance enhancements across multiple challenging medical imaging tasks. Specifically, it achieved a 3.8% to 8.48% improvement in segmenting lesions in adolescent Whole-Body Bone Scans (WBS) and a 2.88% improvement in the extended 3D MRI brain tumor segmentation experiment. Additionally, the method was validated in asymmetric scenarios, showing an improvement of 0.63% to 10.57% in chest X-ray anatomical segmentation. These results underscore DSA’s effectiveness and versatility across diverse experimental settings. It presents a novel and promising perspective for explicitly leveraging prior knowledge in medical image analysis.},
  archive      = {J_NEUCOM},
  author       = {Zeao Zhang and Pei Yang and Ruomeng Liu and Huawei Cai and Zhen Zhao and Quan Guo and Zhang Yi},
  doi          = {10.1016/j.neucom.2025.129757},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129757},
  shortjournal = {Neurocomputing},
  title        = {Deformable symmetry attention for nuclear medicine image segmentation},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio–visual self-supervised representation learning: A
survey. <em>NEUCOM</em>, <em>634</em>, 129750. (<a
href="https://doi.org/10.1016/j.neucom.2025.129750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence developers leverage the inherent relationships among video, text, and audio to create enhanced representations of the world, mirroring the way humans use multiple senses to understand their environment. As such, multimodal learning, which integrates various data input modalities to augment the learning of intrinsic features, has been gaining traction. While applications in multimodal understanding have made strides with deep learning, they often rely heavily on supervised learning and extensive human annotation. This paper provides a comprehensive review of audio–visual self-supervised learning, a promising alternative that uses vast amounts of unlabeled data. It holds the potential to reshape areas like computer vision, and speech recognition. We begin by explaining the concept of audio–visual modalities in machine learning and then move into their role within self-supervised learning by discussing terminology, general pipelines, and underlying motivations. This is followed by an exploration of common pretext tasks in audio–visual self-supervised learning, along with the evaluation methods, datasets, and downstream tasks associated with it. We then highlight prevailing challenges in both audio–visual and self-supervised learning realms. The paper concludes by presenting open challenges, suggesting avenues for future research in this dynamic domain.},
  archive      = {J_NEUCOM},
  author       = {Manal AlSuwat and Sarah Al-Shareef and Manal AlGhamdi},
  doi          = {10.1016/j.neucom.2025.129750},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129750},
  shortjournal = {Neurocomputing},
  title        = {Audio–visual self-supervised representation learning: A survey},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on self-adaptive grid point cloud down-sampling
method based on plane fitting and mahalanobis distance gaussian
weighting. <em>NEUCOM</em>, <em>634</em>, 129746. (<a
href="https://doi.org/10.1016/j.neucom.2025.129746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this manuscript, a self-adaptive grid point cloud down-sampling method based on plane fitting was proposed, which could effectively reduce redundant data while better preserving the geometric features of the original model and maintaining high accuracy. This method first constructs initial voxel grids and divides the grids into large density and small density ones according to the point cloud density. After that, for small density grids, the boundary points are extracted first, and the rest areas are uniformly sampled, while for large density grids, a method based on Mahalanobis distance Gaussian weighting is proposed and adopted to estimate the normal vector of points, and feature points are determined and retained by calculating the information entropy. Then, three models in the public dataset, the Cat model, Bed_0355 model and Fandisk model, were employed as test subjects to compare the proposed method with two commonly used down-sampling methods: uniform sampling and voxel grid sampling methods. The results indicated that this new method was able to better retain the geometric features of the original models, especially high curvature and sharp parts, with smaller errors and fewer holes. Finally, this method was applied to the down-sampling of 3D scanning point clouds of two typical metal machine parts, threaded joint and sheet metal part, and the measured results demonstrated that this method not only effectively preserved the model features, but also guaranteed accuracy of key geometric dimensions after high reduction ratio down-sampling, such as the relative errors of thread tooth angles and hole inner diameters being less than 1 %.},
  archive      = {J_NEUCOM},
  author       = {Hongfei Zu and Jing Zhu and Xinfeng Wang and Xiang Zhang and Ning Chen and Gangxiang Guo and Zhangwei Chen},
  doi          = {10.1016/j.neucom.2025.129746},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129746},
  shortjournal = {Neurocomputing},
  title        = {Research on self-adaptive grid point cloud down-sampling method based on plane fitting and mahalanobis distance gaussian weighting},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LTB-solver: Long-tailed bias solver for image synthesis of
diffusion models. <em>NEUCOM</em>, <em>634</em>, 129651. (<a
href="https://doi.org/10.1016/j.neucom.2025.129651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though diffusion models have shown the merits of generating high-quality visual data while preserving better diversity in recent studies, they do not generalize well on long-tailed datasets due to the minority classes lacking of diversity and semantic information. To overcome the aforementioned challenges, we first take a closer look at the collapse of tail category patterns under long-tail distributed data and propose an alternative but easy-to-use and effective solution, a L ong- T ailed B ias Solver in diffusion model image synthesis ( LTB-Solver ), which thereby enhances the overall diversity and quality of synthetic samples building upon the properties of the long-tailed distribution training data. Especially, we extract rich generative distribution knowledge of ‘head’ categories within proxy model and transfer the head-tail consistency distance to ‘tail’ categories, enabling the target diffusion model to learn diverse generation preserving inter-sample variation during the diffusion training process. Moreover, we incorporate the minority guidance loss function that better aligns training objectives with sampling behaviors and adjust the loss values for different classes by multiplying them with different weights. Extensive experiments are conducted on various datasets and several state-of-the-art diffusion model frameworks to verify the effectiveness of the proposed method. The results show that our method significantly improves the performance of diffusion models on long-tailed datasets by a large margin.},
  archive      = {J_NEUCOM},
  author       = {Siming Fu and Xiaoxuan He and Haoji Hu},
  doi          = {10.1016/j.neucom.2025.129651},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129651},
  shortjournal = {Neurocomputing},
  title        = {LTB-solver: Long-tailed bias solver for image synthesis of diffusion models},
  volume       = {634},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate binary neural network based on rich information
flow. <em>NEUCOM</em>, <em>633</em>, 129837. (<a
href="https://doi.org/10.1016/j.neucom.2025.129837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The binary neural network (BNN) compresses network parameters significantly, greatly reducing storage consumption and resource usage. This also means extremely low inference power consumption, making it a primary method for deploying machine learning on edge devices. However, due to this, there is a significant decrease in accuracy compared to full-precision networks, which restricts its potential for further advancement. To address this challenge, this study introduces the following novel contributions: (1) Introducing an innovative BNN structure named Rich Information Flow Network (RIF-Net) for amplitude correction, which effectively alleviates the information bottleneck in BNN and thus enhances its accuracy. (2) Incorporating additional shortcut connections and Batch Normalization (BN) layers into the architecture to further mitigate the impact of binary convolution on the output feature map&#39;s numerical distribution. (3) To demonstrate RIF-Net&#39;s performance on edge devices, the study implements the RIF-Net architecture on an FPGA development board. The experimental results show that RIF-Net can achieve the most advanced performance. For example, we achieved a Top-1 accuracy of 63.3 % on ImageNet using the ResNet-18 framework and a 1.0 % absolute accuracy improvement on the CIFAR-10 dataset using the ResNet-20 framework compared to other state-of-the-art methods. The RIF-Net accelerator implemented on the FPGA showcases precise real-time image classification capabilities.},
  archive      = {J_NEUCOM},
  author       = {Shilong Zhang and Xiaobo Zhou and Siyuan Chen},
  doi          = {10.1016/j.neucom.2025.129837},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129837},
  shortjournal = {Neurocomputing},
  title        = {Accurate binary neural network based on rich information flow},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic-static feature fusion learning network for speech
emotion recognition. <em>NEUCOM</em>, <em>633</em>, 129836. (<a
href="https://doi.org/10.1016/j.neucom.2025.129836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech is a paramount mode of human communication, and enhancing the quality and fluency of Human-Computer Interaction (HCI) greatly benefits from the significant contribution of Speech Emotion Recognition (SER). Feature representation poses a persistent challenge in SER. A single feature is difficult to adequately represent speech emotion, while directly concatenating multiple features may overlook the complementary nature and introduce interference due to redundant information. Towards these difficulties, this paper proposes a Multi-feature Learning network based on Dynamic-Static feature Fusion (ML-DSF) to obtain an effective hybrid feature representation for SER. Firstly, a Time-Frequency domain Self-Calibration Module (TFSC) is proposed to help the traditional convolutional neural networks in extracting static image features from the Log-Mel spectrograms. Then, a Lightweight Temporal Convolutional Network (L-TCNet) is used to acquire multi-scale dynamic temporal causal knowledge from the Mel Frequency Cepstrum Coefficients (MFCC). At last, both extracted features groups are fed into a connection attention module, optimized by Principal Component Analysis (PCA), facilitating emotion classification by reducing redundant information and enhancing the complementary information between features. For ensuring the independence of feature extraction, this paper adopts the training separation strategy. Evaluating the proposed model on two public datasets yielded a Weighted Accuracy (WA) of 93.33 % and an Unweighted Accuracy (UA) of 93.12 % on the RAVDESS dataset, and 94.95 % WA and 94.56 % UA on the EmoDB dataset. The obtained results outperformed the State-Of-The-Art (SOTA) findings. Meanwhile, the effectiveness of each module is validated by ablation experiments, and the generalization analysis is carried out on the cross-corpus SER tasks.},
  archive      = {J_NEUCOM},
  author       = {Peiyun Xue and Xiang Gao and Jing Bai and Zhenan Dong and Zhiyu Wang and Jiangshuai Xu},
  doi          = {10.1016/j.neucom.2025.129836},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129836},
  shortjournal = {Neurocomputing},
  title        = {A dynamic-static feature fusion learning network for speech emotion recognition},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilinear-experts network with self-adaptive sampler for
long-tailed visual recognition. <em>NEUCOM</em>, <em>633</em>, 129832.
(<a href="https://doi.org/10.1016/j.neucom.2025.129832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tail distributed data hinders the practical application of state-of-the-art deep models in computer vision. Consequently, exclusive methodologies for handling the long-tailed problem are proposed, focusing on different hierarchies. For embedding hierarchy, existing works manually augment the diversity of tail-class features for specific datasets. However, prior knowledge about datasets is not always available for practical use, which brings unsatisfactory generalization ability in human fine-turned augmentation under such circumstances. To figure out this problem, we introduce a novel model named Bilinear-Experts Network (BENet) with Self-Adaptive Sampler (SAS). This model leverages model-driven perturbations to tail-class embeddings while preserving generalization capability on head classes through a designed bilinear experts system. The designed perturbations adaptively augment tail-class space and shift the class boundary away from the tail-class centers. Moreover, we find that SAS automatically assigns more significant perturbations to specific tail classes with relatively fewer training samples, which indicates SAS is capable of filtering tail classes with lower quality and enhancing them. Also, experiments conducted across various long-tailed benchmarks validate the comparable performance of the proposed BENet.},
  archive      = {J_NEUCOM},
  author       = {Qin Wang and Sam Kwong and Xizhao Wang},
  doi          = {10.1016/j.neucom.2025.129832},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129832},
  shortjournal = {Neurocomputing},
  title        = {Bilinear-experts network with self-adaptive sampler for long-tailed visual recognition},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of SCADA-based condition monitoring for wind
turbines via artificial neural networks. <em>NEUCOM</em>, <em>633</em>,
129830. (<a href="https://doi.org/10.1016/j.neucom.2025.129830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the supervisory control and data acquisition (SCADA) data has gained increasing research attention in the field of wind turbine condition monitoring. Artificial intelligence (AI) techniques have been widely applied to address condition monitoring challenges, and artificial neural networks (ANNs), recognized as a foundational component of modern AI, have proven to be particularly effective tools. Wind turbine condition monitoring focuses on analyzing the operational parameters of turbines to realize early fault detection, precise diagnostics, and accurate prognostics, thereby mitigating the risk of catastrophic faults, enhancing system reliability, and improving wind farm operational efficiency. Due to inherent issues in raw SCADA data, including missing values and abnormal data, preprocessing steps such as data cleaning are critical before feeding the data into ANN models. Additionally, the choice of ANN architecture typically depends on the specific requirements of condition monitoring tasks (e.g., fault detection, diagnosis, or prediction/prognosis) and the characteristics of SCADA datasets such as imbalance problem of fault samples. Hence, current research with respect to wind turbine condition monitoring generally follows two approaches: (1) utilizing classification models to identify fault types at specific time points, and (2) employing regression models to construct normal behavior models (NBMs) or track and predict continuous key performance indicators. This survey systematically reviews SCADA-based wind turbine condition monitoring methods within five years, emphasizing neural networks as key approaches, and structures the discussion around three core aspects: data preprocessing, classification models, and regression models. Moreover, the comparative strengths, capabilities, and limitations of various ANNs in each link are discussed. By providing an in-depth analysis, this paper aims to offer theoretical and practical insights to support the further development of condition monitoring technologies for wind turbines.},
  archive      = {J_NEUCOM},
  author       = {Li Sheng and Chunyu Li and Ming Gao and Xiaopeng Xi and Donghua Zhou},
  doi          = {10.1016/j.neucom.2025.129830},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129830},
  shortjournal = {Neurocomputing},
  title        = {A review of SCADA-based condition monitoring for wind turbines via artificial neural networks},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot low-dose CT denoising across variable schemes via
strip-scanning diffusion models. <em>NEUCOM</em>, <em>633</em>, 129828.
(<a href="https://doi.org/10.1016/j.neucom.2025.129828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artifacts and noise in low-dose CT (LDCT) may degrade image quality, potentially impacting subsequent diagnoses. In recent years, supervised image post-processing methods have been extensively studied for their effectiveness in noise reduction. However, clinical conditions often make it difficult to obtain paired normal-dose and low-dose CT images. Additionally, scanning protocols in clinical settings are diverse, necessitating different thickness or dose settings, which further complicates and increases the cost of low-dose data collection. These challenges limit the practical application and widespread adoption of supervised methods. This study introduces a novel end-to-end zero-shot strip-scanning diffusion model (SSDiff) that requires only a single model trained on normal-dose CT (NDCT) images to achieve LDCT image denoising across various scanning protocols with different slice thicknesses, doses, or devices. The sampling process employs a strip scanning strategy that combines overlapping strip information and input LDCT images to solve the maximum a posteriori problem to produce denoising results sequentially. We use only simple convolutional and attentional architectures and perform extensive experiments on three different datasets involving different doses, thicknesses, and devices; the results show that our method outperforms supervised methods in most cases, and visualization and blinded evaluations indicate that our method is very close to NDCT.},
  archive      = {J_NEUCOM},
  author       = {Bo Su and Jiabo Xu and Xiangyun Hu and Yunfei Zha and Jun Wan and Jiancheng Li},
  doi          = {10.1016/j.neucom.2025.129828},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129828},
  shortjournal = {Neurocomputing},
  title        = {Zero-shot low-dose CT denoising across variable schemes via strip-scanning diffusion models},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LN-DETR: An efficient transformer architecture for lung
nodule detection with multi-scale feature fusion. <em>NEUCOM</em>,
<em>633</em>, 129827. (<a
href="https://doi.org/10.1016/j.neucom.2025.129827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains one of the leading causes of mortality worldwide, and early detection is crucial for improving patient survival rates. Traditional lung nodule detection methods are inefficient and inaccurate, making them inadequate for clinical needs. Although deep learning methods have made progress in medical image analysis, existing approaches still perform poorly in detecting small, morphologically complex lung nodules, leading to missed detections and false positives. Additionally, the high computational complexity of previous models hinders real-time detection. To address these challenges, this study proposes a Transformer-based lung nodule detection model called LN-DETR. The model integrates a Partial Convolution-based Efficient Multi-scale Attention (PC-EMA) module, a Grouped and Shuffled Convolutional Cross-scale Feature Fusion (GS-CCFM) module, and introduces a Channel Transformer (CTrans) module. PC-EMA combines Efficient Multi-Scale Attention with partial convolution to enhance multi-scale feature extraction while optimizing computational efficiency. GS-CCFM uses Grouped and Shuffled Convolution (GSConv) to achieve efficient cross-scale feature fusion. The CTrans module employs a cross-channel attention mechanism to further strengthen feature fusion capabilities. Experimental results on the LUNA16 and Tianchi lung nodule datasets demonstrate that LN-DETR outperforms existing object detection models in detection accuracy, computational efficiency, and model complexity. On the LUNA16 dataset, LN-DETR achieved an F1 score of 91.5% and a mean Average Precision (mAP) of 93.1%; on the Tianchi dataset, the F1 score was 87.4% and the mAP was 86.4%, both significantly higher than baseline models. Furthermore, the reduced parameter count and computational overhead make the model more suitable for broader clinical applications.},
  archive      = {J_NEUCOM},
  author       = {Jiade Tang and Xiao Chen and Linyuan Fan and Zhenliang Zhu and Chen Huang},
  doi          = {10.1016/j.neucom.2025.129827},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129827},
  shortjournal = {Neurocomputing},
  title        = {LN-DETR: An efficient transformer architecture for lung nodule detection with multi-scale feature fusion},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Observer-based adaptive neural network force tracking
control for pneumatic polishing system end-effector with actuator
saturation. <em>NEUCOM</em>, <em>633</em>, 129824. (<a
href="https://doi.org/10.1016/j.neucom.2025.129824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the issue of addressing an adaptive neural network dynamic contact force tracking control for pneumatic polishing system end-effector with unmeasurable states and unknown environmental model. To tackle this problem, we utilize a neural network to approximate uncertain nonlinear functions in end-effector system model and design a neural network state observer to estimate unmeasurable states. Additionally, we combine the saturation characteristics of the pneumatic actuator and set the end-effector input voltage saturation condition. By employing the adaptive backstepping approach and the finite time convergence theory, we formulate an adaptive neural network force tracking control method. The proposed control scheme ensures the input and state stability of the end-effector of the pneumatic polishing system, and the ability to accurately track the desired contact force signal in a finite time is verified by stability analysis. Finally, the effectiveness and superiority of the proposed control scheme is verified through the tracking control experiments of static contact and dynamic contact in a variety of polishing scenarios, which provides a new idea for the application of pneumatic technology in polishing scenarios.},
  archive      = {J_NEUCOM},
  author       = {Zhiguo Yang and Jiange Kou and Zhanxin Li and Yushan Ma and Wenbo Zhao and Yixuan Wang and Yan Shi},
  doi          = {10.1016/j.neucom.2025.129824},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129824},
  shortjournal = {Neurocomputing},
  title        = {Observer-based adaptive neural network force tracking control for pneumatic polishing system end-effector with actuator saturation},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlation-based switching mean teacher for semi-supervised
medical image segmentation. <em>NEUCOM</em>, <em>633</em>, 129818. (<a
href="https://doi.org/10.1016/j.neucom.2025.129818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean teacher framework is one of the mainstream approaches in semi-supervised medical image segmentation. While training together in the traditional mean teacher framework, the teacher model and the student model share the same structure. An Exponential Moving Average (EMA) updating strategy is applied to optimize the teacher model. Although the EMA approach facilitates a smooth training process, it causes the model coupling and error accumulation problems. These issues constrain the model from precisely delineating the regions of pathological structures, especially for the low-contrast regions in medical images. In this paper, we propose a new semi-supervised segmentation model, namely Correlation-based Switching Mean Teacher (CS-MT), which comprises two teacher models and one student model to alleviate these problems. Particularly, two teacher models adopt a switching training strategy at every epoch to avoid the convergence and similarity between the teacher models and the student model. In addition, we introduce a feature correlation module in each model to leverage the similarity information in the feature maps to improve the model’s predictions. Furthermore, the stochastic process of CutMix operation destroys the structures of organs in medical images, generating adverse mixed results. We propose an adaptive CutMix manner to mitigate the negative effects of these mixed results in model training. Extensive experiments validate that CS-MT outperforms the state-of-the-art semi-supervised methods on the LA, Pancreas-NIH, ACDC and BraTS 2019 datasets.},
  archive      = {J_NEUCOM},
  author       = {Guiyuhan Deng and Hao Sun and Wei Xie},
  doi          = {10.1016/j.neucom.2025.129818},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129818},
  shortjournal = {Neurocomputing},
  title        = {Correlation-based switching mean teacher for semi-supervised medical image segmentation},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffuseVAE++: Mitigating training-sampling mismatch based on
additional noise for higher fidelity image generation. <em>NEUCOM</em>,
<em>633</em>, 129814. (<a
href="https://doi.org/10.1016/j.neucom.2025.129814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated remarkable results in image generation. However, there exist a mismatch between the training and sampling process in current diffusion models, in addition, the U-Net denoising network based on simple residual blocks cannot predict noise information accurately, which affects the generated quality. To address these limitations, we present a novel image generation method that achieves higher fidelity. First, by additionally adding the standard Gaussian noise in the diffusion forward process, which does not disrupt the forward process, our method alleviates the mismatch. Subsequently, an important efficient denoising network based on U-Net is presented, where our proposed Simple Squeeze-Excitation and Simple GLU, combined with Depthwise Separable Convolution, enhance the ability of the model to predict real noise using the Simplified Nonlinear No Activation (SNNA) block. Furthermore, considering the structural characteristics of the baseline model, we introduce an additional cross-attention mechanism to enable DDPM to focus on VAE stage characteristics. Allowing the model to more accurately capture and learn the noise information. Finally, it is shown after extensive experiments the proposed DiffuseVAE++ obtains significant gains in FID scores, improving from 3.84 to 2.41 on CIFAR-10 and from 3.94 to 2.30 on CelebA-64. In particular, the IS scores on CIFAR-10 reaches 10.10, which is comparable to the current state-of-the-art methods competitively ( e.g., U-ViT , StyleGAN2 ).},
  archive      = {J_NEUCOM},
  author       = {Xiaobao Yang and Wei Luo and Hailong Ning and Guorui Zhang and Wei Sun and Sugang Ma},
  doi          = {10.1016/j.neucom.2025.129814},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129814},
  shortjournal = {Neurocomputing},
  title        = {DiffuseVAE++: Mitigating training-sampling mismatch based on additional noise for higher fidelity image generation},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMGN: Text GNN and RWKV MLP-mixer combined with
cross-feature fusion for fake news detection. <em>NEUCOM</em>,
<em>633</em>, 129811. (<a
href="https://doi.org/10.1016/j.neucom.2025.129811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of social media, the influence and harm of fake news have gradually increased, making accurate detection of fake news particularly important. Current fake news detection methods primarily rely on the main text of the news, neglecting the interrelationships between additional texts. We propose a cross-feature fusion network with additional text graph construction to address this issue and improve fake news detection. Specifically, we utilize a text graph neural network (GNN) to model the graph relationships of additional texts to enhance the model’s perception capabilities. Additionally, we employ the RWKV MLP-mixer to process the news text and design a cross-feature fusion mechanism to achieve mutual fusion of different features, thereby improving fake news detection. Experiments on the LIAR, FA-KES, IFND, and CHEF datasets demonstrate that our proposed model outperforms existing methods in fake news detection.},
  archive      = {J_NEUCOM},
  author       = {ShaoDong Cui and Kaibo Duan and Wen Ma and Hiroyuki Shinnou},
  doi          = {10.1016/j.neucom.2025.129811},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129811},
  shortjournal = {Neurocomputing},
  title        = {CMGN: Text GNN and RWKV MLP-mixer combined with cross-feature fusion for fake news detection},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H-SGANet: Hybrid sparse graph attention network for
deformable medical image registration. <em>NEUCOM</em>, <em>633</em>,
129810. (<a href="https://doi.org/10.1016/j.neucom.2025.129810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Convolutional Neural Networks (ConvNets) and Transformers has become a strong candidate for image registration, combining the strengths of both models and utilizing a large parameter space. However, this hybrid model, which treats brain MRI volumes as grid or sequence structures, struggles to accurately represent anatomical connectivity, diverse brain regions, and critical connections within the brain’s architecture. There are also concerns about the computational expense and GPU memory usage of this model. To address these issues, we propose a lightweight hybrid sparse graph attention network (H-SGANet). The network includes Sparse Graph Attention (SGA), a core mechanism based on Vision Graph Neural Networks (ViG) with predefined anatomical connections. The SGA module expands the model’s receptive field and integrates seamlessly into the network. To further enhance the hybrid network, Separable Self-Attention (SSA) is used as an advanced token mixer, combined with depth-wise convolution to form SSAFormer. This strategic integration is designed to more effectively extract long-range dependencies. As a hybrid ConvNet-ViG-Transformer model, H-SGANet offers three key benefits for volumetric medical image registration. It optimizes fixed and moving images simultaneously through a hybrid feature fusion layer and an end-to-end learning framework. Compared to VoxelMorph, a model with a similar parameter count, H-SGANet demonstrates significant performance enhancements of 3.5% and 1.5% in Dice score on the OASIS dataset and LPBA40 dataset, respectively. The code is publicly available at https://github.com/2250432015/H-SGANet/ .},
  archive      = {J_NEUCOM},
  author       = {Yufeng Zhou and Wenming Cao},
  doi          = {10.1016/j.neucom.2025.129810},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129810},
  shortjournal = {Neurocomputing},
  title        = {H-SGANet: Hybrid sparse graph attention network for deformable medical image registration},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled contrastive learning for multilingual multimodal
medical pre-trained model. <em>NEUCOM</em>, <em>633</em>, 129809. (<a
href="https://doi.org/10.1016/j.neucom.2025.129809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilingual multimodal pre-training aims to facilitate the integration of conceptual representations across diverse languages and modalities within a shared, high-dimensional semantic space. This endeavor in healthcare faces challenges related to language diversity, suboptimal multimodal interactions, and an absence of coherent multilingual multimodal representations. In response to these challenges, we introduce a novel multilingual multimodal medical pre-training model. Initially, we employ a strategic augmentation of the medical corpus by expanding the MIMIC-CXR report dataset to 20 distinct languages using machine translation techniques. Subsequently, we develop a targeted label disambiguation technique to address the labeling noise within decoupled contrastive learning. In particular, it categorizes and refines uncertain phrases within the clinical reports based on disease type, promoting finer-grained semantic similarity and improving inter-modality interactions. Building on these proposals, we present a refined multilingual multimodal medical pre-trained model, significantly enhancing the understanding of medical multimodal data and adapting the model to multilingual medical contexts. Experiments reveal that our model outperforms other baselines in medical image classification and multilingual medical image–text retrieval by up to 13.78% and 12.6%, respectively.},
  archive      = {J_NEUCOM},
  author       = {Qiyuan Li and Chen Qiu and Haijiang Liu and Jinguang Gu and Dan Luo},
  doi          = {10.1016/j.neucom.2025.129809},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129809},
  shortjournal = {Neurocomputing},
  title        = {Decoupled contrastive learning for multilingual multimodal medical pre-trained model},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPAM-net: An efficient pose-driven attention-guided
multimodal network for video action recognition. <em>NEUCOM</em>,
<em>633</em>, 129781. (<a
href="https://doi.org/10.1016/j.neucom.2025.129781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multimodal-based human action recognition approaches are computationally intensive, limiting their deployment in real-time applications. In this work, we present a novel and efficient pose-driven attention-guided multimodal network (EPAM-Net) for action recognition in videos. Specifically, we propose eXpand temporal Shift (X-ShiftNet) convolutional architectures for RGB and pose streams to capture spatio-temporal features from RGB videos and their skeleton sequences. The X-ShiftNet tackles the high computational cost of the 3D CNNs by integrating the Temporal Shift Module (TSM) into an efficient 2D CNN, enabling efficient spatiotemporal learning. Then skeleton features are utilized to guide the visual network stream, focusing on keyframes and their salient spatial regions using the proposed spatial–temporal attention block. Finally, the predictions of the two streams are fused for final classification. The experimental results show that our method, with a significant reduction in floating-point operations (FLOPs), outperforms and competes with the state-of-the-art methods on NTU RGB-D 60, NTU RGB-D 120, PKU-MMD, and Toyota SmartHome datasets. The proposed EPAM-Net provides up to a 72.8x reduction in FLOPs and up to a 48.6x reduction in the number of network parameters. The code will be available at https://github.com/ahmed-nady/Multimodal-Action-Recognition .},
  archive      = {J_NEUCOM},
  author       = {Ahmed Abdelkawy and Asem Ali and Aly Farag},
  doi          = {10.1016/j.neucom.2025.129781},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129781},
  shortjournal = {Neurocomputing},
  title        = {EPAM-net: An efficient pose-driven attention-guided multimodal network for video action recognition},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal feature symbiosis for personalized meta-path
generation in heterogeneous networks. <em>NEUCOM</em>, <em>633</em>,
129780. (<a href="https://doi.org/10.1016/j.neucom.2025.129780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heterogeneous graph neural networks (HGNNs), the capture of intricate relationships among various types of entities is essential to achieve advanced machine learning applications. Heterogeneous Information Networks (HINs), composed of interconnected multi-type nodes and edges, face significant challenges in managing semantic diversity and inherent heterogeneity. Traditional methods, which rely on manually designed meta-paths, struggle to adapt dynamically to personalized needs and often neglect the integration of structural and attribute features. To address these limitations, this paper introduces the Cross-Modal Symbiotic Meta-Path Generator (CSMPG) framework. CSMPG integrates two key modules: a Cross-Modal State Generation Module that encodes node structure and attribute information into task-aware state vectors and a Personalized Meta-Path Generation Module that dynamically generates and refines meta-paths using reinforcement learning. By leveraging downstream task feedback, CSMPG optimizes path selection to maximize performance. The framework effectively balances cross-modal feature integration and semantic diversity, uncovering impactful meta-paths that are often overlooked by traditional approaches. Experimental results demonstrate that CSMPG consistently enhances recommendation quality and significantly outperforms structure-only and predefined-path-based models.},
  archive      = {J_NEUCOM},
  author       = {Xiaotong Wu and Liqing Qiu and Weidong Zhao},
  doi          = {10.1016/j.neucom.2025.129780},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129780},
  shortjournal = {Neurocomputing},
  title        = {Cross-modal feature symbiosis for personalized meta-path generation in heterogeneous networks},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot text-to-parameter realtime translation for game
character auto-creation and identity consistency editing.
<em>NEUCOM</em>, <em>633</em>, 129779. (<a
href="https://doi.org/10.1016/j.neucom.2025.129779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern video games, character customization often requires adjusting numerous complex parameters, which can be inefficient. Consequently, there is growing interest in using text-based methods to generate 3D characters. The text-based method enhances players’ immersion in the game while improving development efficiency by simplifying the creation of diverse characters. This paper introduces a Text-to-Parameter Editable model (T2PE) that enables players to rapidly create high-quality game characters from textual descriptions. T2PE leverages supervision of CLIP to optimize both continuous and discrete facial parameters simultaneously. This differs from existing Text-to-Parameter (T2P) models, which rely on evolutionary search algorithm that interacts with the game engine to optimize discrete parameters. We also propose a game character editing method that allows modifications using new textual descriptions while preserving other features. Our experimental results show that the proposed model generates higher-quality 3D game characters than T2P models while significantly reducing latency. We observe that T2PE achieves 26x faster inference times and 3.3% increase in CLIP score on lower TFLOPS devices. To the best of our knowledge, T2PE is the first algorithm to support both text-based character generation and text-based character editing. Building upon this, we develop Fast-T2PE, an end-to-end translator trained using text–parameter pairs obtained from T2PE, which enables even faster generation of game characters. Experimental results show that Fast-T2PE further reduces response latency by 62.5% compared to T2PE.},
  archive      = {J_NEUCOM},
  author       = {Tao Wu and Xu Lu and Jia Yu and Weijun Cao and Wei Chen and Haidi Fan and Zhiqiang Zhang and Gen Dong and Siwei Zhou},
  doi          = {10.1016/j.neucom.2025.129779},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129779},
  shortjournal = {Neurocomputing},
  title        = {Zero-shot text-to-parameter realtime translation for game character auto-creation and identity consistency editing},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A variable-gain fixed-time convergent neurodynamic network
for time-variant quadratic programming under unknown noises.
<em>NEUCOM</em>, <em>633</em>, 129778. (<a
href="https://doi.org/10.1016/j.neucom.2025.129778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a variable-gain fixed-time convergent and noise-tolerant error-dynamics based neurodynamic network (VGFxTNT-EDNN) to solve time-varying quadratic programming problems, while being robust to unknown noises. Unlike existing finite-time convergent EDNNs, the newly designed VGFxTNT-EDNN guarantees fixed-time convergence by dynamically adjusting its variable parameters. Moreover, the VGFxTNT-EDNN effectively handles unknown noise, addressing a limitation of existing fixed-time or predefined-time convergent models, which typically assume that the noise is known. Theoretical analysis utilizing Lyapunov theory proves that the VGFxTNT-EDNN possesses fixed-time convergence and robustness properties. Numerical validations demonstrate superior noise tolerance and fixed-time convergence of the VGFxTNT-EDNN, as compared with the existing models. Finally, a path-tracking experiment is conducted by utilizing a Franka Emika Panda robot to verify the practicality of the VGFxTNT-EDNN.},
  archive      = {J_NEUCOM},
  author       = {Biao Song and Tinghe Hong and Weibing Li and Gang Chen and Yongping Pan and Kai Huang},
  doi          = {10.1016/j.neucom.2025.129778},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129778},
  shortjournal = {Neurocomputing},
  title        = {A variable-gain fixed-time convergent neurodynamic network for time-variant quadratic programming under unknown noises},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LAR-pose: Lightweight human pose estimation with adaptive
regression loss. <em>NEUCOM</em>, <em>633</em>, 129777. (<a
href="https://doi.org/10.1016/j.neucom.2025.129777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, LAR-Pose, a lightweight, high-resolution network for human pose estimation driven by adaptive regression loss is proposed and experimentally demonstrated based on MS COCO and MPII. The architecture of the LAR-Pose comprises two main components. One is a lightweight high-resolution backbone network, which utilizes a parallel high-resolution architecture with conditional channel weighting block to reduce the model size and computational complexity. The other is a dynamic residual refinement network, which calculates residuals from pseudo-heatmaps and scaling factors, improving training concentration for consistent distribution estimation, rather than predicting coordinates or heatmaps directly. Specific coordinates are derived through integral heatmap regression, effectively minimizing quantization errors. Our adaptive regression loss, which uses a flow model to fit the distribution of residuals in real-time, provides more sensitive parameter feedback than conventional heatmap loss, ensuring differentiability and continuity during backpropagation while enhancing performance. With a relatively small parameter scale, LAR-Pose achieves an AP of 73.5 on MS COCO and a PCKh of 90.9 on MPII, while the results outperform most advanced small networks and approach the performance of large networks.},
  archive      = {J_NEUCOM},
  author       = {Xudong Lou and Xin Lin and Henan Zeng and Xiangxian Zhu},
  doi          = {10.1016/j.neucom.2025.129777},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129777},
  shortjournal = {Neurocomputing},
  title        = {LAR-pose: Lightweight human pose estimation with adaptive regression loss},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Passivity for undirected and directed fractional-order
complex networks with adaptive output coupling. <em>NEUCOM</em>,
<em>633</em>, 129774. (<a
href="https://doi.org/10.1016/j.neucom.2025.129774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the passivity for adaptive output coupled fractional-order complex networks (CNs) under undirected and directed topologies is studied by selecting appropriate coupling weight adjustment strategies, based on which the output synchronization for the presented networks is discussed by leveraging the properties of Mittag-Leffler functions and the Laplace transform technique. Utilizing the developed distributed adaptive schemes, several passivity criteria for the fractional-order CNs with output coupling are derived. Moreover, several output synchronization conditions for the output coupled fractional-order CNs are given by employing the acquired passivity results. Ultimately, simulations from numerical examples are utilized to judge the effectiveness for the adaptive laws and the presented criteria.},
  archive      = {J_NEUCOM},
  author       = {Jin-Liang Wang and Chen-Guang Liu and Shun-Yan Ren and Tingwen Huang},
  doi          = {10.1016/j.neucom.2025.129774},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129774},
  shortjournal = {Neurocomputing},
  title        = {Passivity for undirected and directed fractional-order complex networks with adaptive output coupling},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic diagnosis of early pregnancy fetal nasal bone
development based on complex mid-sagittal section ultrasound imaging.
<em>NEUCOM</em>, <em>633</em>, 129773. (<a
href="https://doi.org/10.1016/j.neucom.2025.129773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early prenatal screening of fetal nasal bone (FNB) development is crucial for detecting chromosomal abnormalities. Existing deep learning approaches primarily focus on detection rather than diagnosis of FNB. This paper introduces an early prenatal FNB development automated diagnostic system (FNB-ADS), which employs a cascaded hierarchical filtering method to reduce noise interference in mid-sagittal plane ultrasound images. Specifically, the system employs YOLOv8 for precise FNB localization, segments the nasal bone, tip, and prenasal skin using a specially designed lightweight segmentation network, and diagnoses developmental abnormalities using Resnet34 classification methods. Furthermore, this paper has collected and publicly released the FNB-UDV dataset, which includes a detection subset and a video subset. The detection subset contains 1,007 two-dimensional ultrasound images, while the video subset comprises 12 ultrasound videos. Upon a comprehensive evaluation, the diagnostic accuracy of FNB-ADS reaches 92.37% with a processing time of 0.14 s per image, and the video diagnostic accuracy is 98.69% with a per-frame inference speed of 0.37 s in the FNB-UDV dataset. Representing the first deep-learning approach tailored specifically for early pregnancy FNB ultrasound video diagnosis, FNB-ADS significantly enhances the standardization of diagnostic procedures and reduces the dependence on subjective clinical assessments. The dataset and code are available at https://github.com/SIGMACX/FNB-AD/tree/FNB-ADS .},
  archive      = {J_NEUCOM},
  author       = {Xi Chen and Xiaoyu Xu and Lyuyang Tong and Huangxuan Zhao and Bo Du},
  doi          = {10.1016/j.neucom.2025.129773},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129773},
  shortjournal = {Neurocomputing},
  title        = {Automatic diagnosis of early pregnancy fetal nasal bone development based on complex mid-sagittal section ultrasound imaging},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCU-net: A multi-prior collaborative deep unfolding network
with gates-controlled spatial attention for accelerated MRI
reconstruction. <em>NEUCOM</em>, <em>633</em>, 129771. (<a
href="https://doi.org/10.1016/j.neucom.2025.129771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep unfolding networks (DUNs) have shown promise in accelerating magnetic resonance imaging (MRI); however, they often face challenges such as high computational costs, slow convergence, and difficulty in fully exploiting complementarity in multiple priors. In the present study, we proposed a novel DUN, termed Multi-prior collaborative deep unfolding network (MCU-Net), to address these limitations. Our method features a parallel structure comprising different optimization-inspired subnetworks based on low-rank and sparsity, respectively. We designed a gates-controlled spatial attention module (GSAM), evaluating the relative confidence (RC) and overall confidence (OC) maps for intermediate reconstructions produced by different subnetworks. RC allocates greater weights to the image regions where each subnetwork excels, enabling precise element-wise collaboration. We designed correction modules to enhance the effectiveness in regions where both subnetworks exhibit limited performance, as indicated by low OC values, thereby obviating the need for additional branches. The gate units within GSAMs are designed to filter necessary information across multiple iterations, thus improving the accuracy of the learned confidence maps and enhancing robustness against accumulated errors. Through evaluating four datasets, MCU-Net outperforms state-of-the-art methods, achieving average PSNR improvements of 0.22, 0.24, 0.41 and 0.12 dB, respectively. Furthermore, applying the proposed strategy to various DUNs consistently results in accelerated convergence and improved reconstruction performance without extra FLOPs. Experimental results demonstrated that the proposed collaborative strategy can improve both reconstruction quality and computational efficiency compared to existing MRI reconstruction methods.},
  archive      = {J_NEUCOM},
  author       = {Xiaoyu Qiao and Weisheng Li and Guofen Wang and Yuping Huang},
  doi          = {10.1016/j.neucom.2025.129771},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129771},
  shortjournal = {Neurocomputing},
  title        = {MCU-net: A multi-prior collaborative deep unfolding network with gates-controlled spatial attention for accelerated MRI reconstruction},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MEAI-net: Multiview embedding and attention interaction for
multivariate time series forecasting. <em>NEUCOM</em>, <em>633</em>,
129769. (<a href="https://doi.org/10.1016/j.neucom.2025.129769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series (MTS) forecasting plays a critical role in diverse societal applications, including stock market analysis and climate change research. While many existing deep learning models have been proven to be effective in MTS forecasting through complex neural network structures and self-attention mechanisms, several challenges remain: (1) insufficient modeling of complex temporal dependencies, (2) limited ability to handle information redundancy and noise, and (3) inadequate capture of periodic characteristics of time series. To address these problems, we propose a Multiview time-dependent feature Embedding and downsampled subsequences Attention Interaction Network (MEAI-Net) for MTS forecasting. First, MEAI-Net adopts a multiview time-dependent feature embedding mechanism to extract various temporal dependency features from the sequences. Second, it reduces redundancy in the temporal sequence features through downsampling. Third, a subsequences cross-attention module is introduced to enhance information exchange between subsequences. Furthermore, we propose the period consistency loss designed to more effectively capture periodic patterns in time series data. Comprehensive experiments conducted on 12 widely used time series datasets demonstrate that MEAI-Net displays promising performance, providing a competitive alternative to current state-of-the-art approaches in MTS forecasting.},
  archive      = {J_NEUCOM},
  author       = {Chunru Dong and Wenqing Xu and Feng Zhang and Qiang Hua and Yong Zhang},
  doi          = {10.1016/j.neucom.2025.129769},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129769},
  shortjournal = {Neurocomputing},
  title        = {MEAI-net: Multiview embedding and attention interaction for multivariate time series forecasting},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view clustering via view-specific consensus kernelized
graph learning. <em>NEUCOM</em>, <em>633</em>, 129766. (<a
href="https://doi.org/10.1016/j.neucom.2025.129766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has received extensive and in-depth research attention in recent years owing to its ability to reflect the nature of the real world from multiple perspectives. Kernel-based methods and subspace learning-based methods are two important categories of multi-view clustering. Compared with subspace-based algorithms, kernel-based algorithms can better address nonlinear relationships in feature spaces. However, the current kernel-based algorithms focus mainly on the diversity of different kernels, and obtaining the optimal kernel via linear combinations of multiple kernels, ignoring the cross-view information and space information in the original feature spaces. To address this issue, our paper proposes a novel algorithm named MC-VCKGL. Specifically, we first obtain view-specific consensus kernelized graphs of each view through kernel-based self-representation learning and by using the kernel trick. Moreover, Laplacian constraints are applied to maintain smoothness in the raw feature space of each view. We stack these kernelized graphs together to obtain a tensor, and then rotate this tensor and apply tensor nuclear norm constraints. As a result, the cross-view complementary information can be explored. We apply our algorithm to seven open datasets, including both text and image datasets. Experiments show that our method outperforms most state-of-the-art multi-view clustering algorithms.},
  archive      = {J_NEUCOM},
  author       = {Bing Hu and Tong Wu and Lixin Han and Shu Li and Yi Xu and Gui-fu Lu},
  doi          = {10.1016/j.neucom.2025.129766},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129766},
  shortjournal = {Neurocomputing},
  title        = {Multi-view clustering via view-specific consensus kernelized graph learning},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain-like contour detector following retinex theory and
gestalt perception grouping principles. <em>NEUCOM</em>, <em>633</em>,
129765. (<a href="https://doi.org/10.1016/j.neucom.2025.129765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous contour detection algorithms draw inspiration from biological vision systems. These algorithms imitate the way simple cells extract edges using Gabor filters. They also suppress edges generated by image textures simulating the non-classical receptive fields (NCRFs), thereby popping up the object contours within the image edges. However, these algorithms are not flawless and may yield imperfect results due to noise pollution, unsatisfactory lighting, limitations in image processing algorithms, and likewise. Weak strengths and pixel loss in contour segments are two common issues. In this paper, we provide two strategies to address these challenges. First, we separate the illumination component from the image following Retinex theory, extract the illumination contour using bio-inspired filters, and boost contour strengths by superimposing the illumination contour. Second, we complete object contours by filling small gaps in contours, using a proposed linking likelihood function that is a joint probability of element distance and orientation difference, following Gestalt perceptual grouping principles. Although not performance-oriented, the experimental results show that our endeavors improve the performance of bio-inspired contour detectors. More importantly, we demonstrate the significance of visual computation theories such as the Retinex theory and the Gestalt perception grouping principle for contour detection.},
  archive      = {J_NEUCOM},
  author       = {Rongtai Cai and Helin Que},
  doi          = {10.1016/j.neucom.2025.129765},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129765},
  shortjournal = {Neurocomputing},
  title        = {Brain-like contour detector following retinex theory and gestalt perception grouping principles},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute graph anomaly detection utilizing memory networks
enhanced by multi-embedding comparison. <em>NEUCOM</em>, <em>633</em>,
129762. (<a href="https://doi.org/10.1016/j.neucom.2025.129762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex attribute networks, accurately pinpointing anomalous nodes is vital for grasping network behavior and safeguarding network security. Traditional anomaly detection methods often struggle to fully harness the intricate relationships that underpin attributes and structures, thus curbing their practical effectiveness. To transcend this limitation, we introduce a novel graph anomaly detection model that harmoniously integrates node attributes and structural information. Our model employs multi-embedding contrast modules, coupled with memory network enhancements, to pinpoint anomalous nodes. Precisely, we crafted a multi-embedding contrast module to encode the attributes and structures inherent within nodes, generating a multitude of embedding representations. By scrutinizing the discrepancies between these representations, our model adeptly identifies nodes that deviate from attribute and structural consistency, indicating anomalies. Furthermore, we incorporate a memory network to reconstruct node attributes, thereby enhancing the attribute decoding process while preserving the straightforwardness of structural decoding. To validate our method, we conducted extensive experiments on five authoritative public graph datasets, comparing various graph anomaly detection methods using rigorous metrics such as AUC, precision, and recall. The experimental results unequivocally demonstrate that our proposed method surpasses current state-of-the-art techniques in detecting anomalous nodes within graphs, solidly validating its efficacy.},
  archive      = {J_NEUCOM},
  author       = {Lianming Zhang and Baolin Wu and Pingping Dong},
  doi          = {10.1016/j.neucom.2025.129762},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129762},
  shortjournal = {Neurocomputing},
  title        = {Attribute graph anomaly detection utilizing memory networks enhanced by multi-embedding comparison},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory-based walk-enhanced dynamic graph neural network for
temporal graph representation learning. <em>NEUCOM</em>, <em>633</em>,
129759. (<a href="https://doi.org/10.1016/j.neucom.2025.129759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depending on the ability of obtaining low-dimensional representations of nodes that preserve valuable structural information, graph representation learning has a wide range of applications in graph analysis and inference. However, real-world complex systems are naturally heterogeneous and time-varying, which makes it difficult to learn high-quality node representations. We propose a M emory-based W alk-enhanced D ynamic G raph neural N etwork (denoted as MWDGN) to fully exploit the dependencies and structural features in temporal graph. To capture long-term dependencies, we use a memory module to store and evolve dynamic node representations. MWDGN captures network structural information by constructing time-constrained walk sequences for each interaction node. The walk sequence features are creatively integrated into the update process of memory module, so as to capture the useful information of neighborhood structure features for the interaction node while preserving the long-term dependency of the temporal graph. In addition, we focus on the enlightenment of non-negligible temporal information for sensing key historical interaction nodes of the target node, and design a new aggregation method of historical interaction nodes information. It exploits the temporal attenuation effect of event impact to model short-term dependencies. We further exploit causal convolutional network to mine the potential associations of historical interaction node features of the target node. Comparison experiments on six datasets with mainstream baseline models demonstrate that MWDGN is capable of jointly extracting the heterogeneity and evolutionary patterns of nodes in the graph, improving the node representation quality, and enhancing the performance of the temporal link prediction and dynamic node classification tasks. The effectiveness of the proposed model is further proved by time complexity analysis, ablation study and parameter sensitivity analysis.},
  archive      = {J_NEUCOM},
  author       = {Zhigang Jin and Renjun Su and Hao Zhang and Xiaofang Zhao},
  doi          = {10.1016/j.neucom.2025.129759},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129759},
  shortjournal = {Neurocomputing},
  title        = {Memory-based walk-enhanced dynamic graph neural network for temporal graph representation learning},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse reinforcement learning by expert imitation for the
stochastic linear–quadratic optimal control problem. <em>NEUCOM</em>,
<em>633</em>, 129758. (<a
href="https://doi.org/10.1016/j.neucom.2025.129758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies inverse reinforcement learning (IRL) for the linear–quadratic stochastic optimal control problem, where two agents are considered. A learner agent lacks knowledge of the expert agent’s cost function, but it reconstructs an underlying cost function by observing the expert agent’s states and controls, thereby imitating the expert agent’s optimal feedback control. We initially present a model-based IRL method, which consists of a policy correction and a policy update from the policy iteration in reinforcement learning, as well as a cost function weight reconstruction informed by the inverse optimal control. Afterward, under this scheme, we propose a model-free off-policy IRL method, which requires no system identification, only collecting behavior data from the learner agent and expert agent once during the iteration process. Moreover, the proofs of the method’s convergence, stability, and non-unique solutions are given. Finally, a numerical example and an inverse mean–variance portfolio optimization example are provided to validate the effectiveness of the presented method.},
  archive      = {J_NEUCOM},
  author       = {Zhongshi Sun and Guangyan Jia},
  doi          = {10.1016/j.neucom.2025.129758},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129758},
  shortjournal = {Neurocomputing},
  title        = {Inverse reinforcement learning by expert imitation for the stochastic linear–quadratic optimal control problem},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding before aligning: Scale-adaptive early-decoding
transformer for visual grounding. <em>NEUCOM</em>, <em>633</em>, 129756.
(<a href="https://doi.org/10.1016/j.neucom.2025.129756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual grounding, the task of precisely localizing objects within an image as described by natural language expressions, has recently seen significant advancements through the integration of transformer decoders and early-fusion mechanisms. These approaches aim to enhance the decoding of crucial grounding-related semantics and the generation of discriminative representations. However, they display significant limitations, including reduced generalizability due to the need for custom plug-ins, a gap in effectively bridging decoded visual and linguistic features, and difficulties in processing single-scale features within complex, hierarchical scenes. To address these issues, this study introduces the Scale-Adaptive Early-Decoding Transformer (SA-EDTR), a novel approach that combines decoding and early-fusion mechanisms within a single transformer layer. By positioning the decoding stage before the alignment stage, the decoder adaptively generates richer, context-aware representations that guide the aligning module. This design allows SA-EDTR to effectively overcome previous limitations in feature alignment. Additionally, we integrate a Scale-Adaptive Fusion network (SAF) to fuse multi-scale and heterogeneous representations from parallel Early-Decoding Transformer layers, enhancing the capture of spatial, coarse, and fine-grained semantic details. Extensive experiments demonstrate that SA-EDTR outperforms current state-of-the-art methods in both REC and RES tasks across various mainstream visual grounding datasets, including RefCOCO, RefCOCO+, RefCOCOg-umd, and RefCOCOg-google. The results highlight the architecture’s remarkable capability in accurately localizing objects and interpreting intricate scenes.},
  archive      = {J_NEUCOM},
  author       = {Liuwu Li and Yi Cai and Jiexin Wang and Cantao Wu and Qingbao Huang and Qing Li},
  doi          = {10.1016/j.neucom.2025.129756},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129756},
  shortjournal = {Neurocomputing},
  title        = {Decoding before aligning: Scale-adaptive early-decoding transformer for visual grounding},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Second-order consensus of matrix-weighted switched
multiagent systems. <em>NEUCOM</em>, <em>633</em>, 129755. (<a
href="https://doi.org/10.1016/j.neucom.2025.129755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the switching characteristics of many practical multi-agent systems, such as automatic speed control systems and hybrid quadcopters, for multidimensional individuals, combined with practical complexity, matrix-weighted switching dynamics are needed to model them. This paper considers the consensus issues of second-order switched multi-agent systems on matrix-weighted undirected and directed networks. A new matrix-weighted control algorithm suitable for both CT and DT subsystems is proposed. Under the proposed algorithms, based on variable transformation, matrix theory and stability theory, the consensus criteria are established for undirected and directed switched multi-agent networks that rely on the eigenvalues of the network and coupling gains, respectively. This also indicates that the matrix-weights and coupling gains have a significant impact on switched matrix-weighted consensus. Finally, through simulations, the validity of the obtained results of this essay are verified.},
  archive      = {J_NEUCOM},
  author       = {Suoxia Miao and Housheng Su},
  doi          = {10.1016/j.neucom.2025.129755},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129755},
  shortjournal = {Neurocomputing},
  title        = {Second-order consensus of matrix-weighted switched multiagent systems},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-weighted subspace clustering with adaptive neighbors.
<em>NEUCOM</em>, <em>633</em>, 129754. (<a
href="https://doi.org/10.1016/j.neucom.2025.129754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering has attracted increasing attention in recent years owing to its ability to process high-dimensional data effectively. However, existing subspace clustering methods often assume that different features are equally important, and on this basis, a similarity matrix is constructed to generate the clustering structure. However, this practice may significantly affect the clustering performance in cases where the importance of different features significantly differs or where many noisy features exist in the original data. To address these challenges, we propose a novel self-weighted subspace clustering method with adaptive neighbors (SWSCAN). A feature weighting scheme is introduced to assign appropriate weights to different features. Then, we use the self-expressive property and adaptive neighbors approach to capture both the global and local structures within the weighted data space. Moreover, we employ the alternating direction method of multipliers (ADMM) to effectively solve the optimization problem of SWSCAN. Empirical results on both synthetic and practical datasets validate that our proposed method outperforms other comparative clustering techniques and can learn appropriate weights for features.},
  archive      = {J_NEUCOM},
  author       = {Zhengyan Liu and Huiwen Wang and Lihong Wang and Qing Zhao},
  doi          = {10.1016/j.neucom.2025.129754},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129754},
  shortjournal = {Neurocomputing},
  title        = {Self-weighted subspace clustering with adaptive neighbors},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Routeformer: Transformer utilizing routing mechanism for
traffic flow forecasting. <em>NEUCOM</em>, <em>633</em>, 129753. (<a
href="https://doi.org/10.1016/j.neucom.2025.129753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is vital for the development of intelligent transportation systems. The challenge lies in accurately capturing the complex and dynamic spatiotemporal dependencies influenced by real road network fluctuations. These dependencies can be simplified into three categories: (i) spatial dependencies among sensors at the same timestamp, (ii) temporal dependencies of the same sensor at different timestamps, and (iii) cross dimensional dependencies between different sensors at different timestamps. The third type of cross dimensional dependency requires considering the relationships between different sensors across multiple time points, which is not only complex but also difficult to capture accurately. Existing methods often describe it indirectly by merging spatiotemporal dependencies, but this approach is frequently insufficiently accurate. We aim to characterize this relationship more precisely by capturing the sequential dependencies among sensors, referred to as inter-series dependencies. Capturing inter-series dependencies does not require directly modeling the relationships between different sensors across multiple time points; rather, it focuses on the dependencies between the temporal patterns of different sensors. Our designed Temporal Routing Transformer captures temporal dependencies along the temporal axis while implicitly modeling the inter-series dependencies between sensors. At the same time, we capture spatial dependencies through the Spatial Routing Transformer and multi-scale temporal dependencies by using the Context-Aware Transformer. A series of evaluations were conducted on seven real world datasets, and Routeformer achieved state-of-the-art performance.},
  archive      = {J_NEUCOM},
  author       = {Jun Qi and Hong Fan},
  doi          = {10.1016/j.neucom.2025.129753},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129753},
  shortjournal = {Neurocomputing},
  title        = {Routeformer: Transformer utilizing routing mechanism for traffic flow forecasting},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot medical relation extraction via prompt tuning
enhanced pre-trained language model. <em>NEUCOM</em>, <em>633</em>,
129752. (<a href="https://doi.org/10.1016/j.neucom.2025.129752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical relation extraction is crucial for developing structured information to support intelligent healthcare systems. However, acquiring large volumes of labeled medical data is challenging due to the specialized nature of medical knowledge and privacy constraints. To address this, we propose a prompt-enhanced few-shot relation extraction (FSRE) model that leverages few-shot and prompt learning techniques to improve performance with minimal data. Our approach introduces a hard prompt concatenated to the original input, enabling contextually enriched learning. We calculate prototype representations by averaging the intermediate states of each relation class in the support set, and classify relations by finding the shortest distance between the query instance and class prototypes. We evaluate our model against existing deep learning based FSRE models using three biomedical datasets: the 2010 i2b2/VA challenge dataset, the CHEMPROT corpus, and the BioRED dataset, focusing on few-shot scenarios with limited training data. Our model demonstrates exceptional performance, achieving the highest accuracy across all datasets in most training configurations under a 3-way-5-shot condition and significantly surpassing the current state-of-the-art. Particularly, it achieves improvements ranging from 1.25% to 11.25% on the 2010 i2b2/VA challenge dataset, 3.4% to 20.2% on the CHEMPROT dataset, and 2.73% to 10.98% on the BioRED dataset compared to existing models. These substantial gains highlight the model’s robust generalization ability, enabling it to effectively handle previously unseen relations during testing. The demonstrated effectiveness of this approach underscores its potential for diverse medical applications, particularly in scenarios where acquiring extensive labeled data is challenging.},
  archive      = {J_NEUCOM},
  author       = {Guoxiu He and Chen Huang},
  doi          = {10.1016/j.neucom.2025.129752},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129752},
  shortjournal = {Neurocomputing},
  title        = {Few-shot medical relation extraction via prompt tuning enhanced pre-trained language model},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-model fusion learning for sparse-reward
reinforcement learning. <em>NEUCOM</em>, <em>633</em>, 129748. (<a
href="https://doi.org/10.1016/j.neucom.2025.129748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address intrinsic reward generation for sparse-reward reinforcement learning, where the agent receives limited extrinsic feedback from the environment. Traditional approaches to intrinsic reward generation often rely on prediction errors from a single model, where the intrinsic reward is derived from the discrepancy between the model’s predicted outputs and the actual targets. This approach exploits the observation that less-visited state–action pairs typically yield higher prediction errors. We extend this framework by incorporating multiple prediction models and propose an adaptive fusion technique specifically designed for the multi-model setting. We establish and mathematically justify key axiomatic conditions that any viable fusion method must satisfy. Our adaptive fusion approach dynamically learns the best way to combine prediction errors during training, leading to improved learning performance. Numerical experiments validate the effectiveness of our method, showing significant performance gains across various tasks compared to existing approaches.},
  archive      = {J_NEUCOM},
  author       = {Giseung Park and Whiyoung Jung and Seungyul Han and Sungho Choi and Youngchul Sung},
  doi          = {10.1016/j.neucom.2025.129748},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129748},
  shortjournal = {Neurocomputing},
  title        = {Adaptive multi-model fusion learning for sparse-reward reinforcement learning},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The SIFT based two-stage STC decoupled learning method for
long-tailed SAR target recognition. <em>NEUCOM</em>, <em>633</em>,
129747. (<a href="https://doi.org/10.1016/j.neucom.2025.129747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In remote sensing (RS) research, Deep Learning (DL) based methods have been widely applied to Synthetic Aperture Radar (SAR) auto target recognition with extraordinary success. However, as the real-world SAR images often exhibit imbalanced data distribution, numerous existing DL approaches fail to deliver optimal performance when the dataset conforms to a long-tailed distribution. The long-tailed characteristics lead in learning bias towards the majority classes, resulting in poor performance on the minority classes. In response to this issue, this paper proposes a Scale Invariant Feature Transform (SIFT) based two-stage decoupled learning method for long-tailed SAR target recognition, named STC decoupled learning. The proposed STC decoupled learning method is consist of two stages: the feature extraction learning stage and the classifier fine-tuning stage. Specifically, a supervised contrastive learning module is incorporated to obtain feature representation on long-tailed SAR samples at the first stage. Then, classed-balanced samples are trained for fine-tuning a linear-layer-based classifier with cross-entropy loss at the second stage. Besides, SIFT feature extraction module is employed on these two stages, which substantially leverages physical scattering attribution of SAR images. Extensive simulations on long-tailed SAR-AIRcraft-1.0, FUSAR and MSTAR datasets validate that the proposed method achieves state-of-the-art classification accuracy on long-tailed SAR target recognition tasks, as well as superior generalization ability towards varying scenarios. Our code is now available on https://github.com/XYLGroup/STC .},
  archive      = {J_NEUCOM},
  author       = {Ye Li and Jingyuan Xia and Huaizhang Liao and Xu Lan and Weidong Jiang},
  doi          = {10.1016/j.neucom.2025.129747},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129747},
  shortjournal = {Neurocomputing},
  title        = {The SIFT based two-stage STC decoupled learning method for long-tailed SAR target recognition},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AA-mDLAM: An accelerated ADMM-based framework for training
deep neural networks. <em>NEUCOM</em>, <em>633</em>, 129744. (<a
href="https://doi.org/10.1016/j.neucom.2025.129744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) and its many variants are the widespread optimization algorithms for training deep neural networks. However, SGD suffers from inevitable drawbacks, including vanishing gradients, lack of theoretical guarantees, and substantial sensitivity to input. The Alternating Direction Method of Multipliers (ADMM) has been proposed to address these shortcomings as an effective alternative to the gradient-based methods. It has been successfully employed for training deep neural networks. However, ADMM-based optimizers have a slow convergence rate. This paper proposes an accelerated framework for training deep neural networks, termed AA-mDLAM, which integrates Anderson acceleration within an Alternating Minimization approach inspired by ADMM to tackle this drawback. The main intention of the AA-mDLAM algorithm is to employ Anderson acceleration to alternating minimization by considering it as a fixed-point iteration and attaining a nearly quadratic convergence rate. We verify the effectiveness and efficiency of the proposed AA-mDLAM algorithm by conducting extensive experiments on seven benchmark datasets contrary to other state-of-the-art optimizers.},
  archive      = {J_NEUCOM},
  author       = {Zeinab Ebrahimi and Gustavo Batista and Mohammad Deghat},
  doi          = {10.1016/j.neucom.2025.129744},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129744},
  shortjournal = {Neurocomputing},
  title        = {AA-mDLAM: An accelerated ADMM-based framework for training deep neural networks},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge graph embedding based on embedding permutation and
high-frequency feature fusion for link prediction. <em>NEUCOM</em>,
<em>633</em>, 129743. (<a
href="https://doi.org/10.1016/j.neucom.2025.129743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding has excellent performance in capturing intrinsic relations and semantics in a wealth of information for link prediction. Knowledge graph embedding methods have achieved impressive results in recent years, especially those using convolutional neural networks. However, many previous approaches focus on interactions between relations and entities, ignoring interactions of internal data elements and the crucial role of high-frequency features. In this paper, we propose a novel approach, a knowledge graph Embedding model using 2D convolution operations integrating Embedding permutation strategy and High-frequency features fusion mechanism, named EHE, for link prediction. First, we design the embedding permutation mechanism for the embedding vectors. This mechanism leverages internal element permutation, efficiently broadening the local interactions of internal elements, especially for far-flung data elements in the one-dimensional space. Subsequently, a high-frequency feature fusion module is proposed to capture the high-frequency feature representations by using Sobel and Laplacian operators. Additionally, the projection attention mechanism is utilized to emphasize the unique semantic regions of interest in entities and relations. We assess our approach on several benchmark link prediction datasets. Considering the important metrics, MRR and H@1, our method achieves the overall best performance compared with existing state-of-the-art methods on five public datasets, showcasing its superior capacity for link prediction.},
  archive      = {J_NEUCOM},
  author       = {Qien Yu and Danilo Vasconcellos Vargas},
  doi          = {10.1016/j.neucom.2025.129743},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129743},
  shortjournal = {Neurocomputing},
  title        = {Knowledge graph embedding based on embedding permutation and high-frequency feature fusion for link prediction},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rehearsal-free continual few-shot relation extraction via
contrastive weighted prompts. <em>NEUCOM</em>, <em>633</em>, 129741. (<a
href="https://doi.org/10.1016/j.neucom.2025.129741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary challenge in continual few-shot relation extraction is mitigating catastrophic forgetting. Prevailing strategies involve saving a set of samples in memory and replaying them. However, these methods pose privacy and data security concerns. To address this, we propose a novel rehearsal-free approach called Contrastive Weighted Prompt (CWP). This approach categorizes learnable prompts into task-generic and task-specific prompts. Task-generic prompts are shared across all tasks and are injected into the higher layers of the BERT encoder to capture general task knowledge. Task-specific prompts are generated by weighting all the prompts in a task-specific prompt pool based on their relevance to individual samples. These task-specific prompts are injected into the lower layers of BERT to extract task-specific knowledge. Task-generic prompts retain knowledge from prior tasks, while task-specific prompts reduce mutual interference among tasks and improve the relevance between prompts and individual samples. To further enhance the discriminability of the prompt embeddings for samples belonging to different relations, we introduced a relation-aware contrastive learning strategy. Experimental results on two standard datasets indicate that the proposed method outperforms baseline methods and demonstrates superiority in mitigating catastrophic forgetting.},
  archive      = {J_NEUCOM},
  author       = {Fengqin Yang and Mengen Ren and Delu Kong and Shuhua Liu and Zhiguo Fu},
  doi          = {10.1016/j.neucom.2025.129741},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129741},
  shortjournal = {Neurocomputing},
  title        = {Rehearsal-free continual few-shot relation extraction via contrastive weighted prompts},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic neural learning for obstacle avoidance of humanoid
robot performing cooperative tasks. <em>NEUCOM</em>, <em>633</em>,
129727. (<a href="https://doi.org/10.1016/j.neucom.2025.129727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to avoid the collision between a humanoid robot’s two manipulators is an important problem when two arms cooperate to perform the end task. In this paper, a dynamic neural learning for obstacle avoidance (DNLOA) scheme of a humanoid robot performing cooperative tasks is proposed. In this scheme, the trajectory tracking and mutual collision avoidance of the dual robot arms are converted to a quadratic programming (QP) framework, and the end-effector’s trajectory tracking, mutual collision avoidance and joint angle range are described as equality constraints or inequality constraints. Then the QP framework is solved by a simplified recurrent neural network (S-RNN). The framework is applied to the trajectory tracking tasks of drawing double crossed circles and writing Chinese characters with the humanoid robot’s dual manipulators. With the proposed DNLOA scheme, the tasks are done precisely without dual-arm mutual collision. Compared the scheme we proposed with the minimize velocity norm scheme (MVN) existed, experiments show that the proposed DNLOA scheme for humanoid robot is effective, accurate and practical.},
  archive      = {J_NEUCOM},
  author       = {Yamei Luo and Mingyang Zhang and Yu Liu and Junjie Lin and Zhijun Zhang},
  doi          = {10.1016/j.neucom.2025.129727},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129727},
  shortjournal = {Neurocomputing},
  title        = {Dynamic neural learning for obstacle avoidance of humanoid robot performing cooperative tasks},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). State prediction for multiple diffusion targets based on
point pattern physics-informed neural network. <em>NEUCOM</em>,
<em>633</em>, 129714. (<a
href="https://doi.org/10.1016/j.neucom.2025.129714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State prediction for multiple diffuse targets focuses on locating diffuse sources and concentration distributions. However, in real scenarios, problems such as multi-target overlap, unknown diffusion parameters, and difficult-to-measure environmental disturbances arise, causing the existing algorithms to have poor prediction accuracy. To address the above problems, this article proposes a multi-diffusion target state prediction algorithm based on point pattern physics-informed neural network (PP-PINN). This article targets the overlapping problem of multiple diffusion targets and uses the point pattern model to realize the accurate clustering of different diffusion targets and estimate their concentration distributions. Then, the physics-informed neural network is trained to estimate the diffusion parameters and modify the initial concentration distribution to reduce the environmental interference. The experimental results show that the proposed algorithm can significantly improve the performance of multi-diffusion target state prediction, and can provide important data support for the assessment of hazardous material leakage accidents.},
  archive      = {J_NEUCOM},
  author       = {Qiankun Sun and Lei Cai and Xiaochen Qin},
  doi          = {10.1016/j.neucom.2025.129714},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129714},
  shortjournal = {Neurocomputing},
  title        = {State prediction for multiple diffusion targets based on point pattern physics-informed neural network},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FMVP: Fine-grained meta visual prompt enabled
domain-specific few-shot classification. <em>NEUCOM</em>, <em>633</em>,
129688. (<a href="https://doi.org/10.1016/j.neucom.2025.129688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is a challenging and promising fundamental research. Inspired by recent advances in large language models (LLMs), visual prompt tuning has achieved notable performance gains in few-shot tasks by introducing only limited trainable parameters in the input space. Though effective, prompt tuning in few-shot settings heavily relies on well-initialized soft prompts and often lacks generalizability. Additionally, in certain specific fields, particularly in agriculture, there is a lack of high-precision fine-grained few-shot classification models. To our knowledge, this study is the first to employ prompt tuning for fine-grained few-shot plant disease classification ( specific to disease severity ). Specifically, we propose a novel F ine-grained M eta V isual P rompt tuning (FMVP) framework to systematically explore how visual prompts can enhance the generalizability of fine-grained few-shot domain-specific models. Firstly, a S parsity-aware M eta V isual P rompt tuning (SMVP) sub-module is proposed to learn a universal visual prompt initialization. SMVP utilizes pixel-level optimizable visual prompts for input transformation, jointly with a novel sparsity-aware meta-learning paradigm for parameter updating, boosting generalizability to unseen classes. Secondly, a F ine-grained C ross- A lignment (FCA) module is introduced to explore intra- and inter-image relational patterns, enhancing fine-grained recognition by extracting object-level cross-image semantic discriminative features. Extensive experiments on datasets such as mini -ImageNet, CUB, and FPV have shown that our model outperforms state-of-the-art (SOTA) models. Our work constitutes a valuable addition to domain-specific models for practical applications.},
  archive      = {J_NEUCOM},
  author       = {Minghui Li and Hongxun Yao},
  doi          = {10.1016/j.neucom.2025.129688},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129688},
  shortjournal = {Neurocomputing},
  title        = {FMVP: Fine-grained meta visual prompt enabled domain-specific few-shot classification},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary multi-objective neural architecture search via
depth equalization supernet. <em>NEUCOM</em>, <em>633</em>, 129674. (<a
href="https://doi.org/10.1016/j.neucom.2025.129674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To provide a diverse selection of models suitable for different application scenarios, neural architecture search (NAS) is constructed as a multi-objective optimization problem aiming to simultaneously optimize multiple metrics such as model size and accuracy. Evolutionary algorithms (EA) have been shown to be an effective multi-objective approach that can balance different metrics. However, EA require many evaluations, and the evaluation of architectures is expensive. Training a supernet to evaluate an architecture is considered a promising method to reduce the cost of EA. But there are still many challenges in applying supernet to multi-objective NAS: (1) Supernet tends to give higher scores to shallower architectures, causing potential deeper architectures to be ignored. (2) The receptive field of the architecture has a large gap between search and evaluation, causing a decrease in performance. (3) Larger models are gradually eliminated during evolution, leading to a diversity disaster. We proposed a framework called DESEvo to solve these problems in this paper. DESEvo trains a depth equalization supernet to improve bias of supernet via a frequency rejection sampling method. In addition, DESEvo adaptively constrainted receptive field of architecture to reduce the gap. Finally, DESEvo developed a diversity-preserving strategy to enhance the diversity. Experimental results validate the efficiency and effectiveness of the algorithm, DESEvo can search a set of architectures that are more competitive compared to other state-of-the-art algorithms within 0.2 days, becoming the most efficient multi-objective NAS method in the supernet-based methods.},
  archive      = {J_NEUCOM},
  author       = {Juan Zou and Yang Liu and Yuan Liu and Yizhang Xia},
  doi          = {10.1016/j.neucom.2025.129674},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129674},
  shortjournal = {Neurocomputing},
  title        = {Evolutionary multi-objective neural architecture search via depth equalization supernet},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolving j-CO-QL+ with fuzzy evaluators for flexible
queryisng of JSON data sets. <em>NEUCOM</em>, <em>633</em>, 129621. (<a
href="https://doi.org/10.1016/j.neucom.2025.129621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to introduce soft querying based on fuzzy sets in the novel J-CO-QL + query language (specifically designed to query collections of JSON documents from NoSQL databases) has been investigated by the authors in their past work. Specifically, capabilities for defining fuzzy operators and fuzzy aggregators were introduced through two distinct concepts on which two different language constructs were based. This paper proposes the unified concept of “fuzzy evaluator”, by means of which it is possible to define complex methods for evaluating the membership degrees of JSON documents to fuzzy sets, so as to capture complex semantics while analyzing data in a soft way. The paper both provides a formal meta-model for fuzzy evaluators, and proposes a novel statement for the J-CO-QL + language, so as to further foster soft-querying capabilities.},
  archive      = {J_NEUCOM},
  author       = {Paolo Fosci and Giuseppe Psaila},
  doi          = {10.1016/j.neucom.2025.129621},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129621},
  shortjournal = {Neurocomputing},
  title        = {Evolving J-CO-QL+ with fuzzy evaluators for flexible queryisng of JSON data sets},
  volume       = {633},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural investigations of multi-reservoir echo state
networks for EEG-based emotion classification. <em>NEUCOM</em>,
<em>632</em>, 129856. (<a
href="https://doi.org/10.1016/j.neucom.2025.129856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo State Networks (ESNs) employ non-gradient-based learning mechanisms that bypass backpropagation and pre-training, significantly reducing computational costs and training times, thus facilitating their widespread use in EEG analysis. To date, existing ESN architectures employed for EEG signal processing predominantly utilize single-layer reservoir structures, with limited research exploring the application of multi-reservoir ESN configurations that possess enhanced feature representation capabilities. This study addresses these gaps by proposing and validating three novel multi-reservoir ESN architectures: Pyramid ESN, Inverse Pyramid ESN, and Hourglass ESN, specifically for EEG-based emotion classification. Furthermore, the study introduces a concatenation mechanism that integrates the original data with the outputs from multiple reservoirs, thereby improving the representation power of multi-reservoir ESNs. The results indicate that the Hourglass ESN, which incorporates this concatenation mechanism, achieves the highest performance, yielding classification accuracies of 94.2 % and 94.3 % for arousal and valence, respectively, in binary classification tasks on the DEAP dataset. In four-class and eight-class tasks, accuracies reached 83.6 % and 78.7 %, respectively. Notably, the model demonstrates a 35.52 % average reduction in parameter complexity compared to single-layer ESNs, while achieving performance levels that meet the current state-of-the-art (SOTA) benchmarks for binary and four-class tasks, and exceed SOTA for the eight-class task. This study also investigates the impact of reservoir size on EEG-based emotion classification performance, revealing a saturation effect that provides valuable insights for the parameter design of multi-reservoir ESN models, enabling an optimal balance between computational efficiency and classification performance.},
  archive      = {J_NEUCOM},
  author       = {Yang Liu and Ruiqi Liang and Shule Xu and Xiang Guo},
  doi          = {10.1016/j.neucom.2025.129856},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129856},
  shortjournal = {Neurocomputing},
  title        = {Structural investigations of multi-reservoir echo state networks for EEG-based emotion classification},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-security image steganography integrating multi-scale
feature fusion with residual attention mechanism. <em>NEUCOM</em>,
<em>632</em>, 129838. (<a
href="https://doi.org/10.1016/j.neucom.2025.129838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing a good cost function is crucial for minimizing embedding distortion in image steganography. Recently, deep learning-based adaptive cost learning in image steganography has achieved significant advancements. For GAN-based image steganography, an encoder-decoder structure is typically employed by the generator. However, the continual encoding process often results in a lack of detailed information. Even if the image resolution is restored through skip connections, the generator will still be limited. To address the issue, this paper proposes a novel GAN structure named UMSA-GAN. Firstly, we design a residual attention mechanism, Res-CBAM, integrated into the generator network, which enables focusing on high-frequency regions in the cover image. Secondly, multi-scale feature information is also fused using skip connections, which enables the generator to learn more shallow features. Finally, unlike most of the previous works that only utilized Xu-Net as the discriminator, dual steganalyzers are also introduced as the discriminator to further enhance performance. Extensive comparative experiments demonstrate that UMSA-GAN effectively learns features from the cover images and generates better embedding probability maps. Compared to traditional and state-of-the-art GAN-based steganographic methods, UMSA-GAN exhibits superior security performance. In addition, the rationality and superiority of UMSA-GAN are further verified by a large number of ablation studies.},
  archive      = {J_NEUCOM},
  author       = {Jiaqi Liang and Wei Xie and Haotian Wu and Junfeng Zhao and Xianhua Song},
  doi          = {10.1016/j.neucom.2025.129838},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129838},
  shortjournal = {Neurocomputing},
  title        = {High-security image steganography integrating multi-scale feature fusion with residual attention mechanism},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised medical image segmentation using spiking
neural p-like convolutional model and pseudo label-guided cross-patch
contrastive learning. <em>NEUCOM</em>, <em>632</em>, 129782. (<a
href="https://doi.org/10.1016/j.neucom.2025.129782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have achieved significant improvements in medical image segmentation in recent years. Due to the large amount of labeled data required for neural network training and the difficulty of medical image labeling, semi-supervised learning (SSL) has received great attention in medical image segmentation. However, existing SSL methods often fail to make full use of unlabeled data. Therefore, this study proposes a novel semi-supervised medical image segmentation framework that aims to produce more accurate predictions from unlabeled data. First, a simple and efficient segmentation network is designed, called UNet-ReS. It uses ResNet as the encoder to extract features, and uses spiking neural P-like convolutional neuron model inspired by nonlinear spiking neural P systems to build the decoder. UNet-ReS can generate high-quality unlabeled data prediction in a semi-supervised framework while obtaining reliable pseudo-label. Second, a pseudo-label guided cross-patch contrastive learning loss is proposed to improve the feature representation of deep semantic information from different classes in segmentation prediction. The intra-class aggregation and inter-class separability of deep semantic features from different classes are improved by minimizing the intra-class distance and maximizing the inter-class distance. This helps to improve the overall accuracy of segmentation predictions. The proposed method is validated on three different types of public datasets, including ACDC, Kvasir-SEG, and CRAG. The experimental results show that the proposed method outperforms other semi-supervised segmentation methods.},
  archive      = {J_NEUCOM},
  author       = {Chi Zhou and Lulin Ye and Hong Peng and Jun Wang and Zhicai Liu},
  doi          = {10.1016/j.neucom.2025.129782},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129782},
  shortjournal = {Neurocomputing},
  title        = {Semi-supervised medical image segmentation using spiking neural P-like convolutional model and pseudo label-guided cross-patch contrastive learning},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-based video reconstruction via attention-based
recurrent network. <em>NEUCOM</em>, <em>632</em>, 129776. (<a
href="https://doi.org/10.1016/j.neucom.2025.129776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras are novel sensors that capture brightness changes in the form of asynchronous events rather than intensity frames, offering unique advantages such as high dynamic range, high temporal resolution, and no motion blur. However, the sparse, asynchronous nature of event data poses significant challenges for visual perception, limiting compatibility with conventional computer vision algorithms that rely on dense, continuous frames. Event-based video reconstruction has emerged as a promising solution, though existing methods still face challenges in capturing fine-grained details and enhancing contrast. This paper presents a novel approach to video reconstruction from asynchronous event streams, leveraging the unique properties of event data to produce high-quality video. Our method integrates channel and pixel attention mechanisms to focus on essential features and incorporates deformable convolutions and adaptive mix-up operations to provide flexible receptive fields and dynamic fusion across down-sampling and up-sampling layers. Experimental results on multiple real-world event datasets demonstrate that our approach outperforms comparable methods trained on the same dataset, achieving superior video quality from pure event data. We also demonstrate the capability of our method for high dynamic range reconstruction and color video reconstruction using an event camera equipped with a Bayer filter.},
  archive      = {J_NEUCOM},
  author       = {Wenwen Ma and Shanxing Ma and Pieter Meiresone and Gianni Allebosch and Wilfried Philips and Jan Aelterman},
  doi          = {10.1016/j.neucom.2025.129776},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129776},
  shortjournal = {Neurocomputing},
  title        = {Event-based video reconstruction via attention-based recurrent network},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilateral-aware and multi-scale region guided u-net for
precise breast lesion segmentation in ultrasound images.
<em>NEUCOM</em>, <em>632</em>, 129775. (<a
href="https://doi.org/10.1016/j.neucom.2025.129775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer ranks among the leading health risks for women globally, with the significance of early diagnosis and intervention being paramount. Using computers to segment breast lesions from ultrasound images serves as a crucial auxiliary tool for diagnosing and studying this disease. However, the effectiveness of breast tumor segmentation is deeply impacted by the severe artifacts, the low contrast, and the diverse tumor shapes found in breast ultrasound images. The focus of this study is to devise an effective strategy to further alleviate the aforementioned issues and achieve more precise lesion segmentation. Specifically, we designed a Bilateral-Aware and Multi-Scale Region Guided U-Net (BA-MRGU-Net) with a bilateral perception strategy to segment breast tumors. Initially, we devised a Foreground and Background Aware Module (FBAM), primarily composed of an Adaptive Spatial Selection Unit (ASSU) and a Background Suppression Unit (BSU). The ASSU can help the network capture spatial context information that is more relevant to lesions. Concurrently, the BSU suppresses the feature responses of ultrasound artifacts and other tissues in the background. The FBAM can effectively distinguish between the foreground and background through these two independent branches. Subsequently, we developed a Multi-Scale Region Guided Module (MRGM) to utilize the feature maps across various scales to boost the network’s perception of the lesion region. We executed a range of experiments utilizing two widely used datasets with several state-of-the-art algorithms. The results reveal that our approach achieves improvements of varying degrees on multiple evaluation metrics and has superior segmentation performance.},
  archive      = {J_NEUCOM},
  author       = {Yangyang Li and Xintong Hou and Xuanting Hao and Ronghua Shang and Licheng Jiao},
  doi          = {10.1016/j.neucom.2025.129775},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129775},
  shortjournal = {Neurocomputing},
  title        = {Bilateral-aware and multi-scale region guided U-net for precise breast lesion segmentation in ultrasound images},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An o(1/k) algorithm for multi-agent optimization with
inequality constraints. <em>NEUCOM</em>, <em>632</em>, 129770. (<a
href="https://doi.org/10.1016/j.neucom.2025.129770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a discrete-time solution algorithm for a constrained multi-agent optimization problem with inequality constraints. Its aim is to seek a solution to minimize the sum of all the agents’ objective functions while satisfy each agent’s local set constraint and nonlinear inequality constraints. Assume that agents’ local constraints are heterogeneous and all the objective functions are convex and continuous, but they may not be differentiable. Similar to the distributed alternating direction method of multipliers (ADMM) algorithm, the designed algorithm can solve the multi-agent optimization problem in a distributed manner and has a fast O ( 1 / k ) convergence rate. Moreover, it can deal with the nonlinear constraints, which cannot be handled by distributed ADMM algorithm. Finally, the proposed algorithm is applied to solve a robust linear regression problem, a lasso problem and a decentralized joint flow and power control problem with inequality constraints, respectively and thus the effectiveness of the proposed algorithm is verified.},
  archive      = {J_NEUCOM},
  author       = {Peng Li and Yiyi Zhao and Jiangping Hu and Jiangtao Ji},
  doi          = {10.1016/j.neucom.2025.129770},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129770},
  shortjournal = {Neurocomputing},
  title        = {An o(1/k) algorithm for multi-agent optimization with inequality constraints},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Words shaping worlds: A comprehensive exploration of
text-driven image and video generation with generative adversarial
networks. <em>NEUCOM</em>, <em>632</em>, 129767. (<a
href="https://doi.org/10.1016/j.neucom.2025.129767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing interest in education, media, and entertainment has increased Artificial Intelligence-powered content generation, mainly via Generative Adversarial Networks (GANs), which shape images, videos, audio, and text. A generative adversarial network (GAN) is the combination of two deep neural networks: generator ( G ) and discriminator ( D ). These two components are trained competitively by pitting one against the other such that G generates new data while D authenticates the data. By leveraging powerful deep neural networks and competitive training, GANs can synthesize reasonable and realistic images and videos from the text description. This paper extensively reviews the recent state-of-the-art GAN models for text-to-image (T2I) and text-to-video (T2V) synthesis. In this regard, databases like ACM, IEEE Explore, Web of Science, and ScienceDirect were searched to find and analyze the relevant research articles conducted in this area in the last decade, specifically from 2014 to 2024. Secondly, T2I and T2V GAN methods were classified according to structure and functionality. Later, a comprehensive evaluation between T2I and T2V GAN-based methods was conducted, employing various qualitative and quantitative evaluation techniques. Finally, the paper concludes by discussing multiple applications, main challenges, and limitations of T2I and T2V GAN models for future consideration.},
  archive      = {J_NEUCOM},
  author       = {Anwar Ullah and Muhammad Numan and Mohd Nor Akmal Khalid and Abdul Majid},
  doi          = {10.1016/j.neucom.2025.129767},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129767},
  shortjournal = {Neurocomputing},
  title        = {Words shaping worlds: A comprehensive exploration of text-driven image and video generation with generative adversarial networks},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TDCR: Transformer based decision conflict resolution model
for collaborative scheduling. <em>NEUCOM</em>, <em>632</em>, 129760. (<a
href="https://doi.org/10.1016/j.neucom.2025.129760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the strong adaptability and iterative capabilities of artificial intelligent algorithms, more and more enterprises are adopting human–machine collaboration to replace traditional enterprise management methods. So in recent years, the use of evolutionary algorithms and reinforcement learning methods to solve multi-objective optimization has made many breakthroughs, but in the intelligent manufacturing industry, such as production scheduling and inventory management and other aspects of decision optimization, because of the actual situation is too complicated, it is not conducive to determine the appropriate constraints, and then affect the performance of the model. This paper proposes a multi-decision optimization model for collaborative scheduling, which comprises two key components: a conflict resolution strategy module and a decision making module. This paper uses the attention mechanism to generate decision preference vectors in different manufacturing scenarios, so that the conflict resolution strategy can be dynamically changed and adds the keyword mask method close to downstream tasks during training to further improve the performance of the model. Finally, we evaluate the performance of our model in the conflict resolution task by selecting multiple data sets from multiple public data sets, and show satisfactory performance in this task, showing robustness in different scenarios. This study provides a valuable reference for conflict resolution between decision making.},
  archive      = {J_NEUCOM},
  author       = {Xiancheng Hu and Jian An and Xiaolin Gui and Long Chen and Siyu Tang and Xin He},
  doi          = {10.1016/j.neucom.2025.129760},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129760},
  shortjournal = {Neurocomputing},
  title        = {TDCR: Transformer based decision conflict resolution model for collaborative scheduling},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Person verification and recognition by combining voice
signal and online handwritten signature using hyperbolic function based
transformer neural network. <em>NEUCOM</em>, <em>632</em>, 129751. (<a
href="https://doi.org/10.1016/j.neucom.2025.129751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of smart portable electronic gadgets, various voice based online person verification systems have been developed. However, these systems are susceptible to attacks where an illegitimate individual feeds a recorded voice of a legitimate person, resulting in false confirmations. To overcome this limitation of voice based person verification systems, this article proposes a hyperbolic function based encoded representation of transformer neural network (ERTNN) framework for person verification and recognition by combining online handwritten signature of the genuine person with his/her voice signal. The proposed hyperbolic function based ERTNN framework for person verification and recognition consists of one multi-headed attention layer with positional encoding, seven convolution layers with skip connections, two dense layers, and an output layer. The positional encoding scheme in the proposed hyperbolic function based ERTNN framework has been implemented using hyperbolic s i n e and hyperbolic c o s i n e functions. The mel-frequency cepstral coefficients (MFCC), MFCC-delta, and MFCC-delta-delta features of the voice signal have been combined with all the temporal points of online handwritten signature sample of a person to make a combined feature matrix. The combined feature matrix of voice signal and online handwritten signature has been fed as an input to the proposed framework to verify and recognize a person corresponding to the input feature matrix. The novelty of this work lies in proposing the hyperbolic function based positional encoding scheme in the ERTNN framework . The experiments have also been carried out using traditional ERTNN framework employing sinusoidal function based positional encoding scheme, learnable positional encoding based ERTNN framework, relative positional encoding based ERTNN framework as well as by removing the positional encoding scheme from the multi-headed attention layer to have a performance comparison with the proposed framework.},
  archive      = {J_NEUCOM},
  author       = {Rohitesh Kumar and Rajib Ghosh},
  doi          = {10.1016/j.neucom.2025.129751},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129751},
  shortjournal = {Neurocomputing},
  title        = {Person verification and recognition by combining voice signal and online handwritten signature using hyperbolic function based transformer neural network},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transref: Multi-scale reference embedding transformer for
reference-guided image inpainting. <em>NEUCOM</em>, <em>632</em>,
129749. (<a href="https://doi.org/10.1016/j.neucom.2025.129749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting for completing complicated semantic environments and diverse hole patterns of corrupted images is challenging even for state-of-the-art learning-based inpainting methods trained on large-scale data. A reference image capturing the same scene of a corrupted image offers informative guidance for completing the corrupted image as it shares similar texture and structure priors to that of the holes of the corrupted image. In this work, we propose a Trans former-based encoder–decoder network for Ref erence-guided image inpainting, named TransRef . Specifically, the guidance is conducted progressively through a reference embedding procedure, in which the referencing features are subsequently aligned and fused with the features of the corrupted image. For precise utilization of the reference features for guidance, a reference-patch alignment (Ref-PA) module is proposed to align the patch features of the reference and corrupted images and harmonize their style differences, while a reference-patch transformer (Ref-PT) module is proposed to refine the embedded reference feature. Moreover, to facilitate the research of reference-guided image restoration tasks, we construct a publicly accessible benchmark dataset containing 50K pairs of input and reference images. Both quantitative and qualitative evaluations demonstrate the efficacy of the reference information and the proposed method over the state-of-the-art methods in completing complex holes. Code and dataset can be accessed at: https://github.com/Cameltr/TransRef .},
  archive      = {J_NEUCOM},
  author       = {Taorong Liu and Liang Liao and Delin Chen and Jing Xiao and Zheng Wang and Chia-Wen Lin and Shin’ichi Satoh},
  doi          = {10.1016/j.neucom.2025.129749},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129749},
  shortjournal = {Neurocomputing},
  title        = {Transref: Multi-scale reference embedding transformer for reference-guided image inpainting},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmentation of 3D neuronal morphologies in microscopy
images utilizing flexible open-curve snakes. <em>NEUCOM</em>,
<em>632</em>, 129742. (<a
href="https://doi.org/10.1016/j.neucom.2025.129742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active contour models, known as snakes, have effectively shown their capability to track and recreate neurites in three dimensions. Nevertheless, their efficacy is constrained when addressing attenuated filament signals tainted by noise. Moreover, tracking outcomes are contingent upon initial seeds, as they depend exclusively on forces generated from image gradients. To address these issues and enhance the traditional OCS tracker through the utilization of advanced algorithms, we present the Flexible Open Curve Snakes (FOCS). This unique framework synthesizes various forces via energy minimization and facilitates mutual reinforcement for localization, directional determination, and concurrent radii estimation to finalize the neurites’ structure. In FOCS, the open-curve tracking process undergoes dynamic deformation via iterative techniques that incorporate new deformation energy, modifications to strain orientations, and alterations in local radii. This method is perpetually enhanced by reducing a controllable energy function that encompasses the fitting forces and the curve’s length. FOCS effectively integrates snake architectures into a tracing framework, capturing the extensive information in volumetric neural data, hence addressing the issues of segmenting, tracing, and fully reconstructing neural structures in their natural environment. We illustrate the remarkable efficacy of FOCS by assessing many datasets, including BigNeuron (human and mouse) and Diadem. In these assessments, FOCS regularly surpassed current neuron tracking and tracing methodologies, achieving superior performance. Our approach significantly enhances performance measures, yielding gains of approximately 1.7% and 17% in the average overlap and distance scores on the BigNeuron dataset, respectively. Furthermore, it demonstrates a significant enhancement of roughly 4.1% in the average overlap score on the DIADEM dataset. The suggested approach is applicable to confocal and two-photon microscopy datasets, as demonstrated by the datasets employed.},
  archive      = {J_NEUCOM},
  author       = {Amir Vatani and Jie Song and Liang Xiao},
  doi          = {10.1016/j.neucom.2025.129742},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129742},
  shortjournal = {Neurocomputing},
  title        = {Segmentation of 3D neuronal morphologies in microscopy images utilizing flexible open-curve snakes},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSTM and GRU type recurrent neural networks in model
predictive control: A review. <em>NEUCOM</em>, <em>632</em>, 129712. (<a
href="https://doi.org/10.1016/j.neucom.2025.129712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) neural networks are known for their capability of modeling numerous dynamical phenomena. Model Predictive Control (MPC) refers to a family of advanced control methods in which a dynamical model predicts online the future behavior of the controlled process, and an optimization procedure finds the best control policy. From the point of view of the control quality and the computational complexity of MPC, two issues are crucial: the model structure and the way the model is used in MPC. Both factors determine the resulting control quality and computational complexity of MPC. This article reviews possible methods of using LSTM and GRU type networks in MPC. First, we characterize several model variants that are utilized in MPC. Next, we review possible approaches to MPC based on LSTMs and GRUs, particularly the MPC methods leading to low computational complexity. Stability and robustness issues are also discussed. For a chemical pH reactor, the efficiency of LSTM and GRU models and a few neural network-based MPC algorithms are compared. Finally, we review numerous applications, including applications to real processes, hardware-in-the-loop solutions and example simulation studies.},
  archive      = {J_NEUCOM},
  author       = {Maciej Ławryńczuk and Krzysztof Zarzycki},
  doi          = {10.1016/j.neucom.2025.129712},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129712},
  shortjournal = {Neurocomputing},
  title        = {LSTM and GRU type recurrent neural networks in model predictive control: A review},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SHoTGCN: Spatial high-order temporal GCN for skeleton-based
action recognition. <em>NEUCOM</em>, <em>632</em>, 129697. (<a
href="https://doi.org/10.1016/j.neucom.2025.129697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition algorithms that leverage human skeleton motion data are highly attractive due to their robustness and high information density. Currently, the majority of algorithms in this domain employ graph convolutional neural networks (GCNs). However, these algorithms often neglect the extraction of high-order features. To address this limitation, we propose a novel approach called the Spatial High-Order Temporal Graph Convolution Network (SHoTGCN), designed to evaluate the impact of high-order features on human action recognition. Our method begins by deriving high-order features from human skeleton time series data through temporal interactions. Utilizing these high-order features significantly improves the algorithm’s ability to recognize human actions. Moreover, we found that the traditional feature extraction method, which employs Depthwise Convolution (DWConv) with a single 2D convolution, is suboptimal compared to a multibranch structure for feature extraction. To address this, we introduce a structure re-parameterization technique with DWConv, termed Rep-tDWConv, to enhance feature extraction. By integrating the Exponential Moving Average (EMA) model during the model fusion process, our proposed model achieves state-of-the-art (SOTA) performance, with accuracies of 90.4% and 92.0% on the XSub and XSet splits of the NTU RGB+D 120 dataset, respectively.},
  archive      = {J_NEUCOM},
  author       = {Qiyu Liu and Ying Wu and Bicheng Li and Yuxin Ma and Hanling Li and Yong Yu},
  doi          = {10.1016/j.neucom.2025.129697},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129697},
  shortjournal = {Neurocomputing},
  title        = {SHoTGCN: Spatial high-order temporal GCN for skeleton-based action recognition},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPDRM: A multi-scale personalized depression recognition
model via facial movements. <em>NEUCOM</em>, <em>632</em>, 129669. (<a
href="https://doi.org/10.1016/j.neucom.2025.129669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic depression recognition based on facial movements in videos has become a research hotspot. However, existing methods tend to confuse individual inherent facial behavioral habits with characteristics specific to depression, which leads to misjudgments. To address this, we propose a Multi-scale Personalized Depression Recognition Model (MPDRM) that mitigates the negative impact of individual differences, enabling the model to focus on general and robust facial depression cues. The proposed model consists of three main components: the Multi-scale Depression Feature Network (MDFN), the Multi-scale Personality Feature Network (MPFN), and the Relational Attention Recognition Module (RARM). The MDFN extracts depression-related information, while the contrastive learning-based MPFN extracts stable personalized information. In both MDFN and MPFN, we insert the Multi-scale Motion Pattern Extraction Module (MMP) to capture rich multi-scale spatiotemporal facial features. Finally, the RARM is designed to enhance the representation of depression and output the results. Cross-validation on a specifically constructed longitudinal dataset demonstrates that our model outperforms other models. Experimental results indicate that suppressing personalized information of facial movements can effectively improve the accuracy of depression recognition.},
  archive      = {J_NEUCOM},
  author       = {Zhenyu Liu and Bailin Chen and Shimao Zhang and Jiaqian Yuan and Yang Wu and Hanshu Cai and Xin Chen and Lin Liu and Yimiao Zhao and Huan Mei and Jiahui Deng and Yanping Bao and Bin Hu},
  doi          = {10.1016/j.neucom.2025.129669},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129669},
  shortjournal = {Neurocomputing},
  title        = {MPDRM: A multi-scale personalized depression recognition model via facial movements},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). A comprehensive overview of generative AI (GAI):
Technologies, applications, and challenges. <em>NEUCOM</em>,
<em>632</em>, 129645. (<a
href="https://doi.org/10.1016/j.neucom.2025.129645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Artificial Intelligence (GAI) represents a forefront research domain and demonstrates the ability to generate innovative and creative content spanning text, images, audio, videos, and other technological forms. Recent breakthroughs in GAI, exemplified by remarkable products like ChatGPT and stable diffusion, have garnered significant attention and hold immense potential to shape the trajectory of societal development. This paper undertakes a comprehensive analysis of the current capabilities and limitations of GAI while exploring optimal strategies for its future application. Specifically, we provide an extensive overview of technical approaches employed in GAI, encompassing text, images, videos, audio, and multi-modal generation models. Furthermore, we summarize the commonly utilized training datasets and evaluation benchmarks of various modalities. These benchmarks serve as integral components for assessing the performance of GAI models. Subsequently, we delve into the current applications and future prospects of GAI across various fields. Finally, we discuss the challenges inherent in GAI and outline prospective directions for future advancements in the field, with the intention of offering valuable insights and inspiration to researchers.},
  archive      = {J_NEUCOM},
  author       = {Zhijun Zhang and Jian Zhang and Xiaodong Zhang and Weijian Mai},
  doi          = {10.1016/j.neucom.2025.129645},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129645},
  shortjournal = {Neurocomputing},
  title        = {A comprehensive overview of generative AI (GAI): Technologies, applications, and challenges},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model adaptive parameter fine-tuning based on contribution
measure for image classification. <em>NEUCOM</em>, <em>632</em>, 129634.
(<a href="https://doi.org/10.1016/j.neucom.2025.129634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-tuning is an important transfer learning technique that has achieved significant success in various image classification tasks lacking training data and requires only a small number of training epochs to achieve satisfactory results. However, with the increasing complexity of the model scale and structure, designing appropriate fine-tuning schemes for specific target tasks becomes increasingly difficult. In this paper, a contribution measure criterion is used to quantify the importance of the pre-trained model parameters to the target task, providing a basis for selecting fine-tuning parameters. In addition, we find that the fine-tuning ratio vary depends on the specific target task. Therefore, we propose an adaptive fine-tuning ratio search strategy to search the appropriate fine-tuning ratio for the given target task. Based on the above strategy, we propose an adaptive fine-tuning algorithm based on parameter contribution to customize the fine-tuning scheme for the target task. The experimental results show that the proposed algorithm can effectively quantify the contribution of model parameters, and our algorithm can adaptively adjust the fine-tuning ratio for the target task. Furthermore, our algorithm achieves state-of-the-art performance on seven publicly available image classification datasets widely used in transfer learning.},
  archive      = {J_NEUCOM},
  author       = {Le Feng and Fujian Feng and Yuan Yang and Mian Tan and Lin Wang},
  doi          = {10.1016/j.neucom.2025.129634},
  journal      = {Neurocomputing},
  month        = {6},
  pages        = {129634},
  shortjournal = {Neurocomputing},
  title        = {Model adaptive parameter fine-tuning based on contribution measure for image classification},
  volume       = {632},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
