<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CVIU_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cviu---5">CVIU - 5</h2>
<ul>
<li><details>
<summary>
(2025). Adversarial style mixup and improved temporal alignment for
cross-domain few-shot action recognition. <em>CVIU</em>, <em>255</em>,
104341. (<a href="https://doi.org/10.1016/j.cviu.2025.104341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Few-Shot Action Recognition (CDFSAR) aims at transferring knowledge from base classes to novel ones with limited labeled data, under distribution shift between base (source domain) and novel (target domain) classes. This paper addresses the issues of insufficient style coverage for the target domain and potential temporal misalignment with chronological order in existing methods. To mitigate distribution shifts across domains, we propose an Adversarial Style Mixup (ASM) module, which enriches the diversity of style distributions covering the target domain. ASM mixes up source and target domain styles through statistical means and variances, with the adversarially learned mixup ratio and style noise. On the other hand, we design an Improved Temporal Alignment (ITA) module to address the issue of temporal misalignment between videos. In the proposed ITA, keyframes are extracted as priors for better temporal alignment with a temporal mixer to reduce the misalignment noise. Extensive experiments on video action recognition datasets demonstrates the superiority of our method compared with the state of the arts for the challenging problem of CDFSAR. Ablation study validates that both the proposed ASM and ITA modules contribute to performance improvement by style distribution expansion and keyframe-based temporal alignment.},
  archive      = {J_CVIU},
  author       = {Kaiyan Cao and Jiawen Peng and Jiaxin Chen and Xinyuan Hou and Andy J. Ma},
  doi          = {10.1016/j.cviu.2025.104341},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104341},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial style mixup and improved temporal alignment for cross-domain few-shot action recognition},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Syntactically and semantically enhanced captioning network
via hybrid attention and POS tagging prompt. <em>CVIU</em>,
<em>255</em>, 104340. (<a
href="https://doi.org/10.1016/j.cviu.2025.104340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning has become a thriving research area, with current methods relying on static visuals or motion information. However, videos contain a complex interplay between multiple objects with unique temporal patterns. Traditional techniques struggle to capture this intricate connection, leading to inaccurate captions due to the gap between video features and generated text. Analyzing these temporal variations and identifying relevant objects remains a challenge. This paper proposes SySCapNet, a novel deep-learning architecture for video captioning, designed to address this limitation. SySCapNet effectively captures objects involved in motions and extracts spatio-temporal action features. This information, along with visual features and motion data, guides the caption generation process. We introduce a groundbreaking hybrid attention module that leverages both visual saliency and spatio-temporal dynamics to extract highly detailed and semantically meaningful features. Furthermore, we incorporate part-of-speech tagging to guide the network in disambiguating words and understanding their grammatical roles. Extensive evaluations on benchmark datasets demonstrate that SySCapNet achieves superior performance compared to existing methods. The generated captions are not only informative but also grammatically correct and rich in context, surpassing the limitations of basic AI descriptions.},
  archive      = {J_CVIU},
  author       = {Deepali Verma and Tanima Dutta},
  doi          = {10.1016/j.cviu.2025.104340},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104340},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Syntactically and semantically enhanced captioning network via hybrid attention and POS tagging prompt},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FrTrGAN: Single image dehazing using the frequency component
of transmission maps in the generative adversarial network.
<em>CVIU</em>, <em>255</em>, 104336. (<a
href="https://doi.org/10.1016/j.cviu.2025.104336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazy images, particularly in outdoor scenes, have reduced visibility due to atmospheric particles, making image dehazing a critical task for enhancing visual clarity. The main challenges in image dehazing involve accurately detecting and removing haze while preserving fine details and maintaining overall image quality. Many existing dehazing methods struggle with varying haze conditions, often compromising the structural and perceptual integrity of the restored images. In this paper, we introduce FrTrGAN, a framework for single-image dehazing that leverages the frequency components of transmission maps. This novel framework addresses these challenges by integrating the Fourier Transform within an enhanced CycleGAN architecture. Unlike traditional spatial-domain dehazing methods, FrTrGAN operates in the frequency domain, where it isolates low-frequency haze components – responsible for blurring fine details – and removes them more precisely. The Inverse Fourier Transform is then applied to map the refined data back to the spatial domain, ensuring that the resulting images maintain clarity, sharpness, and structural integrity. We evaluate our method on multiple datasets, including HSTS, SOTS Outdoor, O-Haze, I-Haze, D-Hazy, BeDDE and Dense-Haze using PSNR and SSIM for quantitative performance assessment. Additionally, we include results based on non-referential metrics such as FADE, SSEQ, BRISQUE and NIQE to further evaluate the perceptual quality of the dehazed images. The results demonstrate that FrTrGAN significantly outperforms existing methods while effectively restoring both frequency components and perceptual image quality. This comprehensive evaluation highlights the robustness of FrTrGAN in diverse haze conditions and underscores the effectiveness of a frequency-domain approach to image dehazing, laying the groundwork for future advancements in the field.},
  archive      = {J_CVIU},
  author       = {Pulkit Dwivedi and Soumendu Chakraborty},
  doi          = {10.1016/j.cviu.2025.104336},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104336},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FrTrGAN: Single image dehazing using the frequency component of transmission maps in the generative adversarial network},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hexagonal mesh-based neural rendering for real-time
rendering and fast reconstruction. <em>CVIU</em>, <em>255</em>, 104335.
(<a href="https://doi.org/10.1016/j.cviu.2025.104335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although recent neural rendering-based methods can achieve high-quality geometry and realistic rendering results in multi-view reconstruction, they incur a heavy computational burden on rendering and training, which limits their application scenarios. To address these challenges, we propose an effective mesh-based neural rendering approach which leverages the characteristic of meshes being able to achieve real-time rendering. Besides, a coarse-to-fine scheme is introduced to efficiently extract the initial mesh so as to significantly reduce the reconstruction time. More importantly, we suggest a hexagonal mesh model to preserve surface regularity by constraining the second-order derivatives of its vertices, where only low level of positional encoding is engaged for neural rendering. Experiments show that our approach significantly reduces the rendering time from several tens of seconds to 0.05s compared to methods based on implicit representation. And it can quickly achieve state-of-the-art results in novel view synthesis and reconstruction. Our full implementation will be made publicly available at https://github.com/FuchengSu/FastMesh .},
  archive      = {J_CVIU},
  author       = {Yisu Zhang and Jianke Zhu and Lixiang Lin},
  doi          = {10.1016/j.cviu.2025.104335},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104335},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hexagonal mesh-based neural rendering for real-time rendering and fast reconstruction},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic anchor: Density map guided small object detector for
tiny persons. <em>CVIU</em>, <em>255</em>, 104325. (<a
href="https://doi.org/10.1016/j.cviu.2025.104325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the application of aerial and space-based equipments, such as drones in the search and rescue process, there is an increasing demand on the detection of small and even tiny human targets. However, most existing detectors rely on generating smaller and denser anchors for small target detection, which introduces a high number of redundant negative anchor samples. To alleviate this issue, we propose a novel density map-guided tiny person detector with dynamic anchor. Specifically, we elaborately design an Anchor Proposals Mask (APM) module to effectively eliminate negative anchor samples and adaptively adjust anchor distribution with the guidance of density maps produced by Density Map Generator (DMG). To promote the quality of the density map, we develop a Multi-Scale Feature Distillation (MSFD) module and incorporate the Focal Inverse Distance Transform (FIDT) map to conduct knowledge distillation for DMG with the assistance of the crowd counting network. Extensive experiments on the TinyPerson and VisDrone datasets demonstrate that our method significantly enhances the performance of two-stage detectors in terms of average precision (AP) and average recall (AR) while effectively reducing the impact of negative anchor boxes.},
  archive      = {J_CVIU},
  author       = {Xingzhou Xu and Zhaoyong Mao and Xin Wang and Qinhao Tu and Junge Shen},
  doi          = {10.1016/j.cviu.2025.104325},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {104325},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dynamic anchor: Density map guided small object detector for tiny persons},
  volume       = {255},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
