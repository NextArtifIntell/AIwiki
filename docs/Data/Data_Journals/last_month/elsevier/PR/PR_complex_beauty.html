<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr---83">PR - 83</h2>
<ul>
<li><details>
<summary>
(2025). A feature pair-based neural network embedded decision tree
for synergistic drug combination prediction. <em>PR</em>, <em>164</em>,
111608. (<a href="https://doi.org/10.1016/j.patcog.2025.111608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of combination therapy, it&#39;s a vital step to evaluate the synergistic effects of anti-tumor drug pairs. However, most existing machine learning methods only focus on the attributes of individual drugs, overlooking the implicit relationships of drug pairs, which are essential for understanding their synergistic effects. To address this issue, this paper constructs a novel Neural network Embedded Decision Tree model (NEDT) under a novel paradigm of synergistic drug combination prediction, synonymous feature pairing for drug pairs . It matches synonymous features of drug pairs to construct molecular-level correlations, capturing the implicit relationships of drugs. Our work distinguishes itself from previous neural decision trees by introducing a bi-objective optimization strategy into the fine-tuning process. Experimental results validate that NEDT performs well in predicting synergistic drug combinations. Systematically interpretability analyses demonstrate that NEDT can yield valuable insights into drug synergy, confirming its potential in the biomedical field.},
  archive      = {J_PR},
  author       = {Jiayu Zou and Lianlian Wu and Kunhong Liu and Yong Xu and Song He and Xiaochen Bo},
  doi          = {10.1016/j.patcog.2025.111608},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111608},
  shortjournal = {Pattern Recognition},
  title        = {A feature pair-based neural network embedded decision tree for synergistic drug combination prediction},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor-based incomplete multiple kernel clustering with
auto-weighted late fusion alignment. <em>PR</em>, <em>164</em>, 111601.
(<a href="https://doi.org/10.1016/j.patcog.2025.111601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, the rapid increase in data volume is accompanied by substantial missing data issues. Incomplete multiple kernel clustering (IMKC) investigates how to perform clustering when certain rows or columns of the predefined kernel matrix are missing. Among existing IMKC methods, the recent proposed late fusion IMKC (LF-IMKC) algorithm has garnered considerable attention due to its superior clustering accuracy and computational efficiency. However, existing LF-IMKC algorithms still suffer from several limitations. Firstly, we observe that in existing methods, the missing kernel imputation, kernel partition learning and subsequent late fusion processes are treated separately, which may lead to suboptimal solutions and adversely affect the clustering performance. Secondly, existing LF-IMKC algorithms treat each base partition equally, overlooking the differences in their contributions to the consistent clustering process. Thirdly, Existing algorithms typically overlook the higher-order correlations between the base partitions as well as the strong correlations between the base and consensus partitions, let alone leveraging these correlations for clustering. To address these issues, we propose a novel method, i.e., tensor-based incomplete multiple kernel clustering with auto-weighted late fusion alignment (TIKC-ALFA). Specifically, we first integrate the missing kernel imputation, base partition learning and subsequent late fusion processes within a unified framework. Secondly, we construct a third-order tensor using the weighted base partitions, offering an innovative perspective on tensor slices through the lens of weight distribution and then utilize the tensor nuclear norm (TNN) to approximate the true rank of the tensor. Furthermore, we incorporate the consensus partition into the tensor structure originally constructed solely from weighted base partitions to further investigate the strong correlations between the base partitions and the consensus partition. The experimental results on six commonly used datasets demonstrate the effectiveness of our algorithm.},
  archive      = {J_PR},
  author       = {Xiaoxing Guo and Gui-Fu Lu},
  doi          = {10.1016/j.patcog.2025.111601},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111601},
  shortjournal = {Pattern Recognition},
  title        = {Tensor-based incomplete multiple kernel clustering with auto-weighted late fusion alignment},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPE: Multi-frame prediction error-based video anomaly
detection framework for robust anomaly inference. <em>PR</em>,
<em>164</em>, 111595. (<a
href="https://doi.org/10.1016/j.patcog.2025.111595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As video surveillance has become increasingly widespread, the necessity of video anomaly detection to support surveillance-related tasks has grown significantly. We propose a novel multi-frame prediction error-based framework (MPE) to enhance anomaly detection accuracy and efficiency. MPE mitigates false positives in prediction models by leveraging multi-frame prediction errors and reduces the time required for their generation through a frame prediction error storage method. The core idea of MPE is to reduce the prediction error of a normal frame while increasing the prediction error of an abnormal frame by leveraging the prediction errors of adjacent frames. We evaluated our method on the Ped2, Avenue, and ShanghaiTech datasets. The experimental results demonstrate that MPE improved the frame-level area under the curve (AUC) of prediction models while maintaining low computational overhead across all datasets. These results show that MPE makes prediction models robust and efficient for video anomaly detection in real-world scenarios.},
  archive      = {J_PR},
  author       = {Yujun Kim and Young-Gab Kim},
  doi          = {10.1016/j.patcog.2025.111595},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111595},
  shortjournal = {Pattern Recognition},
  title        = {MPE: Multi-frame prediction error-based video anomaly detection framework for robust anomaly inference},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HaarFuse: A dual-branch infrared and visible light image
fusion network based on haar wavelet transform. <em>PR</em>,
<em>164</em>, 111594. (<a
href="https://doi.org/10.1016/j.patcog.2025.111594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared-visible image fusion remains challenging due to the inherent conflict between preserving multi-modal complementary features and minimizing reconstruction loss. Existing methods often suffer from inadequate feature representation and information degradation during fusion. To address this, we propose HaarFuse, a wavelet-enhanced auto-encoder network that hierarchically integrates multi-scale features for robust fusion. The network first employs wavelet transform to extend the receptive field of convolutional layers, extracting shared shallow features that encode both low-frequency structural contours and high-frequency texture primitives. Subsequently, the shallow features are decomposed into high-frequency and low-frequency components through Haar wavelet transform, and techniques such as INN, Gabor layer, and Transformer are adopted to further optimize and process these features. Finally, the fused image is reconstructed via the inverse wavelet transform. Experiments on TNO, MSRS, and M3FD benchmarks validate HaarFuse&#39;s superiority: it achieves the highest thermal saliency (SD=45.78, +5.5%↑ on MSRS; EN=6.98, +4.0%↑ on M3FD), optimal edge fidelity (Qabf=0.62, +1.6%↑ on M3FD), and 34.2 × faster inference than SwinFusion with 0.468 MB parameters. Further validation in machine vision and medical imaging confirms its robustness for real-time applications.},
  archive      = {J_PR},
  author       = {Yuequn Wang and Jie Liu and Jianli Wang and Leqiang Yang and Bo Dong and Zhengwei Li},
  doi          = {10.1016/j.patcog.2025.111594},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111594},
  shortjournal = {Pattern Recognition},
  title        = {HaarFuse: A dual-branch infrared and visible light image fusion network based on haar wavelet transform},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SyntaPulse: An unsupervised framework for sentiment
annotation and semantic topic extraction. <em>PR</em>, <em>164</em>,
111593. (<a href="https://doi.org/10.1016/j.patcog.2025.111593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is a critical area within natural language processing, with applications in various domains like marketing, social media analytics, and politics. However, current methods encounter challenges in handling contextual ambiguities, accurately detecting sarcasm and irony, and effectively processing domain-specific vocabulary without extensive labeled datasets. Addressing these issues is essential, as the nuanced nature of language can lead to diverse interpretations across contexts, complicating reliable sentiment analysis. Furthermore, sarcasm and irony remain difficult to identify precisely, while reliance on labeled data and limitations in handling domain-specific vocabulary restrict adaptability across different fields. This paper presents SyntaPulse, a novel framework for sentiment classification in social networks, developed to overcome these challenges. The framework combines an innovative dictionary-based approach with Probabilistic Syntactic Latent Semantic Analysis (PSLSA) for semantic topic extraction. This integration enables it to handle homographs effectively, thereby enhancing sarcasm detection, facilitating the interpretation of domain-specific vocabulary, and reducing dependency on labeled data. Evaluated on 12 datasets, our framework demonstrates adaptability across various domains and achieves high Macro-F1 scores, ranging from 72.89 % to 96.22 %. SyntaPulse has also obtained improvements on seven datasets, with the lowest improvement rate being 0.21 % and the highest reaching 2.97 %.},
  archive      = {J_PR},
  author       = {Hadis Bashiri and Hassan Naderi},
  doi          = {10.1016/j.patcog.2025.111593},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111593},
  shortjournal = {Pattern Recognition},
  title        = {SyntaPulse: An unsupervised framework for sentiment annotation and semantic topic extraction},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-driven feature fusion network and visual feature
coding for multi-label image classification. <em>PR</em>, <em>164</em>,
111584. (<a href="https://doi.org/10.1016/j.patcog.2025.111584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image classification (MLIC) has attracted extensive research attention in recent years. Nevertheless, most of the existing methods have difficulty in effectively fusing multi-scale features and focusing on critical visual information, which makes it difficult to recognize objects from images. Besides, recent studies have utilized graph convolutional networks and attention mechanisms to model label dependencies in order to improve the model performance. However, these methods often rely on manually predefined label structures, which limits flexibility and model generality. And they also fail to capture intrinsic object correlations within images and spatial contexts. To address these challenges, we propose a novel Feature Fusion network combined with Transformer (FFTran) to fuse different visual features. Firstly, to address the difficulties of current methods in recognizing small objects, we propose a Multi-level Scale Information Integration Mechanism (MSIIM) that fuses different feature maps from the backbone network. Secondly, we develop an Intra-Image Spatial-Channel Semantic Mining (ISCM) module for learning important spaces and channel information. Thirdly, we design a Visual Feature Coding based on Transformer (VFCT) module to enhance the contextual information by pooling different visual features. Compared to the baseline model, FFTran achieves a significant boost in mean Average Precision (mAP) on both the VOC2007 and COCO2014 datasets, with enhancements of 2.9% and 5.1% respectively, highlighting its superior performance in multi-label image classification tasks.},
  archive      = {J_PR},
  author       = {Pingzhu Liu and Wenbin Qian and Jintao Huang and Yanqiang Tu and Yiu-Ming Cheung},
  doi          = {10.1016/j.patcog.2025.111584},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111584},
  shortjournal = {Pattern Recognition},
  title        = {Transformer-driven feature fusion network and visual feature coding for multi-label image classification},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online signature verification based on the lagrange
formulation with 2D and 3D robotic models. <em>PR</em>, <em>164</em>,
111581. (<a href="https://doi.org/10.1016/j.patcog.2025.111581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Signature Verification commonly relies on function-based features, such as time-sampled horizontal and vertical coordinates, as well as the pressure exerted by the writer, obtained through a digitizer. Although inferring additional information about the writer’s arm pose, kinematics, and dynamics based on digitizer data can be useful, it constitutes a challenge. In this paper, we tackle this challenge by proposing a new set of features based on the dynamics of online signatures. These new features are inferred through a Lagrangian formulation, obtaining the sequences of generalized coordinates and torques for 2D and 3D robotic arm models. By combining kinematic and dynamic robotic features, our results demonstrate their significant effectiveness for online automatic signature verification and achieving state-of-the-art results when integrated into deep learning models.},
  archive      = {J_PR},
  author       = {Moises Diaz and Miguel A. Ferrer and Juan M. Gil and Rafael Rodriguez and Peirong Zhang and Lianwen Jin},
  doi          = {10.1016/j.patcog.2025.111581},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111581},
  shortjournal = {Pattern Recognition},
  title        = {Online signature verification based on the lagrange formulation with 2D and 3D robotic models},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMFNet: Lightweight multimodal fusion network for
high-resolution remote sensing image segmentation. <em>PR</em>,
<em>164</em>, 111579. (<a
href="https://doi.org/10.1016/j.patcog.2025.111579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the rapid evolution of semantic segmentation for land cover classification in high-resolution remote sensing imagery, integrating multiple data modalities such as Digital Surface Model (DSM), RGB, and Near-infrared (NIR) remains a challenge. Current methods often process only two types of data, missing out on the rich information that additional modalities can provide. Addressing this gap, we propose a novel L ightweight M ultimodal data F usion Net work (LMFNet) to accomplish the tasks of fusion and semantic segmentation of multimodal remote sensing images. LMFNet uniquely accommodates various data types simultaneously, including RGB, NirRG, and DSM, through a weight-sharing, multi-branch vision transformer that minimizes parameter count while ensuring robust feature extraction. Our proposed multimodal fusion module integrates a Multimodal Feature Fusion Reconstruction Layer and Multimodal Feature Self-Attention Fusion Layer , which can reconstruct and fuse multimodal features. Our method achieves a mean Intersection over Union ( m I o U ) of 85.09% on the US3D dataset, marking a significant improvement over existing methods. We also studied the scalability of our method, directly extending the input modality to the SAR and hyperspectral fields. Our experimental results on the C2Seg dataset show that our method has generalization applicability to data of various modalities.},
  archive      = {J_PR},
  author       = {Tong Wang and Guanzhou Chen and Xiaodong Zhang and Chenxi Liu and Jiaqi Wang and Xiaoliang Tan and Wenlin Zhou and Chanjuan He},
  doi          = {10.1016/j.patcog.2025.111579},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111579},
  shortjournal = {Pattern Recognition},
  title        = {LMFNet: Lightweight multimodal fusion network for high-resolution remote sensing image segmentation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAPFormer: Shape-aware propagation transformer for point
clouds. <em>PR</em>, <em>164</em>, 111578. (<a
href="https://doi.org/10.1016/j.patcog.2025.111578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based networks have achieved impressive performance on three-dimensional point cloud data. However, most existing methods focus on aggregating local features in the neighborhoods of a point cloud, ignoring the global feature information. Therefore, it is difficult to capture the long-range dependencies of a point cloud. In this paper, we propose the Shape-Aware Propagation Transformer (SAPFormer) , which flexibly captures the semantic information of point clouds in geometric space and effectively extracts the contextual geometric space information. Specifically, we first design local group self-attention (LGA) to capture the local interaction information in each region. To capture the separated local region feature relationships, we propose local group propagation (LGP) to pass the information between different regions via query points. This allows features to propagate among neighbors for more fine-grained feature information. To further enlarge the receptive field, we propose the global shape feature module (GSFM) to learn global context information through key shape points (KSPs). Finally, to solve the positional information cues between global contexts, we introduce spatial-shape relative position encoding (SS-RPE), which obtains positional relationships between points. Extensive experiments demonstrate the effectiveness and superiority of our method on the S3DIS, SensatUrban, ScanNet V2, ShapeNetPart, and ModelNet40 datasets. The code is available at https://github.com/viivan/SAPFormer-main .},
  archive      = {J_PR},
  author       = {Gang Xiao and Sihan Ge and Yangsheng Zhong and Zhongcheng Xiao and Junfeng Song and Jiawei Lu},
  doi          = {10.1016/j.patcog.2025.111578},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111578},
  shortjournal = {Pattern Recognition},
  title        = {SAPFormer: Shape-aware propagation transformer for point clouds},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFIFC: Adaptive fuzzy neighborhood mutual information-based
feature selection via label correlation. <em>PR</em>, <em>164</em>,
111577. (<a href="https://doi.org/10.1016/j.patcog.2025.111577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing feature selection schemes do not comprehensively consider correlation between features and labels and between labels, and certain neighborhood radius affects the prediction accuracy of multilabel classification. To solve these deficiencies, this paper develops an adaptive fuzzy neighborhood mutual information-based feature selection scheme via label correlation. Firstly, to study different distribution structures of multilabel data, the standard Euclidean distance as classification interval is employed to construct adaptive fuzzy neighborhood radius. Adaptive fuzzy neighborhood similarity relation and fuzzy neighborhood granule will be presented via difference between samples for features. Uncertainty measures via fuzzy neighborhood entropy can be developed. Secondly, to select features strongly associated with labels, adaptive fuzzy neighborhood mutual information measures this correlation between candidate features and labels, and the correlation between features and labels relative to those selected features is computed by mutual information. Then discriminant function of correlation is provided. Thirdly, to improve efficacy of multilabel classification, adaptive fuzzy neighborhood granules are employed to study the membership degree of labels. To assess the correlation between labels, Jaccard similarity and adaptive fuzzy neighborhood mutual information are combined, and to reflect this internal correlation between label and label set, the correlation ratio is studied. Finally, maximum relevance between the candidate features and labels and minimum redundancy between features are calculated, and then a new multilabel feature selection scheme is provided to acquire this best feature subset. Experiments on 12 datasets show the efficacy of this designed scheme in several evaluation metrics.},
  archive      = {J_PR},
  author       = {Lin Sun and Feng Xu and Weiping Ding and Jiucheng Xu},
  doi          = {10.1016/j.patcog.2025.111577},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111577},
  shortjournal = {Pattern Recognition},
  title        = {AFIFC: Adaptive fuzzy neighborhood mutual information-based feature selection via label correlation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent policy gradients with dynamic weighted value
decomposition. <em>PR</em>, <em>164</em>, 111576. (<a
href="https://doi.org/10.1016/j.patcog.2025.111576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world multi-agent systems, multiple agents need to coordinate with other agents due to some limitations of observation and communication ability. Multi-agent policy gradient methods recently have witnessed vigorous progress in such challenging settings. However, multi-agent policy gradient methods have scalability and credit assignment issues due to the centralized critic. To solve these issues, a novel D ynamic Weighted QMI X Based M ulti-Agent Policy Gradients (DXM) is proposed in this paper, where the idea of dynamic weighted value decomposition is introduced into the framework of multi-agent actor-critic. Based on this idea, the proposed DXM approach has a more general decomposition on centralized critic than existing value decomposition methods, which address the scalability and credit assignment issue in both continuous and discrete action spaces. Briefly, in the presented DXM, deep deterministic policy gradient is employed to learn policies and a single centralized but factored critic, which can decompose the dynamic weighted nonlinear nonmonotonic summation of individual value functions. Empirical evaluations on the discrete action space environment StarCraft multi-agent challenge benchmark and the continuous action space environment continuous predator-prey benchmark show that the DXM approach successfully addresses the scalability and credit allocation issues. DXM significantly outperforms other baselines, with an average win rate improvement of &gt;15 %.},
  archive      = {J_PR},
  author       = {Shifei Ding and Xiaomin Dong and Jian Zhang and Lili Guo and Wei Du and Chenglong Zhang},
  doi          = {10.1016/j.patcog.2025.111576},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111576},
  shortjournal = {Pattern Recognition},
  title        = {Multi-agent policy gradients with dynamic weighted value decomposition},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TLR-3DRN: Unsupervised single-view reconstruction via
tri-layer renderer. <em>PR</em>, <em>164</em>, 111568. (<a
href="https://doi.org/10.1016/j.patcog.2025.111568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-view three-dimensional (3D) reconstruction is a challenging task in computer vision, focusing on reconstructing 3D objects from a single image. Existing single-view object reconstruction approaches typically rely on viewpoints, silhouettes, multiple views of the same instance, and strategy-specific priors, which are difficult to obtain in the wild. To address this issue, we propose a novel end-to-end single-view reconstruction method based on a tri-layer renderer, named the Tri-Layer Renderer-based 3D Reconstruction Network (TLR-3DRN). TLR-3DRN recovers 3D structures from original image collections without requiring additional supervision, assumptions, or priors. In particular, TLR-3DRN employs a tri-layer renderer that enables the model to extract more 3D details from unprocessed image data. To obtain an optimizable interlayer, we developed a robust interlayer generation network based on a nonparametric memory bank. Notably, we designed a joint optimization strategy for the overall framework. Additionally, a shape and texture consistency loss based on image–text models is proposed to enhance the optimization process. Owing to the aforementioned proposed modules, TLR-3DRN can achieve high-quality, diverse-category reconstruction under completely unsupervised conditions. TLR-3DRN is validated on synthetic datasets and real-world datasets. Experimental results demonstrate that TLR-3DRN outperforms state-of-the-art unsupervised and two-dimensional supervised methods, achieving performance comparable to 3D supervised methods.},
  archive      = {J_PR},
  author       = {HaoYu Guo and Ying Li and Chunyan Deng},
  doi          = {10.1016/j.patcog.2025.111568},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111568},
  shortjournal = {Pattern Recognition},
  title        = {TLR-3DRN: Unsupervised single-view reconstruction via tri-layer renderer},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-granular multi-label contrastive
learning. <em>PR</em>, <em>164</em>, 111567. (<a
href="https://doi.org/10.1016/j.patcog.2025.111567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is a task wherein predictive models are developed to assign relevant label sets to unseen instances. Label correlation extraction and utilization have been widely implemented in multi-label classification methodologies. However, current multi-label contrastive learning algorithms inadequately incorporate label correlations into the feature space, thus limiting the learning of optimal feature representations for multi-label samples. To address this limitation, a novel hierarchical multi-granularity multi-label contrastive learning approach is proposed in this paper. The proposed method encompasses the construction of multi-label hierarchical correlations, label expansion based on hierarchical relationships, and representation learning through multi-granularity contrastive learning built upon these structures. Experimental results demonstrate the superiority of the proposed method over state-of-the-art techniques across widely used datasets.},
  archive      = {J_PR},
  author       = {Haixiang Li and Min Fang and Xiao Li and Bo Chen and Guizhi Wang},
  doi          = {10.1016/j.patcog.2025.111567},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111567},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical multi-granular multi-label contrastive learning},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustifying vision transformer for image forgery
localization with multi-exit architectures. <em>PR</em>, <em>164</em>,
111565. (<a href="https://doi.org/10.1016/j.patcog.2025.111565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of image manipulation tools has led to an increase in the number of manipulated images being disseminated online, posing risks like the propagation of fake news and telecom fraud. Thus, there is an increasing demand for precise, generic, and robust methods for detecting and locating manipulated images. In this paper, we propose a simple and clean model, named MEAFormer, for image forgery localization that does not heavily rely on pre-trained models. MEAFormer comprises three main components: an encoder network , a neck network , and a decoder network . Specifically, the transformer-based encoder network extracts hierarchical feature representations from the input image, providing rich contextual information in each layer. The neck network , incorporating our proposed cross-layer feature aggregation (CFA), aggregates these hierarchical features. To achieve better spatial feature co-occurrence, instead of using noise or edge artifacts, we introduce a multi-scale graph reasoning (MGR) module within the decoder network via bipartite graphs over the encoder and decoder features in a multi-scale fashion. The cross-level enhancement (CLE) further performs adjacent-level feature fusion to amplify the regions of interest in aggregated manipulation features. Finally, the multi-exit architecture (MEA) guides the model to learn fine-grained features and segment out the manipulated region. Extensive experiments across diverse and challenging datasets conclusively establish the superiority of MEAFormer over existing state-of-the-art methods, excelling in accuracy, generalization, and robustness.},
  archive      = {J_PR},
  author       = {Zenan Shi and Haipeng Chen and Dong Zhang},
  doi          = {10.1016/j.patcog.2025.111565},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111565},
  shortjournal = {Pattern Recognition},
  title        = {Robustifying vision transformer for image forgery localization with multi-exit architectures},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An information bottleneck approach for feature selection.
<em>PR</em>, <em>164</em>, 111564. (<a
href="https://doi.org/10.1016/j.patcog.2025.111564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has been studied extensively over the last few decades. As a widely used method, the information-theoretic feature selection methods have attracted considerable attention due to their better interpretation and desirable performance. From an information-theoretic perspective, a golden rule for feature selection is to maximize the mutual information I ( X s , Y ) between the selected feature subset X s and the class labels Y . Despite its simplicity, explicitly optimizing this objective is a non-trivial task. In this work, we propose a novel global neural network-based feature selection framework with the information bottleneck principle and establish its connection to the rule of maximizing I ( X s , Y ) . Using the matrix-based Rényi’s α -order entropy functional, our framework enjoys a simple and tractable objective without any variational approximation or distributional assumption. We further extend the framework to multi-view scenarios and verify it with two large-scale, high-dimensional real-world biomedical applications. Comprehensive experimental results demonstrate the superior performance of our framework not only in terms of classification accuracy but also in terms of good interpretability within and across each view, effectively proving that the proposed framework is trustworthy. Code is available at https://github.com/archy666/IBFS .},
  archive      = {J_PR},
  author       = {Qi Zhang and Mingfei Lu and Shujian Yu and Jingmin Xin and Badong Chen},
  doi          = {10.1016/j.patcog.2025.111564},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111564},
  shortjournal = {Pattern Recognition},
  title        = {An information bottleneck approach for feature selection},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient approach for finger vein verification to
solving the biometric recognition technique. <em>PR</em>, <em>164</em>,
111563. (<a href="https://doi.org/10.1016/j.patcog.2025.111563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vein authentication is a novel biometric method to authenticate a person&#39;s individuality. The conventional biometric technique employs shape images and exact segments of finger veins for the verification process. The proposed deep belief structure model aims to improve verification accuracy using a novel Anisotropic Filtered Stromberg Feature Transform based on Tucker&#39;s Congruence Deep Belief Structure Learning (AFSFT-TCDBSL) technique. The main aim of the AFSFT-TCDBSL technique is to improve verification accuracy and minimize time consumption. The proposed AFSFT-TCDBSL technique comprises one input layer, three hidden layers, and one output layer. The numbers of images are collected in the input layer, and the input images are pre-processed using anisotropic diffusion filtering in the first hidden layer. Then the pre-processed input images are sent to the next layer, where the feature extraction process is carried out using the Stromberg wavelet transform. Finally, the verification process is performed using Tucker&#39;s congruence correlation coefficient. Based on the correlation, the verification results are obtained at the output layer. In this way, accurate finger vein verification is performed with superior accuracy and with a minimum false rate. We performed experimental assessments with different factors, such as the Peak Signal-to-Noise Ratio (PSNR), Finger Vein Verification Accuracy (FVVA), False Positive Rate (FPR), Processing Time (PT), and Feature Extraction Time (FET). The results of the proposed ADFSFT-TCDBSL technique were conducted on 9% of improved peak signal-to-noise ratio and accuracy with a minimum 59% false positive rate and 16% time as well as 19% feature extraction time than the state-of-the-art FVV methods; therefore, it better facilitates the application of real-time finger vein verification.},
  archive      = {J_PR},
  author       = {Dharmalingam Muthusamy and Rakkimuthu Ponnusamy},
  doi          = {10.1016/j.patcog.2025.111563},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111563},
  shortjournal = {Pattern Recognition},
  title        = {An efficient approach for finger vein verification to solving the biometric recognition technique},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMSFT: Hierarchical multi-scale spatial-frequency-temporal
collaborative transformer for 3D human pose estimation. <em>PR</em>,
<em>164</em>, 111562. (<a
href="https://doi.org/10.1016/j.patcog.2025.111562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing 3D poses from monocular video sequences faces formidable challenges due to noise-induced jitters and intricate joint dependencies. With this in mind, we propose the hierarchical multi-scale spatial-frequency-temporal collaborative transformer (HMSFT), which obtains robust multi-level joint relations by synergistically combining the complementary strengths of the spatial, frequency, and temporal domains. First, we utilize the spatial kinematics aware block to acquire geometric relationships across joints within the same frame. Subsequently, the adaptive frequency encoding block is presented to optimize the frequency representation of single poses and action sequences in response to distinctive feature attributes, thus alleviating the adverse impact of short-term and long-term jitters. Finally, through comprehensive temporal modeling across multiple scales, we explicitly capture the motion dependencies of the joint-level, part-level and body-level over time. Experimental validations on three benchmarks (Human 3.6M, HumanEva-I and MPI-INF-3DHP) show that the proposed HMSFT obtains significant improvements and excellent robust performance over several state-of-the-art techniques.},
  archive      = {J_PR},
  author       = {Hehao Zhang and Zhengping Hu and Shuai Bi and Jirui Di and Zhe Sun},
  doi          = {10.1016/j.patcog.2025.111562},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111562},
  shortjournal = {Pattern Recognition},
  title        = {HMSFT: Hierarchical multi-scale spatial-frequency-temporal collaborative transformer for 3D human pose estimation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-driven acoustic properties learning for underwater
target ranging. <em>PR</em>, <em>164</em>, 111560. (<a
href="https://doi.org/10.1016/j.patcog.2025.111560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater acoustic ranging (UAR) plays a crucial role in estimating object distances for ocean exploration. However, a reliable UAR method remains elusive, with current approaches either being reliant on inadequate hand-crafted features or neglecting the unique underwater acoustic properties. To address this, we propose Multi-attentional Underwater Acoustic Ranging (MUAR), a highly effective and robust UAR framework. MUAR incorporates multiple attention mechanisms tailored to the acoustic properties. Specifically, to better leverage the rich channel information in UAR data, we design a grouped channel attention module that can efficiently capture informative channels of the input data. Then, a feature-balancing strategy based on spatial-attention is introduced to mitigate information redundancy and conflicts, thereby enhancing the multi-level expressive capability of the model. We further theoretically analyze the connection between the self-attention mechanism and the acoustical signal correlations, such that achieving a better interpretation for the extracted features. Through extensive experiments and analysis on three authentic datasets, we show that MUAR outperforms previous approaches by obtaining state-of-the-art performance, i.e , achieving a MSE of 0.44 (vs. 2.72) and a MAPE of 0.97 (vs. 2.42). The source code of the proposed MUAR is released at https://github.com/TiernosChu/MUAR .},
  archive      = {J_PR},
  author       = {Xiaohui Chu and Hantao Zhou and Yan Zhang and Yachao Zhang and Runze Hu and Haoran Duan and Yawen Huang and Yefeng Zheng and Rongrong Ji},
  doi          = {10.1016/j.patcog.2025.111560},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111560},
  shortjournal = {Pattern Recognition},
  title        = {Attention-driven acoustic properties learning for underwater target ranging},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph augmentation guided federated knowledge distillation
for multisite functional MRI analysis. <em>PR</em>, <em>164</em>,
111559. (<a href="https://doi.org/10.1016/j.patcog.2025.111559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resting-state functional MRI (rs-fMRI) is a non-invasive tool increasingly used to detect abnormalities in brain connectivity for disorder analysis. Many learning models have been explored for fMRI analysis but usually require extensive data for reliable training. Multisite studies increase sample sizes by pooling data from multiple sites, but often face data security and privacy challenges. Federated learning (FL) facilitates collaborative model training without pooling fMRI data from different sites/clients. However, many FL methods share model parameters between clients, posing significant security risks during communication and greatly increasing communication costs. Besides, fMRI data for local model training is usually limited at each site, which may hinder local model training. To this end, we propose a graph augmentation guided federated distillation (GAFD) framework for multisite fMRI analysis and brain disorder identification. At each client, we augment each input functional connectivity network/graph derived from fMRI by perturbing node features and edges, followed by a feature encoder for graph representation learning. A contrastive loss is used to maximize the agreement of learned representations from the same subject, further enhancing discriminative power of fMRI representations. On the server side, the server receives model outputs ( i.e. , logit scores) corresponding to augmented graphs from each client and merges them. The merged logit score is then sent back to each client for knowledge distillation. This can promote knowledge sharing among clients, reduce the risk of privacy leakage, and greatly decrease communication costs. Experimental results on two multisite fMRI datasets indicate that our approach outperforms several state-of-the-arts.},
  archive      = {J_PR},
  author       = {Qianqian Wang and Junhao Zhang and Long Li and Lishan Qiao and Pew-Thian Yap and Mingxia Liu},
  doi          = {10.1016/j.patcog.2025.111559},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111559},
  shortjournal = {Pattern Recognition},
  title        = {Graph augmentation guided federated knowledge distillation for multisite functional MRI analysis},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral approximation of gaussian random graph laplacians
and applications to pattern recognition. <em>PR</em>, <em>164</em>,
111555. (<a href="https://doi.org/10.1016/j.patcog.2025.111555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectral decomposition of Gaussian Random Graph Laplacian (GRGLs) is at the core of the solutions to many graph-based problems. Most prevalent are graph signal processing, graph matching, and graph learning problems. Proposed here is the Eigen Approximation Theorem (EAT), which states that the diagonal entries of a GRGL matrix are reliable empirical approximations of its eigenvalues, given certain general conditions. This theorem provides a more precise bound for eigenvalues in a subspace derived from the Courant–Fischer min–max theorem. Consequently, the k th eigenvalue and eigenvector of a GRGL can be computed efficiently using deflated power iteration. Simulation results demonstrate the accuracy and computational speed of the EAT application. Hence, it can solve problems involving GRGLs like graph signal processing, graph matching, and graph learning. The EAT can also be used directly when approximations to spectral decomposition suffice. The real-time applications are also demonstrated.},
  archive      = {J_PR},
  author       = {Rajeev Airani and Sachin Kamble},
  doi          = {10.1016/j.patcog.2025.111555},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111555},
  shortjournal = {Pattern Recognition},
  title        = {Spectral approximation of gaussian random graph laplacians and applications to pattern recognition},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-light image enhancement via clustering contrastive
learning for visual recognition. <em>PR</em>, <em>164</em>, 111554. (<a
href="https://doi.org/10.1016/j.patcog.2025.111554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual recognition tasks of low-light images remain a big challenge. We propose an unsupervised low-light image enhancement module that can be integrated into any baseline visual model to enhance the performance. The proposed method is based on Clustering Contrastive Learning and Grad-CAM (Gradient-Class Activation Map) feature alignment, called CCGC. The CCGC method enhances the luminance semantic information of low-light images and remains the semantic feature information focusing. Simulation experimental results on various low-light image datasets demonstrate the significant feature enhancement and generalization capability of CCGC. Evaluation of the established CUB-2011 low-light image dataset shows a substantial increase in classification accuracy across multiple benchmark models. Furthermore, the proposed method significantly improves the classification accuracy on a real low-light traditional Chinese medicine dataset and enhances face detection performance on dark face detection datasets.},
  archive      = {J_PR},
  author       = {Guanglei Sheng and Gang Hu and Xiaofeng Wang and Wei Chen and Jinlin Jiang},
  doi          = {10.1016/j.patcog.2025.111554},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111554},
  shortjournal = {Pattern Recognition},
  title        = {Low-light image enhancement via clustering contrastive learning for visual recognition},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contribution-based imbalanced hybrid resampling ensemble.
<em>PR</em>, <em>164</em>, 111553. (<a
href="https://doi.org/10.1016/j.patcog.2025.111553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resampling is an effective method for addressing data imbalance. Prevailing methods adjust the data distribution by either describing information or noise, and exhibit superiority in many scenarios. However, current studies face challenges in considering both information and noise simultaneously, as noisy samples usually have high information levels, potentially leading to misestimation. In this paper, a Contribution-Based Hybrid Resampling Ensemble (CHRE) is proposed to address the correlation problem between information and noise. CHRE is a semi-supervised algorithm based on a novel Global Unified Data Evaluation (GUDE) framework. Firstly, GUDE describes sample contribution by redefining the information and noise levels. Subsequently, based on sample contribution, CHRE removes negatively contributing majority samples, and oversamples minority samples Concurrently, pseudo-labels related to these minority samples are included in the oversampling. Throughout this process, CHRE resamples based on the sample contribution and optimizes the model. GUDE provides sample contribution based on the model feedback, with both interacting for iterative optimization. Extensive experiments are conducted on 53 benchmark datasets, involving three base classifiers and 13 state-of-the-art imbalance algorithms. The results demonstrate significant advantages of CHRE. Noise studies further indicate the high robustness of CHRE.},
  archive      = {J_PR},
  author       = {Lingyun Zhao and Fei Han and Qinghua Ling and Yubin Ge and Yuze Zhang and Qing Liu and Henry Han},
  doi          = {10.1016/j.patcog.2025.111553},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111553},
  shortjournal = {Pattern Recognition},
  title        = {Contribution-based imbalanced hybrid resampling ensemble},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-distribution-based ensemble sampler for imbalanced
semi-supervised learning. <em>PR</em>, <em>164</em>, 111552. (<a
href="https://doi.org/10.1016/j.patcog.2025.111552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) on imbalanced data is largely under-explored and suffers from erroneous pseudo-labels, biased model training, or intolerable training costs. To alleviate these issues, we propose a meta-distribution-based ensemble sampler (MDSampler) approach 1 for imbalanced SSL. MDSampler is a unified framework that integrates SSL, imbalanced learning, and ensemble learning via iterative instance under-sampling and cascade classifier aggregation. Specifically, MDSampler considers the confidence-diversity distribution of both labeled and unlabeled samples and obtains the so-called meta-distribution via 2-D histogram discretization. Sampling on the meta-distribution (1) assigns pseudo-labels to unlabeled data for SSL, (2) alleviates class imbalance since the sampling process is unbiased, (3) improves the diversity of the ensemble learning framework, and (4) is highly efficient and flexible. Additionally, an adaptive instance interpolation strategy is presented to improve the quality of pseudo-labeled samples. Extensive experiments show that MDSampler can be organically combined with various classifiers to achieve superior performance in imbalanced SSL.},
  archive      = {J_PR},
  author       = {Zhihan Ning and Chaoxun Guo and David Zhang},
  doi          = {10.1016/j.patcog.2025.111552},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111552},
  shortjournal = {Pattern Recognition},
  title        = {Meta-distribution-based ensemble sampler for imbalanced semi-supervised learning},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anisotropic multiresolution analyses for deepfake detection.
<em>PR</em>, <em>164</em>, 111551. (<a
href="https://doi.org/10.1016/j.patcog.2025.111551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) can be misused to fabricate elaborate lies. The threat posed by GANs has sparked the need to discern between genuine and fabricated content. We argue that since GANs primarily utilize isotropic convolutions to generate their output, they leave clear traces, their fingerprint, in the coefficient distribution on sub-bands extracted by anisotropic multiresolution transforms. We employ the fully separable wavelet transform and anisotropic multiwavelets to obtain anisotropic features to feed to lightweight convolutional neural network classifiers. The proposed approach is capable of considerably improving the state-of-the-art in detecting fully GAN-generated images. It is particularly resilient to common perturbations, such as compression, noise or blur. We find that anisotropic transforms, when combined with XceptionNet, also significantly enhance the state-of-the-art in detecting partially manipulated images.},
  archive      = {J_PR},
  author       = {Wei Huang and Michelangelo Valsecchi and Michael Multerer},
  doi          = {10.1016/j.patcog.2025.111551},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111551},
  shortjournal = {Pattern Recognition},
  title        = {Anisotropic multiresolution analyses for deepfake detection},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concept-guided domain generalization for semantic
segmentation. <em>PR</em>, <em>164</em>, 111550. (<a
href="https://doi.org/10.1016/j.patcog.2025.111550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent domain generalization semantic segmentation methods are proposed to use vision foundation models (VFMs) for achieving superior performance in unseen domains. However, unlike human vision, which naturally adapts to recognize objects in different contexts, VFMs still suffer from the distribution shift problem. Based on this, a concept-guided domain generalization (CDG) approach is proposed for semantic segmentation. First, considering that humans can recognize objects in various environments after humans learn the conception of objects, a concept token learning module is proposed to learn the semantic concept token from semantic prototypes, which aims to exploit domain-invariant instance-aware knowledge. Second, when the recognition of objects is uncertain, humans recognize the objects by contextual information. Thus, a concept-contextual calibration strategy is proposed to generate concept-contextual relations by the semantic concepts to calibrate uncertain regions for refining final predictions. Extensive experiments demonstrate that the proposed approach achieves superior performance on multiple benchmarks. The code is released on GitHub: https://github.com/seabearlmx/CDG .},
  archive      = {J_PR},
  author       = {Muxin Liao and Wei Li and Chengle Yin and Yuling Jin and Yingqiong Peng},
  doi          = {10.1016/j.patcog.2025.111550},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111550},
  shortjournal = {Pattern Recognition},
  title        = {Concept-guided domain generalization for semantic segmentation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive spatial and scale label assignment for anchor-free
object detection. <em>PR</em>, <em>164</em>, 111549. (<a
href="https://doi.org/10.1016/j.patcog.2025.111549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, anchor-free object detection has attracted widespread attention due to its simplicity and efficiency. The mainstream anchor-free object detectors allocate positive/negative candidate samples through prior guidance at a fixed spatial position and assign positive/negative samples according to predefined scale constraints. However, artificially designing assignment strategies according to prior data distribution may hinder further optimization of label assignment. To this end, this paper proposes Adaptive Spatial and Scale Label Assignment (ASS-LA) to improve the performance of anchor-free object detection. Positive/negative samples are distributed from different pyramid levels using spatial and scale constraints. Specifically, an adaptive Intersection-over-Union (IoU) space assignment is designed to select candidate positive sample points. The membership degree is introduced at each pyramid level to adaptively fuzzy the scale assignment range so that the detector selects the final positive sample from the candidate sample points. Furthermore, a reference box is introduced to design the predicted IoU branch of coupled regression. In the inference stage, the predicted IoU and classification scores are combined as the confidence of the regression bounding box to alleviate the inconsistency between classification and regression. Extensive experiments show that our method achieves comparable performance to other existing label assignment schemes. With the introduction of ASS-LA, the anchor-free object detector has significant performance improvements without introducing other overhead.},
  archive      = {J_PR},
  author       = {Min Dang and Gang Liu and Chao Chen and Di Wang and Xike Li and Quan Wang},
  doi          = {10.1016/j.patcog.2025.111549},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111549},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive spatial and scale label assignment for anchor-free object detection},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced seed selection for k-means clustering with
determinantal point process. <em>PR</em>, <em>164</em>, 111548. (<a
href="https://doi.org/10.1016/j.patcog.2025.111548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means is one of the most popular and effective partitional clustering algorithms. However, in K-means, the initial seeds (centroids) play a critical role in determining the quality of the clusters. The existing methods address this problem either by factoring in the distance between the points on n-dimensional space so that the seeds are spaced apart or by choosing points from the dense regions to avoid the selection of outliers. We introduce a novel approach for seed selection that jointly models diversity as well as the quality of the seeds in a unified probabilistic framework based on a fixed-size determinantal point process. The quality indicator measures the reliability of the point to be considered as a potential seed, while the diversity measure factors in the spatial relation between the points on Euclidean space. The results show that the proposed algorithm outperforms the state-of-the-art models on several datasets.},
  archive      = {J_PR},
  author       = {Namita Bajpai and Jiaul H. Paik and Sudeshna Sarkar},
  doi          = {10.1016/j.patcog.2025.111548},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111548},
  shortjournal = {Pattern Recognition},
  title        = {Balanced seed selection for K-means clustering with determinantal point process},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DC-CLIP: Multilingual CLIP compression via vision-language
distillation and vision-language alignment. <em>PR</em>, <em>164</em>,
111547. (<a href="https://doi.org/10.1016/j.patcog.2025.111547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained vision-language (V-L) models such as CLIP have shown excellent performance in many downstream cross-modal tasks. However, most of them are only applicable to the English context. Subsequent research has focused on this problem and proposed improved models, such as CN-CLIP and AltCLIP, to facilitate their applicability to Chinese and even other languages. Nevertheless, these models suffer from high latency and a large memory footprint in inference, which limits their further deployment on resource-constrained edge devices. In this work, we propose a conceptually simple yet effective multilingual CLIP Compression framework and train a lightweight multilingual vision-language model, called DC-CLIP, for both Chinese and English contexts. In this framework, we collect a high-quality Chinese–English multi-source dataset and design two training stages, including multilingual vision-language feature distillation and alignment. During the first stage, lightweight image/text student models are designed to learn robust visual/multilingual textual feature representation ability from corresponding teacher models, respectively. Subsequently, the multilingual vision-language alignment stage enables effective alignment of visual and multilingual textual features to further improve the model’s multilingual performance. Comprehensive experiments in zero-shot image classification, conducted based on the ELEVATER benchmark, showcase that DC-CLIP achieves superior performance in the English context and competitive performance in the Chinese context, even with less training data, when compared to existing models of similar parameter magnitude. The evaluation demonstrates the effectiveness of our designed training mechanism.},
  archive      = {J_PR},
  author       = {Wenbo Zhang and Yifan Zhang and Jianfeng Lin and Binqiang Huang and Jinlu Zhang and Wenhao Yu},
  doi          = {10.1016/j.patcog.2025.111547},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111547},
  shortjournal = {Pattern Recognition},
  title        = {DC-CLIP: Multilingual CLIP compression via vision-language distillation and vision-language alignment},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural normalized cut: A differential and generalizable
approach for spectral clustering. <em>PR</em>, <em>164</em>, 111545. (<a
href="https://doi.org/10.1016/j.patcog.2025.111545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering, as a popular tool for data clustering, requires an eigen-decomposition step on a given affinity to obtain the spectral embedding. Nevertheless, such a step suffers from the lack of generalizability and scalability. Moreover, the obtained spectral embeddings can hardly provide a good approximation to the ground-truth partition and thus a k -means step is adopted to quantize the embedding. In this paper, we propose a simple yet effective scalable and generalizable approach, called Neural Normalized Cut (NeuNcut), to learn the clustering membership for spectral clustering directly. In NeuNcut, we properly reparameterize the unknown cluster membership via a neural network, and train the neural network via stochastic gradient descent with a properly relaxed normalized cut loss. As a result, our NeuNcut enjoys a desired generalization ability to directly infer clustering membership for out-of-sample unseen data and hence brings us an efficient way to handle clustering task with ultra large-scale data. We conduct extensive experiments on both synthetic data and benchmark datasets and experimental results validate the effectiveness and the superiority of our approach.},
  archive      = {J_PR},
  author       = {Wei He and Shangzhi Zhang and Chun-Guang Li and Xianbiao Qi and Rong Xiao and Jun Guo},
  doi          = {10.1016/j.patcog.2025.111545},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111545},
  shortjournal = {Pattern Recognition},
  title        = {Neural normalized cut: A differential and generalizable approach for spectral clustering},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sampling-based gaussian processes for few-shot
semantic segmentation. <em>PR</em>, <em>164</em>, 111542. (<a
href="https://doi.org/10.1016/j.patcog.2025.111542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation (FSS) is a longstanding challenge in computer vision. Previous methods adopting Gaussian Processes (GPs) aggregate detailed information and manage complex distributions from small support sets, thereby modeling uncertainty of features and handling wide variations in context. However, the exact GP-based FSS methods struggle with computational burden and information redundancy. To tackle the issues, we propose ESGP, an Efficient Sampling-based Gaussian Process framework for few-shot segmentation. The model decouples the GP into a two-step process: weight space approximation for the prior and function space update for the posterior. Additionally, we adopt Deep Kernel Learning to enhance ESGP’s performance. This combination results in a faster, more accurate FSS model that effectively concentrates support sample information. Moreover, GP’s inherent ability to model uncertainty provides robust predictions and valuable insights into segmentation reliability. Experimental results demonstrate that ESGP outperforms previous GP-based methods and achieves competitive performance with state-of-the-art techniques.},
  archive      = {J_PR},
  author       = {Xin-Yi Zhang and Xian-Kai Lu and Yi-Long Yin and Han-Jia Ye and De-Chuan Zhan},
  doi          = {10.1016/j.patcog.2025.111542},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111542},
  shortjournal = {Pattern Recognition},
  title        = {Efficient sampling-based gaussian processes for few-shot semantic segmentation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bijective inference network for interpretable
identification of RNA n6-methyladenosine modification sites.
<em>PR</em>, <em>164</em>, 111541. (<a
href="https://doi.org/10.1016/j.patcog.2025.111541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate identification of N 6 -methyladenosine (m 6 A) modification sites is crucial for unraveling various functional mechanisms. While existing methods primarily focus on learning high-quality embeddings of RNA sequences for this task, few of them consider incorporating specific RNA secondary structures, limiting their interpretability for in-depth post-transcriptional analysis. In this work, we introduce a novel bijective inference network, named m 6 A-BIN, which integrates RNA sequences and secondary structures within a unified parameter-shared framework, enhancing the accuracy of m 6 A modification site identification through the auxiliary supervision of RNA secondary structures. To begin with, m 6 A-BIN constructs sequential and structural graphs from RNA sequences and secondary structures, respectively. Bijective mapping functions are then specifically designed to couple the procedures of graph representation learning and interpretable dependency inference, providing informative supervision for learning sequential and structural embeddings of RNA. By fusing these two types of RNA embeddings, m 6 A-BIN efficiently performs the identification task. The attribution phase of m 6 A-BIN further ascribes the prediction results to nucleotide dependencies acquired during the interpretable dependency inference, including RNA sequence and structural patterns, thereby enhancing its interpretability. Extensive experimental results demonstrate the promising performance of m 6 A-BIN, showcasing its efficacy in terms of both accuracy and interpretability for the identification of novel m 6 A modification sites.},
  archive      = {J_PR},
  author       = {Guodong Li and Yue Yang and Dongxu Li and Xiaorui Su and Zhi Zeng and Pengwei Hu and Lun Hu},
  doi          = {10.1016/j.patcog.2025.111541},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111541},
  shortjournal = {Pattern Recognition},
  title        = {A bijective inference network for interpretable identification of RNA n6-methyladenosine modification sites},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic feature regularized loss for weakly supervised
semantic segmentation. <em>PR</em>, <em>164</em>, 111540. (<a
href="https://doi.org/10.1016/j.patcog.2025.111540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on confronting weakly supervised semantic segmentation with scribble-level annotation. The regularized loss has proven to be an effective solution for this task. However, most existing regularized losses only leverage static shallow features (color, spatial information) to compute the regularized kernel, which limits its final performance since such static shallow features fail to describe pair-wise pixel relationships in complicated cases. In this paper, we propose a new regularized loss that utilizes both shallow and deep features that are dynamically updated to aggregate sufficient information to represent the relationship of different pixels. Moreover, to provide accurate deep features, we design a feature consistency head to train the pair-wise feature relationship. In contrast to most approaches that adopt a multi-stage training strategy with complicated training settings and high time-consuming steps, our approach can be directly trained in an end-to-end manner, in which the feature consistency head and our regularized loss can benefit from each other. We evaluate our approach on different backbones, and extensive experiments show that our approach achieves new state-of-the-art performances on different cases, e.g. , using our approach with a vision transformer outperforms other approaches by a substantial margin (more than 5% mIoU increase). The source code will be released at: https://github.com/zbf1991/DFR .},
  archive      = {J_PR},
  author       = {Bingfeng Zhang and Jimin Xiao and Yao Zhao},
  doi          = {10.1016/j.patcog.2025.111540},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111540},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic feature regularized loss for weakly supervised semantic segmentation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable 2.5D network by hierarchical attention and
consistency learning for 3D MRI classification. <em>PR</em>,
<em>164</em>, 111539. (<a
href="https://doi.org/10.1016/j.patcog.2025.111539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods have been widely applied in diagnostic research on MRI data. Among the existing methods, attention-based multiple-instance learning, which not only provides classification results but also explains significant regions related to the task, has attracted considerable attention from scholars. However, prior methods might be restricted by these issues: (i) the loss of spatial or volume information, (ii) semantic inconsistency of attention weights, (iii) missing information exchange between attention mechanisms within different branches. To overcome these issues, we propose an innovative dual-branch attention-based deep multiple-instance learning framework, namely HA-CSL, which consists of a 2D branch and a 3D branch, a hierarchical attention (HA) module and a consistency learning (CSL) module. Specifically, the 2D and 3D branches employ the 2D and 3D convolutional neural networks to extract 2D and 3D patch-level features, respectively, so as to learn more richer image information. Additionally, the HA module comprises slice-, region- and channel-level attentions to interpret the significance of slices, regions and channels, respectively. Moreover, the CSL module is to enhance the consistency of attention weights obtained by the two branches, so as to reduce the semantic gap of attentions and promote better information exchange of two branches. Experiments on two 3D MRI image datasets demonstrate the superior classification and interpretation performance of the proposed framework over recent state-of-the-art methods. The source codes are available at https://github.com/shuting-pang/HA_CSL .},
  archive      = {J_PR},
  author       = {Shuting Pang and Yidi Chen and Xiaoshuang Shi and Rui Wang and Mingzhe Dai and Xiaofeng Zhu and Bin Song and Kang Li},
  doi          = {10.1016/j.patcog.2025.111539},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111539},
  shortjournal = {Pattern Recognition},
  title        = {Interpretable 2.5D network by hierarchical attention and consistency learning for 3D MRI classification},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-randomized focuses effectively boost metric-based
few-shot classifiers. <em>PR</em>, <em>164</em>, 111538. (<a
href="https://doi.org/10.1016/j.patcog.2025.111538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Towards solving a few-shot image classification task, deep metric learning is the de-facto approach. Usually the idea here is to train a deep metric model on base data, and evaluate it using novel data without any fine-tuning. Enhancing model performance is mostly focused upon improving feature or class representations, or designing or learning new metrics, often ignoring deep exploration of data-augmentation techniques to enhance few-shot learning. Interestingly, we discover that augmentation strategies, such as Cutout, Mixup and CutMix, would in fact greatly enhance performance of few-shot models. We conjecture, this is because such augmentation techniques encourage the model to extend its focus on multiple discriminative regions of an object instead of restricting to just the single-most discriminative point. Following this important discovery, we propose two simple yet effective novel data augmentation methods, viz. CutRot and CutCov, specifically designed to self-randomize focuses within an image itself for metric-based few-shot image classification. While CutRot involves random rotation of any patch within the image, CutCov focuses on random swapping of patches, again within the image. Extensive experiments verify that CutRot or CutCov can significantly boost performances of both classic and recent popular metric-based methods and performs much better than other augmentation methods of Cutout, Mixup, and CutMix on four few-shot image classification datasets. Code is available at https://github.com/liz-lut/CutRot-and-CutCov-main .},
  archive      = {J_PR},
  author       = {Zhen Li and Zhongyuan Liu and Dongliang Chang and Aneeshan Sain and Xiaoxu Li and Zhanyu Ma and Jing-Hao Xue and Yi-Zhe Song},
  doi          = {10.1016/j.patcog.2025.111538},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111538},
  shortjournal = {Pattern Recognition},
  title        = {Self-randomized focuses effectively boost metric-based few-shot classifiers},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging domain spaces for unsupervised domain adaptation.
<em>PR</em>, <em>164</em>, 111537. (<a
href="https://doi.org/10.1016/j.patcog.2025.111537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to transfer knowledge obtained from a labeled source domain to an unlabeled target domain, facing challenges due to domain shift—significant discrepancies in data distribution that impair model performance when applied to unseen domains. While recent approaches have achieved remarkable progress in mitigating these domain shifts, the focus remains on direct adaptation strategies from source to target domains. However, when the gap between the source and target domains is too substantial, directly aligning their distributions becomes increasingly difficult. Pseudo-labeling, a common strategy in direct adaptation, can further exacerbate this issue when the domain shift is severe. In such cases, incorrect pseudo-labels are likely to propagate through the adaptation process, leading to degraded performance and unstable training. Effective adaptation thus requires methods that can address these challenges by improving the reliability of pseudo-labels or reducing dependency on them. To address this challenge, we propose a novel approach that effectively alleviates domain shift by leveraging intermediate domains as bridges between the source and target domains. Specifically, we introduce a fixed ratio-based mixup to generate distinct intermediate domains between the source and target domains. By training on these augmented domains, we construct source-dominant and target-dominant models that possess distinct strengths and weaknesses, enabling us to implement effective complementary learning strategies. Furthermore, we enhance our fixed ratio-based mixup with uncertainty-aware learning, which addresses not only the image-level space but also the feature space, aiming to reduce the uncertainty at the most critical points within these spaces. Finally, we integrate confidence-based learning strategies, including bidirectional matching with high-confidence predictions and self-penalization with low-confidence predictions. Our extensive experiments on seven public benchmarks, including both single-source and multi-source scenarios, demonstrate the effectiveness of our method in UDA tasks.},
  archive      = {J_PR},
  author       = {Jaemin Na and Heechul Jung and Hyung Jin Chang and Wonjun Hwang},
  doi          = {10.1016/j.patcog.2025.111537},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111537},
  shortjournal = {Pattern Recognition},
  title        = {Bridging domain spaces for unsupervised domain adaptation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADGaze: Anisotropic gaussian label distribution learning for
fine-grained gaze estimation. <em>PR</em>, <em>164</em>, 111536. (<a
href="https://doi.org/10.1016/j.patcog.2025.111536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze estimation technology is crucial for enhancing the effectiveness and safety of applications in human–computer interaction, intelligent driving, virtual reality, and medical diagnosis. With advancements in deep learning, gaze estimation methods using deep neural networks have been extensively researched and applied. However, existing methods have yet to address the anisotropic characteristics of eye features. Based on the discovered anisotropic characteristics, we propose an Anisotropic Gaussian Label Distribution Learning Network for Gaze Estimation (ADGaze). ADGaze is capable of catching neighboring information by taking advantage of coarse-to-fine methodology and the anisotropic soft label construct. The coarse-to-fine framework initially performs classification tasks for gaze estimation, grouping gaze images with small variations into the same category, followed by regression tasks for each category. The construction of anisotropic Gaussian label distributions adopts methods based on data statistics and feature similarity. Extensive experimentation on public datasets has been carried out to substantiate the efficacy of this model. Our code is publicly available at https://github.com/dacilab/ADGaze .},
  archive      = {J_PR},
  author       = {Duantengchuan Li and Shutong Wang and Wanli Zhao and Lingyun Kang and Liangshan Dong and Jiazhang Wang and Xiaoguang Wang},
  doi          = {10.1016/j.patcog.2025.111536},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111536},
  shortjournal = {Pattern Recognition},
  title        = {ADGaze: Anisotropic gaussian label distribution learning for fine-grained gaze estimation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HIAN: A hybrid interactive attention network for multimodal
sarcasm detection. <em>PR</em>, <em>164</em>, 111535. (<a
href="https://doi.org/10.1016/j.patcog.2025.111535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sarcasm detection aims to use various modalities of data, such as text, images, etc., to identify whether they contain sarcastic meanings. Both images and texts contain rich sarcastic clues, but there are differences in dimension between them, and the quality of the sarcastic information they contain is very different. Therefore, seeking an appropriate feature fusion strategy to align modal features to maximize the utilization of inconsistent relationships between modalities is a significant challenge in this task. To this end, we introduce a novel sarcasm detection fusion model based on multimodal hybrid interactive attention (HIAN). We concatenate class words obtained from images with text and use the proposed bidirectional long short-term memory network with an interactive attention layer to enhance the extraction of text features. The text features obtained in this way can fully capture the contextual information of the text and the supplementary information in the image. To further enhance the feature fusion between modalities, we propose a multimodal interactive attention network and a fusion-enhanced transformer to promote the sharing of high-order complementary information, which represents the complementary non-linear semantic relationship between the three modalities and captures more inconsistencies between modalities. Extensive experiments conducted on publicly available multimodal sarcasm detection benchmark datasets show that our results surpass those of the baseline model and current state-of-the-art methods for the case of using the base BERT model.},
  archive      = {J_PR},
  author       = {Yongtang Bao and Xin Zhao and Peng Zhang and Yue Qi and Haojie Li},
  doi          = {10.1016/j.patcog.2025.111535},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111535},
  shortjournal = {Pattern Recognition},
  title        = {HIAN: A hybrid interactive attention network for multimodal sarcasm detection},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavior capture guided engagement recognition. <em>PR</em>,
<em>164</em>, 111534. (<a
href="https://doi.org/10.1016/j.patcog.2025.111534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engagement recognition aims to assess an individual’s involvement in various activities, which is essential in fields like education, healthcare, and driving. However, existing methods often suffer from performance degradation due to excessive data and distractions. In this paper, we introduce a novel model, the Behavior Capture-guided Transformer (BCTR). One of its key innovations lies in the proposed architecture for extracting regional features. Specifically, BCTR employs three independent class tokens to capture regional features – ocular, head, and trunk – from image sequences. These features are then used to model the dynamic streams of these regions for video-based engagement recognition. Another unique innovation of BCTR is its ability to mimic the observational techniques used by human teachers. By leveraging both frame-level and video-level class tokens, the model uses dual branches to detect both static and dynamic disengagement behaviors. This approach not only enables BCTR to achieve superior performance – 64.51% accuracy on the DAiSEE dataset and 0.0602 MSE loss on the EmotiW-EP dataset – but also enhances the interpretability of engagement levels by identifying these disengagements.},
  archive      = {J_PR},
  author       = {Yijun Bei and Songyuan Guo and Kewei Gao and Zunlei Feng},
  doi          = {10.1016/j.patcog.2025.111534},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111534},
  shortjournal = {Pattern Recognition},
  title        = {Behavior capture guided engagement recognition},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REHair: Efficient hairstyle transfer robust to face
misalignment. <em>PR</em>, <em>164</em>, 111533. (<a
href="https://doi.org/10.1016/j.patcog.2025.111533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hairstyle transfer is challenging due to intricate nature of hairstyles. In particular, face misalignment leads to distortion or deformation of the transferred hairstyle. To address this issue, we propose a Robust and Efficient Hairstyle transfer (REHair) framework, which comprises three stages: adaptive angle alignment, adaptive depth alignment, and efficient hairstyle editing. Firstly, we perform head pose estimation and adjust the facial rotation angle based on the latent code, thus ensuring consistent facial orientation between the face image and the hairstyle reference image and preventing hair shape and texture loss from iterative optimization methods. Secondly, we employ monocular depth estimation to predict the face depth of both images and perform adaptive depth alignment, ensuring the preservation of more hairstyle details. Finally, we propose a fast image embedding algorithm and integrate it with the latent code, significantly reducing the image embedding time in StyleGAN2. This adaptation enables REHair to be suitable for real-time applications. Quantitative and qualitative evaluations on the FFHQ and CelebA-HQ dataset demonstrate that REHair achieves state-of-the-art performance by successfully transferring hairstyles between images with different poses. The proposed method significantly reduces image embedding time while preserving image quality, and effectively addresses challenges associated with sub-optimal photography conditions and slow generation speed. Source code avaliable at https://github.com/fdwxfy/REHair .},
  archive      = {J_PR},
  author       = {Yiwen Xu and Liping Ling and Qingxu Lin and Ying Fang and Tiesong Zhao},
  doi          = {10.1016/j.patcog.2025.111533},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111533},
  shortjournal = {Pattern Recognition},
  title        = {REHair: Efficient hairstyle transfer robust to face misalignment},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EACE: Explain anomaly via counterfactual explanations.
<em>PR</em>, <em>164</em>, 111532. (<a
href="https://doi.org/10.1016/j.patcog.2025.111532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection aims to identify data points that deviate from the prevailing data distribution. Despite numerous anomaly detection models, there is a prevailing oversight in their interpretability, specifically regarding the rationale behind classifying a specific data point as an anomaly. Therefore, Interpretable Machine Learning has become a current research hotspot and is crucial for users to trust models. As one of the representative models, Counterfactual Explanation (CFE) methods generate alternative scenarios different from the observed data to explain model decisions. CFE tries to answer how the model’s output would change if certain factors (features) were altered. However, most existing CFE methods are designed for classification tasks, and it is a challenge for them to transform anomalies into counterfactual explanation samples effectively. To overcome this limitation, we propose a novel method for Explaining Anomaly via Counterfactual Explanation named EACE. Specifically, based on existing CFE methods’ limitations in handling anomalies, we propose a novel optimization objective by incorporating density loss and boundary loss. Meanwhile, we improved the genetic algorithm to solve this optimization problem since the new loss function is not differentiable. To evaluate the quality of the generated counterfactual explanations, we compare comprehensively with state-of-the-art counterfactual explanation methods and feature importance-based explanation methods. Experimental results demonstrate that EACE has a notable ability to convert anomalies into counterfactual explanation samples that are highly aligned with the normal cluster.},
  archive      = {J_PR},
  author       = {Peng Zhou and Qihui Tong and Shiji Chen and Yunyun Zhang and Xindong Wu},
  doi          = {10.1016/j.patcog.2025.111532},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111532},
  shortjournal = {Pattern Recognition},
  title        = {EACE: Explain anomaly via counterfactual explanations},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-learning of pseudo force field generation and
estimation for enhancing 3D molecular property prediction. <em>PR</em>,
<em>164</em>, 111531. (<a
href="https://doi.org/10.1016/j.patcog.2025.111531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning Energy-based Model via 3D molecular denoising has been shown to be effective in pretraining the 3D molecular representation. However, existing works carry out denoising in task-agnostic manner, causing inevitable domain gap with the downstream tasks. To overcome this issue, we introduce a task-aware pretraining framework, dubbed Mol-MFFGE, for adapting the energy-based pretraining to downstream tasks in meta-learning approach. In this framework, we design learnable pretraining tasks as generating and estimating pseudo force fields. This is achieved by proposing a learnable noise transformation module to generate the noisy motions of atoms and the model is pretrained to estimate them. These tasks are taken as the auxiliary self-supervised training tasks and learned with the downstream task jointly, formulated as a bi-level meta-learning optimization problem. Based on such an approach, the force field generation and estimation tasks are meta-learned to enhance the downstream tasks for molecular property prediction. Extensive experiments are conducted on three molecular property prediction datasets, and results demonstrate performance improvement over the state-of-the-art 3D molecular pretrained models.},
  archive      = {J_PR},
  author       = {Yufei Luo and Heran Yang and Jian Sun},
  doi          = {10.1016/j.patcog.2025.111531},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111531},
  shortjournal = {Pattern Recognition},
  title        = {Meta-learning of pseudo force field generation and estimation for enhancing 3D molecular property prediction},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive domain adaptation with test-time training for
out-of-context news detection. <em>PR</em>, <em>164</em>, 111530. (<a
href="https://doi.org/10.1016/j.patcog.2025.111530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-context news is a common type of misinformation on online media platforms. This involves posting a caption, alongside a mismatched news image. Reflecting its importance, researchers have developed models to detect such misinformation. However, a common limitation of these models is that they only consider the scenario where pre-labelled data is available for each news topic or agency, failing to address the out-of-context news detection on unverified news of other topics or agencies. In this work, we therefore focus on domain adaptive out-of-context news detection. We regard news topic or news agency as the domain . In order to effectively adapt the detection model to unlabelled news topics or agencies, we propose Con trastive D omain A daptation with T est- T ime T raining (ConDA-TTT). It first applies contrastive learning to learn a more separable representation space for news inputs, and then uses maximum mean discrepancy (MMD) to remove the domain-specific features so as to keep the domain-invariant features. During test time, it uses the trained model to predict pseudo labels for the target domain test data, and selects those with higher confidence scores to train the classifier of the model, in order to further adapt the model to the target domain data distribution. This approach adapts the model at both training and test phase, making the domain adaptation more robust to distribution shifts. Experimental results demonstrate that our approach outperforms state-of-the-art baselines in all the domain adaptation settings on two benchmark datasets, by as much as 2.6% in F1 and 2.4% in accuracy.},
  archive      = {J_PR},
  author       = {Yimeng Gu and Mengqi Zhang and Ignacio Castro and Shu Wu and Gareth Tyson},
  doi          = {10.1016/j.patcog.2025.111530},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111530},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive domain adaptation with test-time training for out-of-context news detection},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot sketch-based image retrieval with teacher-guided
and student-centered cross-modal bidirectional knowledge distillation.
<em>PR</em>, <em>164</em>, 111529. (<a
href="https://doi.org/10.1016/j.patcog.2025.111529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of zero-shot learning, the task of using unseen-class sketches as queries to retrieve real images is referred to as Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR). The ZS-SBIR task aims to generalize knowledge learned from known categories to unknown ones. Current research primarily relies on fine-tuning networks via loss functions or by unidirectionally extracting knowledge from fixed-parameter teacher models for training student models. However, unidirectional knowledge extraction from teacher models often lacks mutual learning and knowledge alignment between the teacher and student models, while fine-tuning networks via loss functions struggles to handle both photo and sketch modalities simultaneously. Therefore, we designed a modal perception and distribution alignment scheme based on gradient weighting to explore both photo and sketch features bidirectionally and deeply investigate the relationships between different modalities. Building on this, we propose a teacher-guided and student-centered cross-modal bidirectional knowledge distillation framework. During training, the student and teacher models mutually learn discriminative information based on the relationships between different modalities and synchronize their parameters guided by the teacher model, thus effectively achieving cross-modal alignment. Extensive experiments conducted on the TU-Berlin Ext, Sketchy Ext and QuickDraw Ext datasets demonstrate that our method significantly enhances retrieval performance.},
  archive      = {J_PR},
  author       = {Jiale Du and Yang Liu and Xinbo Gao and Jungong Han and Lei Zhang},
  doi          = {10.1016/j.patcog.2025.111529},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111529},
  shortjournal = {Pattern Recognition},
  title        = {Zero-shot sketch-based image retrieval with teacher-guided and student-centered cross-modal bidirectional knowledge distillation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging facial landmarks improves generalization ability
for deepfake detection. <em>PR</em>, <em>164</em>, 111528. (<a
href="https://doi.org/10.1016/j.patcog.2025.111528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, facial forgery technology has become increasingly sophisticated and published datasets aim to cover a wide range of data variations. Existing deepfake detection models have benefited from the powerful feature embedding of deep networks and carefully designed fine-tuning modules, resulting in an excellent performance on in-dataset evaluations. However, the performance declines in cross-dataset evaluations due to various forgery methods and dataset shifts. In this study, we concentrate on the generalization issue of deepfake detection and find that forgery traces appear to gather around the facial interest points even manipulated by different forgery methods. To facilitate this, we propose a Trail Tracing Network (TTNet) to capture the generalized feature representation, which leverages facial landmarks to eliminate redundant information and expand the forged traces in the feature space. We conduct extensive experiments on the widely employed benchmarks, including FaceForensics++, DFDCp, and Celeb-DF. Experimental results demonstrate the outstanding generalization ability of our method against existing state-of-the-art methods by a large margin. In addition, the proposed method also exhibits excellent performance on the in-dataset evaluation.},
  archive      = {J_PR},
  author       = {Qi Gao and Baopeng Zhang and Jianghao Wu and Wenxin Luo and Zhu Teng and Jianping Fan},
  doi          = {10.1016/j.patcog.2025.111528},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111528},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging facial landmarks improves generalization ability for deepfake detection},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSTKD: Triple-spike train kernel-driven supervised learning
algorithm. <em>PR</em>, <em>164</em>, 111525. (<a
href="https://doi.org/10.1016/j.patcog.2025.111525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise artificial intelligence is one of the most promising research fields, where supervised learning for spiking neurons (SNs) plays an imperative and fundamental role. This study proposes a novel supervised learning algorithm based on triple-spike train kernels to address the shortcomings of the latest learning algorithms, such as local best learning and low learning accuracy. First, we divided the time intervals of the spike trains, including the firing time of the input spikes. Subsequently, we discovered and analyzed the relationship between the firing times of all spikes, added a third spike to solve the existing problem, and constructed a triple-spike-driven (TSD) minimum direct computational unit. In addition to the simple and efficient adjustment of synaptic weights based on pair-spike, TSD maintains a relationship between all useful spikes to approximate the global best learning. Finally, we proposed a triple-spike train kernel driven (TSTKD) supervised learning algorithm to improve the learning performance. Many fundamental experiments were implemented to demonstrate the learning performance, which proved that the successful learning ability and some learning factors of our proposed algorithm in spike train learning. We then verified the positive effect of the TSD on the proposed algorithm. Many experiments also proved the much higher learning accuracy of the proposed state-of-the-art algorithm compared to some of the latest algorithms, especially in the complex spike train learning. In addition, the proposed algorithm is more adaptive to SNs and much better at generalizing, memorizing, and classifying than the corresponding algorithm with pair-spike and some of the latest algorithms. Considering the above experimental results, our study blazes a trail for pattern recognition using spike train supervised learning with global optimization.},
  archive      = {J_PR},
  author       = {Guojun Chen and Guoen Wang},
  doi          = {10.1016/j.patcog.2025.111525},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111525},
  shortjournal = {Pattern Recognition},
  title        = {TSTKD: Triple-spike train kernel-driven supervised learning algorithm},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial multi-label feature selection based on label
distribution learning. <em>PR</em>, <em>164</em>, 111523. (<a
href="https://doi.org/10.1016/j.patcog.2025.111523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial Multi-label Learning (PML) induces a multi-classifier in an imprecise supervised environment, where the candidate labels associated with each training sample are partially valid. The high-dimensional feature space, presented in PML data accompanied by ambiguous labeling information, is a significant challenge for learning. In this paper, we propose a PML feature selection method based on Label Distribution Learning (LDL), which handles the above challenges by correcting misleading and then selecting common and label-specific features. In the first procedure, the error distribution hypothesis is constructed, which divides the structure of ambiguous label information into minority and majority error distribution according to the error amount that may appear in the data annotation process. Under the analysis of the hypothesis, the label credibility distribution data (LCDD) was generated by identifying and correcting errors, where the fractional category of each label associated with each training sample describes the probability that the label belongs to that sample. In the second procedure, a discriminative feature subset is selected for PML based on LCDD by common and label-specific feature constraints. Experiments on three synthetic and five real PML datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Yaojin Lin and Yulin Li and Shidong Lin and Lei Guo and Yu Mao},
  doi          = {10.1016/j.patcog.2025.111523},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111523},
  shortjournal = {Pattern Recognition},
  title        = {Partial multi-label feature selection based on label distribution learning},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEVICE: Depth and visual concepts aware transformer for
OCR-based image captioning. <em>PR</em>, <em>164</em>, 111522. (<a
href="https://doi.org/10.1016/j.patcog.2025.111522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OCR-based image captioning is an important but under-explored task, aiming to generate descriptions containing visual objects and scene text. Recent studies have made encouraging progress, but they are still suffering from a lack of overall understanding of scenes and generating inaccurate captions. One possible reason is that current studies mainly focus on constructing the plane-level geometric relationship of scene text without depth information. This leads to insufficient scene text relational reasoning so that models may describe scene text inaccurately. The other possible reason is that existing methods fail to generate fine-grained descriptions of some visual objects. In addition, they may ignore essential visual objects, leading to the scene text belonging to these ignored objects not being utilized. To address the above issues, we propose a Depth and Visual Concepts Aware Transformer (DEVICE) for OCR-based image captioning. Concretely, to construct three-dimensional geometric relations, we introduce depth information and propose a depth-enhanced feature updating module to ameliorate OCR token features. To generate more precise and comprehensive captions, we introduce semantic features of detected visual concepts as auxiliary information, and propose a semantic-guided alignment module to improve the model’s ability to utilize visual concepts. Our DEVICE is capable of comprehending scenes more comprehensively and boosting the accuracy of described visual entities. Sufficient experiments demonstrate the effectiveness of our proposed DEVICE, which outperforms state-of-the-art models on the TextCaps test set.},
  archive      = {J_PR},
  author       = {Dongsheng Xu and Qingbao Huang and Xingmao Zhang and Haonan Cheng and Feng Shuang and Yi Cai},
  doi          = {10.1016/j.patcog.2025.111522},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111522},
  shortjournal = {Pattern Recognition},
  title        = {DEVICE: Depth and visual concepts aware transformer for OCR-based image captioning},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generically contrastive spatiotemporal representation
enhancement for 3D skeleton action recognition. <em>PR</em>,
<em>164</em>, 111521. (<a
href="https://doi.org/10.1016/j.patcog.2025.111521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition is a central task in computer vision and human–robot interaction. However, most previous methods suffer from overlooking the explicit exploitation of the latent data distributions ( i.e. , the intra-class variations and inter-class relations), thereby leading to confusion about ambiguous samples and sub-optimum solutions of the skeleton encoders. To mitigate this, we propose a C ontrastive S patiotemporal R epresentation E nhancement (CSRE) framework to obtain more discriminative representations from the sequences, which can be incorporated into various previous skeleton encoders and can be removed when testing. Specifically, we decompose the representation into spatial- and temporal-specific features to explore fine-grained motion patterns along the corresponding dimensions. Furthermore, to explicitly exploit the latent data distributions, we employ the attentive features to contrastive learning, which models the cross-sequence semantic relations by pulling together the features from the positive pairs and pushing away the negative pairs. Extensive experiments show that CSRE with five various skeleton encoders (HCN, 2S-AGCN, CTR-GCN, Hyperformer, and BlockGCN) achieves solid improvements on five benchmarks. The code will be released at https://github.com/zhshj0110/CSRE .},
  archive      = {J_PR},
  author       = {Shaojie Zhang and Jianqin Yin and Yonghao Dang},
  doi          = {10.1016/j.patcog.2025.111521},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111521},
  shortjournal = {Pattern Recognition},
  title        = {A generically contrastive spatiotemporal representation enhancement for 3D skeleton action recognition},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TextDiff: Enhancing scene text image super-resolution with
mask-guided residual diffusion models. <em>PR</em>, <em>164</em>,
111513. (<a href="https://doi.org/10.1016/j.patcog.2025.111513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of scene text image super-resolution (STISR) is to reconstruct high-resolution text-line images from unrecognizable low-resolution inputs. The existing methods relying on the optimization of pixel-level loss tend to yield text edges that exhibit a notable degree of blurring, thereby exerting a substantial impact on both the readability and recognizability of the text. To address these issues, we propose TextDiff, the first diffusion-based framework tailored for STISR. It contains two modules: the Text Enhancement Module (TEM) and the Mask-Guided Residual Diffusion Module (MRD). The TEM generates an initial deblurred text image and a mask that encodes the spatial location of the text. The MRD is responsible for effectively sharpening the text edge by modeling the residuals between the ground-truth images and the initial deblurred images. Extensive experiments demonstrate that our TextDiff achieves state-of-the-art (SOTA) performance on public benchmark datasets, with a maximum improvement of 2.0% in recognition accuracy over existing methods while enhancing the readability of scene text images. Moreover, our proposed MRD module is plug-and-play that effectively sharpens the text edges produced by SOTA methods. This enhancement not only improves the readability and recognizability of the results generated by SOTA methods but also does not require any additional joint training.},
  archive      = {J_PR},
  author       = {Baolin Liu and Zongyuan Yang and Chinwai Chiu and Yongping Xiong},
  doi          = {10.1016/j.patcog.2025.111513},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111513},
  shortjournal = {Pattern Recognition},
  title        = {TextDiff: Enhancing scene text image super-resolution with mask-guided residual diffusion models},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative knowledge distillation and pruning for model
compression in unsupervised domain adaptation. <em>PR</em>,
<em>164</em>, 111512. (<a
href="https://doi.org/10.1016/j.patcog.2025.111512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications, deep learning models often face the challenges of inconsistent distribution between training data and test data and insufficient labeled data. To address these problems, unsupervised domain adaptation (UDA) based transfer learning has gained significant attention. However, the existing UDA models are difficult to meet the requirements of real-time and resource-constrained scenarios. Although model compression can accelerate UDA, it usually leads to performance degradation. In this paper, we propose an iterative transfer model compression (ITMC) method, which centers on two key modules, i.e., transfer knowledge distillation (TKD) and adaptive channel pruning (ACP), by executing them alternately. The tight coupling of the two modules realizes the effective compression of the model while ensuring the performance of the model on the target domain. In the TKD phase, the teacher model and the student model are gradually adapted to the target domain, and the real-time updated teacher model efficiently guides the student model learning, while the ACP phase employs a dynamic pruning strategy based on the training epoch, which removes unimportant channels based on the loss of the TKD student model. Experimental results demonstrate that ITMC approach achieves higher accuracy under the same compression ratio compared with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhiyuan Wang and Long Shi and Zhen Mei and Xiang Zhao and Zhe Wang and Jun Li},
  doi          = {10.1016/j.patcog.2025.111512},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111512},
  shortjournal = {Pattern Recognition},
  title        = {Iterative knowledge distillation and pruning for model compression in unsupervised domain adaptation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RhythmFormer: Extracting patterned rPPG signals based on
periodic sparse attention. <em>PR</em>, <em>164</em>, 111511. (<a
href="https://doi.org/10.1016/j.patcog.2025.111511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications. Due to the periodicity nature of rPPG signals, the long-range dependency capturing capacity of the transformer was assumed to be advantageous for such signals. However, existing methods have not conclusively demonstrated the superior performance of transformers over traditional convolutional neural networks. This may be attributed to the quadratic scaling exhibited by transformer with sequence length, resulting in coarse-grained feature extraction, which in turn affects robustness and generalization. To address that, this paper proposes a periodic sparse attention mechanism based on temporal attention sparsity induced by periodicity. A pre-attention stage is introduced before the conventional attention mechanism. This stage learns periodic patterns to filter out a large number of irrelevant attention computations, thus enabling fine-grained feature extraction. Moreover, to address the issue of fine-grained features being more susceptible to noise interference, a fusion stem is proposed to effectively guide self-attention towards rPPG features. It can be easily integrated into existing methods to enhance their performance. Extensive experiments show that the proposed method achieves state-of-the-art performance in both intra-dataset and cross-dataset evaluations. The codes are available at https://github.com/zizheng-guo/RhythmFormer .},
  archive      = {J_PR},
  author       = {Bochao Zou and Zizheng Guo and Jiansheng Chen and Junbao Zhuo and Weiran Huang and Huimin Ma},
  doi          = {10.1016/j.patcog.2025.111511},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111511},
  shortjournal = {Pattern Recognition},
  title        = {RhythmFormer: Extracting patterned rPPG signals based on periodic sparse attention},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning multi-granularity representation with transformer
for visible-infrared person re-identification. <em>PR</em>,
<em>164</em>, 111510. (<a
href="https://doi.org/10.1016/j.patcog.2025.111510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) aims to match pedestrian images from visible and near-infrared modalities. The pedestrian images of two modalities contain discriminative features in different sizes and positions, e.g. , the global color of the cloth, the body’s local pose, and the shoe’s pixel size. However, existing methods mainly capture features at a single granularity, ignoring multi-granularity information contributing to pedestrian identification. Therefore, we propose a cross-modality multi-granularity Transformer (CM 2 GT) framework to solve this issue. CM 2 GT learns coarse-to-fine feature representations and integrates discriminative information across various granularities, which alleviates problems of the irrelevant matching and ambiguous alignment caused by matching single granularity features. Specifically, we first design a multi-granularity feature extractor (MGFE) module based on Transformer to capture the global-patch-pixel level features of each modality, which can flexibly represent semantic information at multiple scales. Secondly, a multi-granularity fusion Transformer (MGFT) module mines the hierarchical relationships between multi-granularity features by a saliency-enhanced Transformer, which ensures the identity-wise saliency consistency across different granularities and modalities. Furthermore, to further enhance cross-modality intra-class clustering in latent space, we design a cross-modality nearest-neighbor clustering (CNC) loss function to minimize the distance between the anchor sample and its cross-modality nearest neighbor. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yujian Feng and Feng Chen and Guozi Sun and Fei Wu and Yimu Ji and Tianliang Liu and Shangdong Liu and Xiao-Yuan Jing and Jiebo Luo},
  doi          = {10.1016/j.patcog.2025.111510},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111510},
  shortjournal = {Pattern Recognition},
  title        = {Learning multi-granularity representation with transformer for visible-infrared person re-identification},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mask-aware 3D axial transformer for video inpainting.
<em>PR</em>, <em>164</em>, 111509. (<a
href="https://doi.org/10.1016/j.patcog.2025.111509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Mask-Aware 3D Axial Transformer for efficient and effective video inpainting, which aims to recover the missing content of a video by leveraging long-range information in an efficient way. Recent works show that the transformer architecture achieves promising video inpainting performance, due to its powerful capability to exploit long-range consistency across frames. However, it requires high time complexity to compute the global self-attention. On the other hand, existing transformer-based inpainting methods treat the valid and invalid regions in the masked image the same when calculating self-attention, causing the network not to distinguish their differences. To address these issues, we first design a 3D Axial Transformer which splits the input features into three shapes of stripes, including a horizontal stripe, a vertical stripe to perform intra-frame attention, and a temporal stripe for inter-frame attention. With three such transformer blocks stacked, the relevance between two arbitrary spatial–temporal pixels across all video frames can be reached while maintaining high efficiency. We also devise a mask-aware module to predict the reliability score of masked pixels, which helps the transformer avoid leveraging information from the invalid region. Extensive experimental results on the Youtube-VOS and DAVIS datasets show that our approach outperforms the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Hongyi Sun and Wanhua Li and Jiwen Lu and Jie Zhou},
  doi          = {10.1016/j.patcog.2025.111509},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111509},
  shortjournal = {Pattern Recognition},
  title        = {Mask-aware 3D axial transformer for video inpainting},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised group re-identification from aerial perspective
via strategic member harmonization. <em>PR</em>, <em>164</em>, 111508.
(<a href="https://doi.org/10.1016/j.patcog.2025.111508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group re-identification (G-ReID) aims to match group images of the same identity. Existing G-ReID methods perform well on ground-based datasets, but remain unexplored in aerial perspective. One reason is the significant human effort required for aerial associations and the inability of unsupervised methods to address low-quality aerial pedestrian detection and reduced feature visibility. To address these issues, we propose Strategic Member Harmonization. Strategic members are harmonized to complement potential information lost or destroyed due to low-quality detections or significant member variations, thus forming harmonization groups. Harmonization groups introduce a richer layer of the underlying information, mitigating clustering inaccuracies gradually. To address the lack of aerial G-ReID datasets, we construct a new aerial dataset with 10,168 group images and 653 different group identities. Our approach achieves state-of-the-art performance on our dataset and performs well on other ground-based datasets. Our dataset is available at https://github.com/chen1hx/UAV-Group.},
  archive      = {J_PR},
  author       = {Hongxu Chen and Quan Zhang and Xiaohua Xie and Jianhuang Lai},
  doi          = {10.1016/j.patcog.2025.111508},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111508},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised group re-identification from aerial perspective via strategic member harmonization},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scientific poster generation: A new dataset and approach.
<em>PR</em>, <em>164</em>, 111507. (<a
href="https://doi.org/10.1016/j.patcog.2025.111507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating poster creation from research papers saves scientists time. However, training models for this task is challenging due to limited datasets. Moreover, existing methods are mostly rule/template-based, which lack the flexibility to adapt to different content and design requirements in scientific posters. Our contributions aim to address these issues. We introduce Sci-PosterLayout , a dataset comprising 1,226 scientific posters with greater variety in content , layout and domains . Using a template-free method with a seq2seq model and Design Pattern Schema ( DPS ), we learn various content and design patterns for poster layout generation. Evaluations against existing methods and datasets show our approach produces high-quality posters with diverse layouts. Our work seeks to advance research in scientific poster generation by building a new dataset and proposing template-free methods that require minimal human intervention. The Sci-PosterLayout dataset will be publicly available at https://github.com/kitman0000/Sci-PosterLayout-Data .},
  archive      = {J_PR},
  author       = {Xinyi Zhong and Zusheng Tan and Jing Li and Shen Gao and Jing Ma and Shanshan Feng and Billy Chiu},
  doi          = {10.1016/j.patcog.2025.111507},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111507},
  shortjournal = {Pattern Recognition},
  title        = {Scientific poster generation: A new dataset and approach},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S2DiNet: Towards lightweight and fast high-resolution
dichotomous image segmentation. <em>PR</em>, <em>164</em>, 111506. (<a
href="https://doi.org/10.1016/j.patcog.2025.111506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dichotomous Image Segmentation task aims to achieve ultra-high precision binary segmentation for category-agnostic objects, including salient, camouflaged, structurally complex, or feature-similar entities. Traditional methods designed for low-resolution inputs produce blurred segmentation, failing to meet such critical safety and stability requirements. Although existing DIS methods achieve high accuracy, they are often parameter-heavy and slow, neglecting practical application needs. To address these challenges, this paper proposes a light-weight and fast framework, aims at improving processing efficiency while ensuring accuracy in high-resolution natural scenes. The proposed method utilizes a shared-weight ResNet-18 backbone to process inputs of different scales. A Feature Synchronization module is employed to enhance the correlation between encoded features of different resolutions. To reduce the parameter and increase the inference speed, the number of feature channels are decreased; however, this also resulted in information loss. The Star Fusion module is introduced to mitigate this issue. Furthermore, a Decoupling and Integration Decoder is adopted to progressively decode and fuse the body, detail, and mask features of the object, enhancing feature decoding accuracy. The proposed model runs at 26.3 FPS with a 48.7 MB size, reducing parameters by 72.4% and increasing speed by 30.8% compared to baseline method ISNet, while maintaining superior performance. Moreover, it surpasses several existing high-resolution methods in terms of accuracy.},
  archive      = {J_PR},
  author       = {Shuhan Chen and Haonan Tang and Yuan Huang and Lifeng Zhang and Xuelong Hu},
  doi          = {10.1016/j.patcog.2025.111506},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111506},
  shortjournal = {Pattern Recognition},
  title        = {S2DiNet: Towards lightweight and fast high-resolution dichotomous image segmentation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CGViT: Cross-image GroupViT for zero-shot semantic
segmentation. <em>PR</em>, <em>164</em>, 111505. (<a
href="https://doi.org/10.1016/j.patcog.2025.111505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the increase of image-text data, such coarse data has also been introduced to address the image semantic segmentation task. However, previous works simply transfer the methods used in other visual tasks to image semantic segmentation, ignoring the task characteristics of semantic segmentation. In this work, we propose a C ross-image G roup ViT (CGViT) for zero-shot semantic segmentation, constructing a semantically consistent feature representation across images. Specifically, we improve the previous work GroupViT in two aspects. We propose two grouping blocks and update them with a momentum-based method, constructing a semantically consistent feature representation across images. Then we introduce an image-level supervision for learning semantic information and a token-level supervision for fine-grained information, obtaining hierarchical information for semantic segmentation. We train the model with image-text data and transfer it to zero-shot semantic segmentation without fine-tuning. Furthermore, the CGViT achieves new state-of-the-art results on three challenging datasets. Especially, the CGViT obtains 49.30% in mIoU on PASCAL VOC dataset, when only pre-trained on CC12M dataset.},
  archive      = {J_PR},
  author       = {Jie Jiang and Xingjian He and Xinxin Zhu and Weining Wang and Jing Liu},
  doi          = {10.1016/j.patcog.2025.111505},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111505},
  shortjournal = {Pattern Recognition},
  title        = {CGViT: Cross-image GroupViT for zero-shot semantic segmentation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to restore arbitrary hybrid adverse weather
conditions in one go. <em>PR</em>, <em>164</em>, 111504. (<a
href="https://doi.org/10.1016/j.patcog.2025.111504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adverse conditions typically suffer from stochastic hybrid weather degradations (e.g., rainy and hazy night), while existing image restoration algorithms envisage that weather degradations occur independently, thus may fail to handle real-world complicated scenarios. Besides, supervised training is not feasible due to the lack of comprehensive paired dataset to characterize hybrid weather conditions. To this end, we have advanced the forementioned limitations with two tactics: framework and data. First, we present a novel unified framework, dubbed RAHC, to Restore Arbitrary Hybrid adverse weather Conditions in one go. Specifically, our RAHC leverages a multi-head aggregation architecture to learn multiple degradation representation subspaces and then constrains the network to flexibly handle multiple hybrid adverse weather in a unified paradigm through a discrimination mechanism in the output space. Furthermore, we devise a reconstruction vectors aided scheme to provide auxiliary visual content cues for reconstruction, thus can comfortably cope with hybrid scenarios with insufficient remaining image constituents. Second, we establish a new dataset, termed HAC, for learning and benchmarking arbitrary Hybrid Adverse Conditions restoration. HAC contains 31 scenarios composed of an arbitrary fusion of five common weather, with a total of ∼ 316 K adverse-weather/clean pairs. As for fabrication, the training set is automatically generated by a dedicated AdverseGAN with no-frills labor, while the test set is manually modulated by experts for authoritative evaluation. Extensive experiments yield superior results and in particular establish new state-of-the-art results on both HAC and conventional datasets.},
  archive      = {J_PR},
  author       = {Yecong Wan and Mingwen Shao and Yuanshuo Cheng and Yuexian Liu and Zhiyuan Bao},
  doi          = {10.1016/j.patcog.2025.111504},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111504},
  shortjournal = {Pattern Recognition},
  title        = {Learning to restore arbitrary hybrid adverse weather conditions in one go},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive class-aware instance enhancement for aircraft
detection in remote sensing imagery. <em>PR</em>, <em>164</em>, 111503.
(<a href="https://doi.org/10.1016/j.patcog.2025.111503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aircraft detection and type identification in optical remote sensing imagery are critical for civilian and military applications, including air traffic control and strategic surveillance. However, existing methods ignore the unique cross-shaped geometric structure and low spatial occupancy of aircraft, leading to inaccurate localization and category confusion. In response, this paper proposes a novel anchor-free detection network that leverages point set representation, integrating the progressive class-aware dual branches (PCA-DB) and instance-guided enhancement module (IGEM). Specifically, considering the underlying structure of aircraft, PCA-DB consists of the coarse foreground instance branch and the refined cross-shaped branch to facilitate high-quality point set generation. Through multi-task learning, the auxiliary branches implicitly inject geometric priors into shared features, effectively suppressing background interference. Subsequently, IGEM introduces the interactive attention mechanism to adaptively fuse the instance-level information in the auxiliary branch with features in the main branches, explicitly enhancing the discriminative features of aircraft. Extensive experiments validate the superior performance of the proposed method on several aircraft datasets, including MAR20, FAIR1M-Plane, and CORS-ADD. There are 5.42%, 4.28%, and 1.37% improvements in mAP in our method compared to the baseline network.},
  archive      = {J_PR},
  author       = {Tianjun Shi and Jinnan Gong and Jianming Hu and Yu Sun and Guangzhen Bao and Pengfei Zhang and Junjie Wang and Xiyang Zhi and Wei Zhang},
  doi          = {10.1016/j.patcog.2025.111503},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111503},
  shortjournal = {Pattern Recognition},
  title        = {Progressive class-aware instance enhancement for aircraft detection in remote sensing imagery},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for efficient registration
between intraoral-scan meshes and CT images. <em>PR</em>, <em>164</em>,
111502. (<a href="https://doi.org/10.1016/j.patcog.2025.111502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Registration between computed tomography (CT) images and intraoral-scan (IOS) meshes facilitates dental procedure planning. However, the spatial complexity of 3D-space computations presents a significant challenge, necessitating the reduction of computational cost through efficient sampling while maintaining robustness via global approximation without segmentation. Herein, we introduce an efficient and robust method for registering CT images and IOS meshes, eliminating the need for segmentation. We utilized an effective sampling technique to identify key vertices in IOS meshes by calculating the negative curvatures between adjacent faces. The significant vertices are transformed into a novel graph representation, serving as the input state for the graph convolution-based backbone network within a deep reinforcement learning (DRL) framework. This framework approximates an optimal solution through sequential decision-making, selecting the best among 12 actions by considering translation and rotation to accurately locate the 3D mesh at arbitrary positions and angles on maxillary or mandibular teeth in CT images. The proposed method was evaluated against conventional and deep learning-based methods, demonstrating mean absolute errors of 1.955 ± 1.310 and 1.399 ± 0.644 mm for maxillary and mandibular teeth, respectively. Additionally, it required only 0.48 M floating-point operations for the calculations, making it more efficient than existing methods.},
  archive      = {J_PR},
  author       = {Seungpil Choi and Seoyeon Jang and Sunghee Jung and Heon Jae Cho and Byunghwan Jeon},
  doi          = {10.1016/j.patcog.2025.111502},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111502},
  shortjournal = {Pattern Recognition},
  title        = {Deep reinforcement learning for efficient registration between intraoral-scan meshes and CT images},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal fusion via voting network for 3D object detection
in indoors. <em>PR</em>, <em>164</em>, 111501. (<a
href="https://doi.org/10.1016/j.patcog.2025.111501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, 3D object detection has become essential in machine vision systems, as it requires more spatial information, such as position and size, compared to traditional 2D detection. Numerous studies have successfully achieved accurate localization, size estimation, orientation estimation, and classification of objects in diverse scenarios. Building on this foundation, we propose a novel method called &quot;Point Cloud and Image VoteNet,&quot; which enhances 3D object detection through the early fusion of radar point cloud data and image features. Our Point-Fusion technique projects radar point clouds onto images to extract complementary features. By incorporating point cloud density parameters, we improve the object matching mechanism, resulting in precise detections. Experimental results demonstrate that our model effectively leverages the combined information from point clouds and images, achieving superior performance. The fusion techniques and optimization strategies employed significantly enhance accuracy and robustness, showcasing promising potential in applications such as autonomous driving, robotics, and augmented reality.},
  archive      = {J_PR},
  author       = {Jianxin Li and Guannan Si and Xinyu Liang and Zhaoliang An and Pengxin Tian and Fengyu Zhou and Xiaoliang Wang},
  doi          = {10.1016/j.patcog.2025.111501},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111501},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal fusion via voting network for 3D object detection in indoors},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based vector quantized variational autoencoder for
anomaly detection by using orthogonal subspace constraints. <em>PR</em>,
<em>164</em>, 111500. (<a
href="https://doi.org/10.1016/j.patcog.2025.111500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new framework that uses a vector quantized variational autoencoder (VQVAE) enhanced by orthogonal subspace constraints (OSC) and pyramid criss-cross attention (PCCA). The framework was designed for anomaly detection in industrial product image datasets. Previous studies on modeling low-dimensional feature distributions have been unable to effectively distinguish between normal features and noisy/abnormal information, which is effectively addressed using OSC in this study. Then, the vector quantized mechanism is embodied in these two complementary subspaces to obtain normal and abnormal embedding subspaces and discrete representations for normal and noisy information, respectively. The proposed approach robustly represents low-dimensional discrete manifolds to present the information from normal data using a limited number of feature vectors. Additionally, two PCCA modules are proposed to capture feature maps from different layers in the encoder and decoder, benefitting the low-dimensional mapping and reconstruction process. The features of different layers are treated as the query (Q), key (K), and value (V), which could capture both low-level and high-level features, incorporating comprehensive contextual information. The effectiveness of the proposed framework for anomaly detection is assessed by comparing its performance with those of the state-of-the-art approaches on various publicly available industrial product image datasets.},
  archive      = {J_PR},
  author       = {Qien Yu and Shengxin Dai and Ran Dong and Soichiro Ikuno},
  doi          = {10.1016/j.patcog.2025.111500},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111500},
  shortjournal = {Pattern Recognition},
  title        = {Attention-based vector quantized variational autoencoder for anomaly detection by using orthogonal subspace constraints},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal urban traffic flow prediction based on
multi-scale time series imaging. <em>PR</em>, <em>164</em>, 111499. (<a
href="https://doi.org/10.1016/j.patcog.2025.111499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is of great significance for administrators and travelers to make informed decisions in advance. Since the increasing correlation between predicted flow and historical flow from more recent periods, most existing traffic flow prediction models generally learn spatial-temporal patterns from historical single-scale time series data with time steps not exceeding 12. However, such short-term flow does not contain continuous and dynamic long-term spatial-temporal patterns. Thus, how to comprehensively learn the diverse patterns and achieve a satisfactory balance between effectiveness and efficiency presents a challenge. To this end, we propose a multimodal urban traffic flow prediction model based on multi-scale time series imaging (MM-TSI). Specifically, a data processing mechanism of multi-scale time series imaging is specially designed to efficiently learn both short- and long-term spatial-temporal patterns. After that, an image-based module is proposed to be parallelly integrated into a traditional time series-based module. By adaptively fusing the two-modal features extracted from image-based module and time series-based module, MM-TSI is capable of effectively learning more comprehensive and diverse spatial-temporal patterns while maintaining efficiency to a certain extent. Extensive experiments are conducted on three real-world urban traffic flow datasets. The results demonstrate that the proposed MM-TSI significantly outperforms the state-of-the-art (SOTA) models and exhibits generalization ability in both short- and long-term prediction.},
  archive      = {J_PR},
  author       = {Qinzhi Lv and Lijuan Liu and Ruotong Yang and Yan Wang},
  doi          = {10.1016/j.patcog.2025.111499},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111499},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal urban traffic flow prediction based on multi-scale time series imaging},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian dual-pathway network for unsupervised domain
adaptation. <em>PR</em>, <em>164</em>, 111498. (<a
href="https://doi.org/10.1016/j.patcog.2025.111498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) endeavors to address the challenges presented by domain shifts between domains characterized by differing yet related distributions. Traditional adversarial approaches typically adopt a single-pathway adversarial paradigm, which relies on a singular pathway to align the marginal distributions at the domain level. Despite notable advancements, this paradigm is constrained by two major limitations that lead to sub-optimal performance in both source and target domains. First, naive domain-level alignment often results in class mismatches. Second, the single-pathway adversarial approach grapples with the conflicting demands of reducing domain shift while simultaneously learning comprehensive features. Drawing inspiration from cognitive neuroscience, we propose a Bayesian Dual-Pathway Network (BDNet) for UDA to compute a classification prior for each domain, comprising a domain-shared pathway and a domain-specific pathway, designed to enhance target domain performance while preserving source domain efficacy. Specifically, the domain-shared pathway is employed to learn classification prior features through an adversarial paradigm grounded in structural alignment. Concurrently, a domain-specific pathway is crafted to extract distinct features, incorporating domain likelihood and domain prior features. Comprehensive features are synthesized through the fusion of common and specific attributes via a lightweight fusion module. Extensive experiments across three publicly available datasets demonstrate the efficacy of our approach, evidencing superior performance in both source and target domains.},
  archive      = {J_PR},
  author       = {Yuhang He and Junzhe Chen and Jiehua Zhang and Wei Ke and Yihong Gong},
  doi          = {10.1016/j.patcog.2025.111498},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111498},
  shortjournal = {Pattern Recognition},
  title        = {A bayesian dual-pathway network for unsupervised domain adaptation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guiding prototype networks with label semantics for few-shot
text classification. <em>PR</em>, <em>164</em>, 111497. (<a
href="https://doi.org/10.1016/j.patcog.2025.111497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot text classification aims to recognize unseen classes with limited labeled text samples. Typical meta-learning methods, e.g., Prototypical Networks, face several problems. (1) The limited words in each sentence make it difficult to extract fine-grained class-related semantic information. (2) The semantic information from labels is not fully utilized, leading to ambiguities in class definitions. (3) The randomly selected support samples cannot represent their corresponding classes well. In this paper, we propose to leverage label semantics tackling the above problems and present L abel G uided P rototype N etworks (LGPN). Firstly, we use prompt encoding to generate text representations instead of aggregating the words in the sentences, extracting more class-related semantic information. Secondly, we propose Label-guided Distance Scaling (LDS), in the training stage, we design label-guided loss to pull the samples closer to their corresponding labels, making class distributions distinguishable. Thirdly, in the testing stage, we scale the text representations with the label semantics to pull each support sample closer to the class center, which reduces the prediction contradictions caused by randomly selected support samples (i.e., unsatisfactory support sample representations). We conduct extensive experiments on six benchmark datasets, and our LGPN shows obvious advantages over state-of-the-art models. Additionally, we further explore the effectiveness and universality of our modules.},
  archive      = {J_PR},
  author       = {Xinyue Liu and Yunlong Gao and Linlin Zong and Wenxin Liang and Bo Xu},
  doi          = {10.1016/j.patcog.2025.111497},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111497},
  shortjournal = {Pattern Recognition},
  title        = {Guiding prototype networks with label semantics for few-shot text classification},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rank gap sensitive deep AUC maximization for CTR prediction.
<em>PR</em>, <em>164</em>, 111496. (<a
href="https://doi.org/10.1016/j.patcog.2025.111496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Network (DNN) stands out as one widely adopted and effective technique for Click-Through Rate (CTR) prediction in live recommender systems. However, the prevalent DNN-based CTR methods exhibit two main drawbacks. On one hand, they fail to align their optimization objectives with the benchmark metric, such as the Area Under the ROC Curve (AUC), designed for ranking tasks. On the other hand, current DNN-based CTR solutions indiscriminately treat all positive-negative item pairs, ignoring the fact that each item pair differently contributes to AUC optimization. To this end, we propose R ank G ap S ensitive Deep AUC maximization method for accurate CTR prediction, namely RgsAUC. Specifically, we target AUC as the learning objective by relaxing the Heaviside function via sigmoid function to render it differentiable and thus can be optimized directly using gradient-descent methods, which is the de facto choice for solving DNN-based CTR tasks. Furthermore, we incorporate a rank gap sensitive weight in estimating gradients for items, aiming to assign greater significance to item pairs with substantial rank gaps during the learning process. In particular, we reduce the computational complexity from quadratic to linear through reformulation, enabling efficient deployment. Consequently, these designs sharply minimize the number of erroneously-ranked item pairs, which is beneficial to AUC optimization. Notably, RgsAUC is model-agnostic and we implement it in five classic DNN models for the CTR prediction task. Extensive experiments on six real-world datasets clearly demonstrate the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Fangyuan Luo and Yankai Chen and Jun Wu and Yidong Li},
  doi          = {10.1016/j.patcog.2025.111496},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111496},
  shortjournal = {Pattern Recognition},
  title        = {Rank gap sensitive deep AUC maximization for CTR prediction},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extremely fast graph integration for semi-supervised
learning via gaussian fields with neumann approximation. <em>PR</em>,
<em>164</em>, 111495. (<a
href="https://doi.org/10.1016/j.patcog.2025.111495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth in data availability, it has become more important to utilize multiple data sources containing different but complementary information for a given task. Using multiple graphs can technically be interpreted as finding the optimal combination of each graph. There have been various approaches for graph integration or graph fusion, but most of them have suffered from scalability issues as data size increases due to long computation time. This makes them difficult to utilize in the current trend of data size becoming huge. To circumvent this difficulty, our approach introduces a fast graph integration method based on semi-supervised learning (SSL), which incorporates the Neumann approximation during the maximum likelihood estimation process. Empirical studies show that the proposed method significantly reduces computation time by at least a factor of two compared to state-of-the-art methods, while still performing competitively with other methods. This advantage becomes more apparent as the size of the data increases, since the complexity of the proposed method depends mostly on the number of graphs to be integrated and not on the number of nodes, unlike other methods. Experimental results demonstrate the scalability and efficiency of the proposed method for graph integration.},
  archive      = {J_PR},
  author       = {Taehwan Yun and Myung Jun Kim and Hyunjung Shin},
  doi          = {10.1016/j.patcog.2025.111495},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111495},
  shortjournal = {Pattern Recognition},
  title        = {Extremely fast graph integration for semi-supervised learning via gaussian fields with neumann approximation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auxiliary action unit model for facial expression
adversarial training. <em>PR</em>, <em>164</em>, 111493. (<a
href="https://doi.org/10.1016/j.patcog.2025.111493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adversarial training of neural networks against adversarial attacks is increasingly gaining attention due to the demands for artificial intelligence security. However, there have been few studies on adversarial training for facial expression recognition (FER) models. In this work, we propose a novel adversarial training method for FER models. Specifically, we employ an action unit (AU) model to enhance the adversarial robustness of the FER model during the training process. Experimental results demonstrate that our method (i) exhibits greater generalization and robustness than other existing methods for FER models; (ii) incurs feasible computational training costs; and (iii) can converge under extreme circumstances, such as random labels. Our research makes sense as it paves the way for future studies in adversarial training for FER models.},
  archive      = {J_PR},
  author       = {Yudao Sun and Fan Zhang and Minjiao Yang},
  doi          = {10.1016/j.patcog.2025.111493},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111493},
  shortjournal = {Pattern Recognition},
  title        = {Auxiliary action unit model for facial expression adversarial training},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A perturbed match filtering approach for face image quality
assessment. <em>PR</em>, <em>164</em>, 111492. (<a
href="https://doi.org/10.1016/j.patcog.2025.111492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face image quality assessment (FIQA) that estimates the utility of face images is essential for reliable face recognition. However, current state-of-the-art analysis-based FIQA methods suffer from excessively long execution time. Inspired by an observation that comparing to low-quality images, the features of high-quality images more likely maintain stronger robustness after perturbation, we propose a novel training-free FIQA approach, called Perturbed Match Filtering (PMF), in which a quality score function is defined based on the output of a match filter that takes as input the perturbed feature manipulated by selectively dropping a proportion of elements within an internal activation of the network. In addition, our proposed PMF approach can be implemented as a post-processing step for a pre-trained quality regression model to further improve its performance. We conduct extensive experiments on eight benchmark datasets with four target face recognition models. The experimental results demonstrate the superiority of our proposed approach compared to twelve state-of-the-art FIQA algorithms.},
  archive      = {J_PR},
  author       = {Yuying Zhao and Mei Wang and Jiani Hu and Weihong Deng and Chun-Guang Li},
  doi          = {10.1016/j.patcog.2025.111492},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111492},
  shortjournal = {Pattern Recognition},
  title        = {A perturbed match filtering approach for face image quality assessment},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Refining attention weights for facial super-resolution with
counterfactual attention learning. <em>PR</em>, <em>164</em>, 111491.
(<a href="https://doi.org/10.1016/j.patcog.2025.111491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face Super-resolution is a challenging problem involving reconstructing High- Resolution (HR) images from Low-Resolution (LR) inputs with attention mechanisms being a widely used approach. This paper introduces counterfactual attention learning (CAL), a novel framework based on causal inference that enhances attention quality in super-resolution tasks. CAL provides a strong supervisory signal, enabling the refinement of attention mechanisms during training. Through counterfactual interventions, CAL optimizes learned attention to improve super-resolution outcomes. This method is evaluated using the Scale- Arbitrary Super-Resolution model (ArbSR), which accommodates non-integer scale factors. Experiments conducted on CelebA, FFHQ, and CMU Multi-PIE datasets across different scale factors show that CAL significantly enhances super-resolution performance. On the CMU Multi-PIE dataset, CAL improves Peak Signal-to-Noise Ratio (PSNR) by up to 13.6 % compared to baseline attention mechanisms, even under challenging variations in illumination, pose, and expression. PSNR improvement of 15.5 % was observed for CelebA dataset whereas for the FFHQ dataset, 14.5 % improvement was observed under occlusion conditions. These results highlight the robustness and effectiveness of CAL in advancing the state of super-resolution, offering substantial quantitative and qualitative improvements and showcasing its potential for face superresolution in real-world conditions.},
  archive      = {J_PR},
  author       = {Jayanthi Raghavan and Majid Ahmadi},
  doi          = {10.1016/j.patcog.2025.111491},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111491},
  shortjournal = {Pattern Recognition},
  title        = {Refining attention weights for facial super-resolution with counterfactual attention learning},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NRGAN: A noise-resilient GAN with adaptive feature
modulation for SAR image segmentation. <em>PR</em>, <em>164</em>,
111490. (<a href="https://doi.org/10.1016/j.patcog.2025.111490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information extraction of offshore aquaculture rafts from synthetic aperture radar (SAR) images is important for large-scale marine resource exploration and utilization. In this paper, a deep learning model, called Noise-Resilient Generative Adversarial Network (NRGAN), is proposed for SAR image segmentation captured under varying sea conditions to monitor aquaculture rafts. NRGAN consists of an image generator and two regressors. The image generator is used for image segmentation and the regressors for discriminating the generated results and the actual labels. As a key component of the generator, a pixel-level contextual feature adaptation module is designed to improve the performance of the model in dealing with issues such as noise interference and complex image features commonly found in SAR images. The module consists of three parts: one for spatial-feature adaptation to aggregate spatial information from input feature maps and generate a spatial attention map to focus on relevant areas in images, one for contextual-feature adaptation to integrate contextual information for improving feature learning and increasing the expressiveness of input data, and one for pixel-level feature adaptation to refine the contribution of regions within the images, thereby enhancing the coherence of the overall segmentation.},
  archive      = {J_PR},
  author       = {Shuo Lian and Jianchao Fan and Jun Wang},
  doi          = {10.1016/j.patcog.2025.111490},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111490},
  shortjournal = {Pattern Recognition},
  title        = {NRGAN: A noise-resilient GAN with adaptive feature modulation for SAR image segmentation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modality average precision optimization for visible
thermal person re-identification. <em>PR</em>, <em>164</em>, 111489. (<a
href="https://doi.org/10.1016/j.patcog.2025.111489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning has emerged as a popular approach for addressing the challenges of visible thermal person re-identification (VT-ReID), such as the cross-modality discrepancy and intra-class variations. However, existing metric learning-based methods often focus on optimizing the model for hard positive samples, neglecting the importance of high-ranking ones, due to failing to consider the overall ranking order within a batch. To overcome this limitation, we propose a novel approach called Cross-modality Average Precision (CAP) that directly optimizes the cross-modality overall ranking order in VT-ReID. Unlike the recently introduced Smooth Average Precision (Smooth-AP), which primarily corrects misordered samples at high ranks, CAP specifically targets the main challenge of cross-modality discrepancy in VT-ReID. Our method involves setting a query instance from one modality and calculating the CAP using galleries from another modality. CAP encompasses two complementary aspects: CAP with Visible queries (CAPV) and CAP with Thermal queries (CAPT). By jointly optimizing these two aspects, we can effectively improve the cross-modality overall ranking order. Additionally, to enhance the effectiveness of CAP, we introduce two techniques. The first technique is Dynamic Modality Alignment (DMA), which reduces the cross-modality discrepancy by adaptively adjusting the weights of modality alignment. The second technique involves implementing CAP and DMA on the Global and Local Features (GLF), enabling us to optimize the model at both global and local levels, further enhancing the advantages of CAP and DMA. We conducted extensive experiments on two VT-ReID datasets, and the results demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Yongguo Ling and Zhiming Luo and Dazhen Lin and Shaozi Li and Min Jiang and Nicu Sebe and Zhun Zhong},
  doi          = {10.1016/j.patcog.2025.111489},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111489},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modality average precision optimization for visible thermal person re-identification},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view diabetic retinopathy grading via cross-view
spatial alignment and adaptive vessel reinforcing. <em>PR</em>,
<em>164</em>, 111487. (<a
href="https://doi.org/10.1016/j.patcog.2025.111487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our research introduces a novel deep learning framework that leverages multi-view fundus images for Diabetic Retinopathy (DR) grading. Existing models for fundus image analysis often prioritize salient features, such as the optic disk, potentially overlooking finer details critical for DR detection, like retinal vessel information. To address this, we introduce a learnable retinal vessel reinforcement block to enhance the representation of retinal vessels. Additionally, recognizing the limitations of traditional multi-view models in capturing the spatial correlation between 2D appearances from different views, we propose a cross-view spatial region aligning vision transformer (ViT). This ViT-structured model is crucial for modeling cross-view relationships and integrating lesion information across individual views. Furthermore, a multi-view decision fusion module synergistically fuses diagnostic insights from multiple perspectives, enhancing the model’s diagnostic capabilities. Our method demonstrates significant superiority over existing single-view and multi-view models across key performance metrics, including accuracy, precision, sensitivity, specificity, and F1 score.},
  archive      = {J_PR},
  author       = {Yuxin Lin and Xiaoyan Dou and Xiaoling Luo and Zhihao Wu and Chengliang Liu and Tianyi Luo and Jie Wen and Bingo Wing-kuen Ling and Yong Xu and Wei Wang},
  doi          = {10.1016/j.patcog.2025.111487},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111487},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view diabetic retinopathy grading via cross-view spatial alignment and adaptive vessel reinforcing},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLEAR: Cross-transformers with pre-trained language model
for person attribute recognition and retrieval. <em>PR</em>,
<em>164</em>, 111486. (<a
href="https://doi.org/10.1016/j.patcog.2025.111486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person attribute recognition and attribute-based person retrieval are two core human-centric tasks. In the recognition task, the challenge lies in identifying attributes based on a person’s appearance, while the retrieval task involves searching for matching persons using attribute-based queries. In this paper, we present CLEAR , a unified network designed to address both tasks. We leverage our C 2 T-Net, a strong Cross-Transformers backbone that achieved state-of-the-art performance in the person attribute recognition task during the UPAR Challenge 2024, to extract visual embeddings. We then adapt it for the attribute-based person retrieval task.To extend its capabilities for the attribute-based person retrieval task, we construct pseudo-textual descriptions for attribute queries, leverage a pretrained language model to generate language-rich feature embeddings, and introduce an effective training strategy, which involves finetuning only a few additional parameters in the form of adapters to produce visual and query embeddings within the retrieval space. As the visual embeddings extracted by C 2 T-Net are highly discriminative, they align well with the proposed query embeddings during the finetuning process, facilitating improved retrieval performance.The unified CLEAR , model is evaluated on five benchmarks: PETA, PA100K, Market-1501, RAPv2, and UPAR2024, achieving state-of-the-art or competitive results for both tasks. Notably, it ranks as the top performer on the large-scale UPAR2024 dataset, specifically designed to test domain generalizability in real-world scenarios where test samples differ from training samples.},
  archive      = {J_PR},
  author       = {Doanh C. Bui and Thinh V. Le and Ba Hung Ngo and Tae Jong Choi},
  doi          = {10.1016/j.patcog.2025.111486},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111486},
  shortjournal = {Pattern Recognition},
  title        = {CLEAR: Cross-transformers with pre-trained language model for person attribute recognition and retrieval},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced multi-modal learning with hierarchical fusion for
fake news detection. <em>PR</em>, <em>164</em>, 111485. (<a
href="https://doi.org/10.1016/j.patcog.2025.111485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal fake news detection (MFND) leverages data from various modalities, including text, image, video, and audio, to identify the authenticity of news content. Most existing MFND methods focus on extracting feature representations of each modality and integrating them by fusion strategies. However, they ignore the problem of modality imbalance where the dominant modality suppresses the performance of other modalities during optimization process, which leads to insufficient utilization of multi-modal information. To address the issue of modality imbalance and guarantee the effective utilization of each modality, we propose an approach called Balanced Multi-modal Learning with Hierarchical Fusion (BMLHF), which contains a Multi-modal Information Balancing (MIB) module and a Hierarchical Fusion (HF) module. Specifically, we extract multi-view semantic and pattern features of text and image. MIB calculates the modal information firstly to estimate the modal difference ratio, and it dynamically allocates corresponding weight for optimization of each view of modalities, which facilitates the modal information balance state. HF fully explores the diversity and correlation of multi-modal information in two stages. Intra-modal multi-view information fusion stage designs multi-view attention sub-network to sufficiently fuse semantic and pattern features within modalities. Inter-modal correlation fusion stage designs the joint correlation matrix based cross-attention strategy to learn multi-modal fused features with complementary characteristics. Extensive benchmark experiments demonstrate that our approach significantly surpasses state-of-the-art MFND methods.},
  archive      = {J_PR},
  author       = {Fei Wu and Shu Chen and Guangwei Gao and Yimu Ji and Xiao-Yuan Jing},
  doi          = {10.1016/j.patcog.2025.111485},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111485},
  shortjournal = {Pattern Recognition},
  title        = {Balanced multi-modal learning with hierarchical fusion for fake news detection},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAD: Domain generalized diabetic retinopathy grading by
grade-aware de-stylization. <em>PR</em>, <em>164</em>, 111484. (<a
href="https://doi.org/10.1016/j.patcog.2025.111484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is a prevalent complication of diabetes that can result in vision impairment and blindness, making accurate DR grading essential for early diagnosis and treatment. Most existing DR grading methods assume that the training and test images share the same distribution. However, the generalization performance on unseen target domains has not been adequately addressed. In this paper, we observe that images from the same domain tend to cluster together in the feature space, rather than images of the same grade. This is largely due to the fact that when the representation of lesions is influenced by style variations, the network tends to remember features of different image domains through separate channels. This phenomenon significantly impacts the generalization capability of deep learning models. To address this issue, we propose a global-aware channel similarity to reduce the influence of lesion position and size when measuring the distance in the feature space. This is further utilized in a grade-aware contrastive learning approach, which guides the learning of domain-invariant features by mapping images of the same grade into a compact subspace. Additionally, we develop a multi-scale de-stylization method to explicitly eliminate style information from the features, which also compels the model to exploit diverse representations of the lesion. Extensive experiments on multiple DR grading datasets show the state-of-the-art generalization ability of the proposed method.},
  archive      = {J_PR},
  author       = {Qi Bi and Jingjun Yi and Hao Zheng and Haolan Zhan and Yawen Huang and Wei Ji and Yuexiang Li and Yefeng Zheng},
  doi          = {10.1016/j.patcog.2025.111484},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111484},
  shortjournal = {Pattern Recognition},
  title        = {GAD: Domain generalized diabetic retinopathy grading by grade-aware de-stylization},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised feature selection via maximum relevance and
minimum global redundancy. <em>PR</em>, <em>164</em>, 111483. (<a
href="https://doi.org/10.1016/j.patcog.2025.111483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection has been an important research topic in various fields since real datasets often lack complete label information. Many existing unsupervised feature selection algorithms select features based on feature relevance only without considering the redundancy between features, which may result in the selection of suboptimal features. To obtain the optimal feature subset, we propose a new unsupervised feature selection algorithm, called unsupervised Feature Selection with Max-Relevance and Minimum Global Redundancy (MRMGRFS) to select a subset of features with max-relevance and minimum global redundancy. In terms of relevance, we propose an unsupervised Feature Selection algorithm based on Spectral Clustering (SCFS), which divides all features into different clusters using spectral clustering and evaluates the relevance of features by measuring the distance between the features and the mean centers of own-cluster and heterogeneous-clusters. In terms of redundancy, the SCFS algorithm only considers the relevance of features and ignores the redundancy between features, which may select the redundant features that degrade performance. To tackle this issue, a Global Redundancy Minimization model (SJGRM) based on the SCFS and Jensen–Shannon divergence (JSD) is proposed to optimize the relevance score of the features. Furthermore, we propose an effective iterative algorithm for solving SJGRM based on the Alternating Direction Method of Multipliers (ADMM). Extensive experimental results on various public datasets demonstrate the superiority of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Xianyu Zuo and Wenbo Zhang and Xiangyu Wang and Lanxue Dang and Baojun Qiao and Yadi Wang},
  doi          = {10.1016/j.patcog.2025.111483},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111483},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised feature selection via maximum relevance and minimum global redundancy},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On inferring prototypes for multi-label few-shot learning
via partial aggregation. <em>PR</em>, <em>164</em>, 111482. (<a
href="https://doi.org/10.1016/j.patcog.2025.111482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label few-shot learning (ML-FSL) aims to endow the learning system to recognize multiple objects within an image, trained with insufficient samples. Existing methods have significantly improved ML-FSL and focused on mining the correlation of labels, resulting in a discriminative prototype per class. However, those methods often engage irrelevant information, i.e. , the tangled region with other classes, in the phase of constructing prototypes, limiting their performance gain. Following the intuition that only part regions of an image correspond to a target label, this paper addresses this issue by creating prototypes via a partial aggregation scheme. This is realized by first generating aggregation weights via partial optimal transport (POT) between image and label features and producing features per class using relevant regions within an image. Having the refined class features in a support set, one can obtain a better prototype for each class. We evaluate our model on multiple benchmarks and obtain state-of-the-art performance. A thorough study also reveals the superiority of POT as a way of mining important information for generating prototypes.},
  archive      = {J_PR},
  author       = {Pengfei Fang and Zhihong Chen and Hui Xue},
  doi          = {10.1016/j.patcog.2025.111482},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111482},
  shortjournal = {Pattern Recognition},
  title        = {On inferring prototypes for multi-label few-shot learning via partial aggregation},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALStereo: Active learning for stereo matching. <em>PR</em>,
<em>164</em>, 111480. (<a
href="https://doi.org/10.1016/j.patcog.2025.111480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in deep stereo matching, recent networks have achieved impressive accuracy in estimating depth information from image pairs. However, stereo matching networks require sufficient disparity labels, which always come at high annotation costs. In this paper, we propose the ALStereo framework for training stereo matching networks under limited labeling budgets, which selects informative samples for manual labeling and conducts semi-supervised learning to propagate the knowledge to unlabeled samples. Specifically, we embed image pairs as nodes in a graph representation, where edges denote the similarity in terms of stereo matching challenges. Based on the graph representation, we divide the labeling budget into two parts for conducting representativeness-based and uncertainty-based strategies, balancing the selection of the most representative and challenging samples. To fully exploit the labeled samples to train networks, we propose a two-stage semi-supervised training pipeline, where the first stage mitigates the domain shifts and the second stage propagates the knowledge of manually annotated samples to unlabeled samples. We set the first benchmark for evaluating training stereo matching networks under limited labeling budgets and demonstrate our method significantly outperforms the compared methods. We also provide analysis to demonstrate our graph representation effectively models the similarity between samples in terms of stereo matching challenges.},
  archive      = {J_PR},
  author       = {Jiawei Zhang and Jiahe Li and Meiying Gu and Xiaohan Yu and Jin Zheng and Xiao Bai and Edwin Hancock},
  doi          = {10.1016/j.patcog.2025.111480},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111480},
  shortjournal = {Pattern Recognition},
  title        = {ALStereo: Active learning for stereo matching},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Token-aware and step-aware acceleration for stable
diffusion. <em>PR</em>, <em>164</em>, 111479. (<a
href="https://doi.org/10.1016/j.patcog.2025.111479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stable Diffusion has shown strong ability to generate high-quality and diverse images. However, Stable Diffusion suffers from high computational cost, due to the heavy model and step-by-step denoising process. To address these issues, we propose a token-aware and step-aware acceleration approach for Stable Diffusion, named TSA-SD. We first build a simple and efficient baseline by combining exiting intra-step and cross-step acceleration strategies, including token merging and feature caching, into Stable Diffusion. To improve image generation quality of the baseline, we introduce token-aware merging–unmerging and step-aware acceleration. The token-aware merging–unmerging aims to select informative tokens when merging and recover merged tokens using token ratio information. Therefore, the token-aware merging–unmerging can fully utilize token-specific information, thereby reducing token information loss. In addition, we observe that different steps have different functional linearity, and propose step-aware acceleration to perform different merging operations according to functional linearity at different steps. With these two modules, our proposed TSA-SD is able to generate high-quality images at a high speed. We perform the experiments on two widely-used datasets, including ImageNet and MS-COCO. The experimental results demonstrate the effectiveness and efficiency of our proposed method. For instance, on ImageNet validation set, compared to Stable Diffusion, ToMe-SD has a lower FID of 33.68 at 1.96 × speedup, while our method achieves a lower FID of 32.49 at 4.68 × speedup.},
  archive      = {J_PR},
  author       = {Ting Zhen and Jiale Cao and Xuebin Sun and Jing Pan and Zhong Ji and Yanwei Pang},
  doi          = {10.1016/j.patcog.2025.111479},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111479},
  shortjournal = {Pattern Recognition},
  title        = {Token-aware and step-aware acceleration for stable diffusion},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optical remote sensing image salient object detection via
bidirectional cross-attention and attention restoration. <em>PR</em>,
<em>164</em>, 111478. (<a
href="https://doi.org/10.1016/j.patcog.2025.111478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) in optical remote sensing images (ORSI) has garnered considerable attention in recent years. The inherent high complexity of scenes in ORSI poses significant challenges to SOD methods. Current models face two major limitations. First, they fail to fully exploit the relationships between features due to ignoring the bidirectional attention relationship and continuity among adjacent feature layers. Second, while non-local modules are designed to enhance global context understanding by modeling pixel-wise relationships, their traditional implementations suffer from attention vacuity, as they treat all spatial locations equally without focusing on the most informative regions. To overcome these limitations, we introduce a novel Bidirectional Cross-Attention and Attention Restoration Neural Network (BCAR-Net), comprising the Bidirectional Cross-Attention Module (BCAM) and the Attention Restoration Module (ARM). BCAM enhances the semantic representation of detail features in lower-level maps and improves the detail representation of semantic features in higher-level maps. This is achieved by computing cross-attention between two adjacent layers in a parallel bidirectional manner, playing a crucial role in spatial information representation. Additionally, ARM addresses attention vacuity through the Foreground-Background Decoupling (FBD) and Local Attention Vacuity Supplementation (LAVS) components. Specifically, FBD refines the segmentation of salient objects from their backgrounds, while LAVS remedies local object detection omissions. Experimental results demonstrate that our proposed model performs favorably and outperforms existing methods overall. Specifically, on the ORSSD and EORSSD benchmark datasets, our method outperforms the SOTA approaches by 10% and 5% in terms of MAE, respectively. The source codes and the outcomes can be accessed at https://github.com/ClimBin/BCARNet .},
  archive      = {J_PR},
  author       = {Yubin Gu and Siting Chen and Xiaoshuai Sun and Jiayi Ji and Yiyi Zhou and Rongrong Ji},
  doi          = {10.1016/j.patcog.2025.111478},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111478},
  shortjournal = {Pattern Recognition},
  title        = {Optical remote sensing image salient object detection via bidirectional cross-attention and attention restoration},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning pathways for automatic sign language
processing. <em>PR</em>, <em>164</em>, 111475. (<a
href="https://doi.org/10.1016/j.patcog.2025.111475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study provides a comprehensive review of the current state of the sign language processing (SLP) field, encompassing sign language recognition (SLR), translation (SLT), production (SLPn), and the associated datasets (SLD). It analyzes the advancements and challenges in each area, highlighting key methodologies and technologies. The authors explore feature extraction techniques, model architectures, and multimodal data integration in SLR. For SLT, they examine neural machine translation and sequence-to-sequence frameworks, emphasizing the need for context-aware systems. In SLPn, they review avatar-based systems and motion capture techniques, identifying gaps in generating natural and expressive sign language. The survey of SLD evaluates existing datasets and underscores the importance of comprehensive data collection. It also discusses current SLP systems’ limitations and proposes future research directions to enhance accuracy, naturalness, and user-centric applications.},
  archive      = {J_PR},
  author       = {Mukhiddin Toshpulatov and Wookey Lee and Jaesung Jun and Suan Lee},
  doi          = {10.1016/j.patcog.2025.111475},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111475},
  shortjournal = {Pattern Recognition},
  title        = {Deep learning pathways for automatic sign language processing},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inversed pyramid network with spatial-adapted and
task-oriented tuning for few-shot learning. <em>PR</em>, <em>164</em>,
111415. (<a href="https://doi.org/10.1016/j.patcog.2025.111415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of artificial intelligence, deep neural networks have achieved great performance in many tasks. However, traditional deep learning methods require a large amount of training data, which may not be available in certain practical scenarios. In contrast, few-shot learning aims to learn a model that can be readily adapted to new unseen classes from only one or a few labeled examples. Despite this success, most existing methods rely on pre-trained feature extractor networks trained with global features, ignoring the discrimination of local features, and weak generalization capabilities limit their performance. To address the problem, according to the human’s coarse-to-fine cognition paradigm, we propose an Inverted Pyramid Network with Spatial-adapted and Task-oriented Tuning (TIPN) for few-shot learning. Specifically, the proposed framework represents local features for categories that are difficult to distinguish by global features and recognizes objects from both global and local perspectives. Moreover, to ensure the calibration validity of the proposed model at the local stage, we introduce the Spatial-adapted Layer to preserve the discriminative global representation ability of the pre-trained backbone network. Meanwhile, as the representations extracted from the past categories are not applicable to the current new tasks, we further propose the Task-oriented Tuning strategy to adjust the parameters of the Batch Normalization layer in the pre-trained feature extractor network, to explicitly transfer knowledge from base classes to novel classes according to the support samples of each task. Extensive experiments conducted on multiple benchmark datasets demonstrate that our method can significantly outperform many state-of-the-art few-shot learning methods.},
  archive      = {J_PR},
  author       = {Xiaowei Zhao and Duorui Wang and Shihao Bai and Shuo Wang and Yajun Gao and Yu Liang and Yuqing Ma and Xianglong Liu},
  doi          = {10.1016/j.patcog.2025.111415},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {111415},
  shortjournal = {Pattern Recognition},
  title        = {Inversed pyramid network with spatial-adapted and task-oriented tuning for few-shot learning},
  volume       = {164},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
