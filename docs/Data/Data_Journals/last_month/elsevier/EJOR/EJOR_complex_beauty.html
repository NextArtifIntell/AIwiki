<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EJOR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ejor---46">EJOR - 46</h2>
<ul>
<li><details>
<summary>
(2025). Diversification for infinite-mean pareto models without risk
aversion. <em>EJOR</em>, <em>323</em>(1), 341–350. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study stochastic dominance between portfolios of independent and identically distributed (iid) extremely heavy-tailed (i.e., infinite-mean) Pareto random variables. With the notion of majorization order, we show that a more diversified portfolio of iid extremely heavy-tailed Pareto random variables is larger in the sense of first-order stochastic dominance. This result is further generalized for Pareto random variables caused by triggering events, random variables with tails being Pareto, bounded Pareto random variables, and positively dependent Pareto random variables. These results provide an important implication in investment: Diversification of extremely heavy-tailed Pareto profits uniformly increases investors’ profitability, leading to a diversification benefit. Remarkably, different from the finite-mean setting, such a diversification benefit does not depend on the decision maker’s risk aversion.},
  archive      = {J_EJOR},
  author       = {Yuyu Chen and Taizhong Hu and Ruodu Wang and Zhenfeng Zou},
  doi          = {10.1016/j.ejor.2025.01.039},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {341-350},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Diversification for infinite-mean pareto models without risk aversion},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sustainable optimal stock portfolios: What relationship
between sustainability and performance? <em>EJOR</em>, <em>323</em>(1),
323–340. (<a href="https://doi.org/10.1016/j.ejor.2025.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to compare different strategies to combine sustainability and optimality in stock portfolios to assess whether there is an association between their average ESG (Environmental, Social, Governance) score and their financial performance and, if so, whether it depends on the specific strategy used. To this end, we confront the risk-adjusted performance of three ESG-compliant optimal portfolios resulting from: (i) optimizing on an ESG-screened sample, (ii) including a portfolio ESG-score constraint in the optimization on an unscreened sample, (iii) our original proposal of optimizing with an ESG-score constraint (so as to reach a target) over a slightly screened sample (so as to exclude companies with lowest sustainability). The optimization is implemented with Bloomberg ESG scores over a sample from the EURO STOXX Index in the period January 2007–August 2022 by minimizing portfolio residual risk. Two are the main conclusions from our results. First, we never find a significant negative association between portfolios’ average ESG score and performance independently of the strategy used. Second, we find a positive association when the first and the third strategy are implemented with a high screening level. To be noted that the relationship between the ESG score and the risk-return ratio in the initial investment set plays a relevant role. If, as in our dataset, this relationship is essentially convex, with an appropriate level of screening portfolios are composed only by stocks whereby a higher ESG score is associated with a higher risk-return profile.},
  archive      = {J_EJOR},
  author       = {Beatrice Bertelli and Costanza Torricelli},
  doi          = {10.1016/j.ejor.2025.01.021},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {323-340},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Sustainable optimal stock portfolios: What relationship between sustainability and performance?},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive distributions and the market return: The role of
market illiquidity. <em>EJOR</em>, <em>323</em>(1), 309–322. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper evaluates the role of volatility-free stock market illiquidity proxies in forecasting monthly stock market returns. We adopt a probabilistic approach to multivariate time-series modelling using Bayesian nonparametric vector autoregressions. These models flexibly capture complex joint dynamics among financial variables through data-driven regime switching. Out-of-sample forecasts maintain accuracy as the horizon increases. Adding illiquidity generates statistical improvements in out-of-sample predictive accuracy. We highlight the operational importance of market illiquidity after selecting the most appropriate forecasting model that delivers profitable strategies that outperform a range of multivariate models; as well as the historical mean.},
  archive      = {J_EJOR},
  author       = {Michael Ellington and Maria Kalli},
  doi          = {10.1016/j.ejor.2025.01.006},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {309-322},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Predictive distributions and the market return: The role of market illiquidity},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning approach for solution space reduction in
aircraft disruption recovery. <em>EJOR</em>, <em>323</em>(1), 297–308.
(<a href="https://doi.org/10.1016/j.ejor.2024.11.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aircraft recovery, a critical step in airline operations recovery, aims to minimize the cost of disrupted aircraft schedules. The exact methods for aircraft recovery are computationally expensive and operationally infeasible in practice. Heuristics and hybrid approaches offer faster solutions but have inconsistent solution quality, often leading to large losses. We propose a supervised machine learning approach to accelerate aircraft recovery by pruning the solution space of the optimization problem. It leverages similarities with previously solved problem instances through an offline model-training phase, identifies components of the optimal solutions for new problem instances in the online phase, and links them to the optimization model to rapidly generate high-quality solutions. Computational results, from multiple historical disruption instances for a large US airline, demonstrate that this approach significantly outperforms exact methods on computational runtime while producing similarly high-quality solutions. It also outperforms existing heuristics due to its ability to prune solution spaces in a more principled manner, leading to higher quality solutions in similarly short runtimes. For a runtime budget of two minutes, our approach provides a solution within 1.5% of the true optimal cost, resulting in an average daily saving of over $390,000 compared to all existing approaches. The main drivers of these improvements are explainable in terms of key airline operational metrics and are validated through extensive sensitivity and robustness tests.},
  archive      = {J_EJOR},
  author       = {Navid Rashedi and Nolan Sankey and Vikrant Vaze and Keji Wei},
  doi          = {10.1016/j.ejor.2024.11.025},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {297-308},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A machine learning approach for solution space reduction in aircraft disruption recovery},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A rolling horizon heuristic approach for a multi-stage
stochastic waste collection problem. <em>EJOR</em>, <em>323</em>(1),
276–296. (<a href="https://doi.org/10.1016/j.ejor.2024.11.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a multi-stage stochastic optimization model to solve an inventory routing problem for the collection of recyclable municipal waste. The objective is the maximization of the total expected profit of the waste collection company. The decisions are related to the selection of the bins to be visited and the corresponding routing plan in a predefined time horizon. Stochasticity in waste accumulation is modeled through scenario trees generated via conditional density estimation and dynamic stochastic approximation techniques. The proposed formulation is solved through a rolling horizon approach, providing a rigorous worst-case analysis on its performance. Extensive computational experiments are carried out on small- and large-sized instances based on real data provided by a large Portuguese waste collection company. The impact of stochasticity on waste generation is examined through stochastic measures, showing the importance of adopting a stochastic model over a deterministic formulation when addressing a waste collection problem. The performance of the rolling horizon approach is evaluated, demonstrating that this heuristic provides cost-effective solutions in short computational time. Managerial insights related to different geographical configurations of the instances and varying levels of uncertainty are finally discussed.},
  archive      = {J_EJOR},
  author       = {Andrea Spinelli and Francesca Maggioni and Tânia Rodrigues Pereira Ramos and Ana Paula Barbosa-Póvoa and Daniele Vigo},
  doi          = {10.1016/j.ejor.2024.11.041},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {276-296},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A rolling horizon heuristic approach for a multi-stage stochastic waste collection problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed solution of the day-ahead pump and valve
scheduling problem for dynamically adaptive water distribution networks
with storage. <em>EJOR</em>, <em>323</em>(1), 267–275. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the computation of daily schedules of pumps and boundary valves for the minimization of energy costs in water distribution networks (WDN) with dynamically adaptive configurations. The considered problem combines integer (“on”/“off”) pump control variables, non-convex energy conservation constraints and time-coupling mass conservation constraints. For operational WDNs, the resulting non-convex mixed-integer non-linear program (MINLP) is too large to be solved using available methods. We propose a tailored heuristic solution method based on the Alternating Direction Method of Multipliers which distributes and coordinates the solution of smaller problems corresponding to individual time steps of the original MINLP. The proposed method is applied to a large-scale WDN from the UK. The daily schedule of pumps and boundary valves obtained for the dynamically adaptive network configuration, computed in 12 min, is shown to be at most 6% suboptimal and nearly 5% cheaper than the globally optimal schedule corresponding to the traditional (sectorized) network configuration. The proposed algorithm outperforms alternative off-the-shelf and tailored approaches, providing a scalable method to compute good solutions to the complex day-ahead pump and valve scheduling problem in operational dynamically adaptive WDNs.},
  archive      = {J_EJOR},
  author       = {Aly-Joy Ulusoy and Ivan Stoianov},
  doi          = {10.1016/j.ejor.2024.11.035},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {267-275},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Distributed solution of the day-ahead pump and valve scheduling problem for dynamically adaptive water distribution networks with storage},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adoption model of cryptocurrencies. <em>EJOR</em>,
<em>323</em>(1), 253–266. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network effect, measured by users’ adoption, is considered an important driver of cryptocurrency market dynamics. This study examines the role of adoption timing in cryptocurrency markets by decomposing total adoption into two components: innovators (early adopters) and imitators (late adopters). We find that the innovators’ component is the primary driver of the association between user adoption and cryptocurrency returns, both in-sample and out-of-sample. Next, we show that innovators’ adoption improves price efficiency, while imitators’ adoption contributes to noisier prices. Furthermore, we demonstrate that the adoption model captures significant cryptocurrency market phenomena, such as herding behaviour, more effectively, making it better suited for forecasting models in cryptocurrency pricing. These results suggest that our methodology for linking early and late adopters to market dynamics can be applied to various domains, offering a framework for future research at the intersection of operational research and financial markets.},
  archive      = {J_EJOR},
  author       = {Khaladdin Rzayev and Athanasios Sakkas and Andrew Urquhart},
  doi          = {10.1016/j.ejor.2024.11.024},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {253-266},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An adoption model of cryptocurrencies},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal co-development contracts for companion diagnostics.
<em>EJOR</em>, <em>323</em>(1), 241–252. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The market for companion diagnostics is expected to be a US$10.07 billion by 2026. Companion diagnostics have the potential to make expensive drugs cost-effective by identifying patients who would benefit from them. We consider the contract design problem between a pharmaceutical company which owns a drug that is effective for a particular subset of the patient population and a biotech company which owns some technology that could facilitate the development of a companion diagnostic. We obtain theoretical and practical results. We determine when both parties enter such a contract and fully characterize the optimal solutions in closed-form. We find sufficient conditions under which the optimal contract exhibits a particular structure. We show that the first-best can be achieved in some cases and identify sufficient conditions under which the biotech company would not work alone but participates in the project with the pharmaceutical company’s subsidy. We find that heuristics based on practical preferences could be costly to the pharmaceutical company and hence the principal should use the second-best solution; and contract type depends heavily on the biotech company’s workforce level, unit cost of workforce and information level.},
  archive      = {J_EJOR},
  author       = {Sakine Batun and Mehmet A. Begen and Gregory S. Zaric},
  doi          = {10.1016/j.ejor.2024.11.031},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {241-252},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal co-development contracts for companion diagnostics},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating non-overfitted convex production technologies: A
stochastic machine learning approach. <em>EJOR</em>, <em>323</em>(1),
224–240. (<a href="https://doi.org/10.1016/j.ejor.2024.11.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overfitting is a classical statistical issue that occurs when a model fits a particular observed data sample too closely, potentially limiting its generalizability. While Data Envelopment Analysis (DEA) is a powerful non-parametric method for assessing the relative efficiency of decision-making units (DMUs), its reliance on the minimal extrapolation principle can lead to concerns about overfitting, particularly when the goal extends beyond evaluating the specific DMUs in the sample to making broader inferences. In this paper, we propose an adaptation of Stochastic Gradient Boosting to estimate production possibility sets that mitigate overfitting while satisfying shape constraints such as convexity and free disposability. Our approach is not intended to replace DEA but to complement it, offering an additional tool for scenarios where generalization is important. Through simulation experiments, we demonstrate that the proposed method performs well compared to DEA, especially in high-dimensional settings. Furthermore, the new machine learning-based technique is compared to the Corrected Concave Non-parametric Least Squares (C 2 NLS), showing competitive performance. We also illustrate how the usual efficiency measures in DEA can be implemented under our approach. Finally, we provide an empirical example based on data from the Program for International Student Assessment (PISA) to demonstrate the applicability of the new method.},
  archive      = {J_EJOR},
  author       = {Maria D. Guillen and Vincent Charles and Juan Aparicio},
  doi          = {10.1016/j.ejor.2024.11.030},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {224-240},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Estimating non-overfitted convex production technologies: A stochastic machine learning approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond leagues: A single incomplete round robin tournament
for multi-league sports timetabling. <em>EJOR</em>, <em>323</em>(1),
208–223. (<a href="https://doi.org/10.1016/j.ejor.2024.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most sports associations regularly face the problem of determining and scheduling games for dozens if not hundreds of non-professional (youth) teams. For practical reasons and player convenience, it is key that the schedule respects venue capacities and minimizes travel distance. A classic approach is to split up teams over leagues, and then have each league play a round robin tournament. In a round robin tournament, each team competes against every other team in the tournament an equal number of times. This paper proposes an alternative approach, organizing a single yet incomplete round robin tournament involving all teams. In this format, which can be seen as a static Swiss system tournament, each team plays the same number of games, but teams are not required to face the same opponents. We exploit this flexibility to reduce the total travel distance and venue capacity conflicts. We provide theoretical results on the computational complexity of finding an incomplete round robin tournament, as well as sufficient conditions on its existence. Besides a Benders’ decomposition for the classic round robin approach, we develop a relax-and-fix and an iterative two-phase decomposition metaheuristic for the incomplete round robin approach. The metaheuristic first determines the home-away status of teams based on their club’s venue capacity, and thereafter selects suitable opponents while minimizing travel distances. Extensive experiments using real-life benchmark instances from the literature confirm the advantage of an incomplete round robin tournament compared to the classic multi-league round robin approach and validate the effectiveness of the proposed heuristics.},
  archive      = {J_EJOR},
  author       = {Miao Li and David Van Bulck and Dries Goossens},
  doi          = {10.1016/j.ejor.2024.11.007},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {208-223},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Beyond leagues: A single incomplete round robin tournament for multi-league sports timetabling},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consensus modeling for maximum expert with quadratic cost
under various uncertain contexts: A data-driven robust approach.
<em>EJOR</em>, <em>323</em>(1), 192–207. (<a
href="https://doi.org/10.1016/j.ejor.2024.10.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus optimization models are valuable tools for addressing negotiated group decision-making challenges, particularly those involving critical decision-related data such as costs and preferences. However, the idealized approach to information fusion in consensus decision-making presents challenges in adapting to practical conditions, leading to less credible consensus solutions. To simulate a more realistic decision-making scenario, this study integrates unit adjustment costs in a quadratic form into a maximum expert consensus model. This quadratic cost formulation captures the complex resistance to cost changes encountered by experts when adjusting solutions, promoting a deliberate approach to solution updates and facilitating improved decision-making. Moreover, economic insights elucidate the effect of quadratic costs on decision-making behavior. Additionally, the feasibility of reaching a consensus may be impeded by high uncertainty in real-world decision-making scenarios. This study separately tackles decision environments characterized by unit adjustment costs and individual preference uncertainty. It employs a robust optimization approach to incorporate uncertain costs and preferences into the optimization model. Data-driven robust maximum expert consensus models are then developed to objectively manage available historical data. An enhanced genetic algorithm is introduced as a solution method to address the proposed models. The proposed models are ultimately applied to evaluate policy options for the development of new energy vehicles in Changsha. Comparative and sensitivity analyses are conducted, showing the superior performance of the proposed models.},
  archive      = {J_EJOR},
  author       = {Jinpeng Wei and Xuanhua Xu and Shaojian Qu and Qiuhan Wang},
  doi          = {10.1016/j.ejor.2024.10.034},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {192-207},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Consensus modeling for maximum expert with quadratic cost under various uncertain contexts: A data-driven robust approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-attribute utility preference robust optimization: A
continuous piecewise linear approximation approach. <em>EJOR</em>,
<em>323</em>(1), 170–191. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a bi-attribute decision making problem where the decision maker’s (DM’s) objective is to maximize the expected utility of outcomes with two attributes but where the true utility function which captures the DM’s risk preference is ambiguous. To tackle this ambiguity, we propose a maximin bi-attribute utility preference robust optimization (BUPRO) model where the optimal decision is based on the worst-case utility function in an ambiguity set of plausible utility functions constructed using partially available information such as the DM’s specific preference for certain lotteries. Specifically, we consider a BUPRO model with two attributes, where the DM’s risk attitude is bivariate risk-averse and the ambiguity set is defined by a linear system of inequalities represented by the Lebesgue–Stieltjes integrals of the DM’s utility functions. To solve the inner infinite-dimensional minimization problem, we propose a continuous piecewise linear approximation approach to approximate the DM’s unknown true utility. Unlike the univariate case, we partition the domain of the utility function into a set of small non-overlapping rectangles and then divide each rectangle into two triangles by either the main diagonal (Type-1) or the counter diagonal (Type-2). The inner minimization problem based on the piecewise linear utility function can be reformulated as a mixed-integer linear program and the outer maximization problem can be solved efficiently by the derivative-free method. In the case that all the small triangles are partitioned either in Type-1 or in Type-2, the inner minimization can be formulated as a finite dimensional linear program and the overall maximin as a single mixed-integer program. To quantify the approximation errors, we derive, under some mild conditions, the error bound for the difference between the BUPRO model and the approximate BUPRO model in terms of the ambiguity set, the optimal value and the optimal solutions. Finally, we carry out some numerical tests to examine the performance of the proposed models and computational schemes. The results demonstrate the efficiency of the computational schemes and highlight the stability of the BUPRO model against data perturbations.},
  archive      = {J_EJOR},
  author       = {Qiong Wu and Wei Wang and Sainan Zhang and Huifu Xu},
  doi          = {10.1016/j.ejor.2024.11.001},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {170-191},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Bi-attribute utility preference robust optimization: A continuous piecewise linear approximation approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tactical workforce sizing and scheduling decisions for
last-mile delivery. <em>EJOR</em>, <em>323</em>(1), 153–169. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problems of workforce sizing and shift scheduling of a logistic operator delivering parcels in the last-mile segment of the supply chain. Our working hypothesis is that the relevant decisions are affected by two main trade-offs: workforce size and shift stability. A large workforce can deal with demand fluctuations but incurs higher fixed costs; by contrast, a small workforce might require excessive outsourcing to third-party logistic providers. Stable shifts, i.e., with predictable start times and lengths, improve worker satisfaction and reduce turnover; at the same time, they might be less able to adapt to an unsteady demand. We test these assumptions through an extensive computational campaign based on a novel mathematical formulation. We find that extreme shift stability is, indeed, unsuitable for last-mile operations. At the same time, introducing a very limited amount of flexibility achieves similar effects as moving to a completely flexible system while ensuring a better work-life balance for the workers. Several recent studies in the social sciences have warned about the consequences of precarious working conditions for couriers and retail workers and have recommended — among other things — stable work schedules. Our work shows that it is possible to offer better working conditions in terms of shift stability without sacrificing the company’s bottom line. Thus, companies prioritising profitability (as is often the case) can improve workers’ well-being and increase retention with a negligible cost impact.},
  archive      = {J_EJOR},
  author       = {Minakshi Punam Mandal and Alberto Santini and Claudia Archetti},
  doi          = {10.1016/j.ejor.2024.12.006},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {153-169},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Tactical workforce sizing and scheduling decisions for last-mile delivery},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic scheduling and routing decisions in online meal
delivery platforms with mixed force. <em>EJOR</em>, <em>323</em>(1),
139–152. (<a href="https://doi.org/10.1016/j.ejor.2024.11.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates stochastic scheduling and routing problems in the online meal delivery (OMD) service. The huge increase in meal delivery demand requires the service providers to construct a highly efficient logistics network to deal with a large-volume of time-sensitive and fluctuating fulfillment, often using inhouse and crowdsourced drivers to secure the ambitious service quality. We aim to address the problem of developing an effective scheduling and routing policy that can handle real-life situations. To this end, we first model the dynamic problem as a Markov Decision Process (MDP) and analyze the structural properties of the optimal policy. Then we propose four integrated approaches to solve the operational level scheduling and routing problem. In addition, we provide a continuous approximation formula to estimate the bounds of required fleet size for the inhouse drivers. Numerical experiments based on a real dataset show the effectiveness of the proposed solution approaches. We also obtain several managerial insights that can help decision makers in solving similar resource allocation problems in real-time.},
  archive      = {J_EJOR},
  author       = {Yanlu Zhao and Laurent Alfandari and Claudia Archetti},
  doi          = {10.1016/j.ejor.2024.11.028},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {139-152},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stochastic scheduling and routing decisions in online meal delivery platforms with mixed force},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information sharing across competing platforms with varying
information capabilities. <em>EJOR</em>, <em>323</em>(1), 125–138. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competing online retail platforms frequently function as both agency and reselling channels. This paper explores a manufacturer’s channel selection strategy in the context of downstream platform competition and information sharing, taking into account the platforms’ varying levels of information capability. Our research indicates that the manufacturer opts for a hybrid channel approach. Competing platforms aim to be selected as agency channels by offering information sharing and reduced commission fees. Interestingly, the manufacturer chooses the platform with lesser information capability as her agency channel to gain access to shared demand data, while opting for the platform with greater capability as reselling channel without accessing his demand data. The platform with inferior information capability is more inclined to establish a revenue-sharing partnership with the manufacturer to mitigate risks, leading him to decrease his commission rate to attract the manufacturer to select him as the agency channel. We demonstrate that, under conditions of demand uncertainty, a significant distinction between agency and reselling channels lies in the distribution of risk, i.e., whether the platform assumes the risk alone or shares it with the manufacturer. Furthermore, we highlight the free-ride effect , wherein an agency platform can benefit from his rival’s superior information capability. As a result, a complex relationship, characterized by both cooperation and rivalry, may develop between the two platforms.},
  archive      = {J_EJOR},
  author       = {Haoruo Zhu and Yaodong Ni and Yongbo Xiao},
  doi          = {10.1016/j.ejor.2024.11.048},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {125-138},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Information sharing across competing platforms with varying information capabilities},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing bus bridging services with mode choice in
response to urban rail transit emergencies. <em>EJOR</em>,
<em>323</em>(1), 108–124. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During urban rail transit (URT) emergencies, stranded passengers may choose to seek alternative modes of transportation instead of waiting in the URT system for the bus bridging service to commence. To tackle this challenge, we present an optimization-based approach focused on identifying promising bus bridging lines and devising efficient services. Specifically, we introduce a candidate line generation (CLG) model designed to identify potential bus bridging lines. This model is akin to solving a k-all pair elementary shortest path problem with resource constraints ( k -APESPPRC). We develop an exact algorithm based on the label setting algorithm and Lawler&#39;s algorithm to solve this model effectively. Subsequently, our approach allocates limited bus resources to determine line selection, frequency determination, bus deployment, and passenger assignment on the integrated network (i.e., partial URT network and bus network) with the consideration of mode choice. Given the inherent complexity of this problem, we introduce an optimization-based tabu search method ( opt -tabu) designed to efficiently solve real-size instances. To demonstrate the effectiveness of our approach, we present a real case study conducted in Hong Kong, showcasing its efficiency and practicality. In summary, this study makes a valuable contribution to the transportation industry by providing a practical and efficient approach to managing URT emergencies, emphasizing the significance of considering mode choice in the context of bus bridging services.},
  archive      = {J_EJOR},
  author       = {Yun Wang and Yu Zhou and Hai Yang and Bin Yu and Xiaobing Liu},
  doi          = {10.1016/j.ejor.2024.11.042},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {108-124},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing bus bridging services with mode choice in response to urban rail transit emergencies},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The location routing problem with time windows and
load-dependent travel times for cargo bikes. <em>EJOR</em>,
<em>323</em>(1), 97–107. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Last-mile delivery with traditional delivery trucks is ecologically unfriendly and leads to high road utilization. Thus, cities seek for different delivery options to solve these problems. One promising option is the use of cargo bikes in last-mile delivery. These bikes are typically released at micro hubs, which are small containers or facilities located at advantageous places in the city center. Since the bike’s travel speed depends on its remaining load and the street gradient, placing the hubs at valleys might cause additional work for rides. Therefore, the following question arises: How high is the impact of load-dependent travel times on micro hubs’ cost-optimal placements? To answer this question, we introduce the location routing problem with time windows and load-dependent travel times. We formulate the problem as a mixed-integer linear program and introduce an adaptive large neighborhood search with a problem-specific procedure for micro hub placements and problem-specific operators to solve larger instances. In a numerical study, we find that load-dependent travel times significantly influence the location of hubs, following that hubs with a higher elevation are preferably used. Moreover, customers are served from hubs with a similar elevation. This would not be the case if load-dependent travel times are ignored, resulting in an increase in costs by up to 2.7 % or, instead, to up to 26 % infeasible solutions as time windows are not adhered to.},
  archive      = {J_EJOR},
  author       = {Alexander Rave and Pirmin Fontaine},
  doi          = {10.1016/j.ejor.2024.11.040},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {97-107},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The location routing problem with time windows and load-dependent travel times for cargo bikes},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven ordering policies for target oriented newsvendor
with censored demand. <em>EJOR</em>, <em>323</em>(1), 86–96. (<a
href="https://doi.org/10.1016/j.ejor.2024.10.045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s fiercely competitive business environment, meeting and surpassing earnings expectations is paramount for public companies. This study focuses on how companies selling newsvendor-type products determine the order quantity to maximize the probability of achieving a target profit (known as profitability). Decision-makers often face challenges in real-life situations where the true demand distributions are unknown, and they have to rely on historical demand data. In some cases, they may only have access to sales data, which is referred to as censored demand. We propose data-driven ordering policies that aim to maximize profitability based solely on historical demand data and sales data respectively. Specifically, we first develop a data-driven nonparametric model using historical demand data, and then present a mixed-integer programming to solve the model. In the case of censored demand, we further propose an enhanced data-driven nonparametric model that leverages the Kaplan–Meier estimator to correct sales data. We prove that the proposed data-driven ordering policies are asymptotically optimal and consistent, regardless of whether the demand is censored or not. To avoid overestimation of true profitability due to sampling error, we propose nonparametric bootstrap methods to estimate the lower confidence bound of profitability, providing a conservative estimate. We also demonstrate the consistency of the lower confidence bound of profitability obtained through the bootstrap-based numerical methods. Finally, we conduct numerical experiments using synthetic data to showcase the effectiveness of the proposed methods.},
  archive      = {J_EJOR},
  author       = {Wanpeng Wang and Shiming Deng and Yuying Zhang},
  doi          = {10.1016/j.ejor.2024.10.045},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {86-96},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data-driven ordering policies for target oriented newsvendor with censored demand},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One benders cut to rule all schedules in the neighbourhood.
<em>EJOR</em>, <em>323</em>(1), 62–85. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logic-Based Benders Decomposition (LBBD) and its Branch-and-Cut variant, namely Branch-and-Check, enjoy an extensive applicability on a broad variety of problems, including scheduling. As the application of LBBD to resource-constrained scheduling remains less explored, we propose a position-based Mixed-Integer Linear Programming (MILP) formulation for scheduling on unrelated parallel machines. To improve upon it, we notice that certain k − OPT neighbourhoods could be explored by regular local search operators, thus allowing us to integrate Local Branching into Branch-and-Check. After enumerating such neighbourhoods and obtaining their local optima – hence, proving that they are suboptimal – a local branching cut (applied as a Benders cut) eliminates all their solutions at once, thus avoiding an overload of the master problem with Benders cuts. However, to guarantee convergence to optimality, the constructed neighbourhood should be exhaustively explored, hence this time-consuming procedure must be accelerated by domination rules or selectively implemented on nodes which are more likely to reduce the optimality gap. In this study, we apply this idea on the ‘internal (job) swaps’ to construct formulation-specific 4-OPT neighbourhoods. We experiment extensively with the minimisation of total completion times and total tardiness on unrelated machines with sequence-dependent and resource-constrained setups. Our results show that our proposed use of local branching reduces optimality gaps considerably compared to standard Branch-and-Check or a monolithic Constraint Programming implementation. The simplicity of our approach allows its transferability to other neighbourhoods of the same or analogous formulations.},
  archive      = {J_EJOR},
  author       = {Ioannis Avgerinos and Ioannis Mourtos and Stavros Vatikiotis and Georgios Zois},
  doi          = {10.1016/j.ejor.2024.12.009},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {62-85},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {One benders cut to rule all schedules in the neighbourhood},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-phase algorithm for the three-dimensional loading
vehicle routing problem with split pickups and time windows.
<em>EJOR</em>, <em>323</em>(1), 45–61. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a survey of Belgian logistics service providers, the efficiency of first-mile pickup operations was identified as a key area for improvement, given the increasing number of returns in e-commerce, which has a significant impact on traffic congestion, carbon emissions, energy consumption and operational costs. However, the complexity of first-mile pickup operations, resulting from the small number of parcels to be collected at each pickup location, customer time windows, and the need to efficiently accommodate the highly heterogeneous cargo inside the vans, has hindered the development of real-world solution approaches. This article tackles this operational problem as a vehicle routing problem with time windows, time-dependent travel durations, and split pickups and integrates practical 3D container loading constraints such as vertical and horizontal stability as well as a more realistic reachability constraint to replace the classical “Last In First Out” (LIFO) constraint. To solve it, we propose a three-phase heuristic based on a savings constructive heuristic, an extreme point concept for the loading aspect and a General Variable Neighborhood Search as an improvement phase for both routing and packing. Numerical experiments are conducted to assess the performance of the algorithm on benchmark instances and new instances are tested to validate the positive managerial impacts on cost when allowing split pickups and on driver working duration when extending customer time windows. In addition, we show the impacts of considering the reachability constraint on cost and of the variation of speed during peak hours on schedule feasibility.},
  archive      = {J_EJOR},
  author       = {Emeline Leloup and Célia Paquay and Thierry Pironet and José Fernando Oliveira},
  doi          = {10.1016/j.ejor.2024.12.005},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {45-61},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A three-phase algorithm for the three-dimensional loading vehicle routing problem with split pickups and time windows},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-index rule for managing temporary congestion.
<em>EJOR</em>, <em>323</em>(1), 34–44. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work in healthcare operations provide empirical evidence for the deterioration of service quality due to congestion. Motivated by these findings, we formulate a novel scheduling problem to study how a service provider should prioritize jobs in order to mitigate the impact of temporary congestion-related issues. We analyze the model and show that the optimal policy can be interpreted as a dynamic priority rule that operates in two phases. When the system is overloaded, it is optimal to process jobs according to an index that generalizes Smith’s rule by incorporating the congestion cost. Once the system is no longer overloaded, Smith’s rule becomes optimal. However, the decision about which job to process earlier versus later appears to be challenging (we establish a polynomial time reduction from the partition problem). Our work shows that to respond to congestion, the decision maker should deviate from default scheduling practices and adjust jobs’ urgency at times of congestion to account for potential congestion-related costs. This increases the priority that should be given to shorter jobs (which reduces the time the system is congested), while still taking into account other job characteristics.},
  archive      = {J_EJOR},
  author       = {Yaron Shaposhnik},
  doi          = {10.1016/j.ejor.2024.11.045},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {34-44},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A dual-index rule for managing temporary congestion},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiskilled workforce staffing and scheduling: A
logic-based benders’ decomposition approach. <em>EJOR</em>,
<em>323</em>(1), 20–33. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the staffing and scheduling problem of a multiskilled workforce with uncertain demand. We formulate the problem as a two-stage stochastic integer program. The first stage considers strategic decisions, including recruiting permanent staff from an available pool and training them with additional skills, and the second stage focuses on operational decisions, involving the allocation of the multiskilled workforce and the hiring of temporary staff to accommodate uncertain demand. To effectively solve problems of practical sizes, we develop a novel solution algorithm based on the logic-based Benders’ decomposition (LBBD) approach, incorporating a customized analytical cut. We validate our approach through a case study using the data from a prefabrication company, demonstrating the significant cost savings achieved through workforce multiskilling. Our experimental results show that the proposed method is substantially more efficient than the latest Gurobi solver, up to 133 times faster and on average 29 times faster than directly solving the monolithic deterministic equivalent problem (MDEP).},
  archive      = {J_EJOR},
  author       = {Araz Nasirian and Lele Zhang and Alysson M. Costa and Babak Abbasi},
  doi          = {10.1016/j.ejor.2024.11.033},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {20-33},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multiskilled workforce staffing and scheduling: A logic-based benders’ decomposition approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost efficiency in water supply systems: An applied review
on optimization models for the pump scheduling problem. <em>EJOR</em>,
<em>323</em>(1), 1–19. (<a
href="https://doi.org/10.1016/j.ejor.2024.07.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for efficient pump operation in water supply systems (WSS) has become increasingly important over time, driven by the growing energy consumption and the associated energy costs. Forecasts for 2050 anticipate a global increase in water demand by 55%, indicating an increasing surge in WSS energy consumption. Control of pumping stations, which consume 70% of the energy in WSS, is the most critical area for optimization. This optimization challenge is commonly referred as the pump scheduling problem (PSP), and can be addressed using a variety of mathematical formulations. While numerous formulations exist to solve this optimization problem, the large majority of the studies are focus on the optimization techniques, sidelining the problem formulation. Due to the unique physical characteristics of each WSS, individual mathematical formulations may exhibit different levels of performance. In addition to general pumps’ operation optimization, the employment of variable speed pumps (VSP) can lead to significant energy savings compared to fixed speed pumps (FSP). However, despite their apparent benefits, many established optimization models for the PSP have not yet incorporated VSP decision variables into their formulations. Therefore, this work aims to review the main mathematical formulations for the pump scheduling problem for WSS with VSP and to present a quantitative comparative study of three mathematical formulations applied to a case study in the literature. The comparative analysis here presented revealed that the optimization model based on duty cycles is more cost-efficient when compared to alternative approaches discussed in the literature.},
  archive      = {J_EJOR},
  author       = {Marlene Brás and Ana Moura and António Andrade-Campos},
  doi          = {10.1016/j.ejor.2024.07.039},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Cost efficiency in water supply systems: An applied review on optimization models for the pump scheduling problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of asset and liability management applied to
brazilian pension funds. <em>EJOR</em>, <em>322</em>(3), 1059–1076. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asset and Liability Management (ALM) is a critical framework for pension funds, ensuring they have sufficient assets to meet future liabilities (pension payments) while managing investment risks effectively. This paper utilizes Brazilian data to develop an ALM model specifically for pension funds in the country. The model employs an optimization strategy that minimizes expected contributions made by individuals throughout their working lives. This optimization adheres to cash flow limitations and regulatory restrictions. The objective function leverages a min–max robust optimization approach based on a three-scenario planning scheme inspired by Brazil’s Interbank Rate. We incorporate a machine learning approach based on CMARS to predict confidence intervals for the key stochastic model parameters, particularly those related to the real returns of Brazilian investment classes. The findings empower pension fund managers to formulate well-informed investment strategies. We highlight allocation strategies that can reduce contribution rates without jeopardizing fund solvency, even for managers with a more aggressive risk profile favoring higher stock market allocations. Additionally, the study is enriched by an empirical analysis using data from a Brazilian pension fund, demonstrating the model’s practical application. In short, this model offers valuable insights that can benefit a wide range of pension funds in the Brazilian market, and it could also be applied to similar situations globally.},
  archive      = {J_EJOR},
  author       = {Wilton Bernardino and Rodrigo Falcão and João Jr. and Raydonal Ospina and Filipe Costa de Souza and José Jonas Alves Correia},
  doi          = {10.1016/j.ejor.2024.11.016},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {1059-1076},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A study of asset and liability management applied to brazilian pension funds},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integration of support vector machines and mean-variance
optimization for capital allocation. <em>EJOR</em>, <em>322</em>(3),
1045–1058. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel methodology for portfolio optimization that is the first to integrate support vector machines (SVMs) with cardinality-constrained mean–variance optimization. We propose augmenting cardinality-constrained mean–variance optimization with a preference for portfolios with the property that a low-dimensional hyperplane can separate assets eligible for investment from those ineligible. We present convex mixed-integer quadratic programming models that jointly select a portfolio and a separating hyperplane. This joint selection optimizes a tradeoff between risk-adjusted returns, hyperplane margin, and classification errors made by the hyperplane. The models are amenable to standard commercial branch-and-bound solvers, requiring no custom implementation. We discuss the properties of the proposed optimization models and draw connections between existing portfolio optimization and SVM approaches. We develop a parameter selection strategy to address the selection of big- M s and provide a financial interpretation of the proposed approach’s parameters. The parameter strategy yields valid big- M values, ensures the risk of the resulting portfolio is within a factor of the lowest possible risk, and produces informative hyperplanes for practitioners. The mathematical programming models and the associated parameter selection strategy are amenable to financial backtesting. The models are evaluated in-sample and out-of-sample on two distinct datasets in a rolling horizon backtesting framework. The portfolios resulting from the proposed approach display improved out-of-sample risk-adjusted returns compared to cardinality-constrained mean–variance optimization.},
  archive      = {J_EJOR},
  author       = {David Islip and Roy H. Kwon and Seongmoon Kim},
  doi          = {10.1016/j.ejor.2024.11.022},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {1045-1058},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integration of support vector machines and mean-variance optimization for capital allocation},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The yin and yang of banking: Modeling desirable and
undesirable outputs. <em>EJOR</em>, <em>322</em>(3), 1025–1044. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel by-production approach to modeling desirable and undesirable output production processes in the US banking sector. We utilize the structural proxy variable framework in which desirable outputs (different types of loans and other income-generating activities) are exogenous, which is a common practice in the banking literature. The undesirable output is non-performing loans (NPLs). To address the endogeneity of variable inputs (purchased funds and core deposits) in the production of desirable outputs, we employ an input distance function and rely on the bank’s cost-minimizing behavioral assumption. We specify the undesirable output technology as a function of desirable outputs as well as other factors such as total non-transaction accounts, undivided profits, and capital reserves. Using US commercial bank data from 2001 to 2020, we find that bank productivity exhibits steady growth in desirable outputs. Banks prioritize reducing the overall productivity impact of NPLs post-crisis, shifting focus from pre-crisis service provision.},
  archive      = {J_EJOR},
  author       = {Yulu Wang and Subal C. Kumbhakar and Man Jin},
  doi          = {10.1016/j.ejor.2024.11.004},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {1025-1044},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The yin and yang of banking: Modeling desirable and undesirable outputs},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the valuation of legacy power production in liberalized
markets via option-pricing. <em>EJOR</em>, <em>322</em>(3), 1005–1024.
(<a href="https://doi.org/10.1016/j.ejor.2024.10.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legacy assets can constitute entry barriers in liberalized power markets. Regulations pertaining to such assets have many objectives, the most important of which are to transfer the benefits of an economical production technology to consumers and foster competition. To that end, countries have adopted various regulations but there is no consensus today on identifying the first best solution. Inspired by the French regulation of historical nuclear production and considering the market risk that now prevails in the sector, we propose an option-based approach to regulating legacy assets that reflects production costs and encompasses optionality at the same time. To achieve that aim, we study a competitive, but financially incomplete market where the incumbent and several competitors exchange legacy production via a regulated call option. Agents do not face the same risk exposure and their attitudes toward risk, which we model by coherent risk measures, might differ. The result is a stochastic equilibrium model of regulated option-pricing in incomplete markets that we calibrate numerically and solve for the French market. We quantify the option value and assess its impact on the system for various regimes of the spot market, including the one of very high and volatile prices of the recent energy crisis. We also analyze the impacts of risk aversion and the option’s maturity. Based on our analysis, we provide recommendations for enhancing the current French regulation of historical nuclear production.},
  archive      = {J_EJOR},
  author       = {Ibrahim Abada and Mustapha Belkhouja and Andreas Ehrenmann},
  doi          = {10.1016/j.ejor.2024.10.033},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {1005-1024},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On the valuation of legacy power production in liberalized markets via option-pricing},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-averse algorithmic support and inventory management.
<em>EJOR</em>, <em>322</em>(3), 993–1004. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how managers allocate resources in response to algorithmic recommendations that are programmed with specific levels of risk aversion. Using the anchoring and adjustment heuristic, we derive our predictions and test them in a series of multi-item newsvendor experiments. We find that highly risk-averse algorithmic recommendations have a strong and persistent influence on order decisions, even after the recommendations are no longer available. Furthermore, we show that these effects are similar regardless of factors such as source of advice (i.e., human vs. algorithm) and decision autonomy (i.e., whether the algorithm is externally assigned or chosen by the subjects themselves). Finally, we disentangle the effect of risk attitude from that of anchor distance and find that subjects selectively adjust their order decisions by relying more on algorithmic advice that contrasts with their inherent risk preferences. Our findings suggest that organizations can strategically utilize risk-averse algorithmic tools to improve inventory decisions while preserving managerial autonomy.},
  archive      = {J_EJOR},
  author       = {Pranadharthiharan Narayanan and Jeeva Somasundaram and Matthias Seifert},
  doi          = {10.1016/j.ejor.2024.11.013},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {993-1004},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Risk-averse algorithmic support and inventory management},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementing no free disposability in data envelopment
analysis. <em>EJOR</em>, <em>322</em>(3), 978–992. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data envelopment analysis (DEA) relies on two main postulates of convexity and inefficiency (free disposability). No free disposability postulate is suggested to address undesirable measures. In this study, we demonstrate how no-disposability assumption can be correctly integrated into the DEA framework. We propose the appropriate constraints that should be used in the absence of the free disposability postulate in a DEA model. The additional constraints bound the previously unbounded feasible region (production technology) rather than altering the strongly efficient frontier. We also discuss that treating an undesirable output (input) as a desirable input (output) does not affect the corresponding efficient frontier of a dataset, but misrepresents its corresponding production technology in the presence of free disposability postulate. We provide numerical examples to clarify the concerns in treating an undesirable measure as a desirable measure. A real-life example of United States’ electric power plants is also discussed.},
  archive      = {J_EJOR},
  author       = {Dariush Khezrimotlagh and Joe Zhu},
  doi          = {10.1016/j.ejor.2024.11.029},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {978-992},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Implementing no free disposability in data envelopment analysis},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective route planning of an unmanned air vehicle in
continuous terrain: An exact and an approximation algorithm.
<em>EJOR</em>, <em>322</em>(3), 960–977. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) are widely used for military and civilian purposes. Effective route planning is an important component of their successful missions. In this study, we address the route planning problem of a UAV tasked with collecting information from various target locations in a protected terrain. We consider multiple targets, three objectives, and time-dependent information availability. Modeling the movement of UAVs in a continuous terrain in the presence of multiple objectives is complex. Conflicting objectives typically lead to a continuum of efficient trajectory options between two targets. We formulate the routing problem as a mixed-integer programming (MIP) model that captures the movement in the continuous terrain. We demonstrate the superiority of the continuous terrain formulation over the simplified discretized terrain formulation. We also develop an approximation algorithm that reduces the computational requirements of the MIP model substantially while ensuring a desired level of precision.},
  archive      = {J_EJOR},
  author       = {Erdi Dasdemir and Murat Köksalan and Diclehan Tezcaner Öztürk},
  doi          = {10.1016/j.ejor.2024.11.015},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {960-977},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-objective route planning of an unmanned air vehicle in continuous terrain: An exact and an approximation algorithm},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effectiveness of social distancing under partial compliance
of individuals. <em>EJOR</em>, <em>322</em>(3), 949–959. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social distancing reduces infectious disease transmission by limiting contact frequency and proximity within a community. However, compliance varies due to its impact on daily life. This paper explores the effects of compliance on social distancing effectiveness through a “social distancing game”, where community members make decisions based on personal utility. We conducted numerical experiments to evaluate how different policy settings for social distancing affect disease transmission. Our findings suggest several key points for developing effective social distancing policies. Firstly, while generally effective, overly strict policies may lead to noncompliance and reduced effectiveness. Secondly, the public health benefits of social distancing need to be balanced against social costs, emphasizing policy efficiency. Lastly, for diseases with low reinfection risk, a segmented policy exempting immune individuals could lessen both infections and socioeconomic costs.},
  archive      = {J_EJOR},
  author       = {Hyelim Shin and Taesik Lee},
  doi          = {10.1016/j.ejor.2024.11.006},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {949-959},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Effectiveness of social distancing under partial compliance of individuals},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Process improvement under the reference price effect.
<em>EJOR</em>, <em>322</em>(3), 937–948. (<a
href="https://doi.org/10.1016/j.ejor.2024.10.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-reducing process improvement, leading to price reductions and consequently sales growth, is becoming increasingly relevant in today’s uncertain economic environment. However, existing process improvement studies involving sales growth typically assume that consumers only consider the current price of a product when making a purchase. In reality, sales growth also often stems from the reference price effect, where consumers factor in both the current and past prices. By incorporating the reference price effect, we examine the process improvement investment decisions and pricing strategies in a decentralized supply chain. We develop a two-period game-theoretic model, where the supplier invests in process improvement to reduce production costs, and the supplier and the retailer set their prices. This approach differs from existing reference price effect literature, where prices are predetermined exogenously in a decentralized supply chain. We find that the reference price effect stimulates process improvement investment, making both firms more profitable. However, a more prominent reference price effect may significantly decrease supply chain efficiency in the presence of process improvement, resulting in lower profits that move away from what an integrated firm would achieve. When firms set their own prices, the reference price effect intensifies competition for profits and worsens misalignment caused by process improvement. This outcome contrasts with existing studies, which usually argue that the reference price effect increases efficiency. Therefore, managers should consider consumer responses to price changes when making process improvement investment decisions and analyze their impacts on both supply chain profitability and efficiency.},
  archive      = {J_EJOR},
  author       = {Zeming Wang and Jasper Veldman and Ruud Teunter},
  doi          = {10.1016/j.ejor.2024.10.037},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {937-948},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Process improvement under the reference price effect},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust multilinear target-based decision analysis
considering high-dimensional interactions. <em>EJOR</em>,
<em>322</em>(3), 920–936. (<a
href="https://doi.org/10.1016/j.ejor.2024.10.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multilinear Target-based Preference Functions (MTPFs) support multi-attribute decision problems characterized by attribute interactions and targets. However, existing research falls short in flexibly modeling high-dimensional interactions and lacks robustness in decision-making recommendations when faced with uncertain parameters and targets. The paper proposes a robust multilinear target-based decision analysis framework considering high-dimensional interactions, along with uncertainties in parameters and targets. First, the necessity of high-dimensional interactions and the limitations of available MTPFs in modeling high-dimensional interactions are demonstrated. Second, the MTPFs based on the 2-interactive fuzzy measure and the Nonmodularity index are proposed to model the high-dimensional interactions and simultaneously reduce the computational challenges of parameter identification. Third, new descriptive measures are proposed based on the Stochastic Multicriteria Acceptability Analysis to evaluate the robustness of decision recommendations subject to uncertain targets and parameters. The validation and advantages of the framework are illustrated with simulation studies and an application in customer competitive evaluation of smart thermometer patches.},
  archive      = {J_EJOR},
  author       = {Qiong Feng and Shurong Tong and Salvatore Corrente and Xinwei Zhang},
  doi          = {10.1016/j.ejor.2024.10.036},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {920-936},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust multilinear target-based decision analysis considering high-dimensional interactions},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Queues with service resetting. <em>EJOR</em>,
<em>322</em>(3), 908–919. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service time fluctuations heavily affect the performance of queueing systems, causing long waiting times and backlogs. Recently, it was shown that when service times are solely determined by the server, service resetting can mitigate the deleterious effects of service time fluctuations and drastically improve queue performance (Bonomo et al., 2022). Yet, in many queueing systems, service times have two independent sources: the intrinsic server slowdown ( S ) and the jobs’ inherent size ( X ). In these, so-called S &amp; X queues (Gardner et al., 2017), service resetting results in a newly drawn server slowdown while the inherent job size remains unchanged. Remarkably, resetting can be useful even then. To show this, we develop a comprehensive theory of S &amp; X queues with service resetting. We consider cases where the total service time is either a product or a sum of the service slowdown and the jobs’ inherent size. For both cases, we derive expressions for the total service time distribution and its mean under a generic service resetting policy. Two prevalent resetting policies are discussed in more detail. We first analyze the constant-rate (Poissonian) resetting policy and derive explicit conditions under which resetting reduces the mean service time and improves queue performance. Next, we consider the sharp (deterministic) resetting policy. While results hold regardless of the arrival process, we dedicate special attention to the S &amp; X -M/G/1 queue with service resetting, and obtain the distribution of the number of jobs in the system and their sojourn time. Our analysis highlights situations where service resetting can be used as an effective tool to improve the performance of S &amp; X queueing systems. Several examples are given to illustrate our analytical results, which are corroborated using numerical simulations.},
  archive      = {J_EJOR},
  author       = {Ofek Lauber Bonomo and Uri Yechiali and Shlomi Reuveni},
  doi          = {10.1016/j.ejor.2024.12.044},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {908-919},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Queues with service resetting},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximating g(t)/GI/1 queues with deep learning.
<em>EJOR</em>, <em>322</em>(3), 889–907. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world queueing systems exhibit a time-dependent arrival process and can be modeled as a G ( t ) / G I / 1 queue. Despite its wide applicability, little can be derived analytically about this system, particularly its transient behavior. Yet, many services operate on a schedule where the system is empty at the beginning and end of each day; thus, such systems are unlikely to enter a steady state. In this paper, we apply a supervised machine learning approach to solve a fundamental problem in queueing theory: estimating the transient distribution of the number in the system for a G ( t ) / G I / 1 . We develop a neural network mechanism that provides a fast and accurate predictor of these distributions for moderate horizon lengths and practical settings. It is based on using a Recurrent Neural Network (RNN) architecture based on the first several moments of the time-dependent inter-arrival and the stationary service time distributions; we call it the Moment-Based Recurrent Neural Network (RNN) method ( MBRNN ). Our empirical study suggests MBRNN requires only the first four inter-arrival and service time moments. We use simulation to generate a substantial training dataset and present a thorough performance evaluation to examine the accuracy of our method using two different test sets. We perform sensitivity analysis over different ranges of Squared Coefficient of Variation (SCV) of the inter-arrival and service time distribution and average utilization level. We show that even under the configuration with the worst performance errors, the mean number of customers over the entire timeline has an error of less than 3%. We further show that our method outperforms fluid and diffusion approximations. While simulation modeling can achieve high accuracy (in fact, we use it as the ground truth), the advantage of the MBRNN over simulation is runtime. While the runtime of an accurate simulation of a G ( t ) / G I / 1 queue can be measured in hours, the MBRNN analyzes hundreds of systems within a fraction of a second. We demonstrate the benefit of this runtime speed when our model is used as a building block in optimizing the service capacity for a given time-dependent arrival process. This paper focuses on a G ( t ) / G I / 1 , however the MBRNN approach demonstrated here can be extended to other queueing systems, as the training data labeling is based on simulations (which can be applied to more complex systems) and the training is based on deep learning, which can capture very complex time sequence tasks. In summary, the MBRNN has the potential to revolutionize our ability for transient analysis of queueing systems.},
  archive      = {J_EJOR},
  author       = {Eliran Sherzer and Opher Baron and Dmitry Krass and Yehezkel Resheff},
  doi          = {10.1016/j.ejor.2024.12.030},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {889-907},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Approximating G(t)/GI/1 queues with deep learning},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-sharing in energy communities. <em>EJOR</em>,
<em>322</em>(3), 870–888. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy communities are considered one of the pillars of the energy transition, owing to the rapid development of digital smart appliances and metering. They benefit from strong political support to accommodate their penetration in Europe. Nevertheless, the pace at which they have developed has been very slow compared with what was expected a decade ago. Many articles have revealed some of the underlying reasons, among which are social heterogeneity among participants, unfavorable local regulations, and inadequate governance. Most recently, a nascent body of research has highlighted the need to find adequate sharing rules for the benefits of community projects. Because of the complexity of these rules, the appointment of a community manager or coordinator may be necessary. This paper follows suit by providing guidance to policy makers or community managers about optimal risk-sharing schemes among members of an energy community. By modeling and simulating energy communities that invest in a rooftop photo-voltaic project and face some degree of production and remuneration risk, we find that a high level of risk aversion makes it impossible to allocate the risk in a stable way. Furthermore, we show that some communities whose members’ risk aversion is too heterogeneous cannot form successfully. Besides, even when risk can be allocated in a stable manner, we show that fair allocations are so complex that they require the intervention of a coordinator or a community manager. Finally, we analyze the advantages of developing judicious risk-sharing instruments between communities and a central entity for providing stability.},
  archive      = {J_EJOR},
  author       = {Ibrahim Abada and Andreas Ehrenmann and Xavier Lambin},
  doi          = {10.1016/j.ejor.2024.12.029},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {870-888},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Risk-sharing in energy communities},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Truck–drone routing problem with stochastic demand.
<em>EJOR</em>, <em>322</em>(3), 854–869. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Truck–drone combination involves launch/retrieval of rotary-wing drones on trucks, which can address the issues of limited endurance and capacity of rotary-wing drones in delivery systems. Truck–drone combination technologies provide a compelling alternative to traditional emergency logistics systems that rely on on-ground transportation networks. Thus far, little research has been conducted on the truck–drone routing variant with stochastic demand, which is closely related to emergency logistics systems. Herein, we formally define the truck–drone routing problem with stochastic demand (TDRP-SD), which involves drones responding quickly to stochastic demands and restocking the supply. In particular, a new restocking policy, termed the truck–drone synchronized (TDS) restocking policy, is introduced to complement the traditional restocking operations that rely on ground vehicles. We analyze the characteristics of the introduced restocking policy and develop several propositions to address the computational burden caused by the dynamic programming computation of the expected cost. We propose a hybrid heuristic that combines the state-of-the-art Slack Induction by String Removals (SISRs) and greedy insertion utilizing blink rules. Several mechanisms, such as short-route deep search, lower-bound and upper-bound guiding, and simulated annealing, are adopted to ensure the algorithm performance. In computational experiments, the hybrid heuristic solves two types of benchmark instances and achieves new solutions. In addition, a collection of converted instances with up to 302 customers is effectively solved. The sensitivity analysis demonstrates the performance of the TDS restocking policy.},
  archive      = {J_EJOR},
  author       = {Feilong Wang and Hongqi Li and Hanxi Xiong},
  doi          = {10.1016/j.ejor.2024.11.036},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {854-869},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Truck–drone routing problem with stochastic demand},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of vendor preferences on commission policy of
e-commerce platform. <em>EJOR</em>, <em>322</em>(3), 841–853. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the prevalent online marketplaces, vendors manage daily operations while e-commerce platforms (EPs) that provide auxiliary services and charge commission fees. Two commission policies are examined in this article: Fixed Commission Policy (FCP), involving a fixed usage fee, and Ordinary Commission Policy (OCP), incorporating an additional fee proportional to sales revenue alongside the fixed usage fee, with this proportion referred to as the commission rate. We develop a two-stage game-theoretical model of an e-commerce supply chain, wherein the EP sets the commission policy and a risk-sensitive vendor determines its stock level. The vendor&#39;s risk attitude is characterized by three key preferences: reference preference, utility weight preference, and loss aversion preference. Under reasonable assumptions, we establish the existence and uniqueness of the game equilibrium, yielding several key insights: (i) Vendor preferences significantly influence the commission policy, with reference preference being central in shaping the optimal commission rate. Specifically, while the FCP is optimal for risk-neutral vendors, it may not be suitable when vendors are risk-sensitive. (ii) The ratio of unit cost to retail price is the primary driver of variations in optimal commission rate. Moreover, the optimal commission rate tends to decrease as this ratio increases. (iii) In the presence of risk sensitivity, a commission policy maximizing the EP&#39;s profit can lead to Pareto improvement compared to one aimed at centralized profit maximization. Our analysis offers valuable insights into the design of commission policies for EPs, providing credible explanations for various economic phenomena associated with these policies in e-commerce practices.},
  archive      = {J_EJOR},
  author       = {Jiansheng Dai and Xinyu Zhang},
  doi          = {10.1016/j.ejor.2024.11.037},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {841-853},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Impact of vendor preferences on commission policy of E-commerce platform},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resilient transportation network design with disruption
uncertainty and lead times. <em>EJOR</em>, <em>322</em>(3), 827–840. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-efficient and reliable transports are needed to supply products competitively. Thus, particularly in increasingly complex and global supply chains, identifying the optimal transportation mode is a critical decision. Transportation modes, however, are prone to disruptions, such as hurricanes, low water levels, or port shutdowns, resulting in transportation stops and cost increases. To counteract these disruptions, different resilience strategies are studied to increase the capability of a network to withstand, adapt, and recover from disruptions. For a cost-optimal use, it is necessary to determine the optimal mix of strategic, tactical, and operational strategies. We provide a decision-support model that decides on the optimal mix of resilience strategies, such as multi-sourcing, inventory, or operational re-routing, for a supply chain with transportation disruption uncertainty to minimize total expected costs. The problem is formulated as a two-stage stochastic mixed-integer linear program that explicitly considers lead times. To handle large instances, we propose a Benders decomposition approach enhanced through lower-bound lifting and valid inequalities, branch-and-benders-cut, and a warm-start heuristic. Computational experiments show that large instances can be solved to near-optimality, whereas a commercial solver does not find feasible solutions. We present a case study for a company’s inbound supply chain design with recurring transportation cost uncertainty. Considering disruption and lead time effects, a mix of resilience strategies from strategic to operational level leads to cost improvements of up to 50%. Furthermore, we show that the ability to predict disruptions can further reduce resilience-related costs by 10% if sufficient operational re-routing capacities are available.},
  archive      = {J_EJOR},
  author       = {Daniel Müllerklein and Pirmin Fontaine},
  doi          = {10.1016/j.ejor.2024.11.021},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {827-840},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Resilient transportation network design with disruption uncertainty and lead times},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of channel role on the outsourcing of after-sales
service with asymmetric retailer competition. <em>EJOR</em>,
<em>322</em>(3), 812–826. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After-sales service is support provided to a customer after purchase, which potentially leads to higher customer satisfaction and is demand-enhancing. Using a game-theoretic model in which a manufacturer determines its after-sales service and distribution channel strategies in the presence of two asymmetric retailers, we identify channel position as an important criterion in determining the outsourcing of after-sales service. Specifically, outsourcing to a third-party provider, due to its lack of channel interaction, is never an optimal choice for the manufacturer unless the third-party has a significant cost advantage in providing after-sales service. However, because of the channel role of the retailers, the manufacturer outsources to the large retailer rather than undertaking the after-sales service in-house, when the competing small retailer is less competitive and the cost of service provision is high. The trade-off between the manufacturer outsourcing the service and undertaking that in-house involves whether the manufacturer accommodates the small retailer in the market. When service provision is outsourced, the large retailer enjoys a lower wholesale price if the small retailer is present, and therefore the large retailer subsidizes the manufacturer to induce the manufacturer to accommodate the small retailer. However, the manufacturer, when undertaking the service by itself, forgoes the small retailer. Finally, we show that when the manufacturer adopts a multi-retailer distribution channel, the large retailer benefits because improved after-sales service increases demand and consumer valuation of the product. We also demonstrate the robustness of our key results in multiple extensions.},
  archive      = {J_EJOR},
  author       = {Shuguang Zhang and Wei Shi Lim and Ziqiu Ye},
  doi          = {10.1016/j.ejor.2024.11.020},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {812-826},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The impact of channel role on the outsourcing of after-sales service with asymmetric retailer competition},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retailer involvement in eco-conscious consumer-oriented
carbon footprint reduction. <em>EJOR</em>, <em>322</em>(3), 795–811. (<a
href="https://doi.org/10.1016/j.ejor.2024.10.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retailer involvement and consumer eco-consciousness are two critical considerations for firms when designing comprehensive carbon footprint reduction (CFR) plans. This paper constructs a supply chain (SC) where one manufacturer and one retailer make CFR and pricing strategies in response to eco-conscious consumers. Eco-conscious consumers form a reference carbon footprint to assess the product&#39;s green level. To achieve SC coordination, a cost-sharing contract is introduced. Our results suggest that retailer involvement in CFR always benefits the environment, SC performance, consumer surplus, and social welfare. During CFR cooperation, the manufacturer strategically affects the retailer&#39;s CFR decision by adjusting the CFR level, impacting environmental and economic outcomes. Although higher CFR efficiency by the manufacturer can benefit the environment, SC performance, consumer surplus, and social welfare, a similar emphasis by the retailer may have adverse effects. Surprisingly, a lower reference carbon footprint for eco-conscious consumers may be worse for the environment, depending on the retailer&#39;s CFR efficiency. Furthermore, implementing a cost-sharing contract under the retailer&#39;s different CFR efficiency yields two distinct impacts: a multi-win situation or an incentive conflict. Extended studies are further examined to validate the robustness of these main results.},
  archive      = {J_EJOR},
  author       = {Feiying Jiang and Weilai Huang and Jun Yang and Hongchen Duan},
  doi          = {10.1016/j.ejor.2024.10.030},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {795-811},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Retailer involvement in eco-conscious consumer-oriented carbon footprint reduction},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A speed-up procedure and new heuristics for the classical
job shop scheduling problem: A computational evaluation. <em>EJOR</em>,
<em>322</em>(3), 783–794. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The speed-up procedure proposed for the permutation flowshop scheduling problem with makespan minimisation (commonly denoted as Taillard’s acceleration) remains, after 30 years, one of the most important and relevant studies in the scheduling literature. Since its proposal, this procedure has been included in countless approximate optimisation algorithms, and its use is mandatory for several scheduling problems. Unfortunately, despite the importance of such a procedure in solving scheduling problems, we are not aware of any related speed-up procedure proposed for the classical job-shop scheduling problem. First, this study aims to fill this gap by proposing a novel speed-up procedure for the job-shop scheduling problem with makespan minimisation, capable of reducing the complexity of insertion-based procedures n times. Second, to test its performance, the procedure is embedded in a critical-path-based local search method. Furthermore, we thirdly propose five constructive and composite heuristics to obtain high-quality solutions in short time intervals. The composite heuristics apply the previous procedure to reduce their computational efforts. Finally, to complete the study, we conduct an extensive computational evaluation on 243 test instances from eight distinct benchmarks. In this evaluation, 30 heuristics are re-implemented and compared under the same computer conditions. The results indicate the superiority of the proposed approaches compared to the competitive algorithms for the problem under study.},
  archive      = {J_EJOR},
  author       = {Victor Fernandez-Viagas and Carla Talens and Bruno de Athayde Prata},
  doi          = {10.1016/j.ejor.2024.11.026},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {783-794},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A speed-up procedure and new heuristics for the classical job shop scheduling problem: A computational evaluation},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-dimensional bin packing with pattern-dependent
processing time. <em>EJOR</em>, <em>322</em>(3), 770–782. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper the classical one-dimensional bin packing problem is integrated with scheduling elements: a due date is assigned to each item and the time required to process each bin depends on the pattern being used. The objective is to minimize a convex combination of the material waste and the delay costs, both significant in many real-world contexts. We present a novel pattern-based mixed integer linear formulation suitable for different classical scheduling objective functions, and focus on the specific case where the delay cost corresponds to the maximum tardiness. The formulation is tackled by a branch-and-price algorithm where the pricing of the column generation scheme is a quadratic problem solved by dynamic programming. A sequential value correction heuristic (SVC) is used to feed with warm starting solutions the column generation which, in turn, feeds the SVC with optimal prices so as to compute refined feasible solutions during the enumeration. Computational tests show that both column generation and branch-and-price substantially outperform standard methods in computing dual bounds and exact solutions. Additional tests are presented to analyze the sensitivity to parameters’ changes.},
  archive      = {J_EJOR},
  author       = {Fabrizio Marinelli and Andrea Pizzuti and Wei Wu and Mutsunori Yagiura},
  doi          = {10.1016/j.ejor.2024.11.023},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {770-782},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {One-dimensional bin packing with pattern-dependent processing time},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ε-constraint procedures for pareto front optimization of
large size discrete time/cost trade-off problem. <em>EJOR</em>,
<em>322</em>(3), 753–769. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete time/cost trade-off problem (DTCTP) optimizes the project duration and/or cost while considering the trade-off between activity durations and their direct costs. The complete and non-dominated time-cost profile over the set of feasible project durations is achieved within the framework of Pareto front problem. Despite the importance of Pareto front optimization in project and portfolio management, exact procedures have achieved very limited success in solving the problem for large size instances. This study develops exact procedures based on combinations of mixed-integer linear programming (MILP), ε -constraint method, network and problem reduction techniques, and present new bounding strategies to solve the Pareto problem for large size instances. This study also provides new large size benchmark problem instances aiming to represent the size of real-life projects for the DTCTP. The new instances, therefore, are generated to include up to 990 activities and nine execution modes. Computational experiments reveal that the procedures presented herein can remarkably outperform the state-of-the-art exact methods. The new exact procedures enabled obtaining the optimal Pareto front for instances with serial networks that include more than 200 activities for the first time.},
  archive      = {J_EJOR},
  author       = {Saman Aminbakhsh and Rifat Sönmez and Tankut Atan},
  doi          = {10.1016/j.ejor.2024.11.032},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {753-769},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {ε-constraint procedures for pareto front optimization of large size discrete time/cost trade-off problem},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overcoming poor data quality: Optimizing validation of
precedence relation data. <em>EJOR</em>, <em>322</em>(3), 740–752. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insufficient data quality prevents data usage by decision support systems (DSS) in many areas of business. This is the case for data on precedence relations between tasks, which is relevant, for instance, in project scheduling and assembly line balancing. Inaccurate data on unnecessary precedence relations cannot be used, otherwise the recommendations of DSS may turn infeasible. So, unnecessary relations must be satisfied, diminishing the baseline problem’s solution space and the business result. Experts can validate the data, but their time is limited. We apply an optimization lens and formulate the data validation problem (DVP). Restricted by the available time budget, an expert dynamically receives queries about specific data entries and corrects or validates them. The DVP searches for an interview policy that states queries to the expert, each using up some of the time budget, in a way that maximizes the (weighted) number of removed precedence relations. We model the DVP as a dynamic program, derive optimal policies for several important special cases and design a heuristic interview policy LSTD. In a case study of an automobile manufacturer, this policy substantially reduces the stations’ idle time after selectively addressing about 8% of the data entries. We prove theoretically and numerically that data validation by experts can lead to significant savings. The number of queries required to validate the data exhaustively is much less than naive estimates. Additionally, the probability to remove an unnecessary precedence relation per query in a series of queries is high, even for simple interview policies.},
  archive      = {J_EJOR},
  author       = {Benedikt Finnah and Jochen Gönsch and Alena Otto},
  doi          = {10.1016/j.ejor.2024.11.009},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {740-752},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Overcoming poor data quality: Optimizing validation of precedence relation data},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of maintenance optimization: Reflections and
perspectives. <em>EJOR</em>, <em>322</em>(3), 725–739. (<a
href="https://doi.org/10.1016/j.ejor.2024.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the occasion of the 50th anniversary of the Association of European Operational Research Societies (EURO), we share our perspectives and reflections on maintenance research. We review the main methods and techniques for optimizing when and what to maintain, providing concrete examples as illustrations. We also discuss the optimization of the logistics support system surrounding the act of maintenance. In doing so, we highlight the multidisciplinary nature of maintenance research and its interface with other domains, such as spare parts inventory management, production scheduling, and transportation planning. We support our reflections with basic text-mining analyses of the archive of the European Journal of Operational Research , the journal published in collaboration with EURO. With this paper, we introduce interested researchers to maintenance optimization and share opportunities to close the gaps between the current state of research and real-world needs.},
  archive      = {J_EJOR},
  author       = {Joachim Arts and Robert N. Boute and Stijn Loeys and Heletjé E. van Staden},
  doi          = {10.1016/j.ejor.2024.07.002},
  journal      = {European Journal of Operational Research},
  month        = {5},
  number       = {3},
  pages        = {725-739},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of maintenance optimization: Reflections and perspectives},
  volume       = {322},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
