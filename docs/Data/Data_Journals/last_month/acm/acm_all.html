<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>acm_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="acm">ACM</h1>
<h2 id="cacm---21">CACM - 21</h2>
<ul>
<li><details>
<summary>
(2025). A heavenly host. <em>CACM</em>, <em>68</em>(2), 108–ff. (<a
href="https://doi.org/10.1145/3708556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708556},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {108-ff},
  shortjournal = {Commun. ACM},
  title        = {A heavenly host},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asleep at the keyboard? Assessing the security of GitHub
copilot’s code contributions. <em>CACM</em>, <em>68</em>(2), 96–105. (<a
href="https://doi.org/10.1145/3610721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described “AI pair programmer,” GitHub Copilot, which is a language model trained over open-source GitHub code. However, code often contains bugs—and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot’s code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk cybersecurity weaknesses, for example, those from MITRE’s “Top 25” Common Weakness Enumeration (CWE) list. We explore Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40% to be vulnerable.},
  archive      = {J_CACM},
  doi          = {10.1145/3610721},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {96-105},
  shortjournal = {Commun. ACM},
  title        = {Asleep at the keyboard? assessing the security of GitHub copilot’s code contributions},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsafe code still a hurdle copilot must clear.
<em>CACM</em>, <em>68</em>(2), 95. (<a
href="https://doi.org/10.1145/3660529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3660529},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {95},
  shortjournal = {Commun. ACM},
  title        = {Unsafe code still a hurdle copilot must clear},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Negative-weight single-source shortest paths in near-linear
time. <em>CACM</em>, <em>68</em>(2), 87–94. (<a
href="https://doi.org/10.1145/3631536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the single-source shortest paths problem, the goal is to compute the shortest path tree from a designated source vertex in a weighted, directed graph. We present the first near-linear time algorithm for the problem that can also handle negative edge-weights; the runtime is O ( m log 8 ( n ) log W ) . In contrast to all recent developments that rely on sophisticated continuous optimization methods and dynamic algorithms, our algorithm is simple: it requires only a simple graph decomposition and elementary combinatorial tools. In fact, ours is the first combinatorial algorithm for negative-weight single-source shortest paths to break through the classic O ~ ( m n log W ) bound from over three decades ago (Gabow and Tarjan, SICOMP’89.)},
  archive      = {J_CACM},
  doi          = {10.1145/3631536},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {87-94},
  shortjournal = {Commun. ACM},
  title        = {Negative-weight single-source shortest paths in near-linear time},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shortening the path to designing efficient graph algorithms.
<em>CACM</em>, <em>68</em>(2), 86. (<a
href="https://doi.org/10.1145/3660528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3660528},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {86},
  shortjournal = {Commun. ACM},
  title        = {Shortening the path to designing efficient graph algorithms},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Program correctness through self-certification.
<em>CACM</em>, <em>68</em>(2), 74–84. (<a
href="https://doi.org/10.1145/3689624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3689624},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {74-84},
  shortjournal = {Commun. ACM},
  title        = {Program correctness through self-certification},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Justice, equity, diversity, and inclusion at UbiComp/ISWC:
Best practices for accessible and equitable computing conferences.
<em>CACM</em>, <em>68</em>(2), 64–73. (<a
href="https://doi.org/10.1145/3689820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3689820},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {64-73},
  shortjournal = {Commun. ACM},
  title        = {Justice, equity, diversity, and inclusion at UbiComp/ISWC: Best practices for accessible and equitable computing conferences},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta’s hyperscale infrastructure: Overview and insights.
<em>CACM</em>, <em>68</em>(2), 52–63. (<a
href="https://doi.org/10.1145/3701296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701296},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {52-63},
  shortjournal = {Commun. ACM},
  title        = {Meta’s hyperscale infrastructure: Overview and insights},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Questioning the criteria for evaluating non-cryptographic
hash functions. <em>CACM</em>, <em>68</em>(2), 46–51. (<a
href="https://doi.org/10.1145/3704255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3704255},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {46-51},
  shortjournal = {Commun. ACM},
  title        = {Questioning the criteria for evaluating non-cryptographic hash functions},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). It is time to standardize principles and practices for
software memory safety. <em>CACM</em>, <em>68</em>(2), 40–45. (<a
href="https://doi.org/10.1145/3708553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708553},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {40-45},
  shortjournal = {Commun. ACM},
  title        = {It is time to standardize principles and practices for software memory safety},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Life lessons from the first half-century of my career.
<em>CACM</em>, <em>68</em>(2), 36–39. (<a
href="https://doi.org/10.1145/3637905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3637905},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {36-39},
  shortjournal = {Commun. ACM},
  title        = {Life lessons from the first half-century of my career},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building on shaky ground. <em>CACM</em>, <em>68</em>(2),
34–35. (<a href="https://doi.org/10.1145/3707467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3707467},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {34-35},
  shortjournal = {Commun. ACM},
  title        = {Building on shaky ground},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical credit. <em>CACM</em>, <em>68</em>(2), 30–33. (<a
href="https://doi.org/10.1145/3690043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3690043},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {30-33},
  shortjournal = {Commun. ACM},
  title        = {Technical credit},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence then and now. <em>CACM</em>,
<em>68</em>(2), 24–29. (<a
href="https://doi.org/10.1145/3708554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708554},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {24-29},
  shortjournal = {Commun. ACM},
  title        = {Artificial intelligence then and now},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating research software engineering: Toward RSE
research. <em>CACM</em>, <em>68</em>(2), 20–23. (<a
href="https://doi.org/10.1145/3685265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3685265},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {20-23},
  shortjournal = {Commun. ACM},
  title        = {Investigating research software engineering: Toward RSE research},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain implants give people back what they lost.
<em>CACM</em>, <em>68</em>(2), 17–19. (<a
href="https://doi.org/10.1145/3701222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701222},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {17-19},
  shortjournal = {Commun. ACM},
  title        = {Brain implants give people back what they lost},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The evolution of computer science at the university level.
<em>CACM</em>, <em>68</em>(2), 14–16. (<a
href="https://doi.org/10.1145/3701223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701223},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {14-16},
  shortjournal = {Commun. ACM},
  title        = {The evolution of computer science at the university level},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can LLMs make robots smarter? <em>CACM</em>, <em>68</em>(2),
11–13. (<a href="https://doi.org/10.1145/3701227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701227},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {11-13},
  shortjournal = {Commun. ACM},
  title        = {Can LLMs make robots smarter?},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Go-ing to the cloud. <em>CACM</em>, <em>68</em>(2), 8–9. (<a
href="https://doi.org/10.1145/3701221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701221},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {8-9},
  shortjournal = {Commun. ACM},
  title        = {Go-ing to the cloud},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fit for people, fit for purpose: Designing tech that
matters. <em>CACM</em>, <em>68</em>(2), 7. (<a
href="https://doi.org/10.1145/3708470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708470},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {7},
  shortjournal = {Commun. ACM},
  title        = {Fit for people, fit for purpose: Designing tech that matters},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building safer and interoperable AI systems. <em>CACM</em>,
<em>68</em>(2), 5. (<a href="https://doi.org/10.1145/3709744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3709744},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {5},
  shortjournal = {Commun. ACM},
  title        = {Building safer and interoperable AI systems},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tap---1">TAP - 1</h2>
<ul>
<li><details>
<summary>
(2025). Toward understanding the effects of intelligence of a
virtual character during an immersive jigsaw puzzle co-solving task.
<em>TAP</em>, <em>22</em>(2), 1–28. (<a
href="https://doi.org/10.1145/3700822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality, creating intelligent virtual characters has been a long-lasting endeavor. However, while researchers have investigated several aspects of a virtual character’s intelligence, little attention has been paid to the impact of the implemented intelligence levels assigned to a virtual character during human–virtual character collaboration. Thus, we conducted a within-group study ( \(N=24\) ) to explore how three different intelligence levels (low vs. medium vs. high) assigned to a virtual character can impact how study participants perceive that virtual character and interact with the task they are instructed to complete. Specifically, for our study, we developed a jigsaw puzzle game and instructed our participants to solve it with the help of a virtual character. During the jigsaw puzzle solving process, we collected application logs related to how the participants executed the task and observed the virtual environment. Moreover, after each condition, we asked the participants to respond using a questionnaire that examined their social presence, how they perceived the character’s intelligence and compared it with their own, and how they rated the virtual character’s realism. Our results indicated that the different intelligence levels assigned to the virtual characters impacted participants’ responses on several variables, including co-presence, perceived intelligence, intelligence comparison, and character interaction and behavior realism. Moreover, based on the collected logged data, we found that the intelligence levels assigned to our virtual character significantly impacted the performance of our participants. Our results could be valuable to the research community for creating more engaging experiences with intelligent virtual characters for collaborative tasks in immersive environments.},
  archive      = {J_TAP},
  doi          = {10.1145/3700822},
  journal      = {ACM Transactions on Applied Perception},
  month        = {1},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Toward understanding the effects of intelligence of a virtual character during an immersive jigsaw puzzle co-solving task},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tist---8">TIST - 8</h2>
<ul>
<li><details>
<summary>
(2025). Explaining neural news recommendation with attributions onto
reading histories. <em>TIST</em>, <em>16</em>(1), 1–25. (<a
href="https://doi.org/10.1145/3673233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important aspect of responsible recommendation systems is the transparency of the prediction mechanisms. This is a general challenge for deep-learning-based systems such as the currently predominant neural news recommender architectures, which are optimized to predict clicks by matching candidate news items against users’ reading histories. Such systems achieve state-of-the-art click-prediction performance, but the rationale for their decisions is difficult to assess. At the same time, the economic and societal impact of these systems makes such insights very much desirable. In this article, we ask the question to what extent the recommendations of current news recommender systems are actually based on content-related evidence from reading histories. We approach this question from an explainability perspective. Building on the concept of integrated gradients, we present a neural news recommender that can accurately attribute individual recommendations to news items and words in input reading histories while maintaining a top scoring click-prediction performance. Using our method as a diagnostic tool, we find that: (a), a substantial number of users’ clicks on news are not explainable from reading histories, and many history-explainable items are actually skipped; (b), while many recommendations are based on content-related evidence in histories, for others the model does not attend to reasonable evidence, and recommendations stem from a spurious bias in user representations. Our code is publicly available at https://github.com/lucasmllr/xnrs .},
  archive      = {J_TIST},
  doi          = {10.1145/3673233},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Explaining neural news recommendation with attributions onto reading histories},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aspect-enhanced explainable recommendation with multi-modal
contrastive learning. <em>TIST</em>, <em>16</em>(1), 1–24. (<a
href="https://doi.org/10.1145/3673234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable recommender systems ( ERS ) aim to enhance users’ trust in the systems by offering personalized recommendations with transparent explanations. This transparency provides users with a clear understanding of the rationale behind the recommendations, fostering a sense of confidence and reliability in the system’s outputs. Generally, the explanations are presented in a familiar and intuitive way, which is in the form of natural language, thus enhancing their accessibility to users. Recently, there has been an increasing focus on leveraging reviews as a valuable source of rich information in both modeling user-item preferences and generating textual interpretations, which can be performed simultaneously in a multi-task framework. Despite the progress made in these review-based recommendation systems, the integration of implicit feedback derived from user-item interactions and user-written text reviews has yet to be fully explored. To fill this gap, we propose a model named SERMON (A s pect-enhanced E xplainable R ecommendation with M ulti-modal C o ntrast Lear n ing). Our model explores the application of multimodal contrastive learning to facilitate reciprocal learning across two modalities, thereby enhancing the modeling of user preferences. Moreover, our model incorporates the aspect information extracted from the review, which provides two significant enhancements to our tasks. Firstly, the quality of the generated explanations is improved by incorporating the aspect characteristics into the explanations generated by a pre-trained model with controlled textual generation ability. Secondly, the commonly used user-item interactions are transformed into user-item-aspect interactions, which we refer to as interaction triple, resulting in a more nuanced representation of user preference. To validate the effectiveness of our model, we conduct extensive experiments on three real-world datasets. The experimental results show that our model outperforms state-of-the-art baselines, with a 2.0% improvement in prediction accuracy and a substantial 24.5% enhancement in explanation quality for the TripAdvisor dataset.},
  archive      = {J_TIST},
  doi          = {10.1145/3673234},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Aspect-enhanced explainable recommendation with multi-modal contrastive learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness and bias in algorithmic hiring: A multidisciplinary
survey. <em>TIST</em>, <em>16</em>(1), 1–54. (<a
href="https://doi.org/10.1145/3696457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of , algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.},
  archive      = {J_TIST},
  doi          = {10.1145/3696457},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-54},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Fairness and bias in algorithmic hiring: A multidisciplinary survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extracting political interest model from interaction data
based on novel word-level bias assignment. <em>TIST</em>,
<em>16</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3702649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In democratic countries, political interest is deeply involved in people’s daily lives. Research in political consumerism shows that product purchase decision is also influenced by the political orientation of the consumer. In traditional recommendation system design, user interest in an item is provided by a unified model. Recently, interest disentanglement methods have been introduced. It is shown that by disentangling interest factors such as conformity and private interest, recommendation performance can be significantly improved. However, few studies attempt to disentangle political interest in purchase behavior, which is bipolar. In this article, we propose a method to extract political interest model from e-commerce interaction data, which is supported by a novel word-level political bias assignment. For the bias assignment part, we improved a political bias distilling method. For the political interest model extraction part, we extend a one-side bias method to make it support bipolar bias. We compare our method with state-of-the-art baseline methods in several evaluation settings, and the experimental results show that our method can achieve superior performance. Further investigation shows that our method is consistent with theories of political consumerism.},
  archive      = {J_TIST},
  doi          = {10.1145/3702649},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Extracting political interest model from interaction data based on novel word-level bias assignment},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge manipulations for the maximum vertex-weighted bipartite
b-matching. <em>TIST</em>, <em>16</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3702650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we explore the Mechanism Design aspects of the Maximum Vertex-Weighted \(b\) -matching (MVbM) problem on bipartite graphs \((A\cup T,E)\) . The set \(A\) comprises agents, while \(T\) represents tasks. The set \(E\) , which connects \(A\) and \(T\) , is the private information of either agents or tasks. In this framework, we investigate three mechanisms— \(\mathbb{M}_{BFS}\) , \(\mathbb{M}_{DFS}\) , and \(\mathbb{M}_{G}\) . We examine scenarios in which either agents or tasks are strategic and report their adjacent edges to one of the three mechanisms. In both cases, we assume that the strategic entities are bounded by their statements: They can hide edges, but they cannot report edges that do not exist. First, we consider the case in which agents can manipulate. In this framework, \(\mathbb{M}_{BFS}\) and \(\mathbb{M}_{DFS}\) are optimal but not truthful. By characterizing the Nash Equilibria induced by \(\mathbb{M}_{BFS}\) and \(\mathbb{M}_{DFS}\) , we reveal that both mechanisms have a Price of Anarchy ( \(PoA\) ) and Price of Stability ( \(PoS\) ) of \(2\) . These efficiency guarantees are tight; no deterministic mechanism can achieve a lower \(PoA\) or \(PoS\) . In contrast, the third mechanism, \(\mathbb{M}_{G}\) , is not optimal, but truthful and its approximation ratio is \(2\) . We demonstrate that this ratio is optimal; no deterministic and truthful mechanism can outperform it. We then shift our focus to scenarios where tasks can exhibit strategic behavior. In this case, \(\mathbb{M}_{BFS}\) , \(\mathbb{M}_{DFS}\) , and \(\mathbb{M}_{G}\) all maintain truthfulness, making \(\mathbb{M}_{BFS}\) and \(\mathbb{M}_{DFS}\) truthful and optimal mechanisms. In conclusion, we investigate the manipulability of \(\mathbb{M}_{BFS}\) and \(\mathbb{M}_{DFS}\) through experiments on randomly generated graphs. We observe that (i) \(\mathbb{M}_{BFS}\) is less prone to be manipulated by the first agent than \(\mathbb{M}_{DFS}\) , and (ii) \(\mathbb{M}_{BFS}\) is more manipulable on instances in which the total capacity of the agents is equal to the number of tasks. 1},
  archive      = {J_TIST},
  doi          = {10.1145/3702650},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Edge manipulations for the maximum vertex-weighted bipartite b-matching},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and accurate evacuation planning algorithm with
bayesian optimization. <em>TIST</em>, <em>16</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3704920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a method for generating an evacuation plan at a high speed to realize safe and swift evacuation in the event of a large-scale disaster such as an earthquake and its accompanying tsunami. Existing conventional methods have several problems. Simulation-based methods that use agents and methods that use existing time expansion networks have high computational costs, which makes it difficult for evacuation routes to be immediately changed according to the effects of disasters such as collapsed buildings and roads. Although heuristics with reduced calculation costs are also being researched, they may result in very long evacuation completion times and cannot generate optimal evacuation plans. We guarantee the optimal solution by reducing the number of maximum flow problem calculations, which constitute the bottleneck for methods using the existing time expansion network, through the use of the Bayesian optimization machine learning method. This reduces the calculation cost of the entire algorithm. The performance of our method is evaluated from the two viewpoints of the evacuation completion time, which indicates the quality of the evacuation plan, and the time required for the generation by the solution of the algorithm in computer experiments under multiple scenarios. In addition, the impact of the number of evacuees and the locations of the sinks are analyzed. We show that our method can quickly generate an optimal evacuation plan.},
  archive      = {J_TIST},
  doi          = {10.1145/3704920},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Fast and accurate evacuation planning algorithm with bayesian optimization},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous graph neural networks using self-supervised
reciprocally contrastive learning. <em>TIST</em>, <em>16</em>(1), 1–21.
(<a href="https://doi.org/10.1145/3706115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural network (HGNN) is a popular technique for modeling and analyzing heterogeneous graphs. Most existing HGNN-based approaches are supervised or semi-supervised learning methods requiring graphs to be annotated, which is costly and time-consuming. Self-supervised contrastive learning has been proposed to address the problem of requiring annotated data by mining intrinsic properties in the given data. However, the existing contrastive learning methods are not suitable for heterogeneous graphs because they construct contrastive views only based on data perturbation or pre-defined structural properties (e.g., meta-path) in graph data while ignoring noises in node attributes and graph topologies. We develop a robust heterogeneous graph contrastive learning approach, namely HGCL, which introduces two views on respective guidances of node attributes and graph topologies and integrates and enhances them by a reciprocally contrastive mechanism to better model heterogeneous graphs. In this new approach, we adopt distinct but suitable attribute and topology fusion mechanisms in the two views, which are conducive to mining relevant information in attributes and topologies separately. We further use both attribute similarity and topological correlation to construct high-quality contrastive samples. Extensive experiments on four large real-world heterogeneous graphs demonstrate the superiority and robustness of HGCL over several state-of-the-art methods.},
  archive      = {J_TIST},
  doi          = {10.1145/3706115},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Heterogeneous graph neural networks using self-supervised reciprocally contrastive learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tucker decomposition-enhanced dynamic graph convolutional
networks for crowd flows prediction. <em>TIST</em>, <em>16</em>(1),
1–19. (<a href="https://doi.org/10.1145/3706116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd flows prediction is an important problem for traffic management and public safety. Graph Convolutional Network (GCN), known for its ability to effectively capture and utilize topological information, has demonstrated significant advancements in addressing this problem. However, GCN-based models were often based on predefined crowd-flow graphs via historical movement behaviors of human beings and traffic vehicles, which ignored the abnormal changes in crowd flows. In this study, we propose a multi-scale fusion GCN-based framework with Tucker decomposition named mTDNet to enhance dynamic GCN for crowd flows prediction. Following the paradigm of extant methods, we also employ the predefined crowd-flow graphs as a part of mTDNet to effectively capture the historical movement behaviors of crowd flows. To capture the abnormal changes, we propose a Tucker decomposition-based network with the product of the adjacency matrix of historical movement pattern graphs and an Adaptive Learning Tensor ( ALT ) by reconstructing the crowd flows. Particularly, we utilize the Tucker decomposition scheme to decompose ALT , which enhances the dynamic learning of graph structures, allowing for effective capturing of the dynamic changes in crowd flow, including abnormal changes. Furthermore, a multi-scale 3DGCN is utilized to mine and fuse the multi-scale spatio-temporal information from crowd flows, to further boost the mTDNet prediction performance. Experiments conducted on two real-world datasets showed that the proposed mTDNet surpasses other crowd flow prediction methods.},
  archive      = {J_TIST},
  doi          = {10.1145/3706116},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Tucker decomposition-enhanced dynamic graph convolutional networks for crowd flows prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tkdd---4">TKDD - 4</h2>
<ul>
<li><details>
<summary>
(2025). Modeling on-road trajectories with multi-task learning.
<em>TKDD</em>, <em>19</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3705005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of GPS modules, there are various urban applications such as car navigation relying on trajectory data modeling. In this work, we study the problem of modeling on-road trajectories, which is to predict the next road segment given a partial GPS trajectory. Existing methods that model trajectories with Markov chain or recurrent neural network suffer from various issues, including limited capability of sequential modeling, insufficiency of incorporating the road network context, and lack of capturing the underlying semantics of trajectories. In this article, we propose a new trajectory modeling framework called Multi-task Modeling for Trajectories (MMTraj+), which avoids these issues. Specifically, MMTraj+ uses multi-head self-attention networks for sequential modeling, captures the overall road network as the context information for road segment embedding, and performs an auxiliary task of predicting the trajectory destination information (namely the ID and bearing angle) to better guide the main trajectory modeling task (controlled by a carefully designed gating mechanism). In addition, we tailor MMTraj+ for the cases where the destination information is known by dropping its auxiliary task of predicting the trajectory destination information. Extensive experiments conducted on real-world datasets demonstrate the superiority of the proposed method over the baseline methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3705005},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Modeling on-road trajectories with multi-task learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure amplification on multi-layer stochastic block
models. <em>TKDD</em>, <em>19</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3706111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much of the complexity of social, biological, and engineering systems arises from the complicated interactions among the entities in the corresponding networks. A number of network analysis tools have been successfully used to discover latent structures termed communities in such networks. However, some communities with relatively weak structures can be difficult to uncover because they are obscured by other stronger connections. To cope with this situation, our previous work proposes an algorithm called HICODE to detect and amplify the dominant and hidden community structures. In this work, we conduct a comprehensive and systematic theoretical analysis on the impact of hidden community structure and the efficacy of the HICODE algorithm, as well as provide illustrations of the detection process and results. Specifically, we define a multi-layer stochastic block model and use this model to explain why the existence of hidden structure makes the detection of dominant structure harder than equivalent random noises, which can also explain why many community detection algorithms only focusing on the dominant structure do not work well as expected. We then provide theoretical analysis that the iterative reducing methods could help to enhance the discovery of hidden structure as well as the dominant structure in the multi-layer stochastic block model for the two cases of accurate and inaccurate detection. Finally, visual simulations and experimental results are presented to show the process of HICODE algorithm and the impact of different number of layers on the detection quality.},
  archive      = {J_TKDD},
  doi          = {10.1145/3706111},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Structure amplification on multi-layer stochastic block models},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending federated recommender systems against untargeted
attacks: A contribution-aware robust aggregation scheme. <em>TKDD</em>,
<em>19</em>(1), 1–28. (<a
href="https://doi.org/10.1145/3706112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated recommender systems (FedRSs) effectively tackle the tradeoff between recommendation accuracy and privacy preservation. However, recent studies have revealed severe vulnerabilities in FedRSs, particularly against untargeted attacks seeking to undermine their overall performance. Defense methods employed in traditional recommender systems are not applicable to FedRSs, and existing robust aggregation schemes for other federated learning-based applications have proven ineffective in FedRSs. Building on the observation that malicious clients contribute negatively to the training process, we design a novel contribution-aware robust aggregation scheme to defend FedRSs against untargeted attacks, named contribution-aware Bayesian knowledge distillation aggregation (ConDA), comprising two key components for the defense. In the first contribution estimation component, we decentralize the estimation from the server side to the client side and propose an ensemble-based Shapley value to enable the efficient calculation of contributions, addressing the limitations of lacking auxiliary validation data and high computational complexity. In the second contribution-aware aggregation component, we merge the decentralized contributions via a majority voting mechanism and integrate the merged contributions into a Bayesian knowledge distillation aggregation scheme for robust aggregation, mitigating the impact of unreliable contributions induced by attacks. We evaluate the effectiveness and efficiency of ConDA on two real-world datasets from movie and music service providers. Through extensive experiments, we demonstrate the superiority of ConDA over the baseline robust aggregation schemes.},
  archive      = {J_TKDD},
  doi          = {10.1145/3706112},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Defending federated recommender systems against untargeted attacks: A contribution-aware robust aggregation scheme},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Margin-aware noise-robust contrastive learning for partially
view-aligned problem. <em>TKDD</em>, <em>19</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3707646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study a challenging problem in contrastive learning when just a portion of data is aligned in multi-view dataset due to temporal, spatial, or spatio-temporal asynchronism across views. It is important to study partially view-aligned data since this type of data is common in real-world application and easily leads to data inconsistency among different views. Such a Partially View-aligned Problem (PVP) in contrastive learning has been relatively less touched so far, especially in downstream tasks, i.e., classification and clustering. In order to solve this problem, we introduce a flexible margin and propose margin-aware noise-robust contrastive learning to simultaneously identify the within-category counterparts from the other view of one data point based on the established cross-view correspondence and learn a shared representation. To be specific, the proposed learning framework is built on a novel margin-aware noise-robust contrastive loss. Since data pairs are used as input for the proposed margin-aware noise-robust contrastive learning, we build positive pairs according to the known correspondences and negative pairs in the manner of random sampling. Our margin-aware noise-robust contrastive learning framework is able to effectively reduce or remove the impacts caused by the possible existing noise for the constructed pairs in a margin-aware manner, i.e., false negative pairs led by random sampling in PVP. We relax the proposed margin-aware noise-robust contrastive loss and then give a detailed mathematical analysis for the effectiveness of our loss. As an instantiation, we construct an example under the proposed margin-aware noise-robust contrastive learning framework for validation in this work. To the best of our knowledge, this is the first attempt of extending contrastive learning to a margin-aware noise-robust version for dealing with PVP. We also enrich the learning paradigm when there is noise in the data. Extensive experiments on different datasets demonstrate the promising performance of the proposed method in the classification and clustering tasks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3707646},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Margin-aware noise-robust contrastive learning for partially view-aligned problem},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="todaes---8">TODAES - 8</h2>
<ul>
<li><details>
<summary>
(2025). STRIVE: Empowering a low power tensor processing unit with
fault detection and error resilience. <em>TODAES</em>, <em>30</em>(2),
1–25. (<a href="https://doi.org/10.1145/3705003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid growth in Deep Neural Network (DNN) workloads has increased the energy footprint of the Artificial Intelligence (AI) computing realm. For optimum energy efficiency, we propose operating a DNN hardware in the Low-Power Computing (LPC) region. However, operating at LPC causes increased delay sensitivity to Process Variation (PV). Delay faults are an intriguing consequence of PV. In this article, we demonstrate the vulnerability of DNNs to delay variations, substantially lowering the prediction accuracy. To overcome delay faults, we present STRIVE—a post-fabrication fault detection and reactive error reduction technique. We also introduce a time-borrow correction technique to ensure error-free DNN computation.},
  archive      = {J_TODAES},
  doi          = {10.1145/3705003},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {STRIVE: Empowering a low power tensor processing unit with fault detection and error resilience},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global placement exploiting soft 2D regularity.
<em>TODAES</em>, <em>30</em>(2), 1–21. (<a
href="https://doi.org/10.1145/3705729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell placement is a step of paramount importance in chip physical design and requests relentless effort for continuous improvement. Recently, designs with two-dimensional (2D) processing element arrays have become popular primarily due to their deep neural network hardware applications. The 2D array regularity is similar to but different from the regularity of conventional datapath designs. To exploit the 2D array regularity, this work develops a new global placement technique, Placement of Arrays with SOft Regularity (PASOR), built upon RePlAce, the state-of-the-art placement framework. Experimental results from various designs show that the proposed approach can reduce global routing wirelength by 11% and 6% compared to RePlAce and a previous work on datapath driven placement, respectively.},
  archive      = {J_TODAES},
  doi          = {10.1145/3705729},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Global placement exploiting soft 2D regularity},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIMTAM: Generation diversity test programs for FPGA
simulation tools testing via timing area mutation. <em>TODAES</em>,
<em>30</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3705730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field-Programmable Gate Array (FPGA) timing simulation is essential in electronic circuit design, allowing for the verification of timing characteristics like delays and clock frequencies. However, bugs in timing simulation tools can lead to inaccurate results, potentially causing designers to miss critical issues in chip performance. Traditional testing methods often fall short in thoroughly assessing these tools, as current FPGA testing primarily focuses on synthesis and behavioral simulation, neglecting timing aspects. To address this issue, we propose SIMTAM for testing timing simulation tools. Specifically, SIMTAM consists of three components: equivalent delay region construction, diversity program segment generation, and differential testing. Given a seed circuit design file written by hardware description language such as Verilog, the delay region construction component randomly identifies delay structures for inertial delay in the design file to construct equivalent delay sleep regions. In the sleep region, the simulator skips the signal pulse whose width is less than the specified delay, thus ensuring the equivalence of the variations. The diversity program segment generation component combines Verilog expressions using generation operators and injects them into the sleep region to generate diverse design files. The differential testing component compares the seed and variant design files to find compilation inconsistency issues. In 5 months, SIMTAM reported 16 bugs to developers in two popular timing simulation tools, Iverilog and Vivado, 10 of which are confirmed.},
  archive      = {J_TODAES},
  doi          = {10.1145/3705730},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {SIMTAM: Generation diversity test programs for FPGA simulation tools testing via timing area mutation},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed learning based multiphysics simulation for
fast transient TSV electromigration analysis. <em>TODAES</em>,
<em>30</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3706106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through Silicon Vias (TSVs) are vulnerable to electromigration (EM) degradation due to their high local current densities, thereby reducing the reliability of 3D ICs with stack dies and TSVs. Due to the broad application of 3D ICs, it is necessary to analyze the electromigration reliability of TSVs. To overcome the weakness of traditional method for EM modeling of TSVs, we propose a physics-informed learning approach for transient analysis of electromigration modeling in TSV by solving the conventional mass balance equation. The proposed method allows simultaneous consideration of atomic depletion and accumulation, effective resistance degradation, electric current evolution, and stress distribution. In particular, we propose a customized neural network to simulate the EM process in TSV without the need for fine grid meshing and temporal iteration in traditional methods. Considering that the loss function of the proposed model is a combination of different loss terms, we propose a modified self-adaptive loss balanced method to automatically adjust the weights of multiple loss terms to enhance network performance. Given the prediction uncertainty due to data randomness or model architecture constraints, Gaussian probabilistic model is constructed to define the self-adaptive weights and update the dynamic weights per epoch built on maximum likelihood estimation. Compared with the finite element method, the proposed physics informed neural network method can lead to a speedup with less than 0.1% mean square error. Experimental results also show that the proposed model achieves excellent performance over other competing methods and high robustness under values of initial weights, different numbers of hidden layers and neurons per layer.},
  archive      = {J_TODAES},
  doi          = {10.1145/3706106},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Physics-informed learning based multiphysics simulation for fast transient TSV electromigration analysis},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PACE: A piece-wise approximate floating-point divider with
runtime configurability and high energy efficiency. <em>TODAES</em>,
<em>30</em>(2), 1–23. (<a
href="https://doi.org/10.1145/3706634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing emerges as a viable solution to enhance energy efficiency in applications sensitive to human perception, particularly on edge devices. This work introduces a novel piece-wise approximate floating-point divider that boasts resource efficiency and runtime configurability. Our method leverages a piece-wise approximation algorithm for computing 1/ y by exploiting powers of 2, complemented by an error compensation technique grounded in thorough mathematical analysis. This approach facilitates the realization of a reciprocal-based floating-point divider devoid of multipliers, which not only mitigates hardware resource consumption but also reduces latency. Additionally, we unveil a multi-level runtime configurable hardware architecture that significantly improves flexibility across diverse application contexts. Compared to the existing state-of-the-art approximate dividers and truncated exact dividers, our proposed solution achieves a superior compromise between precision and resource efficiency. Application-level evaluations reveal that our design provides over 87.7% energy saving while maintaining a negligible impact on output quality.},
  archive      = {J_TODAES},
  doi          = {10.1145/3706634},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {PACE: A piece-wise approximate floating-point divider with runtime configurability and high energy efficiency},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the effectiveness of STLs for GPUs via bounded
model checking. <em>TODAES</em>, <em>30</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3706635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics Processing Units (GPUs) are becoming widespread, even in safety-critical applications. In that case, it is imperative to guarantee that the probability of producing critical failures due to hardware faults is lower than a given threshold. To detect possible permanent hardware faults as soon as they appear during the operational phase (e.g., due to aging), Software Test Libraries (STLs) have gained significant traction as a widely adopted test solution due to their effectiveness in terms of fault detection capabilities, test application time, and flexibility. However, a major drawback of this solution is the lack of automation in the STL generation phase. As a result, high manual labor is required for their generation. This becomes even more arduous in complex architectures that require in-depth knowledge to cover hard-to-test faults. In this article, we introduce a methodology based on Bounded Model Checking to support the generation and improvement of stuck-at-oriented STLs for hard-to-test units in GPUs, showing that we can enhance the test coverage achieved by pre-existing STLs while also identifying a set of functionally untestable faults. To experimentally validate the proposed method’s effectiveness, we use the FlexGripPlus GPU model to target two hard-to-test units, one medium to low complexity sub-unit and one high complexity sub-unit, as study cases. For both units, we had pre-existing STLs written for the stuck-at model. Resorting to the proposed method, the STLs’ test coverage was increased by 9.57% and 2.19%, respectively. In addition, the method also identified a significant number of functionally untestable faults.},
  archive      = {J_TODAES},
  doi          = {10.1145/3706635},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Enhancing the effectiveness of STLs for GPUs via bounded model checking},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISOAcc: In-situ shift operation-based accelerator for
efficient in-SRAM multiplication. <em>TODAES</em>, <em>30</em>(2), 1–24.
(<a href="https://doi.org/10.1145/3707205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital SRAM-based CIM architectures must balance three critical factors: quantized neural network bitwidth, accuracy loss, and computational efficiency, each crucial to optimizing performance and efficiency. In Domain Specific Accelerators (DSAs), flexible and specific hardware design, when incorporated with tailored Power-of-2 (P-2) quantization schemes, addresses this issue. However, in CIMs, the absence of flexible and specific hardware to support dynamic switching between general and tailored quantization schemes hinders the adoption of efficient quantization methods. In this article, we propose the I n-situ S hift O peration based Acc elerator ( ISOAcc ) for efficient SRAM-based multiplication. The key idea is to introduce transmission gates near the SRAM array to enable the selection of bits from either the same or the neighbor line when data flows from one row to another. This functionally equals a shift operation. By configuring the transmission gates array in a cascade manner, ISOAcc can support 0 to 15-bit shift with a negligible overhead. The ISOAcc can directly leverage P-2 quantization schemes in hardware, thereby greatly reducing multiplication cycles. We have chosen five well-known neural networks to evaluate ISOAcc. The evaluations show that ISOAcc achieves an average performance improvement of 3.24× and an energy reduction of 75%, compared with the state-of-the-art (SOTA) SRAM-based CIM design, Bit-Parallel.},
  archive      = {J_TODAES},
  doi          = {10.1145/3707205},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {ISOAcc: In-situ shift operation-based accelerator for efficient in-SRAM multiplication},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harnessing machine learning in dynamic thermal management in
embedded CPU-GPU platforms. <em>TODAES</em>, <em>30</em>(2), 1–32. (<a
href="https://doi.org/10.1145/3708890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing transistor density, modern heterogeneous embedded processors often exhibit high temperature gradients due to complex application scheduling scenarios which may have missed design considerations. In many use cases, off-chip ”active” cooling solutions are considered prohibitive in such reduced form factors. Core frequency throttling by existing dynamic thermal management techniques often compromises the Quality-of-Service (QoS) and violates real-time deadlines. This necessitates the adoption of intelligent resource management that simultaneously manages both thermal and latency performance. Coupled with the complexity of modern heterogeneous multi-cores, the periodic application updates that cater to ever-changing user requirements often render model-driven thermal-aware resource allocation approaches unsuitable for heterogeneous multi-core systems. For such application-architecture scenarios, we propose a novel self-learning based resource manager using Reinforcement Learning that intelligently manipulates core frequencies and task set mappings to fulfill thermal and latency objectives. Our framework employs a data-driven system modeling technique using Gaussian Process Regression to enable efficient offline training of this learning-based resource manager to avoid challenges associated with initial online training. We evaluate the approach on a heterogeneous embedded CPU-GPU platform with real workloads and observe a significant reduction in peak operating temperature when compared to the default onboard frequency governor as well as other learning-based state-of-the-art approaches.},
  archive      = {J_TODAES},
  doi          = {10.1145/3708890},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-32},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Harnessing machine learning in dynamic thermal management in embedded CPU-GPU platforms},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tosem---4">TOSEM - 4</h2>
<ul>
<li><details>
<summary>
(2025). SimClone: Detecting tabular data clones using value
similarity. <em>TOSEM</em>, <em>34</em>(1), 1–27. (<a
href="https://doi.org/10.1145/3676961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data clones are defined as multiple copies of the same data among datasets. The presence of data clones between datasets can cause issues such as difficulties in managing data assets and data license violations when using datasets with clones to build AI software. However, detecting data clones is not trivial. The majority of the prior studies in this area rely on structural information to detect data clones (e.g., font size, column header). However, tabular datasets used to build AI software are typically stored without any structural information. In this article, we propose a novel method called SimClone for data clone detection in tabular datasets without relying on structural information. SimClone method utilizes value similarities for data clone detection. We also propose a visualization approach as a part of our SimClone method to help locate the exact position of the cloned data between a dataset pair. Our results show that our SimClone outperforms the current state-of-the-art method by at least 20% in terms of both F1-score and AUC. In addition, SimClone’s visualization component helps identify the exact location of the data clone in a dataset with a Precision@10 value of 0.80 in the top 20 true positive predictions.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3676961},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {SimClone: Detecting tabular data clones using value similarity},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the understandability of design-level security practices
in infrastructure-as-code scripts and deployment architectures.
<em>TOSEM</em>, <em>34</em>(1), 1–37. (<a
href="https://doi.org/10.1145/3691630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrastructure as Code (IaC) automates IT infrastructure deployment, which is particularly beneficial for continuous releases, for instance, in the context of microservices and cloud systems. Despite its flexibility in application architecture, neglecting security can lead to vulnerabilities. The lack of comprehensive architectural security guidelines for IaC poses challenges in adhering to best practices. We studied how developers interpret IaC scripts (source code) in two IaC technologies, Ansible and Terraform, compared to semi-formal IaC deployment architecture models and metrics regarding design-level security understanding. In a controlled experiment involving ninety-four participants, we assessed the understandability of IaC-based deployment architectures through source code inspection compared to semi-formal representations in models and metrics. We hypothesized that providing semi-formal IaC deployment architecture models and metrics as supplementary material would significantly improve the comprehension of IaC security-related practices, as measured by task correctness . Our findings suggest that semi-formal IaC deployment architecture models and metrics as supplementary material enhance the understandability of IaC security-related practices without significantly increasing duration . We also observed a significant correlation between task correctness and duration when models and metrics were provided.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3691630},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the understandability of design-level security practices in infrastructure-as-code scripts and deployment architectures},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision support model for selecting the optimal blockchain
oracle platform: An evaluation of key factors. <em>TOSEM</em>,
<em>34</em>(1), 1–35. (<a
href="https://doi.org/10.1145/3697011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contract-based applications are executed in a blockchain environment, and they cannot directly access data from external systems, which is required for the service provision of these applications. Instead, smart contracts use agents known as blockchain oracles to collect and provide data feeds to the contracts. The functionality and compatibility with smart contract applications need to be considered when selecting the best-fit oracle platform. As the number of oracle alternatives and their features increases, the decision-making process becomes increasingly complex. Selecting the wrong or sub-optimal oracle is costly and may lead to severe security risks. This article provides a decision support model for the oracle selection problem. The model supports smart contract decision-makers in selecting a secure, cost-effective, and feasible oracle platform for their applications. We interviewed oracle co-founders and smart contracts experts to refine and validate the decision model. Two real-world smart contract application case studies were used to evaluate the model. Our model prioritises and suggests more than one possible oracle platform based on the developer’s required criteria, security assessment and cost analysis. Moreover, this guided decision model serves to reveal issues that may go unnoticed if done haphazardly, reduce decision-making efforts and provide a cost-effective solution.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3697011},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Decision support model for selecting the optimal blockchain oracle platform: An evaluation of key factors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiPri: Distance-based seed prioritization for greybox
fuzzing—RCR report. <em>TOSEM</em>, <em>34</em>(1), 1–13. (<a
href="https://doi.org/10.1145/3701298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This replicated computational results (RCR) report describes how to (1) set up DiPri and (2) replicate the experimental results. The primary artifact is the C/C++ prototype of DiPri , which is essentially an extension of the state-of-the-art greybox fuzzer AFL++ (version 4.06). Other artifacts include the Java implementation of DiPri on Zest, the materials for integrating DiPri into FuzzBench and Magma, and the scripts for running docker and processing data. All artifacts can be found at our GitHub repository 1 and Zenodo archive. 2},
  archive      = {J_TOSEM},
  doi          = {10.1145/3701298},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DiPri: Distance-based seed prioritization for greybox Fuzzing—RCR report},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
