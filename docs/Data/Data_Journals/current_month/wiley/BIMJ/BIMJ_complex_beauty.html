<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIMJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="bimj---11">BIMJ - 11</h2>
<ul>
<li><details>
<summary>
(2025). Toward power analysis for partial least squares-based
methods. <em>BIMJ</em>, <em>67</em>(2), e70050. (<a
href="https://doi.org/10.1002/bimj.70050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, power analysis has become widely used in applied sciences, with the increasing importance of the replicability issue. When distribution-free methods, such as partial least squares (PLS)-based approaches, are considered, formulating power analysis is challenging. In this study, we introduce the methodological framework of a new procedure for performing power analysis when PLS-based methods are used. Data are simulated by the Monte Carlo method, assuming the null hypothesis of no effect is false and exploiting the latent structure estimated by PLS in the pilot data. In this way, the complex correlation data structure is explicitly considered in power analysis and sample size estimation. The paper offers insights into selecting test statistics for the power analysis procedure, comparing accuracy-based tests and those based on continuous parameters estimated by PLS. Simulated and real data sets are investigated to show how the method works in practice.},
  archive      = {J_BIMJ},
  author       = {Angela Andreella and Livio Finos and Bruno Scarpa and Matteo Stocchero},
  doi          = {10.1002/bimj.70050},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70050},
  shortjournal = {Bio. J.},
  title        = {Toward power analysis for partial least squares-based methods},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new frequentist implementation of the daniels and hughes
bivariate meta-analysis model for surrogate endpoint evaluation.
<em>BIMJ</em>, <em>67</em>(2), e70048. (<a
href="https://doi.org/10.1002/bimj.70048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate endpoints are used when the primary outcome is difficult to measure accurately. Determining if a measure is suitable to use as a surrogate endpoint is a challenging task and a variety of meta-analysis models have been proposed for this purpose. The Daniels and Hughes bivariate model for trial-level surrogate endpoint evaluation is gaining traction but presents difficulties for frequentist estimation and hitherto only Bayesian solutions have been available. This is because the marginal model is not a conventional linear model and the number of unknown parameters increases at the same rate as the number of studies. This second property raises immediate concerns that the maximum likelihood estimator of the model&#39;s unknown variance component may be downwardly biased. We derive maximum likelihood estimating equations to motivate a bias adjusted estimator of this parameter. The bias correction terms in our proposed estimating equation are easily computed and have an intuitively appealing algebraic form. A simulation study is performed to illustrate how this estimator overcomes the difficulties associated with maximum likelihood estimation. We illustrate our methods using two contrasting examples from oncology.},
  archive      = {J_BIMJ},
  author       = {Dan Jackson and Michael Sweeting and Robbie C. M. van Aert and Sylwia Bujkiewicz and Keith R. Abrams and Wolfgang Viechtbauer},
  doi          = {10.1002/bimj.70048},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70048},
  shortjournal = {Bio. J.},
  title        = {A new frequentist implementation of the daniels and hughes bivariate meta-analysis model for surrogate endpoint evaluation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On sample size determination for augmented tests based on
restricted mean survival time in randomized clinical trials.
<em>BIMJ</em>, <em>67</em>(2), e70046. (<a
href="https://doi.org/10.1002/bimj.70046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restricted mean survival time (RMST) is gaining attention as a measure to quantify the treatment effect on survival outcomes in randomized clinical trials. Several methods to determine sample size based on the RMST-based tests have been proposed. However, to the best of our knowledge, there is no discussion about the power and sample size regarding the augmented version of RMST-based tests, which utilize baseline covariates for a gain in estimation efficiency and in power for testing no treatment effect. The conventional event-driven study design based on the logrank test allows us to calculate the power for a given hazard ratio without specifying the survival functions. In contrast, the existing sample size determination methods for the RMST-based tests relies on the adequacy of the assumptions of the entire survival curves of two groups. Furthermore, to handle the augmented test, the correlation between the baseline covariates and the martingale residuals must be handled. To address these issues, we propose an approximated sample size formula for the augmented version of the RMST-based test, which does not require specifying the entire survival curve in the treatment group, and also a sample size recalculation approach to update the correlations between the baseline covariates and the martingale residuals with the blinded data. The proposed procedure will enable the studies to have the target power for a given RMST difference even when correct survival functions cannot be specified at the design stage.},
  archive      = {J_BIMJ},
  author       = {Satoshi Hattori and Hajime Uno},
  doi          = {10.1002/bimj.70046},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70046},
  shortjournal = {Bio. J.},
  title        = {On sample size determination for augmented tests based on restricted mean survival time in randomized clinical trials},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-arm crossover randomized controlled trial versus
meta-analysis of n-of-1 studies: Comparison of statistical efficiency in
determining an intervention effect. <em>BIMJ</em>, <em>67</em>(2),
e70045. (<a href="https://doi.org/10.1002/bimj.70045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {N-of-1 trials are currently receiving broader attention in healthcare research when assessing the effectiveness of interventions. In contrast to the most commonly applied two-arm randomized controlled trial (RCT), in an N-of-1 design, the individual acts as their own control condition in the sense of a multiple crossover trial. N-of-1 trials can lead to a higher quality of patient by examining the effectiveness of an intervention at an individual level. Moreover, when a series of N-of-1 trials are properly aggregated, it becomes possible to detect an intervention effect at a population level. This work investigates whether a meta-analysis of summary data of a series of N-of-1 trials allows us to detect a statistically significant intervention effect with fewer participants than in a traditional, prospectively powered two-arm RCT and crossover design when evaluating a digital health intervention in cardiovascular care. After introducing these different analysis approaches, we compared the empirical properties in a simulation study both under the null hypothesis and with respect to power with different between-subject heterogeneity settings and in the presence of a carry-over effect. We further investigate the performance of a sequential aggregation procedure. In terms of simulated power, the threshold of 80% was achieved earlier for the aggregating procedure, requiring fewer participants.},
  archive      = {J_BIMJ},
  author       = {Anna Eleonora Carrozzo and Georg Zimmermann and Arne C. Bathke and Daniel Neunhaeuserer and Josef Niebauer and Stefan T. Kulnik},
  doi          = {10.1002/bimj.70045},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70045},
  shortjournal = {Bio. J.},
  title        = {Two-arm crossover randomized controlled trial versus meta-analysis of N-of-1 studies: Comparison of statistical efficiency in determining an intervention effect},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The shared weighted lindley frailty model for clustered
failure time data. <em>BIMJ</em>, <em>67</em>(2), e70044. (<a
href="https://doi.org/10.1002/bimj.70044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary goal of this paper is to introduce a novel frailty model based on the weighted Lindley (WL) distribution for modeling clustered survival data. We study the statistical properties of the proposed model. In particular, the amount of unobserved heterogeneity is directly parameterized by the variance of the frailty distribution such as gamma and inverse Gaussian frailty models. Parametric and semiparametric versions of the WL frailty model are studied. A simple expectation–maximization (EM) algorithm is proposed for parameter estimation. Simulation studies are conducted to evaluate its finite sample performance. Finally, we apply the proposed model to a real data set to analyze times after surgery in patients diagnosed with infiltrating ductal carcinoma and compare our results with classical frailty models carried out in this application, which shows the superiority of the proposed model. We implement an R package that includes estimation for fitting the proposed model based on the EM algorithm.},
  archive      = {J_BIMJ},
  author       = {Diego I. Gallardo and Marcelo Bourguignon and John L. Santibáñez},
  doi          = {10.1002/bimj.70044},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70044},
  shortjournal = {Bio. J.},
  title        = {The shared weighted lindley frailty model for clustered failure time data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wavelet-mixed landmark survival models for the effect of
short-term changes of potassium in heart failure patients.
<em>BIMJ</em>, <em>67</em>(2), e70043. (<a
href="https://doi.org/10.1002/bimj.70043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods to study the association between a longitudinal biomarker and the risk of death are very relevant for the long-term care of subjects affected by chronic illnesses, such as potassium in heart failure patients. Particularly in the presence of comorbidities or pharmacological treatments, sudden crises can cause potassium to undergo very abrupt yet transient changes. In the context of the monitoring of potassium, there is a need for a dynamic model that can be used in clinical practice to assess the risk of death related to an observed patient&#39;s potassium trajectory. We considered different landmark survival approaches, starting from the simple approach considering the most recent measurement. We then propose a novel method based on wavelet filtering and landmarking to retrieve the prognostic role of past short-term potassium shifts. We argue that while taking into account the smooth changes in the biomarker, short-term changes cannot be overlooked. State-of-the-art dynamic survival models are prone to give more importance to the smooth component of the potassium profiles. However, our findings suggest that it is essential to also take into account recent potassium instability to capture all the relevant prognostic information. The data used comes from over 2000 subjects, with a total of over 80,000 repeated potassium measurements collected through administrative health records. The proposed wavelet landmark method revealed the prognostic role of past short-term changes in potassium. We also performed a simulation study to assess how and when to apply the proposed wavelet-mixed landmark model.},
  archive      = {J_BIMJ},
  author       = {Caterina Gregorio and Giulia Barbati and Arjuna Scagnetto and Andrea di Lenarda and Francesca Ieva},
  doi          = {10.1002/bimj.70043},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70043},
  shortjournal = {Bio. J.},
  title        = {Wavelet-mixed landmark survival models for the effect of short-term changes of potassium in heart failure patients},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stacking model-based classifiers for dealing with multiple
sets of noisy labels. <em>BIMJ</em>, <em>67</em>(2), e70042. (<a
href="https://doi.org/10.1002/bimj.70042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised learning in presence of multiple sets of noisy labels is a challenging task that is receiving increasing interest in the ever-evolving landscape of healthcare analytics. Such an issue arises when multiple annotators are tasked to manually label the same training samples, potentially giving rise to discrepancies in class assignments among the supplied labels with respect to the ground truth. Commonly, the labeling process is entrusted to a small group of domain experts, and different level of experience and subjectivity may result in noisy training labels. To solve the classification task leveraging on the availability of multiple data annotators, we introduce a novel ensemble methodology constructed combining model-based classifiers separately trained on single sets of noisy labels. Eigenvalue Decomposition Discriminant Analysis is employed for the definition of the base learners, and six distinct averaging strategies are proposed to combine them. Two solutions necessitate a priori information, such as the partial knowledge of the ground truth labels or the annotators&#39; level of expertise. Differently, the remaining four approaches are entirely data-driven. A simulation study and an application on real data showcase the improved predictive performance of our proposal, while also demonstrating the ability of automatically inferring annotators&#39; expertise level as a by-product of the learning process.},
  archive      = {J_BIMJ},
  author       = {Giulia Montani and Andrea Cappozzo},
  doi          = {10.1002/bimj.70042},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70042},
  shortjournal = {Bio. J.},
  title        = {Stacking model-based classifiers for dealing with multiple sets of noisy labels},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survivor average causal effects for continuous time: A
principal stratification approach to causal inference with semicompeting
risks. <em>BIMJ</em>, <em>67</em>(2), e70041. (<a
href="https://doi.org/10.1002/bimj.70041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semicompeting risks problems, nonterminal time-to-event outcomes, such as time to hospital readmission, are subject to truncation by death. These settings are often modeled with illness-death models for the hazards of the terminal and nonterminal events, but evaluating causal treatment effects with hazard models is problematic due to conditioning on survival—a posttreatment outcome—that is embedded in the definition of a hazard. Extending an existing survivor average causal effect (SACE) estimand, we frame the evaluation of treatment effects in the context of semicompeting risks with principal stratification and introduce two new causal estimands: the time-varying survivor average causal effect (TV-SACE) and the restricted mean survivor average causal effect (RM-SACE). These principal causal effects are defined among units that would survive regardless of assigned treatment. We adopt a Bayesian estimation procedure that parameterizes illness-death models for both treatment arms. We outline a frailty specification that can accommodate within-person correlation between nonterminal and terminal event times, and we discuss potential avenues for adding model flexibility. The method is demonstrated in the context of hospital readmission among late-stage pancreatic cancer patients.},
  archive      = {J_BIMJ},
  author       = {Leah Comment and Fabrizia Mealli and Sebastien Haneuse and Corwin M. Zigler},
  doi          = {10.1002/bimj.70041},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70041},
  shortjournal = {Bio. J.},
  title        = {Survivor average causal effects for continuous time: A principal stratification approach to causal inference with semicompeting risks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatially informed nonnegative matrix trifactorization for
coclustering mass spectrometry data. <em>BIMJ</em>, <em>67</em>(2),
e70031. (<a href="https://doi.org/10.1002/bimj.70031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mass spectrometry imaging techniques measure molecular abundance in a tissue sample at a cellular resolution, all while preserving the spatial structure of the tissue. This kind of technology offers a detailed understanding of the role of several molecular factors in biological systems. For this reason, the development of fast and efficient computational methods that can extract relevant signals from massive experiments has become necessary. A key goal in mass spectrometry data analysis is the identification of molecules with similar functions in the analyzed biological system. This result can be achieved by studying the spatial distribution of the molecules&#39; abundance patterns. To do so, one can perform coclustering, that is, dividing the molecules into groups according to their expression patterns over the tissue and segmenting the tissue according to the molecules&#39; abundance levels. We present TRIFASE, a semi-nonnegative matrix trifactorization technique that performs coclustering while accounting for the spatial correlation of the data. We propose an estimation algorithm that solves the proposed matrix trifactorization problem. Moreover, to improve scalability, we also propose two heuristic approximations of the most expensive steps, which help the algorithm converge while significantly streamlining the computational cost. We validated our method on a series of simulation experiments, comparing the different estimating strategies discussed in the article. Last, we analyzed a mouse brain tissue sample processed with MALDI-MSI technology, showing how TRIFASE extracts specific expression patterns of molecule abundance in localized tissue areas and discovers blocks of proteins whose activation is directly linked to specific biological mechanisms.},
  archive      = {J_BIMJ},
  author       = {Andrea Sottosanti and Francesco Denti and Stefania Galimberti and Davide Risso and Giulia Capitoli},
  doi          = {10.1002/bimj.70031},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70031},
  shortjournal = {Bio. J.},
  title        = {Spatially informed nonnegative matrix trifactorization for coclustering mass spectrometry data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified conditional borrowing-by-part power prior for
dynamic and parameter-specific information borrowing of the gaussian
endpoint. <em>BIMJ</em>, <em>67</em>(2), e70029. (<a
href="https://doi.org/10.1002/bimj.70029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Borrowing external controls to augment the concurrent control arm is a popular topic in clinical trials. Bayesian dynamic borrowing methods adaptively discount external controls according to prior-data conflict. For the Gaussian endpoint, parameter-specific information borrowing enables differential discounting between the population mean and variance. The borrowing-by-part power prior employs two power parameters to separately downweight external likelihoods concerning the sample mean and variance. However, within the fully Bayesian framework, the posterior inference of the average treatment effect (ATE) defined as the population mean difference is significantly affected by the variance-specific prior-data conflict that reflects the heterogeneity of population variance. Here, we propose the modified conditional borrowing-by-part power prior (MCBPP) that separately discounts the external sample mean and variance according to parameter-specific prior-data conflicts, resulting in a more stable posterior estimation of ATE than its competitors under the same degree of mean-specific prior-data conflict. By fully discounting the external sample variance, the robust MCBPP (rMCBPP) can yield robust posterior inference of ATE against the variance-specific prior-data conflict. Although the population variance is considered a nuisance parameter, its homogeneity is equally important to justify information borrowing. We recommend the rMCBPP for borrowing external controls with a similar sample variance to concurrent controls because it exhibits better control of bias and Type I error rate than the modified power prior (MPP) assuming unknown variance in the absence of population variance heterogeneity. However, when faced with a significant sample variance discrepancy, the MPP assuming unknown variance is preferred given its better performance under severe population variance heterogeneity.},
  archive      = {J_BIMJ},
  author       = {Kai Wang and Han Cao and Chen Yao},
  doi          = {10.1002/bimj.70029},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70029},
  shortjournal = {Bio. J.},
  title        = {Modified conditional borrowing-by-part power prior for dynamic and parameter-specific information borrowing of the gaussian endpoint},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric estimation of a biometric function using
ranked set sampling with ties information. <em>BIMJ</em>,
<em>67</em>(2), e70007. (<a
href="https://doi.org/10.1002/bimj.70007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual life (MRL) function plays an important role in the summary and analysis of survival data. The main advantage of this function is that it summarizes the information in units of time instead of a probability scale, which requires careful interpretation. Ranked set sampling (RSS) is a sampling technique designed for situations, where obtaining precise measurements of sample units is expensive or difficult, but ranking them without referring to their accurate values is cost-effective or easy. However, the practical application of RSS is hindered because each sample unit is required to assign a unique rank. To alleviate this difficulty, Frey developed a novel variation of RSS, called RSS-t, that records and utilizes the tie structure in the ranking process. In this paper, we propose several different nonparametric estimators for the MRL function based on RSS-t. Then, we compare the proposed estimators with their counterparts in simple random sampling (SRS) and RSS, where tie information is not utilized. We also implemented our proposed estimators on a real data set related to patient waiting times for liver transplantation, to show their applicability and efficiency in practice. Our results show that using ties information leads to an improved statistical inference for the MRL function, and therefore a smaller sample size is needed to reach a predetermined precision.},
  archive      = {J_BIMJ},
  author       = {Leila Jabari Koopaei and Ehsan Zamanzade and Afshin Parvardeh and Xinlei Wang},
  doi          = {10.1002/bimj.70007},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70007},
  shortjournal = {Bio. J.},
  title        = {Nonparametric estimation of a biometric function using ranked set sampling with ties information},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
