<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim---25">SIM - 25</h2>
<ul>
<li><details>
<summary>
(2025). Group sequential test for two-sample ordinal outcome
measures. <em>SIM</em>, <em>44</em>(6), e70053. (<a
href="https://doi.org/10.1002/sim.70053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group sequential trials include interim monitoring points to potentially reach futility or efficacy decisions early. This approach to trial design can safeguard patients, provide efficacious treatments for patients early, and save money and time. Group sequential methods are well developed for bell-shaped continuous, binary, and time-to-event outcomes. In this paper, we propose a group sequential design using the Mann-Whitney-Wilcoxon test for general two-sample ordinal data. We establish that the proposed test statistic has asymptotic normality and that sequential statistics satisfy the assumptions of Brownian motion. We also include results of finite sample simulation studies that show our proposed approach has the advantage over existing methods for controlling Type I errors while maintaining power for small sample sizes. A real data set is used to illustrate the proposed method and a sample size calculation approach is proposed for designing new studies.},
  archive      = {J_SIM},
  author       = {Yuan Wu and Ryan A. Simmons and Baoshan Zhang and Jesse D. Troy},
  doi          = {10.1002/sim.70053},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70053},
  shortjournal = {Stat. Med.},
  title        = {Group sequential test for two-sample ordinal outcome measures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive regression trees for group testing data.
<em>SIM</em>, <em>44</em>(6), e70052. (<a
href="https://doi.org/10.1002/sim.70052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When screening for low-prevalence diseases, pooling specimens (e.g., blood, urine, swabs, etc.) through group testing has the potential to substantially reduce costs when compared to testing specimens individually. A common goal in group testing applications is to estimate the relationship between an individual&#39;s true disease status and their individual-level covariate information. However, estimating such a relationship is a non-trivial problem because true individual disease statuses are unknown due to the group testing protocol and the possibility of imperfect testing. While several regression methods have been developed in recent years to accommodate the complexity of group testing data, the functional form of covariate effects is typically assumed to be known. To avoid model misspecification and to provide a more flexible approach, we propose a Bayesian additive regression trees framework to model the individual-level probability of disease with potentially misclassified group testing data. Our methods can be used to analyze data arising from any group testing protocol with the goal of estimating unknown functions of covariates and assay classification accuracy probabilities.},
  archive      = {J_SIM},
  author       = {Madeleine E. St. Ville and Christopher S. McMahan and Joe D. Bible and Joshua M. Tebbs and Christopher R. Bilder},
  doi          = {10.1002/sim.70052},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70052},
  shortjournal = {Stat. Med.},
  title        = {Bayesian additive regression trees for group testing data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive weight selection for time-to-event data under
non-proportional hazards. <em>SIM</em>, <em>44</em>(6), e70045. (<a
href="https://doi.org/10.1002/sim.70045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When planning a clinical trial for a time-to-event endpoint, we require an estimated effect size and need to consider the type of effect. Usually, an effect of proportional hazards is assumed with the hazard ratio as the corresponding effect measure. Thus, the standard procedure for survival data is generally based on a single-stage log-rank test. Knowing that the assumption of proportional hazards is often violated and sufficient knowledge to derive reasonable effect sizes is usually unavailable, such an approach is relatively rigid. We introduce a more flexible procedure by combining two methods designed to be more robust in case we have little to no prior knowledge. First, we employ a more flexible adaptive multi-stage design instead of a single-stage design. Second, we apply combination-type tests in the first stage of our suggested procedure to benefit from their robustness under uncertainty about the deviation pattern. We can then use the data collected during this period to choose a more specific single-weighted log-rank test for the subsequent stages. In this step, we employ Royston-Parmar spline models to extrapolate the survival curves to make a reasonable decision. Based on a real-world data example, we show that our approach can save a trial that would otherwise end with an inconclusive result. Additionally, our simulation studies demonstrate a sufficient power performance while maintaining more flexibility.},
  archive      = {J_SIM},
  author       = {Moritz Fabian Danzer and Ina Dormuth},
  doi          = {10.1002/sim.70045},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70045},
  shortjournal = {Stat. Med.},
  title        = {Adaptive weight selection for time-to-event data under non-proportional hazards},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new semiparametric power-law regression model with
long-term survival, change-point detection and regularization.
<em>SIM</em>, <em>44</em>(6), e70043. (<a
href="https://doi.org/10.1002/sim.70043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney cancer, a potentially life-threatening malignancy affecting the kidneys, demands early detection and proactive intervention to enhance prognosis and survival. Advancements in medical and health sciences and the emergence of novel treatments are expected to lead to a favorable response in a subset of patients. This, in turn, is anticipated to enhance overall survival and disease-free survival rates. Cure fraction models have become essential for estimating the proportion of individuals considered cured and free from adverse events. This article presents a novel piecewise power-law cure fraction model with a piecewise decreasing hazard function, deviating from the traditional piecewise constant hazard assumption. By analyzing real medical data, we evaluate various factors to explain the survival of individuals. Consistently, positive outcomes are observed, affirming the significant potential of our approach. Furthermore, we use a local influence analysis to detect potentially influential individuals and perform a postdeletion analysis to analyze their impact on our inferences.},
  archive      = {J_SIM},
  author       = {Nixon Jerez-Lillo and Alejandra Tapia and Victor Hugo Lachos and Pedro Luiz Ramos},
  doi          = {10.1002/sim.70043},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70043},
  shortjournal = {Stat. Med.},
  title        = {A new semiparametric power-law regression model with long-term survival, change-point detection and regularization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential monitoring of covariate-adaptive randomized
clinical trials with non-parametric approaches. <em>SIM</em>,
<em>44</em>(6), e70042. (<a
href="https://doi.org/10.1002/sim.70042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of covariate adjustment in clinical trials has been underscored by the U.S. FDA&#39;s guidance. Inference, with or without covariates, after implementing covariate adaptive randomization (CAR), is garnering increased interest. This paper investigates the sequential monitoring of covariate-adaptive randomized clinical trials through non-parametric methods, a critical advancement for enhancing the precision and efficiency of medical research. CAR, which incorporates baseline patient characteristics into the randomization process, aims to mitigate the risk of confounding and improve the balance of covariates across treatment groups, thereby addressing patients&#39; heterogeneity. Although CAR is known for its benefits in reducing biases and enhancing statistical power, its integration into sequentially monitored clinical trials—a standard practice—poses methodological challenges, particularly in controlling the type I error rate. By employing a non-parametric approach, we demonstrate through theoretical proofs and numerical analyses that our methods effectively control the type I error rate and surpass traditional randomization and analysis methods. This paper not only fills a gap in the literature on sequential monitoring of CAR without model misspecification but also proposes practical solutions for enhancing trial design and analysis, thereby contributing significantly to the field of clinical research.},
  archive      = {J_SIM},
  author       = {Xiaotian Chen and Jun Yu and Hongjian Zhu and Li Wang},
  doi          = {10.1002/sim.70042},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70042},
  shortjournal = {Stat. Med.},
  title        = {Sequential monitoring of covariate-adaptive randomized clinical trials with non-parametric approaches},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can the unit size predict outcomes? Testing for
informativeness in three-level designs. <em>SIM</em>, <em>44</em>(6),
e70041. (<a href="https://doi.org/10.1002/sim.70041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilevel data are frequently encountered in biomedical research, and several statistical methods have been developed to analyze such data. Informativeness of the number of units on certain levels often manifests itself in multilevel data analysis and failure to account for this phenomenon will lead to biased inference. Moreover, utilizing an incorrect marginalization approach will also lead to invalid conclusions. To identify the appropriate marginal distribution to be tested in multilevel designs, we propose a sequential testing procedure to test for informativeness of unit sizes in multilevel structures with three levels. At a given level of the design, a bootstrap method is developed to estimate the null distribution of no informativeness of unit size. Simulation studies confirm the efficacy of our sequential procedure in maintaining an overall Type I error rate. Additionally, we extend our testing procedure to a multilevel regression setting, enhancing its practical applicability. We demonstrate the utility of our proposed methods through the analysis of data from a study on periodontal disease and a study on stress levels of preschoolers.},
  archive      = {J_SIM},
  author       = {Samuel Anyaso-Samuel and Somnath Datta and Eva Roos and Jaakko Nevalainen},
  doi          = {10.1002/sim.70041},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70041},
  shortjournal = {Stat. Med.},
  title        = {Can the unit size predict outcomes? testing for informativeness in three-level designs},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pattern mixture sensitivity analyses via multiple
imputations for non-ignorable dropout in joint modeling of cognition and
risk of dementia. <em>SIM</em>, <em>44</em>(6), e70040. (<a
href="https://doi.org/10.1002/sim.70040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the Swedish Betula study, we consider the joint modeling of longitudinal memory assessments and the hazard of dementia. In the Betula data, the time-to-dementia onset or its absence is available for all participants, while some memory measurements are missing. In longitudinal studies of aging, one cannot rule out the possibility of dropout due to health issues resulting in missing not at random longitudinal measurements. We, therefore, propose a pattern-mixture sensitivity analysis for missing not-at-random data in the joint modeling framework. The sensitivity analysis is implemented via multiple imputation as follows: (i) multiply impute missing not at random longitudinal measurements under a set of plausible pattern-mixture imputation models that allow for acceleration of memory decline after dropout, (ii) fit the joint model to each imputed longitudinal memory and time-to-dementia dataset, and (iii) combine the results of step (ii). Our work illustrates that sensitivity analyses via multiple imputations are an accessible, pragmatic method to evaluate the consequences of missing not at-random data on inference and prediction. This flexible approach can accommodate a range of models for the longitudinal and event-time processes. In particular, the pattern-mixture modeling approach provides an accessible way to frame plausible missing not at random assumptions for different missing data patterns. Applying our approach to the Betula study shows that worse memory levels and steeper memory decline were associated with a higher risk of dementia for all considered scenarios.},
  archive      = {J_SIM},
  author       = {Tetiana Gorbach and James R. Carpenter and Chris Frost and Maria Josefsson and Jennifer Nicholas and Lars Nyberg},
  doi          = {10.1002/sim.70040},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70040},
  shortjournal = {Stat. Med.},
  title        = {Pattern mixture sensitivity analyses via multiple imputations for non-ignorable dropout in joint modeling of cognition and risk of dementia},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating additional evidence as prior information to
resolve non-identifiability in bayesian disease model calibration: A
tutorial. <em>SIM</em>, <em>44</em>(6), e70039. (<a
href="https://doi.org/10.1002/sim.70039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease models are used to examine the likely impact of therapies, interventions, and public policy changes. Ensuring that these are well calibrated on the basis of available data and that the uncertainty in their projections is properly quantified is an important part of the process. The question of non-identifiability poses a challenge to disease model calibration where multiple parameter sets generate identical model outputs. For statisticians evaluating the impact of policy interventions such as screening or vaccination, this is a critical issue. This study explores the use of the Bayesian framework to provide a natural way to calibrate models and address non-identifiability in a probabilistic fashion in the context of disease modeling. We present Bayesian approaches for incorporating expert knowledge and external data to ensure that appropriately informative priors are specified on the joint parameter space. These approaches are applied to two common disease models: a basic susceptible-infected-susceptible (SIS) model and a much more complex agent-based model which has previously been used to address public policy questions in HPV and cervical cancer. The conditions that allow the problem of non-identifiability to be resolved are demonstrated for the SIS model. For the larger HPV model, an overview of the findings is presented, but of key importance is a discussion on how the non-identifiability impacts the calibration process. Through case studies, we demonstrate how informative priors can help resolve non-identifiability and improve model inference. We also discuss how sensitivity analysis can be used to assess the impact of prior specifications on model results. Overall, this work provides an important tutorial for researchers interested in applying Bayesian methods to calibrate models and handle non-identifiability in disease models.},
  archive      = {J_SIM},
  author       = {Daria Semochkina and Cathal D. Walsh},
  doi          = {10.1002/sim.70039},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70039},
  shortjournal = {Stat. Med.},
  title        = {Incorporating additional evidence as prior information to resolve non-identifiability in bayesian disease model calibration: A tutorial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference in presence of intra-patient correlation
due to repeated measurements of exposure and outcome in longitudinal
settings. <em>SIM</em>, <em>44</em>(6), e70037. (<a
href="https://doi.org/10.1002/sim.70037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Antoine Gavoille and Fabien Rollot and Romain Casey and Sandra Vukusic and Muriel Rabilloud and Fabien Subtil},
  doi          = {10.1002/sim.70037},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70037},
  shortjournal = {Stat. Med.},
  title        = {Causal inference in presence of intra-patient correlation due to repeated measurements of exposure and outcome in longitudinal settings},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spline-based approach to smoothly constrain hazard ratios
with a view to apply treatment effect waning. <em>SIM</em>,
<em>44</em>(6), e70035. (<a
href="https://doi.org/10.1002/sim.70035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Angus C. Jennings and Mark J. Rutherford and Paul C. Lambert},
  doi          = {10.1002/sim.70035},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70035},
  shortjournal = {Stat. Med.},
  title        = {A spline-based approach to smoothly constrain hazard ratios with a view to apply treatment effect waning},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guidelines and best practices for the use of targeted
maximum likelihood and machine learning when estimating causal effects
of exposures on time-to-event outcomes. <em>SIM</em>, <em>44</em>(6),
e70034. (<a href="https://doi.org/10.1002/sim.70034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeted maximum likelihood estimation (TMLE) is an increasingly popular framework for the estimation of causal effects. It requires modeling both the exposure and outcome but is doubly robust in the sense that it is valid if at least one of these models is correctly specified. In addition, TMLE allows for flexible modeling of both the exposure and outcome with machine learning methods. This provides better control for measured confounders since the model specification automatically adapts to the data, instead of needing to be specified by the analyst a priori . Despite these methodological advantages, TMLE remains less popular than alternatives in part because of its less accessible theory and implementation. While some tutorials have been proposed, none address the case of a time-to-event outcome. This tutorial provides a detailed step-by-step explanation of the implementation of TMLE for estimating the effect of a point binary or multilevel exposure on a time-to-event outcome, modeled as counterfactual survival curves and causal hazard ratios. The tutorial also provides guidelines on how best to use TMLE in practice, including aspects related to study design, choice of covariates, controlling biases and use of machine learning. R-code is provided to illustrate each step using simulated data ( https://github.com/detal9/SurvTMLE ). To facilitate implementation, a general R function implementing TMLE with options to use machine learning is also provided. The method is illustrated in a real-data analysis concerning the effectiveness of statins for the prevention of a first cardiovascular disease among older adults in Québec, Canada, between 2013 and 2018.},
  archive      = {J_SIM},
  author       = {Denis Talbot and Awa Diop and Miceline Mésidor and Yohann Chiu and Caroline Sirois and Andrew J. Spieker and Antoine Pariente and Pernelle Noize and Marc Simard and Miguel Angel Luque Fernandez and Michael Schomaker and Kenji Fujita and Danijela Gnjidic and Mireille E. Schnitzer},
  doi          = {10.1002/sim.70034},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70034},
  shortjournal = {Stat. Med.},
  title        = {Guidelines and best practices for the use of targeted maximum likelihood and machine learning when estimating causal effects of exposures on time-to-event outcomes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing coarsened and missing data by imputation methods.
<em>SIM</em>, <em>44</em>(6), e70032. (<a
href="https://doi.org/10.1002/sim.70032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various missing data problems, values are not entirely missing, but are coarsened. For coarsened observations, instead of observing the true value, a subset of values - strictly smaller than the full sample space of the variable - is observed to which the true value belongs. In our motivating example for patients with endometrial carcinoma, the degree of lymphovascular space invasion (LVSI) can be either absent, focally present, or substantially present. For a subset of individuals, however, LVSI is reported as being present, which includes both non-absent options. In the analysis of such a dataset, difficulties arise when coarsened observations are to be used in an imputation procedure. To our knowledge, no clear-cut method has been described in the literature on how to handle an observed subset of values, and treating them as entirely missing could lead to biased estimates. Therefore, in this paper, we evaluated the best strategy to deal with coarsened and missing data in multiple imputation. We tested a number of plausible ad hoc approaches, possibly already in use by statisticians. Additionally, we propose a principled approach to this problem, consisting of an adaptation of the SMC-FCS algorithm (SMC-FCS : Coarsening compatible), that ensures that imputed values adhere to the coarsening information. These methods were compared in a simulation study. This comparison shows that methods that prevent imputations of incompatible values, like the SMC-FCS method, perform consistently better in terms of a lower bias and RMSE, and achieve better coverage than methods that ignore coarsening or handle it in a more naïve way. The analysis of the motivating example shows that the way the coarsening information is handled can matter substantially, leading to different conclusions across methods. Overall, our proposed SMC-FCS method outperforms other methods in handling coarsened data, requires limited additional computation cost and is easily extendable to other scenarios.},
  archive      = {J_SIM},
  author       = {Lars L. J. van der Burg and Stefan Böhringer and Jonathan W. Bartlett and Tjalling Bosse and Nanda Horeweg and Liesbeth C. de Wreede and Hein Putter},
  doi          = {10.1002/sim.70032},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70032},
  shortjournal = {Stat. Med.},
  title        = {Analyzing coarsened and missing data by imputation methods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preservation of type i error for partially-unblinded sample
size re-estimation. <em>SIM</em>, <em>44</em>(6), e70030. (<a
href="https://doi.org/10.1002/sim.70030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size re-estimation (SSR) at an interim analysis allows for adjustments based on accrued data. Existing strategies rely on either blinded or unblinded methods to inform such adjustments and, ideally, perform these adjustments in a way that preserves Type I error at the nominal level. Here, we propose an approach that uses partially-unblinded methods for SSR for both binary and continuous endpoints. Although this approach has operational unblinding, its partial use of the unblinded information for SSR does not include the interim effect size, hence the term ‘partially-unblinded.’ Through proof-of-concept and simulation studies, we demonstrate that these adjustments can be made without compromising the Type I error rate. We also investigate different mathematical expressions for SSR under different variance scenarios: homogeneity, heterogeneity, and a combination of both. Of particular interest is the third form of dual variance, for which we provide additional clarifications for binary outcomes and derive an analogous form for continuous outcomes. We show that the corresponding mathematical expressions for the dual variance method are a compromise between those for variance homogeneity and heterogeneity, resulting in sample size estimates that are bounded between those produced by the other expressions, and extend their applicability to adaptive trial design.},
  archive      = {J_SIM},
  author       = {Ann Marie K. Weideman and Kevin J. Anstrom and Gary G. Koch and Xianming Tan},
  doi          = {10.1002/sim.70030},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70030},
  shortjournal = {Stat. Med.},
  title        = {Preservation of type i error for partially-unblinded sample size re-estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian calibration of stochastic agent based model via
random forest. <em>SIM</em>, <em>44</em>(6), e70029. (<a
href="https://doi.org/10.1002/sim.70029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-based models (ABM) provide an excellent framework for modeling outbreaks and interventions in epidemiology by explicitly accounting for diverse individual interactions and environments. However, these models are usually stochastic and highly parametrized, requiring precise calibration for predictive performance. When considering realistic numbers of agents and properly accounting for stochasticity, this high-dimensional calibration can be computationally prohibitive. This paper presents a random forest-based surrogate modeling technique to accelerate the evaluation of ABMs and demonstrates its use to calibrate an epidemiological ABM named CityCOVID via Markov chain Monte Carlo (MCMC). The technique is first outlined in the context of CityCOVID&#39;s quantities of interest, namely hospitalizations and deaths, by exploring dimensionality reduction via temporal decomposition with principal component analysis (PCA) and via sensitivity analysis. The calibration problem is then presented, and samples are generated to best match COVID-19 hospitalization and death numbers in Chicago from March to June in 2020. These results are compared with previous approximate Bayesian calibration (IMABC) results, and their predictive performance is analyzed, showing improved performance with a reduction in computation.},
  archive      = {J_SIM},
  author       = {Connor Robertson and Cosmin Safta and Nicholson Collier and Jonathan Ozik and Jaideep Ray},
  doi          = {10.1002/sim.70029},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70029},
  shortjournal = {Stat. Med.},
  title        = {Bayesian calibration of stochastic agent based model via random forest},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric modeling of biomarker trajectory and
variability with correlated measurement errors. <em>SIM</em>,
<em>44</em>(6), e70028. (<a
href="https://doi.org/10.1002/sim.70028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prognostic significance of biomarker variability in predicting associated disease risk is well-established. However, prevailing methods that assess the relationship between biomarker variability and time to event often overlook within-subject correlation in longitudinal measurement errors, resulting in biased parameter estimates and erroneous statistical inference. Additionally, these methods typically assume that biomarker trajectory can be represented as a linear combination of spline basis functions with normally distributed random effects. This not only leads to significant computational demands due to the necessity of high-dimensional integration over the random effects but also limits the applicability because of the normality restriction imposed on the random effects. This paper addresses these limitations by incorporating correlated longitudinal measurement errors and proposing a novel semiparametric multiplicative random effects model. This model does not assume normality for the random effects and eliminates the need for integration with respect to them. The biomarker variability is incorporated as a covariate within a Cox model for time-to-event data, thus facilitating a joint modeling strategy. We demonstrate the asymptotic properties of the proposed estimators and validate their performance through simulation studies. The methodology is applied to assess the impact of systolic blood pressure variability on cardiovascular mortality using data from the Atherosclerosis Risk in Communities study.},
  archive      = {J_SIM},
  author       = {Renwen Luo and Chuoxin Ma and Jianxin Pan},
  doi          = {10.1002/sim.70028},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70028},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric modeling of biomarker trajectory and variability with correlated measurement errors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable selection for progressive multistate processes
under intermittent observation. <em>SIM</em>, <em>44</em>(6), e70023.
(<a href="https://doi.org/10.1002/sim.70023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistate models offer a natural framework for studying many chronic disease processes. Interest often lies in identifying which among a large list of candidate variables play a role in the progression of such processes. We consider the problem of variable selection for progressive multistate processes under intermittent observation based on penalized log-likelihood. An Expectation-Maximization (EM) algorithm is developed such that the maximization step can exploit existing software for penalized Poisson regression thereby allowing for the use of common penalty functions. Simulation studies show good performance in identifying important markers with different penalty functions. In a motivating application involving a cohort of patients with psoriatic arthritis, we identify which, among a large group of candidate HLA markers, are associated with rapid disease progression.},
  archive      = {J_SIM},
  author       = {Xianwei Li and Richard J. Cook and Liqun Diao},
  doi          = {10.1002/sim.70023},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70023},
  shortjournal = {Stat. Med.},
  title        = {Variable selection for progressive multistate processes under intermittent observation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active-controlled trial design for HIV prevention trials
with a counterfactual placebo. <em>SIM</em>, <em>44</em>(6), e70022. (<a
href="https://doi.org/10.1002/sim.70022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the quest for enhanced HIV prevention methods, the advent of antiretroviral drugs as pre-exposure prophylaxis (PrEP) has marked a significant stride forward. However, the ethical challenges in conducting placebo-controlled trials for new PrEP agents against a backdrop of highly effective existing PrEP options necessitate innovative approaches. This manuscript delves into the design and implementation of active-controlled trials that incorporate a counterfactual placebo estimate—a theoretical estimate of what HIV incidence would have been without effective prevention. We introduce a novel statistical framework for regulatory approval of new PrEP agents, predicated on the assumption of an available and consistent counterfactual placebo estimate. Our approach aims to assess the absolute efficacy (i.e., against placebo) of the new PrEP agent relative to the absolute efficacy of the active control. We propose a two-step procedure for hypothesis testing and further develop an approach that addresses potential biases inherent in non-randomized comparisons to counterfactual placebos. By exploring different scenarios with moderately and highly effective active controls and counterfactual placebo estimates from various sources, we demonstrate how our design can significantly reduce sample sizes compared to traditional non-inferiority trials and offer a robust framework for evaluating new PrEP agents. This work contributes to the methodological repertoire for HIV prevention trials and underscores the importance of adaptability in the face of ethical and practical challenges.},
  archive      = {J_SIM},
  author       = {Fei Gao and Holly Janes and Susan Buchbinder and Deborah Donnell},
  doi          = {10.1002/sim.70022},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70022},
  shortjournal = {Stat. Med.},
  title        = {Active-controlled trial design for HIV prevention trials with a counterfactual placebo},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of causal effects using
non-concurrent controls in platform trials. <em>SIM</em>,
<em>44</em>(6), e70017. (<a
href="https://doi.org/10.1002/sim.70017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform trials are multi-arm designs that simultaneously evaluate multiple treatments for a single disease within the same overall trial structure. Unlike traditional randomized controlled trials, they allow treatment arms to enter and exit the trial at distinct times while maintaining a control arm throughout. This control arm comprises both concurrent controls, where participants are randomized concurrently to either the treatment or control arm, and non-concurrent controls, who enter the trial when the treatment arm under study is unavailable. While flexible, platform trials introduce the challenge of using non-concurrent controls, raising questions about estimating treatment effects. Specifically, which estimands should be targeted? Under what assumptions can these estimands be identified and estimated? Are there any efficiency gains? In this article, we discuss issues related to the identification and estimation assumptions of common choices of estimand. We conclude that the most robust strategy to increase efficiency without imposing unwarranted assumptions is to target the concurrent average treatment effect (cATE), the ATE among only concurrent units, using a covariate-adjusted doubly robust estimator. Our studies suggest that, for the purpose of obtaining efficiency gains, collecting important prognostic variables is more important than relying on non-concurrent controls. We also discuss the perils of targeting ATE due to an untestable extrapolation assumption that will often be invalid. We provide simulations illustrating our points and an application to the ACTT platform trial, resulting in a 20% improvement in precision compared to the naive estimator that ignores non-concurrent controls and prognostic variables.},
  archive      = {J_SIM},
  author       = {Michele Santacatterina and Federico Macchiavelli Giron and Xinyi Zhang and Iván Díaz},
  doi          = {10.1002/sim.70017},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70017},
  shortjournal = {Stat. Med.},
  title        = {Identification and estimation of causal effects using non-concurrent controls in platform trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A seamless design for the combination of a case–control and
a cohort diagnostic accuracy study. <em>SIM</em>, <em>44</em>(6),
e70016. (<a href="https://doi.org/10.1002/sim.70016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In determining the accuracy of a new diagnostic test, often two steps are performed. In the first step, a case–control study is performed as an efficient but potentially biased design. In a second step, a population-based cohort study is performed as an unbiased but less efficient design. In order to accelerate diagnostic research, it has recently been suggested to combine the two designs in one seamless design. In this article, we present a more in-depth description of this idea. The seamless diagnostic accuracy study design is formally introduced by comparison with the traditional pathway, and the basic design decisions are discussed: A stopping rule and a stopping time. An appealing feature of the design is the possibility to ignore the seamless design in the final analysis, although part of the data is used already in an interim analysis. The justification for this strategy is provided by a large-scale simulation study. The simulation study suggests also that the risk of a loss of power due to using a seamless design can be limited by a reasonable choice of the futility boundaries, defining the stopping rule. We conclude that the seamless diagnostic accuracy study design seems to be ready to use. It promises to accelerate diagnostic research, in particular if population-based cohort studies can be started without great efforts and if the reference standard can be evaluated with little delay.},
  archive      = {J_SIM},
  author       = {Eric Bibiza-Freiwald and Werner Vach and Antonia Zapf},
  doi          = {10.1002/sim.70016},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70016},
  shortjournal = {Stat. Med.},
  title        = {A seamless design for the combination of a Case–Control and a cohort diagnostic accuracy study},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for a two-stage adaptive seamless
design using different binary endpoints. <em>SIM</em>, <em>44</em>(6),
e70003. (<a href="https://doi.org/10.1002/sim.70003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive seamless design, which integrates phases II and III into a single trial comprising two stages, is garnering increasing interest in the efficient drug development. The first stage involves selecting promising treatment group(s), followed by comparing the efficacy between the selected and control groups in the second stage. This study focused on a two-stage adaptive seamless design where treatment selection is based on a short-term binary endpoint, while the comparison is based on a long-term binary endpoint. Recently, exact and mid- p $$ p $$ tests were proposed in this setting. However, treatment effects at the second stage were estimated using the conventional maximum likelihood estimator (MLE), leading to upward bias owing to treatment selection. We propose the conditional mean-adjusted estimator (CMAE) and uniformly minimum variance conditional unbiased estimator (UMVCUE) to address the bias in this setting. Additionally, confidence intervals for exact and mid- tests were constructed using the Clopper-Pearson method. Simulation studies were performed to compare the six inference methods defined by combinations of the three estimators and two statistical tests. The simulation results showed that MLE of the treatment effect at the second stage exhibited a notable bias, while CMAE and UMVCUE substantially reduced the bias. The exact test was conservative in terms of the type-I error rate of the comparison at the second stage, while the mid- test yielded results close to the nominal level. In conclusion, we recommend statistical inferences based on the CMAE + mid- test or UMVCUE + mid- test in our setting.},
  archive      = {J_SIM},
  author       = {Ryota Ishii and Kenichi Takahashi and Kazushi Maruo and Masahiko Gosho},
  doi          = {10.1002/sim.70003},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70003},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for a two-stage adaptive seamless design using different binary endpoints},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble of sequential learning models with distributed data
centers and its applications. <em>SIM</em>, <em>44</em>(6), e70002. (<a
href="https://doi.org/10.1002/sim.70002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling massive datasets poses a significant challenge in modern data analysis, particularly within epidemiology and medicine. In this study, we introduce a novel approach using sequential ensemble learning to effectively analyze extensive datasets. Our method prioritizes efficiency from both statistical and computational perspectives, addressing challenges such as data communication and privacy, as discussed in federated learning literature. To demonstrate the efficacy of our approach, we present compelling real-world examples using COVID-19 data alongside simulation studies.},
  archive      = {J_SIM},
  author       = {Zhanfeng Wang and Jingyu Huang and Yuan-chin Ivan Chang},
  doi          = {10.1002/sim.70002},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70002},
  shortjournal = {Stat. Med.},
  title        = {Ensemble of sequential learning models with distributed data centers and its applications},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and hypothesis testing of strain-specific vaccine
efficacy with missing strain types with application to a COVID-19
vaccine trial. <em>SIM</em>, <em>44</em>(6), e10345. (<a
href="https://doi.org/10.1002/sim.10345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on data from a randomized, controlled vaccine efficacy trial, this article develops statistical methods for assessing vaccine efficacy (VE) to prevent COVID-19 infections by a discrete set of genetic strains of SARS-CoV-2. Strain-specific VE adjusting for possibly time-varying covariates is estimated using augmented inverse probability weighting to address missing viral genotypes under a competing risks model that allows separate baseline hazards for different risk groups. Hypothesis tests are developed to assess whether the vaccine provides at least a specified level of VE against some viral genotypes and whether VE varies across genotypes. Asymptotic properties providing analytic inferences are derived and finite-sample properties of the estimators and hypothesis tests are studied through simulations. This research is motivated by the fact that previous analyses of COVID-19 vaccine efficacy did not account for missing genotypes, which can cause severe bias and efficiency loss. The theoretical properties and simulations demonstrate superior performance of the new methods. Application to the Moderna COVE trial identifies several SARS-CoV-2 genotype features with differential vaccine efficacy across genotypes, including lineage (Reference, Epsilon, Gamma, Zeta), indicators of residue match vs. mismatch to the vaccine-strain residue at Spike amino acid positions (identifying signatures of differential VE), and a weighted Hamming distance to the vaccine strain. The results show VE decreases against genotypes more distant from the vaccine strain, highlighting the need to update COVID-19 vaccine strains.},
  archive      = {J_SIM},
  author       = {Fei Heng and Yanqing Sun and Li Li and Peter B. Gilbert},
  doi          = {10.1002/sim.10345},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e10345},
  shortjournal = {Stat. Med.},
  title        = {Estimation and hypothesis testing of strain-specific vaccine efficacy with missing strain types with application to a COVID-19 vaccine trial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling volume-outcome relationships in health care.
<em>SIM</em>, <em>44</em>(6), e10339. (<a
href="https://doi.org/10.1002/sim.10339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the ongoing strong interest in associations between quality of care and the volume of health care providers, a unified statistical framework for analyzing them is missing, and many studies suffer from poor statistical modelling choices. We propose a flexible, additive mixed model for studying volume-outcome associations in health care that takes into account individual patient characteristics as well as provider-specific effects through a hierarchical approach. More specifically, we treat volume as a continuous variable, and its effect on the considered outcome is modeled as a smooth function. We take account of different case-mixes by including patient-specific risk factors and clustering on the provider level through random intercepts. This strategy enables us to extract a smooth volume effect as well as volume-independent provider effects. These two quantities can be compared directly in terms of their magnitude, which gives insight into the sources of variability of quality of care. Based on a causal DAG, we derive conditions under which the volume-effect can be interpreted as a causal effect. The paper provides confidence sets for each of the estimated quantities relying on joint estimation of all effects and parameters. Our approach is illustrated through simulation studies and an application to German health care data about mortality of very low birth weight infants.},
  archive      = {J_SIM},
  author       = {Maurilio Gutzeit and Johannes Rauh and Maximilian Kähler and Jona Cederbaum},
  doi          = {10.1002/sim.10339},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e10339},
  shortjournal = {Stat. Med.},
  title        = {Modelling volume-outcome relationships in health care},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient computation of high-dimensional penalized
piecewise constant hazard random effects models. <em>SIM</em>,
<em>44</em>(6), e10311. (<a
href="https://doi.org/10.1002/sim.10311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and characterizing relationships between treatments, exposures, or other covariates and time-to-event outcomes has great significance in a wide range of biomedical settings. In research areas such as multi-center clinical trials, recurrent events, and genetic studies, proportional hazard mixed effects models (PHMMs) are used to account for correlations observed in clusters within the data. In high dimensions, proper specification of the fixed and random effects within PHMMs is difficult and computationally complex. In this paper, we approximate the proportional hazards mixed effects model with a piecewise constant hazard mixed effects survival model. We estimate the model parameters using a modified Monte Carlo expectation conditional minimization (MCECM) algorithm, allowing us to perform variable selection on both the fixed and random effects simultaneously. We also incorporate a factor model decomposition of the random effects in order to more easily scale the variable selection method to larger dimensions. We demonstrate the utility of our method using simulations, and we apply our method to a multi-study pancreatic ductal adenocarcinoma gene expression dataset to select features important for survival.},
  archive      = {J_SIM},
  author       = {Hillary M. Heiling and Naim U. Rashid and Quefeng Li and Xianlu L. Peng and Jen Jen Yeh and Joseph G. Ibrahim},
  doi          = {10.1002/sim.10311},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e10311},
  shortjournal = {Stat. Med.},
  title        = {Efficient computation of high-dimensional penalized piecewise constant hazard random effects models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overcoming model uncertainty — how equivalence tests can
benefit from model averaging. <em>SIM</em>, <em>44</em>(6), e10309. (<a
href="https://doi.org/10.1002/sim.10309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, for example, based on gender, age, or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. Classical approaches are based on testing the equivalence of single quantities, for example, the mean, the area under the curve or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them. In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures. In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth Bayesian information criterion weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and illustrate its practical relevance considering a time-response case study with toxicological gene expression data.},
  archive      = {J_SIM},
  author       = {Niklas Hagemann and Kathrin Möllenhoff},
  doi          = {10.1002/sim.10309},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e10309},
  shortjournal = {Stat. Med.},
  title        = {Overcoming model uncertainty — how equivalence tests can benefit from model averaging},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
