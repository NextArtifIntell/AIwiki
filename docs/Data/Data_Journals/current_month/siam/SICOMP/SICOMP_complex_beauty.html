<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SICOMP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sicomp---5">SICOMP - 5</h2>
<ul>
<li><details>
<summary>
(2025). Differentially private sampling from distributions.
<em>SICOMP</em>, <em>54</em>(2), 419–468. (<a
href="https://doi.org/10.1137/22M1538703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We initiate an investigation of private sampling from distributions. Given a dataset with independent observations from an unknown distribution , a sampling algorithm must output a single observation from a distribution that is close in total variation distance to while satisfying differential privacy. Sampling abstracts the goal of generating small amounts of realistic-looking data. We provide tight upper and lower bounds for the dataset size needed for this task for three natural families of distributions: arbitrary distributions on , arbitrary product distributions on , and product distributions on with bias in each coordinate bounded away from 0 and 1. We demonstrate that, in some parameter regimes, private sampling requires asymptotically fewer observations than learning a description of nonprivately; in other regimes, however, private sampling proves to be as difficult as private learning. Notably, for some classes of distributions, the overhead in the number of observations needed for private learning compared to nonprivate learning is completely captured by the number of observations needed for private sampling.},
  archive      = {J_SICOMP},
  author       = {Sofya Raskhodnikova and Satchit Sivakumar and Adam Smith and Marika Swanberg},
  doi          = {10.1137/22M1538703},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {419-468},
  shortjournal = {SIAM J. Comput.},
  title        = {Differentially private sampling from distributions},
  volume       = {54},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An exponential time parameterized algorithm for planar
disjoint paths. <em>SICOMP</em>, <em>54</em>(2), 321–418. (<a
href="https://doi.org/10.1137/20M1355902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In the disjoint paths problem, the input is an undirected graph on vertices and a set of vertex pairs, and the task is to find pairwise vertex-disjoint paths such that the ’th path connects to . In this paper, we give a parameterized algorithm with running time for planar disjoint paths, the variant of the problem where the input graph is required to be planar. Our algorithm is based on the unique linkage/treewidth reduction theorem for planar graphs by Adler et al. [J. Combin. Theory Ser. B, 122 (2017), pp. 815–843], the algebraic cohomology based technique of Schrijver [SIAM J. Comput., 23 (1994), pp. 780–788], and one of the key combinatorial insights developed by Cygan et al. [Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, 2013, pp. 197–206] in their algorithm for disjoint paths on directed planar graphs. To the best of our knowledge, our algorithm is the first parameterized algorithm to exploit the fact that the treewidth of the input graph is small, and it does so in a way that is completely different from the use of dynamic programming.},
  archive      = {J_SICOMP},
  author       = {Daniel Lokshtanov and Pranabendu Misra and Michal Pilipczuk and Saket Saurabh and Meirav Zehavi},
  doi          = {10.1137/20M1355902},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {321-418},
  shortjournal = {SIAM J. Comput.},
  title        = {An exponential time parameterized algorithm for planar disjoint paths},
  volume       = {54},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agreement tests on graphs and hypergraphs. <em>SICOMP</em>,
<em>54</em>(2), 279–320. (<a
href="https://doi.org/10.1137/21M1397684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Agreement tests are a generalization of low degree tests that capture a local-to-global phenomenon, which forms the combinatorial backbone of most probabilistically checkable proof (PCP) constructions. In an agreement test, a function is given by an ensemble of local restrictions. The agreement test checks that the restrictions agree when they overlap, and the main question is whether average agreement of the local pieces implies that there exists a global function that agrees with most local restrictions. There are very few structures that support agreement tests, essentially either coming from algebraic low degree tests or from direct product tests (and recently also from high-dimensional expanders). In this work, we prove a new agreement theorem which extends direct product tests to higher dimensions, analogously to how low degree tests extend linearity testing. As a corollary of our main theorem, it follows that an ensemble of small graphs on overlapping sets of vertices can be glued together to one global graph assuming they agree with each other on average. We prove the agreement theorem by (re)proving the agreement theorem for dimension 1, and then generalizing it to higher dimensions (with the dimension 1 case being the direct product test and dimension 2 being the graph case). A key technical step in our proof is the reverse union bound, which allows us to treat dependent events as if they are disjoint, and may be of independent interest. An added benefit of the reverse union bound is that it can be used to show that the “majority decoded” function also serves as a global function that explains the local consistency of the agreement theorem, a fact that was not known even in the direct product setting (dimension 1) prior to our work. Beyond the motivation to understand fundamental local-to-global structures, our main theorem allows us to lift structure theorems from the standard uniform distribution to the -biased distribution . As a simple demonstration of this paradigm, we show how the low degree testing result of Alon et al. [IEEE Trans. Inform. Theory, 51 (2005), pp. 4032–4039,] and Bhattacharyya et al. [Proc. 51st FOCS, IEEE, 2010, pp. 488–497], originally proved for , can be extended to the -biased hypercube , even for very small subconstant .},
  archive      = {J_SICOMP},
  author       = {Irit Dinur and Yuval Filmus and Prahladh Harsha},
  doi          = {10.1137/21M1397684},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {279-320},
  shortjournal = {SIAM J. Comput.},
  title        = {Agreement tests on graphs and hypergraphs},
  volume       = {54},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved classical and quantum algorithms for the shortest
vector problem via bounded distance decoding. <em>SICOMP</em>,
<em>54</em>(2), 233–278. (<a
href="https://doi.org/10.1137/22M1486959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The most important computational problem on lattices is the shortest vector problem . In this paper, we present new algorithms that improve the state-of-the-art for provable classical/quantum algorithms for . We present the following results: (1) A new algorithm for that provides a smooth tradeoff between time complexity and memory requirement. For any positive integer , our algorithm takes time and requires memory. This tradeoff, which ranges from enumeration to sieving ( constant), is a consequence of a new time-memory tradeoff for discrete Gaussian sampling above the smoothing parameter. (2) A quantum algorithm for that runs in time and requires classical memory and qubits. In a quantum random access memory (QRAM) model, this algorithm takes only time and requires a QRAM of size , qubits and classical space. This improves over the previously fastest classical (which is also the fastest quantum) algorithm due to [D. Aggarwal et al., Solving the shortest vector problem in 2n time using discrete Gaussian sampling: Extended abstract, in Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC), 2015, pp. 733–742] that has a time and space complexity . (3) A classical algorithm for that runs in time time and space. This improves over an algorithm of [Y. Chen, K. Chung, and C. Lai, Quantum Inf. Comput., 18 (2018), pp. 285–306] that has the same space complexity. The time complexity of our classical and quantum algorithms are obtained using a known upper bound on a quantity related to the lattice kissing number, which is . We conjecture that for most lattices this quantity is a . Assuming that this is the case, our classical algorithm runs in time , our quantum algorithm runs in time , and our quantum algorithm in a QRAM model runs in time . As a direct application of our result, using the reduction in [L. Ducas, Des. Codes. Cryptogr., 92 (2024), pp. 909–916], we obtain a provable quantum algorithm for the lattice isomorphism problem in the case of the trivial lattice (LIP) that runs in time . Our algorithm requires a QRAM of size , qubits and classical space.},
  archive      = {J_SICOMP},
  author       = {Divesh Aggarwal and Yanlin Chen and Rajendra Kumar and Yixin Shen},
  doi          = {10.1137/22M1486959},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {233-278},
  shortjournal = {SIAM J. Comput.},
  title        = {Improved classical and quantum algorithms for the shortest vector problem via bounded distance decoding},
  volume       = {54},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational complexity of the hylland–zeckhauser mechanism
for one-sided matching markets. <em>SICOMP</em>, <em>54</em>(2),
193–232. (<a href="https://doi.org/10.1137/23M157586X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In 1979, Hylland and Zeckhauser [J. Polit. Econ., 87 (1979), pp. 293–314] gave a simple and general mechanism for a one-sided matching market, given cardinal utilities of agents over goods. They use the power of a pricing mechanism, which endows their mechanism with several desirable properties—it produces an allocation that is Pareto optimal and envy free, and the mechanism is incentive compatible in the large. It therefore provides an attractive, off-the-shelf method for running an application involving such a market. With matching markets becoming ever more prevalent and impactful, it is imperative to characterize the computational complexity of this mechanism. We present the following results: (1) A combinatorial, strongly polynomial time algorithm for the dichotomous case, i.e., utilities, and more generally, when each agent’s utilities come from a bivalued set. (2) An example that has only irrational equilibria; hence this problem is not in PPAD. (3) A proof of membership of the problem in the class FIXP; as a corollary we get that a Hylland–Zeckhauser (HZ) equilibrium can always be expressed via algebraic numbers. For this purpose, we give a new proof of the existence of an HZ equilibrium using Brouwer’s fixed point theorem; the proof of Hylland and Zeckhauser used Kakutani’s fixed point theorem, which is more involved. (4) A proof of membership of the problem of computing an approximate HZ equilibrium in the class PPAD. In subsequent work [T. Chen et al., SODA 2022, SIAM, Philadelphia, pp. 2253–2268], the problem of computing an approximate HZ equilibrium was shown to be PPAD-hard, thereby establishing it to be PPAD-complete. We leave open the (difficult) question of determining if computing an exact HZ equilibrium is FIXP-hard. We also give pointers to the substantial body of work on cardinal-utility matching markets which followed [V. V. Vazirani and M. Yannakakis, LIPIcs. Leibniz Int. Proc. Inform. 185, Schloss Dagstuhl, Wadern Germany, 59].},
  archive      = {J_SICOMP},
  author       = {Vijay V. Vazirani and Mihalis Yannakakis},
  doi          = {10.1137/23M157586X},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {193-232},
  shortjournal = {SIAM J. Comput.},
  title        = {Computational complexity of the Hylland–Zeckhauser mechanism for one-sided matching markets},
  volume       = {54},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
