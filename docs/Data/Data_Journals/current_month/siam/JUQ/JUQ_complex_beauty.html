<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JUQ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="juq---12">JUQ - 12</h2>
<ul>
<li><details>
<summary>
(2025). Conditional optimal transport on function spaces.
<em>JUQ</em>, <em>13</em>(1), 304–338. (<a
href="https://doi.org/10.1137/23M1618922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a systematic study of conditional triangular transport maps in function spaces from the perspective of optimal transportation and with a view towards amortized Bayesian inference. More specifically, we develop a theory of constrained optimal transport problems that describe block-triangular Monge maps that characterize conditional measures along with their Kantorovich relaxations. This work generalizes the theory of optimal triangular transport to separable infinite dimensional function spaces with general cost functions. We further tailor our results to the case of Bayesian inference problems and obtain regularity estimates on the conditioning maps from the prior to the posterior. Finally, we present numerical experiments that demonstrate the computational applicability of our theoretical results for amortized and likelihood-free inference of functional parameters.},
  archive      = {J_JUQ},
  author       = {Bamdad Hosseini and Alexander W. Hsu and Amirhossein Taghvaei},
  doi          = {10.1137/23M1618922},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {304-338},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Conditional optimal transport on function spaces},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMC and underdamped langevin united in the unadjusted convex
smooth case. <em>JUQ</em>, <em>13</em>(1), 278–303. (<a
href="https://doi.org/10.1137/23M1608963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider a family of unadjusted generalized HMC samplers, which includes standard position HMC samplers and discretizations of the underdamped Langevin process. A detailed analysis and optimization of the parameters is conducted in the Gaussian case, which shows an improvement from to for the convergence rate in terms of the condition number by using partial velocity refreshment, with respect to classical full refreshments. A similar effect is observed empirically for two related algorithms, namely Metropolis-adjusted gHMC and kinetic piecewise-deterministic Markov processes. Then, a stochastic gradient version of the samplers is considered, for which dimension-free convergence rates are established for log-concave smooth targets over a large range of parameters, gathering in a unified framework previous results on position HMC and underdamped Langevin and extending them to HMC with inertia.},
  archive      = {J_JUQ},
  author       = {Nicolaï Gouraud and Pierre Le Bris and Adrien Majka and Pierre Monmarché},
  doi          = {10.1137/23M1608963},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {278-303},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {HMC and underdamped langevin united in the unadjusted convex smooth case},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entropy-based burn-in time analysis and ranking for (a)MCMC
algorithms in high dimension. <em>JUQ</em>, <em>13</em>(1), 251–277. (<a
href="https://doi.org/10.1137/23M1611932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many recent and often (adaptive) Markov chain Monte Carlo (A)MCMC methods are associated in practice to unknown rates of convergence. We propose a simulation-based methodology to estimate and compare MCMC’s performance in terms of shortest burn-in time, using a Kullback divergence criterion requiring an estimate of the entropy of the algorithm densities at each iteration, computed from i.i.d. simulated chains. In previous works, we proved some consistency results in an MCMC setup for an entropy estimate based on Monte Carlo integration of a kernel density estimate proposed in [L. Györfi and E. C. Van Der Meulen, An entropy estimate based on a kernel density estimation, in Limit Theorems in Probability and Statistics, North-Holland, Amsterdam, 1989, pp. 229–240], and we investigate an alternative nearest neighbor (NN) entropy estimate from [L. Kozachenko and N. N. Leonenko, Problemy Peredachi Informatsii, 23 (1987), pp. 9–16]. This estimate has been used mostly in univariate situations until recently, when entropy estimation in higher dimensions has been considered in other fields like neuroscience or system biology. Unfortunately, in higher dimensions, both estimators converge slowly with a noticeable bias. The present work goes several steps further, with bias reduction and automatic (A)MCMC burn-in time analysis in mind. First, for bias reduction, we apply in our situation a “crossed NN-type” nonparametric estimate of the Kullback divergence between two densities, based on i.i.d. samples from each, introduced by [Q. Wang, S. R. Kulkarni, and S. Verdú, A nearest-neighbor approach to estimating divergence between continuous random vectors, in IEEE International Symposium on Information Theory, Seattle, WA, 2006, pp. 242–246], [Q. Wang, S. R. Kulkarni, and S. Verdú, IEEE Trans. Inform. Theory, 55 (2009), pp. 2392–2405]. We prove the consistency of these entropy estimates under recent uniform control conditions, for the successive densities of a generic class of MCMC algorithm to which most of the methods proposed in the recent literature belong. Second, we propose an original solution based on a PCA for reducing relevant dimension and bias in even higher dimensions whenever PCA is efficient. Our algorithms for MCMC simulation and entropy estimation are progressively added to the R package EntropyMCMC, taking advantage of recent advances in high-performance (parallel) computing.},
  archive      = {J_JUQ},
  author       = {Didier Chauveau and Pierre Vandekerkhove},
  doi          = {10.1137/23M1611932},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {251-277},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Entropy-based burn-in time analysis and ranking for (A)MCMC algorithms in high dimension},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective optimization using expected quantile
improvement for decision making in disease outbreaks. <em>JUQ</em>,
<em>13</em>(1), 228–250. (<a
href="https://doi.org/10.1137/24M1633625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Optimization under uncertainty is important in many applications, particularly to inform policy and decision making in areas such as public health. A key source of uncertainty arises from the incorporation of environmental variables as inputs into computational models or simulators. Such variables represent uncontrollable features of the optimization problem, and reliable decision making must account for the uncertainty they propagate to the simulator outputs. Often, multiple, competing objectives are defined from these outputs such that the final optimal decision is a compromise between different goals. Here, we present emulation-based optimization methodology for such problems that extends expected quantile improvement (EQI) to address multiobjective optimization. Focusing on the practically important case of two objectives, we use a sequential design strategy to identify the Pareto front of optimal solutions. Uncertainty from the environmental variables is integrated out using Monte Carlo samples from the simulator. Interrogation of the expected output from the simulator is facilitated by use of (Gaussian process) emulators. The methodology is demonstrated on an optimization problem from public health involving the dispersion of anthrax spores across a spatial terrain. Environmental variables include meteorological features that impact the dispersion, and the methodology identifies the Pareto front even when there is considerable input uncertainty.},
  archive      = {J_JUQ},
  author       = {Daria Semochkina and Alexander I. J. Forrester and David C. Woods},
  doi          = {10.1137/24M1633625},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {228-250},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multiobjective optimization using expected quantile improvement for decision making in disease outbreaks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elastic bayesian model calibration. <em>JUQ</em>,
<em>13</em>(1), 195–227. (<a
href="https://doi.org/10.1137/24M1644092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Functional data are ubiquitous in scientific modeling. For instance, quantities of interest are modeled as functions of time, space, energy, density, etc. Uncertainty quantification methods for computer models with functional response have resulted in tools for emulation, sensitivity analysis, and calibration that are widely used. However, many of these tools do not perform well when the computer model’s parameters control both the amplitude variation of the functional output and its alignment (or phase variation). This paper introduces a framework for Bayesian model calibration when the model responses are misaligned functional data. The approach generates two types of data out of the misaligned functional responses: (1) aligned functions so that the amplitude variation is isolated and (2) warping functions that isolate the phase variation. These two types of data are created for the computer simulation data (both of which may be emulated) and the experimental data. The calibration approach uses both types so that it seeks to match both the amplitude and phase of the experimental data. The framework is careful to respect constraints that arise, especially when modeling phase variation, and is framed in a way that it can be done with readily available calibration software. We demonstrate the techniques on two simulated data examples and on two dynamic material science problems: a strength model calibration using flyer plate experiments and an equation of state model calibration using experiments performed on the Sandia National Laboratories’ Z-machine.},
  archive      = {J_JUQ},
  author       = {Devin Francom and J. Derek Tucker and Gabriel Huerta and Kurtis Shuler and Daniel Ries},
  doi          = {10.1137/24M1644092},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {195-227},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Elastic bayesian model calibration},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Worst-case learning under a multifidelity model.
<em>JUQ</em>, <em>13</em>(1), 171–194. (<a
href="https://doi.org/10.1137/24M1671025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Inspired by multifidelity methods in computer simulations, this article introduces procedures for designing surrogates for the input/output relationship of a high-fidelity code. These surrogates should be learned from runs of both the high-fidelity and low-fidelity codes and be accompanied by error guarantees that are deterministic rather than stochastic. For this purpose, the article advocates a framework tied to a theory focusing on worst-case guarantees, namely optimal recovery. The multifidelity considerations triggered new theoretical results in three scenarios: the globally optimal estimation of linear functionals, the globally optimal approximation of arbitrary quantities of interest in Hilbert spaces, and their locally optimal approximation, still within Hilbert spaces. The latter scenario boils down to the determination of the Chebyshev center for the intersection of two hyperellipsoids. It is worth noting that the mathematical framework presented here, together with its possible extension, seems to be relevant in several other contexts briefly discussed.},
  archive      = {J_JUQ},
  author       = {Simon Foucart and Nicolas Hengartner},
  doi          = {10.1137/24M1671025},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {171-194},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Worst-case learning under a multifidelity model},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive hierarchical ensemble kalman filter with reduced
basis models. <em>JUQ</em>, <em>13</em>(1), 140–170. (<a
href="https://doi.org/10.1137/24M1653690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The use of reduced order modeling techniques in combination with ensemble-based methods for estimating the state of systems described by nonlinear partial differential equations has been of great interest in recent years in the data assimilation community. Methods such as the multifidelity ensemble Kalman filter and the multilevel ensemble Kalman filter are recognized as state-of-the-art techniques. However, in many cases, the construction of low-fidelity models in an offline stage, before solving the data assimilation problem, prevents them from being both accurate and computationally efficient. In our work, we investigate the use of adaptive reduced basis techniques in which the approximation space is modified online by combining information extracted from a limited number of full order solutions and information extracted from reduced models trained at previous time steps. This allows us to simultaneously ensure good accuracy and low cost for the employed models and thus improve the performance of the multifidelity and multilevel methods.},
  archive      = {J_JUQ},
  author       = {Francesco A. B. Silva and Cecilia Pagliantini and Karen Veroy},
  doi          = {10.1137/24M1653690},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {140-170},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An adaptive hierarchical ensemble kalman filter with reduced basis models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable method for bayesian experimental design without
integrating over posterior distribution. <em>JUQ</em>, <em>13</em>(1),
114–139. (<a href="https://doi.org/10.1137/23M1603364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We address the computational efficiency of finding the A-optimal Bayesian experimental design, where the observation map is based on partial differential equations and thus computationally expensive to evaluate. A-optimality is a widely used and easily interpreted criterion, that seeks the optimal experimental design by minimizing the expected conditional variance. Our study presents a novel likelihood-free approach to the A-optimal experimental design that does not require sampling or integration over the Bayesian posterior distribution. In our proposed approach, we estimate the expected conditional variance via the variance of the conditional expectation and approximate the conditional expectation using its orthogonal projection property. We derive an asymptotic error estimate for the proposed estimator of the expected conditional variance and verify it with numerical experiments. Furthermore, we extend our approach to the case where the domain of the experimental design parameters is continuous. Specifically, we propose a nonlocal approximation of the conditional expectation using an artificial neural network and apply transfer learning and data augmentation to reduce the number of evaluations of the measurement model. Through numerical experiments, we demonstrate that our method greatly reduces the number of measurement model evaluations compared with widely used importance sampling-based approaches. Code is available at https://github.com/vinh-tr-hoang/DOEviaPACE.},
  archive      = {J_JUQ},
  author       = {Vinh Hoang and Luis Espath and Sebastian Krumscheid and Raúl Tempone},
  doi          = {10.1137/23M1603364},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {114-139},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Scalable method for bayesian experimental design without integrating over posterior distribution},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification in machine learning based
segmentation: A post-hoc approach for left ventricle volume estimation
in MRI. <em>JUQ</em>, <em>13</em>(1), 90–113. (<a
href="https://doi.org/10.1137/23M161433X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recent studies have confirmed cardiovascular diseases remain responsible for the highest mortality rate among noncommunicable diseases. The accurate left ventricular (LV) volume estimation is critical for valid diagnosis and management of various cardiovascular conditions, but poses a significant challenge due to inherent uncertainties associated with the segmentation algorithms in magnetic resonance imaging. Recent machine learning advancements, particularly U-Net-like convolutional networks, have facilitated automated segmentation for medical images, but struggles under certain pathologies and/or different scanner vendors and imaging protocols. This study proposes a novel methodology for post-hoc uncertainty estimation in the LV volume prediction using Itô stochastic differential equations to model pathwise behavior for the prediction error. The model describes the area of the left ventricle along the heart’s long axis. The method is agnostic to the underlying segmentation algorithm, facilitating its use with various existing and future segmentation technologies. The proposed approach provides a mechanism for quantifying uncertainty, enabling medical professionals to intervene for unreliable predictions. This is of utmost importance in critical applications such as medical diagnosis, where prediction accuracy and reliability can directly impact patient outcomes. The method is also robust to dataset changes, enabling application for medical centers with limited access to labeled data. Our findings highlight the proposed uncertainty estimation methodology’s potential to enhance automated segmentation robustness and generalizability, paving the way for more reliable and accurate LV volume estimation in clinical settings as well as opening new avenues for uncertainty quantification in biomedical image segmentation, providing promising directions for future research.},
  archive      = {J_JUQ},
  author       = {Felix Terhag and Philipp Knechtges and Achim Basermann and Raúl Tempone},
  doi          = {10.1137/23M161433X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {90-113},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification in machine learning based segmentation: A post-hoc approach for left ventricle volume estimation in MRI},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equispaced fourier representations for efficient gaussian
process regression from a billion data points. <em>JUQ</em>,
<em>13</em>(1), 63–89. (<a
href="https://doi.org/10.1137/23M1565310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a Fourier-based fast algorithm for Gaussian process regression in low dimensions. It approximates a translationally invariant covariance kernel by complex exponentials on an equispaced Cartesian frequency grid of nodes. This results in a weight-space system matrix with Toeplitz structure, which can thus be applied to a vector in operations via the fast Fourier transform (FFT), independent of the number of data points . The linear system can be set up in operations using nonuniform FFTs. This enables efficient massive-scale regression via an iterative solver, even for kernels with fat-tailed spectral densities (large ). We provide bounds on both kernel approximation and posterior mean errors. Numerical experiments for squared-exponential and Matérn kernels in one, two, and three dimensions often show 1–2 orders of magnitude acceleration over state-of-the-art rank-structured solvers at comparable accuracy. Our method allows two-dimensional Matérn- regression from data points to be performed in two minutes on a standard desktop, with posterior mean accuracy . This opens up spatial statistics applications 100 times larger than previously possible.},
  archive      = {J_JUQ},
  author       = {Philip Greengard and Manas Rachh and Alex H. Barnett},
  doi          = {10.1137/23M1565310},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {63-89},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Equispaced fourier representations for efficient gaussian process regression from a billion data points},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sampling low-fidelity outputs for estimation of
high-fidelity density and its tails. <em>JUQ</em>, <em>13</em>(1),
30–62. (<a href="https://doi.org/10.1137/24M1639142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In a multifidelity setting, data are available under the same conditions from two (or more) sources, e.g., computer codes, one being lower-fidelity but computationally cheaper, and the other higher-fidelity and more expensive. This paper studies for which low-fidelity outputs one should obtain high-fidelity outputs if the goal is to estimate the probability density function of the latter, especially when it comes to the distribution tails and extremes. It is suggested to approach this problem from the perspective of the importance sampling of low-fidelity outputs according to some proposal distribution, combined with special considerations for the distribution tails based on extreme value theory. The notion of an optimal proposal distribution is introduced and investigated, in both theory and simulations. The approach is motivated and illustrated with an application to estimate the probability density function of record extremes of ship motions, obtained through two computer codes of different fidelities.},
  archive      = {J_JUQ},
  author       = {Minji Kim and Kevin O’Connor and Vladas Pipiras and Themistoklis Sapsis},
  doi          = {10.1137/24M1639142},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {30-62},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sampling low-fidelity outputs for estimation of high-fidelity density and its tails},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial chaos surrogate construction for random fields
with parametric uncertainty. <em>JUQ</em>, <em>13</em>(1), 1–29. (<a
href="https://doi.org/10.1137/23M1613505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Engineering and applied science rely on computational experiments to rigorously study physical systems. The mathematical models used to probe these systems are highly complex, and sampling-intensive studies often require prohibitively many simulations for acceptable accuracy. Surrogate models provide a means of circumventing the high computational expense of sampling such complex models. In particular, polynomial chaos expansions (PCEs) have been successfully used for uncertainty quantification studies of deterministic models where the dominant source of uncertainty is parametric. We discuss an extension to conventional PCE surrogate modeling to enable surrogate construction for stochastic computational models that have intrinsic noise in addition to parametric uncertainty. We develop a PCE surrogate on a joint space of intrinsic and parametric uncertainty, enabled by Rosenblatt transformations, which are evaluated via kernel density estimation of the associated conditional cumulative distributions. Furthermore, we extend the construction to random field data via the Karhunen–Loève expansion. We then take advantage of closed-form solutions for computing PCE Sobol indices to perform a global sensitivity analysis of the model which quantifies the intrinsic noise contribution to the overall model output variance. Additionally, the resulting joint PCE is generative in the sense that it allows generating random realizations at any input parameter setting that are statistically approximately equivalent to realizations from the underlying stochastic model. The method is demonstrated on a chemical catalysis example model and a synthetic example controlled by a parameter that enables a switch from unimodal to bimodal response distributions.},
  archive      = {J_JUQ},
  author       = {Joy N. Mueller and Khachik Sargsyan and Craig J. Daniels and Habib N. Najm},
  doi          = {10.1137/23M1613505},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Polynomial chaos surrogate construction for random fields with parametric uncertainty},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
