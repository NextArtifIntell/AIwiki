<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>aaai_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="aaai">AAAI</h1>
<h2 id="jair---8">JAIR - 8</h2>
<ul>
<li><details>
<summary>
(2025). An extensive empirical evaluation of inferring preconditions
and effects of compound tasks in ground HTN planning problems.
<em>JAIR</em>, <em>82</em>, 1407–1444. (<a
href="https://doi.org/10.1613/jair.1.17279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HTN planning requires the decomposition of compound tasks into primitive and executable actions. In the currently most frequently used formalism, compound tasks lack explicit preconditions and effects. Those are, however, useful, e.g., for pruning techniques, heuristics, or the comprehension of domains. Previously, we introduced and formalized different kinds of inferred preconditions and effects of compound tasks based on their decomposition methods together with a complexity analysis. In this paper, we present an empirical evaluation of computing these inferred preconditions and effects using the IPC benchmark sets. Specifically, we analyze their frequency of occurrence and compare the performance of an approximation to the exact preconditions and effects. Our goal is to provide a comprehensive overview of the proposed techniques, enabling researchers to determine the extent to which they can be utilized in their given application.},
  archive      = {J_JAIR},
  author       = {Conny Olz and Alexander Lodemann and Benedikt Jutz and Mario Schmautz and Maximilian Borowiec and Susanne Biundo and Pascal Bercher},
  doi          = {10.1613/jair.1.17279},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1407-1444},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {An extensive empirical evaluation of inferring preconditions and effects of compound tasks in ground HTN planning problems},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbolic search for cost-optimal planning with expressive
model extensions. <em>JAIR</em>, <em>82</em>, 1349–1405. (<a
href="https://doi.org/10.1613/jair.1.16869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classical planning, the task is to derive a sequence of deterministic actions that changes the current fully-observable world state into one that satisfies a set of goal criteria. Algorithms for classical planning are domain-independent, i.e., they are not limited to a particular application and instead can be used to solve different types of reasoning problems. The main language for modeling such problems is the Planning Domain Definition Language (PDDL). Even though it provides many language features for expressing a wide range of planning tasks, most of today’s classical planners, especially optimal ones, support only a small subset of its features. The most widely supported fragment is lifted STRIPS plus types and action costs. While this fragment suffices to model some interesting planning tasks, using it to model more realistic problems often incurs a much higher modeling effort. Even if modeling is possible at all, solving the resulting tasks is often infeasible in practice, as the required encoding size increases exponentially. To address these issues, we show how to support more expressive modeling languages natively in optimal classical planning algorithms. Specifically, we focus on symbolic search, a state-of-the-art search algorithm that operates on sets of world states. We show how to extend symbolic search to support classical planning with conditional effects, axioms, and state-dependent action costs. All of these modeling features are expressive in the sense that compiling them away incurs a significant blow-up, so is it often necessary to support them natively. Except for blind (non-symbolic) search, our new symbolic search is the first optimal classical planning algorithm that supports these three modeling extensions in combination, and it even compares favorably to other state-of-the-art approaches that only support a subset of the extensions.},
  archive      = {J_JAIR},
  author       = {David Speck and Jendrik Seipp and Alvaro Torralba},
  doi          = {10.1613/jair.1.16869},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1349–1405},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Symbolic search for cost-optimal planning with expressive model extensions},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConSCompF: Consistency-focused similarity comparison
framework for generative large language models. <em>JAIR</em>,
<em>82</em>, 1325–1347. (<a
href="https://doi.org/10.1613/jair.1.17028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLM) are one of the most important discoveries in machine learning in recent years. LLM-based artificial intelligence (AI) assistants, such as ChatGPT, have consistently attracted attention from researchers, investors, and the general public, driving the rapid growth of this industry. With dozens of new LLMs released every month, it becomes quite challenging to differentiate between them, thereby creating a demand for new LLM comparison methods. In this research, the Consistency-focused Similarity Comparison Framework (ConSCompF) for generative large language models is proposed. It compares texts generated by two LLMs and produces a similarity score, indicating the overall degree of similarity between their responses. The main advantage of this framework is that it can operate on a small number of unlabeled data, such as chatbot instruction prompts, and does not require LLM developers to disclose any information about their product. To evaluate the efficacy of ConSCompF, two experiments aimed at identifying similarities between multiple LLMs are conducted. Additionally, these experiments examine the correlation between the similarity scores generated by ConSCompF and the differences in outputs produced by other benchmarking techniques, such as ROUGE-L. Finally, a series of few-shot LLM comparison experiments is conducted to evaluate the performance of ConSCompF in a few-shot LLM comparison scenario. The proposed framework can be used for calculating similarity matrices of multiple LLMs, which can be effectively visualized using principal component analysis (PCA). The outputs of ConSCompF may provide useful insights into data that might have been used during LLM training and help detect potential investment fraud attempts.},
  archive      = {J_JAIR},
  author       = {Alexey Karev and Dong Xu},
  doi          = {10.1613/jair.1.17028},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1325-1347},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {ConSCompF: Consistency-focused similarity comparison framework for generative large language models},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laplace-HDC: Understanding the geometry of binary
hyperdimensional computing. <em>JAIR</em>, <em>82</em>, 1293–1323. (<a
href="https://doi.org/10.1613/jair.1.17688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the geometry of binary hyperdimensional computing (HDC), a computational scheme in which data are encoded using high-dimensional binary vectors. We establish a result about the similarity structure induced by the HDC binding operator and show that the Laplace kernel naturally arises in this setting, motivating our new encoding method Laplace-HDC, which improves upon previous methods. We describe how our results indicate limitations of binary HDC in encoding spatial information from images and discuss potential solutions, including using Haar convolutional features and the definition of a translation-equivariant HDC encoding. Several numerical experiments highlighting the improved accuracy of Laplace-HDC in contrast to alternative methods are presented. We also numerically study other aspects of the proposed framework, such as robustness and the underlying translation-equivariant encoding.},
  archive      = {J_JAIR},
  author       = {Saeid Pourmand and Wyatt D. Whiting and Alireza Aghasi and Nicholas F. Marshall},
  doi          = {10.1613/jair.1.17688},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1293-1323},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Laplace-HDC: Understanding the geometry of binary hyperdimensional computing},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving mutual information based feature selection by
boosting unique relevance. <em>JAIR</em>, <em>82</em>, 1267–1292. (<a
href="https://doi.org/10.1613/jair.1.17219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutual Information (MI) based feature selection makes use of MI to evaluate each feature and eventually shortlists a relevant feature subset, in order to address issues associated with high-dimensional datasets. Despite the effectiveness of MI in feature selection, we notice that many state-of-the-art algorithms disregard the so-called unique relevance (UR) of features, which is a necessary condition for the optimal feature subset. In our study of five representative MI based feature selection (MIBFS) algorithms, we find that all of them underperform as they ignore the UR of features and arrive at a suboptimal selected feature subset. We point out that the heart of the problem is that all these MIBFS algorithms follow the criterion of Maximize Relevance with Minimum Redundancy (MRwMR), which does not explicitly target UR. This motivates us to augment the existing criterion with the objective of boosting unique relevance (BUR), leading to a new criterion called MRwMR-BUR. Depending on the task being addressed, MRwMR-BUR has two variants, termed MRwMR-BUR-KSG and MRwMR-BUR-CLF, which estimate UR differently. MRwMR-BUR-KSG estimates UR via a nearest-neighbor based approach called the KSG estimator and is designed for three major tasks: (i) Classification Performance (i.e., higher classification accuracy). (ii) Feature Interpretability (i.e., a more precise selected feature subset for practitioners to explore the hidden relationship between features and labels). (iii) Classifier Generalization (i.e., the selected feature subset generalizes well to various classifiers). MRwMR-BUR-CLF estimates UR via a classifier based approach. It adapts UR to different classifiers, further improving the competitiveness of MRwMR-BUR for classification performance oriented tasks. The performance of MRwMR-BUR-KSG and MRwMR-BUR-CLF is validated via experiments using six public datasets and four popular classifiers. Specifically, as compared to MRwMR, the proposed MRwMR-BUR-KSG improves the test accuracy by 2% – 3% with 25% – 30% fewer features being selected, without increasing the algorithm complexity. MRwMR-BUR-CLF further improves the classification performance by 3.8% – 5.5% (relative to MRwMR), and it also outperforms three popular classifier dependent feature selection methods.},
  archive      = {J_JAIR},
  author       = {Shiyu Liu and Mehul Motani},
  doi          = {10.1613/jair.1.17219},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1267-1292},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improving mutual information based feature selection by boosting unique relevance},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic alignment of malicious question based on
contrastive semantic networks and data augmentation. <em>JAIR</em>,
<em>82</em>, 1243–1266. (<a
href="https://doi.org/10.1613/jair.1.16369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {&amp;nbsp;The identification and filtration of malicious texts in social media environments represent a significant technical challenge aimed at protecting users from online violence and disinformation. This complexity stems from the diversity and innovativeness of social media texts, which include unique expressions and special sentence structures. Particularly, malicious texts in interrogative forms pose alignment challenges with traditional corpora due to existing methods&#39; failure to exploit the text&#39;s deep global semantic representations. This issue is compounded by the scant research on Chinese texts, leading to inefficiencies in recognition accuracy. To mitigate these challenges, we introduce an innovative framework based on a Global Contrastive Semantic Network (GCSN), designed to enhance malicious text recognition efficiency and accuracy by deeply learning global semantic knowledge. It comprises an encoder for global semantic information modelling and a graph-matching network for semantic similarity evaluation between question pairs, enabling the accurate identification and filtering of malicious texts with complex structures. Furthermore, we introduce a semantic consistency-based data augmentation method (COMBINE), using real-world data to generate balanced positive and negative samples, enriching the dataset and enhancing the model&#39;s ability to distinguish semantic consistency through contrastive learning. Experimental validation on two Chinese datasets demonstrates our model&#39;s exceptional performance, affirming its applicationa value in social media malicious text recognition. Our code is available at https://github.com/Wxy13131313131/GCSN-COMBINE},
  archive      = {J_JAIR},
  author       = {Xinyan Wang and Jinshuo Liu and Juan Deng and Meng Wang and Qian Deng and Youcheng Yan and Lina Wang and Yunsong Ma and Jeff Z. Pan},
  doi          = {10.1613/jair.1.16369},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1243-1266},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Semantic alignment of malicious question based on contrastive semantic networks and data augmentation},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for belief-based programs and their
verification. <em>JAIR</em>, <em>82</em>, 1205–1242. (<a
href="https://doi.org/10.1613/jair.1.15796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Belief-based programming is a probabilistic extension of the GOLOG program family where every action and sensing result can be noisy and every test condition refers to the agent’s subjective beliefs. Inherited from GOLOG programs, the action-centered feature makes belief programs fairly suitable for high-level robot control under uncertainty. An important step before deploying such a program is to verify whether it satisfies certain properties. At least two problems exist in verifying such programs: how to formally specify program properties and what is the complexity of the verification problem. In this paper, we propose a formalism for belief programs based on a modal logic of actions and beliefs which allows us to conveniently express PCTL-like temporal properties. We also investigate the decidability and undecidability of the verification problem.},
  archive      = {J_JAIR},
  author       = {Daxin Liu and Gerhard Lakemeyer},
  doi          = {10.1613/jair.1.15796},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1205-1242},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A framework for belief-based programs and their verification},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rumor detection with adaptive data augmentation and
adversarial training. <em>JAIR</em>, <em>82</em>, 1175–1204. (<a
href="https://doi.org/10.1613/jair.1.16963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rumors are widely spread on social media, which has a negative impact on social stability. To address this problem, many rumor detection methods have been proposed. However, most existing methods overlook the potential impact of noise and adversarial attacks on their detection performance, which could compromise their effectiveness when applied in an unknown environment. To overcome these challenges and improve the framework robustness to noise and adversarial attacks, we propose a novel rumor detection framework with Adaptive Data Augmentation and Adversarial Training, named ADAAT. Our framework utilizes the adaptive data augmentation module to calculate the importance of edges and features and adaptively modify the less important among them with a greater probability. In addition, it contains a hard sample generation module which generates adversarial representations through adversarial training. These adversarial representations are treated as hard samples, which are utilized in contrastive learning to learn essential features, thereby improving the robustness of the framework. Our framework proves superiority in rumor detection tasks, increasing the accuracy by an average of 3.6%, 4.5% and 2.5% over the state-of-the-art methods on Twitter15, Twitter16 and PHEME, respectively. When the ADAAT framework is applied to attacked test data, the detection accuracy decreases by only 1.3%, 1.4%, and 1.2%. This paper appears in the AI &amp;amp; Society Track.},
  archive      = {J_JAIR},
  author       = {Ying Wang and Fuyuan Ma and Zhaoqi Yang and Yaodi Zhu and Bo Yang and Pengfei Shen and Lei Yun},
  doi          = {10.1613/jair.1.16963},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1175-1204},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Rumor detection with adaptive data augmentation and adversarial training},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
