<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ios_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="ios">IOS</h1>
<h2 id="jifs---24">JIFS - 24</h2>
<ul>
<li><details>
<summary>
(2025). Human activities recognition from video images by using
convolutional neural network. <em>JIFS</em>, <em>48</em>(6), 929–940.
(<a href="https://doi.org/10.3233/JIFS-236068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, automatic human activity recognition from video images is necessary for monitoring applications and caring for disabled people. The use of surveillance cameras and the processing of the obtained images leads to the achievement of a smart, accurate system for the recognition of human behavior. Since human detection in different scenes is associated with many challenges, several approaches have been implemented to detect human activity from video image processing. Due to the complexity of human activities, background noises and other factors affect the detection. For the solution of these problems, two deep learning-based algorithms have been described in the current article. According to the convolutional neural networks, the LSTM + CNN method and the 3D CNN method have been used to recognize the human activities in the images of the video. Each algorithm is explained and analyzed in detail. The experiments designed in this paper are performed by two datasets: the HMDB-51 dataset and the UCF101 dataset. In the HMDB-51 dataset, the highest obtained accuracy for CNN + LSTM method was equal to 70.2 and for method 3D CNN equal to 54.4. In the UCF101 dataset, the highest obtained accuracy for CNN + LSTM method was equal to 95.1 and for method 3D CNN equal to 90.8.},
  archive      = {J_JIFS},
  author       = {Wang, Dan and Yao, Jingfa and Zhang, Yanmin},
  doi          = {10.3233/JIFS-236068},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {929-940},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Human activities recognition from video images by using convolutional neural network},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep neural network for vehicle detection in aerial
images. <em>JIFS</em>, <em>48</em>(6), 915–927. (<a
href="https://doi.org/10.3233/JIFS-236059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research paper highlights the significance of vehicle detection in aerial images for surveillance systems, focusing on deep learning methods that outperform traditional approaches. However, the challenge of high computation complexity due to diverse vehicle appearances persists. The motivation behind this study is to highlight the crucial role of vehicle detection in aerial images for surveillance systems, emphasizing the superior performance of deep learning methods compared to traditional approaches. To address this, a lightweight deep neural network-based model is developed, striking a balance between accuracy and efficiency enabling real-time operation. The model is trained and evaluated on a standardized dataset, with extensive experiments demonstrating its ability to achieve accurate vehicle detection with significantly reduced computation costs, offering a practical solution for real-world aerial surveillance scenarios.},
  archive      = {J_JIFS},
  author       = {Du, Rong and Cheng, Yan},
  doi          = {10.3233/JIFS-236059},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {915-927},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {A deep neural network for vehicle detection in aerial images},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hosoya polynomial and wiener index of abid-waheed graph (
AW) a 8 and ( AW) a 10. <em>JIFS</em>, <em>48</em>(6), 907–914. (<a
href="https://doi.org/10.3233/JIFS-236051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular structures are characterised by the Hosoya polynomial and Wiener index, ideas from mathematical chemistry and graph theory. The graph representation of a chemical compound that has atoms as vertices and chemical bonds as edges is called a molecular graph, and the Hosoya polynomial is a polynomial related to this graph. As a graph attribute that remains unchanged under graph isomorphism, the Hosoya polynomial is known as a graph invariant. It offers details regarding the quantity of distinct non-empty subgraphs within a specified graph. A topological metric called the Wiener index is employed to measure the branching complexity and size of a molecular graph. For every pair of vertices in a molecular network, the Wiener index is the total of those distances. In this paper, discussed the Hosoya polynomial, Wiener index and Hyper-Wiener index of the Abid-Waheed graphs (AW) a 8 and (AW) a 10 . This graph is similar to Jahangir’s graph. Further, we have extended the research work on the applications of the described graphs.},
  archive      = {J_JIFS},
  author       = {Meenakshi, A. and Bramila, M.},
  doi          = {10.3233/JIFS-236051},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {907-914},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Hosoya polynomial and wiener index of abid-waheed graph ( AW) a 8 and ( AW) a 10},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian optimization based bloat prevention for secure IoT
healthcare. <em>JIFS</em>, <em>48</em>(6), 895–906. (<a
href="https://doi.org/10.3233/JIFS-235933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health information technology is a subcategory of health technology that covers medical and healthcare information technology. It allows for the secure exchange of health information among consumers, providers, payers, and quality monitors, as well as the management of health information across computerized systems. In recent scenario, Internet of Medical Things (IoMT) collects the medical healthcare data via sensors, are further transmitted to remote servers, to be evaluated by the doctors for earlier disease detection. Conversely, there is always a threat on using wireless communication and the user’s private data can be targeted by the attackers. In this paper, Bayesian optimization-based bloat prevention for secure IoT healthcare, for identifying Attacks in secure healthcare system (BO-BLOAT). The gathered input datasets are pre-processed using the Natural Language Processing (NLP) techniques namely Sentence segmentation, Tokenization, Word Stemming and Removing stop words for removing irrelevant data. After preprocessing the features are extracted using RNN-BiLSTM and feature selection technique is done by Bayesian Optimization. The deep learning (DL) based Mobilenet network is utilized for attack detection. Finally, the classification and identifying the types of attack is performed by using DL based Ghost net. For performance analysis, the two dataset is utilized namely UNBDS-NB-15, KDD99. The classification results show that the proposed BO-BLOAT model attains higher rate of accuracy in attack detection than existing models. The proposed BO-BLOAT method has been simulated using MATLAB. The Proposed BO-BLOAT method improves the overall accuracy of the proposed BO-BLOAT, HFL, LRO-S, and GOL is 99.04%, 93.47%, 92.82% and 90.64% respectively.},
  archive      = {J_JIFS},
  author       = {Ramya, M. and Sudhakaran, Pradeep},
  doi          = {10.3233/JIFS-235933},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {895-906},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Bayesian optimization based bloat prevention for secure IoT healthcare},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart edge segmentation and localization method for building
detection in satellite imagery. <em>JIFS</em>, <em>48</em>(6), 875–894.
(<a href="https://doi.org/10.3233/JIFS-235150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancements in satellite imaging technology have brought about an unprecedented influx of high-resolution satellite imagery. One of the critical tasks in this domain is the automated detection of buildings within satellite imagery. Building detection holds substantial significance for urban planning, disaster management, environmental monitoring, and various other applications. The challenges in this field are manifold, including variations in building sizes, shapes, orientations, and surrounding environments. Furthermore, satellite imagery often contains occlusions, shadows, and other artifacts that can hinder accurate building detection. The proposed method introduces a novel approach to improve the boundary detection of detected buildings in high-resolution remote sensed images having shadows and irregular shapes. It aims to enhance the accuracy of building detection and classification. The proposed algorithm is compared with Customized Faster R-CNNs and Single-Shot Multibox Detectors to show the significance of the results. We have used different datasets for training and evaluating the algorithm. Experimental results show that SESLM for Building Detection in Satellite Imagery can detect 98.5% of false positives at a rate of 8.4%. In summary, SESLM showcases high accuracy and improved robustness in detecting buildings, particularly in the presence of shadows.},
  archive      = {J_JIFS},
  author       = {Hashmi, Hina and Dwivedi, Rakesh and Kumar, Anil and Kumar, Aman},
  doi          = {10.3233/JIFS-235150},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {875-894},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Smart edge segmentation and localization method for building detection in satellite imagery},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on end-route planning for community group
purchasing for vehicles with different energy sources. <em>JIFS</em>,
<em>48</em>(6), 859–873. (<a
href="https://doi.org/10.3233/JIFS-234773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of final delivery route planning in the community group purchase model, this study takes into full consideration logistics vehicles of different energy types. With the goal of minimizing the sum of vehicle operating costs, delivery timeliness costs, goods loss costs, and carbon emissions costs, a multi-objective optimization model for community group purchase final delivery route planning is constructed. An improved genetic algorithm with a hill-climbing algorithm is utilized to enhance adaptive genetic operators, preventing the algorithm from getting stuck in local optima and improving the solution efficiency. Finally, a case study simulation is conducted to validate the feasibility of the model and algorithm. Experimental results indicate that currently, among the three types of vehicles, fuel logistics vehicles still have an advantage in terms of vehicle usage cost. Electric logistics vehicles exhibit the poorest performance with the highest cost per hundred kilometers, but their sole advantage lies in their high energy release efficiency, enabling optimal low-carbon vehicle performance. Battery-swapping logistics vehicles perform the best in terms of carbon emissions, combining the advantages of both fuel-based and electric logistics vehicles. Therefore, battery-swapping logistics vehicles are a favorable choice for replacing fuel-based logistics vehicles in the future, offering promising prospects for future development.},
  archive      = {J_JIFS},
  author       = {Wang, Jing and Gao, Tingting and Du, Hongxu and Tu, Chuang},
  doi          = {10.3233/JIFS-234773},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {859-873},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Research on end-route planning for community group purchasing for vehicles with different energy sources},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A text dependent copy-paste plagiarism and text-rewriting
plagiarism model. <em>JIFS</em>, <em>48</em>(6), 849–858. (<a
href="https://doi.org/10.3233/JIFS-234726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plagiarism is common in English writing exams. Researchers classify plagiarism into copy-paste and text-rewriting plagiarism, but existing models need help with problems such as the single way of checking and unsatisfactory results. Aiming at the copy-paste problem in English writing plagiarism, this paper proposes a digital fingerprint model based on the N-Gram window jumping mechanism. The model incorporates a sliding window and an improved matching tool to solve the problems of excessive fingerprint density and low checking efficiency in text extraction. Meanwhile, the model adds a Fisher-Yates shuffle algorithm with a salt parameter to crack the hash collision in text matching. The experiments show that the model can detect copy-paste plagiarism in English composition. For text rewriting plagiarism, this paper designs a TextCNN-BiGRU-based model, which combines TextCNN and BiGRU so that the extracted text semantic information considers the text’s local and contextual features. The experiments show that the model improves the accuracy by 1.9 percentage points and the F1 value by 1.2 percentage points on the MRPC dataset compared with other models.},
  archive      = {J_JIFS},
  author       = {Guo, Qingkai and Huang, Guimin and Wang, Yabing and Zhou, Ya},
  doi          = {10.3233/JIFS-234726},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {849-858},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {A text dependent copy-paste plagiarism and text-rewriting plagiarism model},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion of GBDT and neural network for click-through rate
estimation. <em>JIFS</em>, <em>48</em>(6), 835–847. (<a
href="https://doi.org/10.3233/JIFS-234713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the issue that the current click-through rate prediction methods ignore the varying impacts of different input features on prediction accuracy and exhibit low accuracy when dealing with large-scale data, a click-through rate prediction method (GBIFM) which combines Gradient Boosting Decision Tree (GBDT) and Input-aware Factorization Machine (IFM) is proposed in this paper. The proposed GBIFM method employs GBDT for data processing, which can flexibly handle various types of data without the need for one-hot encoding of discrete features. An Input-aware strategy is introduced to refine the weight vector and embedding vector of each feature for different instances, adaptively learning the impact of each input vector on feature representation. Furthermore, a fully connected network is incorporated to capture high-order features in a non-linear manner, enhancing the method’s ability to express and generalize complex structured data. A comprehensive experiment is conducted on the Criteo and Avazu datasets, the results show that compared to typical methods such as DeepFM, AFM, and IFM, the proposed method GBIFM can increase the AUC value by 10% –12% and decrease the Logloss value by 6% –20%, effectively improving the accuracy of click-through rate prediction.},
  archive      = {J_JIFS},
  author       = {Zhao, Bin and Cao, Wei and Zhang, Jiqun and Gao, Yilong and Li, Bin and Chen, Fengmei},
  doi          = {10.3233/JIFS-234713},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {835-847},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Fusion of GBDT and neural network for click-through rate estimation},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning model based glaucoma detection using retinal
images. <em>JIFS</em>, <em>48</em>(6), 823–834. (<a
href="https://doi.org/10.3233/JIFS-234131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The retinal illness that causes vision loss frequently on the globe is glaucoma. Hence, the earlier detection of Glaucoma is important. In this article, modified AlexNet deep leaning model is proposed to category the source retinal images into either healthy or Glaucoma through the detection and segmentations of optic disc (OD) and optic cup (OC) regions in retinal pictures. The retinal images are preprocessed and OD region is detected and segmented using circulatory filter. Further, OC regions are detected and segmented using K-means classification algorithm. Then, the segmented OD and OC region are classified and trained by the suggested AlexNet deep leaning model. This model classifies the source retinal image into either healthy or Glaucoma. Finally, performance measures have been estimated in relation to ground truth pictures in regards to accuracy, specificity and sensitivity. These performance measures are contrasted with the other previous Glaucoma detection techniques on publicly accessible retinal image datasets HRF and RIGA. The suggested technique as described in this work achieves 91.6% GDR for mild case and also achieves 100% GDR for severe case on HRF dataset. The suggested method as described in this work achieves 97.7% GDR for mild case and also achieves 100% GDR for severe case on RIGA dataset.},
  archive      = {J_JIFS},
  author       = {Ruby Elizabeth, J. and Kesavaraja, D. and Juliet, S. Ebenezer},
  doi          = {10.3233/JIFS-234131},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {823-834},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {A deep learning model based glaucoma detection using retinal images},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ComMatch: A semi-supervised learning classification
algorithm based on model calibration. <em>JIFS</em>, <em>48</em>(6),
811–822. (<a href="https://doi.org/10.3233/JIFS-233940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) aims to reduce reliance on labeled data. Achieving high performance often requires more complex algorithms, therefore, generic SSL algorithms are less effective when it comes to image classification tasks. In this study, we propose ComMatch, a simpler and more effective algorithm that combines negative learning, dynamic thresholding, and predictive stability discriminations into the consistency regularization approach. The introduction of negative learning is to help facilitate training by selecting negative pseudo-labels during stages when the network has low confidence. And ComMatch filters positive and negative pseudo-labels more accurately as training progresses by dynamic thresholds. Since high confidence does not always mean high accuracy due to network calibration issues, we also introduce network predictive stability, which filters out samples by comparing the standard deviation of the network output with a set threshold, thus largely reducing the influence of noise in the training process. ComMatch significantly outperforms existing algorithms over several datasets, especially when there is less labeled data available. For example, ComMatch achieves 1.82% and 3.6% error rate reduction over FlexMatch and FixMatch on CIFAR-10 with 40 labels respectively. And with 4000 labeled samples, ComMatch achieves 0.54% and 2.65% lower error rates than FixMatch and MixMatch, respectively.},
  archive      = {J_JIFS},
  author       = {Li, Ye and Zhou, Jingkang},
  doi          = {10.3233/JIFS-233940},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {811-822},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {ComMatch: A semi-supervised learning classification algorithm based on model calibration},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective community detection with topic modeling in article
recommender systems using LS-SLM and PCC-LDA. <em>JIFS</em>,
<em>48</em>(6), 793–809. (<a
href="https://doi.org/10.3233/JIFS-233851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an innovative approach, the LS-SLM (Local Search with Smart Local Moving) technique, for enhancing the efficiency of article recommendation systems based on community detection and topic modeling. The methodology undergoes rigorous evaluation using a comprehensive dataset extracted from the “dblp. v12.json” citation network. Experimental results presented herein provide a clear depiction of the superior performance of the LS-SLM technique when compared to established algorithms, namely the Louvain Algorithm (LA), Stochastic Block Model (SBM), Fast Greedy Algorithm (FGA), and Smart Local Moving (SLM). The evaluation metrics include accuracy, precision, specificity, recall, F-Score, modularity, Normalized Mutual Information (NMI), betweenness centrality (BTC), and community detection time. Notably, the LS-SLM technique outperforms existing solutions across all metrics. For instance, the proposed methodology achieves an accuracy of 96.32%, surpassing LA by 16% and demonstrating a 10.6% improvement over SBM. Precision, a critical measure of relevance, stands at 96.32%, showcasing a significant advancement over GCR-GAN (61.7%) and CR-HBNE (45.9%). Additionally, sensitivity analysis reveals that the LS-SLM technique achieves the highest sensitivity value of 96.5487%, outperforming LA by 14.2%. The LS-SLM also demonstrates superior specificity and recall, with values of 96.5478% and 96.5487%, respectively. The modularity performance is exceptional, with LS-SLM obtaining 95.6119%, significantly outpacing SLM, FGA, SBM, and LA. Furthermore, the LS-SLM technique excels in community detection time, completing the process in 38,652 ms, showcasing efficiency gains over existing techniques. The BTC analysis indicates that LS-SLM achieves a value of 94.6650%, demonstrating its proficiency in controlling information flow within the network.},
  archive      = {J_JIFS},
  author       = {Rachamadugu, Sandeep Kumar and Pushphavathi, T.P.},
  doi          = {10.3233/JIFS-233851},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {793-809},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Effective community detection with topic modeling in article recommender systems using LS-SLM and PCC-LDA},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical analysis of evolutionary computing approaches
for IoT security assessment. <em>JIFS</em>, <em>48</em>(6), 779–791. (<a
href="https://doi.org/10.3233/JIFS-233759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) strategy enables physical objects to easily produce, receive, and exchange data. IoT devices are getting more common in our daily lives, with diverse applications ranging from consumer sector to industrial and commercial systems. The rapid expansion and widespread use of IoT devices highlight the critical significance of solid and effective cybersecurity standards across the device development life cycle. Therefore, if vulnerability is exploited directly affects the IoT device and the applications. In this paper we investigated and assessed the various real-world critical IoT attacks/vulnerabilities that have affected IoT deployed in the commercial, industrial and consumer sectors since 2010. Subsequently, we evoke the vulnerabilities or type of attack, exploitation techniques, compromised security factors, intensity of vulnerability and impacts of the expounded real-world attacks/vulnerabilities. We first categorise how each attack affects information security parameters, and then we provide a taxonomy based on the security factors that are affected. Next, we perform a risk assessment of the security parameters that are encountered, using two well-known multi-criteria decision-making (MCDM) techniques namely Fuzzy-Analytic Hierarchy Process (F-AHP) and Fuzzy-Analytic Network Process (F-ANP) to determine the severity of severely impacted information security measures.},
  archive      = {J_JIFS},
  author       = {Kumar Sahu, Vinay and Pandey, Dhirendra and Singh, Priyanka and Haque Ansari, Md Shamsul and Khan, Asif and Varish, Naushad and Khan, Mohd Waris},
  doi          = {10.3233/JIFS-233759},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {6},
  pages        = {779-791},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {An empirical analysis of evolutionary computing approaches for IoT security assessment},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid LSTM-graph attention network for
cross-subject analysis on thinking and speaking state using EEG signals.
<em>JIFS</em>, <em>48</em>(5), 767–778. (<a
href="https://doi.org/10.3233/JIFS-233143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, the rapid advancement of deep learning has led to increased interest in utilizing Electroencephalogram (EEG) signals for automatic speech recognition. However, due to the significant variation observed in EEG signals from different individuals, the field of EEG-based speech recognition faces challenges related to individual differences across subjects, which ultimately impact recognition performance. In this investigation, a novel approach is proposed for EEG-based speech recognition that combines the capabilities of Long Short Term Memory (LSTM) and Graph Attention Network (GAT). The LSTM component of the model is designed to process sequential patterns within the data, enabling it to capture temporal dependencies and extract pertinent features. On the other hand, the GAT component exploits the interconnections among data points, which may represent channels, nodes, or features, in the form of a graph. This innovative model not only delves deeper into the connection between connectivity features and thinking as well as speaking states, but also addresses the challenge of individual disparities across subjects. The experimental results showcase the effectiveness of the proposed approach. When considering the thinking state, the average accuracy for single subjects and cross-subject are 65.7% and 67.3% respectively. Similarly, for the speaking state, the average accuracies were 65.4% for single subjects and 67.4% for cross-subject conditions, all based on the KaraOne dataset. These outcomes highlight the model’s positive impact on the task of cross-subject EEG speech recognition. The motivations for conducting cross subject are real world applicability, Generalization, Adaptation and personalization and performance evaluation.},
  archive      = {J_JIFS},
  author       = {Ramkumar, N. and Karthika Renuka, D.},
  doi          = {10.3233/JIFS-233143},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {767-778},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {A novel hybrid LSTM-graph attention network for cross-subject analysis on thinking and speaking state using EEG signals},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMF-MF: Interactive moment localization with adaptive
multimodal fusion and self-attention. <em>JIFS</em>, <em>48</em>(5),
755–766. (<a href="https://doi.org/10.3233/JIFS-233071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise video moment retrieval is crucial for enabling users to locate specific moments within a large video corpus. This paper presents Interactive Moment Localization with Multimodal Fusion (IMF-MF), a novel interactive moment localization with multimodal fusion model that leverages the power of self-attention to achieve state-of-the-art performance. IMF-MF effectively integrates query context and multimodal features, including visual and audio information, to accurately localize moments of interest. The model operates in two distinct phases: feature fusion and joint representation learning. The first phase dynamically calculates fusion weights for adapting the combination of multimodal video content, ensuring that the most relevant features are prioritized. The second phase employs bi-directional attention to tightly couple video and query features into a unified joint representation for moment localization. This joint representation captures long-range dependencies and complex patterns, enabling the model to effectively distinguish between relevant and irrelevant video segments. The effectiveness of IMF-MF is demonstrated through comprehensive evaluations on three benchmark datasets: TVR for closed-world TV episodes and Charades for open-world user-generated videos, DiDeMo dataset, Open-world, diverse video moment retrieval dataset. The empirical results indicate that the proposed approach surpasses existing state-of-the-art methods in terms of retrieval accuracy, as evaluated by metrics like Recall (R1, R5, R10, and R100) and Intersection-of-Union (IoU). The results consistently demonstrate IMF-MF’s superior performance compared to existing state-of-the-art methods, highlighting the benefits of its innovative interactive moment localization approach and the use of self-attention for feature representation and attention modeling.},
  archive      = {J_JIFS},
  author       = {Singh, Pratibha and Kushwaha, Alok Kumar Singh and Varshney, Neeraj},
  doi          = {10.3233/JIFS-233071},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {755-766},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {IMF-MF: Interactive moment localization with adaptive multimodal fusion and self-attention},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Further investigation on the super classical mean labeling
of graphs obtained from paths. <em>JIFS</em>, <em>48</em>(5), 747–753.
(<a href="https://doi.org/10.3233/JIFS-232328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the graph G , with the injection Ω from node set to the first p + q natural numbers. Let us assume that the ceiling function of the classical average of the node labels of the end nodes of each link is the induced link assignment Ω * . If the union of range of Ω of node set and the range of Ω * of link set is all the first p + q natural numbers, then Ω is called a classical mean labeling. A super classical mean graph is a graph with super classical mean labeling. In this research effort, we attempted to address the super classical meanness of graphs generated by paths and those formed by the union of two graphs.},
  archive      = {J_JIFS},
  author       = {Rajesh Kannan, A. and Thirupathi, G. and Murali Krishnan, S.},
  doi          = {10.3233/JIFS-232328},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {747-753},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Further investigation on the super classical mean labeling of graphs obtained from paths},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-task TSK fuzzy system modeling method based on
multi-task fuzzy clustering. <em>JIFS</em>, <em>48</em>(5), 731–746. (<a
href="https://doi.org/10.3233/JIFS-232312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional multi-task Takagi-Sugeno-Kang (TSK) fuzzy system modeling methods pay more attention to utilizing the inter-task correlation to learn the consequent parameters but ignore the importance of the antecedent parameters of the model. To this end, we propose a novel multi-task TSK fuzzy system modeling method based on multi-task fuzzy clustering. This method first proposes a novel multi-task fuzzy c-means clustering method that learns multiple specific clustering centers for each task and some common clustering centers for all tasks. Secondly, for the consequent parameters of the fuzzy system, the novel low-rank and row-sparse constraints are proposed to better implement multi-task learning. The experimental results demonstrate that the proposed model shows better performance compared with other existing methods.},
  archive      = {J_JIFS},
  author       = {Yao, Ziyang},
  doi          = {10.3233/JIFS-232312},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {731-746},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {A novel multi-task TSK fuzzy system modeling method based on multi-task fuzzy clustering},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive feature reduction with varied missing data and
feature selection for arthritis disease prediction. <em>JIFS</em>,
<em>48</em>(5), 715–729. (<a
href="https://doi.org/10.3233/JIFS-231537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the expansive domain of data-driven research, the curse of dimensionality poses challenges such as increased computational complexity, noise sensitivity, and the risk of overfitting models. Dimensionality reduction is vital to handle high-dimensional datasets effectively. The pilot study disease dataset (PSD) with 53 features contains patients with Rheumatoid Arthritis (RA) and Osteoarthritis (OA). Our work aims to reduce the dimension of the features in the PSD dataset, identify a suitable feature selection technique for the reduced-dimensional dataset, analyze an appropriate Machine Learning (ML) model, select significant features to predict the RA and OA disease and reveal significant features that predict the arthritis disease. The proposed study, Progressive Feature Reduction with Varied Missing Data (PFRVMD), was employed to reduce the dimension of features by using PCA loading scores in the random value imputed PSD dataset. Subsequently, notable feature selection methods, such as backward feature selection, the Boruta algorithm, the extra tree classifier, and forward feature selection, were implemented on the reduced-dimensional feature set. The significant features/biomarkers are obtained from the best feature selection technique. ML models such as the K-Nearest Neighbour Classifier (KNNC), Linear Discriminant Analysis (LDA), Logistic Regression (LR), Naïve Bayes Classifier (NBC), Random Forest Classifier (RFC) and Support Vector Classifier (SVC) are used to determine the best feature selection method. The results indicated that the Extra Tree Classifier (ETC) is the promising feature selection method for the PSD dataset because the significant features obtained from ETC depicted the highest accuracy on SVC.},
  archive      = {J_JIFS},
  author       = {Ramasamy, Uma and Santhoshkumar, Sundar},
  doi          = {10.3233/JIFS-231537},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {715-729},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Progressive feature reduction with varied missing data and feature selection for arthritis disease prediction},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive evaluation measures of nonlinear estimation
algorithm performance. <em>JIFS</em>, <em>48</em>(5), 705–714. (<a
href="https://doi.org/10.3233/JIFS-231376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many scholars say that their algorithms are better than others in the state estimation problem, only a fewer convincing algorithms were applied to engineering practices. The reason is that their algorithms outperform others only in some aspects such as the estimation accuracy or the computation load. To solve the problem of performance evaluation of state estimation algorithms, in this paper, the comprehensive evaluation measures (CEM) for evaluating the nonlinear estimation algorithm (NEA) is proposed, which can comprehensively reflect the performance of the NEAs. First, we introduce three types of the NEAs. Second, the CEM combining the flatness, estimation accuracy and computation time of the NEAs, is designed to evaluate the above NEAs. Finally, the superiority of the CEM is verified by a numerical example, which helps decision makers of nonlinear estimation algorithms theoretically and technically.},
  archive      = {J_JIFS},
  author       = {Peng, Weishi and Fang, Yangwang and Ma, Yongzhong},
  doi          = {10.3233/JIFS-231376},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {705-714},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Comprehensive evaluation measures of nonlinear estimation algorithm performance},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-disclosure ESG rating method based on the fuzzy set
and reward mechanism of disclosure. <em>JIFS</em>, <em>48</em>(5),
691–703. (<a href="https://doi.org/10.3233/JIFS-230777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The environmental, social, and governance (ESG) rating method is a powerful tool that can help investors to judge the investment value of companies based on the information disclosure. However, mainstream ESG rating methods ignore the distinction between companies with incomplete information disclosure and companies without information disclosure, which decreases the initiative and enthusiasm of companies to disclose information. In this study, a self-disclosure ESG (SDESG) rating method is proposed to evaluate companies’ ESG performance capabilities. First, based on the fuzzy set, fuzzy data is defined and applied to the SDESG rating method. Second, analogous to the academic reward system of a university, a reward mechanism of disclosure is used in the SDESG rating method. Finally, the effectiveness and reliability of the SDESG rating method are demonstrated through Refinitiv’s case. The results show that the SDESG rating method can distinguish companies with incomplete information disclosure from companies without information disclosure and allow companies that proactively disclose information to obtain better ESG scores under each industry. The implications of the study would increase companies’ enthusiasm to disclose information and maintain transparency within a company.},
  archive      = {J_JIFS},
  author       = {Yin, Songyi and Wang, Yu and Fu, Yelin},
  doi          = {10.3233/JIFS-230777},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {691-703},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {A self-disclosure ESG rating method based on the fuzzy set and reward mechanism of disclosure},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VIKOR optimization decision model based on poset.
<em>JIFS</em>, <em>48</em>(5), 673–689. (<a
href="https://doi.org/10.3233/JIFS-230680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vlsekriterijumska Optimizacija I Komprosmisno Resenie (VIKOR) method to some extent modifies the utility function to a value function that can consider different risk preferences. However, the weight and risk attitude parameters involved in the model are difficult to determine, which limits its application. To overcome this problem, a Poset-VIKOR model is proposed. A partial order set is a non-parametric decision-making method. Through the combination of partial order set and VIKOR model, the parameters can be “eliminated”, and a robust method that can run the model is obtained. This method uses the Hasse diagram to express the evaluation results, which can not only directly display the hierarchical and clustering information, but also show the robustness characteristics of the alternative comparison.},
  archive      = {J_JIFS},
  author       = {Yue, Lizhu and Lv, Yue},
  doi          = {10.3233/JIFS-230680},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {673-689},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {VIKOR optimization decision model based on poset},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated license plate authentication framework using
multi-view vehicle images. <em>JIFS</em>, <em>48</em>(5), 645–671. (<a
href="https://doi.org/10.3233/JIFS-230607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current framework for detecting Fake License Plates (FLP) in real-time is not robust enough for patrol teams. The objective of this paper is to develop a robust license plate authentication framework, based on the Vehicle Make and Model Recognition (VMMR) and the License Plate Recognition (LPR) algorithms that is implementable at the edge devices. The contributions of this paper are (i) Development of license plate database for 547 Indian cars, (ii) Development of an image dataset with 3173 images of 547 Indian cars in 8 classes, (iii) Development of an ensemble model to recognize vehicle make and model from frontal, rear, and side images, and (iv) Development of a framework to authenticate the license plates with frontal, rear, and side images. The proposed ensemble model is compared with the state-of-the-art networks from the literature. Among the implemented networks for VMMR, the Ensembling model with a size of 303.2 MB achieves the best accuracy of 89%. Due to the limited memory size, Easy OCR is chosen to recognize license plate. The total size of the authentication framework is 308 MB. The performance of the proposed framework is compared with the literature. According to the results, the proposed framework enhances FLP recognition due to the recognition of vehicles from side images. The dataset is made public at https://www.kaggle.com/ganeshmailecture/datasets .},
  archive      = {J_JIFS},
  author       = {Ganesh, M.A. and Saravana Perumaal, S. and Gomathi Sankar, S.M.},
  doi          = {10.3233/JIFS-230607},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {645-671},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Automated license plate authentication framework using multi-view vehicle images},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combined unet and CNN image classification model for COVID
disease detection using CXR/CT imaging. <em>JIFS</em>, <em>48</em>(5),
627–643. (<a href="https://doi.org/10.3233/JIFS-230523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate SARS-CoV-2 screening is made possible by automated Computer-Aided Diagnosis (CAD) which reduces the stress on healthcare systems. Since Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) is highly contagious, the transition chain can be broken through an early diagnosis by clinical knowledge and Artificial Intelligence (AI). Manual findings are time and labor-intensive. Even if Reverse Transcription-Polymerase Chain Reaction (RT-PCR) delivers quick findings, Chest X-ray (CXR) imaging is still a more trustworthy tool for disease classification and assessment. Several studies have been conducted using Deep Learning (DL) algorithms for COVID-19 detection. One of the biggest challenges in modernizing healthcare is extracting useful data from high-dimensional, heterogeneous, and complex biological data. Intending to introduce an automated COVID-19 diagnosis model, this paper develops a proficient optimization model that enhances the classification performance with better accuracy. The input images are initially pre-processed with an image filtering approach for noise removal and data augmentation to extend the dataset. Secondly, the images are segmented via U-Net and are given to classification using the Fused U-Net Convolutional Neural Network (FUCNN) model. Here, the performance of U-Net is enhanced through the modified Moth Flame Optimization (MFO) algorithm named Chaotic System-based MFO (CSMFO) by optimizing the weights of U-Net. The significance of the implemented model is confirmed over a comparative evaluation with the state-of-the-art models. Specifically, the proposed CSMFO-FUCNN attained 98.45% of accuracy, 98.63% of sensitivity, 98.98% of specificity, and 98.98% of precision.},
  archive      = {J_JIFS},
  author       = {Haennah, J.H. Jensha and Christopher, C. Seldev and King, G.R. Gnana},
  doi          = {10.3233/JIFS-230523},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {627-643},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Combined unet and CNN image classification model for COVID disease detection using CXR/CT imaging},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel prophet model based on gaussian linear fuzzy
information granule for long-term time series prediction 1.
<em>JIFS</em>, <em>48</em>(5), 611–625. (<a
href="https://doi.org/10.3233/JIFS-230313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper focuses on how to improve the prediction accuracy of time series and the interpretability of prediction results. First, a novel Prophet model based on Gaussian linear fuzzy approximate representation (GF-Prophet) is proposed for long-term prediction, which uniformly predicts the data with consistent trend characteristics. By taking Gaussian linear fuzzy information granules as inputs and outputs, GF-Prophet predicts with significantly smaller cumulative error. Second, noticing that trend extraction affects prediction accuracy seriously, a novel granulation modification algorithm is proposed to merge adjacent information granules that do not have significant differences. This is the first attempt to establish Prophet based on fuzzy information granules to predict trend characteristics. Experiments on public datasets show that the introduction of Gaussian linear fuzzy information granules significantly improves prediction performance of traditional Prophet model. Compared with other classical models, GF-Prophet has not only higher prediction accuracy, but also better interpretability, which can clearly give the change information, fluctuation amplitude and duration of a certain trend in the future that investors actually pay attention to.},
  archive      = {J_JIFS},
  author       = {Yang, Hong and Wang, Lina},
  doi          = {10.3233/JIFS-230313},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {611-625},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {A novel prophet model based on gaussian linear fuzzy information granule for long-term time series prediction 1},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent technique for traffic congestion prediction in
internet of vehicles using randomized machine learning. <em>JIFS</em>,
<em>48</em>(5), 597–609. (<a
href="https://doi.org/10.3233/JIFS-220929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic congestion is a challenging issue faced by people and government traffic agencies. Traffic congestion not only increases travel time but also increases noise pollution, air pollution, and financial losses. There are many factors which affect the speed of a vehicle. Some of the factors are weather, wind speed, road conditions, and construction work. On highways, the low speed of vehicles can cause traffic congestion or delays. Machine learning can play a vital role in the detection of traffic congestion and hence in avoiding delays. When accurate parameters and correct structure are fed to the machine learning model, traffic congestion can be predicted accurately. This paper designs a technique to predict traffic congestion states with the help of the Extra Tree Classifier machine learning model. The proposed Extremely Randomized Machine Learning (ERML) system model predicts 94% accuracy for congestion state classification. It gives better results as compared to other machine learning models.},
  archive      = {J_JIFS},
  author       = {Dureja, Ajay and , Suman and Dureja, Aman and Rathore, Rajkumar Singh},
  doi          = {10.3233/JIFS-220929},
  journal      = {Journal of Intelligent &amp; Fuzzy Systems},
  month        = {3},
  number       = {5},
  pages        = {597-609},
  shortjournal = {J. Intell. Fuzzy Syst.},
  title        = {Intelligent technique for traffic congestion prediction in internet of vehicles using randomized machine learning},
  volume       = {48},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="sw---20">SW - 20</h2>
<ul>
<li><details>
<summary>
(2025). Temporal relevance for representing learning over temporal
knowledge graphs. <em>SW</em>, <em>15</em>(6), 2695–2711. (<a
href="https://doi.org/10.3233/SW-243699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning for link prediction is one of the leading approaches to deal with incompleteness problem of real world knowledge graphs. Such methods are often called knowledge graph embedding models which represent entities and relationships in knowledge graphs in continuous vector spaces. By doing this, semantic relationships and patterns can be captured in the form of compact vectors. In temporal knowledge graphs, the connection of temporal and relational information is crucial for representing facts accurately. Relations provide the semantic context for facts, while timestamps indicate the temporal validity of facts. The importance of time is different for the semantics of different facts. Some relations in some temporal facts are time-insensitive, while others are highly time-dependent. However, existing embedding models often overlook the time sensitivity of different facts in temporal knowledge graphs. These models tend to focus on effectively representing connection between individual components of quadruples, consequently capturing only a fraction of the overall knowledge. Ignoring importance of temporal properties reduces the ability of temporal knowledge graph embedding models in accurately capturing these characteristics. To address these challenges, we propose a novel embedding model based on temporal relevance, which can effectively capture the time sensitivity of semantics and better represent facts. This model operates within a complex space with real and imaginary parts to effectively embed temporal knowledge graphs. Specifically, the real part of the final embedding of our proposed model captures semantic characteristic with temporal sensitivity by learning the relational information and temporal information through transformation and attention mechanism. Simultaneously, the imaginary part of the embeddings learns the connections between different elements in the fact without predefined weights. Our approach is evaluated through extensive experiments on the link prediction task, where it majorly outperforms state-of-the-art models. The proposed model also demonstrates remarkable effectiveness in capturing the complexities of temporal knowledge graphs.},
  archive      = {J_SW},
  author       = {Song, Bowen and Amouzouvi, Kossi and Xu, Chengjin and Wang, Maocai and Lehmann, Jens and Vahdati, Sahar},
  doi          = {10.3233/SW-243699},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2695-2711},
  shortjournal = {Semantic Web},
  title        = {Temporal relevance for representing learning over temporal knowledge graphs},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Background knowledge in ontology matching: A survey.
<em>SW</em>, <em>15</em>(6), 2639–2693. (<a
href="https://doi.org/10.3233/SW-223085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology matching is an integral part for establishing semantic interoperability. One of the main challenges within the ontology matching operation is semantic heterogeneity, i.e. modeling differences between the two ontologies that are to be integrated. The semantics within most ontologies or schemas are, however, typically incomplete because they are designed within a certain context which is not explicitly modeled. Therefore, external background knowledge plays a major role in the task of (semi-) automated ontology and schema matching. In this survey, we introduce the reader to the general ontology matching problem. We review the background knowledge sources as well as the approaches applied to make use of external knowledge. Our survey covers all ontology matching systems that have been presented within the years 2004–2021 at a well-known ontology matching competition together with systematically selected publications in the research field. We present a classification system for external background knowledge, concept linking strategies, as well as for background knowledge exploitation approaches. We provide extensive examples and classify all ontology matching systems under review in a resource/strategy matrix obtained by coalescing the two classification systems. Lastly, we outline interesting and yet underexplored research directions of applying external knowledge within the ontology matching process.},
  archive      = {J_SW},
  author       = {Portisch, Jan and Hladik, Michael and Paulheim, Heiko},
  doi          = {10.3233/SW-223085},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2639-2693},
  shortjournal = {Semantic Web},
  title        = {Background knowledge in ontology matching: A survey},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTab4D: Semantic annotation of tabular data with DBpedia.
<em>SW</em>, <em>15</em>(6), 2613–2637. (<a
href="https://doi.org/10.3233/SW-223098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic annotation of tabular data is the process of matching table elements with knowledge graphs. As a result, the table contents could be interpreted or inferred using knowledge graph concepts, enabling them to be useful in downstream applications such as data analytics and management. Nevertheless, semantic annotation tasks are challenging due to insufficient tabular data descriptions, heterogeneous schema, and vocabulary issues. This paper presents an automatic semantic annotation system for tabular data, called MTab4D, to generate annotations with DBpedia in three annotation tasks: 1) matching table cells to entities, 2) matching columns to entity types, and 3) matching pairs of columns to properties. In particular, we propose an annotation pipeline that combines multiple matching signals from different table elements to address schema heterogeneity, data ambiguity, and noisiness. Additionally, this paper provides insightful analysis and extra resources on benchmarking semantic annotation with knowledge graphs. Experimental results on the original and adapted datasets of the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab 2019) show that our system achieves an impressive performance for the three annotation tasks. MTab4D’s repository is publicly available at https://github.com/phucty/mtab4dbpedia .},
  archive      = {J_SW},
  author       = {Nguyen, Phuc and Kertkeidkachorn, Natthawut and Ichise, Ryutaro and Takeda, Hideaki},
  doi          = {10.3233/SW-223098},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2613-2637},
  shortjournal = {Semantic Web},
  title        = {MTab4D: Semantic annotation of tabular data with DBpedia},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The community solid server: Supporting research &amp;
development in an evolving ecosystem. <em>SW</em>, <em>15</em>(6),
2597–2611. (<a href="https://doi.org/10.3233/SW-243726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Solid project aims to empower people with control over their own data through the separation of data, identity, and applications. The goal is an environment with clear interoperability between all servers and clients that adhere to the specification. Solid is a standards-driven way to extend the Linked Data vision from public to private data, and everything in between. Multiple implementations of the Solid Protocol exist, but due to the evolving nature of the ecosystem, there is a strong need for an implementation that enables qualitative and quantitative research into new features and allows developers to quickly set up varying development environments. To meet these demands, we created the Community Solid Server, a modular server that can be configured to suit the needs of researchers and developers. In this article, we provide an overview of the server architecture and how it is positioned within the Solid ecosystem. The server supports many orthogonal feature combinations on axes such as authorization, authentication, and data storage. The Community Solid Server comes with several predefined configurations that allow researchers and developers to quickly set up servers with different content and backends, and can easily be modified to change many of its features. The server will help evolve the specification, and support further research into Solid and its possibilities.},
  archive      = {J_SW},
  author       = {Van Herwegen, Joachim and Verborgh, Ruben},
  doi          = {10.3233/SW-243726},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2597-2611},
  shortjournal = {Semantic Web},
  title        = {The community solid server: Supporting research &amp;amp; development in an evolving ecosystem},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRSC: From PG to RDF and back, using schemas. <em>SW</em>,
<em>15</em>(6), 2555–2595. (<a
href="https://doi.org/10.3233/SW-243675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Property graphs (PG) and RDF graphs are two popular database graph models, but they are not interoperable: data modeled in PG cannot be directly integrated with other data modeled in RDF. This lack of interoperability also impedes the use of the tools of one model when data are modeled in the other. In this paper, we propose PRSC, a configurable conversion to transform a PG into an RDF graph. This conversion relies on PG schemas and user-defined mappings called PRSC contexts. We also formally prove that a subset of PRSC contexts, called well-behaved contexts, can be used to reverse back to the original PG, and provide the related algorithm. Algorithms for conversion and reversion are available as open-source implementations.},
  archive      = {J_SW},
  author       = {Bruyat, Julian and Champin, Pierre-Antoine and Médini, Lionel and Laforest, Frédérique},
  doi          = {10.3233/SW-243675},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2555-2595},
  shortjournal = {Semantic Web},
  title        = {PRSC: From PG to RDF and back, using schemas},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on SPARQL query relaxation under the lens of RDF
reification. <em>SW</em>, <em>15</em>(6), 2507–2554. (<a
href="https://doi.org/10.3233/SW-243621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query relaxation has been proposed to cope with the problem of queries that produce none or insufficient answers. The goal is to modify these queries to be able to produce alternative results close to those expected in the original query. Existing approaches querying RDF datasets generally relax the SPARQL query constraints based on logical relaxations through RDFS entailment and RDFS ontologies. Techniques also exist that use the similarity of instances based on resource descriptions. These relaxation approaches defined for SPARQL queries over RDF triples have proved their efficiency. Nevertheless, significant challenges arise for query relaxation techniques in the presence of statement-level annotations, i.e., RDF reification. In this survey, we overview SPARQL query relaxation works with a particular focus on issues and challenges posed by representative RDF reification models, namely, standard reification, named graphs, n-ary relations, singleton properties, and RDF-Star.},
  archive      = {J_SW},
  author       = {Fakih, Ginwa and Serrano-Alvarado, Patricia},
  doi          = {10.3233/SW-243621},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2507-2554},
  shortjournal = {Semantic Web},
  title        = {A survey on SPARQL query relaxation under the lens of RDF reification},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path-based and triplification approaches to mapping data
into RDF: User behaviours and recommendations. <em>SW</em>,
<em>15</em>(6), 2479–2505. (<a
href="https://doi.org/10.3233/SW-243585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping complex structured data to RDF, e.g. for the creation of linked data, requires a clear understanding of the data, but also a clear understanding of the paradigm used by the mapping tool. We illustrate this with an empirical study comparing two different mapping tools, in particular considering the likelihood of user error. One tool uses path descriptions, e.g. JSONPath or XPath, to access data elements; the other uses a default triplification which can be queried, e.g. with SPARQL. As an example of the former, the study used YARRRML , to map from CSV, JSON and XML to RDF. As an example of the latter, the study used an extension of SPARQL, SPARQL Anything , to query the same data and CONSTRUCT a set of triples. Our study was a qualitative one, based on observing the kinds of errors made by participants using the two tools with identical mapping tasks, and using a grounded approach to categorize these errors. Whilst there are difficulties common to the two tools, there are also difficulties specific to each tool. For each tool, we present recommendations which help ensure that the mapping code is consistent with the data and the desired RDF. We propose future developments to reduce the difficulty users experience with YARRRML and SPARQL Anything. We also make some general recommendations about the future development of mapping tools and techniques. Finally, we propose some research questions for future investigation.},
  archive      = {J_SW},
  author       = {Warren, Paul and Mulholland, Paul and Daga, Enrico and Asprino, Luigi},
  doi          = {10.3233/SW-243585},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2479-2505},
  shortjournal = {Semantic Web},
  title        = {Path-based and triplification approaches to mapping data into RDF: User behaviours and recommendations},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RQSS: Referencing quality scoring system for wikidata.
<em>SW</em>, <em>15</em>(6), 2419–2475. (<a
href="https://doi.org/10.3233/SW-243695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wikidata is a collaborative multi-purpose Knowledge Graph (KG) with the unique feature of adding provenance data to the statements of items as a reference. More than 73% of Wikidata statements have provenance metadata; however, few studies exist on the referencing quality in this KG, focusing only on the relevancy and trustworthiness of external sources. While there are existing frameworks to assess the quality of Linked Data, and in some aspects their metrics investigate provenance, there are none focused on reference quality. We define a comprehensive referencing quality assessment framework based on Linked Data quality dimensions, such as completeness and understandability. We implement the objective metrics of the assessment framework as the Referencing Quality Scoring System – RQSS. The system provides quantified scores by which the referencing quality can be analyzed and compared. RQSS scripts can also be reused to monitor the referencing quality regularly. Due to the scale of Wikidata, we have used well-defined subsets to evaluate the quality of references in Wikidata using RQSS. We evaluate RQSS over three topical subsets: Gene Wiki, Music, and Ships, corresponding to three Wikidata WikiProjects, along with four random subsets of various sizes. The evaluation shows that RQSS is practical and provides valuable information, which can be used by Wikidata contributors and project holders to identify the quality gaps. Based on RQSS, the average referencing quality in Wikidata subsets is 0.58 out of 1. Random subsets (representative of Wikidata) have higher overall scores than topical subsets by 0.05, with Gene Wiki having the highest scores amongst topical subsets. Regarding referencing quality dimensions, all subsets have high scores in accuracy, availability, security, and understandability, but have weaker scores in completeness, verifiability, objectivity, and versatility. Although RQSS is developed based on the Wikidata RDF model, its referencing quality assessment framework can be applied to KGs in general.},
  archive      = {J_SW},
  author       = {Hosseini Beghaeiraveri, Seyed Amir and Gray, Alasdair and McNeill, Fiona},
  doi          = {10.3233/SW-243695},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2419-2475},
  shortjournal = {Semantic Web},
  title        = {RQSS: Referencing quality scoring system for wikidata},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On assessing weaker logical status claims in wikidata
cultural heritage records. <em>SW</em>, <em>15</em>(6), 2395–2417. (<a
href="https://doi.org/10.3233/SW-243686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work analyses the usage of different approaches adopted in Wikidata to represent information with weaker logical status (WLS, e.g., uncertain information, competing hypotheses, temporally evolving information). The study examines four main approaches: non-asserted statements, ranked statements, non-existing valued objects, and statements qualified with properties P5102 : nature of statement , P1480 : sourcing circumstances , and P2241 : reason for deprecated rank . We analyse their prevalence, success, and clarity in Wikidata. The analysis is performed over Cultural Heritage artefacts stored in Wikidata, divided into three subsets (i.e., visual heritage, textual heritage, and audio-visual heritage), and compared with astronomical data (stars and galaxies entities). Our findings indicate that (1) the presence of weaker logical status information is limited, with only a small proportion of items reporting such information, (2) the usage of WLS claims varies significantly between the two datasets in terms of prevalence and success of such approaches, and (3) precise assessment of WLS statements is made complicated by the ambiguities and overlappings between WLS and non-WLS claims allowed by the chosen representations. Finally, we list a few proposals to simplify and standardise this information representation in Wikidata, hoping to increase its clarity, accuracy and richness.},
  archive      = {J_SW},
  author       = {Di Pasquale, Alessio and Pasqual, Valentina and Tomasi, Francesca and Vitali, Fabio},
  doi          = {10.3233/SW-243686},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2395-2417},
  shortjournal = {Semantic Web},
  title        = {On assessing weaker logical status claims in wikidata cultural heritage records},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InteractOA: Showcasing the representation of knowledge from
scientific literature in wikidata. <em>SW</em>, <em>15</em>(6),
2381–2393. (<a href="https://doi.org/10.3233/SW-243685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge generated during the scientific process is still mostly stored in the form of scholarly articles. This lack of machine-readability hampers efforts to find, query, and reuse such findings efficiently and contributes to today’s information overload. While attempts have been made to semantify journal articles, widespread adoption of such approaches is still a long way off. One way to demonstrate the usefulness of such approaches to the scientific community is by showcasing the use of freely available, open-access knowledge graphs such as Wikidata as sustainable storage and representation solutions. Here we present an example from the life sciences in which knowledge items from scholarly literature are represented in Wikidata, linked to their exact position in open-access articles. In this way, they become part of a rich knowledge graph while maintaining clear ties to their origins. As example entities, we chose small regulatory RNAs (sRNAs) that play an important role in bacterial and archaeal gene regulation. These post-transcriptional regulators can influence the activities of multiple genes in various manners, forming complex interaction networks. We stored the information on sRNA molecule interaction taken from open-access articles in Wikidata and built an intuitive web interface called InteractOA , which makes it easy to visualize, edit, and query information. The tool also links information on small RNAs to their reference articles from PubMed Central on the statement level. InteractOA encourages researchers to contribute, save, and curate their own similar findings. InteractOA is hosted at https://interactoa.zbmed.de and its code is available under a permissive open source licence. In principle, the approach presented here can be applied to any other field of research.},
  archive      = {J_SW},
  author       = {Elhossary, Muhammad and Förstner, Konrad U.},
  doi          = {10.3233/SW-243685},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2381-2393},
  shortjournal = {Semantic Web},
  title        = {InteractOA: Showcasing the representation of knowledge from scientific literature in wikidata},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formalizing and validating wikidata’s property constraints
using SHACL and SPARQL. <em>SW</em>, <em>15</em>(6), 2333–2380. (<a
href="https://doi.org/10.3233/SW-243611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we delve into the crucial role of constraints in maintaining data integrity in knowledge graphs with a specific focus on Wikidata, one of the most extensive collaboratively maintained open data knowledge graphs on the Web. The World Wide Web Consortium (W3C) recommends the Shapes Constraint Language (SHACL) as the constraint language for validating Knowledge Graphs, which comes in two different levels of expressivity, SHACL-Core, as well as SHACL-SPARQL. Despite the availability of SHACL, Wikidata currently represents its property constraints through its own RDF data model, which relies on Wikidata’s specific reification mechanism based on authoritative namespaces, and – partially ambiguous – natural language definitions. In the present paper, we investigate whether and how the semantics of Wikidata property constraints, can be formalized using SHACL-Core, SHACL-SPARQL, as well as directly as SPARQL queries. While the expressivity of SHACL-Core turns out to be insufficient for expressing all Wikidata property constraint types, we present SPARQL queries to identify violations for all 32 current Wikidata constraint types. We compare the semantics of this unambiguous SPARQL formalization with Wikidata’s violation reporting system and discuss limitations in terms of evaluation via Wikidata’s public SPARQL query endpoint, due to its current scalability. Our study, on the one hand, sheds light on the unique characteristics of constraints defined by the Wikidata community, in order to improve the quality and accuracy of data in this collaborative knowledge graph. On the other hand, as a “byproduct”, our formalization extends existing benchmarks for both SHACL and SPARQL with a challenging, large-scale real-world use case.},
  archive      = {J_SW},
  author       = {Ferranti, Nicolas and De Souza, Jairo Francisco and Ahmetaj, Shqiponja and Polleres, Axel},
  doi          = {10.3233/SW-243611},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2333-2380},
  shortjournal = {Semantic Web},
  title        = {Formalizing and validating wikidata’s property constraints using SHACL and SPARQL},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using wikidata lexemes and items to generate text from
abstract representations. <em>SW</em>, <em>15</em>(6), 2319–2332. (<a
href="https://doi.org/10.3233/SW-243564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ninai/Udiron, a living function-based natural language generation system, uses knowledge in Wikidata lexemes and items to transform abstract representations of factual statements into human-readable text. The combined system first produces syntax trees based on those abstract representations (Ninai) and then yields sentences from those syntax trees (Udiron). The system relies on information about individual lexical units and links to the concepts those units represent, as well as rules encoded in various types of functions to which users may contribute, to make decisions about words, phrases, and other morphemes to use and how to arrange them. Various system design choices work toward using the information in Wikidata lexemes and items efficiently and effectively, making different components individually contributable and extensible, and making the overall resultant outputs from the system expectable and analyzable. These targets accompany the intentions for Ninai/Udiron to ultimately power the Abstract Wikipedia project as well as be hosted on the Wikifunctions project.},
  archive      = {J_SW},
  author       = {Morshed, Mahir},
  doi          = {10.3233/SW-243564},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2319-2332},
  shortjournal = {Semantic Web},
  title        = {Using wikidata lexemes and items to generate text from abstract representations},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical ontology design patterns and shapes from wikidata.
<em>SW</em>, <em>15</em>(6), 2293–2317. (<a
href="https://doi.org/10.3233/SW-243613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ontology underlying the Wikidata knowledge graph (KG) has not been formalized. Instead, its semantics emerges bottom-up from the use of its classes and properties. Flexible guidelines and rules have been defined by the Wikidata project for the use of its ontology, however, it is still often difficult to reuse the ontology’s constructs. Based on the assumption that identifying ontology design patterns from a knowledge graph contributes to making its (possibly) implicit ontology emerge, in this paper we present a method for extracting what we term empirical ontology design patterns (EODPs) from a knowledge graph. This method takes as input a knowledge graph and extracts EODPs as sets of axioms/constraints involving the classes instantiated in the KG. These EODPs include data about the probability of such axioms/constraints happening . We apply our method on two domain-specific portions of Wikidata, addressing the music and art, architecture, and archaeology domains, and we compare the empirical ontology design patterns we extract with the current support present in Wikidata. We show how these patterns can provide guidance for the use of the Wikidata ontology and its potential improvement, and can give insight into the content of (domain-specific portions of) the Wikidata knowledge graph.},
  archive      = {J_SW},
  author       = {Carriero, Valentina Anita and Groth, Paul and Presutti, Valentina},
  doi          = {10.3233/SW-243613},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2293-2317},
  shortjournal = {Semantic Web},
  title        = {Empirical ontology design patterns and shapes from wikidata},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can you trust wikidata? <em>SW</em>, <em>15</em>(6),
2271–2292. (<a href="https://doi.org/10.3233/SW-243577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to use a value retrieved from a Knowledge Graph (KG) for some computation, the user should, in principle, ensure that s/he trusts the veracity of the claim, i.e., considers the statement as a fact. Crowd-sourced KGs, or KGs constructed by integrating several different information sources of varying quality, must be used via a trust layer. The veracity of each claim in the underlying KG should be evaluated, considering what is relevant to carrying out some action that motivates the information seeking. The present work aims to assess how well Wikidata (WD) supports the trust decision process implied when using its data. WD provides several mechanisms that can support this trust decision, and our KG Profiling, based on WD claims and schema, elaborates an analysis of how multiple points of view, controversies, and potentially incomplete or incongruent content are presented and represented.},
  archive      = {J_SW},
  author       = {Santos, Veronica and Schwabe, Daniel and Lifschitz, Sérgio},
  doi          = {10.3233/SW-243577},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2271-2292},
  shortjournal = {Semantic Web},
  title        = {Can you trust wikidata?},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evidence of large-scale conceptual disarray in multi-level
taxonomies in wikidata. <em>SW</em>, <em>15</em>(6), 2253–2270. (<a
href="https://doi.org/10.3233/SW-243562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distinction between types and individuals is key to most conceptual modeling techniques and knowledge representation languages. Despite that, there are a number of situations in which modelers navigate this distinction inadequately, leading to problematic models. We show evidence of a large number of representation mistakes associated with the failure to employ this distinction in the Wikidata knowledge graph, which can be identified with the incorrect use of instantiation , which is a relation between an instance and a type, and specialization (or subtyping ), which is a relation between two types. The prevalence of the problems in Wikidata’s taxonomies suggests that methodological and computational tools are required to mitigate the issues identified, which occur in many settings when individuals, types, and their metatypes are included in the domain of interest. We conduct a conceptual analysis of entities involved in recurrent erroneous cases identified in this empirical data, and present a tool that supports users in identifying some of these mistakes.},
  archive      = {J_SW},
  author       = {Dadalto, Atílio A. and Almeida, João Paulo A. and Fonseca, Claudenir M. and Guizzardi, Giancarlo},
  doi          = {10.3233/SW-243562},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2253-2270},
  shortjournal = {Semantic Web},
  title        = {Evidence of large-scale conceptual disarray in multi-level taxonomies in wikidata},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dura-europos stories: Developing interactive storytelling
applications using knowledge graphs for cultural heritage exploration.
<em>SW</em>, <em>15</em>(6), 2237–2251. (<a
href="https://doi.org/10.3233/SW-243552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Dura-Europos Stories, a multimedia application for viewing artifacts and places related to the Dura-Europos archaeological excavation. We describe the process of mapping data to the Wikidata data model as well as the process of contributing data to Wikidata. We provide an overview of the functionality of an interactive application for viewing images of the artifacts in the context of their metadata. We contextualize this project as an example of using knowledge graphs in research projects in order to leverage technologies of the Semantic Web in such a way that data related to the project can be easily combined with other data on the web. Presenting artifacts in this story-based application allows users to explore these objects visually, and provides pathways for further exploration of related information.},
  archive      = {J_SW},
  author       = {Thornton, Katherine and Seals-Nutt, Kenneth and Chen, Anne},
  doi          = {10.3233/SW-243552},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2237-2251},
  shortjournal = {Semantic Web},
  title        = {Dura-europos stories: Developing interactive storytelling applications using knowledge graphs for cultural heritage exploration},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wikidata subsetting: Approaches, tools, and evaluation.
<em>SW</em>, <em>15</em>(6), 2209–2235. (<a
href="https://doi.org/10.3233/SW-233491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wikidata is a massive Knowledge Graph (KG), including more than 100 million data items and nearly 1.5 billion statements, covering a wide range of topics such as geography, history, scholarly articles, and life science data. The large volume of Wikidata is difficult to handle for research purposes; many researchers cannot afford the costs of hosting 100 GB of data. While Wikidata provides a public SPARQL endpoint, it can only be used for short-running queries. Often, researchers only require a limited range of data from Wikidata focusing on a particular topic for their use case. Subsetting is the process of defining and extracting the required data range from the KG; this process has received increasing attention in recent years. Specific tools and several approaches have been developed for subsetting, which have not been evaluated yet. In this paper, we survey the available subsetting approaches, introducing their general strengths and weaknesses, and evaluate four practical tools specific for Wikidata subsetting – WDSub, KGTK, WDumper, and WDF – in terms of execution performance, extraction accuracy, and flexibility in defining the subsets. Results show that all four tools have a minimum of 99.96% accuracy in extracting defined items and 99.25% in extracting statements. The fastest tool in extraction is WDF, while the most flexible tool is WDSub. During the experiments, multiple subset use cases have been defined and the extracted subsets have been analyzed, obtaining valuable information about the variety and quality of Wikidata, which would otherwise not be possible through the public Wikidata SPARQL endpoint.},
  archive      = {J_SW},
  author       = {Hosseini Beghaeiraveri, Seyed Amir and Labra Gayo, Jose Emilio and Waagmeester, Andra and Ammar, Ammar and Gonzalez, Carolina and Slenter, Denise and Ul-Hasan, Sabah and Willighagen, Egon and McNeill, Fiona and Gray, Alasdair J.G.},
  doi          = {10.3233/SW-233491},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2209-2235},
  shortjournal = {Semantic Web},
  title        = {Wikidata subsetting: Approaches, tools, and evaluation},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QALD-10 – the 10th challenge on question answering over
linked data. <em>SW</em>, <em>15</em>(6), 2193–2207. (<a
href="https://doi.org/10.3233/SW-233471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Question Answering (KGQA) has gained attention from both industry and academia over the past decade. Researchers proposed a substantial amount of benchmarking datasets with different properties, pushing the development in this field forward. Many of these benchmarks depend on Freebase, DBpedia, or Wikidata. However, KGQA benchmarks that depend on Freebase and DBpedia are gradually less studied and used, because Freebase is defunct and DBpedia lacks the structural validity of Wikidata. Therefore, research is gravitating toward Wikidata-based benchmarks. That is, new KGQA benchmarks are created on the basis of Wikidata and existing ones are migrated. We present a new, multilingual, complex KGQA benchmarking dataset as the 10th part of the Question Answering over Linked Data (QALD) benchmark series. This corpus formerly depended on DBpedia. Since QALD serves as a base for many machine-generated benchmarks, we increased the size and adjusted the benchmark to Wikidata and its ranking mechanism of properties. These measures foster novel KGQA developments by more demanding benchmarks. Creating a benchmark from scratch or migrating it from DBpedia to Wikidata is non-trivial due to the complexity of the Wikidata knowledge graph, mapping issues between different languages, and the ranking mechanism of properties using qualifiers. We present our creation strategy and the challenges we faced that will assist other researchers in their future work. Our case study, in the form of a conference challenge, is accompanied by an in-depth analysis of the created benchmark.},
  archive      = {J_SW},
  author       = {Usbeck, Ricardo and Yan, Xi and Perevalov, Aleksandr and Jiang, Longquan and Schulz, Julius and Kraft, Angelie and Möller, Cedric and Huang, Junbo and Reineke, Jan and Ngonga Ngomo, Axel-Cyrille and Saleem, Muhammad and Both, Andreas},
  doi          = {10.3233/SW-233471},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2193-2207},
  shortjournal = {Semantic Web},
  title        = {QALD-10 – the 10th challenge on question answering over linked data},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ProVe: A pipeline for automated provenance verification of
knowledge graphs against textual sources. <em>SW</em>, <em>15</em>(6),
2159–2192. (<a href="https://doi.org/10.3233/SW-233467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs are repositories of information that gather data from a multitude of domains and sources in the form of semantic triples, serving as a source of structured data for various crucial applications in the modern web landscape, from Wikipedia infoboxes to search engines. Such graphs mainly serve as secondary sources of information and depend on well-documented and verifiable provenance to ensure their trustworthiness and usability. However, their ability to systematically assess and assure the quality of this provenance, most crucially whether it properly supports the graph’s information, relies mainly on manual processes that do not scale with size. ProVe aims at remedying this, consisting of a pipelined approach that automatically verifies whether a Knowledge Graph triple is supported by text extracted from its documented provenance. ProVe is intended to assist information curators and consists of four main steps involving rule-based methods and machine learning models: text extraction, triple verbalisation, sentence selection, and claim verification. ProVe is evaluated on a Wikidata dataset, achieving promising results overall and excellent performance on the binary classification task of detecting support from provenance, with 87.5 % accuracy and 82.9 % F1-macro on text-rich sources. The evaluation data and scripts used in this paper are available in GitHub and Figshare.},
  archive      = {J_SW},
  author       = {Amaral, Gabriel and Rodrigues, Odinaldo and Simperl, Elena},
  doi          = {10.3233/SW-233467},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2159-2192},
  shortjournal = {Semantic Web},
  title        = {ProVe: A pipeline for automated provenance verification of knowledge graphs against textual sources},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Psychiq and wwwyzzerdd: Wikidata completion using wikipedia.
<em>SW</em>, <em>15</em>(6), 2145–2158. (<a
href="https://doi.org/10.3233/SW-233450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its size, Wikidata remains incomplete and inaccurate in many areas. Hundreds of thousands of articles on English Wikipedia have zero or limited meaningful structure on Wikidata. Much work has been done in the literature to partially or fully automate the process of completing knowledge graphs, but little of it has been practically applied to Wikidata. This paper presents two interconnected practical approaches to speeding up the Wikidata completion task. The first is Wwwyzzerdd, a browser extension that allows users to quickly import statements from Wikipedia to Wikidata. Wwwyzzerdd has been used to make over 100 thousand edits to Wikidata. The second is Psychiq, a new model for predicting instance and subclass statements based on English Wikipedia articles. Psychiq’s performance and characteristics make it well suited to solving a variety of problems for the Wikidata community. One initial use is integrating the Psychiq model into the Wwwyzzerdd browser extension.},
  archive      = {J_SW},
  author       = {Erenrich, Daniel},
  doi          = {10.3233/SW-233450},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2145-2158},
  shortjournal = {Semantic Web},
  title        = {Psychiq and wwwyzzerdd: Wikidata completion using wikipedia},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
