<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dke---6">DKE - 6</h2>
<ul>
<li><details>
<summary>
(2025). Fake news detection algorithms – a systematic literature
review. <em>DKE</em>, <em>158</em>, 102441. (<a
href="https://doi.org/10.1016/j.datak.2025.102441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media and news platforms make available to their users, in real-time and simultaneously, access to a significant amount of content that may be true or false. It is remarkable that, with the evolution of Industry 4.0 technologies, the production and dissemination of fake news also increased in recent years. Some content quickly reaches considerable popularity because it is accessed and shared on a large scale, especially in social networks, thus having a potential for going viral. Thus, this study aimed to identify the algorithms and software used for fake news detection. The choice for this combination is justified because in Brazil this process is carried out manually by verification agencies and thus, based on the mapping of the algorithms identified in the literature, an architecture proposal will be developed using artificial intelligence. As a methodology, a systematic literature review (SLR) was conducted in the Science Direct and Scopus databases using the keywords &quot;fake news&quot; and &quot;machine learning&quot; to locate reviews and research articles published in Engineering fields from 2018 to 2023. A total of 24 articles were analyzed, and the results pointed out that Facebook and X 1 were the social networks most used to disseminate fake news. Moreover, the main topics addressed were the COVID-19 pandemic and the United States presidential elections of 2016 and 2020. As for the most used algorithms, a predominance of neural networks was observed. The contribution of this study is in mapping the most used algorithms and their degree of assertiveness, as well as identifying the themes, countries and related researchers that help in the evolution of the fake news theme.},
  archive      = {J_DKE},
  author       = {Ana Julia Dal Forno and Graziela Piccoli Richetti and Vinícius Heinz Knaesel},
  doi          = {10.1016/j.datak.2025.102441},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102441},
  shortjournal = {Data Knowl. Eng.},
  title        = {Fake news detection algorithms – a systematic literature review},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling process durations with gamma mixtures for
right-censored data: Applications in customer clustering, pattern
recognition, drift detection, and rationalisation. <em>DKE</em>,
<em>158</em>, 102430. (<a
href="https://doi.org/10.1016/j.datak.2025.102430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer modelling, particularly concerning length of stay or process duration, is vital for identifying customer patterns and optimising business processes. Recent advancements in computing and database technologies have revolutionised statistics and business process analytics by producing heterogeneous data that reflects diverse customer behaviours. Different models should be employed for distinct customer categories, culminating in an overall mixture model. Furthermore, some customers may remain “alive” at the conclusion of the observation period, meaning their journeys are incomplete, resulting in right-censored (RC) duration data. This combination of heterogeneous and right-censored data introduces complexity to process duration modelling and analysis. This paper presents a general approach to modelling process duration data using a gamma mixture model, where each gamma distribution represents a specific customer pattern. The model is adapted to account for RC data by modifying the likelihood function during model fitting. The paper explores three key application scenarios: (1) offline pattern clustering, which categorises customers who have completed their journeys; (2) online pattern tracking, which monitors and predicts customer behaviours in real-time; and (3) concept drift detection and rationalisation, which identifies shifts in customer patterns and explains their underlying causes. The proposed method has been validated using synthetically generated data and real-world data from a hospital billing process. In all instances, the fitted models effectively represented the data and demonstrated strong performance across the three application scenarios.},
  archive      = {J_DKE},
  author       = {Lingkai Yang and Sally McClean and Kevin Burke and Mark Donnelly and Kashaf Khan},
  doi          = {10.1016/j.datak.2025.102430},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102430},
  shortjournal = {Data Knowl. Eng.},
  title        = {Modelling process durations with gamma mixtures for right-censored data: Applications in customer clustering, pattern recognition, drift detection, and rationalisation},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accessibility in conceptual modeling—a systematic literature
review, a keyboard-only UML modeling tool, and a research roadmap.
<em>DKE</em>, <em>158</em>, 102423. (<a
href="https://doi.org/10.1016/j.datak.2025.102423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reports on Disability by the World Health Organization show that the number of people with disabilities is increasing. Consequently, accessibility should play an essential role in information systems engineering research. While there is an increasingly rich set of available web accessibility guidelines, testing frameworks, and generally accessibility features in modern web-based software systems, software development frameworks, and Integrated Development Environments, this paper shows, based on a systematic review of the literature and current modeling tools, that accessibility is, so far, only scarcely focused in conceptual modeling research. With this paper, we assess the state of the art of accessibility in conceptual modeling, we identify current research gaps, and we delineate a vision toward more accessible conceptual modeling methods and tools. As a concrete step forward toward this vision, we present a generic concept of a keyboard-only modeling tool interaction that is implemented as a new module for the Graphical Language Server Platform (GLSP) framework. We show—using a currently developed UML modeling tool—how efficiently this module allows GLSP-based tool developers to introduce accessibility features into their modeling tools, thereby engaging physically disabled users in conceptual modeling.},
  archive      = {J_DKE},
  author       = {Aylin Sarioğlu and Haydar Metin and Dominik Bork},
  doi          = {10.1016/j.datak.2025.102423},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102423},
  shortjournal = {Data Knowl. Eng.},
  title        = {Accessibility in conceptual modeling—A systematic literature review, a keyboard-only UML modeling tool, and a research roadmap},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving cross-network service recommendation via
federated learning of unified user representations. <em>DKE</em>,
<em>158</em>, 102422. (<a
href="https://doi.org/10.1016/j.datak.2025.102422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of cloud computing, the Internet of Things, and other large-scale environments, recommender systems have been faced with several issues, mainly (i) the distribution of user–item data across multiple information networks, (ii) privacy restrictions and the partial profiling of users and items caused by this distribution, (iii) the heterogeneity of user–item knowledge in different information networks. Furthermore, most approaches perform recommendations based on a single source of information, and do not handle the partial representation of users’ and items’ information in a federated way. Such isolated and non-collaborative behavior, in multi-source and cross-network information settings, often results in inaccurate and low-quality recommendations. To address these issues, we exploit the strengths of network representation learning and federated learning to propose a service recommendation approach in smart service networks. While NRL is employed to learn rich representations of entities (e.g., users, services, IoT objects), federated learning helps collaboratively infer a unified profile of users and items, based on the concept of anchor user , which are bridge entities connecting multiple information networks. These unified profiles are, finally, fed into a federated recommendation algorithm to select the top-rated services. Using a scenario from the smart healthcare context, the proposed approach was developed and validated on a multiplex information network built from real-world electronic medical records (157 diseases, 491 symptoms, 273 174 patients, treatments and anchors data). Experimental results under varied federated settings demonstrated the utility of cross-client knowledge (i.e. anchor links) and the collaborative reconstruction of composite embeddings (i.e. user representations) for improving recommendation accuracy. In terms of RMSE@K and MAE@K, our approach achieved an improvement of 54.41% compared to traditional single-network recommendation, as long as the federation and communication scale increased. Moreover, the gap with four federated approaches has reached 19.83 %, highlighting our approach’s ability to map local embeddings (i.e. user’s partial representations) into a complete view.},
  archive      = {J_DKE},
  author       = {Mohamed Gaith Ayadi and Haithem Mezni and Hela Elmannai and Reem Ibrahim Alkanhel},
  doi          = {10.1016/j.datak.2025.102422},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102422},
  shortjournal = {Data Knowl. Eng.},
  title        = {Privacy-preserving cross-network service recommendation via federated learning of unified user representations},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph theoretic approach to assess quality of data for
classification task. <em>DKE</em>, <em>158</em>, 102421. (<a
href="https://doi.org/10.1016/j.datak.2025.102421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correctness of predictions rendered by an AI/ML model is key to its acceptability. To foster researchers’ and practitioners’ confidence in the model, it is necessary to render an intuitive understanding of the workings of a model. In this work, we attempt to explain a model’s working by providing some insights into the quality of data. While doing this, it is essential to consider that revealing the training data to the users is not feasible for logistical and security reasons. However, sharing some interpretable parameters of the training data and correlating them with the model’s performance can be helpful in this regard. To this end, we propose a new measure based on Euclidean Minimum Spanning Tree (EMST) for quantifying the intrinsic separation (or overlaps) between the data classes. For experiments, we use datasets from diverse domains such as finance, medical, and marketing. We use state-of-the-art measure known as Davies Bouldin Index (DBI) to validate our approach on four different datasets from aforementioned domains. The experimental results of this study establish the viability of the proposed approach in explaining the working and efficiency of a classifier. Firstly, the proposed measure of class-overlap quantification has shown a better correlation with the classification performance as compared to DBI scores. Secondly, the results on multi-class datasets demonstrate that the proposed measure can be used to determine the feature importance so as to learn a better classification model.},
  archive      = {J_DKE},
  author       = {Payel Sadhukhan and Samrat Gupta},
  doi          = {10.1016/j.datak.2025.102421},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102421},
  shortjournal = {Data Knowl. Eng.},
  title        = {A graph theoretic approach to assess quality of data for classification task},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CriMOnto: A generalized domain-specific ontology for
modeling procedural norms of the lebanese criminal law. <em>DKE</em>,
<em>158</em>, 102419. (<a
href="https://doi.org/10.1016/j.datak.2025.102419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Criminal (or penal) law regulates offenses, offenders, and legal punishments. Modeling criminal law is gaining much attention in the ontology engineering community. However, a significant aspect is neglected: the explicit representation of procedural knowledge. Procedural norms, such as regulative norms, are addressed to agents in the normative system. They govern the different interactions among these agents. In this study, we propose a formal and faithful representation of the procedural aspect of legal norms in the context of the Lebanese Criminal Code. A modular domain-specific ontology named CriMOnto is developed for this purpose. CriMOnto is grounded in the Unified Foundational Ontology (UFO) and the legal core ontology UFO-L by applying the Ontology-Driven Conceptual Modeling (ODCM) process. Conceptual Ontology Patterns (COPs) are reused from UFO and UFO-L to build the hierarchical and procedural content of the ontology. CriMOnto is validated as a formal ontology and evaluated using a dual evaluation approach. The potential use of CriMOnto for lightweight rule-based decision support is discussed in this study.},
  archive      = {J_DKE},
  author       = {Mirna El Ghosh and Hala Naja and Habib Abdulrab and Mohamad Khalil},
  doi          = {10.1016/j.datak.2025.102419},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102419},
  shortjournal = {Data Knowl. Eng.},
  title        = {CriMOnto: A generalized domain-specific ontology for modeling procedural norms of the lebanese criminal law},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
