<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EJOR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ejor---44">EJOR - 44</h2>
<ul>
<li><details>
<summary>
(2025). Investigating the research and development performance of
chinese industry: A two-stage prospect data envelopment analysis
approach. <em>EJOR</em>, <em>323</em>(3), 1040–1059. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing investments inindustry research and development (R&amp;D) innovation in China, evaluating whether R&amp;D resources assigned to industries areeffectively used is essential. However, limited research has been conducted on the assessment of R&amp;D effectiveness in Chinese industries that encompasses both the internal process of R&amp;D production and the psychological risks encountered by decision-makers (DMs).Hence, this study puts forward a two-stage prospect data envelopment analysis approach that can characterise the risk attitude of DM in evaluation. By employing this approach, we assess the R&amp;D activities of 28 industries in China from an overall perspective and explore the actual influence of DMs’ risk psychology on the evaluation results through sensitivity and comparative analyses. Furthermore, we categorise the R&amp;D performance of 28 Chinese industries into four quadrants for analysis and focus on the R&amp;D performance of key industries such as extraction of petroleum and natural gas, mining of ferrous metal ores and manufacture of tobacco. Based on the findings, we provide a range of policy recommendations regarding the R&amp;D activities of Chinese industries.},
  archive      = {J_EJOR},
  author       = {Hui-hui Liu and Guo-liang Yang and Jian-wei Gao and Ya-ping Wang and Guo-hua Ni},
  doi          = {10.1016/j.ejor.2025.01.002},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {1040-1059},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Investigating the research and development performance of chinese industry: A two-stage prospect data envelopment analysis approach},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Business cycle and realized losses in the consumer credit
industry. <em>EJOR</em>, <em>323</em>(3), 1024–1039. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the determinants of losses given default (LGD) in consumer credit. Utilizing a unique dataset encompassing over 6 million observations of Italian consumer credit over a long time span, we find that macroeconomic and social (MS) variables significantly enhance the forecasting performance at both individual and portfolio levels, improving R 2 by up to 10 percentage points. Our findings are robust across various model specifications. Non-linear forecast combination schemes employing neural networks consistently rank among the top performers in terms of mean absolute error, RMSE, R 2 , and model confidence sets in every tested scenario. Notably, every model that belongs to the superior set systematically includes MS variables. The relationship between expected LGD and macro predictors, as revealed by accumulated local effects plots and Shapley values, supports the intuition that lower real activity, a rising cost-of-debt to GDP ratio, and heightened economic uncertainty are associated with higher LGD for consumer credit. Our results on the influence of MS variables complement and slightly differ from those of related papers. These discrepancies can be attributed to the comprehensive nature of our database – spanning broader dimensions in space, time, sectors, and types of consumer credit – the variety of models utilized, and the analyses conducted.},
  archive      = {J_EJOR},
  author       = {Walter Distaso and Francesco Roccazzella and Frédéric Vrins},
  doi          = {10.1016/j.ejor.2024.12.026},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {1024-1039},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Business cycle and realized losses in the consumer credit industry},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expected information of noisy attribute forecasts for
probabilistic forecasts. <em>EJOR</em>, <em>323</em>(3), 1013–1023. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends the maximum entropy (ME) model to include uncertainty about noisy moment forecasts. In this framework the noise propagates to the ME model through the constrained optimization’s Lagrange multipliers. The mutual information and expected Fisher information are included for assessing effects of the noisy moment forecasts on the ME model and its parameters. A new mean–variance decomposition of the mutual information is derived for the normal distribution when the mean and variance are both noisy. A simulation estimator is used to estimate the expected information for noisy ME models on finite support. A family of ensemble of individual level noisy ME forecast models is introduced which includes individual level versions of the conditional logit and multiplicative competitive interaction models as specific cases. To illustrate the implementation and merits of the proposed noisy ME framework, the classic loaded dice problem and discrete choice analysis are examined.},
  archive      = {J_EJOR},
  author       = {Omid M. Ardakani and Robert F. Bordley and Ehsan S. Soofi},
  doi          = {10.1016/j.ejor.2024.12.024},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {1013-1023},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Expected information of noisy attribute forecasts for probabilistic forecasts},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does the quantity discount mechanism offer a loophole for
retailer collusion? Impacts and responses. <em>EJOR</em>,
<em>323</em>(3), 999–1012. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantity discount mechanism is an effective and widely used tool by manufacturers to encourage downstream retailers to increase their order volumes. As wholesale prices decrease with larger order quantities, retailers have an incentive to collude and achieve joint procurement. Two joint procurement modes—group buying (GB) and agency procurement (AP)—are considered to characterize the phenomenon of retailer collusion. In GB mode, retailers purchase as a group and enjoy the same per-unit wholesale price. In contrast, in the AP mode, a leading retailer assumes responsibility for aggregating orders and submitting the total order to the manufacturer while having the authority to set the resale price. A dual-channel model is developed to investigate joint procurement among competing retailers, aiming to identify its underlying driving forces and impacts. Our findings indicate that, compared to individual purchasing (IP), GB is always attainable for retailers, whereas AP is only attainable under intense competition when retailers are symmetric. We reveal that retailers engaging in joint procurement do not always aim to achieve lower wholesale prices. In some cases, the objective may be to mitigate price competition. This finding suggests that joint procurement by retailers results in a reduction in total order quantity, which significantly diminishes the manufacturer’s profit. In response to the challenges of retailer collusion, we explore the feasibility and potential value of offering a coordinated quantity discount mechanism, wherein the manufacturer gives up the pursuit of maximizing its own profit in favor of optimizing the profits of the entire supply chain, making concessions to the retailers. We identify the scenarios in which a coordinated quantity discount contract can eliminate the loophole for retailer collusion and highlight both the value and necessity of achieving contract coordination.},
  archive      = {J_EJOR},
  author       = {Shaofu Du and Xiahui Sun and Li Hu and Tsan-Ming Choi},
  doi          = {10.1016/j.ejor.2024.12.007},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {999-1012},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Does the quantity discount mechanism offer a loophole for retailer collusion? impacts and responses},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-activity shift scheduling under uncertainty: The value
of shift flexibility. <em>EJOR</em>, <em>323</em>(3), 988–998. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a multi-activity shift scheduling problem under demand uncertainty, exploring various levels of flexibility in adapting aspects of the shift schedule (e.g., activity assignment, break assignment, selection of shift type and shift end time) to late-arriving demand information. To address the resulting complex two-stage stochastic combinatorial optimisation problems, we propose a novel two-stage stochastic mixed-integer programming formulation leveraging state-expanded networks and a clustering-based sequential sampling approach for efficiently solving large-scale problem instances. In computational experiments on stochastic problems derived from well-known multi-activity shift scheduling instances, we show that this method effectively solves instances with up to 10 activities and 100 demand scenarios, approaching near-optimality within an average time of less than one hour. From a managerial standpoint, our study provides insights into the structure of good first-stage scheduling decisions as well as into the impact of different flexibility levels on expected costs of the solutions, thereby offering valuable support for decisions such as adjusting employees’ salaries in exchange for increased shift flexibility.},
  archive      = {J_EJOR},
  author       = {Felix Hagemann and Till Frederik Porrmann and Michael Römer},
  doi          = {10.1016/j.ejor.2024.12.028},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {988-998},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Multi-activity shift scheduling under uncertainty: The value of shift flexibility},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring technical efficiency under variable returns to
scale using debreu’s loss function. <em>EJOR</em>, <em>323</em>(3),
975–987. (<a href="https://doi.org/10.1016/j.ejor.2024.12.050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a model that makes two contributions to the measurement of technical efficiency under a technology with variable returns to scale. First, the criteria for identifying an optimal benchmark are not limited to technical dominance and Pareto efficiency, but also include maximum average productivity, defined as the ratio between a weighted linear aggregate of outputs and inputs. Second, the paper contributes a conceptual basis for correcting the shadow prices of inputs and outputs to reflect the influence of returns to scale. Debreu&#39;s loss function is used to value inefficiency as the difference between the virtual input and output using the shadow prices of the supporting hyperplane at the optimal reference. The efficiency score is a virtual profitability index with endogenous shadow prices that reflect the valuation of inputs and outputs with a microeconomic rationale, i.e., it is not a distance measure based on aggregation with exogenous weights of the difference between observed and optimal quantities. Two further results follow from these contributions. First, the radial input-output orientation to maximise productivity is endogenous. It is conditioned by the nature of the returns to scale. Second, the efficiency measure based on the loss function exhibits the desirable properties in a radial context, including the indication property, because the efficiency score incorporates non-radial slack.},
  archive      = {J_EJOR},
  author       = {Juan José Díaz-Hernández and David-José Cova-Alonso and Eduardo Martínez-Budría},
  doi          = {10.1016/j.ejor.2024.12.050},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {975-987},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Measuring technical efficiency under variable returns to scale using debreu&#39;s loss function},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The copeland ratio ranking method for abstract decision
problems. <em>EJOR</em>, <em>323</em>(3), 966–974. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of ranking a finite number of alternatives on the basis of a dominance relation. We firstly investigate some disadvantages of the Copeland ranking method, of the degree ratio ranking method and of the modified degree ratio ranking method which were characterized by using clone properties and classical axiomatic properties. Then, we introduce some alternative axiomatic properties and propose a new ranking method which is defined by the Copeland ratio of alternatives (i.e., the Copeland score of an alternative divided by its total degree). We show that this proposed ranking method coincides with the Copeland ranking method, the degree ratio ranking method and the modified degree ratio ranking method for abstract decision problems with complete and asymmetric dominance relations. Subsequently, we prove that this new ranking method is able to overcome the mentioned disadvantages of these ranking methods. After that, we provide a characterization for the Copeland ratio ranking method using the introduced axiomatic properties.},
  archive      = {J_EJOR},
  author       = {Weibin Han and Adrian Van Deemen},
  doi          = {10.1016/j.ejor.2024.12.042},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {966-974},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The copeland ratio ranking method for abstract decision problems},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective evolutionary algorithm with
mutual-information-guided improvement phase for feature selection in
complex manufacturing processes. <em>EJOR</em>, <em>323</em>(3),
952–965. (<a href="https://doi.org/10.1016/j.ejor.2024.12.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex manufacturing processes (CMP) involve numerous features that impact product quality. Therefore, selecting key process features (KPF) is crucial for effective quality prediction and control in CMPs. This paper proposes a KPF (feature) selection method for the high-dimensional CMP data. The KPF selection problem is formulated as a bi-objective combinatorial optimization task of maximizing the geometric mean measure and minimizing the number of selected features. To solve this challenging high-dimensional KPF selection problem, we propose a novel multi-objective evolutionary algorithm (MOEA) called NSGAII-MIIP. NSGAII-MIIP applies an improvement phase (called MIIP) to purify the non-dominated solutions obtained by genetic operators during the iteration process to improve the FS performance. The improvement phase is guided by a mutual-information-based feature importance measure considering both a feature’s relevance degree to class (product quality level) and its redundancy degree to selected features. This allows MIIP to efficiently update non-dominated solutions by selecting relevant features and eliminating redundant features. Moreover, MIIP is seamlessly integrated into the solution ranking process of NSGAII-MIIP so that solutions from the improvement phase can be ranked together with original solutions in the population efficiently. Experiments on eight datasets show that NSGAII-MIIP has better KPF selection performance than eight state-of-the-art multi-objective FS methods. Moreover, NSGAII-MIIP exhibits superior search performance compared to eight typical multi-objective optimization algorithms.},
  archive      = {J_EJOR},
  author       = {An-Da Li and Zhen He and Qing Wang and Yang Zhang and Yanhui Ma},
  doi          = {10.1016/j.ejor.2024.12.036},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {952-965},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A multi-objective evolutionary algorithm with mutual-information-guided improvement phase for feature selection in complex manufacturing processes},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opinion convergence and management: Opinion dynamics in
interactive group decision-making. <em>EJOR</em>, <em>323</em>(3),
938–951. (<a href="https://doi.org/10.1016/j.ejor.2024.12.046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making processes are significantly influenced by internal social network interactions and external information inputs. While previous research has highlighted the role of social networks in opinion evolution, the dynamics of information dissemination and its interaction with these networks are less understood. To bridge this gap, we introduce the Social-Information-Opinion Dynamic Supernetwork (SIO-DS) model, which integrates critical factors such as the impact of external information and opinion propagation, alongside the influence of internal social network structures and individual willingness to adjust opinions. This model takes into account the varied levels of confidence and individualized dynamic influence among decision makers, recognizing both their asymmetry and diversity. It performs opinion dynamics using bounded confidence models and parameters that govern information dissemination. We found that scale-free networks, which feature influential leaders, are more effective at reaching consensus compared to small-world networks, which are hindered by limited inter-group connections. The speed of information dissemination is critical; moderate speeds help in maintaining a stable consensus by balancing social influence, while very fast or slow speeds risk exacerbating polarization based on how social influence is managed. The SIO-DS model has broad implications for enhancing decision-making in corporate management by optimizing network structures, in public policy by managing public opinion, and in crisis management by developing effective communication strategies. Ultimately, this model not only deepens our understanding of opinion dynamics but also provides practical tools for improving decision-making quality and efficiency in various contexts.},
  archive      = {J_EJOR},
  author       = {Yuan Xu and Shifeng Liu and T.C.E. Cheng and Xue Feng and Jun Wang and Xiaopu Shang},
  doi          = {10.1016/j.ejor.2024.12.046},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {938-951},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Opinion convergence and management: Opinion dynamics in interactive group decision-making},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven preference learning methods for sorting problems
with multiple temporal criteria. <em>EJOR</em>, <em>323</em>(3),
918–937. (<a href="https://doi.org/10.1016/j.ejor.2024.12.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present novel preference learning approaches for sorting problems with multiple temporal criteria. They leverage an additive value function as the basic preference model, adapted for accommodating time series data. Given assignment examples concerning reference alternatives, we learn such a model using convex quadratic programming. It is characterized by fixed-time discount factors and operates within a regularization framework. This approach enables the consideration of temporal interdependencies between timestamps while mitigating the risk of overfitting. To enhance scalability and accommodate learnable time discount factors, we introduce a novel monotonic Recurrent Neural Network (mRNN). It captures the evolving dynamics of preferences over time, while upholding critical properties inherent to multiple criteria sorting problems. These include criteria monotonicity, preference independence, and the natural ordering of classes. The proposed mRNN can describe the preference dynamics by depicting piecewise linear marginal value functions and personalized time discount factors along with time. Thus, it effectively combines the interpretability of traditional sorting methods with the predictive potential offered by deep preference learning models. We comprehensively assess the proposed models on synthetic data scenarios and a real-case study centered on classifying valuable users within a mobile gaming app based on their historical in-game behavioral sequences. Empirical findings underscore the notable performance improvements achieved by the proposed models when compared to a spectrum of baseline methods, spanning machine learning, deep learning, and conventional multiple criteria sorting approaches.},
  archive      = {J_EJOR},
  author       = {Yijun Li and Mengzhuo Guo and Miłosz Kadziński and Qingpeng Zhang and Chenxi Xu},
  doi          = {10.1016/j.ejor.2024.12.020},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {918-937},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Data-driven preference learning methods for sorting problems with multiple temporal criteria},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conical free disposal hull estimators of directional
distances and luenberger productivity indices for general technologies.
<em>EJOR</em>, <em>323</em>(3), 907–917. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directional distances are a popular tool in productivity and efficiency analysis due to their versatility in evaluating the distance of Decision Making Units (DMU) to the efficient frontier of the production set. The theoretical and statistical properties of these measures are well-established in various contexts. However, the measurement of directional distances to the cone spanned by the attainable set has not yet been explored. This cone is necessary to define the Luenberger indices for general technologies. This paper aims to fill this gap by presenting a method for defining and estimating directional distances to this cone, applicable to general technologies without imposing convexity. We also discuss the statistical properties of these measures, enabling us to measure distances to non-convex attainable sets under Constant Returns to Scale (CRS), as well as measure and estimate Luenberger productivity indices and their decompositions for general technologies. In addition, we provide a detailed description of how to make inferences on these indices. Finally, we offer simulated data and a practical example of inference on Luenberger productivity indices and their decompositions using a well-known real data set.},
  archive      = {J_EJOR},
  author       = {Cinzia Daraio and Simone Di Leo and Léopold Simar},
  doi          = {10.1016/j.ejor.2024.12.025},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {907-917},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Conical free disposal hull estimators of directional distances and luenberger productivity indices for general technologies},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The collaborative berth allocation problem with
row-generation algorithms for stable cost allocations. <em>EJOR</em>,
<em>323</em>(3), 888–906. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent supply chain disruptions and crisis response policies (e.g., the COVID-19 pandemic and the Red Sea crisis) have highlighted the role of container terminals as crucial and scarce resources in the global economy. To tackle these challenges, the industry increasingly aims for advanced operational collaboration among multiple stakeholders, as demonstrated by the ambitions of the recently founded Gemini alliance. Nonetheless, collaborative planning models often disregard the requirements and incentives of stakeholders or simply solve idealized small instances. Motivated by the above, we design novel and effective collaboration mechanisms among terminal operators that share the resources (berths and quay cranes). We first define the collaborative berth allocation problem and propose a mixed integer linear programming (MILP) model to minimize the total cost of all terminals, referred to as the coalitional costs. We adopt the core and the nucleolus concepts from cooperative game theory to allocate the coalitional costs such that stakeholders have stable incentives to collaborate. To obtain solutions for realistic instance sizes, we propose two exact row-generation-based core and nucleolus algorithms that are versatile and can be used for various combinatorial optimization problems. To the best of our knowledge, the proposed row-generation approach for the nucleolus is the first of its kind for combinatorial optimization problems. Extensive experiments demonstrate that the collaborative berth allocation approach achieves up to 28.44% of cost savings, increasing the solution space in disruptive situations, while the proposed core and nucleolus solutions guarantee the collaboration incentives for individual terminals.},
  archive      = {J_EJOR},
  author       = {Xiaohuan Lyu and Eduardo Lalla-Ruiz and Frederik Schulte},
  doi          = {10.1016/j.ejor.2024.12.048},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {888-906},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The collaborative berth allocation problem with row-generation algorithms for stable cost allocations},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic decentralization of self-branded and contract
manufacturing businesses. <em>EJOR</em>, <em>323</em>(3), 868–887. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the incentive of a competitive contract manufacturer (CCM) to adopt a decentralized structure by segregating contract manufacturing from its self-branded business. We consider an original equipment manufacturer (OEM) with the option to outsource production either to a CCM producing its self-branded product, or to a non-competitive contract manufacturer (NCM) also serving another OEM. The CCM has the option to centralize or decentralize its two businesses and competes in quantity with both OEMs in the end-user market. Our analysis of the strategic interactions between the OEM&#39;s outsourcing decision and the CCM&#39;s organizational structure choice shows that the likelihood of the OEM outsourcing to the CCM increases when the CCM adopts a decentralized structure compared to a centralized one. Under decentralization, a sufficiently low wholesale price offered by the contract manufacturing division provides the OEM with a competitive advantage. Consequently, the CCM is motivated to strategically deploy a decentralized structure to attract contract manufacturing business from the OEM, even though decentralization yields a lower profit than centralization. However, the CCM must be cautious when implementing a decentralized structure to secure orders from the OEM. The resulting intensified market competition undermines its profit from self-branded business and potentially makes it worse off from producing for the OEM. In such case, the CCM should maintain a centralized structure and uphold a purely competitive relationship with the OEM. Moreover, we demonstrate how the profitability of another OEM supplied by the NCM is influenced by the interplay between the CCM and the OEM.},
  archive      = {J_EJOR},
  author       = {Wei Li and Yanglei Li and Jing Chen and Bintong Chen},
  doi          = {10.1016/j.ejor.2025.01.017},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {868-887},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Strategic decentralization of self-branded and contract manufacturing businesses},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Managing social responsibility efforts with the
consideration of violation probability. <em>EJOR</em>, <em>323</em>(3),
852–867. (<a href="https://doi.org/10.1016/j.ejor.2025.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate social responsibility (CSR) has a strong impact on the external image of the enterprise. The violation of CSR not only harms the enterprise but also negatively affects other firms in the supply chain. This paper establishes a game-theoretical model to study the management of social responsibility efforts with considerations of violation probability. The upstream manufacturer and downstream retailer can reduce the violation probability by exerting CSR efforts. Specifically, we study the following four models, including both participants exerting efforts, only the manufacturer exerting effort, only the retailer exerting effort, and neither participant exerting effort. Our analysis shows that as the effort cost of the manufacturer increases, the retailer may increase or decrease his effort level under both participants exerting efforts, due to the complementary and substitution effects between the efforts of the manufacturer and retailer. We also find that compared with both participants exerting efforts, the retailer may increase or decrease his effort level under only the retailer exerting effort, and the effort level of the manufacturer may grow or shrink under only the manufacturer exerting effort. In addition, we study the decision matrix for the manufacturer and retailer, and find that in equilibrium the manufacturer always has incentives to exert CSR effort, while the retailer may prefer a free ride and sometimes chooses not to exert effort. Interestingly, we find that the total supply chain profit may not be the highest under both participants exerting efforts, but it is the lowest under neither participant exerting effort.},
  archive      = {J_EJOR},
  author       = {Jiayan Xu and Housheng Duan and Sijing Deng},
  doi          = {10.1016/j.ejor.2025.01.016},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {852-867},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Managing social responsibility efforts with the consideration of violation probability},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact algorithms for routing electric autonomous mobile
robots in intralogistics. <em>EJOR</em>, <em>323</em>(3), 830–851. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In intralogistics and manufacturing, autonomous mobile robots (AMRs) are usually electrically powered and recharged by battery swapping or induction. We investigate AMR route planning in these settings by studying different variants of the electric vehicle routing problem with due dates (EVRPD). We consider three common recharging strategies: battery swapping, inductive recharging with full recharges, and inductive recharging with partial recharges. Moreover, we consider two different objective functions: the standard objective of minimizing the total distance traveled and the minimization of the total completion times of transport jobs. The latter is of particular interest in intralogistics, where time aspects are of crucial importance and the earliest possible completion of jobs often has priority. In this context, recharging decisions also play an essential role. For solving the EVRPD variants, we propose exact branch-price-and-cut algorithms that rely on ad-hoc labeling algorithms tailored to the respective variants. We perform an extensive computational study to generate managerial insights on the AMR route planning problem and to assess the performance of our solution approach. The experiments are based on newly introduced instances featuring typical characteristics of AMR applications in intralogistics and manufacturing and on standard benchmark instances from the literature. The detailed analysis of our results reveals that inductive recharging with partial recharges is competitive with battery swapping, while using a full-recharges strategy has considerable drawbacks in an AMR setup.},
  archive      = {J_EJOR},
  author       = {Anne Meyer and Timo Gschwind and Boris Amberg and Dominik Colling},
  doi          = {10.1016/j.ejor.2024.12.041},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {830-851},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exact algorithms for routing electric autonomous mobile robots in intralogistics},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexibility-based price discrimination in a competitive
context considering consumers’ socioeconomic status. <em>EJOR</em>,
<em>323</em>(3), 810–829. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the impact of flexibility-based price discrimination (FBPD) on the pricing and quality strategy of the adopting firm and its competitor, as well as the impact on the welfare of consumers. We assume that the inflexible consumers being targeted for price discrimination can be either high-income consumers or low-income consumers, and the high-income consumers are more sensitive to product quality. We show that depending on who the targeted inflexible consumers are, the impact of FBPD on all firms and consumers can be either negative or positive. If an FBPD is to exploit the inflexibility of low-income consumers, it will not only make the vulnerable group even more disadvantaged but also lower the firms’ incentive to produce high-quality products. On the contrary, if an FBPD is to exploit the inflexibility of high-income consumers, it will increase the firms’ incentive to produce high-quality products, and the targeted consumers will be compensated by having higher quality products. However, the firms might engage in excessive quality enhancement, leading to a situation where the competition between the firms falls into a prisoner’s dilemma. Our research results suggest that the application of FBPD could necessitate a comprehensive regulatory framework to ensure ethical implementation while safeguarding consumer welfare, particularly that of vulnerable groups.},
  archive      = {J_EJOR},
  author       = {Jian Zhang and Emily B. Laidlaw and Raymond A. Patterson},
  doi          = {10.1016/j.ejor.2025.01.005},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {810-829},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Flexibility-based price discrimination in a competitive context considering consumers’ socioeconomic status},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinate or collaborate? Reducing food waste in
perishable-product supply chains. <em>EJOR</em>, <em>323</em>(3),
795–809. (<a href="https://doi.org/10.1016/j.ejor.2024.12.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing food waste in supply chains (SCs) with multiple decision-makers is challenging. A common approach grocery retailers use to reduce waste is requiring manufacturers to only send products with a long remaining shelf life (“minimum life on receipt”-MLOR). However, its impact on manufacturers remains unclear. To evaluate the effectiveness of MLOR agreements on food waste, we investigate two strategies: (1) collaborating on setting the MLOR level and (2) coordinating the SC via contract. Through collaboration, we analytically show that if the MLOR agreement does not demand solely fresh products, it raises manufacturer profits, enabling potential wholesale price reduction. This might incentivize retailers to collaborate to reduce the MLOR level. We demonstrate that the coordinating strategy can reduce waste in the SC and is most beneficial when the wholesale price is high, and the issuing policy is FIFO. We introduce possible coordination contracts and show that in coordinated SCs, manufacturers always provide the highest MLOR level without requiring any restrictive MLOR agreements. Governments mainly focus on reducing retail waste and promoting retailers to request higher MLOR. However, these efforts can backfire by creating more waste for manufacturers. Reducing the MLOR allows retailers to negotiate lower wholesale prices, increasing profitability while reducing waste. Although SC coordination is known for reducing inefficiency, it may not be the best strategy for reducing waste, especially when the issuing policy is more LIFO than FIFO. Specifically, while coordination might be a better strategy for online retailers, collaboration can be a better strategy for brick-and-mortar retailers.},
  archive      = {J_EJOR},
  author       = {Navid Mohamadi and Sandra Transchel and Jan C. Fransoo},
  doi          = {10.1016/j.ejor.2024.12.039},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {795-809},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Coordinate or collaborate? reducing food waste in perishable-product supply chains},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Judgmental selection of parameters for simple forecasting
models. <em>EJOR</em>, <em>323</em>(3), 780–794. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era dominated by big data and machine and deep learning solutions, judgment has still an important role to play in decision making. Behavioural operations are on the rise as judgment complements automated algorithms in many practical settings. Over the years, new and exciting uses of judgment have emerged, with some providing fresh and innovative insights on algorithmic approaches. The forecasting field, in particular, has seen judgment infiltrating in several stages of the forecasting process, such as the production of purely judgmental forecasts, judgmental revisions of formal (statistical) forecasts, and as an alternative to statistical selection between forecasting models. In this paper, we take the first steps towards exploring a neglected use of judgment in forecasting: the manual selection of the parameters for forecasting models. We focus on a simple but widely-used forecasting model, the Simple Exponential Smoothing, and, through a behavioural experiment, we investigate the performance of individuals versus algorithms in selecting optimal modelling parameters under different conditions. Our results suggest that the use of judgment on the task of parameter selection could improve forecasting accuracy. However, individuals also suffer from anchoring biases.},
  archive      = {J_EJOR},
  author       = {Fotios Petropoulos and Evangelos Spiliotis},
  doi          = {10.1016/j.ejor.2024.12.034},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {780-794},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Judgmental selection of parameters for simple forecasting models},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trade-off between utility and fairness in two-agent
single-machine scheduling. <em>EJOR</em>, <em>323</em>(3), 767–779. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem arising when two agents, each owning a set of jobs, compete to schedule their jobs on a common processing resource. Each schedule implies a certain utility for each agent and an overall system utility. We are interested in solutions that incorporate some criterion of fairness for the agents and, at the same time, are satisfactory from the viewpoint of system utility. More precisely, we investigate the trade-off between fairness and system utility when both agents want to minimize the total completion time of their respective jobs. We analyze the structure of the set of such trade-off solutions, and propose an exact algorithm for their computation, based on the Lagrangian relaxation of a MILP formulation of the problem. A large set of computational experiments has been carried out to show the viability of the approach. Moreover, the results show that in most cases a solution having a high degree of fairness can be obtained by sacrificing a very limited amount of system utility.},
  archive      = {J_EJOR},
  author       = {Alessandro Agnetis and Mario Benini and Gaia Nicosia and Andrea Pacifici},
  doi          = {10.1016/j.ejor.2025.01.025},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {767-779},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Trade-off between utility and fairness in two-agent single-machine scheduling},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new effective heuristic for the prisoner transportation
problem. <em>EJOR</em>, <em>323</em>(3), 753–766. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Prisoner Transportation Problem is an NP-hard combinatorial problem and a complex variant of the Dial-a-Ride Problem. Given a set of requests for pick-up and delivery and a homogeneous fleet, it consists of assigning requests to vehicles to serve all requests, respecting the problem constraints such as route duration, capacity, ride time, time windows, multi-compartment assignment of conflicting prisoners and simultaneous services in order to optimize a given objective function. In this paper, we present a new solution framework to address this problem that leads to an efficient heuristic. A comparison with computational results from previous papers shows that the heuristic is very competitive for some classes of benchmark instances from the literature and clearly superior in the remaining cases. Finally, suggestions for future studies are presented.},
  archive      = {J_EJOR},
  author       = {Luciano Ferreira and Marcos Vinicius Milan Maciel and José Valério de Carvalho and Elsa Silva and Filipe Pereira Alvelos},
  doi          = {10.1016/j.ejor.2025.01.029},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {753-766},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A new effective heuristic for the prisoner transportation problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formulations and branch-and-cut algorithms for the period
travelling salesman problem. <em>EJOR</em>, <em>323</em>(3), 739–752.
(<a href="https://doi.org/10.1016/j.ejor.2025.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address two variants of the Period Travelling Salesman Problem: one where some nodes cannot be visited consecutively over the time horizon, and another one where this restriction is not imposed. A new flow-based formulation that uses specific information about the visit patterns of nodes is studied and empirical tests show that it is able to solve test instances where a flow-based formulation based on the Single Commodity Flow formulation for the Travelling Salesman Problem reached the time limit. Non-compact formulations are studied in this work as well. We propose two new sets of exponentially-sized valid inequalities that have not been studied yet in the literature. A formulation which is based on connectivity cuts per period enhanced with these sets of valid inequalities proved to be the most efficient and it was able to solve several instances.},
  archive      = {J_EJOR},
  author       = {Sofia Henriques and Ana Paias},
  doi          = {10.1016/j.ejor.2025.01.015},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {739-752},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Formulations and branch-and-cut algorithms for the period travelling salesman problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness in repetitive scheduling. <em>EJOR</em>,
<em>323</em>(3), 724–738. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research found that fairness plays a key role in customer satisfaction. Therefore, many manufacturing and services industries have become aware of the need to treat customers fairly. Still, there is a huge lack of models that enable industries to make operational decisions fairly, such as a fair scheduling of the customers’ jobs. Our main aim in this research is to provide a unified framework to enable schedulers to make fair decisions in repetitive scheduling environments. For doing so, we consider a set of repetitive scheduling problems involving a set of n clients. In each out of q consecutive operational periods ( e.g. days), each one of the customers submits a job for processing by an operational system. The scheduler’s aim is to provide a schedule for each of the q periods such that the quality of service (QoS) received by each of the clients will meet a certain predefined threshold. The QoS of a client may take several different forms, e.g. , the number of days that the customer receives its job later than a given due date, the number of times the customer receives his preferred time slot for service, or the sum of waiting times for service. We analyze the single machine variant of the problem for several different definitions of QoS, and classify the complexity of the corresponding problems using the theories of classical and parameterized complexity. We also study the price of fairness, i.e., the loss in the system’s efficiency that results from the need to provide fair solutions.},
  archive      = {J_EJOR},
  author       = {Danny Hermelin and Hendrik Molter and Rolf Niedermeier and Michael Pinedo and Dvir Shabtay},
  doi          = {10.1016/j.ejor.2024.12.052},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {724-738},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fairness in repetitive scheduling},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving the parallel processor scheduling and bin packing
problems with contiguity constraints: Mathematical models and
computational studies. <em>EJOR</em>, <em>323</em>(3), 701–723. (<a
href="https://doi.org/10.1016/j.ejor.2024.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parallel processor scheduling and bin packing problems with contiguity constraints are important in the field of combinatorial optimization because both problems can be used as components of effective exact decomposition approaches for several two-dimensional packing problems. In this study, we provide an extensive review of existing mathematical formulations for the two problems, together with some model enhancements and lower bounding techniques, and we empirically evaluate the computational behavior of each of these elements using a state-of-the-art solver on a large set of literature instances. We also assess whether recent developments such as meet-in-the middle patterns and the reflect formulation can be used to solve the two problems more effectively. Our experiments demonstrate that some features, such as the mathematical model used, have a major impact on whether an approach is able to solve an instance, whereas other features, such as the use of symmetry-breaking constraints, do not bring any empirical advantage despite being useful in theory. Overall, our goal is to help the research community design more effective yet simpler algorithms to solve the parallel processor scheduling and bin packing problems with contiguity constraints and closely related extensions so that, eventually, those can be integrated into a larger number of exact methods for two-dimensional packing problems.},
  archive      = {J_EJOR},
  author       = {Fatih Burak Akçay and Maxence Delorme},
  doi          = {10.1016/j.ejor.2024.09.013},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {3},
  pages        = {701-723},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solving the parallel processor scheduling and bin packing problems with contiguity constraints: Mathematical models and computational studies},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Milk adulteration testing and impact of farmers efficiency
heterogeneity: A strategic analysis. <em>EJOR</em>, <em>323</em>(2),
686–700. (<a href="https://doi.org/10.1016/j.ejor.2024.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by economic motives, dairy farmers adulterate milk to increase its perceived quality, posing a serious risk to consumer health. We analyse a milk supply chain in which smallholder dairy farmers can adulterate milk and explore the feasibility of selling it to end consumers through an aggregator. Using a non-cooperative sequential game between the aggregator and farmers, we examine the impact of two testing strategies offered by the aggregator to curb adulteration - (i) individual (testing milk procured from each farmer individually) and (ii) composite (testing the milk after aggregating the portions procured from all the farmers). Our analyses reveal that the aggregator can control milk adulteration by judiciously using testing and penalty mechanisms. We find that a higher market price (aggregation effect) , fetched by the aggregator because of its bargaining power owing to the consolidation of milk supplies, is essential for its operation. It leads to higher revenue for the aggregator and expands the zone in which it is profitable for the aggregator to operate. However, our results show that the efficiency heterogeneity among farmers, which leads to the less efficient farmers free-riding on the more efficient ones, has a detrimental effect on the aggregator operation. We also explore the impact of external uncertainties on the supply chain and observe that the composite testing strategy becomes more profitable for the aggregator when external uncertainties increase. Our results provide important policy recommendations for aggregators adopting optimal testing strategies.},
  archive      = {J_EJOR},
  author       = {Samir Biswas and Preetam Basu and Balram Avittathur},
  doi          = {10.1016/j.ejor.2024.12.001},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {686-700},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Milk adulteration testing and impact of farmers efficiency heterogeneity: A strategic analysis},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from the aggregated optimum: Managing port wine
inventory in the face of climate risks. <em>EJOR</em>, <em>323</em>(2),
671–685. (<a href="https://doi.org/10.1016/j.ejor.2024.11.046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Port wine stocks ameliorate during storage, facilitating product differentiation according to age. This induces a trade-off between immediate revenues and further maturation. Varying climate conditions in the limited supply region lead to stochastic purchase prices for wine grapes. Decision makers must integrate recurring purchasing, production, and issuance decisions. Because stocks from different age classes can be blended to create final products, the solution space increases exponentially in the number of age classes. We model the problem of managing port wine inventory as a Markov decision process, considering decay as an additional source of uncertainty. For small problems, we derive general management strategies from the long-run behavior of the optimal policy. Our solution approach for otherwise intractable large problems, therefore, first aggregates age classes to create a tractable problem representation. We then use machine learning to train tree-based decision rules that reproduce the optimal aggregated policy and the enclosed management strategies. The derived rules are scaled back to solve the original problem. Learning from the aggregated optimum outperforms benchmark rules by 21.4% in annual profits (while leaving a 2.8%-gap to an upper bound). For an industry case, we obtain a 17.4%-improvement over current practices. Our research provides distinct strategies for how producers can mitigate climate risks. The purchasing policy dynamically adapts to climate-dependent price fluctuations. Uncertainties are met with lower production of younger products, whereas strategic surpluses of older stocks ensure high production of older products. Moreover, a wide spread in the age classes used for blending reduces decay risk exposure.},
  archive      = {J_EJOR},
  author       = {Alexander Pahr and Martin Grunow and Pedro Amorim},
  doi          = {10.1016/j.ejor.2024.11.046},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {671-685},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Learning from the aggregated optimum: Managing port wine inventory in the face of climate risks},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible enhanced indexation models through stochastic
dominance and ordered weighted average optimization. <em>EJOR</em>,
<em>323</em>(2), 657–670. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss portfolio selection strategies for Enhanced Indexation (EI), which are based on stochastic dominance relations. The goal is to select portfolios that stochastically dominate a given benchmark but that, at the same time, must generate some excess return with respect to a benchmark index. To achieve this goal, we propose a new methodology that selects portfolios using the ordered weighted average (OWA) operator, which generalizes previous approaches based on minimax selection rules and still leads to solving linear programming models. We also introduce a new type of approximate stochastic dominance rule and show that it implies the almost Second-order Stochastic Dominance (SSD) criterion proposed by Lizyayev and Ruszczyński (2012). We prove that our EI model based on OWA selects portfolios that dominate a given benchmark through this new form of stochastic dominance criterion. We test the performance of the obtained portfolios in an extensive empirical analysis based on real-world datasets. The computational results show that our proposed approach outperforms several SSD-based strategies widely used in the literature, as well as the global minimum variance portfolio.},
  archive      = {J_EJOR},
  author       = {Francesco Cesarone and Justo Puerto},
  doi          = {10.1016/j.ejor.2024.11.050},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {657-670},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Flexible enhanced indexation models through stochastic dominance and ordered weighted average optimization},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal fulfillment and replenishment for omnichannel
retailers with standard shipping contracts. <em>EJOR</em>,
<em>323</em>(2), 642–656. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-commerce sales rise exponentially and represent an increasing proportion of global retail. To benefit from this, traditional brick-and-mortar stores enter the e-commerce market and become omnichannel retailers. However, the profitability of omnichannel retailers remains questionable due to high shipment and fulfillment costs. This paper addresses this challenge, focusing on using standard shipping contracts as a potential solution. Such contracts promise delivery within a given number of periods. Once a customer orders, the retailer should set a delivery period. In this way, retailers are flexible in setting exact delivery days, providing an opportunity for jointly optimizing product replenishment and customer fulfillment. We provide a generic model for the use of standard shipping contracts and formulate it as a Markov decision process. We provide optimal solutions using a modified policy iteration algorithm. Our results show that using standard shipping contracts creates a win-win situation: It increases profits and customer service. The observed profit increase is directly linked to maintaining less on-hand inventory. This effect is more pronounced for higher valued products and longer replenishment lead times. Additionally, we propose a heuristic policy that performs within 4% of the optimal policy.},
  archive      = {J_EJOR},
  author       = {Bartu Arslan and Albert H. Schrotenboer and Zümbül Atan},
  doi          = {10.1016/j.ejor.2024.11.051},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {642-656},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal fulfillment and replenishment for omnichannel retailers with standard shipping contracts},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank matrix estimation via nonconvex spectral
regularized methods in errors-in-variables matrix regression.
<em>EJOR</em>, <em>323</em>(2), 626–641. (<a
href="https://doi.org/10.1016/j.ejor.2025.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional matrix regression has been studied in various aspects, such as statistical properties, computational efficiency and application to specific instances including multivariate regression, system identification and matrix compressed sensing. Current studies mainly consider the idealized case that the covariate matrix is obtained without noise, while the more realistic scenario that the covariates may always be corrupted with noise or missing data has received little attention. We consider the general errors-in-variables matrix regression model and proposed a unified framework for low-rank estimation based on nonconvex spectral regularization. Then from the statistical aspect, recovery bounds for any stationary points are provided to achieve statistical consistency. From the computational aspect, the proximal gradient method is applied to solve the nonconvex optimization problem and is proved to converge to a small neighborhood of the global solution in polynomial time. Consequences for concrete models such as matrix compressed sensing models with additive noise and missing data are obtained via verifying corresponding regularity conditions. Finally, the performance of the proposed nonconvex estimation method is illustrated by numerical experiments on both synthetic and real neuroimaging data.},
  archive      = {J_EJOR},
  author       = {Xin Li and Dongya Wu},
  doi          = {10.1016/j.ejor.2025.02.005},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {626-641},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Low-rank matrix estimation via nonconvex spectral regularized methods in errors-in-variables matrix regression},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From collaborative filtering to deep learning: Advancing
recommender systems with longitudinal data in the financial services
industry. <em>EJOR</em>, <em>323</em>(2), 609–625. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RS) are highly relevant for multiple domains, allowing to construct personalized suggestions for consumers. Previous studies have strongly focused on collaborative filtering approaches, but the inclusion of longitudinal data (LD) has received limited attention. To address this gap, we investigate the impact of incorporating LD for recommendations, comparing traditional collaborative filtering approaches, multi-label classifier (MLC) algorithms, and a deep learning model (DL) in the form of gated recurrent units (GRU). Additional analysis for the best performing model is provided through SHapley Additive exPlanations (SHAP), to uncover relations between the different recommended products and features. Thus, this article contributes to operational research literature by (1) comparing several MLC techniques and RS, including state-of-the-art DL models in a real-life scenario, (2) the comparison of various featurization techniques to assess the impact of incorporating LD on MLC performance, (3) the evaluation of LD as sequential input through the use of DL models, (4) offering interpretable model insights to improve the understanding of RS with LD. The results uncover that DL models are capable of extracting information from longitudinal features for overall higher and statistically significant performance. Further, SHAP values reveal that LD has the higher impact on model output and managerial relevant temporal patterns emerge across product categories.},
  archive      = {J_EJOR},
  author       = {Stephanie Beyer Díaz and Kristof Coussement and Arno De Caigny},
  doi          = {10.1016/j.ejor.2025.01.022},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {609-625},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {From collaborative filtering to deep learning: Advancing recommender systems with longitudinal data in the financial services industry},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On enhancing the explainability and fairness of tree
ensembles. <em>EJOR</em>, <em>323</em>(2), 599–608. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree ensembles are one of the most powerful methodologies in Machine Learning. In this paper, we investigate how to make tree ensembles more flexible to incorporate explainability and fairness in the training process, possibly at the expense of a decrease in accuracy. While explainability helps the user understand the key features that play a role in the classification task, with fairness we ensure that the ensemble does not discriminate against a group of observations that share a sensitive attribute. We propose a Mixed Integer Linear Optimization formulation to train an ensemble of trees that, apart from minimizing the misclassification cost, controls for sparsity as well as the accuracy in the sensitive group. Our formulation is scalable in the number of observations since its number of binary decision variables is independent of the number of observations. In our numerical results, we show that for standard datasets used in the fairness literature, we can dramatically enhance the fairness of the benchmark, namely the popular Random Forest, while using only a few features, all without damaging the misclassification cost.},
  archive      = {J_EJOR},
  author       = {Emilio Carrizosa and Kseniia Kurishchenko and Dolores Romero Morales},
  doi          = {10.1016/j.ejor.2025.01.008},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {599-608},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On enhancing the explainability and fairness of tree ensembles},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The manufacturer’s resale strategy for trade-ins.
<em>EJOR</em>, <em>323</em>(2), 583–598. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with the ever-increasing number of used cars, many automobile manufacturers now offer trade-in programs whereby they resell used cars to generate revenue. Consumers have the alternative of selling their used cars via an online peer-to-peer (P2P) resale platform, which charges a commission on each transaction. This paper studies a manufacturer’s traded-in resale strategy and assess how the manufacturer’s resale strategy and profits are affected by the presence of online P2P platforms. We find that in the absence of P2P platforms, the manufacturer may opt against implementing a resale program, whereas it will always do so in the presence of P2P platforms. This suggests a notable shift in manufacturers’ optimal choice of trade-in resale strategies due to the emergence of P2P platforms. Furthermore, the study reveals that the introduction of P2P platforms may diminishes the profits of manufacturers who have implemented a resale program. Importantly, the study underscores that manufacturers are not necessarily obliged to adopt a planned obsolescence strategy. When P2P platforms are absent, implementing a resale program allows manufacturers to increase profits by producing products that are either less or more durable. However, in the face of competition from P2P platforms, profitability can only be enhanced by making products more durable. This suggests that a platform’s emergence can alter how the depreciation rate affects a manufacturer’s profit and hence its optimal product design strategies. Understanding these dynamics is crucial for effectively navigating the growing used car market.},
  archive      = {J_EJOR},
  author       = {Shu Hu and Stuart X. Zhu and Ke Fu},
  doi          = {10.1016/j.ejor.2024.12.017},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {583-598},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The manufacturer’s resale strategy for trade-ins},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connections between multiple-objective programming and
weight restricted data envelopment analysis: The role of the ordering
cone. <em>EJOR</em>, <em>323</em>(2), 571–582. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores some new, important and interesting connections between Multiple-Objective Programming (MOP) and Data Envelopment Analysis (DEA). We show that imposing weight restrictions in DEA corresponds to changing the ordering cone in MOP in a specific way. The new ordering cone is constructed and its properties are proved, providing useful insights about the connections between MOP and DEA. After providing several theoretical results, we illustrate them on a real-world data set. In addition to their theoretical appeal, our results hold significant practical importance for several reasons which are addressed in the paper.},
  archive      = {J_EJOR},
  author       = {Pekka Korhonen and Majid Soleimani-damaneh and Jyrki Wallenius},
  doi          = {10.1016/j.ejor.2024.12.002},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {571-582},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Connections between multiple-objective programming and weight restricted data envelopment analysis: The role of the ordering cone},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An incremental preference elicitation-based approach to
learning potentially non-monotonic preferences in multi-criteria
sorting. <em>EJOR</em>, <em>323</em>(2), 553–570. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging assignment example preference information, to determine the shape of marginal utility functions and category thresholds of the threshold-based multi-criteria sorting (MCS) model, has emerged as a focal point of current research within the realm of MCS. Most studies assume decision makers can provide all assignment example preference information in batch and that their preferences over criteria are monotonic, which may not align with practical MCS problems. This paper introduces a novel incremental preference elicitation-based approach to learning potentially non-monotonic preferences in MCS problems, enabling decision makers to progressively provide assignment example preference information. Specifically, we first construct a max-margin optimization-based model to model potentially non-monotonic preferences and inconsistent assignment example preference information in each iteration of the incremental preference elicitation process. Using the optimal objective function value of the max-margin optimization-based model, we devise information amount measurement methods and question selection strategies to pinpoint the most informative alternative in each iteration within the framework of uncertainty sampling in active learning. Once the termination criterion is satisfied, the sorting result for non-reference alternatives can be determined through the use of two optimization models, i.e., the max-margin optimization-based model and the complexity controlling optimization model. Subsequently, two incremental preference elicitation-based algorithms are developed to learn potentially non-monotonic preferences, considering different termination criteria. Ultimately, we apply the proposed approach to a firm financial state rating problem to elucidate the detailed implementation steps, and perform computational experiments on both artificial and real-world data sets to compare the proposed question selection strategies with several benchmark strategies.},
  archive      = {J_EJOR},
  author       = {Zhuolin Li and Zhen Zhang and Witold Pedrycz},
  doi          = {10.1016/j.ejor.2024.11.047},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {553-570},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coevolutionary algorithm for exploiting a large fuzzy
outranking relation. <em>EJOR</em>, <em>323</em>(2), 540–552. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outranking approach in Multiple Criteria Decision Analysis (MCDA) uses ranking procedures to exploit a fuzzy outranking relation, which captures the decision maker&#39;s notion of a ranking. However, as decision problems become more complex and computer performance improves, new ranking procedures are needed to rank complex data sets that decision-makers may not interpret. This paper discusses recent efforts and potential directions for developing ranking procedures that use multiobjective evolutionary algorithms (MOEAs) to exploit a fuzzy outranking relation. After that, based on the cooperative coevolutionary algorithms (CCEA) approach, we suggest some fundamental modifications to extend the RP 2 -NSGA-II+H algorithm that improve the scalability of this MOEA to exploit large-sized fuzzy outranking relations. Empirical results indicate that adjustments improve the RP 2 -NSGA-II+H algorithm for the addressed problem. The proposed ranking procedure outperforms RP 2 -NSGA-II+H in terms of ranking error rates based on the experiments conducted. Our experimental results also demonstrate that the proposed approach can be scaled for instances of the ranking problem of up to one thousand alternatives.},
  archive      = {J_EJOR},
  author       = {Jesús Jaime Solano Noriega and Juan Carlos Leyva López and Carlos Andrés Oñate Ochoa and José Rui Figueira},
  doi          = {10.1016/j.ejor.2024.12.012},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {540-552},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A coevolutionary algorithm for exploiting a large fuzzy outranking relation},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain adoption and coordination strategies for green
supply chains considering consumer privacy concern. <em>EJOR</em>,
<em>323</em>(2), 525–539. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumers’ uncertainty about the value of green products will reduce their willingness to pay, thereby obstructing green product promotion. Blockchain can eliminate this uncertainty but bring privacy concerns. We develop a game theoretical model to study a green supply chain composed of one manufacturer and one retailer, aiming to explore the implications of partial or full blockchain adoption on green product manufacturing. Subsequently, we consider the use of revenue-sharing and cost-sharing contracts as mechanisms to coordinate the supply chain that adopts blockchain technologies. We show that adopting blockchain for some products benefits the manufacturer and the retailer, and consumers’ privacy concerns make it impossible for blockchain to be adopted for all products. Interestingly, partial or full blockchain adoption does not affect the green investment level. Furthermore, we find that revenue-sharing and cost-sharing contracts are always beneficial for the manufacturer. However, it can be beneficial for the retailer only when the revenue-sharing or cost-sharing ratio is small. Surprisingly, the effectiveness of the coordinating contract is not affected by consumers’ privacy concerns. Finally, when comparing the wholesale price contract with two coordination mechanisms, we find that the manufacturer and the retailer can agree on adopting a cost-sharing contract when both revenue- and cost-sharing ratios are low. When the revenue-sharing ratio is moderate and the cost-sharing ratio is low, a revenue-sharing contract is adopted. In all other cases, trading is conducted according to the wholesale price contract. These insights can contribute to optimize the application of blockchain in green supply chains.},
  archive      = {J_EJOR},
  author       = {Changhua Liao and Qihui Lu and Salar Ghamat and Helen Huifen Cai},
  doi          = {10.1016/j.ejor.2024.12.022},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {525-539},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Blockchain adoption and coordination strategies for green supply chains considering consumer privacy concern},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An operation-agnostic stochastic user equilibrium model for
mobility-on-demand networks with congestible capacities. <em>EJOR</em>,
<em>323</em>(2), 504–524. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the impact of privately-owned Mobility-on-Demand (MoD) services is important from a regulatory perspective. There is a need to model multimodal equilibria with MoD to support policymaking. While there exists a large body of literature on MoD services focusing on service design under equilibrium modeling, these studies commonly adopt assumptions of MoD operational policies. However, such policies might not be shared with regulatory agencies due to commercial privacy concerns of private operators. We model multimodal equilibrium with MoD systems in an operation-agnostic manner based on empirical observations of flow and capacity. This is done with a Flow-Capacity Interaction (FC) matrix that captures systematic effect of congestible capacities, a phenomenon in MoD systems where capacities are affected by flows. The FC matrix encapsulates the operation and demand patterns by capturing the empirical equilibrium relationship between flows and capacities. An operation-agnostic logit-based stochastic user equilibrium (SUE) formulation is proposed and proof of equivalence of the SUE formulation is derived. The proof shows that, unlike static capacities, path delays are not just the sum of the Lagrange multipliers of the links on the paths, but dependent on the whole network. We name this phenomenon as “non-separable link delays”. A solution algorithm that finds SUE with a bounded path set is proposed, with a custom Frank-Wolfe algorithm to solve the non-linear SUE formulation. Since the FC matrix cannot be directly observed, an inverse optimization problem is introduced to estimate it with observed flow and capacity data. Two numerical examples are provided with sensitivity tests. An empirical example with yellow taxi data of downtown Manhattan, NY is provided to demonstrate effectiveness of estimating the FC matrix from real data, and for determining the equilibrium that captures the underlying flow-capacity dynamics.},
  archive      = {J_EJOR},
  author       = {Bingqing Liu and David Watling and Joseph Y.J. Chow},
  doi          = {10.1016/j.ejor.2024.12.038},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {504-524},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An operation-agnostic stochastic user equilibrium model for mobility-on-demand networks with congestible capacities},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Product line extensions and distribution channels in
pharmaceutical supply chain. <em>EJOR</em>, <em>323</em>(2), 490–503.
(<a href="https://doi.org/10.1016/j.ejor.2024.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to aggressive generic competition after the original drug’s patent expires, various original firms extend their product lines by introducing an authorized generic drug with both lower quality and cost, either via internal distribution or third-party distribution. In this paper, we develop a game-theoretic model to investigate the product line extension and distribution channel decisions for an original firm that has already sold an original drug and considers introducing an authorized generic drug to compete against the generic firm. We show that product line extension enables the original firm to leverage the value of drug differentiation by price discriminating the patients with heterogeneous preferences for quality, but it also leads to original drug’s profit loss caused by the internal cannibalization. Given an internal distribution channel, when the cost gap is not small for the internal cannibalization to be less aggressive, the original firm will extend the product line, which could surprisingly benefit the generic firm but harm the patients. In contrast, under a third-party distribution channel, the original firm always prefers to extend the product line by setting a low wholesale price, which always reduces the generic firm’s profit but increases the patient surplus. Finally, contrary to the conventional wisdom that a decentralized channel always harms the original firm compared with a centralized one due to the double marginalization, our results suggest that when the original drug has a small cost gap or a large quality gap relative to the generic drug, the original firm is better off with using the third-party distribution to introduce the authorized generic drug than the internal distribution, as it permits higher original drug’s profit due to alleviated internal cannibalization, although at the expense of lower authorized generic drug’s profit.},
  archive      = {J_EJOR},
  author       = {Ran Tao and Yanfei Lan and Ruiqing Zhao and Rong Gao},
  doi          = {10.1016/j.ejor.2024.12.013},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {490-503},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Product line extensions and distribution channels in pharmaceutical supply chain},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated differentiated time slot pricing and order
dispatching with uncertain customer demand in on-demand food delivery.
<em>EJOR</em>, <em>323</em>(2), 471–489. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiated time slot pricing (DTSP) is a promising approach to enhance the efficiency and cost-effectiveness of food delivery platforms by influencing customers’ choices regarding delivery time slots. In this paper, we investigate the integrated problem of DTSP at the tactical level and order dispatching at the operational level, formulating it as a two-stage stochastic programming model. The first-stage model determines the delivery price for each time slot to maximize the system’s expected profit. The second-stage model generates the optimal order dispatching plan to minimize the generalized system cost under each stochastic scenario. To efficiently estimate the order dispatching cost for each scenario, we develop an order consolidation dispatching algorithm (OCDA) to solve the second-stage order dispatching subproblem under each demand scenario. Building on OCDA, we propose a hybrid adaptive large neighborhood search (HALNS) heuristic to solve the integrated problem. Extensive case studies based on real-world data verify the effectiveness of the proposed approach and demonstrate the benefits of DTSP strategy. Our numerical analysis provides important managerial insights for operating food delivery platforms.},
  archive      = {J_EJOR},
  author       = {Bo Zhang and Elkafi Hassini and Yun Zhou and Meng Zhao and Xiangpei Hu},
  doi          = {10.1016/j.ejor.2024.12.011},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {471-489},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integrated differentiated time slot pricing and order dispatching with uncertain customer demand in on-demand food delivery},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal forecast reconciliation with time series selection.
<em>EJOR</em>, <em>323</em>(2), 455–470. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecast reconciliation ensures forecasts of time series in a hierarchy adhere to aggregation constraints, enabling aligned decision making. While forecast reconciliation can enhance overall accuracy in a hierarchical or grouped structure, it can lead to worse forecasts for certain series, with the greatest gains typically seen in series that originally have poorly performing base forecasts. In practical applications, some series in a structure often produce poor base forecasts due to model misspecification or low forecastability. To mitigate their negative impact, we propose two categories of forecast reconciliation methods that incorporate automatic time series selection based on out-of-sample and in-sample information, respectively. These methods keep “poor” base forecasts unused in forming reconciled forecasts, while adjusting the weights assigned to the remaining series accordingly when generating bottom-level reconciled forecasts. Additionally, our methods ameliorate disparities stemming from varied estimators of the base forecast error covariance matrix, alleviating challenges associated with estimator selection. Empirical evaluations through two simulation studies and applications using Australian labour force and domestic tourism data demonstrate the potential of the proposed methods to exclude series with high scaled forecast errors and show promising results.},
  archive      = {J_EJOR},
  author       = {Xiaoqian Wang and Rob J. Hyndman and Shanika L. Wickramasuriya},
  doi          = {10.1016/j.ejor.2024.12.004},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {455-470},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal forecast reconciliation with time series selection},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the discrete and continuous edge improvement
problems: Models and algorithms. <em>EJOR</em>, <em>323</em>(2),
441–454. (<a href="https://doi.org/10.1016/j.ejor.2024.12.051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the edge improvement problem where the fixed edge traversal time assumption of the traditional network flow problems is relaxed. We consider two variants of the problem: one where improvement decisions are restricted to a discrete set (discrete edge improvement problem), and the other where they can take any value within a specified range (continuous edge improvement problem). We first analyze both problem variants on a tree-shaped network and discuss their computational complexities. For the general case, where the underlying network has no special structure, we provide mixed-integer programming (MIP) formulations for both versions of the problem. To the best of our knowledge, this study is the first to propose and compare different formulations for the discrete edge improvement problem and to present a formulation for the continuous edge improvement problem. Since the developed models do not perform well for medium and large problem instances, we introduce a Benders decomposition algorithm to solve the discrete edge improvement problem. Additionally, we employ it heuristically to find high-quality solution for the continuous edge improvement problem within reasonable times. We also devise an MIP formulation to find lower bounds for the continuous edge improvement problem, leveraging the McCormick envelopes and optimal solution properties. Our experiments demonstrate that the Benders decomposition algorithm outperforms the other formulations for the discrete edge improvement problem, while the heuristic method proposed for the continuous edge improvement problem provides quite well results even for large problem instances.},
  archive      = {J_EJOR},
  author       = {Esra Koca and A. Burak Paç},
  doi          = {10.1016/j.ejor.2024.12.051},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {441-454},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exploring the discrete and continuous edge improvement problems: Models and algorithms},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast and effective breakpoints heuristic algorithm for the
quadratic knapsack problem. <em>EJOR</em>, <em>323</em>(2), 425–440. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Quadratic Knapsack Problem (QKP) involves selecting a subset of elements that maximizes the sum of pairwise and singleton utilities without exceeding a given budget. The pairwise utilities are nonnegative, the singleton utilities may be positive, negative, or zero, and the node costs are nonnegative. We introduce a Breakpoints Algorithm for QKP, named QKBP, which is based on a technique proposed in Hochbaum (2009) for efficiently generating the concave envelope of the solutions to the relaxation of the problem for all values of the budget. Our approach utilizes the fact that breakpoints in the concave envelopes are optimal solutions for their respective budgets. For budgets between breakpoints, a fast greedy heuristic derives high-quality solutions from the optimal solutions of adjacent breakpoints. The QKBP algorithm is a heuristic which is highly scalable due to an efficient parametric cut procedure used to generate the concave envelope. This efficiency is further improved by a newly developed compact problem formulation. Our extensive computational study on both existing and new benchmark instances, with up to 10,000 elements, shows that while some leading algorithms perform well on a few instances, QKBP consistently delivers high-quality solutions regardless of instance size, density, or budget. Moreover, QKBP achieves these results in significantly faster running times than all leading algorithms. The source code of the QKBP algorithm, the benchmark instances, and the detailed results are publicly available on GitHub.},
  archive      = {J_EJOR},
  author       = {D.S. Hochbaum and P. Baumann and O. Goldschmidt and Y. Zhang},
  doi          = {10.1016/j.ejor.2024.12.019},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {425-440},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A fast and effective breakpoints heuristic algorithm for the quadratic knapsack problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving the multiobjective quasi-clique problem.
<em>EJOR</em>, <em>323</em>(2), 409–424. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a simple undirected graph G , a quasi-clique is a subgraph of G whose density is at least γ ( 0 &lt; γ ≤ 1 ) . Finding a maximum quasi-clique has been addressed from two different perspectives: ( i ) maximizing vertex cardinality for a given edge density; and ( i i ) maximizing edge density for a given vertex cardinality. However, when no a priori preference information about cardinality and density is available, a more natural approach is to consider the problem from a multiobjective perspective. We introduce the Multiobjective Quasi-clique (MOQC) problem, which aims to find a quasi-clique by simultaneously maximizing both vertex cardinality and edge density. To efficiently address this problem, we explore the relationship among MOQC, its single-objective counterpart problems, and a bi-objective optimization problem, along with several properties of the MOQC problem and quasi-cliques. We propose a baseline approach using ɛ -constraint scalarization and introduce a Two-phase strategy, which applies a dichotomic search based on weighted sum scalarization in the first phase and an ɛ -constraint methodology in the second phase. Additionally, we present a Three-phase strategy that combines the dichotomic search used in Two-phase with a vertex-degree-based local search employing novel sufficient conditions to assess quasi-clique efficiency, followed by an ɛ -constraint in a final stage. Experimental results on synthetic and real-world sparse graphs indicate that the integrated use of dichotomic search and local search, together with mechanisms to assess quasi-clique efficiency, makes the Three-phase strategy an effective approach for solving the MOQC problem in sparse graphs in terms of running time and ability to produce new efficient quasi-cliques.},
  archive      = {J_EJOR},
  author       = {Daniela Scherer dos Santos and Kathrin Klamroth and Pedro Martins and Luís Paquete},
  doi          = {10.1016/j.ejor.2024.12.018},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {409-424},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solving the multiobjective quasi-clique problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete optimization: A quantum revolution? <em>EJOR</em>,
<em>323</em>(2), 378–408. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop several quantum procedures and investigate their potential to solve discrete optimization problems. First, we introduce a binary search procedure and illustrate how it can be used to effectively solve the binary knapsack problem. Next, we introduce two other procedures: a hybrid branch-and-bound procedure that allows to exploit the structure of the problem and a random-ascent procedure that can be used to solve problems that have no clear structure and/or are difficult to solve using traditional methods. We explain how to assess the performance of these procedures and perform an elaborate computational experiment. Our results show that we can match the worst-case performance of the best classical algorithms when solving the binary knapsack problem. After improving and generalizing our procedures, we show that they can be used to solve any discrete optimization problem. To illustrate, we show how to solve the quadratic binary knapsack problem. For this problem, our procedures outperform the best classical algorithms. In addition, we demonstrate that our procedures can be used as heuristics to find (near-) optimal solutions in limited time Not only does our work provide the tools required to explore a myriad of future research directions, it also shows that quantum computing has the potential to revolutionize the field of discrete optimization.},
  archive      = {J_EJOR},
  author       = {Stefan Creemers and Luis Fernando Pérez Armas},
  doi          = {10.1016/j.ejor.2024.12.016},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {378-408},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Discrete optimization: A quantum revolution?},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of multiple criteria decision analysis: From
classical methods to robust ordinal regression. <em>EJOR</em>,
<em>323</em>(2), 351–377. (<a
href="https://doi.org/10.1016/j.ejor.2024.07.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Criteria Decision Analysis (MCDA) is a subfield of Operational Research that aims to support Decision-Makers (DMs) in the decision-making process through mathematical models and computational procedures. In this perspective, MCDA employs structured and traceable protocols to identify potential actions and the criteria for evaluating them. MCDA procedures aim to define recommendations consistent with the preferences of DMs for the specific decision problem at hand. These problems are generally formulated in terms of either choosing the best action, classifying actions into pre-defined and ordered decision classes, or ranking actions from best to worst. As the evaluation criteria are generally conflicting, the main challenge is to aggregate them into a mathematical preference model representing the DM value system. We review the development of MCDA over the past fifty years and describe its evolution with examples of distinctive methods. They are distinguished by the type of preference information elicited by DMs, the type of the preference model (criteria aggregation), and the way of converting the preference relation induced by the preference model in the set of potential actions into a decision recommendation. We focus on MCDA methods with a finite set of actions. References to specific application areas will be given. In the conclusion section, some prospective avenues of research will be outlined.},
  archive      = {J_EJOR},
  author       = {Salvatore Greco and Roman Słowiński and Jyrki Wallenius},
  doi          = {10.1016/j.ejor.2024.07.038},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {351-377},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of multiple criteria decision analysis: From classical methods to robust ordinal regression},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
