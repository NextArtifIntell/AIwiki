<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv---18">ICV - 18</h2>
<ul>
<li><details>
<summary>
(2025). Image–text feature learning for unsupervised
visible–infrared person re-identification. <em>ICV</em>, <em>158</em>,
105520. (<a href="https://doi.org/10.1016/j.imavis.2025.105520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible–infrared person re-identification (VI-ReID) focuses on matching infrared and visible images of the same person. To reduce labeling costs, unsupervised VI-ReID (UVI-ReID) methods typically use clustering algorithms to generate pseudo-labels and iteratively optimize the model based on these pseudo-labels. Although existing UVI-ReID methods have achieved promising performance, they often overlook the effectiveness of text semantics in inter-modality matching and modality-invariant feature learning. In this paper, we propose an image–text feature learning (ITFL) method, which not only leverages text semantics to enhance intra-modality identity-related learning but also incorporates text semantics into inter-modality matching and modality-invariant feature learning. Specifically, ITFL first performs modality-aware feature learning to generate pseudo-labels within each modality. Then, ITFL employs modality-invariant text modeling (MTM) to learn a text feature for each cluster in the visible modality, and utilizes inter-modality dual-semantics matching (IDM) to match inter-modality positive clusters. To obtain modality-invariant and identity-related image features, we not only introduce a cross-modality contrastive loss in ITFL to mitigate the impact of modality gaps, but also develop a text semantic consistency loss to further promote modality-invariant feature learning. Extensive experimental results on VI-ReID datasets demonstrate that ITFL not only outperforms existing unsupervised methods but also competes with some supervised approaches.},
  archive      = {J_ICV},
  author       = {Jifeng Guo and Zhiqi Pang},
  doi          = {10.1016/j.imavis.2025.105520},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105520},
  shortjournal = {Image Vis. Comput.},
  title        = {Image–text feature learning for unsupervised visible–infrared person re-identification},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MrgaNet: Multi-scale recursive gated aggregation network for
tracheoscopy images. <em>ICV</em>, <em>158</em>, 105503. (<a
href="https://doi.org/10.1016/j.imavis.2025.105503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is a potentially fatal disease worldwide, and improving the accuracy of diagnosis plays a key role in enhancing patient outcomes. In this study, we extended computer-aided work to the task of assisting tracheoscopy in predicting lung cancer subtypes. To solve the problem of information fusion in different spatial scales and channels, we proposed MrgaNet. The network enhances classification performance by expanding interactions from low to high orders, dynamically adjusting feature weights, and incorporating a channel competition operator for efficient feature selection. Our network achieved a precision of 0.87 in the endobronchial dataset. In addition, the accuracy of 89.25% and 96.76% was achieved in the Kvasir-v2 dataset and the Kvasir-Capsule dataset, respectively. The results demonstrate that MrgaNet achieves superior performance compared to existing excellent methods.},
  archive      = {J_ICV},
  author       = {Ying Wang and Yun Tie and Dalong Zhang and Fenghui Liu and Lin Qi},
  doi          = {10.1016/j.imavis.2025.105503},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105503},
  shortjournal = {Image Vis. Comput.},
  title        = {MrgaNet: Multi-scale recursive gated aggregation network for tracheoscopy images},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal information mining and fusion feature-guided
modal alignment for video-based visible-infrared person
re-identification. <em>ICV</em>, <em>157</em>, 105518. (<a
href="https://doi.org/10.1016/j.imavis.2025.105518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The video-based visible-infrared person re-identification (Re-ID) aims to recognize the same person across modalities through video sequences. The core challenges of this task lie in narrowing the modal differences and deeply mining the rich spatio-temporal information contained in video to enhance model performance. However, existing research primarily focuses on addressing the modality gap, with insufficient utilization of the spatio-temporal information in video sequences. To address this, this paper proposes a novel spatio-temporal information mining and fusion feature-guided modal alignment framework for video-based visible-infrared person Re-ID. Specifically, we propose a spatio-temporal information mining method. This method employs the proposed feature correlation mechanism to enhance the discriminative features of person across different frames, while utilizing a temporal Transformer to mine person motion features. The advantage of this method lies in its ability to alleviate issues such as occlusion and frame misalignment, improving the discriminability of person features. Additionally, we introduce a fusion modality-guided modal alignment strategy, which reduces modality differences between infrared and visible video frames by aligning single-modality features with fusion features. The advantage of this strategy is that each modality not only learns its specific features but also absorbs person information from the other modality, thereby alleviating modality differences and further enhancing the discriminability of person features. Extensive comparative and ablation experiments conducted on the HITSZ-VCM and BUPTCampus datasets confirm the effectiveness and superiority of the proposed framework. The source code is available at https://github.com/lhf12278/SIMFGA .},
  archive      = {J_ICV},
  author       = {Zhigang Zuo and Huafeng Li and Yafei Zhang and Minghong Xie},
  doi          = {10.1016/j.imavis.2025.105518},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105518},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatio-temporal information mining and fusion feature-guided modal alignment for video-based visible-infrared person re-identification},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stealth sight: A multi perspective approach for camouflaged
object detection. <em>ICV</em>, <em>157</em>, 105517. (<a
href="https://doi.org/10.1016/j.imavis.2025.105517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is a challenging task due to the inherent similarity between objects and their surroundings. This paper introduces Stealth Sight , a novel framework integrating multi-view feature fusion and depth-based refinement to enhance segmentation accuracy. Our approach incorporates a pretrained multi-view CLIP encoder and a depth extraction network, facilitating robust feature representation. Additionally, we introduce a cross-attention transformer decoder and a post-training pruning mechanism to improve efficiency. Extensive evaluations on benchmark datasets demonstrate that Stealth Sight outperforms state-of-the-art methods in camouflaged object segmentation. Our method significantly enhances detection in complex environments, making it applicable to medical imaging, security, and wildlife monitoring.},
  archive      = {J_ICV},
  author       = {Domnic S. and Jayanthan K.S.},
  doi          = {10.1016/j.imavis.2025.105517},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105517},
  shortjournal = {Image Vis. Comput.},
  title        = {Stealth sight: A multi perspective approach for camouflaged object detection},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFKD: Multi-dimensional feature alignment for knowledge
distillation. <em>ICV</em>, <em>157</em>, 105514. (<a
href="https://doi.org/10.1016/j.imavis.2025.105514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a popular technique for compressing and transferring models in the field of deep learning. However, existing distillation methods often focus on optimizing a single dimension and overlook the importance of aligning and transforming knowledge across multiple dimensions, leading to suboptimal results. In this article, we introduce a novel approach called multi-dimensional feature alignment for knowledge distillation (MFKD) to address this limitation. The MFKD framework is built on the observation that knowledge from different dimensions can complement each other effectively. We extract knowledge from features in the spatcial, sample and channel dimensions separately. Our spatial-level part separates the foreground and background information, guiding the student to focus on crucial image regions by mimicking the teacher’s spatial and channel attention maps. Our sample-level part distills knowledge encoded in semantic correlations between sample activations by aligning the student’s activations to emulate the teacher’s clustering patterns using the Spearman correlation coefficient. Furthermore, our channel-level part encourages the student to learn standardized feature representations aligned with the teacher’s channel-wise interdependencies. Finally, we dynamically balance the loss factors of the different dimensions to optimize the overall performance of the distillation process. To validate the effectiveness of our methodology, we conduct experiments on benchmark datasets such as CIFAR-100, ImageNet and COCO. The experimental results demonstrate substantial performance improvements compared to baseline and recent state-of-the-art methods, confirming the efficacy of our MFKD framework. Furthermore, we provide a comprehensive analysis of the experimental results, offering deeper insight into the benefits and effectiveness of our approach. Through this analysis, we reinforce the significance of aligning and leveraging knowledge across multiple dimensions in knowledge distillation.},
  archive      = {J_ICV},
  author       = {Zhen Guo and Pengzhou Zhang and Peng Liang},
  doi          = {10.1016/j.imavis.2025.105514},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105514},
  shortjournal = {Image Vis. Comput.},
  title        = {MFKD: Multi-dimensional feature alignment for knowledge distillation},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusing grid and adaptive region features for image
captioning. <em>ICV</em>, <em>157</em>, 105513. (<a
href="https://doi.org/10.1016/j.imavis.2025.105513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning aims to automatically generate grammatically correct and reasonable description sentences for given images. Improving feature optimization and processing is crucial for enhancing performance in this task. A common approach is to leverage the complementary advantages of grid features and region features. However, incorporating region features in most current methods may lead to incorrect guidance during training, along with high acquisition costs and the requirement of pre-caching. These factors impact the effectiveness and practical application of image captioning. To address these limitations, this paper proposes a method called fusing grid and adaptive region features for image captioning (FGAR). FGAR dynamically explores pseudo-region information within a given image based on the extracted grid features. Subsequently, it utilizes a combination of computational layers with varying permissions to fuse features, enabling comprehensive interaction between information from different modalities while preserving the unique characteristics of each modality. The resulting enhanced visual features provide improved support to the decoder for autoregressively generating sentences describing the content of a given image. All processes are integrated within a fully end-to-end framework, facilitating both training and inference processes while achieving satisfactory performance. Extensive experiments validate the effectiveness of the proposed FGAR method.},
  archive      = {J_ICV},
  author       = {Jiahui Wei and Zhixin Li and Canlong Zhang and Huifang Ma},
  doi          = {10.1016/j.imavis.2025.105513},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105513},
  shortjournal = {Image Vis. Comput.},
  title        = {Fusing grid and adaptive region features for image captioning},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention head purification: A new perspective to harness
CLIP for domain generalization. <em>ICV</em>, <em>157</em>, 105511. (<a
href="https://doi.org/10.1016/j.imavis.2025.105511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Generalization (DG) aims to learn a model from multiple source domains to achieve satisfactory performance on unseen target domains. Recent works introduce CLIP to DG tasks due to its superior image-text alignment and zeros-shot performance. Previous methods either utilize full fine-tuning or prompt-learning paradigms to harness CLIP for DG tasks. Those works focus on avoiding catastrophic forgetting of the original knowledge encoded in CLIP but ignore that the knowledge encoded in CLIP in nature may contain domain-specific cues that constrain its domain generalization performance. In this paper, we propose a new perspective to harness CLIP for DG, i.e., attention head purification. We observe that different attention heads may encode different properties of an image and selecting heads appropriately may yield remarkable performance improvement across domains. Based on such observations, we purify the attention heads of CLIP from two levels, including task-level purification and domain-level purification . For task-level purification, we design head-aware LoRA to make each head more adapted to the task we considered. For domain-level purification, we perform head selection via a simple gating strategy. We utilize MMD loss to encourage masked head features to be more domain-invariant to emphasize more generalizable properties/heads. During training, we jointly perform task-level purification and domain-level purification. We conduct experiments on various representative DG benchmarks. Though simple, extensive experiments demonstrate that our method performs favorably against previous state-of-the-arts.},
  archive      = {J_ICV},
  author       = {Yingfan Wang and Guoliang Kang},
  doi          = {10.1016/j.imavis.2025.105511},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105511},
  shortjournal = {Image Vis. Comput.},
  title        = {Attention head purification: A new perspective to harness CLIP for domain generalization},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDMCB: Open-world object detection empowered by denoising
diffusion models and calibration balance. <em>ICV</em>, <em>157</em>,
105508. (<a href="https://doi.org/10.1016/j.imavis.2025.105508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-world object detection (OWOD) differs from traditional object detection by being more suited to real-world, dynamic scenarios. It aims to recognize unseen objects and have the skill to learn incrementally based on newly introduced knowledge. However, the current OWOD usually relies on supervising of known objects in identifying unknown objects, using high objectness scores as critical indicators of potential unknown objects. While these methods can detect unknown objects with features similar to known objects, they also classify regions dissimilar to known objects as background, leading to label bias issues. To address this problem, we leverage the knowledge from large visual models to provide auxiliary supervision for unknown objects. Additionally, we apply the Denoising Diffusion Probabilistic Model (DDPM) in OWOD scenarios. We propose an unsupervised modeling approach based on DDPM, which significantly improves the accuracy of unknown object detection. Despite this, the classifier trained during the model training process only encounters known classes, resulting in higher confidence for known classes during inference; thus, bias issues again occur. Therefore, we propose a probability calibration technique for post-processing predictions during inference. The calibration aims to reduce the probabilities of known objects and increase the probabilities of unknown objects, thereby balancing the final probability predictions. Our experiments demonstrate that the proposed method achieves significant improvements on OWOD benchmarks, with an unknown objects detection recall rate of 54.7 U-Recall , surpassing the current state-of-the-art (SOTA) methods by 44.3% . In terms of real-time performance, Our model uses a few parameters, and pure convolutional neural networks instead of intensive attention mechanisms, achieving an inference speed of 35.04 FPS , exceeding the SOTA OWOD methods based on Faster R-CNN and Deformable DETR by 2.79 and 10.95 FPS , respectively.},
  archive      = {J_ICV},
  author       = {Yangyang Huang and Xing Xi and Ronghua Luo},
  doi          = {10.1016/j.imavis.2025.105508},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105508},
  shortjournal = {Image Vis. Comput.},
  title        = {DDMCB: Open-world object detection empowered by denoising diffusion models and calibration balance},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised monocular depth learning from unknown
cameras: Leveraging the power of raw data. <em>ICV</em>, <em>157</em>,
105505. (<a href="https://doi.org/10.1016/j.imavis.2025.105505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised monocular depth estimation from wild videos with unknown camera intrinsics is a practical and challenging task in computer vision. Most of the existing methods in literature employed a camera decoder and a pose decoder to estimate camera intrinsics and poses respectively, however, their performances would be degraded significantly in many complex scenarios with severe noise and large camera rotations. To address this problem, we propose a novel self-supervised monocular depth estimation method, which could be trained from wild videos with a joint optimization strategy for simultaneously estimating camera intrinsics and poses. In the proposed method, a depth encoder is employed to learn scene depth features, and then by taking these features as inputs, a Neighborhood Influence Module (NIM) is designed for predicting each pixel’s depth by fusing the depths of its neighboring pixels, which could explicitly enforce the depth accuracy. In addition, a knowledge distillation mechanism is introduced to learn a lightweight depth encoder from a large-scale depth encoder, for achieving a balance between computational speed and accuracy. Experimental results on four public datasets demonstrate that the proposed method outperforms some state-of-the-art methods in most cases. Moreover, once the proposed method is trained with a mixed set of different datasets, its performance would be further boosted in comparison to the proposed method trained with each involved single dataset. Codes are available at: https://github.com/ZhuYongChaoUSST/IntrLessMonoDepth .},
  archive      = {J_ICV},
  author       = {Xiaofei Qin and Yongchao Zhu and Lin Wang and Xuedian Zhang and Changxiang He and Qiulei Dong},
  doi          = {10.1016/j.imavis.2025.105505},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105505},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-supervised monocular depth learning from unknown cameras: Leveraging the power of raw data},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced deep learning and large language models:
Comprehensive insights for cancer detection. <em>ICV</em>, <em>157</em>,
105495. (<a href="https://doi.org/10.1016/j.imavis.2025.105495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the rapid advancement of machine learning (ML), particularly deep learning (DL), has revolutionized various fields, with healthcare being one of the most notable beneficiaries. DL has demonstrated exceptional capabilities in addressing complex medical challenges, including the early detection and diagnosis of cancer. Its superior performance, surpassing both traditional ML methods and human accuracy, has made it a critical tool in identifying and diagnosing diseases such as cancer. Despite the availability of numerous reviews on DL applications in healthcare, a comprehensive and detailed understanding of DL’s role in cancer detection remains lacking. Most existing studies focus on specific aspects of DL, leaving significant gaps in the broader knowledge base. This paper aims to bridge these gaps by offering a thorough review of advanced DL techniques, namely transfer learning (TL), reinforcement learning (RL), federated learning (FL), Transformers, and large language models (LLMs). These cutting-edge approaches are pushing the boundaries of cancer detection by enhancing model accuracy, addressing data scarcity, and enabling decentralized learning across institutions while maintaining data privacy. TL enables the adaptation of pre-trained models to new cancer datasets, significantly improving performance with limited labeled data. RL is emerging as a promising method for optimizing diagnostic pathways and treatment strategies, while FL ensures collaborative model development without sharing sensitive patient data. Furthermore, Transformers and LLMs, traditionally utilized in natural language processing (NLP), are now being applied to medical data for enhanced interpretability and context-based predictions. In addition, this review explores the efficiency of the aforementioned techniques in cancer diagnosis, it addresses key challenges such as data imbalance, and proposes potential solutions. It aims to be a valuable resource for researchers and practitioners, offering insights into current trends and guiding future research in the application of advanced DL techniques for cancer detection.},
  archive      = {J_ICV},
  author       = {Yassine Habchi and Hamza Kheddar and Yassine Himeur and Adel Belouchrani and Erchin Serpedin and Fouad Khelifi and Muhammad E.H. Chowdhury},
  doi          = {10.1016/j.imavis.2025.105495},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105495},
  shortjournal = {Image Vis. Comput.},
  title        = {Advanced deep learning and large language models: Comprehensive insights for cancer detection},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rif-diff: Improving image fusion based on diffusion model
via residual prediction. <em>ICV</em>, <em>157</em>, 105494. (<a
href="https://doi.org/10.1016/j.imavis.2025.105494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an image fusion framework Rif-Diff, which adopts several strategies and approaches to improve current fusion methods based on diffusion model. Rif-Diff employs residual images as the generation target of the diffusion model to optimize the model’s convergence process and enhance the fusion performance. For fusion tasks lacking ground truth, image fusion prior is utilized to facilitate the production of residual images. Simultaneously, to overcome the limitations of the model’s learning capacity imposed by training with image fusion prior, Rif-Diff introduces the idea of image restoration to enable the initial fused images to incorporate more expected information. Additionally, a dual-step decision module is designed to address the blurriness issue of fused images in existing multi-focus image fusion methods that do not rely on decision maps. Extensive experiments demonstrate the effectiveness of Rif-Diff across multiple fusion tasks including multi-focus image fusion, multi-exposure image fusion, and infrared-visible image fusion. The code is available at: https://github.com/peixuanWu/Rif-Diff .},
  archive      = {J_ICV},
  author       = {Peixuan Wu and Shen Yang and Jin Wu and Qian Li},
  doi          = {10.1016/j.imavis.2025.105494},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105494},
  shortjournal = {Image Vis. Comput.},
  title        = {Rif-diff: Improving image fusion based on diffusion model via residual prediction},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strengthening incomplete multi-view clustering: An attention
contrastive learning method. <em>ICV</em>, <em>157</em>, 105493. (<a
href="https://doi.org/10.1016/j.imavis.2025.105493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering presents greater challenges than traditional multi-view clustering. In recent years, significant progress has been made in this field, multi-view clustering relies on the consistency and integrity of views to ensure the accurate transmission of data information. However, during the process of data collection and transmission, data loss is inevitable, leading to partial view loss and increasing the difficulty of joint learning on incomplete multi-view data. To address this issue, we propose a multi-view contrastive learning framework based on the attention mechanism. Previous contrastive learning mainly focused on the relationships between isolated sample pairs, which limited the robustness of the method. Our method selects positive samples from both global and local perspectives by utilizing the nearest neighbor graph to maximize the correlation between local features and latent features of each view. Additionally, we use a cross-view encoder network with self-attention structure to fuse the low dimensional representations of each view into a joint representation, and guide the learning of the joint representation through a high confidence structure. Furthermore, we introduce graph constraint learning to explore potential neighbor relationships among instances to facilitate data reconstruction. The experimental results on six multi-view datasets demonstrate that our method exhibits significant effectiveness and superiority compared to existing methods.},
  archive      = {J_ICV},
  author       = {Shudong Hou and Lanlan Guo and Xu Wei},
  doi          = {10.1016/j.imavis.2025.105493},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105493},
  shortjournal = {Image Vis. Comput.},
  title        = {Strengthening incomplete multi-view clustering: An attention contrastive learning method},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early progression detection from MCI to AD using multi-view
MRI for enhanced assisted living. <em>ICV</em>, <em>157</em>, 105491.
(<a href="https://doi.org/10.1016/j.imavis.2025.105491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is a progressive neurodegenerative disorder. Early detection is crucial for timely intervention and treatment to improve assisted living. Although magnetic resonance imaging (MRI) is a widely used neuroimaging modality for the diagnosis of AD, most studies focus on a single MRI plane, missing comprehensive spatial information. In this study, we proposed a novel approach that leverages multiple MRI planes (axial, coronal, and sagittal) from 3D MRI volumes to predict progression from stable mild cognitive impairment (sMCI) to progressive MCI (pMCI) and AD. We employed a list of convolutional neural networks, including EfficientNet-B7, ConvNext, and DenseNet-121, to extract deep features from each MRI plane, followed by a feature enhancement step through an attention module. The optimized feature set was then passed through a Bayesian-optimized pool of classification heads (i.e., multilayer perceptron (MLP), long short-term memory (LSTM), and multi-head attention (MHA)) to obtain the most effective model for each MRI plane. The optimal model for each MRI plane was then integrated into homogeneous and heterogeneous ensembles to further enhance the performance of the model. Using the ADNI dataset, the proposed model achieved 91% accuracy, 87% sensitivity, 88% specificity, and 92% AUC. To enhance the interpretability of the model, we used the Grad-CAM explainability technique to generate attention maps for each MRI plane, which identified critical brain regions affected by disease progression. These attention maps revealed consistent patterns of tissue damage across the MRI scans. The results demonstrate the effectiveness of combining multiplane MRI data with ensemble learning and attention mechanisms to improve the early detection and tracking of AD progression in patients with MCI, offering a more comprehensive diagnostic tool and enhanced clinical decision-making. The datasets, results, and code used to conduct the comprehensive analysis are made available to the research community through the following link: https://github.com/nasir3843/Early_Progression_detection_MCI-to_AD},
  archive      = {J_ICV},
  author       = {Nasir Rahim and Naveed Ahmad and Waseem Ullah and Jatin Bedi and Younhyun Jung},
  doi          = {10.1016/j.imavis.2025.105491},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105491},
  shortjournal = {Image Vis. Comput.},
  title        = {Early progression detection from MCI to AD using multi-view MRI for enhanced assisted living},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal few-shot image recognition with enhanced
semantic and visual integration. <em>ICV</em>, <em>157</em>, 105490. (<a
href="https://doi.org/10.1016/j.imavis.2025.105490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot Learning (FSL) enables models to recognize new classes with only a few examples by leveraging knowledge from known classes. Although some methods incorporate class names as prior knowledge, effectively integrating visual and semantic information remains challenging. Additionally, conventional similarity measurement techniques often result in information loss, obscure distinctions between samples, and fail to capture intra-sample diversity. To address these issues, this paper presents a Multi-modal Few-shot Image Recognition (MFSIR) approach. We first introduce the Multi-Scale Interaction Module (MSIM), which facilitates multi-scale interactions between semantic and visual features, significantly enhancing the representation of visual features. We also propose the Hybrid Similarity Measurement Module (HSMM), which integrates information from multiple dimensions to evaluate the similarity between samples by dynamically adjusting the weights of various similarity measurement methods, thereby improving the accuracy and robustness of similarity assessments. Experimental results demonstrate that our approach significantly outperforms existing methods on four FSL benchmarks, with marked improvements in FSL accuracy under 1-shot and 5-shot scenarios.},
  archive      = {J_ICV},
  author       = {Chunru Dong and Lizhen Wang and Feng Zhang and Qiang Hua},
  doi          = {10.1016/j.imavis.2025.105490},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105490},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-modal few-shot image recognition with enhanced semantic and visual integration},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object tracking based on temporal and spatial context
information. <em>ICV</em>, <em>157</em>, 105488. (<a
href="https://doi.org/10.1016/j.imavis.2025.105488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, numerous advanced trackers improve stability by optimizing the target visual appearance models or by improving interactions between templates and search areas. Despite these advancements, appearance-based trackers still primarily depend on the visual information of targets without adequately integrating spatio-temporal context information, thus limiting their effectiveness in handling similar objects around the target. To address this challenge, a novel object tracking method, TSCTrack, which leverages spatio-temporal context information, has been introduced. TSCTrack overcomes the shortcomings of traditional center-cropping preprocessing techniques by introducing Global Spatial Position Embedding, effectively preserving spatial information and capturing motion data of targets. Additionally, TSCTrack incorporates a Spatial Relationship Aggregation module and a Temporal Relationship Aggregation module—the former captures static spatial context information per frame, while the latter integrates dynamic temporal context information. This sophisticated integration allows the Dynamic Tracking Prediction module to generate precise target coordinates effectively, greatly reducing the impact of target deformations and scale changes on tracking performance. Demonstrated across multiple public tracking datasets including LaSOT, TrackingNet, UAV123, GOT-10k, and OTB, TSCTrack showcases superior performance and validates its exceptional tracking capabilities in diverse scenarios.},
  archive      = {J_ICV},
  author       = {Yan Chen and Tao Lin and Jixiang Du and Hongbo Zhang},
  doi          = {10.1016/j.imavis.2025.105488},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105488},
  shortjournal = {Image Vis. Comput.},
  title        = {Object tracking based on temporal and spatial context information},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An edge-aware high-resolution framework for camouflaged
object detection. <em>ICV</em>, <em>157</em>, 105487. (<a
href="https://doi.org/10.1016/j.imavis.2025.105487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged objects are often seamlessly assimilated into their surroundings and exhibit indistinct boundaries. The complex environmental conditions and the high intrinsic similarity between camouflaged targets and their backgrounds present significant challenges in accurately locating and fully segmenting these objects. Although existing methods have achieved remarkable performance across various real-world scenarios, they still struggle with challenging cases such as small targets, thin structures, and blurred boundaries. To address these issues, we propose a novel edge-aware high-resolution network. Specifically, we design a High-Resolution Feature Enhancement Module to exploit multi-scale features while preserving local details. Furthermore, we introduce an Edge Prediction Module to generate high-quality edge prediction maps. Subsequently, we develop an Attention-Guided Fusion Module to effectively leverage the edge prediction maps. With these key modules, the proposed model achieves real-time performance at 58 FPS and surpasses 21 state-of-the-art algorithms across six standard evaluation metrics. Source code will be publicly available at https://github.com/clelouch/EHNet .},
  archive      = {J_ICV},
  author       = {Jingyuan Ma and Tianyou Chen and Jin Xiao and Xiaoguang Hu and Yingxun Wang},
  doi          = {10.1016/j.imavis.2025.105487},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105487},
  shortjournal = {Image Vis. Comput.},
  title        = {An edge-aware high-resolution framework for camouflaged object detection},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive scale matching for remote sensing object detection
based on aerial images. <em>ICV</em>, <em>157</em>, 105482. (<a
href="https://doi.org/10.1016/j.imavis.2025.105482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing object detection based on aerial images presents challenges due to their complex backgrounds, and the utilization of specific a contextual information can enhance detection accuracy. Inadequate long-range background information may lead to erroneous detection of small remotely sensed objects, with variations in background complexity observed across different object types. In this paper, we propose a new YOLO -based real-time object detector. The detector aims to S cale- M atch the proportions of various objects in remote sensing images using the model named YOLO-SM . Specifically, this paper proposes a straightforward yet highly efficient building block that dynamically adjusts the necessary receptive field for each object, minimizing the loss of feature information caused by consecutive convolutions. Additionally, a supplementary bottom-up pathway is incorporated to improve the representation of smaller objects. Empirical evaluations conducted on DOTA-v1.0, DOTA-v1.5, DIOR-R, and HRSC2016 datasets confirm the efficacy of the proposed methodology. On DOTA-v1.0, compared to RTMDet-R-L, YOLO-SM-S achieved competitive accuracy while significantly reducing parameters by 74.8% and FLOPs by 78.5%. Compared to LSKNet on HRSC2016, YOLO-SM-Tiny dramatically reduces 76% of parameters and 90% of FLOPs and improves FPS by about three times while maintaining stable accuracy.},
  archive      = {J_ICV},
  author       = {Lu Han and Nan Li and Zeyuan Zhong and Dong Niu and Bingbing Gao},
  doi          = {10.1016/j.imavis.2025.105482},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105482},
  shortjournal = {Image Vis. Comput.},
  title        = {Adaptive scale matching for remote sensing object detection based on aerial images},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video wire inpainting via hierarchical feature mixture.
<em>ICV</em>, <em>157</em>, 105460. (<a
href="https://doi.org/10.1016/j.imavis.2025.105460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video wire inpainting aims at automatically eliminating visible wires from film footage, significantly streamlining post-production workflows. Previous models address redundancy in wire removal by eliminating redundant blocks to enhance focus on crucial wire details for more accurate reconstruction. However, once redundancy is removed, the disorganized non-redundant blocks disrupt temporal and spatial coherence, making seamless inpainting challenging. The absence of multi-scale feature fusion further limits the model’s ability to handle different wire scales and blend inpainted regions with complex backgrounds. To address these challenges, we propose a Hierarchical Feature Mixture Network (HFM-Net) that integrates two novel modules: a Hierarchical Transformer Module (HTM) and a Spatio-temporal Feature Mixture Module (SFM). Specifically, the HTM employs redundancy-aware attention modules and lightweight transformers to reorganize and fuse key high- and low-dimensional patches. The lightweight transformers are sufficient due to the reduced number of non-redundant blocks processing. By aggregating similar features, these transformers guide the alignment of non-redundant blocks and achieve effective spatio-temporal synchronization. Building on this, the SFM incorporates gated convolutions and GRU to enhance spatial and temporal integration further. Gated convolutions fuse low- and high-dimensional features, while the GRU captures temporal dependencies, enabling seamless inpainting of dynamic wire patterns. Additionally, we introduce a lightweight 3D separable convolution discriminator to improve video quality during the inpainting process while reducing computational costs. Experimental results demonstrate that HFM-Net achieves state-of-the-art performance on the video wire removal task.},
  archive      = {J_ICV},
  author       = {Zhong Ji and Yimu Su and Yan Zhang and Shuangming Yang and Yanwei Pang},
  doi          = {10.1016/j.imavis.2025.105460},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105460},
  shortjournal = {Image Vis. Comput.},
  title        = {Video wire inpainting via hierarchical feature mixture},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
