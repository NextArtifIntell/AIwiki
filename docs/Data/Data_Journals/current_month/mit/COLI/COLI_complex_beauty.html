<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COLI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coli---9">COLI - 9</h2>
<ul>
<li><details>
<summary>
(2025). Automatic language identification in texts. <em>COLI</em>,
<em>51</em>(1), 339–341. (<a
href="https://doi.org/10.1162/coli_r_00521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language identification (LI) for text data, in the ideal scenario, determines the human languages used at every location in a corpus. In practice this often means choosing the likeliest language at the document level: This is already quite useful, for example, when presenting a webpage to the user and deciding (a) whether to translate it and (b) which model to use for that purpose. However, nuances like code-switching (language alternation), dialect variation, and ambiguously short content are increasingly common with the ubiquity of digital communication like text messaging and micro-blogs. Geographic areas like Africa and the Indian subcontinent bring enormous linguistic diversity and flexibility that break the document-level LI paradigm. While standard references (Jurafsky and Martin 2023) introduce LI, touch on these subtleties, and often present related methods and models in other contexts, Automatic Language Identification in Texts is specifically dedicated to LI in its full practical variety.In the course of producing a broad and thorough survey, perhaps the most striking takeaway from Jauhiainen et al. is the chaotic state of research on this critical task. This might be due to the view that, for digitally well-attested languages occurring in domains with monolingual documents of at least modest length, LI is solved: These circumstances are common, and the emphasis on massive data sets can make the rarer cases seem less important. When challenges arise in specific, applied downstream research, they are often addressed in an ad hoc fashion, such as through active learning techniques for gathering human annotations or linear programming to incorporate prior knowledge (Lippincott and Van Durme 2016), without consolidation into broader outcomes for the research community. Throughout Automatic Language Identification in Texts, the authors have the consistent goal of improving this situation. The book is structured into six chapters:Chapter 1 introduces the history of LI, stretching from early feature-engineering approaches to still-standard models based on character n-grams closely related to fundamental models of communication (Shannon 1948), and the burgeoning collection of shared tasks aimed at specific domains, such as ancient scripts or regional dialects. Unlike much of machine learning for natural language processing tasks, traditional models have remained highly competitive for LI compared with deep neural networks: perhaps data sparsity prevents effective training, or traditional features are already well-suited for LI. Downstream use-cases and challenges are summarized, with copious citations to prior and ongoing work.Chapter 2 begins with the authors’ efforts to standardize the discourse around LI by specifying a common notation that subsumes the variety utilized in the literature. While the notation is a modest shift from those that treat data as a sequence of fully distinct documents, treating documents as boundaries within a single large sequence of characters consolidates the spectrum of methods that will be covered. In terms of linguistic features, the focus is on character n-grams, and the authors address several standard concerns: weighting, smoothing, and incorporating linguistic knowledge. The latter is particularly interesting and perhaps under-explored, since there is often less practical motivation to move beyond the immediate use-case and consider, for example, the phylogenetic structure of world languages. The bulk of the chapter is devoted to describing a wide range of classification methods that use these features, some standard (e.g., logistic regression, naive Bayes), others the specific ensembles or statistical tests adopted by existing research.Chapter 3 addresses evaluation, the other end of the experimental pipeline that requires standardization. While a handful of metrics have been used historically, most research has converged on macro balanced F-score, which equally weights precision and recall as well as performance on each language. In the absence of a clearly articulated reason to do otherwise, this is the most even-handed approach. The bulk of the chapter is devoted to a survey of standard data sets and shared tasks, both historical and ongoing. This is a useful reference for researchers in search of venues aimed at their specific goals, or looking for broader patterns in outcomes.Chapter 4 considers the primary axes that may elevate LI from “solved” to “challenging”: language similarity, low-resource languages, orthographic systems and variation, short text, and code-switching. Some of these involve questions of representation: What do we treat as a “language”? What is the “correct” label of a short text that’s valid in multiple languages, such as “quando?”, which is a common question in Portuguese and Italian? How should one label a text containing multiple languages, such as “I’ll ask mi hombre next time I see him”? Chapter 5 then considers the pursuit of a maximally general model capable of characterizing massive collections of heterogeneous content, unknown languages, and domain shift.Chapter 6 discusses several prominent or otherwise compelling uses of LI, from the pragmatic needs of machine translation to subtle tasks like determining the native language based on characteristic patterns in a second language. For instance, corpora of writing from known L2 speakers of English are widespread due to the popularity of English as a second language throughout education, allowing the study of orthographic mistakes grounded in phonetic properties of a native language. Stylistics and authorship attribution share useful features with LI, as they strive to avoid learning topical properties that are often correlated with language.The authors conclude by reiterating the diversity of phenomena that existing LI techniques rarely treat as first-order challenges (until they become immediately relevant), and the difficulty of drawing broader conclusions from the current literature. The book effectively catalogues these challenges and heterogeneity while also providing a stable reference for the community working to organize and extend research in this area. This is useful for several audiences and purposes: Students seeking to understand the history and landscape of LIResearchers hoping to unify or extend existing methodsPractitioners or stakeholders who need to select and justify an approach to a specific taskThe only notable “limitation” of the book is in fact endemic to the topic: The poorly mapped variety of LI research is naturally going to show through any thorough survey. The authors are up-front about this state of affairs and succeed at improving on it.},
  archive      = {J_COLI},
  author       = {Lippincott, Tom},
  doi          = {10.1162/coli_r_00521},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {339-341},
  shortjournal = {Comput. Lingu.},
  title        = {Automatic language identification in texts},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on LLM-generated text detection: Necessity,
methods, and future directions. <em>COLI</em>, <em>51</em>(1), 275–338.
(<a href="https://doi.org/10.1162/coli_a_00549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable ability of large language models (LLMs) to comprehend, interpret, and generate complex language has rapidly integrated LLM-generated text into various aspects of daily life, where users increasingly accept it. However, the growing reliance on LLMs underscores the urgent need for effective detection mechanisms to identify LLM-generated text. Such mechanisms are critical to mitigating misuse and safeguarding domains like artistic expression and social networks from potential negative consequences. LLM-generated text detection, conceptualized as a binary classification task, seeks to determine whether an LLM produced a given text. Recent advances in this field stem from innovations in watermarking techniques, statistics-based detectors, and neural-based detectors. Human-assisted methods also play a crucial role. In this survey, we consolidate recent research breakthroughs in this field, emphasizing the urgent need to strengthen detector research. Additionally, we review existing datasets, highlighting their limitations and developmental requirements. Furthermore, we examine various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues, and ineffective evaluation frameworks. Finally, we outline intriguing directions for future research in LLM-generated text detection to advance responsible artificial intelligence. This survey aims to provide a clear and comprehensive introduction for newcomers while offering seasoned researchers valuable updates in the field. 1},
  archive      = {J_COLI},
  author       = {Wu, Junchao and Yang, Shu and Zhan, Runzhe and Yuan, Yulin and Chao, Lidia Sam and Wong, Derek Fai},
  doi          = {10.1162/coli_a_00549},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {275-338},
  shortjournal = {Comput. Lingu.},
  title        = {A survey on LLM-generated text detection: Necessity, methods, and future directions},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural semantic parsing with extremely rich symbolic meaning
representations. <em>COLI</em>, <em>51</em>(1), 235–274. (<a
href="https://doi.org/10.1162/coli_a_00542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current open-domain neural semantics parsers show impressive performance. However, closer inspection of the symbolic meaning representations they produce reveals significant weaknesses: Sometimes they tend to merely copy character sequences from the source text to form symbolic concepts, defaulting to the most frequent word sense based in the training distribution. By leveraging the hierarchical structure of a lexical ontology, we introduce a novel compositional symbolic representation for concepts based on their position in the taxonomical hierarchy. This representation provides richer semantic information and enhances interpretability. We introduce a neural “taxonomical” semantic parser to utilize this new representation system of predicates, and compare it with a standard neural semantic parser trained on the traditional meaning representation format, employing a novel challenge set and evaluation metric for evaluation. Our experimental findings demonstrate that the taxonomical model, trained on much richer and complex meaning representations, is slightly subordinate in performance to the traditional model using the standard metrics for evaluation, but outperforms it when dealing with out-of-vocabulary concepts. We further show through neural model probing that training on a taxonomic representation enhances the model’s ability to learn the taxonomical hierarchy. This finding is encouraging for research in computational semantics that aims to combine data-driven distributional meanings with knowledge-based symbolic representations.},
  archive      = {J_COLI},
  author       = {Zhang, Xiao and Bouma, Gosse and Bos, Johan},
  doi          = {10.1162/coli_a_00542},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {235-274},
  shortjournal = {Comput. Lingu.},
  title        = {Neural semantic parsing with extremely rich symbolic meaning representations},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating synthetic data generation from user generated
text. <em>COLI</em>, <em>51</em>(1), 191–233. (<a
href="https://doi.org/10.1162/coli_a_00540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-generated content provides a rich resource to study social and behavioral phenomena. Although its application potential is currently limited by the paucity of expert labels and the privacy risks inherent in personal data, synthetic data can help mitigate this bottleneck. In this work, we introduce an evaluation framework to facilitate research on synthetic language data generation for user-generated text. We define a set of aspects for assessing data quality, namely, style preservation, meaning preservation, and divergence, as a proxy for privacy. We introduce metrics corresponding to each aspect. Moreover, through a set of generation strategies and representative tasks and baselines across domains, we demonstrate the relation between the quality aspects of synthetic user generated content, generation strategies, metrics, and downstream performance. To our knowledge, our work is the first unified evaluation framework for user-generated text in relation to the specified aspects, offering both intrinsic and extrinsic evaluation. We envisage it will facilitate developments towards shareable, high-quality synthetic language data.},
  archive      = {J_COLI},
  author       = {Chim, Jenny and Ive, Julia and Liakata, Maria},
  doi          = {10.1162/coli_a_00540},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {191-233},
  shortjournal = {Comput. Lingu.},
  title        = {Evaluating synthetic data generation from user generated text},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compositionality and sentence meaning: Comparing semantic
parsing and transformers on a challenging sentence similarity dataset.
<em>COLI</em>, <em>51</em>(1), 139–190. (<a
href="https://doi.org/10.1162/coli_a_00536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major outstanding questions in computational semantics is how humans integrate the meaning of individual words into a sentence in a way that enables understanding of complex and novel combinations of words, a phenomenon known as compositionality. Many approaches to modeling the process of compositionality can be classified as either “vector-based” models, in which the meaning of a sentence is represented as a vector of numbers, or “syntax-based” models, in which the meaning of a sentence is represented as a structured tree of labeled components. A major barrier in assessing and comparing these contrasting approaches is the lack of large, relevant datasets for model comparison. This article aims to address this gap by introducing a new dataset, STS3k, which consists of 2,800 pairs of sentences rated for semantic similarity by human participants. The sentence pairs have been selected to systematically vary different combinations of words, providing a rigorous test and enabling a clearer picture of the comparative strengths and weaknesses of vector-based and syntax-based methods. Our results show that when tested on the new STS3k dataset, state-of-the-art transformers poorly capture the pattern of human semantic similarity judgments, while even simple methods for combining syntax- and vector-based components into a novel hybrid model yield substantial improvements. We further show that this improvement is due to the ability of the hybrid model to replicate human sensitivity to specific changes in sentence structure. Our findings provide evidence for the value of integrating multiple methods to better reflect the way in which humans mentally represent compositional meaning.},
  archive      = {J_COLI},
  author       = {Fodor, James and Deyne, Simon De and Suzuki, Shinsuke},
  doi          = {10.1162/coli_a_00536},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {139-190},
  shortjournal = {Comput. Lingu.},
  title        = {Compositionality and sentence meaning: Comparing semantic parsing and transformers on a challenging sentence similarity dataset},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine translation meta evaluation through translation
accuracy challenge sets. <em>COLI</em>, <em>51</em>(1), 73–137. (<a
href="https://doi.org/10.1162/coli_a_00537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent machine translation (MT) metrics calibrate their effectiveness by correlating with human judgment. However, these results are often obtained by averaging predictions across large test sets without any insights into the strengths and weaknesses of these metrics across different error types. Challenge sets are used to probe specific dimensions of metric behavior but there are very few such datasets and they either focus on a limited number of phenomena or a limited number of language pairs. We introduce ACES , a contrastive challenge set spanning 146 language pairs, aimed at discovering whether metrics can identify 68 translation accuracy errors. These phenomena range from basic alterations at the word/character level to more intricate errors based on discourse and real-world knowledge. We conducted a large-scale study by benchmarking ACES on 47 metrics submitted to the WMT 2022 and WMT 2023 metrics shared tasks. We also measure their sensitivity to a range of linguistic phenomena. We further investigate claims that large language models (LLMs) are effective as MT evaluators, addressing the limitations of previous studies by using a dataset that covers a range of linguistic phenomena and language pairs and includes both low- and medium-resource languages. Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods are unreliable. We expose a number of major flaws with existing methods: Most metrics ignore the source sentence; metrics tend to prefer surface level overlap; and over-reliance on language-agnostic representations leads to confusion when the target language is similar to the source language. To further encourage detailed evaluation beyond singular scores, we expand ACES to include error span annotations, denoted as SPAN-ACES, and we use this dataset to evaluate span-based error metrics, showing that these metrics also need considerable improvement. Based on our observations, we provide a set of recommendations for building better MT metrics, including focusing on error labels instead of scores, ensembling, designing metrics to explicitly focus on the source sentence, focusing on semantic content rather than relying on the lexical overlap, and choosing the right pre-trained model for obtaining representations.},
  archive      = {J_COLI},
  author       = {Moghe, Nikita and Fazla, Arnisa and Amrhein, Chantal and Kocmi, Tom and Steedman, Mark and Birch, Alexandra and Sennrich, Rico and Guillou, Liane},
  doi          = {10.1162/coli_a_00537},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {73-137},
  shortjournal = {Comput. Lingu.},
  title        = {Machine translation meta evaluation through translation accuracy challenge sets},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ERST: A signaled graph theory of discourse relations and
organization. <em>COLI</em>, <em>51</em>(1), 23–72. (<a
href="https://doi.org/10.1162/coli_a_00538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we present Enhanced Rhetorical Structure Theory (eRST), a new theoretical framework for computational discourse analysis, based on an expansion of Rhetorical Structure Theory (RST). The framework encompasses discourse relation graphs with tree-breaking, non-projective and concurrent relations, as well as implicit and explicit signals which give explainable rationales to our analyses. We survey shortcomings of RST and other existing frameworks, such as Segmented Discourse Representation Theory, the Penn Discourse Treebank, and Discourse Dependencies, and address these using constructs in the proposed theory. We provide annotation, search, and visualization tools for data, and present and evaluate a freely available corpus of English annotated according to our framework, encompassing 12 spoken and written genres with over 200K tokens. Finally, we discuss automatic parsing, evaluation metrics, and applications for data in our framework.},
  archive      = {J_COLI},
  author       = {Zeldes, Amir and Aoyama, Tatsuya and Liu, Yang Janet and Peng, Siyao and Das, Debopam and Gessler, Luke},
  doi          = {10.1162/coli_a_00538},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {23-72},
  shortjournal = {Comput. Lingu.},
  title        = {ERST: A signaled graph theory of discourse relations and organization},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MUCking in, or fifty years in information extraction.
<em>COLI</em>, <em>51</em>(1), 7–22. (<a
href="https://doi.org/10.1162/coli_a_00547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I want to thank the ACL for this Lifetime Achievement Award. I am deeply honored to be receiving it. I would also like to thank the students, faculty, and researchers who were members of the Proteus Project during most of my professional lifetime. It was an honor to serve that group.},
  archive      = {J_COLI},
  author       = {Grishman, Ralph},
  doi          = {10.1162/coli_a_00547},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {7-22},
  shortjournal = {Comput. Lingu.},
  title        = {MUCking in, or fifty years in information extraction},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opening a new chapter for computational linguistics.
<em>COLI</em>, <em>51</em>(1), 1–5. (<a
href="https://doi.org/10.1162/coli_e_00552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By the end of 2024, the journal Computational Linguistics has reached a significant milestone: It has published exactly 50 volumes over the past half-century. As we launch the first issue of Volume 51, this is an opportune moment to reflect on the journal’s legacy, ongoing evolution, and the exciting changes that lie ahead. Together, we embark on a journey to open a new chapter for this storied publication.},
  archive      = {J_COLI},
  author       = {Lu, Wei},
  doi          = {10.1162/coli_e_00552},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {1-5},
  shortjournal = {Comput. Lingu.},
  title        = {Opening a new chapter for computational linguistics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
