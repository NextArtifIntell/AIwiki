<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TACL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tacl---9">TACL - 9</h2>
<ul>
<li><details>
<summary>
(2025). Transformers as transducers. <em>TACL</em>, <em>13</em>,
200–219. (<a href="https://doi.org/10.1162/tacl_a_00736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of (total functional) transductions. We do so using variants of RASP, a programming language designed to help people “think like transformers,” as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence transductions and show that it computes exactly the first-order rational transductions (such as string rotation). Then, we introduce two new extensions. B-RASP[ pos ] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular transductions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular transductions. Finally, we show that masked average-hard attention transformers can simulate S-RASP.},
  archive      = {J_TACL},
  author       = {Strobl, Lena and Angluin, Dana and Chiang, David and Rawski, Jonathan and Sabharwal, Ashish},
  doi          = {10.1162/tacl_a_00736},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {200-219},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Transformers as transducers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OPT-tree: Speculative decoding with adaptive draft tree
structure. <em>TACL</em>, <em>13</em>, 188–199. (<a
href="https://doi.org/10.1162/tacl_a_00735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a “draft and then verify” mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which do not adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we propose OPT-Tree, an algorithm to construct adaptive and scalable draft trees, which can be applied to any autoregressive draft model. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree .},
  archive      = {J_TACL},
  author       = {Wang, Jikai and Su, Yi and Li, Juntao and Xia, Qingrong and Ye, Zi and Duan, Xinyu and Wang, Zhefeng and Zhang, Min},
  doi          = {10.1162/tacl_a_00735},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {188-199},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {OPT-tree: Speculative decoding with adaptive draft tree structure},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A confidence-based acquisition model for self-supervised
active learning and label correction. <em>TACL</em>, <em>13</em>,
167–187. (<a href="https://doi.org/10.1162/tacl_a_00734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labeling. To address these challenges, we present CAMEL ( C onfidence-based A cquisition M odel for E fficient self-supervised active L earning), a pool-based active learning framework tailored to sequential multi-output problems. CAMEL possesses two core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, and (2) it facilitates self-supervision for the remainder of the sequence. By deploying a label correction mechanism, CAMEL can also be utilized for data cleaning. We evaluate CAMEL on two sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited and noisy datasets. Our experiments demonstrate that CAMEL significantly outperforms the baselines in terms of efficiency. Furthermore, the data corrections suggested by our method contribute to an overall improvement in the quality of the resulting datasets. 1},
  archive      = {J_TACL},
  author       = {Niekerk, Carel van and Geishauser, Christian and Heck, Michael and Feng, Shutong and Lin, Hsien-chin and Lubis, Nurul and Ruppik, Benjamin and Vukovic, Renato and Gašić, Milica},
  doi          = {10.1162/tacl_a_00734},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {167-187},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A confidence-based acquisition model for self-supervised active learning and label correction},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning syntax without planting trees: Understanding
hierarchical generalization in transformers. <em>TACL</em>, <em>13</em>,
121–141. (<a href="https://doi.org/10.1162/tacl_a_00733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers trained on natural language data have been shown to exhibit hierarchical generalization without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such preference for hierarchical generalization. We extensively experiment with transformers trained on five synthetic, controlled datasets using several training objectives and show that, while objectives such as sequence-to-sequence modeling, classification, etc., often fail to lead to hierarchical generalization, the language modeling objective consistently leads to transformers generalizing hierarchically. We then study how different generalization behaviors emerge during the training by conducting pruning experiments that reveal the joint existence of subnetworks within the model implementing different generalizations. Finally, we take a Bayesian perspective to understand transformers’ preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and if the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization. Overall, our work presents new insights on the origins of hierarchical generalization in transformers and provides a theoretical framework for studying generalization in language models.},
  archive      = {J_TACL},
  author       = {Ahuja, Kabir and Balachandran, Vidhisha and Panwar, Madhur and He, Tianxing and Smith, Noah A. and Goyal, Navin and Tsvetkov, Yulia},
  doi          = {10.1162/tacl_a_00733},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {121-141},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Learning syntax without planting trees: Understanding hierarchical generalization in transformers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating critical period effects in language
acquisition through neural language models. <em>TACL</em>, <em>13</em>,
96–120. (<a href="https://doi.org/10.1162/tacl_a_00725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans appear to have a critical period (CP) for language acquisition: Second language (L 2 ) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L 1 ) after this period (but not before) typically does not lead to substantial loss of L 1 proficiency. It is unknown whether these CP effects result from innately determined brain maturation or as a stabilization of neural connections naturally induced by experience. In this study, we use language models (LMs) to test the extent to which these phenomena are peculiar to humans, or shared by a broader class of language learners. We vary the age of exposure by training LMs on language pairs in various experimental conditions, and find that LMs, which lack any direct analog to innate maturational stages, do not show CP effects when the age of exposure of L 2 is delayed. Our results contradict the claim that CP effects are an inevitable result of statistical learning, and they are consistent with an innate mechanism for CP effects. We show that we can reverse-engineer the CP by introducing a regularizer partway through training to simulate a maturational decrease in plasticity. All in all, our results suggest that L 1 learning on its own may not be enough to induce a CP, and additional engineering is necessary to make language models more cognitively plausible.},
  archive      = {J_TACL},
  author       = {Constantinescu, Ionut and Pimentel, Tiago and Cotterell, Ryan and Warstadt, Alex},
  doi          = {10.1162/tacl_a_00725},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {96-120},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Investigating critical period effects in language acquisition through neural language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Salute the classic: Revisiting challenges of machine
translation in the age of large language models. <em>TACL</em>,
<em>13</em>, 73–95. (<a
href="https://doi.org/10.1162/tacl_a_00730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017 ) that have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch , amount of parallel data , rare word prediction , translation of long sentences , attention model as word alignment , and sub-optimal beam search . Our empirical findings show that LLMs effectively reduce reliance on parallel data for major languages during pretraining and significantly improve translation of long sentences containing approximately 80 words, even translating documents up to 512 words. Despite these improvements, challenges in domain mismatch and rare word prediction persist. While NMT-specific challenges like word alignment and beam search may not apply to LLMs, we identify three new challenges in LLM-based translation: inference efficiency, translation of low-resource languages during pretraining, and human-aligned evaluation.},
  archive      = {J_TACL},
  author       = {Pang, Jianhui and Ye, Fanghua and Wong, Derek Fai and Yu, Dian and Shi, Shuming and Tu, Zhaopeng and Wang, Longyue},
  doi          = {10.1162/tacl_a_00730},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {73-95},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Salute the classic: Revisiting challenges of machine translation in the age of large language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLAPnq: Cohesive long-form answers from passages in natural
questions for RAG systems. <em>TACL</em>, <em>13</em>, 53–72. (<a
href="https://doi.org/10.1162/tacl_a_00729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present CLAPnq , a benchmark Long-form Question Answering dataset for the full RAG pipeline. CLAPnq includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The CLAPnq answers are concise , 3x smaller than the full passage, and cohesive , meaning that the answer is composed fluently, often by integrating multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at CLAPnq . We present baseline experiments and analysis for CLAPnq that highlight areas where there is still significant room for improvement in grounded RAG. CLAPnq is publicly available at https://github.com/primeqa/clapnq .},
  archive      = {J_TACL},
  author       = {Rosenthal, Sara and Sil, Avirup and Florian, Radu and Roukos, Salim},
  doi          = {10.1162/tacl_a_00729},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {53-72},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {CLAPnq: Cohesive long-form answers from passages in natural questions for RAG systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpiRit-LM: Interleaved spoken and written language model.
<em>TACL</em>, <em>13</em>, 30–52. (<a
href="https://doi.org/10.1162/tacl_a_00728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce SpiRit-LM , a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code. 1 , 2},
  archive      = {J_TACL},
  author       = {Nguyen, Tu Anh and Muller, Benjamin and Yu, Bokai and Costa-jussa, Marta R. and Elbayad, Maha and Popuri, Sravya and Ropers, Christophe and Duquenne, Paul-Ambroise and Algayres, Robin and Mavlyutov, Ruslan and Gat, Itai and Williamson, Mary and Synnaeve, Gabriel and Pino, Juan and Sagot, Benoît and Dupoux, Emmanuel},
  doi          = {10.1162/tacl_a_00728},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {30-52},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {SpiRit-LM: Interleaved spoken and written language model},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dolomites: Domain-specific long-form methodical tasks.
<em>TACL</em>, <em>13</em>, 1–29. (<a
href="https://doi.org/10.1162/tacl_a_00727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experts in various fields routinely perform methodical writing tasks to plan, organize, and report their work. From a clinician writing a differential diagnosis for a patient, to a teacher writing a lesson plan for students, these tasks are pervasive, requiring to methodically generate structured long-form output for a given input. We develop a typology of methodical tasks structured in the form of a task objective, procedure, input, and output, and introduce DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited from hundreds of experts from across 25 fields. Our benchmark further contains specific instantiations of methodical tasks with concrete input and output examples (1,857 in total) which we obtain by collecting expert revisions of up to 10 model-generated examples of each task. We use these examples to evaluate contemporary language models, highlighting that automating methodical tasks is a challenging long-form generation problem, as it requires performing complex inferences, while drawing upon the given context as well as domain knowledge. Our dataset is available at https://dolomites-benchmark.github.io/ .},
  archive      = {J_TACL},
  author       = {Malaviya, Chaitanya and Agrawal, Priyanka and Ganchev, Kuzman and Srinivasan, Pranesh and Huot, Fantine and Berant, Jonathan and Yatskar, Mark and Das, Dipanjan and Lapata, Mirella and Alberti, Chris},
  doi          = {10.1162/tacl_a_00727},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {1-29},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Dolomites: Domain-specific long-form methodical tasks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
