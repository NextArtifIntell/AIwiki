<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TBD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tbd---45">TBD - 45</h2>
<ul>
<li><details>
<summary>
(2025). Combine the growth of cascades and impact of users for
diffusion prediction. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 887–895. (<a
href="https://doi.org/10.1109/TBDATA.2024.3460530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Information diffusion and diffusion prediction have attracted a great deal of research attention over the past decades. Existing approaches usually make predictions based on the order of the activated users, while recently, some studies have taken the social network into consideration and begun to analyze the influence of neighbors via some graph neural networks. However, they ignore the fact that the interests of users and their neighbors may dynamically change along with the growth of the cascade, and thus fail to model the potential impact of activated users. To address the above shortcomings, we proposed in this paper a deep learning model that combines the Mode of cascades Growth and potential Impact of users (MGI). It leverages GCNs to represent users from the social network to model their static features. Besides, we designed an attention mechanism on the cascade sequence to compute features of activated users, and added the popularity variable to model features of users in cascades. Finally, we combined the growth of cascades and impact of users in our model for diffusion prediction. We conducted extensive experiments on several real-world datasets, and the experimental results demonstrate that our model significantly outperforms the state-of-the-art methods in diffusion prediction.},
  archive  = {J},
  author   = {Pengfei Jiao and Peng Yan and Jilin Zhang and Biao Wang and Wang Zhang and Nailiang Zhao},
  doi      = {10.1109/TBDATA.2024.3460530},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {887-895},
  title    = {Combine the growth of cascades and impact of users for diffusion prediction},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic NN-descent: An efficient k-NN graph construction
method. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 879–886.
(<a href="https://doi.org/10.1109/TBDATA.2024.3460534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a classic k-NN graph construction method, NN-Descent has been adopted in various applications for its simplicity, genericness, and efficiency. However, its memory consumption is high due to the employment of two extra supporting graph structures. In this paper, a novel k-NN graph construction method is proposed. Similar to NN-Descent, the k-NN graph is constructed by doing cross-matching continuously on the sampled neighbors on each neighborhood. Whereas different from NN-Descent, the cross-matching is undertaken directly on the k-NN graph under construction. It makes the extra graph structures adopted to support the cross-matching no longer necessary. Moreover, no synchronization between different threads is needed within one iteration. The high-quality graph is constructed at the high-speed efficiency and considerably better memory efficiency over NN-Descent on both the multi-thread CPU and the GPU.},
  archive  = {J},
  author   = {Jie-Feng Wang and Wan-Lei Zhao and Shihai Xiao and Jiajie Yao and Xuecang Zhang},
  doi      = {10.1109/TBDATA.2024.3460534},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {879-886},
  title    = {Dynamic NN-descent: An efficient k-NN graph construction method},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-based distributed spatio-temporal <span
class="math inline"><em>k</em></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;
nearest neighbors join. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 861–878. (<a
href="https://doi.org/10.1109/TBDATA.2024.3442539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid development of positioning technology produces an extremely large volume of spatio-temporal data with various geometry types such as point, line string, polygon, or a mixed combination of them. As one of the most fundamental but time-consuming operations, $k$ nearest neighbors join ($k$NN join) has attracted much attention. However, most existing works for $k$NN join either ignore temporal information or consider only point data. Besides, most of them do not automatically adapt to the different features of spatio-temporal data. This paper proposes to address a novel and useful problem, i.e., ST-$k$NN join, which considers both spatial closeness and temporal concurrency. To support ST-$k$NN join over a large amount of spatio-temporal data with any geometry types efficiently, we propose a novel distributed solution based on Apache Spark. Specifically, our method adopts a two-round join framework. In the first round join, we propose a new spatio-temporal partitioning method that achieves spatio-temporal locality and load balance at the same time. We also propose a lightweight index structure, i.e., Time Range Count Index (TRC-index), to enable efficient ST-$k$NN join. In the second round join, to reduce the data transmission among different machines, we remove duplicates based on spatio-temporal reference points before shuffling local results. Furthermore, we design a set of models based on Bayesian optimization to automatically determine the values for the introduced parameters. Extensive experiments are conducted using three real big datasets, showing that our method is much more scalable and achieves 9X faster than baselines, and that the proposed models can always predict appropriate parameters for different datasets.},
  archive  = {J},
  author   = {Ruiyuan Li and Jiajun Li and Minxin Zhou and Rubin Wang and Huajun He and Chao Chen and Jie Bao and Yu Zheng},
  doi      = {10.1109/TBDATA.2024.3442539},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {861-878},
  title    = {Learning-based distributed spatio-temporal $k$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt; nearest neighbors join},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A query-aware method for approximate range search in hamming
space. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 848–860.
(<a href="https://doi.org/10.1109/TBDATA.2024.3436636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The range search in Hamming space is to explore the binary vectors whose Hamming distances with a query vector are within a given searching threshold. It arises as the core component of many applications, such as image retrieval, pattern recognition, and machine learning. Existing searching methods in Hamming space require much pre-processing overhead, which are not suitable for processing multiple batches of incoming data in a short time. Moreover, significant pre-processing overhead can be a burden when the number of queries is relatively small. In this paper, we propose a query-aware method for the approximate range search in Hamming space with no pre-process. Specifically, to eliminate the impact of data skewness, we introduce JS-divergence to measure the divergence between data&#39;s distribution and query&#39;s distribution, and specially design a Query-Aware Dimension Partitioning (QADP) strategy to partition the dimensions into several subspaces according to the scales of given searching thresholds. In the subspaces, the candidates can be efficiently obtained by the basic Pigeonhole Principle and our proposed Anti-Pigeonhole Principle. Furthermore, a sampling strategy is designed to estimate the Hamming distance between the query vector and arbitrary binary vector to obtain the final approximate searching results among the candidates. Experimental results on four real-world datasets illustrate that, in comparison with benchmark methods, our method possesses the superior advantages on searching accuracy and efficiency. The proposed method can increase the searching efficiency up to nearly 16 times with high searching accuracy.},
  archive  = {J},
  author   = {Yang Song and Yu Gu and Min Huang and Ge Yu},
  doi      = {10.1109/TBDATA.2024.3436636},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {848-860},
  title    = {A query-aware method for approximate range search in hamming space},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ANF: Crafting transferable adversarial point clouds via
adversarial noise factorization. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 835–847. (<a
href="https://doi.org/10.1109/TBDATA.2024.3436593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transfer-based adversarial attacks involve generating adversarial point clouds in surrogate models and transferring them to other models to assess 3D model robustness. However, current methods rely too much on surrogate model parameters, limiting transferability. In this work, we use Shapley value to identify positive and negative features, guiding optimization of adversarial noise in feature space. To effectively mislead the 3D classifier, we factorize the adversarial noise into positive and negative noise, with the former keeping the features of the adversarial point cloud close to the negative features, and the latter and the adversarial noise moving it away from the positive features. Finally, a novel adversarial point cloud attack method with Adversarial Noise Factorization is proposed, which is abbreviated as ANF. ANF simultaneously optimizes the adversarial noise and its positive and negative noise in the feature space, only relying on partial network parameters, which significantly reduces the reliance on the surrogate model and improves the transferability of the adversarial point cloud. Experiments on well-recognized benchmark datasets show that the transferability of adversarial point clouds generated by ANF could be improved by more than 26.7$\%$ on average over state-of-the-art transfer-based adversarial attack methods.},
  archive  = {J},
  author   = {Hai Chen and Shu Zhao and Xiao Yang and Huanqian Yan and Yuan He and Hui Xue and Fulan Qian and Hang Su},
  doi      = {10.1109/TBDATA.2024.3436593},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {835-847},
  title    = {ANF: Crafting transferable adversarial point clouds via adversarial noise factorization},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal deep learning for semisupervised classification
of hyperspectral and LiDAR data. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 821–834. (<a
href="https://doi.org/10.1109/TBDATA.2024.3433494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning (DL) has emerged as a competitive method in single-modality-dominated remote sensing (RS) data classification tasks, but its classification performance inevitably encounters a bottleneck due to the lack of representation diversity in complicated spatial structures with various land cover types. Therefore, the RS community has been actively researching multimodal feature learning techniques for the same scene. However, expert annotation of multisource data consumes a significant amount of time and cost. This article proposes an end-to-end method called semisupervised multimodal dual-path network (SMDN). This method simultaneously explores spatial-spectral features contained in hyperspectral images (HSI) and elevation information provided by light detection and ranging (LiDAR). SMDN exploits an unsupervised novel encoder-decoder structure as the backbone network to construct a multimodal DL architecture by jointly training with a data-specific branch. To obtain discriminative multimodal representations, SMDN is able to guide the collaborative training of two different unsupervised features mapped in the latent subspace with limited labeled training samples. Furthermore, after a simple modification of the fusion strategy in SMDN, it can be applied to unsupervised classification problems. Experimental results on benchmark RS datasets validate the effectiveness of the developed SMDN compared over many state-of-the-art methods.},
  archive  = {J},
  author   = {Chunyu Pu and Yingxu Liu and Shuai Lin and Xu Shi and Zhengying Li and Hong Huang},
  doi      = {10.1109/TBDATA.2024.3433494},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {821-834},
  title    = {Multimodal deep learning for semisupervised classification of hyperspectral and LiDAR data},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFS-FCM with memory: A model for air quality
multi-dimensional prediction with interpretability. <em>IEEE
Transactions on Big Data</em>, <em>11</em>(2), 810–820. (<a
href="https://doi.org/10.1109/TBDATA.2024.3433467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In order to represent the influences of different semantics on targets and improve the prediction with interpretability ability for multi-dimensional time series, we integrate Axiomatic Fuzzy Set (AFS) and Fuzzy Cognitive Map (FCM) with memory for fuzzy knowledge representation and prediction in this paper. The AFS is used to extract semantics of concepts for fuzzy representation using data distribution. The FCM with memory is trained to model the influence relationships between different semantics of concepts and multiple targets based on multi-dimensional time series data. And a multi- dimensional learning algorithm of AFS-FCM with memory based on gradient descent is developed to investigate the influences of different semantics of concepts on multiple targets. Finally, we validate our model by comparing with other FCMs, intrinsic interpretable models and machine learning methods for prediction of air quality multidimensional time series data, and discuss the performance of AFS-FCM with different transformation functions. The model can not only predict air quality accurately, but also explicitly reveal the specific quantitative relationship of different semantics of meteorology on air quality.},
  archive  = {J},
  author   = {Zhen Peng and Wanquan Liu and Sung-Kwun Oh},
  doi      = {10.1109/TBDATA.2024.3433467},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {810-820},
  title    = {AFS-FCM with memory: A model for air quality multi-dimensional prediction with interpretability},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systemic pipeline of identifying lncRNA-disease
associations to the prognosis and treatment of hepatocellular carcinoma.
<em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 800–809. (<a
href="https://doi.org/10.1109/TBDATA.2024.3433380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Exploring disease mechanisms at the lncRNA level provides valuable guidance for disease prognosis and treatment. Recently, there has been a surge of interest in exploring disease mechanisms via computational methods to overcome the challenge of tremendous manpower and material resources in biological experiments. However, current computational methods suffer from two main limitations: simple data structures that do not consider the close association between multiple types of data, and the lack of a systematic pathogenesis analysis that identified disease-associated lncRNAs are not applied to the downstream disease prognosis and therapeutic analysis from the perspective of data analysis. In this end, we present a systemic pipeline including disease-associated lncRNAs identification and downstream pathogenesis analysis on how the predicted lncRNAs are involved in the disease prognosis and therapy. Due to the importance of identifying disease-associated lncRNAs and the weak interpretability of existing computational identification methods, we propose a novel approach named iLncDA-PT to identify disease-associated lncRNAs considering the interactions between various bio-entities outperforming the other state-of-the-art methods, and then we conduct a systematically subsequent analysis on prognosis and therapy for a specific disease, hepatocellular carcinoma (HCC), as an example. Finally, we reveal a significant association between immune checkpoint expression, tumor microenvironment, and drug treatment.},
  archive  = {J},
  author   = {Wenxiang Zhang and Ye Yuan and Hang Wei and Wenjing Zhang and Bin Liu},
  doi      = {10.1109/TBDATA.2024.3433380},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {800-809},
  title    = {A systemic pipeline of identifying lncRNA-disease associations to the prognosis and treatment of hepatocellular carcinoma},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALD-GCN: Graph convolutional networks with attribute-level
defense. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
788–799. (<a href="https://doi.org/10.1109/TBDATA.2024.3433553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Neural Networks(GNNs), such as Graph Convolutional Network, have exhibited impressive performance on various real-world datasets. However, many researches have confirmed that deliberately designed adversarial attacks can easily confuse GNNs on the classification of target nodes (targeted attacks) or all the nodes (global attacks). According to our observations, different attributes tend to be differently treated when the graph is attacked. Unfortunately, most of the existing defense methods can only defend at the graph or node level, which ignores the diversity of different attributes within each node. To address this limitation, we propose to leverage a new property, named Attribute-level Smoothness (ALS), which is defined based on the local differences of graph. We then propose a novel defense method, named GCN with Attribute-level Defense (ALD-GCN), which utilizes the ALS property to provide attribute-level protection to each attributes. Extensive experiments on real-world graphs have demonstrated the superiority of the proposed work and the potentials of our ALS property in the attacks.},
  archive  = {J},
  author   = {Yihui Li and Yuanfang Guo and Junfu Wang and Shihao Nie and Liang Yang and Di Huang and Yunhong Wang},
  doi      = {10.1109/TBDATA.2024.3433553},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {788-799},
  title    = {ALD-GCN: Graph convolutional networks with attribute-level defense},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secret specification based personalized privacy-preserving
analysis in big data. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 774–787. (<a
href="https://doi.org/10.1109/TBDATA.2024.3433433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The pursuit of refined data analysis and the preservation of privacy in Big Data pose significant concerns. Among the paramount paradigms for addressing these challenges, differential privacy stands out as a vital area of research. However, traditional differential privacy tends to be excessively restrictive when it comes to individuals’ control over their own data. It often treats all data as inherently sensitive, whereas in reality, not all information related to individuals is sensitive and requires an identical level of protection. In this paper, we define secret specification-based differential privacy (SSDP), where the term “secret specification” implies enabling users to decide what aspects of their information are sensitive and what are not, prior to data generation or processing. By allowing individuals to independently define their secret specifications, the SSDP achieves personalized privacy protection and facilitates effective data analysis. To enable the targeted application of SSDP, we further present task-specific mechanisms designed for database and graph data scenarios. Finally, we assess the trade-offs between privacy and utility inherent in the proposed mechanisms through comparative experiments conducted on real datasets, demonstrating the utility enhancements offered by SSDP mechanisms in practical applications.},
  archive  = {J},
  author   = {Jiajun Chen and Chunqiang Hu and Zewei Liu and Tao Xiang and Pengfei Hu and Jiguo Yu},
  doi      = {10.1109/TBDATA.2024.3433433},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {774-787},
  title    = {Secret specification based personalized privacy-preserving analysis in big data},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local high-order graph learning for multi-view clustering.
<em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 761–773. (<a
href="https://doi.org/10.1109/TBDATA.2024.3433525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As the accumulation of multi-view data continues to grow, multi-view clustering has become increasingly important in research fields like data mining. However, current methods have been criticized for their unsatisfactory performance, such as insufficient exploration of intra-view high-order relationships and poor characterization of inter-view diverse features. To overcome these challenges, we propose a novel approach called Local High-order Graph Learning for Multi-View Clustering (LHGL_MVC). Our method aims to explore high-order relationships within a view while also considering diverse information between views. In LHGL_MVC, we learn the initial graphs of each view through self-representation, which are decomposed into consistent and diverse parts to better capture the diversity of different views. Based on consistent parts, we propose a novel local high-order graph learning approach to more effectively explore high-order relationships between samples within each view. At the same time, we leverage high-order relationships between views using the rotated tensor nuclear norm. Finally, we obtain a unified graph for clustering by fusing all consistent affinity graphs and their high-order graphs with adaptive weights. All procedures are integrated into an overall objective function, which mutually promotes during the optimization process. The comprehensive experiments conducted on eleven real-world datasets demonstrate that LHGL_MVC significantly outperforms existing algorithms in various measurements, highlighting the superiority of the proposed method.},
  archive  = {J},
  author   = {Zhi Wang and Qiang Lin and Yaxiong Ma and Xiaoke Ma},
  doi      = {10.1109/TBDATA.2024.3433525},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {761-773},
  title    = {Local high-order graph learning for multi-view clustering},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient learning for billion-scale heterogeneous
information networks. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 748–760. (<a
href="https://doi.org/10.1109/TBDATA.2024.3428331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Heterogeneous graph neural networks (HGNNs) excel at understanding heterogeneous information networks (HINs) and have demonstrated state-of-the-art performance across numerous tasks. However, previous works tend to study small datasets, which deviate significantly from real-world scenarios. More specifically, their heterogeneous message passing results in substantial memory and time overheads, as it requires aggregating heterogeneous neighbor features multiple times. To address this, we propose an Efficient Heterogeneous Graph Neural Network (EHGNN) that leverages heterogeneous personalized PageRank (HPPR) to preserve the influence between all nodes, then approximates message passing and selectively loads neighbor information for one aggregation, significantly reducing memory and time usage. In addition, we employ some lightweight techniques to ensure the performance of EHGNN. Evaluations on various HIN benchmarks in node classification and link prediction tasks unequivocally establish the superiority of EHGNN, surpassing the State-of-the-Art by 11$\%$ in terms of performance. In addition, EHGNN achieves a remarkable 400$\%$ boost in training and inference speed while utilizing less memory. Notably, EHGNN can handle a 200-million-node, 1-billion-link HIN within 18 hours on a single machine, using only 170 GB of memory, which is much lower than the previous minimum requirement of 600 GB.},
  archive  = {J},
  author   = {Ruize Shi and Hong Huang and Xue Lin and Kehan Yin and Wei Zhou and Hai Jin},
  doi      = {10.1109/TBDATA.2024.3428331},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {748-760},
  title    = {Efficient learning for billion-scale heterogeneous information networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph convolutional networks with collaborative feature
fusion for sequential recommendation. <em>IEEE Transactions on Big
Data</em>, <em>11</em>(2), 735–747. (<a
href="https://doi.org/10.1109/TBDATA.2024.3426355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sequential recommendation seeks to understand user preferences based on their past actions and predict future interactions with items. Recently, several techniques for sequential recommendation have emerged, primarily leveraging graph convolutional networks (GCNs) for their ability to model relationships effectively. However, real-world scenarios often involve sparse interactions, where early and recent short-term preferences play distinct roles in the recommendation process. Consequently, vanilla GCNs struggle to effectively capture the explicit correlations between these early and recent short-term preferences. To address these challenges, we introduce a novel approach termed Graph Convolutional Networks with Collaborative Feature Fusion (COFF). Specifically, our method addresses the issue by initially dividing each user interaction sequence into two segments. We then construct two separate graphs for these segments, aiming to capture the user&#39;s early and recent short-term preferences independently. To obtain robust prediction, we employ multiple GCNs in a collaborative distillation manner, incorporating a feature fusion module to establish connections between the early and recent short-term preferences. This approach enables a more precise representation of user preferences. Experimental evaluations conducted on five popular sequential recommendation datasets demonstrate that our COFF model outperforms recent state-of-the-art methods in terms of recommendation accuracy.},
  archive  = {J},
  author   = {Jianping Gou and Youhui Cheng and Yibing Zhan and Baosheng Yu and Weihua Ou and Yi Zhang},
  doi      = {10.1109/TBDATA.2024.3426355},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {735-747},
  title    = {Graph convolutional networks with collaborative feature fusion for sequential recommendation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust joint graph learning for multi-view clustering.
<em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 722–734. (<a
href="https://doi.org/10.1109/TBDATA.2024.3426277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In real-world applications, multi-view datasets often comprise diverse data sources or views, inevitably accompanied by noise. However, most existing graph-based multi-view clustering methods utilize fixed graph similarity matrices to handle noisy multi-view data, necessitating additional clustering steps for obtaining the final clustering. This paper proposes a Robust Joint Graph learning for Multi-view Clustering (RJGMC) based on $ \ell _{1}$-norm to address these problems. RJGMC integrates the learning processes of the graph similarity matrix and the unified graph matrix to improve mutual reinforcement between these graph matrices. Simultaneously, employing the $ \ell _{1}$-norm to generate the unified graph matrix enhances the algorithm&#39;s robustness. A rank constraint is imposed on the graph Laplacian matrix of the unified graph matrix, where clustering can be divided directly without additional processing. In addition, we also introduce a method for automatically assigning optimal weights to each view. The optimization of this objective function employs an alternating optimization approach. Experimental results on synthetic and real-world datasets demonstrate that the proposed method outperforms other state-of-the-art techniques regarding clustering performance and robustness.},
  archive  = {J},
  author   = {Yanfang He and Umi Kalsom Yusof},
  doi      = {10.1109/TBDATA.2024.3426277},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {722-734},
  title    = {Robust joint graph learning for multi-view clustering},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Worker similarity-based label completion for crowdsourcing.
<em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 710–721. (<a
href="https://doi.org/10.1109/TBDATA.2024.3426310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In real-world crowdsourcing scenarios, it is a common phenomenon that each worker only annotates a few instances, resulting in a significantly sparse crowdsourcing label matrix. Consequently, only a small number of workers influence the inferred integrated label of each instance, which may weaken the performance of label integration algorithms. To address this problem, we propose a novel label completion algorithm called Worker Similarity-based Label Completion (WSLC). WSLC is grounded on the assumption that workers with similar cognitive abilities will annotate similar labels on the same instances. Specifically, we first construct a data set for each worker that includes all instances annotated by this worker and learn a feature vector for each worker. Then, we define a metric based on cosine similarity to estimate worker similarity based on the learned feature vectors. Finally, we complete the labels for each worker on unannotated instances based on the worker similarity and the annotations of similar workers. The experimental results on one real-world and 34 simulated crowdsourced data sets consistently show that WSLC effectively addresses the problem of the sparse crowdsourcing label matrix and enhances the integration accuracies of label integration algorithms.},
  archive  = {J},
  author   = {Xue Wu and Liangxiao Jiang and Wenjun Zhang and Chaoqun Li},
  doi      = {10.1109/TBDATA.2024.3426310},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {710-721},
  title    = {Worker similarity-based label completion for crowdsourcing},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denoising implicit feedback for graph collaborative
filtering via causal intervention. <em>IEEE Transactions on Big
Data</em>, <em>11</em>(2), 696–709. (<a
href="https://doi.org/10.1109/TBDATA.2024.3423727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The performance of graph collaborative filtering (GCF) models could be affected by noisy user-item interactions. Existing studies on data denoising either ignore the nature of noise in implicit feedback or seldom consider the long-tail distribution of historical interaction data. For the first challenge, we analyze the role of noise from a causal perspective: noise is an unobservable confounder. Therefore, we use the instrumental variable for causal intervention without requiring confounder observation. For the second challenge, we consider degree distribution of nodes in the course of causal intervention. And then we propose a model named causal graph collaborative filtering (CausalGCF) to denoise implicit feedback for GCF. Specifically, we design a degree augmentation strategy as the instrumental variable. First, we divide nodes into head and tail nodes according to their degree. Then, we purify the interactions of the head nodes and enrich those of the tail nodes based on similarity. We perform degree augmentation strategy from the user and item sides to obtain two different graph structures, which are trained together with self-supervised learning. Empirical studies on four real and four synthetic datasets demonstrate the effectiveness of CausalGCF, which is more robust against noisy interactions in implicit feedback than the baselines.},
  archive  = {J},
  author   = {Huiting Liu and Huaxiu Zhang and Peipei Li and Peng Zhao and Xindong Wu},
  doi      = {10.1109/TBDATA.2024.3423727},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {696-709},
  title    = {Denoising implicit feedback for graph collaborative filtering via causal intervention},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual graph convolutional networks for social network
alignment. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
684–695. (<a href="https://doi.org/10.1109/TBDATA.2024.3423699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social network alignment aims to discover the potential correspondence between users across different social platforms. Recent advances in graph representation learning have brought a new upsurge to network alignment. Most existing representation-based methods extract local structural information of social networks from users’ neighborhoods, but the global structural information has not been fully exploited. Therefore, this manuscript proposes a dual graph convolutional networks-based method (DualNA) for social network alignment, which combines user representation learning and user alignment in a unified framework. Specifically, we design dual graph convolutional networks as feature extractors to capture the local and global structural information of social networks, and apply a two-part constraint mechanism, including reconstruction loss and contrastive loss, to jointly optimize the graph representation learning process. As a result, the learned user representations can not only preserve the local and global features of original networks, but also be distinguishable and suitable for the downstream task of social network alignment. Extensive experiments on three real-world datasets show that our proposed method outperforms all baselines. The ablation studies further illustrate the rationality and effectiveness of our method.},
  archive  = {J},
  author   = {Xiaoyu Guo and Yan Liu and Daofu Gong and Fenlin Liu},
  doi      = {10.1109/TBDATA.2024.3423699},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {684-695},
  title    = {Dual graph convolutional networks for social network alignment},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minerva: Decentralized collaborative query processing over
InterPlanetary file system. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 669–683. (<a
href="https://doi.org/10.1109/TBDATA.2024.3423729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data silos create barriers to accessing and utilizing data dispersed over networks. Directly sharing data easily suffers from the long downloading time, the single point failure and the untraceable data usage. In this paper, we present Minerva, a peer-to-peer cross-cluster data query system based on the InterPlanetary File System (IPFS). Minerva makes use of the distributed Hash table (DHT) lookup to pinpoint the locations that store content chunks. We theoretically model the DHT query delay and introduce a fat Merkle tree structure as well as the DHT caching to reduce it. We design the query plan for read and write operations on top of Apache Drill that enables the collaborative query with decentralized workers. We conduct comprehensive experiments on Minerva, and the results show that Minerva achieves up to $2.08 \times$ query performance acceleration compared to the original IPFS data query, and can complete data analysis queries on the Internet-like environments within an average latency of 0.615 second. With a collaborative query, Minerva could perform up to $1.39 \times$ performance acceleration than the centralized query with raw data shipment.},
  archive  = {J},
  author   = {Zhiyi Yao and Bowen Ding and Qianlan Bai and Yuedong Xu},
  doi      = {10.1109/TBDATA.2024.3423729},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {669-683},
  title    = {Minerva: Decentralized collaborative query processing over InterPlanetary file system},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence of federated learning algorithms without
data similarity. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
659–668. (<a href="https://doi.org/10.1109/TBDATA.2024.3423693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data similarity assumptions have traditionally been relied upon to understand the convergence behaviors of federated learning methods. Unfortunately, this approach often demands fine-tuning step sizes based on the level of data similarity. When data similarity is low, these small step sizes result in an unacceptably slow convergence speed for federated methods. In this paper, we present a novel and unified framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions. Our analysis centers on an inequality that captures the influence of step sizes on algorithmic convergence performance. By applying our theorems to well-known federated algorithms, we derive precise expressions for three widely used step size schedules: fixed, diminishing, and step-decay step sizes, which are independent of data similarity conditions. Finally, we conduct comprehensive evaluations of the performance of these federated learning algorithms, employing the proposed step size strategies to train deep neural network models on benchmark datasets under varying data similarity conditions. Our findings demonstrate significant improvements in convergence speed and overall performance, marking a substantial advancement in federated learning research.},
  archive  = {J},
  author   = {Ali Beikmohammadi and Sarit Khirirat and Sindri Magnússon},
  doi      = {10.1109/TBDATA.2024.3423693},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {659-668},
  title    = {On the convergence of federated learning algorithms without data similarity},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable federated disentangling network for non-IID domain
feature. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
648–658. (<a href="https://doi.org/10.1109/TBDATA.2024.3423694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated Learning (FL), as an efficient decentralized distributed learning approach, enables multiple institutions to collaboratively train a model without sharing their local data. Despite its advantages, the performance of FL models is substantially impacted by the domain feature shift arising from different acquisition devices/clients. Moreover, existing FL methods often prioritize accuracy without considering reliability factors such as confidence or uncertainty, leading to unreliable predictions in safety-critical applications. Thus, our goal is to enhance FL performance by addressing non-domain feature issues and ensuring model reliability. In this study, we introduce a novel approach named RFedDis (Reliable Federated Disentangling Network). RFedDis leverages feature disentangling to capture a global domain-invariant cross-client representation while preserving local client-specific feature learning. Additionally, we incorporate an uncertainty-aware decision fusion mechanism to effectively integrate the decoupled features. This ensures dynamic integration at the evidence level, producing reliable predictions accompanied by estimated uncertainties. Therefore, RFedDis is the FL approach to combine evidential uncertainty with feature disentangling, enhancing both performance and reliability in handling non-IID domain features. Extensive experimental results demonstrate that RFedDis outperforms other state-of-the-art FL approaches, providing outstanding performance coupled with a high degree of reliability.},
  archive  = {J},
  author   = {Meng Wang and Kai Yu and Chun-Mei Feng and Yiming Qian and Ke Zou and Lianyu Wang and Rick Siow Mong Goh and Xinxing Xu and Yong Liu and Huazhu Fu},
  doi      = {10.1109/TBDATA.2024.3423694},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {648-658},
  title    = {Reliable federated disentangling network for non-IID domain feature},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCLCSE: Dynamic curriculum learning based contrastive
learning of sentence embeddings. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 635–647. (<a
href="https://doi.org/10.1109/TBDATA.2024.3423650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, Contrastive Learning (CL) has made impressive progress in natural language processing, especially in sentence representation learning. Plenty of data augmentation methods have been proposed for the generation of positive samples. However, due to the highly abstract nature of natural language, these augmentations cannot maintain the quality of generated positive samples, e.g., too easy or hard samples. To this end, we propose to improve the quality of positive examples from a data arrangement perspective and develop a novel model-agnostic approach: Dynamic Curriculum Learning based Contrastive Sentence Embedding framework (DCLCSE) for sentence embeddings. Specifically, we propose to incorporate a curriculum learning strategy to control the positive example usage. At the early learning stage, easy samples are selected to optimize the CL-based model. As the model&#39;s capability increases, we gradually select harder samples for model training, ensuring the learning efficiency of the model. Furthermore, we design a novel difficulty measurement module to calculate the difficulty of generated positives, in which the model&#39;s capability is considered for the accurate sample difficulty measurement. Based on this, we develop multiple arrangement strategies to facilitate the model learning process based on learned difficulties. Finally, extensive experiments over multiple representative models demonstrate the superiority of DCLCSE. As a byproduct, we have released the codes to facilitate other researchers.},
  archive  = {J},
  author   = {Chang Liu and Dacao Zhang and Meng Wang},
  doi      = {10.1109/TBDATA.2024.3423650},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {635-647},
  title    = {DCLCSE: Dynamic curriculum learning based contrastive learning of sentence embeddings},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive superpixel segmentation with non-uniform seed
initialization. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
620–634. (<a href="https://doi.org/10.1109/TBDATA.2024.3423719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Superpixel segmentation is a powerful image pre-processing tool in computer vision applications. However, fewer superpixel segmentation methods consider automatically determining the number of initial superpixels. Focusing on high-precision and connectivity, we propose a superpixel segmentation algorithm with non-uniform seed initialization. The proposed algorithm can adaptively determine the number and the position of initial seeds, and is robust to the segmentation of small objects and slender regions. First, we propose a seed initialization scheme based on the side of the circumscribed rectangle of the small object and interval boundary gradient. To enhance the regularity of superpixels, we equally added seeds for grids with sparse seed distribution. Second, we construct a weighted distance measure with search region and feature constraints, which reduces the computational complexity and enhances the precision of pixel label assignment. Finally, we quantify the disconnected regions are present in abundance, and propose a post-processing method based on the area of predefined small objects. The proposed method can significantly improve the connectivity and regularity of the generated superpixels. Extensive experiments on the widely-used BSDS and CamVid datasets demonstrate that the non-uniform seed initialization is effective, and the performance of the proposed superpixel segmentation is favorably compared with the state-of-the-art methods.},
  archive  = {J},
  author   = {Xinlin Xie and Jing Fan and Xinying Xu and Gang Xie},
  doi      = {10.1109/TBDATA.2024.3423719},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {620-634},
  title    = {Adaptive superpixel segmentation with non-uniform seed initialization},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGE: Age-gender effect on faculty career progression in
american universities. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 606–619. (<a
href="https://doi.org/10.1109/TBDATA.2024.3423726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study was undertaken to examine the impact of age and gender on faculty career progression in academia and to identify key performance indicators leading to attaining promotion. To explore any evidence of age-gender effect on faculty career progression, gender compositions, promotion rates, and appointment lengths at the assistant and associate professor levels are investigated. Furthermore, the underlying factors influencing faculty performance evaluation decisions are analyzed using the commercial data provided by Academic Analytics, LLC, which comprises the scholarly records of 336 793 faculty members from 472 Ph.D.-granting universities in the United States during 2011-2020. Various machine learning techniques, including ensemble learning and association rule mining, are performed to determine the important features that provide the most significant insights into academic career growth. Our results indicate strong evidence of age-gender effect on faculty career advancement and underscore the significance of journal article and citation counts for career progression in higher education.},
  archive  = {J},
  author   = {H. Rahmani and Anthony J. Olejniczak and Gary R. Weckman},
  doi      = {10.1109/TBDATA.2024.3423726},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {606-619},
  title    = {AGE: Age-gender effect on faculty career progression in american universities},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DBNetVizor: Visual analysis of dynamic basketball player
networks. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
591–605. (<a href="https://doi.org/10.1109/TBDATA.2024.3423721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Visual analysis has been increasingly integrated into the exploration of temporal networks, as visualization methods have the capability to present time-varying attributes and relationships of entities in an easy-to-read manner. Visualization techniques have been employed in a variety of dynamic network datasets, including social media networks, academic citation networks, and financial transaction networks. However, effectively visualizing dynamic basketball player network data, which consists of numerical networks, intensive timestamps, and subtle changes, remains a challenge for analysts. To address this issue, we propose a snapshot extraction algorithm that involves human-in-the-loop methodology to help users divide a series of networks into hierarchical snapshots for subsequent network analysis tasks, such as node exploration and network pattern analysis. Furthermore, we design and implement a prototype system, called DBNetVizor, for dynamic basketball player network data visualization. DBNetVizor integrates a graphical user interface to help users extract snapshots visually and interactively, as well as multiple linked visualization charts to display macro- and micro-level information of dynamic basketball player network data. To demonstrate the usability and efficiency of our proposed methods, we present two case studies based on dynamic basketball player network data in a competition. Additionally, we conduct an evaluation and receive positive feedback.},
  archive  = {J},
  author   = {Baofeng Chang and Guodao Sun and Sujia Zhu and Qi Jiang and Wang Xia and Jingwei Tang and Ronghua Liang},
  doi      = {10.1109/TBDATA.2024.3423721},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {591-605},
  title    = {DBNetVizor: Visual analysis of dynamic basketball player networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbolic knowledge reasoning on hyper-relational knowledge
graphs. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 578–590.
(<a href="https://doi.org/10.1109/TBDATA.2024.3423670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge reasoning has been widely researched in knowledge graphs (KGs), but there has been relatively less research on hyper-relational KGs, which also plays an important role in downstream tasks. Existing reasoning methods on hyper-relational KGs are based on representation learning. Though this approach is effective, it lacks interpretability and ignores the graph structure information. In this paper, we make the first attempt at symbolic reasoning on hyper-relational KGs. We introduce rule extraction methods based on both individual facts and paths, and propose a rule-based symbolic reasoning approach, HyperPath. This approach is simple and interpretable, it can serve as a baseline model for symbolic reasoning in hyper-relational KGs. We provide experimental results on almost all datasets, including five large-scale datasets and seven sub-datasets of them. Experiments show that the expressive power of the proposed model is similar to simple neural networks like convolutional networks, but not as advanced as more complex networks such as Transformer and graph convolutional networks, which is consistent with the performance of symbolic methods on KGs. Furthermore, we also analyze the impact of rule length and hyperparameters on the model&#39;s performance, which can provide insights for future research in hypergraph symbolic reasoning.},
  archive  = {J},
  author   = {Zikang Wang and Linjing Li and Daniel Dajun Zeng},
  doi      = {10.1109/TBDATA.2024.3423670},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {578-590},
  title    = {Symbolic knowledge reasoning on hyper-relational knowledge graphs},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HEART: Historically information embedding and subspace
re-weighting transformer-based tracking. <em>IEEE Transactions on Big
Data</em>, <em>11</em>(2), 566–577. (<a
href="https://doi.org/10.1109/TBDATA.2024.3423672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transformers-based trackers offer significant potential for integrating semantic interdependence between template and search features in tracking tasks. Transformers possess inherent capabilities for processing long sequences and extracting correlations within them. Several researchers have explored the feasibility of incorporating Transformers to model continuously changing search areas in tracking tasks. However, their approach has substantially increased the computational cost of an already resource-intensive Transformer. Additionally, existing Transformers-based trackers rely solely on mechanically employing multi-head attention to obtain representations in different subspaces, without any inherent bias. To address these challenges, we propose HEART (Historical Information Embedding And Subspace Re-weighting Tracker). Our method embeds historical information into the queries in a lightweight and Markovian manner to extract discriminative attention maps for robust tracking. Furthermore, we develop a multi-head attention distribution mechanism to retrieve the most promising subspace weights for tracking tasks. HEART has demonstrated its effectiveness on five datasets, including OTB-100, LaSOT, UAV123, TrackingNet, and GOT-10k.},
  archive  = {J},
  author   = {Tianpeng Liu and Jing Li and Amin Beheshti and Jia Wu and Jun Chang and Beihang Song and Lezhi Lian},
  doi      = {10.1109/TBDATA.2024.3423672},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {566-577},
  title    = {HEART: Historically information embedding and subspace re-weighting transformer-based tracking},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep cross-modal hashing with ranking learning for noisy
labels. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 553–565.
(<a href="https://doi.org/10.1109/TBDATA.2024.3423704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep hashing technology has recently become an essential tool for cross-modal retrieval on large-scale datasets. However, their performances heavily depend on accurate annotations to train the hashing model. In real applications, we usually only obtain low-quality label annotations owing to labor and time consumption limitations. To mitigate the performance degradation caused by noisy labels, in this paper, we propose a robust deep hashing method, called deep hashing with ranking learning (DHRL), for cross-modal retrieval. The proposed DHRL method consists of a refined semantic concept alignment module and a ranking-swapping module. In this first module, we adopt two transformers to perform the semantic alignment tasks between different modalities on a set of refined concepts, and then convert them into hash codes to reduce heterogeneous differences between multimodalities. The second module first identifies the noisy labels in the training set and ranks them according to ranking loss. Then it swaps the ranking information of different modal network branches. Unlike existing robust hashing methods for assuming noise distribution, our proposed DHRL method requires no prior assumptions for the input data. Extensive experiments on three benchmark datasets have shown that our proposed DHRL method has stronger advantages over other state-of-the-art hashing methods.},
  archive  = {J},
  author   = {Zhenqiu Shu and Yibing Bai and Kailing Yong and Zhengtao Yu},
  doi      = {10.1109/TBDATA.2024.3423704},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {553-565},
  title    = {Deep cross-modal hashing with ranking learning for noisy labels},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGAMF: Sparse gated attention-based multimodal fusion method
for fake news detection. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 540–552. (<a
href="https://doi.org/10.1109/TBDATA.2024.3414341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the field of fake news detection, deep learning techniques have emerged as superior performers in recent years. Nevertheless, the majority of these studies primarily concentrate on either unimodal feature-based methodologies or image-text multimodal fusion techniques, with a minimal focus on the fusion of unstructured text features and structured tabular features. In this study, we present SGAMF, a Sparse Gated Attention-based Multimodal Fusion strategy, designed to amalgamate text features and auxiliary features for the purpose of fake news identification. Compared with traditional multimodal fusion methods, SGAMF can effectively balance accuracy and inference time while selecting the most important features. A novel sparse-gated-attention mechanism has been proposed which instigates a shift in text representation conditioned on auxiliary features, thereby selectively filtering out non-essential features. We have further put forward an enhanced ALBERT for the encoding of text features, capable of balancing efficiency and accuracy. To corroborate our methodology, we have developed a multimodal COVID-19 fake news detection dataset. Comprehensive experimental outcomes on this dataset substantiate that our proposed SGAMF delivers competitive performance in comparison to the existing state-of-the-art techniques in terms of accuracy and $F_{1}$ score.},
  archive  = {J},
  author   = {Pengfei Du and Yali Gao and Linghui Li and Xiaoyong Li},
  doi      = {10.1109/TBDATA.2024.3414341},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {540-552},
  title    = {SGAMF: Sparse gated attention-based multimodal fusion method for fake news detection},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Update selective parameters: Federated machine unlearning
based on model explanation. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 524–539. (<a
href="https://doi.org/10.1109/TBDATA.2024.3409947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning is a promising privacy-preserving paradigm for distributed machine learning. In this context, there is sometimes a need for a specialized process called machine unlearning, which is required when the effect of some specific training samples needs to be removed from a learning model due to privacy, security, usability, and/or legislative factors. However, problems arise when current centralized unlearning methods are applied to existing federated learning, in which the server aims to remove all information about a class from the global model. Centralized unlearning usually focuses on simple models or is premised on the ability to access all training data at a central node. However, training data cannot be accessed on the server under the federated learning paradigm, conflicting with the requirements of the centralized unlearning process. Additionally, there are high computation and communication costs associated with accessing clients’ data, especially in scenarios involving numerous clients or complex global models. To address these concerns, we propose a more effective and efficient federated unlearning scheme based on the concept of model explanation. Model explanation involves understanding deep networks and individual channel importance, so that this understanding can be used to determine which model channels are critical for classes that need to be unlearned. We select the most influential channels within an already-trained model for the data that need to be unlearned and fine-tune only influential channels to remove the contribution made by those data. In this way, we can simultaneously avoid huge consumption costs and ensure that the unlearned model maintains good performance. Experiments with different training models on various datasets demonstrate the effectiveness of the proposed approach.},
  archive  = {J},
  author   = {Heng Xu and Tianqing Zhu and Lefeng Zhang and Wanlei Zhou and Philip S. Yu},
  doi      = {10.1109/TBDATA.2024.3409947},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {524-539},
  title    = {Update selective parameters: Federated machine unlearning based on model explanation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reinforcement learning framework for n-ary document-level
relation extraction. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 512–523. (<a
href="https://doi.org/10.1109/TBDATA.2024.3410099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge Bases (KBs) have become more complex because some facts in KBs include more than two entities. The construction and completion of these KBs require a new relation extraction task to retrieve complex facts from the text. To address this issue, we present a new N-ary Document-Level relation extraction task that involves extracting relations that 1) include an arbitrary number of entities, and 2) can span multiple sentences within a document. This new task requires inferring relation labels and entity completeness, i.e., whether the entities in the document are (insufficient to describe the relation. We propose a reinforcement learning-based relation classifier training framework that can adapt most existing binary document-level relation extractors to this task. Extensive experimental evaluation demonstrates that our proposed framework is effective in reducing the impact of noise introduced by distant supervision or unrelated sentences in the document.},
  archive  = {J},
  author   = {Chenhan Yuan and Ryan Rossi and Andrew Katz and Hoda Eldardiry},
  doi      = {10.1109/TBDATA.2024.3410099},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {512-523},
  title    = {A reinforcement learning framework for N-ary document-level relation extraction},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate network alignment via consistency in node
evolution. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
499–511. (<a href="https://doi.org/10.1109/TBDATA.2024.3407543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Network alignment, which integrates multiple network resources by identifying anchor nodes that exist in different networks, is beneficial for conducting comprehensive network analysis. Although there have been many studies on network alignment, most of them are limited to static scenarios and only can achieve acceptable top-$\alpha$ ($\alpha &amp;gt; 10$) results. In the absence of considering dynamic changes in networks, accurate network alignment (i.e., top-1 result) faces two problems: 1) Missing information: focusing solely on aligning networks at a specific time leads to low top-1 performance due to the lack of information from other time periods; 2) Confusing information: ignoring temporal information and focusing on aligning networks across the entire time span leads to low top-1 performance due to inability to distinguish the neighborhood nodes of anchor nodes. In this paper, we propose a dynamic network alignment method, which aims to achieve better top-1 alignment results with consider changing network structures over time. Towards this end, we learn the representations of nodes in the changing network structure with time, and preserve the consistency of anchor node pairs during the time-evolution process. First, we employ a Structure-Time-aware module to capture network dynamics while preserving network structure and learning node representations that incorporate temporal information. Second, we ensure the global and local consistency of anchor node pairs over time by utilizing linear and similarity functions, respectively. Finally, we determine whether two nodes are anchor node pairs by maintaining consistency between global, local, and node representations. Experimental results obtained from real-world datasets demonstrate that the proposed model achieves performance comparable to several state-of-the-art methods.},
  archive  = {J},
  author   = {Qiyao Peng and Yinghui Wang and Pengfei Jiao and Huaming Wu and Wenjun Wang},
  doi      = {10.1109/TBDATA.2024.3407543},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {499-511},
  title    = {Accurate network alignment via consistency in node evolution},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised projected sample selector for active learning.
<em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 485–498. (<a
href="https://doi.org/10.1109/TBDATA.2024.3407545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Active learning, as a technique, aims to effectively label specific data points while operating within a designated query budget. Nevertheless, the majority of unsupervised active learning algorithms are based on shallow linear representation and lack sufficient interpretability. Furthermore, certain diversity-based methods face challenges in selecting samples that adequately represent the entire data distribution. Inspired by these reasons, in this paper, we propose an unsupervised active learning method on orthogonal projections to construct a deep neural network model. By optimizing the orthogonal projection process, we establish the connection between projection and active learning, consequently enhancing the interpretability of the proposed method. The proposed method can efficiently project the feature space onto a spanned subspace, deriving an indicator matrix while calculating the projection loss. Moreover, we consider the redundancy among samples to ensure both data point diversity and enhancement of clustering-based algorithms. Through extensive comparative experiments on six public datasets, the results demonstrate that the proposed method can effectively select more informative and representative samples and improve performance by up to 11%.},
  archive  = {J},
  author   = {Yueyang Pi and Yiqing Shi and Shide Du and Yang Huang and Shiping Wang},
  doi      = {10.1109/TBDATA.2024.3407545},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {485-498},
  title    = {Unsupervised projected sample selector for active learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). System identification with fourier transformation for
long-term time series forecasting. <em>IEEE Transactions on Big
Data</em>, <em>11</em>(2), 474–484. (<a
href="https://doi.org/10.1109/TBDATA.2024.3407568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Time-series prediction has drawn considerable attention during the past decades fueled by the emerging advances of deep learning methods. However, most neural network based methods fail in extracting the hidden mechanism of the targeted physical system. To overcome these shortcomings, an interpretable sparse system identification method without any prior knowledge is proposed in this study. This method adopts the Fourier transform to reduces the irrelevant items in the dictionary matrix, instead of indiscriminate usage of polynomial functions in most system identification methods. It shows an visible system representation and greatly reduces computing cost. With the adoption of $l_{1}$ norm in regularizing the parameter matrix, a sparse description of the system model can be achieved. Moreover, three data sets including the water conservancy data, global temperature data and financial data are used to test the performance of the proposed method. Although no prior knowledge was known about the physical background, experimental results show that our method can achieve long-term prediction regardless of the noise and incompleteness in the original data more accurately than the widely-used baseline data-driven methods. This study may provide some insight into time-series prediction investigations, and suggests that a white-box system identification method may extract the easily overlooked yet inherent periodical features and may beat neural-network based black-box methods on long-term prediction tasks.},
  archive  = {J},
  author   = {Xiaoyi Liu and Duxin Chen and Wenjia Wei and Xia Zhu and Hao Shi and Wenwu Yu},
  doi      = {10.1109/TBDATA.2024.3407568},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {474-484},
  title    = {System identification with fourier transformation for long-term time series forecasting},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imperceptible physical attack against face recognition
systems via LED illumination modulation. <em>IEEE Transactions on Big
Data</em>, <em>11</em>(2), 461–473. (<a
href="https://doi.org/10.1109/TBDATA.2024.3403377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although face recognition starts to play an important role in our daily life, we need to pay attention that data-driven face recognition vision systems are vulnerable to adversarial attacks. However, current digital adversarial attacks and physical adversarial attacks both have drawbacks, with the former ones impractical and the latter one conspicuous, high-computational and low-executable. To address the issues, we propose a practical, executable, stealthy and low computational adversarial attack based on LED illumination modulation. To fool the systems, the proposed attack generates physically imperceptible luminance changes to human eyes through fast intensity modulation of scene LED illumination and uses the rolling shutter effect of CMOS image sensors in face recognition systems to implant luminance information perturbation to the captured face images. In summary, we present a denial-of-service (DoS) attack for face detection and an evasion attack for face verification. We also evaluate their effectiveness against well-known face detection models, Dlib, MTCNN and RetinaFace, and face verification models, Dlib, FaceNet, and ArcFace. The extensive physical experiments show that the success rates of DoS attacks against face detection models reach 97.67$\%$, 100$\%$, and 100$\%$, respectively, and the success rates of evasion attacks against all face verification models reach 100$\%$.},
  archive  = {J},
  author   = {Junbin Fang and Canjian Jiang and You Jiang and Puxi Lin and Zhaojie Chen and Yujing Sun and Siu-Ming Yiu and Zoe L. Jiang},
  doi      = {10.1109/TBDATA.2024.3403377},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {461-473},
  title    = {Imperceptible physical attack against face recognition systems via LED illumination modulation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). <span class="math inline">$\tt {zkFL}$</span>&lt;Mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi
mathvariant=“monospace”&gt;zkFL&lt;/mml:mi&gt;&lt;/mml:math&gt;:
Zero-knowledge proof-based gradient aggregation for federated learning.
<em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 447–460. (<a
href="https://doi.org/10.1109/TBDATA.2024.3403370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. FL can be a scalable machine learning solution in Big Data scenarios. Traditional FL relies on the trust assumption of the central aggregator, which forms cohorts of clients honestly. However, a malicious aggregator, in reality, could abandon and replace the client&#39;s training models, or insert fake clients, to manipulate the final training results. In this work, we introduce $ \tt {zkFL}$, which leverages zero-knowledge proofs to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator provides a proof per round, demonstrating to the clients that the aggregator executes the intended behavior faithfully. To further reduce the verification cost of clients, we use blockchain to handle the proof in a zero-knowledge way, where miners (i.e., the participants validating and maintaining the blockchain data) can verify the proof without knowing the clients’ local and aggregated models. The theoretical analysis and empirical results show that $ \tt {zkFL}$ achieves better security and privacy than traditional FL, without modifying the underlying FL network structure or heavily compromising the training speed.},
  archive  = {J},
  author   = {Zhipeng Wang and Nanqing Dong and Jiahao Sun and William Knottenbelt and Yike Guo},
  doi      = {10.1109/TBDATA.2024.3403370},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {447-460},
  title    = {$ \tt {zkFL}$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi mathvariant=&quot;monospace&quot;&gt;zkFL&lt;/mml:mi&gt;&lt;/mml:math&gt;: zero-knowledge proof-based gradient aggregation for federated learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An open dataset of cyber asset graphs for cybercrime
research. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
438–446. (<a href="https://doi.org/10.1109/TBDATA.2024.3403371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cybercrime poses a severe threat to the entire Internet ecosystem. Various cyber assets, such as domain name, IP address, and security certificate, are staple infrastructures of cybercrime. A cyber asset graph (CAG) is a collection of closely related cyber assets held by a cybercrime gang to support online criminal activities. Analyzing CAGs provides rich data insights for cybercrime investigation and governance. This paper introduces an open dataset of CAGs comprised of 2.37 million nodes with eight types of cyber assets and 3.28 million edges with eleven types of relations. This paper introduces the dataset construction process, applied areas, and the experience of using the dataset in the ChinaVis Data Challenge 2022. This dataset contains numerous CAGs of cybercrime gangs in the real world, which is the first open dataset of CAGs for cybercrime research. This dataset can also support the development of other application-oriented areas, such as cyber asset management and cyber-physical-social system, and various graph-related research areas, such as graph theory, graph mining, and graph visualization.},
  archive  = {J},
  author   = {Xin Zhao and Shaolong Li and Ying Zhao and Shuowen Fu and Yunpeng Chen and Fangfang Zhou and Xin Huang and Yuwei Li and Zhuo Chen},
  doi      = {10.1109/TBDATA.2024.3403371},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {438-446},
  title    = {An open dataset of cyber asset graphs for cybercrime research},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-evidence based fact verification via a confidential
graph neural network. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 426–437. (<a
href="https://doi.org/10.1109/TBDATA.2024.3403382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fact verification tasks aim to identify the integrity of textual contents according to the truthful corpus. Existing fact verification models usually build a fully connected reasoning graph, which regards claim-evidence pairs as nodes and connects them with edges. They employ the graph to propagate the semantics of the nodes. Nevertheless, the noisy nodes usually propagate their semantics via the edges of the reasoning graph, which misleads the semantic representations of other nodes and amplifies the noise signals. To mitigate the propagation of noisy semantic information, we introduce a Confidential Graph Attention Network (CO-GAT), which proposes a node masking mechanism for modeling the nodes. Specifically, CO-GAT calculates the node confidence score by estimating the relevance between the claim and evidence pieces. Then, the node masking mechanism uses the node confidence scores to control the noise information flow from the vanilla node to the other graph nodes. CO-GAT achieves a 73.59% FEVER score on the FEVER dataset and shows the generalization ability by broadening the effectiveness to the science-specific domain.},
  archive  = {J},
  author   = {Yuqing Lan and Zhenghao Liu and Yu Gu and Xiaoyuan Yi and Xiaohua Li and Liner Yang and Ge Yu},
  doi      = {10.1109/TBDATA.2024.3403382},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {426-437},
  title    = {Multi-evidence based fact verification via a confidential graph neural network},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based anomaly detection approach for air
pollution assessment. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 414–425. (<a
href="https://doi.org/10.1109/TBDATA.2024.3403392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Environmental air pollution has become a cause of global concern due to its adverse effects. Unusually high concentration of air pollutants can be regarded as an anomaly indicating certain air quality problems. This paper presents a deep learning based anomaly detection approach to identify anomalous concentrations of five different air pollutants: Carbon Monoxide ($CO$), Ozone ($O_{3}$), Nitogen Oxide ($NO_{X}$) and Particulate Matters ($PM_{2.5}$, $PM_{10}$) in a real-life environmental dataset. The collected data is multivariate in nature containing hourly generated information about several air pollutants and atmospheric parameters from a non-polluted city of India. The proposed framework contains a Bidirectional Long Short Term Memory (Bi-LSTM) based predictor model with self-attention to capture the normal pollutant levels in the time series dataset. The predictor model is responsible for predicting the value at the next timestamp, corresponding to a given window of the time series data. A subsequent anomaly detector is utilized to identify the anomalous pollutant levels based on the predictions of predictor model. Anomalies detected by the proposed framework are utilized to analyze the correlation of temporal and atmospheric parameters with the anomalous concentration levels. Experimental results illustrate the predominance of proposed approach over existing approaches towards air pollution assessment.},
  archive  = {J},
  author   = {Anindita Borah},
  doi      = {10.1109/TBDATA.2024.3403392},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {414-425},
  title    = {Deep learning based anomaly detection approach for air pollution assessment},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective personalized search with heterogeneous graph based
hawkes process. <em>IEEE Transactions on Big Data</em>, <em>11</em>(2),
402–413. (<a href="https://doi.org/10.1109/TBDATA.2024.3399606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Personalized search aims at re-ranking search results with reference to users’ background information. The state-of-the-art personalized search methods often consider both the short-term search interests from current session behaviors and the long-term search interests from previous session behaviors. However, sessions in real-world search scenarios are usually very short, and a large number of sessions contain only one query, which makes it difficult to model short-term search interests. Intuitively, apart from current session behaviors, some recent historical session behaviors could also contribute to the current search interests, and the influence of these behaviors typically decays over time. Based on this intuition, we propose a novel heterogeneous graph based Hawkes process to improve the effectiveness of personalized search. Specifically, we first construct a heterogeneous graph to model multiple relations between users, queries, and documents. Then, we propose a heterogeneous graph neural network based algorithm to encode the representations of users’ historical search behaviors. After that, we develop a multivariate Hawkes process to capture the influence of historical search behaviors on the current search intent. Our approach can dynamically model the influence of historical behaviors in a continuous time space. Thus, both the current session behaviors and the historical session behaviors can be utilized to characterize a more accurate current search intent. We evaluate our method using three real-life datasets, and the results show that our approach significantly outperforms the state-of-the-art methods in terms of several widely-used precision metrics.},
  archive  = {J},
  author   = {Xiang Wu and Hongchao Qin and Rong-Hua Li and Yuchen Meng and Huanzhong Duan and Yanxiong Lu and Yujing Gao and Fusheng Jin and Guoren Wang},
  doi      = {10.1109/TBDATA.2024.3399606},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {402-413},
  title    = {Effective personalized search with heterogeneous graph based hawkes process},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep umbra: A generative approach for sunlight access
computation in urban spaces. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 388–401. (<a
href="https://doi.org/10.1109/TBDATA.2024.3382964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sunlight and shadow play critical roles in how urban spaces are utilized, thrive, and grow. While access to sunlight is essential to the success of urban environments, shadows can provide shaded places to stay during the hot seasons, mitigate heat island effect, and increase pedestrian comfort levels. Properly quantifying sunlight access and shadows in large urban environments is key in tackling some of the important challenges facing cities today. In this paper, we propose Deep Umbra, a novel computational framework that enables the quantification of sunlight access and shadows at a global scale. Our framework is based on a conditional generative adversarial network that considers the physical form of cities to compute high-resolution spatial information of accumulated sunlight access for the different seasons of the year. We use data from seven different cities to train our model, and show, through an extensive set of experiments, its low overall RMSE (below 0.1) as well as its extensibility to cities that were not part of the training set. Additionally, we contribute a set of case studies and a comprehensive dataset with sunlight access information for more than 100 cities across six continents of the world.},
  archive  = {J},
  author   = {Kazi Shahrukh Omar and Gustavo Moreira and Daniel Hodczak and Maryam Hosseini and Nicola Colaninno and Marcos Lage and Fabio Miranda},
  doi      = {10.1109/TBDATA.2024.3382964},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {388-401},
  title    = {Deep umbra: A generative approach for sunlight access computation in urban spaces},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal transformer network for weather forecasting.
<em>IEEE Transactions on Big Data</em>, <em>11</em>(2), 372–387. (<a
href="https://doi.org/10.1109/TBDATA.2024.3378061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spatio-temporal neural networks have been successfully applied to weather forecasting tasks recently. The key notion is to learn spatio-temporal features concurrently from spatial and temporal dependencies. Existing methods are mainly based on local smoothness assumptions where the features are learned by accumulating information in local spatio-temporal regions. However, the weather conditions in a certain spatio-temporal region are usually influenced by global meteorological changes and long-range historical weather conditions. Therefore, these methods that ignore the large-scale spatio-temporal effects can hardly learn effective features. In this paper, we propose a novel spatio-temporal Transformer network in weather forecasting to address the above challenges. The main idea is to leverage the Transformer architecture to carefully capture the multi-scale spatial and long-range temporal information in weather data. First, we propose to combine the global and local position encodings based on absolute geographic locations and relative geodesic distances and insert them into the spatial Transformer to extract the multi-scale spatial information in meteorological graphs. Then, we further capture the long-range temporal dependencies by a temporal Transformer where the attention mechanism is used to improve the representation ability and scalability of the models. Extensive experiments over real weather datasets demonstrate the effectiveness of our framework.},
  archive  = {J},
  author   = {Junzhong Ji and Jing He and Minglong Lei and Muhua Wang and Wei Tang},
  doi      = {10.1109/TBDATA.2024.3378061},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {372-387},
  title    = {Spatio-temporal transformer network for weather forecasting},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly-supervised cross-domain segmentation of electron
microscopy with sparse point annotation. <em>IEEE Transactions on Big
Data</em>, <em>11</em>(2), 359–371. (<a
href="https://doi.org/10.1109/TBDATA.2024.3378062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate segmentation of organelle instances from electron microscopy (EM) images plays an essential role in many neuroscience researches. However, practical scenarios usually suffer from high annotation costs, label scarcity, and large domain diversity. While unsupervised domain adaptation (UDA) that assumes no annotation effort on the target data is promising to alleviate these challenges, its performance on complicated segmentation tasks is still far from practical usage. To address these issues, we investigate a highly annotation-efficient weak supervision, which assumes only sparse center-points on a small subset of object instances in the target training images. To achieve accurate segmentation with partial point annotations, we introduce instance counting and center detection as auxiliary tasks and design a multitask learning framework to leverage correlations among the counting, detection, and segmentation, which are all tasks with partial or no supervision. Building upon the different domain-invariances of the three tasks, we enforce counting estimation with a novel soft consistency loss as a global prior for center detection, which further guides the per-pixel segmentation. To further compensate for annotation sparsity, we develop a cross-position cut-and-paste for label augmentation and an entropy-based pseudo-label selection. The experimental results highlight that, by simply using extremely weak annotation, e.g., 15% sparse points, for model training, the proposed model is capable of significantly outperforming UDA methods and produces comparable performance as the supervised counterpart. The high robustness of our model shown in the validations and the low requirement of expert knowledge for sparse point annotation further improve the potential application value of our model.},
  archive  = {J},
  author   = {Dafei Qiu and Shan Xiong and Jiajin Yi and Jialin Peng},
  doi      = {10.1109/TBDATA.2024.3378062},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {359-371},
  title    = {Weakly-supervised cross-domain segmentation of electron microscopy with sparse point annotation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint multi-feature information entity alignment for
cross-lingual temporal knowledge graph with BERT. <em>IEEE Transactions
on Big Data</em>, <em>11</em>(2), 345–358. (<a
href="https://doi.org/10.1109/TBDATA.2024.3378113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Entity alignment is the most critical technology of knowledge fusion, which aims to identify and align entities that exist in different knowledge graphs (KGs) but represent the same real-world objects. Temporal knowledge graphs (TKGs) extend static triples into quadruples by introducing temporal information, which is more in line with the dynamic changes of the real world and is often used to enhance the performance of various applications. However, the existing entity alignment methods mainly focus on traditional KGs, ignoring the temporal information contained in TKGs may lead to misalignment between some similar entities. The latest method enhances the performance of entity alignment by learning temporal information embedding, but it does not make full use of the advantages of temporal information. In this paper, we propose a new Entity Alignment method for Cross-lingual TKG with BERT (EACTB). EACTB uses the BERT model of multi-language training to learn the semantic relevance of entity description. For temporal information, we propose a new and more efficient method to calculate the similarity of temporal information. EACTB uses graph convolution network (GCN) to embed structural information. In addition, iterative method is used to deal with the problem of insufficient training datasets. Experimental results on three cross-lingual TKGs datasets show that EACTB is significantly superior to existing methods.},
  archive  = {J},
  author   = {Luyi Bai and Xiuting Song and Lin Zhu},
  doi      = {10.1109/TBDATA.2024.3378113},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {345-358},
  title    = {Joint multi-feature information entity alignment for cross-lingual temporal knowledge graph with BERT},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConDTC: Contrastive deep trajectory clustering for
fine-grained mobility pattern mining. <em>IEEE Transactions on Big
Data</em>, <em>11</em>(2), 333–344. (<a
href="https://doi.org/10.1109/TBDATA.2024.3362195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Trajectory clustering is a cornerstone task in the field of trajectory mining. With the proliferation of deep learning, deep trajectory clustering has been widely researched to mine mobility patterns from massive unlabeled trajectories. Nevertheless, existing methods mostly ignore trajectories’ temporal regularities, which are essential for mining fine-grained mobility patterns for applications including traveling group identification, transportation mode discovering, social security emergency, etc. To fill this gap, we propose ConDTC, a contrastive deep trajectory clustering method targeting for fine-grained mobility pattern mining. Specifically, we first design a spatial-temporal trajectory representation learning method which can capture both spatial and temporal regularities of trajectories synchronously. The proposed trajectory representation model can be used as a pre-trained model to serve various downstream trajectory mining tasks. Then, we construct a contrastive trajectory clustering module which optimizes trajectory representations and clustering performance simultaneously. Experimental results on three datasets validate that ConDTC can identify fine-grained mobility patterns by clustering trajectories with similar spatial-temporal mobility patterns together while separating those with different mobility patterns apart. Actually, ConDTC outperforms all state-of-the-art competitors substantially in terms of effectiveness, efficiency and robustness.},
  archive  = {J},
  author   = {Junjun Si and Jin Yang and Yang Xiang and Li Li and Bo Tu and Rongqing Zhang},
  doi      = {10.1109/TBDATA.2024.3362195},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {333-344},
  title    = {ConDTC: Contrastive deep trajectory clustering for fine-grained mobility pattern mining},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on truth discovery: Concepts, methods,
applications, and opportunities. <em>IEEE Transactions on Big Data</em>,
<em>11</em>(2), 314–332. (<a
href="https://doi.org/10.1109/TBDATA.2024.3423677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the era of data information explosion, there are different observations on an object (e.g., the height of the Himalayas) from different sources on the web, social sensing, crowd sensing, and data sensing applications. Observations from different sources on an object can conflict with each other due to errors, missing records, typos, outdated data, etc. How to discover truth facts for objects from various sources is essential and urgent. In this paper, we aim to deliver a comprehensive and exhaustive survey on truth discovery problems from the perspectives of concepts, methods, applications, and opportunities. We first systematically review and compare problems from objects, sources, and observations. Based on these problem properties, different methods are analyzed and compared in depth from observation with single or multiple values, independent or dependent sources, static or dynamic sources, and supervised or unsupervised learning, followed by the surveyed applications in various scenarios. For future studies in truth discovery fields, we summarize the code sources and datasets used in above methods. Finally, we point out the potential challenges and opportunities on truth discovery, with the goal of shedding light and promoting further investigation in this area.},
  archive  = {J},
  author   = {Shuang Wang and He Zhang and Quan Z. Sheng and Xiaoping Li and Zhu Sun and Taotao Cai and Wei Emma Zhang and Jian Yang and Qing Gao},
  doi      = {10.1109/TBDATA.2024.3423677},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {314-332},
  title    = {A survey on truth discovery: Concepts, methods, applications, and opportunities},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
