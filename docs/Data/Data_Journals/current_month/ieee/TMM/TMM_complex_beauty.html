<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm---87">TMM - 87</h2>
<ul>
<li><details>
<summary>
(2025). Enhancing 3D human pose estimation amidst severe occlusion
with dual transformer fusion. <em>TMM</em>, <em>27</em>, 1617–1624. (<a
href="https://doi.org/10.1109/TMM.2024.3521755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of 3D Human Pose Estimation from monocular videos, the presence of diverse occlusion types presents a formidable challenge. Prior research has made progress by harnessing spatial and temporal cues to infer 3D poses from 2D joint observations. This paper introduces a Dual Transformer Fusion (DTF) algorithm, a novel approach to obtain a holistic 3D pose estimation, even in the presence of severe occlusions. Confronting the issue of occlusion-induced missing joint data, we propose a temporal interpolation-based occlusion guidance mechanism. To enable precise 3D Human Pose Estimation, our approach leverages the innovative DTF architecture, which first generates a pair of intermediate views. Each intermediate-view undergoes spatial refinement through a self-refinement schema. Subsequently, these intermediate-views are fused to yield the final 3D human pose estimation. The entire system is end-to-end trainable. Through extensive experiments conducted on the Human3.6 M and MPI-INF-3DHP datasets, our method&#39;s performance is rigorously evaluated. Notably, our approach outperforms existing state-of-the-art methods on both datasets, yielding substantial improvements.},
  archive      = {J_TMM},
  author       = {Mehwish Ghafoor and Arif Mahmood and Muhammad Bilal},
  doi          = {10.1109/TMM.2024.3521755},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1617-1624},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing 3D human pose estimation amidst severe occlusion with dual transformer fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated hallucination translation and source-free
regularization adaptation in decentralized domain adaptation for foggy
scene understanding. <em>TMM</em>, <em>27</em>, 1601–1616. (<a
href="https://doi.org/10.1109/TMM.2024.3521711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic foggy scene understanding (SFSU) emerges a challenging task under out-of-domain distribution (OD) due to uncertain cognition caused by degraded visibility. With the strong assumption of data centralization, unsupervised domain adaptation (UDA) reduces vulnerability under OD scenario. Whereas, enlarged domain gap and growing privacy concern heavily challenge conventional UDA. Motivated by gap decomposition and data decentralization, we establish a decentralized domain adaptation (DDA) framework called Translate thEn Adapt (abbr. TEA) for privacy preservation. Our highlights lie in. (1) Regarding federated hallucination translation, a Disentanglement and Contrastive-learning based Generative Adversarial Network (abbr. DisCoGAN) is proposed to impose contrastive prior and disentangle latent space in cycle-consistent translation. To yield domain hallucination, client minimizes cross-entropy of local classifier but maximizes entropy of global model to train translator. (2) Regarding source-free regularization adaptation, a Prototypical-knowledge based Regularization Adaptation (abbr. ProRA) is presented to align joint distribution in output space. Soft adversarial learning relaxes binary label to rectify inter-domain discrepancy and inner-domain divergence. Structure clustering and entropy minimization drive intra-class features closer and inter-class features apart. Extensive experiments exhibit efficacy of our TEA which achieves 55.26% or 46.25% mIoU in adaptation from GTA5 to Foggy Cityscapes or Foggy Zurich, outperforming other DDA methods for SFSU.},
  archive      = {J_TMM},
  author       = {Xiating Jin and Jiajun Bu and Zhi Yu and Hui Zhang and Yaonan Wang},
  doi          = {10.1109/TMM.2024.3521711},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1601-1616},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Federated hallucination translation and source-free regularization adaptation in decentralized domain adaptation for foggy scene understanding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards neural codec-empowered 360<span
class="math inline"><sup>∘</sup></span> video streaming: A
saliency-aided synergistic approach. <em>TMM</em>, <em>27</em>,
1588–1600. (<a href="https://doi.org/10.1109/TMM.2024.3521770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networked 360$^\circ$ video has become increasingly popular. Despite the immersive experience for users, its sheer data volume, even with the latest H.266 coding and viewport adaptation, remains a significant challenge to today&#39;s networks. Recent studies have shown that integrating deep learning into video coding can significantly enhance compression efficiency, providing new opportunities for high-quality video streaming. In this work, we conduct a comprehensive analysis of the potential and issues in applying neural codecs to 360$^\circ$ video streaming. We accordingly present $\mathsf {NETA}$, a synergistic streaming scheme that merges neural compression with traditional coding techniques, seamlessly implemented within an edge intelligence framework. To address the non-trivial challenges in the short viewport prediction window and time-varying viewing directions, we propose implicit-explicit buffer-based prefetching grounded in content visual saliency and bitrate adaptation with smart model switching around viewports. A novel Lyapunov-guided deep reinforcement learning algorithm is developed to maximize user experience and ensure long-term system stability. We further discuss the concerns towards practical development and deployment and have built a working prototype that verifies $\mathsf {NETA}$’s excellent performance. For instance, it achieves a 27% increment in viewing quality, a 90% reduction in rebuffering time, and a 64% decrease in quality variation on average, compared to state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Jianxin Shi and Miao Zhang and Linfeng Shen and Jiangchuan Liu and Lingjun Pu and Jingdong Xu},
  doi          = {10.1109/TMM.2024.3521770},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1588-1600},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards neural codec-empowered 360$^\circ$ video streaming: A saliency-aided synergistic approach},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-aware pre-selected neural rendering for light
field reconstruction. <em>TMM</em>, <em>27</em>, 1574–1587. (<a
href="https://doi.org/10.1109/TMM.2024.3521784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As densely-sampled Light Field (LF) images are beneficial to many applications, LF reconstruction becomes an important technology in related fields. Recently, neural rendering shows great potential in reconstruction tasks. However, volume rendering in existing methods needs to sample many points on the whole camera ray or epipolar line, which is time-consuming. In this paper, specifically for LF images with regular angular sampling, we propose a novel Structure-Aware Pre-Selected neural rendering framework for LF reconstruction. Instead of sampling on the whole epipolar line, we propose to sample on several specific positions, which are estimated using the color and inherent scene structure information explored in the regular angular sampled LF images. Sampling only a few points that closely match the target pixel, the feature of the target pixel is quickly rendered with high-quality. Finally, we fuse the features and decode them in the view dimension to obtain the final target view. Experiments show that the proposed method outperforms the state-of-the-art LF reconstruction methods in both qualitative and quantitative comparisons across various tasks. Our method also surpasses the most existing methods in terms of speed. Moreover, without any retraining or fine-tuning, the performance of our method with no-per-scene optimization is even better than the methods with per-scene optimization.},
  archive      = {J_TMM},
  author       = {Song Chang and Youfang Lin and Shuo Zhang},
  doi          = {10.1109/TMM.2024.3521784},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1574-1587},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Structure-aware pre-selected neural rendering for light field reconstruction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image shooting parameter-guided cascade image retouching
network: Think like an artist. <em>TMM</em>, <em>27</em>, 1566–1573. (<a
href="https://doi.org/10.1109/TMM.2024.3521779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photo retouching aims to adjust the hue, luminance, contrast, and saturation of the image to make it more human and aesthetically desirable. Based on researches on image imaging process and artists&#39; retouching processes, we propose three improvements to existing automatic retouching methods. Firstly, in the past retouching methods, all the imaging conditions in EXIF were ignored. According to this, we design a simple module to introduce these imaging conditions into a network called ECM (EXIF Condition Module). This module can improve the performance of several existing auto-retouching methods with only a small parameter cost. Additionally, artists&#39; operations also were ignored. By investigating artists&#39; operations in retouching, we propose a two-stage network that brightens images first and then enriches them in the chrominance plane to mimic artists. Finally, we find that there is a color imbalance in the existing retouching dataset, thus, hue palette loss is designed to resolve the imbalance and make the image more vibrant. Experimental results show that our method is effective on the benchmark MIT-Adobe FiveK dataset and PPR10 K dataset, and achieves SOTA performance in both quantitative and qualitative evaluation.},
  archive      = {J_TMM},
  author       = {Hailong Ma and Sibo Feng and Xi Xiao and Chenyu Dong and Xingyue Cheng},
  doi          = {10.1109/TMM.2024.3521779},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1566-1573},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image shooting parameter-guided cascade image retouching network: Think like an artist},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual stream relation learning network for image-text
retrieval. <em>TMM</em>, <em>27</em>, 1551–1565. (<a
href="https://doi.org/10.1109/TMM.2024.3521736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-text retrieval has made remarkable achievements through the development of feature extraction networks and model architectures. However, almost all region feature-based methods face two serious problems when modeling modality interactions. First, region features are prone to feature entanglement in the feature extraction stage, making it difficult to accurately reason complex intra-model relations between visual objects. Second, region features lack rich contextual information, background, and object details, making it difficult to achieve precise inter-modal alignment with textual information. In this paper, we propose a novel Dual Stream Relation Learning Network (DSRLN) to jointly solve these issues with two key components: a Geometry-sensitive Interactive Self-Attention (GISA) module and a Dual Information Fusion (DIF) module. Specifically, GISA extends the vanilla self-attention network from two aspects to better model the intrinsic relationships between different regions, thereby improving high-level visual-semantic reasoning ability. DIF uses grid features as an additional visual information source, and achieves deeper and complex fusion between the two types of features through a masked cross-attention module and an adaptive gate fusion module, which can capture comprehensive visual information to learn more precise inter-modal alignment. Besides, our method also learns a more comprehensive hierarchical correspondence between images and sentences through local and global alignment. Experimental results on two public datasets, i.e., Flickr30K and MS-COCO, fully demonstrate the superiority and effectiveness of our model.},
  archive      = {J_TMM},
  author       = {Dongqing Wu and Huihui Li and Cang Gu and Lei Guo and Hang Liu},
  doi          = {10.1109/TMM.2024.3521736},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1551-1565},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual stream relation learning network for image-text retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Between/within view information completing for tensorial
incomplete multi-view clustering. <em>TMM</em>, <em>27</em>, 1538–1550.
(<a href="https://doi.org/10.1109/TMM.2024.3521771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-view Clustering (IMvC) receives increasing attention due to its effectiveness in solving data-missing problems. With the information loss in incomplete situations, the core of IMvC needs to consider effectively overcoming the challenge of missing views, that is, exploring the underlying correlations from available data and recovering the missing information. However, most existing IMvC methods overemphasize the recovery-first principle with integrating the existing data from different views while neglecting the influence of view consistency in IMvC task together with valuable within view information. In this paper, a novel Between/Within View Information Completing for Tensorial Incomplete Multi-view Clustering (BWIC-TIMC) has been proposed, in which between/within view information is jointly exploited for effectively completing the missing views. Specifically, the proposed method designs a dual tensor constraint module, which focuses on simultaneously exploring the view-specific correlations of incomplete views and enforcing the between view consistency across different views. With the dual tensor constraint, between/within view information can be effectively integrated for completing missing views for IMvC task. Furthermore, in order to balance different contributions of multiple views and alleviate the problem of feature degeneration, BWIC-TIMC implements an adaptive fusion graph learning strategy for consensus representation learning. Extensive comparative experiments with the-state-of-art baselines can demonstrate the effectiveness of BWIC-TIMC.},
  archive      = {J_TMM},
  author       = {Mingze Yao and Huibing Wang and Yawei Chen and Xianping Fu},
  doi          = {10.1109/TMM.2024.3521771},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1538-1550},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Between/Within view information completing for tensorial incomplete multi-view clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive knowledge distillation from different levels of
teachers for online action detection. <em>TMM</em>, <em>27</em>,
1526–1537. (<a href="https://doi.org/10.1109/TMM.2024.3521772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore the problem of Online Action Detection (OAD), where the task is to detect ongoing actions from streaming videos without access to video frames in the future. Existing methods achieve good detection performance by capturing long-range temporal structures. However, a major challenge of this task is to detect actions at a specific time that arrive with insufficient observations. In this work, we utilize the additional future frames available at the training phase and propose a novel Knowledge Distillation (KD) framework for OAD, where a teacher network looks at more frames from the future and the student network distills the knowledge from the teacher for detecting ongoing actions from the observation up to the current frames. Usually, the conventional KD regards a high-level teacher network (i.e., the network after the last training iteration) to guide the student network throughout all training iterations, which may result in poor distillation due to the large knowledge gap between the high-level teacher and the student network at early training iterations. To remedy this, we propose a novel progressive knowledge distillation from different levels of teachers (PKD-DLT) for OAD, where in addition to a high-level teacher, we also generate several low- and middle-level teachers, and progressively transfer the knowledge (in the order of low- to high-level) to the student network throughout training iterations, for effective distillation. Evaluated on two challenging datasets THUMOS14 and TVSeries, we validate that our PKD-DLT is an effective teacher-student learning paradigm, which can be a plug-in to improve the performance of the existing OAD models and achieve a state-of-the-art.},
  archive      = {J_TMM},
  author       = {Md Moniruzzaman and Zhaozheng Yin},
  doi          = {10.1109/TMM.2024.3521772},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1526-1537},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive knowledge distillation from different levels of teachers for online action detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater image enhancement with cascaded contrastive
learning. <em>TMM</em>, <em>27</em>, 1512–1525. (<a
href="https://doi.org/10.1109/TMM.2024.3521739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is a highly challenging task due to the complexity of underwater environment and the diversity of underwater image degradation. Due to the application of deep learning, current UIE methods have made significant progress. Most of the existing deep learning-based UIE methods follow a single-stage network which cannot effectively address the diverse degradations simultaneously. In this paper, we propose to address this issue by designing a two-stage deep learning framework and taking advantage of cascaded contrastive learning to guide the network training of each stage. The proposed method is called CCL-Net in short. Specifically, the proposed CCL-Net involves two cascaded stages, i.e., a color correction stage tailored to the color deviation issue and a haze removal stage tailored to improve the visibility and contrast of underwater images. To guarantee the underwater image can be progressively enhanced, we also apply contrastive loss as an additional constraint to guide the training of each stage. In the first stage, the raw underwater images are used as negative samples for building the first contrastive loss, ensuring the enhanced results of the first color correction stage are better than the original inputs. While in the second stage, the enhanced results rather than the raw underwater images of the first color correction stage are used as the negative samples for building the second contrastive loss, thus ensuring the final enhanced results of the second haze removal stage are better than the intermediate color corrected results. Extensive experiments on multiple benchmark datasets demonstrate that our CCL-Net can achieve superior performance compared to many state-of-the-art methods. In addition, a series of ablation studies also verify the effectiveness of each key component involved in the proposed CCL-Net.},
  archive      = {J_TMM},
  author       = {Yi Liu and Qiuping Jiang and Xinyi Wang and Ting Luo and Jingchun Zhou},
  doi          = {10.1109/TMM.2024.3521739},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1512-1525},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Underwater image enhancement with cascaded contrastive learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incomplete multi-view clustering with paired and balanced
dynamic anchor learning. <em>TMM</em>, <em>27</em>, 1486–1497. (<a
href="https://doi.org/10.1109/TMM.2024.3521789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to static anchor selection, existing dynamic anchor learning could automatically learn more flexible anchors to improve the performance of large-scale multi-view clustering. Despite improving the flexibility of anchors, these methods do not pay sufficient attention to the alignment and fairness of learned anchors. Specifically, within each cluster, the positions and quantities of cross-view anchors may not align, or even anchor absence in some clusters, leading to severe anchor misalignment and imbalance issues. These issues result in inaccurate graph fusion and a reduction in clustering performance. Besides, in practical applications, missing information caused by sensor malfunctions or data losses could further exacerbate anchor misalignment and imbalance. To overcome such challenges, a novel Incomplete Multi-view Clustering with Paired and Balanced Dynamic Anchor Learning (PBDAL) is proposed to ensure the alignment and fairness of anchors. Unlike existing unsupervised anchor learning, we first design a paired and balanced dynamic anchor learning scheme to supervise dynamic anchors to be aligned and fair in each cluster. Meanwhile, we develop an enhanced bipartite graph tensor learning to refine paired and balanced anchors. Our superiority, effectiveness, and efficiency are all validated by performing extensive experiments on multiple public datasets.},
  archive      = {J_TMM},
  author       = {Xingfeng Li and Yuangang Pan and Yuan Sun and Quansen Sun and Yinghui Sun and Ivor W. Tsang and Zhenwen Ren},
  doi          = {10.1109/TMM.2024.3521789},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1486-1497},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Incomplete multi-view clustering with paired and balanced dynamic anchor learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked attribute description embedding for cloth-changing
person re-identification. <em>TMM</em>, <em>27</em>, 1475–1485. (<a
href="https://doi.org/10.1109/TMM.2024.3521730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloth-changing person re-identification (CC-ReID) aims to match persons who change clothes over long periods. The key challenge in CC-ReID is to extract cloth-irrelated features, such as face, hairstyle, body shape, and gait. Current research mainly focuses on modeling body shape using multi-modal biological features (such as silhouettes and sketches). However, it does not fully leverage the personal description information hidden in the original RGB image. Considering that there are certain attribute descriptions that remain unchanged after the changing of cloth, we propose a Masked Attribute Description Embedding (MADE) method that unifies personal visual appearance and attribute description for CC-ReID. Specifically, handling variable cloth-sensitive information, such as color and type, is challenging for effective modeling. To address this, we mask the clothes type and color information (upper body type, upper body color, lower body type, and lower body color) in the personal attribute description extracted through an attribute detection model. The masked attribute description is then connected and embedded into Transformer blocks at various levels, fusing it with the low-level to high-level features of the image. This approach compels the model to discard cloth information. Experiments are conducted on several CC-ReID benchmarks, including PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADE effectively utilizes attribute description, enhancing cloth-changing person re-identification performance, and compares favorably with state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Chunlei Peng and Boyu Wang and Decheng Liu and Nannan Wang and Ruimin Hu and Xinbo Gao},
  doi          = {10.1109/TMM.2024.3521730},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1475-1485},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Masked attribute description embedding for cloth-changing person re-identification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augment one with others: Generalizing to unforeseen
variations for visual tracking. <em>TMM</em>, <em>27</em>, 1461–1474.
(<a href="https://doi.org/10.1109/TMM.2024.3521842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unforeseen appearance variation is a challenging factor for visual tracking. This paper provides a novel solution from semantic data augmentation, which facilitates offline training of trackers for better generalization. We utilize existing samples to obtain knowledge to augment another in terms of diversity and hardness. First, we propose that the similarity matching space in Siamese-like models has class-agnostic transferability. Based on this, we design the Latent Augmentation (LaAug) to transfer relevant variations and suppress irrelevant ones between training similarity embeddings of different classes. Thus the model can generalize across a more diverse semantic distribution. Then, we propose the Semantic Interaction Mix (SIMix), which interacts moments between different feature samples to contaminate structure and texture attributes and retain other semantic attributes. SIMix simulates the occlusion and complements the training distribution with hard cases. The mixed features with adversarial perturbations can empirically enable the model against external environmental disturbances. Experiments on six challenging benchmarks demonstrate that three representative tracking models, i.e., SiamBAN, TransT and OSTrack, can be consistently improved by incorporating the proposed methods without extra parameters and inference cost.},
  archive      = {J_TMM},
  author       = {Jinpu Zhang and Ziwen Li and Ruonan Wei and Yuehuan Wang},
  doi          = {10.1109/TMM.2024.3521842},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1461-1474},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Augment one with others: Generalizing to unforeseen variations for visual tracking},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Part-level relationship learning for fine-grained few-shot
image classification. <em>TMM</em>, <em>27</em>, 1448–1460. (<a
href="https://doi.org/10.1109/TMM.2024.3521792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, an increasing number of few-shot image classification methods have been proposed, and they aim at seeking a learning paradigm to train a high-performance classification model with limited labeled samples. However, the neglect of part-level relationships causes few-shot methods to struggle to distinguish between closely similar subcategories, which makes it difficult for them to solve the fine-grained image classification problem. To tackle this challenging task, this paper proposes a fine-grained few-shot image classification method that exploits both intra-part and inter-part relationships among different samples. To establish comprehensive relationships, we first extract multiple discriminative descriptors from the input image, representing its different parts. Then, we propose to define the metric spaces by interpolating intra-part relationships, which can help the model adaptively find clear boundaries for these confusing classes. Finally, since the unlabeled image has high similarities to all classes, we project these similarities into a high-dimension space according to the inter-part relationship and interpolate a parameterized classifier to discover the subtle differences among these similar classes. To evaluate our proposed method, we conduct extensive experiments on various fine-grained datasets. Without any pre-train/fine-tuning process, our approach clearly outperforms previous few-shot learning methods, which demonstrates the effectiveness of our approach.},
  archive      = {J_TMM},
  author       = {Chuanming Wang and Huiyuan Fu and Peiye Liu and Huadong Ma},
  doi          = {10.1109/TMM.2024.3521792},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1448-1460},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Part-level relationship learning for fine-grained few-shot image classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VGNet: Multimodal feature extraction and fusion network for
3D CAD model retrieval. <em>TMM</em>, <em>27</em>, 1432–1447. (<a
href="https://doi.org/10.1109/TMM.2024.3521706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reuse of 3D CAD models is crucial for industrial manufacturing because it shortens development cycles and reduces costs. Significant progress has been made in deep learning-based 3D model retrievals. There are many representations for 3D models, among which the multi-view representation has demonstrated a superior retrieval performance. However, directly applying these 3D model retrieval approaches to 3D CAD model retrievals may result in issues such as the loss of the engineering semantic and structural information. In this paper, we find that multiple views and B-rep can complement each other. Therefore, we propose the view graph neural network (VGNet), which effectively combines multiple views and B-rep to accomplish 3D CAD model retrieval. More specifically, based on the characteristics of the regular shape of 3D CAD models, and the richness of the attribute information in the B-rep attribute graph, we separately design two feature extraction networks for each modality. Moreover, to explore the latent relationships between the multiple views and B-rep attribute graphs, a multi-head attention enhancement module is designed. Furthermore, the multimodal fusion module is adopted to make the joint representation of the 3D CAD models more discriminative by using a correlation loss function. Experiments are carried out on a real manufacturing 3D CAD dataset and a public dataset to validate the effectiveness of the proposed approach.},
  archive      = {J_TMM},
  author       = {Feiwei Qin and Gaoyang Zhan and Meie Fang and C. L. Philip Chen and Ping Li},
  doi          = {10.1109/TMM.2024.3521706},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1432-1447},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VGNet: Multimodal feature extraction and fusion network for 3D CAD model retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning local features by reinforcing spatial structure
information. <em>TMM</em>, <em>27</em>, 1420–1431. (<a
href="https://doi.org/10.1109/TMM.2024.3521777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based local feature extraction algorithms have advanced considerably in terms of robustness. While excelling at enhancing feature robustness, some outstanding algorithms tend to neglect discriminability—a crucial aspect in vision tasks. With the increase of deep learning convolutional layers, we observe an amplification of semantic information within images, accompanied by a diminishing presence of spatial structural information. This imbalance primarily contributes to the subpar feature discriminability. Therefore, this paper introduces a novel network framework aimed at imbuing feature descriptors with robustness and discriminative power by reinforcing spatial structural information. Our approach incorporates a spatial structure enhancement module into the network architecture, spanning from shallow to deep layers, ensuring the retention of rich structural information in deeper layers, thereby enhancing discriminability. Finally, we evaluate our method, demonstrating superior performance in visual localization and feature-matching tasks.},
  archive      = {J_TMM},
  author       = {Li Wang and Yunzhou Zhang and Fawei Ge and Wenjing Bai and Yifan Wang},
  doi          = {10.1109/TMM.2024.3521777},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1420-1431},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning local features by reinforcing spatial structure information},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving vision anomaly detection with the guidance of
language modality. <em>TMM</em>, <em>27</em>, 1410–1419. (<a
href="https://doi.org/10.1109/TMM.2024.3521813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen a surge of interest in anomaly detection. However, existing unsupervised anomaly detectors, particularly those for the vision modality, face significant challenges due to redundant information and sparse latent space. In contrast, anomaly detectors demonstrate superior performance in the language modality due to the unimodal nature of the data. This paper tackles the aforementioned challenges for vision modality from a multimodal point of view. Specifically, we propose Cross-modal Guidance (CMG), comprising of Cross-modal Entropy Reduction (CMER) and Cross-modal Linear Embedding (CMLE), to address the issues of redundant information and sparse latent space, respectively. CMER involves masking portions of the raw image and computing the matching score with the corresponding text. Essentially, CMER eliminates irrelevant pixels to direct the detector&#39;s focus towards critical content. To learn a more compact latent space for the vision anomaly detection, CMLE learns a correlation structure matrix from the language modality. Then, the acquired matrix compels the distribution of images to resemble that of texts in the latent space. Extensive experiments demonstrate the effectiveness of the proposed methods. Particularly, compared to the baseline that only utilizes images, the performance of CMG has been improved by 16.81%. Ablation experiments further confirm the synergy among the proposed CMER and CMLE, as each component depends on the other to achieve optimal performance.},
  archive      = {J_TMM},
  author       = {Dong Chen and Kaihang Pan and Guangyu Dai and Guoming Wang and Yueting Zhuang and Siliang Tang and Mingliang Xu},
  doi          = {10.1109/TMM.2024.3521813},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1410-1419},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving vision anomaly detection with the guidance of language modality},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-based freeform handwriting authentication with
energy-oriented self-supervised learning. <em>TMM</em>, <em>27</em>,
1397–1409. (<a href="https://doi.org/10.1109/TMM.2024.3521807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freeform handwriting authentication verifies a person&#39;s identity from their writing style and habits in messy handwriting data. This technique has gained widespread attention in recent years as a valuable tool for various fields, e.g., fraud prevention and cultural heritage protection. However, it still remains a challenging task in reality due to three reasons: (i) severe damage, (ii) complex high-dimensional features, and (iii) lack of supervision. To address these issues, we propose SherlockNet, an energy-oriented two-branch contrastive self-supervised learning framework for robust and fast freeform handwriting authentication. It consists of four stages: (i) pre-processing: converting manuscripts into energy distributions using a novel plug-and-play energy-oriented operator to eliminate the influence of noise; (ii) generalized pre-training: learning general representation through two-branch momentum-based adaptive contrastive learning with the energy distributions, which handles the high-dimensional features and spatial dependencies of handwriting; (iii) personalized fine-tuning: calibrating the learned knowledge using a small amount of labeled data from downstream tasks; and (iv) practical application: identifying individual handwriting from scrambled, missing, or forged data efficiently and conveniently. Considering the practicality, we construct EN-HA, a novel dataset that simulates data forgery and severe damage in real applications. Finally, we conduct extensive experiments on six benchmark datasets including our EN-HA, and the results prove the robustness and efficiency of SherlockNet.},
  archive      = {J_TMM},
  author       = {Jingyao Wang and Luntian Mou and Changwen Zheng and Wen Gao},
  doi          = {10.1109/TMM.2024.3521807},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1397-1409},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image-based freeform handwriting authentication with energy-oriented self-supervised learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discriminative anchor learning for efficient multi-view
clustering. <em>TMM</em>, <em>27</em>, 1386–1396. (<a
href="https://doi.org/10.1109/TMM.2024.3521743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering aims to study the complementary information across views and discover the underlying structure. For solving the relatively high computational cost for the existing approaches, works based on anchor have been presented recently. Even with acceptable clustering performance, these methods tend to map the original representation from multiple views into a fixed shared graph based on the original dataset. However, most studies ignore the discriminative property of the learned anchors, which ruin the representation capability of the built model. Moreover, the complementary information among anchors across views is neglected to be ensured by simply learning the shared anchor graph without considering the quality of view-specific anchors. In this paper, we propose discriminative anchor learning for multi-view clustering (DALMC) for handling the above issues. We learn discriminative view-specific feature representations according to the original dataset and build anchors from different views based on these representations, which increase the quality of the shared anchor graph. The discriminative feature learning and consensus anchor graph construction are integrated into a unified framework to improve each other for realizing the refinement. The optimal anchors from multiple views and the consensus anchor graph are learned with the orthogonal constraints. We give an iterative algorithm to deal with the formulated problem. Extensive experiments on different datasets show the effectiveness and efficiency of our method compared with other methods.},
  archive      = {J_TMM},
  author       = {Yalan Qin and Nan Pu and Hanzhou Wu and Nicu Sebe},
  doi          = {10.1109/TMM.2024.3521743},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1386-1396},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Discriminative anchor learning for efficient multi-view clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of ViT design and training in robustness to common
corruptions. <em>TMM</em>, <em>27</em>, 1374–1385. (<a
href="https://doi.org/10.1109/TMM.2024.3521721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformer (ViT) variants have made rapid advances on a variety of computer vision tasks. However, their performance on corrupted inputs, which are inevitable in realistic use cases due to variations in lighting and weather, has not been explored comprehensively. In this paper, we probe the robustness gap among ViT variants and ask how these modern architectural developments affect performance under common types of corruption. Through extensive and rigorous benchmarking, we demonstrate that simple architectural designs such as overlapping patch embedding and convolutional feed-forward networks can promote the robustness of ViTs. Moreover, since the de facto training of ViTs relies heavily on data augmentation, exactly which augmentation strategies make ViTs more robust is worth investigating. We survey the efficacy of previous methods and verify that adversarial noise training is powerful. In addition, we introduce a novel conditional method for generating dynamic augmentation parameters conditioned on input images, which offers state-of-the-art robustness to common corruptions.},
  archive      = {J_TMM},
  author       = {Rui Tian and Zuxuan Wu and Qi Dai and Micah Goldblum and Han Hu and Yu-Gang Jiang},
  doi          = {10.1109/TMM.2024.3521721},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1374-1385},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {The role of ViT design and training in robustness to common corruptions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing generalizable occlusion modeling for neural human
radiance field. <em>TMM</em>, <em>27</em>, 1362–1373. (<a
href="https://doi.org/10.1109/TMM.2024.3521787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizable human neural rendering aims to render the target views of the human body by leveraging source views and the skinned multi-person linear (SMPL) model. Despite exhibiting promising performance, the target views rendered by previous methods usually contain corrupted parts of the human body. Two primary challenges hinder high-quality human neural rendering. These challenges involve non-correspondences between 2D pixels and 3D SMPL vertices induced by self-occlusion of the human body and erroneous appearance predictions caused by occlusion between the source and target views. To solve these two challenges, we propose an advancing generalizable occlusion modeling method for the neural human radiance field, in which the hurdles from the self-occlusion of the human body and the occlusion between source and target views are explored and solved. Specifically, to alleviate the non-correspondence problem induced by self-occlusion, a geometry perception module is designed to obtain 3D geometric representations of SMPL vertices, enabling the prediction of accurate density values. Furthermore, a visibility aggregation module is designed to estimate the visibility maps with respect to different source views by utilizing the predicted density. Then, the complementary information among multiple source views is integrated with the support of the visibility maps in the visibility aggregation module, thus effectively addressing the occlusion between views. Experiments on the ZJU-MoCap and THUman datasets show that the proposed method achieves promising performance compared with the existing state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Bingzheng Liu and Jianjun Lei and Bo Peng and Zhe Zhang and Jie Zhu and Qingming Huang},
  doi          = {10.1109/TMM.2024.3521787},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1362-1373},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Advancing generalizable occlusion modeling for neural human radiance field},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EvCSLR: Event-guided continuous sign language recognition
and benchmark. <em>TMM</em>, <em>27</em>, 1349–1361. (<a
href="https://doi.org/10.1109/TMM.2024.3521750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical continuous sign language recognition (CSLR) suffers from some main challenges in real-world scenarios: accurate inter-frame movement trajectories may fail to be captured by traditional RGB cameras due to the motion blur, and valid information may be insufficient under low-illumination scenarios. In this paper, we for the first time leverage an event camera to overcome the above-mentioned challenges. Event cameras are bio-inspired vision sensors that could efficiently record high-speed sign language movements under low-illumination scenarios and capture human information while eliminating redundant background interference. To fully exploit the benefits of the event camera for CSLR, we propose a novel event-guided multi-modal CSLR framework, which could achieve significant performance under complex scenarios. Specifically, a time redundancy correction (TRCorr) module is proposed to rectify redundant information in the temporal sequences, directing the model to focus on distinctive features. A multi-modal cross-attention interaction (MCAI) module is proposed to facilitate information fusion between events and frame domains. Furthermore, we construct the first event-based CSLR dataset, named EvCSLR, which will be released as the first event-based CSLR benchmark. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on EvCSLR and PHOENIX-2014 T datasets.},
  archive      = {J_TMM},
  author       = {Yu Jiang and Yuehang Wang and Siqi Li and Yongji Zhang and Qianren Guo and Qi Chu and Yue Gao},
  doi          = {10.1109/TMM.2024.3521750},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1349-1361},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EvCSLR: Event-guided continuous sign language recognition and benchmark},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond subspace isolation: Many-to-many transformer for
light field image super-resolution. <em>TMM</em>, <em>27</em>,
1334–1348. (<a href="https://doi.org/10.1109/TMM.2024.3521795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective extraction of spatial-angular features plays a crucial role in light field image super-resolution (LFSR) tasks, and the introduction of convolution and Transformers leads to significant improvement in this area. Nevertheless, due to the large 4D data volume of light field images, many existing methods opted to decompose the data into a number of lower-dimensional subspaces and perform Transformers in each sub-space individually. As a side effect, these methods inadvertently restrict the self-attention mechanisms to a One-to-One scheme accessing only a limited subset of LF data, explicitly preventing comprehensive optimization on all spatial and angular cues. In this paper, we identify this limitation as subspace isolation and introduce a novel Many-to-Many Transformer (M2MT) to address it. M2MT aggregates angular information in the spatial subspace before performing the self-attention mechanism. It enables complete access to all information across all sub-aperture images (SAIs) in a light field image. Consequently, M2MT is enabled to comprehensively capture long-range correlation dependencies. With M2MT as the foundational component, we develop a simple yet effective M2MT network for LFSR. Our experimental results demonstrate that M2MT achieves state-of-the-art performance across various public datasets, and it offers a favorable balance between model performance and efficiency, yielding higher-quality LFSR results with substantially lower demand for memory and computation. We further conduct in-depth analysis using local attribution maps (LAM) to obtain visual interpretability, and the results validate that M2MT is empowered with a truly non-local context in both spatial and angular subspaces to mitigate subspace isolation and acquire effective spatial-angular representation.},
  archive      = {J_TMM},
  author       = {Zeke Zexi Hu and Xiaoming Chen and Vera Yuk Ying Chung and Yiran Shen},
  doi          = {10.1109/TMM.2024.3521795},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1334-1348},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Beyond subspace isolation: Many-to-many transformer for light field image super-resolution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised referring video object segmentation with
object-centric pseudo-guidance. <em>TMM</em>, <em>27</em>, 1320–1333.
(<a href="https://doi.org/10.1109/TMM.2024.3521741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring video object segmentation (RVOS) is an emerging task for multimodal video comprehension while the expensive annotating process of object masks restricts the scalability and diversity of RVOS datasets. To relax the dependency on expensive mask annotations and take advantage from large-scale partially annotated data, in this paper, we explore a novel extended RVOS task, namely weakly supervised referring video object segmentation (WRVOS), which employs multiple weak supervision sources, including object points and bounding boxes. Correspondingly, we propose a unified WRVOS framework. Specifically, an object-centric pseudo mask generation method is introduced to provide effective shape priors for the pseudo guidance of spatial object location. Then, a pseudo-guided optimization strategy is proposed to effectively optimize the object outlines in terms of spatial location and projection density with a multi-stage online learning strategy. Furthermore, a multimodal cross-frame level set evolution method is proposed to iteratively refine the object boundaries considering both temporal consistency and cross-modal interactions. Extensive experiments are conducted on four publicly available RVOS datasets, including A2D Sentences, J-HMDB Sentences, Ref-DAVIS, and Ref-YoutubeVOS. Performance comparison shows that the proposed method achieves state-of-the-art performance in both point-supervised and box-supervised settings.},
  archive      = {J_TMM},
  author       = {Weikang Wang and Yuting Su and Jing Liu and Wei Sun and Guangtao Zhai},
  doi          = {10.1109/TMM.2024.3521741},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1320-1333},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weakly supervised referring video object segmentation with object-centric pseudo-guidance},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ByteNet: Rethinking multimedia file fragment classification
through visual perspectives. <em>TMM</em>, <em>27</em>, 1305–1319. (<a
href="https://doi.org/10.1109/TMM.2024.3521830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia file fragment classification (MFFC) aims to identify file fragment types, e.g., image/video, audio, and text without system metadata. It is of vital importance in multimedia storage and communication. Existing MFFC methods typically treat fragments as 1D byte sequences and emphasize the relations between separate bytes (interbytes) for classification. However, the more informative relations inside bytes (intrabytes) are overlooked and seldom investigated. By looking inside bytes, the bit-level details of file fragments can be accessed, enabling a more accurate classification. Motivated by this, we first propose Byte2Image, a novel visual representation model that incorporates previously overlooked intrabyte information into file fragments and reinterprets these fragments as 2D grayscale images. This model involves a sliding byte window to reveal the intrabyte information and a rowwise stacking of intrabyte n-grams for embedding fragments into a 2D space. Thus, complex interbyte and intrabyte correlations can be mined simultaneously using powerful vision networks. Additionally, we propose an end-to-end dual-branch network ByteNet to enhance robust correlation mining and feature representation. ByteNet makes full use of the raw 1D byte sequence and the converted 2D image through a shallow byte branch feature extraction (BBFE) and a deep image branch feature extraction (IBFE) network. In particular, the BBFE, composed of a single fully-connected layer, adaptively recognizes the co-occurrence of several some specific bytes within the raw byte sequence, while the IBFE, built on a vision Transformer, effectively mines the complex interbyte and intrabyte correlations from the converted image. Experiments on the two representative benchmarks, including 14 cases, validate that our proposed method outperforms state-of-the-art approaches on different cases by up to 12.2%.},
  archive      = {J_TMM},
  author       = {Wenyang Liu and Kejun Wu and Tianyi Liu and Yi Wang and Kim-Hui Yap and Lap-Pui Chau},
  doi          = {10.1109/TMM.2024.3521830},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1305-1319},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ByteNet: Rethinking multimedia file fragment classification through visual perspectives},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification committee for active deep object detection.
<em>TMM</em>, <em>27</em>, 1277–1288. (<a
href="https://doi.org/10.1109/TMM.2024.3521778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In object detection, the cost of labeling is very high because it needs not only to confirm the categories of multiple objects in an image but also to determine the bounding boxes of each object accurately. Thus, integrating active learning into object detection will raise pretty positive significance. In this paper, we propose a classification committee for the active deep object detection method by introducing a discrepancy mechanism of multiple classifiers for samples&#39; selection when training object detectors. The model contains a main detector and a classification committee. The main detector denotes the target object detector trained from a labeled pool composed of the selected informative images. The role of the classification committee is to select the most informative images according to their uncertainty values from the view of classification, which is expected to focus more on the discrepancy and representative of instances. Specifically, they compute the uncertainty for a specified instance within the image by measuring its discrepancy output by the committee pre-trained via the proposed Maximum Classifiers Discrepancy Group Loss (MCDGL). The most informative images are finally determined by selecting the ones with many high-uncertainty instances. Besides, to mitigate the impact of interference instances, we design a Focusing on Positive Instances Loss (FPIL) to provide the committee the ability to automatically focus on the representative instances as well as precisely encode their discrepancies for the same instance. Experiments are conducted on Pascal VOC and COCO datasets versus some popular object detectors. And results show that our method outperforms the state-of-the-art active learning methods, which verifies the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Lei Zhao and Bo Li and Jixiang Jiang and Xingxing Wei},
  doi          = {10.1109/TMM.2024.3521778},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1277-1288},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Classification committee for active deep object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSDLF-k: A multimodal feature learning approach for
sentiment analysis in korean incorporating text and speech.
<em>TMM</em>, <em>27</em>, 1266–1276. (<a
href="https://doi.org/10.1109/TMM.2024.3521707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, sentiment analysis research has made significant improvements in addressing sentiment and subjectivity within textual content. The advent of multimodal deep learning techniques has further broadened this scope, enabling the integration of diverse modalities such as voice and image features alongside text. However, despite these advancements, the analysis of the Korean language remains challenging due to its inherently agglutinative nature and linguistic ambiguity, primarily examined at the sentence level. To effectively address this challenge, we propose a novel Multimodal Sentimental Deep Learning Framework for Korean (MSDLF-K), which can examine not only Korean text but also its associated speech. Our framework, MSDLF-K, integrates spectrograms and waveforms from Korean voice data with embedding vectors derived from script sentences, creating a unified multimodal representation. This approach facilitates the identification of both shared and unique features within the latent space, thereby offering valuable insights into their respective impacts on sentiment analysis performance. To validate the efficacy of MSDLF-K, we conducted a set of experiments using the emotion speech synthesis dataset. Our findings demonstrate that MSDLF-K achieves a remarkable accuracy of 79.0% in valence and 81.7% in arousal for emotion classification, metrics previously unexplored in the literature. Furthermore, empirical analysis reveals the significant influence of multimodal representations, encompassing both text and voice, on enhancing emotion analysis performance. In summary, our study not only presents a pioneering solution for sentiment analysis in the Korean language but also underscores the importance of incorporating multimodal approaches for more comprehensive and accurate sentiment analysis across diverse linguistic contexts.},
  archive      = {J_TMM},
  author       = {Tae-Young Kim and Jufeng Yang and Eunil Park},
  doi          = {10.1109/TMM.2024.3521707},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1266-1276},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MSDLF-K: A multimodal feature learning approach for sentiment analysis in korean incorporating text and speech},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast disentangled slim tensor learning for multi-view
clustering. <em>TMM</em>, <em>27</em>, 1254–1265. (<a
href="https://doi.org/10.1109/TMM.2024.3521754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor-based multi-view clustering has recently received significant attention due to its exceptional ability to explore cross-view high-order correlations. However, most existing methods still encounter some limitations. (1) Most of them explore the correlations among different affinity matrices, making them unscalable to large-scale data. (2) Although some methods address it by introducing bipartite graphs, they may result in sub-optimal solutions caused by an unstable anchor selection process. (3) They generally ignore the negative impact of latent semantic-unrelated information in each view. To tackle these issues, we propose a new approach termed fast Disentangled Slim Tensor Learning (DSTL) for multi-view clustering. Instead of focusing on the multi-view graph structures, DSTL directly explores the high-order correlations among multi-view latent semantic representations based on matrix factorization. To alleviate the negative influence of feature redundancy, inspired by robust PCA, DSTL disentangles the latent low-dimensional representation into a semantic-unrelated part and a semantic-related part for each view. Subsequently, two slim tensors are constructed with tensor-based regularization. To further enhance the quality of feature disentanglement, the semantic-related representations are aligned across views through a consensus alignment indicator. Our proposed model is computationally efficient and can be solved effectively. Extensive experiments demonstrate the superiority and efficiency of DSTL over state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Deng Xu and Chao Zhang and Zechao Li and Chunlin Chen and Huaxiong Li},
  doi          = {10.1109/TMM.2024.3521754},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1254-1265},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast disentangled slim tensor learning for multi-view clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCN-based multi-modality fusion network for action
recognition. <em>TMM</em>, <em>27</em>, 1242–1253. (<a
href="https://doi.org/10.1109/TMM.2024.3521749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the remarkably expressive power for depicting structural data, Graph Convolutional Network (GCN) has been extensively adopted for skeleton-based action recognition in recent years. However, GCN is designed to operate on irregular graphs of skeletons, making it difficult to deal with other modalities represented on regular grids directly. Thus, although existing works have demonstrated the necessity of multi-modality fusion, few methods in the literature explore the fusion of skeleton and other modalities within a GCN architecture. In this paper, we present a novel GCN-based framework, termed GCN-based Multi-modality Fusion Network (GMFNet), to efficiently utilize complementary information in RGB and skeleton data. GMFNet is constructed by connecting a main stream with a GCN-based multi-modality fusion module (GMFM), whose goal is to gradually combine finer and coarse action-related information extracted from skeletons and RGB videos, respectively. Specifically, a cross-modality data mapping method is designed to transform an RGB video into a $\mathit{skeleton-like}$ (SL) sequence, which is then integrated with the skeleton sequence under a gradual fusion scheme in GMFM. The fusion results are fed into the following main stream to extract more discriminative features and produce the final prediction. In addition, a spatio-temporal joint attention mechanism is introduced for more accurate action recognition. Compared to the multi-stream approaches, GMFNet can be implemented within an end-to-end training pipeline and thereby reduces the training complexity. Experimental results show the proposed GMFNet achieves impressive performance on two large-scale data sets of NTU RGB+D 60 and 120.},
  archive      = {J_TMM},
  author       = {Shaocan Liu and Xingtao Wang and Ruiqin Xiong and Xiaopeng Fan},
  doi          = {10.1109/TMM.2024.3521749},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1242-1253},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GCN-based multi-modality fusion network for action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging enriched skeleton representation with
multi-relational metrics for few-shot action recognition. <em>TMM</em>,
<em>27</em>, 1228–1241. (<a
href="https://doi.org/10.1109/TMM.2024.3521767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot action recognition aims to identify new action classes with limited training samples. Most existing methods overlook the low information content and diversity of skeleton features, failing to exploit useful information in rare samples during meta-training. This leads to poor feature discriminability and recognition accuracy. To address both issues, we propose a novel Enriched Skeleton Representation and Multi-relational Metrics (ESR-MM) method for skeleton-based few-shot action recognition. First, a Frobenius Norm Diversity Loss is introduced to enrich skeleton representation by maximizing the Frobenius norm of the skeleton feature matrix. This mitigates over-smoothing and boosts information content and diversity. Leveraging these enriched features, we propose a multi-relational metrics strategy exploiting cross-sample task-specific information, intra-sample temporal order, and inter-sample distance. Specifically, Support-Adaptive Attention leverages task-specific cues between samples to generate attention-enhanced features. Then, the Bidirectional Temporal Coherent Mean Hausdorff Metric integrates Temporal Coherence Measure into the Bidirectional Mean Hausdorff Metric for class separation by accounting for temporal order. Finally, Prototype-discriminative Contrastive Loss exploits distances from class prototypes to query samples. ESR-MM demonstrates superior performance on two benchmarks.},
  archive      = {J_TMM},
  author       = {Jingyun Tian and Jinjing Gu and Yuanyuan Pu and Zhengpeng Zhao},
  doi          = {10.1109/TMM.2024.3521767},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1228-1241},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging enriched skeleton representation with multi-relational metrics for few-shot action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard-sample style guided patch attack with RL-enhanced
motion pattern for video recognition. <em>TMM</em>, <em>27</em>,
1205–1215. (<a href="https://doi.org/10.1109/TMM.2024.3521832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks have been extensively studied in the image field. In recent years, research has shown that video recognition models are also vulnerable to adversarial examples. However, most studies about adversarial attacks for video models have focused on perturbation-based methods, while patch-based black-box attacks have received less attention. Despite the excellent performance of perturbation-based attacks, these attacks are impractical for real-world implementation. Most existing patch-based black-box attacks require occluding larger areas and performing more queries to the target model. In this paper, we propose a hard-sample style guided patch attack with reinforcement learning (RL) enhanced motion patterns for video recognition (HSPA). Specifically, we utilize the style features of video hard samples and transfer their multi-dimensional style features to images to obtain a texture patch set. Then we use reinforcement learning to locate the patch coordinates and obtain a specific adversarial motion pattern of the patch to successfully perform an effective attack on a video recognition model in both the spatial and temporal dimensions. Our experiments on three widely-used video action recognition models (C3D, LRCN, and TDN) and two mainstream datasets (UCF-101 and HMDB-51) demonstrate the superior performance of our method compared to other state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Jian Yang and Jun Li and Yunong Cai and Guoming Wu and Zhiping Shi and Chaodong Tan and Xianglong Liu},
  doi          = {10.1109/TMM.2024.3521832},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1205-1215},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hard-sample style guided patch attack with RL-enhanced motion pattern for video recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive fusion learning for compositional zero-shot
recognition. <em>TMM</em>, <em>27</em>, 1193–1204. (<a
href="https://doi.org/10.1109/TMM.2024.3521852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Zero-Shot Learning (CZSL) aims to learn visual concepts (i.e., attributes and objects) from seen compositions and combine them to predict unseen compositions. Existing visual encoders in CZSL typically use traditional visual encoders (i.e., CNN and Transformer) or image encoders from Visual-Language Models (VLMs) to encode image features. However, traditional visual encoders need more multi-modal textual information, and image encoders of VLMs exhibit dependence on pre-training data, making them less effective when used independently for predicting unseen compositions. To overcome this limitation, we propose a novel approach based on the joint modeling of traditional visual encoders and VLMs visual encoders to enhance the prediction ability for uncommon and unseen compositions. Specifically, we design an adaptive fusion module that automatically adjusts the weighted parameters of similarity scores between traditional and VLMs methods during training, and these weighted parameters are inherited during the inference process. Given the significance of disentangling attributes and objects, we design a Multi-Attribute Object Module that, during the training phase, incorporates multiple pairs of attributes and objects as prior knowledge, leveraging this rich prior knowledge to facilitate the disentanglement of attributes and objects. Building upon this, we select the text encoder from VLMs to construct the Adaptive Fusion Network. We conduct extensive experiments on the Clothing16 K, UT-Zappos50 K, and C-GQA datasets, achieving excellent performance on the Clothing16 K and UT-Zappos50 K datasets.},
  archive      = {J_TMM},
  author       = {Lingtong Min and Ziman Fan and Shunzhou Wang and Feiyang Dou and Xin Li and Binglu Wang},
  doi          = {10.1109/TMM.2024.3521852},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1193-1204},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive fusion learning for compositional zero-shot recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty guided progressive few-shot learning perception
for aerial view synthesis. <em>TMM</em>, <em>27</em>, 1177–1192. (<a
href="https://doi.org/10.1109/TMM.2024.3521727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View synthesis of aerial scenes has gained attention in the recent development of applications such as urban planning, navigation, and disaster assessment. This development is closely connected to the recent advancement of the Neural Radiance Field (NeRF). However, when autonomousaerial vehicles(AAVs) encounter constraints such as limited perspectives or energy limitations, NeRF degrades with sparsely sampled views in complex aerial scenes. On this basis, we aim to solve this problem in a few-shot manner. In this paper, we propose Uncertainty Guided Perception NeRF (UPNeRF), an uncertainty-guided perceptual learning framework that focuses on applying and improving NeRF in few-shot aerial view synthesis (FSAVS). First, simply optimizing NeRF in complex aerial scenes with sparse input can lead to overfitting in training views, resulting in a collapsed model. To address this, we propose a progressive learning strategy that utilizes the uncertainty present in sparsely sampled views, enabling a gradual transition from easy to hard learning. Second, to take advantage of the inherent inductive bias in the data, we introduce an uncertainty-aware discriminator. This discriminator leverages convolutional capabilities to capture intricate patterns in the rendered patches associated with uncertainty. Third, direct optimization of NeRF lacks prior knowledge of the scene. This, coupled with a reduction in training views, can result in unrealistic rendering. To overcome this, we present a perceptual regularizer that incorporates prior knowledge through prompt tuning of a self-supervised pre-trained vision transformer. In addition, we adopt a sampled scene annealing strategy to enhance training stability. Finally, we conducted experiments with two public datasets, and the positive results indicate our method is effective.},
  archive      = {J_TMM},
  author       = {Zihan Gao and Lingling Li and Xu Liu and Licheng Jiao and Fang Liu and Shuyuan Yang},
  doi          = {10.1109/TMM.2024.3521727},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1177-1192},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty guided progressive few-shot learning perception for aerial view synthesis},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-path deep unsupervised learning for multi-focus image
fusion. <em>TMM</em>, <em>27</em>, 1165–1176. (<a
href="https://doi.org/10.1109/TMM.2024.3521817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion (MFIF) aims at merging multiple images captured at different focal lengths to create an all-in-focus image. This paper introduces a fully unsupervised learning approach for MFIF that uses only pairs of defocused images for end-to-end training, bypassing the need for ground-truths in supervised learning. Unlike existing methods training via a similarity loss between fused and source images, we propose a dual-path learning framework comprising two networks: an image fuser and a mask predictor. The mask predictor is modeled as a self-supervised denoising network on imperfect fusion masks, trained with a masking-based unsupervised learning scheme. The image fuser, crafted with deep unrolling, leverages the output from the mask predictor to supervise its mask generation at each unrolled step. Moreover, we introduce a fusion consistency loss to ensure the alignment between the image fuser and the mask predictor. In extensive experiments, our proposed approach shows superiority over existing end-to-end unsupervised methods and competitive performance against the supervised ones.},
  archive      = {J_TMM},
  author       = {Yuhui Quan and Xi Wan and Tianxiang Zheng and Yan Huang and Hui Ji},
  doi          = {10.1109/TMM.2024.3521817},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1165-1176},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-path deep unsupervised learning for multi-focus image fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene text image super-resolution via semantic distillation
and text perceptual loss. <em>TMM</em>, <em>27</em>, 1153–1164. (<a
href="https://doi.org/10.1109/TMM.2024.3521759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Super-Resolution (SR) technology aims to recover lost information in low-resolution text images. With the proposal of TextZoom, which is the first dataset aiming at text super-resolution in real scenes, more and more scene text super-resolution models have been presented on the basis of it. Although these methods have achieved excellent performance, they do not consider how to make full and efficient use of semantic information. Out of this consideration, a Semantic-aware Trident Network (STNet) for Scene Text Image Super-Resolution is proposed. Specifically, pre-trained text recognition model ASTER (Attentional Scene Text Recognizer) is utilized to assist this process in two ways. Firstly, a novel basic block named Semantic-aware Trident Block (STB) is designed to build the STNet, which incorporates an added branch for semantic distillation to learn semantic information of pre-trained recognition model. Secondly, we expand our model in an adversarial training manner and propose new text perceptual loss based on ASTER to further enhance semantic information in SR images. Extensive experiments on TextZoom dataset show that compared with directly recognizing bicubic images, the proposed STNet boosts the recognition accuracy of ASTER, MORAN (Multi-Object Rectified Attention Network), and CRNN (Convolutional Recurrent Neural Network) by 17.4%, 18.2%, and 24.3%, respectively, which is higher than the performance of several existing state-of-the-art (SOTA) SR network models. Besides, experiments in real scenes (on ICDAR 2015 dataset) and in restricted scenarios (defense against adversarial attacks) validate that addition of semantic information enables the proposed method to achieve promising cross-dataset performance. Since the proposed method is trained on cropped images, when applied to real-world scenarios, locations of text in natural images are firstly localized through scene text detection methods, and then cropped text images are obtained based on detected text positions.},
  archive      = {J_TMM},
  author       = {Cairong Zhao and Rui Shu and Shuyang Feng and Liang Zhu and Xuekuan Wang},
  doi          = {10.1109/TMM.2024.3521759},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1153-1164},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Scene text image super-resolution via semantic distillation and text perceptual loss},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCVP: Leveraging hierarchical contrastive visual prompt for
domain generalization. <em>TMM</em>, <em>27</em>, 1142–1152. (<a
href="https://doi.org/10.1109/TMM.2024.3521719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel Hierarchical Contrastive Visual Prompt (HCVP) methodology. This represents a significant advancement in the field, setting itself apart with a unique generative approach to prompts, alongside an explicit model structure and specialized loss functions. Differing from traditional visual prompts that are often shared across entire datasets, HCVP utilizes a hierarchical prompt generation network enhanced by prompt contrastive learning. These generative prompts are instance-dependent, catering to the unique characteristics inherent to different domains and tasks. Additionally, we devise a prompt modulation network that serves as a bridge, effectively incorporating the generated visual prompts into the vision transformer backbone. Experiments conducted on five DG datasets demonstrate the effectiveness of HCVP, outperforming both established DG algorithms and adaptation protocols.},
  archive      = {J_TMM},
  author       = {Guanglin Zhou and Zhongyi Han and Shiming Chen and Biwei Huang and Liming Zhu and Tongliang Liu and Lina Yao and Kun Zhang},
  doi          = {10.1109/TMM.2024.3521719},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1142-1152},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HCVP: Leveraging hierarchical contrastive visual prompt for domain generalization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing few-shot 3D point cloud classification with soft
interaction and self-attention. <em>TMM</em>, <em>27</em>, 1127–1141.
(<a href="https://doi.org/10.1109/TMM.2024.3521849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is a crucial aspect of modern machine learning that enables models to recognize and classify objects efficiently with limited training data. The shortage of labeled 3D point cloud data calls for innovative solutions, particularly when novel classes emerge more frequently. In this paper, we propose a novel few-shot learning method for recognizing 3D point clouds. More specifically, this paper addresses the challenges of applying few-shot learning to 3D point cloud data, which poses unique difficulties due to the unordered and irregular nature of these data. We propose two new modules for few-shot based 3D point cloud classification, i.e., the Soft Interaction Module (SIM) and Self-Attention Residual Feedforward (SARF) Module. These modules balance and enhance the feature representation by enabling more relevant feature interactions and capturing long-range dependencies between query and support features. To validate the effectiveness of the proposed method, extensive experiments are conducted on benchmark datasets, including ModelNet40, ShapeNetCore, and ScanObjectNN. Our approach demonstrates superior performance in handling abrupt feature changes occurring during the meta-learning process. The results of the experiments indicate the superiority of our proposed method by demonstrating its robust generalization ability and better classification performance for 3D point cloud data with limited training samples.},
  archive      = {J_TMM},
  author       = {Abdullah Aman Khan and Jie Shao and Sidra Shafiq and Shuyuan Zhu and Heng Tao Shen},
  doi          = {10.1109/TMM.2024.3521849},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1127-1141},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing few-shot 3D point cloud classification with soft interaction and self-attention},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the effective dynamic information of spectral
shapes for audio classification. <em>TMM</em>, <em>27</em>, 1114–1126.
(<a href="https://doi.org/10.1109/TMM.2024.3521837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectral shape holds crucial information for Audio Classification (AC), encompassing the spectrum&#39;s envelope, details, and dynamic changes over time. Conventional methods utilize cepstral coefficients for spectral shape description but overlook its variation details. Deep-learning approaches capture some dynamics but demand substantial training or fine-tuning resources. The Learning in the Model Space (LMS) framework precisely captures the dynamic information of temporal data by utilizing model fitting, even when computational resources and data are limited. However, applying LMS to audio faces challenges: 1) The high sampling rate of audio hinders efficient data fitting and capturing of dynamic information. 2) The Dynamic Information of Partial Spectral Shapes (DIPSS) may enhance classification, as only specific spectral shapes are relevant for AC. This paper extends an AC framework called Effective Dynamic Information Capture (EDIC) to tackle the above issues. EDIC constructs Mel-Frequency Cepstral Coefficients (MFCC) sequences within different dimensional intervals as the fitted data, which not only reduces the number of sequence sampling points but can also describe the change of the spectral shape in different parts over time. EDIC enables us to implement a topology-based selection algorithm in the model space, selecting effective DIPSS for the current AC task. The performance on three tasks confirms the effectiveness of EDIC.},
  archive      = {J_TMM},
  author       = {Liangwei Chen and Xiren Zhou and Qiuju Chen and Fang Xiong and Huanhuan Chen},
  doi          = {10.1109/TMM.2024.3521837},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1114-1126},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Investigating the effective dynamic information of spectral shapes for audio classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizable prompt learning via gradient constrained
sharpness-aware minimization. <em>TMM</em>, <em>27</em>, 1100–1113. (<a
href="https://doi.org/10.1109/TMM.2024.3521702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper targets a novel trade-off problem in generalizable prompt learning for vision-language models (VLM), i.e., improving the performance on unseen classes while maintaining the performance on seen classes. Comparing with existing generalizable methods that neglect the seen classes degradation, the setting of this problem is stricter and fits more closely with practical applications. To solve this problem, we start from the optimization perspective, and leverage the relationship between loss landscape geometry and model generalization ability. By analyzing the loss landscapes of the state-of-the-art method and vanilla Sharpness-aware Minimization (SAM) based method, we conclude that the trade-off performance correlates to both loss value and loss sharpness, while each of them is indispensable. However, we find the optimizing gradient of existing methods cannot maintain high relevance to both loss value and loss sharpness during optimization, which severely affects their trade-off performance. To this end, we propose a novel SAM-based method for prompt learning, denoted as Gradient Constrained Sharpness-aware Context Optimization (GCSCoOp), to dynamically constrain the optimizing gradient, thus achieving above two-fold optimization objective simultaneously. Extensive experiments verify the effectiveness of GCSCoOp in the trade-off problem.},
  archive      = {J_TMM},
  author       = {Liangchen Liu and Nannan Wang and Dawei Zhou and Decheng Liu and Xi Yang and Xinbo Gao and Tongliang Liu},
  doi          = {10.1109/TMM.2024.3521702},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1100-1113},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Generalizable prompt learning via gradient constrained sharpness-aware minimization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCCNet: A novel network leveraging gated cross-correlation
for multi-view classification. <em>TMM</em>, <em>27</em>, 1086–1099. (<a
href="https://doi.org/10.1109/TMM.2024.3521733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning is a machine learning paradigm that utilizes multiple feature sets or data sources to improve learning performance and generalization. However, existing multi-view learning methods often do not capture and utilize information from different views very well, especially when the relationships between views are complex and of varying quality. In this paper, we propose a novel multi-view learning framework for the multi-view classification task, called Gated Cross-Correlation Network (GCCNet), which addresses these challenges by integrating the three key operational levels in multi-view learning: representation, fusion, and decision. Specifically, GCCNet contains a novel component called the Multi-View Gated Information Distributor (MVGID) to enhance noise filtering and optimize the retention of critical information. In addition, GCCNet uses cross-correlation analysis to reveal dependencies and interactions between different views, as well as integrates an adaptive weighted joint decision strategy to mitigate the interference of low-quality views. Thus, GCCNet can not only comprehensively capture and utilize information from different views, but also facilitate information exchange and synergy between views, ultimately improving the overall performance of the model. Extensive experimental results on ten benchmark datasets show GCCNet&#39;s outperforms state-of-the-art methods on eight out of ten datasets, validating its effectiveness and superiority in multi-view learning.},
  archive      = {J_TMM},
  author       = {Yuanpeng Zeng and Ru Zhang and Hao Zhang and Shaojie Qiao and Faliang Huang and Qing Tian and Yuzhong Peng},
  doi          = {10.1109/TMM.2024.3521733},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1086-1099},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GCCNet: A novel network leveraging gated cross-correlation for multi-view classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-level multi-object 9D state tracking using
object-centric multi-scale transformer in point cloud stream.
<em>TMM</em>, <em>27</em>, 1072–1085. (<a
href="https://doi.org/10.1109/TMM.2024.3521664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Category-level object pose estimation and tracking has achieved impressive progress in computer vision, augmented reality, and robotics. Existing methods either estimate the object states from a single observation or only track the 6-DoF pose of a single object. In this paper, we focus on category-level multi-object 9-Dimensional (9D) state tracking from the point cloud stream. We propose a novel 9D state estimation network to estimate the 6-DoF pose and 3D size of each instance in the scene. It uses our devised multi-scale global attention and object-level local attention modules to obtain representative latent features to estimate the 9D state of each object in the current observation. We then integrate our network estimation into a Kalman filter to combine previous states with the current estimates and achieve multi-object 9D state tracking. Experiment results on two public datasets show that our method achieves state-of-the-art performance on both category-level multi-object state estimation and pose tracking tasks. Furthermore, we directly apply the pre-trained model of our method to our air-ground robot system with multiple moving objects. Experiments on our collected real-world dataset show our method&#39;s strong generalization ability and real-time pose tracking performance.},
  archive      = {J_TMM},
  author       = {Jingtao Sun and Yaonan Wang and Mingtao Feng and Xiaofeng Guo and Huimin Lu and Xieyuanli Chen},
  doi          = {10.1109/TMM.2024.3521664},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {1072-1085},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Category-level multi-object 9D state tracking using object-centric multi-scale transformer in point cloud stream},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from mistakes: Self-regularizing hierarchical
representations in point cloud semantic segmentation. <em>TMM</em>,
<em>27</em>, 978–989. (<a
href="https://doi.org/10.1109/TMM.2023.3345152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in autonomous robotic technologies have highlighted the growing need for precise environmental analysis. Point cloud semantic segmentation has gained attention to accomplish fine-grained scene understanding by acting directly on raw content provided by sensors. Recent solutions showed how different learning techniques can be used to improve the performance of the model, without any architectural or dataset change. Following this trend, we present a coarse-to-fine setup that LEArns from classification mistaKes (LEAK) derived from a standard model. First, classes are clustered into macro groups according to mutual prediction errors; then, the learning process is regularized by: (1) aligning class-conditional prototypical feature representation for both fine and coarse classes, (2) weighting instances with a per-class fairness index. Our LEAK approach is very general and can be seamlessly applied on top of any segmentation architecture; indeed, experimental results showed that it enables state-of-the-art performances on different architectures, datasets and tasks, while ensuring more balanced class-wise results and faster convergence.},
  archive      = {J_TMM},
  author       = {Elena Camuffo and Umberto Michieli and Simone Milani},
  doi          = {10.1109/TMM.2023.3345152},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {978-989},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning from mistakes: Self-regularizing hierarchical representations in point cloud semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpreting hidden semantics in the intermediate layers of
3D point cloud classification neural network. <em>TMM</em>, <em>27</em>,
965–977. (<a href="https://doi.org/10.1109/TMM.2023.3345147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although 3D point cloud classification neural network models have been widely used, the in-depth interpretation of the activation of the neurons and layers is still a challenge. We propose a novel approach, named Relevance Flow, to interpret the hidden semantics of 3D point cloud classification neural networks. It delivers the class Relevance to the activated neurons in the intermediate layers in a back-propagation manner, and associates the activation of neurons with the input points to visualize the hidden semantics of each layer. Specially, we reveal that the 3D point cloud classification neural network has learned the plane-level and part-level hidden semantics in the intermediate layers, and utilize the normal and IoU to evaluate the consistency of both levels&#39; hidden semantics. Besides, by using the hidden semantics, we generate the adversarial attack samples to attack 3D point cloud classifiers. Experiments show that our proposed method reveals the hidden semantics of the 3D point cloud classification neural network on ModelNet40 and ShapeNet, which can be used for the unsupervised point cloud part segmentation without labels and attacking the 3D point cloud classifiers.},
  archive      = {J_TMM},
  author       = {Weiquan Liu and Minghao Liu and Shijun Zheng and Siqi Shen and Xuesheng Bian and Yu Zang and Ping Zhong and Cheng Wang},
  doi          = {10.1109/TMM.2023.3345147},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {965-977},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Interpreting hidden semantics in the intermediate layers of 3D point cloud classification neural network},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Region-enhanced feature learning for scene semantic
segmentation. <em>TMM</em>, <em>27</em>, 954–964. (<a
href="https://doi.org/10.1109/TMM.2023.3342718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation in complex scenes relies not only on object appearance but also on object location and the surrounding environment. Nonetheless, it is difficult to model long-range context in the format of pairwise point correlations due to the huge computational cost for large-scale point clouds. In this article, we propose using regions as the intermediate representation of point clouds instead of fine-grained points or voxels to reduce the computational burden. We introduce a novel Region-Enhanced Feature Learning Network (REFL-Net) that leverages region correlations to enhance point feature learning. We design a region-based feature enhancement (RFE) module, which consists of a Semantic-Spatial Region Extraction stage and a Region Dependency Modeling stage. In the first stage, the input points are grouped into a set of regions based on their semantic and spatial proximity. In the second stage, we explore inter-region semantic and spatial relationships by employing a self-attention block on region features and then fuse point features with the region features to obtain more discriminative representations. Our proposed RFE module is plug-and-play and can be integrated with common semantic segmentation backbones. We conduct extensive experiments on ScanNetV2 and S3DIS datasets and evaluate our RFE module with different segmentation backbones. Our REFL-Net achieves 1.8% mIoU gain on ScanNetV2 and 1.7% mIoU gain on S3DIS with negligible computational cost compared with backbone models. Both quantitative and qualitative results show the powerful long-range context modeling ability and strong generalization ability of our REFL-Net.},
  archive      = {J_TMM},
  author       = {Xin Kang and Chaoqun Wang and Xuejin Chen},
  doi          = {10.1109/TMM.2023.3342718},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {954-964},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Region-enhanced feature learning for scene semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging single-view images for unsupervised 3D point
cloud completion. <em>TMM</em>, <em>27</em>, 940–953. (<a
href="https://doi.org/10.1109/TMM.2023.3340892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds captured by scanning devices are often incomplete due to occlusion. To overcome this limitation, point cloud completion methods have been developed to predict the complete shape of an object based on its partial input. These methods can be broadly classified as supervised or unsupervised. However, both categories require a large number of 3D complete point clouds, which may be difficult to capture. In this paper, we propose Cross-PCC, an unsupervised point cloud completion method without requiring any 3D complete point clouds. We only utilize 2D images of the complete objects, which are easier to capture than 3D complete and clean point clouds. Specifically, to take advantage of the complementary information from 2D images, we use a single-view RGB image to extract 2D features and design a fusion module to fuse the 2D and 3D features extracted from the partial point cloud. To guide the shape of predicted point clouds, we project the predicted points of the object to the 2D plane and use the foreground pixels of its silhouette maps to constrain the position of the projected points. To reduce the outliers of the predicted point clouds, we propose a view calibrator to move the points projected to the background into the foreground by the single-view silhouette image. To the best of our knowledge, our approach is the first point cloud completion method that does not require any 3D supervision. The experimental results of our method are superior to those of the state-of-the-art unsupervised methods by a large margin. Moreover, our method even achieves comparable performance to some supervised methods.},
  archive      = {J_TMM},
  author       = {Lintai Wu and Qijian Zhang and Junhui Hou and Yong Xu},
  doi          = {10.1109/TMM.2023.3340892},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {940-953},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging single-view images for unsupervised 3D point cloud completion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating point cloud from moving camera videos: A
no-reference metric. <em>TMM</em>, <em>27</em>, 927–939. (<a
href="https://doi.org/10.1109/TMM.2023.3340894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is one of the most widely used digital representation formats for three-dimensional (3D) contents, the visual quality of which may suffer from noise and geometric shift distortions during the production procedure as well as compression and downsampling distortions during the transmission process. To tackle the challenge of point cloud quality assessment (PCQA), many PCQA methods have been proposed to evaluate the visual quality levels of point clouds by assessing the rendered static 2D projections. Although such projection-based PCQA methods achieve competitive performance with the assistance of mature image quality assessment (IQA) methods, they neglect that the 3D model is also perceived in a dynamic viewing manner, where the viewpoint is continually changed according to the feedback of the rendering device. Therefore, in this paper, we evaluate the point clouds from moving camera videos and explore the way of dealing with PCQA tasks via using video quality assessment (VQA) methods. First, we generate the captured videos by rotating the camera around the point clouds through several circular pathways. Then we extract both spatial and temporal quality-aware features from the selected key frames and the video clips through using trainable 2D-CNN and pre-trained 3D-CNN models respectively. Finally, the visual quality of point clouds is represented by the video quality values. The experimental results reveal that the proposed method is effective for predicting the visual quality levels of the point clouds and even competitive with full-reference (FR) PCQA methods. The ablation studies further verify the rationality of the proposed framework and confirm the contributions made by the quality-aware features extracted via the dynamic viewing manner.},
  archive      = {J_TMM},
  author       = {Zicheng Zhang and Wei Sun and Yucheng Zhu and Xiongkuo Min and Wei Wu and Ying Chen and Guangtao Zhai},
  doi          = {10.1109/TMM.2023.3340894},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {927-939},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Evaluating point cloud from moving camera videos: A no-reference metric},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LRDNet: Lightweight LiDAR aided cascaded feature pools for
free road space detection. <em>TMM</em>, <em>27</em>, 652–664. (<a
href="https://doi.org/10.1109/TMM.2022.3230330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans have long fantasized about self-driving vehicles for the sake of luxury, style, safety, and ease. Free road space detection for collision avoidance and path planning is a vital part of autonomous driving vehicles. Despite many researchers focusing on free road space detection, it remains an open and challenging problem for real-world applications. Many studies have attempted to fuse depth and LiDAR features with visual features to improve the overall performance of free road space detection. However, there is no guideline on how such features should be fused to complement the visual features. Additionally, most of the previously proposed methods are computationally expensive and not suitable for real-life applications. The main motivation of this study is to realize a lightweight model that addresses these problems without compromising performance. As the LiDAR and visual features exist in different spaces, the proposed method attempts to learn various transformation and fusion operations from LiDAR features to complement the visual features. To validate the performance of the proposed method, we conduct comprehensive experiments on prominent benchmark datasets. The results of the experiments reveal the superior performance of the proposed model while being lightweight. LRDNet ranks third overall (with a minor difference) and second among LiDAR-based methods on the KITTI road benchmark dataset. Furthermore, the proposed model is the least computationally expensive among state-of-the-art methods and can be considered an optimal trade-off between speed and accuracy.},
  archive      = {J_TMM},
  author       = {Abdullah Aman Khan and Jie Shao and Yunbo Rao and Lei She and Heng Tao Shen},
  doi          = {10.1109/TMM.2022.3230330},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {652-664},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LRDNet: Lightweight LiDAR aided cascaded feature pools for free road space detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory-enhanced confidence calibration for class-incremental
unsupervised domain adaptation. <em>TMM</em>, <em>27</em>, 610–621. (<a
href="https://doi.org/10.1109/TMM.2024.3521834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on Class-Incremental Unsupervised Domain Adaptation (CI-UDA), where the labeled source domain already includes all classes, and the classes in the unlabeled target domain emerge sequentially over time. This task involves addressing two main challenges. The first is the domain gap between the labeled source data and the unlabeled target data, which leads to weak generalization performance. The second is the inconsistency between the source and target category spaces at each time step, which causes catastrophic forgetting during the testing stage. Previous methods focus solely on the alignment of similar samples from different domains, which overlooks the underlying causes of the domain gap/class distribution difference. To tackle the issue, we rethink this task from a causal perspective for the first time. We first build a structural causal graph to describe the CI-UDA problem. Based on the causal graph, we present Memory-Enhanced Confidence Calibration (MECC), which aims to improve confidence in the predicted results. In particular, we argue that the domain discrepancy caused by the different styles is prone to make the model produce less confident predictions and thus weakens the generalization and continual learning abilities. To this end, we first explore using the gram matrix to generate source-style target data, which is combined with the original data to jointly train the model and thereby reduce the domain-shift impact. Second, we utilize the model of the previous time step to select corresponding samples that are used to build a memory bank, which is instrumental in alleviating catastrophic forgetting. Extensive experimental results on multiple datasets demonstrate the superiority of our method.},
  archive      = {J_TMM},
  author       = {Jiaping Yu and Muli Yang and Aming Wu and Cheng Deng},
  doi          = {10.1109/TMM.2024.3521834},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {610-621},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Memory-enhanced confidence calibration for class-incremental unsupervised domain adaptation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combating noisy labels by alleviating the memorization of
DNNs to noisy labels. <em>TMM</em>, <em>27</em>, 597–609. (<a
href="https://doi.org/10.1109/TMM.2024.3521722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data is the essential fuel for deep neural networks (DNNs), and its quality affects the practical performance of DNNs. In real-world training scenarios, the successful generalization performance of DNNs is severely challenged by noisy samples with incorrect labels. To combat noisy samples in image classification, numerous methods based on sample selection and semi-supervised learning (SSL) have been developed, where sample selection is used to provide the supervision signal for SSL, achieving great success in resisting noisy samples. Due to the necessary warm-up training on noisy datasets and the basic sample selection mechanism, DNNs are still confronted with the challenge of memorizing noisy samples. However, existing methods do not address the memorization of noisy samples by DNNs explicitly, which hinders the generalization performance of DNNs. To alleviate this issue, we present a new approach to combat noisy samples. First, we propose a memorized noise detection method to detect noisy samples that DNNs have already memorized during the training process. Next, we design a noise-excluded sample selection method and a noise-alleviated MixMatch to alleviate the memorization of DNNs to noisy samples. Finally, we integrate our approach with the established method DivideMix, proposing Modified-DivideMix. The experimental results on CIFAR-10, CIFAR-100, and Clothing1M demonstrate the effectiveness of our approach.},
  archive      = {J_TMM},
  author       = {Shunjie Yuan and Xinghua Li and Yinbin Miao and Haiyan Zhang and Ximeng Liu and Robert H. Deng},
  doi          = {10.1109/TMM.2024.3521722},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {597-609},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Combating noisy labels by alleviating the memorization of DNNs to noisy labels},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modality semantic consistency learning for
visible-infrared person re-identification. <em>TMM</em>, <em>27</em>,
568–580. (<a href="https://doi.org/10.1109/TMM.2024.3521843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) seeks to identify and match individuals across visible and infrared ranges within intelligent monitoring environments. Most current approaches predominantly explore a two-stream network structure that extract global or rigidly split part features and introduce an extra modality for image compensation to guide networks reducing the huge differences between the two modalities. However, these methods are sensitive to misalignment caused by pose/viewpoint variations and additional noises produced by extra modality generating. Within the confines of this articles, we clearly consider addresses above issues and propose a Cross-modality Semantic Consistency Learning (CSCL) network to excavate the semantic consistent features in different modalities by utilizing human semantic information. Specifically, a Parsing-aligned Attention Module (PAM) is introduced to filter out the irrelevant noises with channel-wise attention and dynamically highlight the semantic-aware representations across modalities in different stages of the network. Then, a Semantic-guided Part Alignment Module (SPAM) is introduced, aimed at efficiently producing a collection of semantic-aligned fine-grained features. This is achieved by incorporating parsing loss and division loss constraints, ultimately enhancing the overall person representation. Finally, an Identity-aware Center Mining (ICM) loss is presented to reduce the distribution between modality centers within classes, thereby further alleviating intra-class modality discrepancies. Extensive experiments indicate that CSCL outperforms the state-of-the-art methods on the SYSU-MM01 and RegDB datasets. Notably, the Rank-1/mAP accuracy on the SYSU-MM01 dataset can achieve 75.72%/72.08%.},
  archive      = {J_TMM},
  author       = {Min Liu and Zhu Zhang and Yuan Bian and Xueping Wang and Yeqing Sun and Baida Zhang and Yaonan Wang},
  doi          = {10.1109/TMM.2024.3521843},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {568-580},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modality semantic consistency learning for visible-infrared person re-identification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-guided cross-modal alignment and progressive
fusion for chest x-ray report generation. <em>TMM</em>, <em>27</em>,
557–567. (<a href="https://doi.org/10.1109/TMM.2024.3521728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of chest X-ray report generation, which aims to simulate the diagnosis process of doctors, has received widespread attention. Compared with the image caption task, chest X-ray report generation is more challenging since it needs to generate a longer and more accurate description of each diagnostic part in chest X-ray images. Most of existing works focus on how to extract better visual features or more accurate text expression based on existing reports. However, they ignore the interactions between visual and text modalities and are thus obviously not in line with human thinking. A small part of works explore the interactions of visual and text modalities, but data-driven learning of cross-modal information mapping can not break the semantic gap between different modalities. In this work, we propose a novel approach called Knowledge-guided Cross-modal Alignment and Progressive fusion (KCAP), which takes the knowledge words from a created medical knowledge dictionary as the bridge to guide the cross-modal feature alignment and fusion, for accurate chest X-ray report generation. In particular, we create the medical knowledge dictionary by extracting medical phrases from the training set and then selecting some phrases with substantive meanings as knowledge words based on their frequency of occurrence. Based on the knowledge words from the medical knowledge dictionary, the visual and text modalities are interacted by a mapping layer for the enhancement of the features of two modalities, and then the alignment fusion module is introduced to mitigate the semantic gap between visual and text modalities. To retain the important details of the original information, we design a progressive fusion scheme to integrate the advantages of both salient fused and original features to generate better medical reports. The experimental results on IU-Xray and MIMIC datasets demonstrate the effectiveness of the proposed KCAP.},
  archive      = {J_TMM},
  author       = {Lili Huang and Yiming Cao and Pengcheng Jia and Chenglong Li and Jin Tang and Chuanfu Li},
  doi          = {10.1109/TMM.2024.3521728},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {557-567},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Knowledge-guided cross-modal alignment and progressive fusion for chest X-ray report generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiview feature decoupling for deep subspace clustering.
<em>TMM</em>, <em>27</em>, 544–556. (<a
href="https://doi.org/10.1109/TMM.2024.3521776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep multi-view subspace clustering aims to reveal a common subspace structure by exploiting rich multi-view information. Despite promising progress, current methods focus only on multi-view consistency and complementarity, often overlooking the adverse influence of entangled superfluous information in features. Moreover, most existing works lack scalability and are inefficient for large-scale scenarios. To this end, we innovatively propose a deep subspace clustering method via Multi-view Feature Decoupling (MvFD). First, MvFD incorporates well-designed multi-type auto-encoders with self-supervised learning, explicitly decoupling consistent, complementary, and superfluous features for every view. The disentangled and interpretable feature space can then better serve unified representation learning. By integrating these three types of information within a unified framework, we employ information theory to obtain a minimal and sufficient representation with high discriminability. Besides, we introduce a deep metric network to model self-expression correlation more efficiently, where network parameters remain unaffected by changes in sample numbers. Extensive experiments show that MvFD yields State-of-the-Art performance in various types of multi-view datasets.},
  archive      = {J_TMM},
  author       = {Yuxiu Lin and Hui Liu and Ren Wang and Qiang Guo and Caiming Zhang},
  doi          = {10.1109/TMM.2024.3521776},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {544-556},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiview feature decoupling for deep subspace clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive pseudo labeling for multi-dataset detection over
unified label space. <em>TMM</em>, <em>27</em>, 531–543. (<a
href="https://doi.org/10.1109/TMM.2024.3521841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-dataset detection works mainly focus on the performance of detector on each of the datasets, with different label spaces. However, in real-world applications, a unified label space across multiple datasets is usually required. To address such a gap, we propose a progressive pseudo labeling (PPL) approach to detect objects across different datasets, over a unified label space. Specifically, we employ the widely used architecture of teacher-student model pair to jointly refine pseudo labels and train the unified object detector. The student model learns from both annotated labels and pseudo labels from the teacher model, which is updated by the exponential moving average (EMA) of the student. Three modules, i.e. Entropy-guided Adaptive Threshold (EAT), Global Classification Module (GCM) and Scene-Aware Fusion (SAF) strategy, are proposed to handle the noise of pseudo labels and fit the overall distribution. Extensive experiments are conducted on different multi-dataset benchmarks. The results demonstrate that our proposed method significantly outperforms the State-of-the-Art and is even comparable with supervised methods trained using annotations of all labels.},
  archive      = {J_TMM},
  author       = {Kai Ye and Zepeng Huang and Yilei Xiong and Yu Gao and Jinheng Xie and Linlin Shen},
  doi          = {10.1109/TMM.2024.3521841},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {531-543},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive pseudo labeling for multi-dataset detection over unified label space},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VRTNet: Vector rectifier transformer for two-view
correspondence learning. <em>TMM</em>, <em>27</em>, 515–530. (<a
href="https://doi.org/10.1109/TMM.2024.3521696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding reliable correspondences in two-view image and recovering the camera poses are key problems in photogrammetry and image signal processing. Multilayer perceptron (MLP) has a wide application in two-view correspondence learning for which is good at learning disordered sparse correspondences, but it is susceptible to the dominant outliers and requires additional functional blocks to capture context information. CNN can naturally extract local context information, but it cannot handle disordered data and extract global context and channel information. In order to overcome the shortcomings of MLP and CNN, we design a correspondence learning network based on Transformer, named Vector Rectifier Transformer (VRTNet). Transformer is an encoder-decoder structure which can handle disordered sparse correspondences and output sequences of arbitrary length. Therefore, we design two sub-Transformers in VRTNet to achieve the mutual conversion between disordered and ordered correspondences. The self-attention and cross-attention mechanisms in them allow VRTNet to focus on the global context relations of all correspondences. To capture local context and channel information, we propose rectifier network (including CNN and channel attention block) as the backbone of VRTNet, which avoids the complex design of additional blocks. Rectifier network can correct the errors of ordered correspondences to obtain rectified correspondences. Finally, outliers are removed by comparing original and rectified correspondences. VRTNet performs better than the state-of-the-art methods in the tasks of relative pose estimation, outlier removal and image registration.},
  archive      = {J_TMM},
  author       = {Meng Yang and Jun Chen and Xin Tian and Longsheng Wei and Jiayi Ma},
  doi          = {10.1109/TMM.2024.3521696},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {515-530},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VRTNet: Vector rectifier transformer for two-view correspondence learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCM-net: A diffusion model-based detection network
integrating the characteristics of copy-move forgery. <em>TMM</em>,
<em>27</em>, 503–514. (<a
href="https://doi.org/10.1109/TMM.2024.3521685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Essentially, directly introducing any object detection network to perform copy-move forgery detection (CMFD) inevitably leads to low detection accuracy. Therefore, DCM-Net, an object detection network dominated by diffusion model that incorporates the characteristics of copy-move forgery, is proposed in this paper for obviously enhancing CMFD performance. DCM-Net, as the first diffusion model-based CMFD network, has the following three improvements. Firstly, the high-similarity box padding strategy pads high-similarity boxes, rather than random boxes used in diffusion model, to ground truth boxes, better guiding subsequent dual-attention detection heads (DDHs) to focus more on high-similarity regions. Secondly, different from previous deep learning based CMFD networks that utilize self-correlation calculation to indiscriminately transform all classification features extracted from feature extraction into high-similarly features, an adaptive feature combination strategy is proposed to obtain the optimal feature transformation capable of achieving the best detection performance, enabling DDHs to more effectively distinguish source and target regions. Finally, to make detection heads have more accurate source/target localization and distinguishment, DDHs equipped with efficient multi-scale attention and contextual transformer, are proposed to generate tampered features fusing the entire precise spatial position information and rich contextual global information. The experimental results carried out on three publicly available datasets including USC-ISI, CoMoFoD, and COVERAGE, demonstrate that DCM-Net outperforms several advanced algorithms in terms of similarity detection ability and source/target differentiation ability.},
  archive      = {J_TMM},
  author       = {Shaowei Weng and Jianhao Zhang and Tanguo Zhu and Lifang Yu and Chunyu Zhang},
  doi          = {10.1109/TMM.2024.3521685},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {503-514},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DCM-net: A diffusion model-based detection network integrating the characteristics of copy-move forgery},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explain vision focus: Blending human saliency into synthetic
face images. <em>TMM</em>, <em>27</em>, 489–502. (<a
href="https://doi.org/10.1109/TMM.2024.3521670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic faces have been extensively researched and applied in various fields, such as face parsing and recognition. Compared to real face images, synthetic faces engender more controllable and consistent experimental stimuli due to the ability to precisely merge expression animations onto the facial skeleton. Accordingly, we establish an eye-tracking database with 780 synthetic face images and fixation data collected from 22 participants. The use of synthetic images with consistent expressions ensures reliable data support for exploring the database and determining the following findings: (1) A correlation study between saliency intensity and facial movement reveals that the variation of attention distribution within facial regions is mainly attributed to the movement of the mouth. (2) A categorized analysis of different demographic factors demonstrates that the bias towards salient regions aligns with differences in some demographic categories of synthetic characters. In practice, inference of facial saliency distribution is commonly used to predict the regions of interest for facial video-related applications. Therefore, we propose a benchmark model that accurately predicts saliency maps, closely matching the ground truth annotations. This achievement is made possible by utilizing channel alignment and progressive summation for feature fusion, along with the incorporation of Sinusoidal Position Encoding. The ablation experiment also demonstrates the effectiveness of our proposed model. We hope that this paper will contribute to advancing the photorealism of generative digital humans.},
  archive      = {J_TMM},
  author       = {Kaiwei Zhang and Dandan Zhu and Xiongkuo Min and Huiyu Duan and Guangtao Zhai},
  doi          = {10.1109/TMM.2024.3521670},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {489-502},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Explain vision focus: Blending human saliency into synthetic face images},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-contrastive fine-grained crowd counting and beyond.
<em>TMM</em>, <em>27</em>, 477–488. (<a
href="https://doi.org/10.1109/TMM.2024.3521823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting has drawn increasing attention across various fields. However, existing crowd counting tasks primarily focus on estimating the overall population, ignoring the behavioral and semantic information of different social groups within the crowd. In this paper, we aim to address a newly proposed research problem, namely fine-grained crowd counting, which involves identifying different categories of individuals and accurately counting them in static images. In order to fully leverage the categorical information in static crowd images, we propose a two-tier salient feature propagation module designed to sequentially extract semantic information from both the crowd and its surrounding environment. Additionally, we introduce a category difference loss to refine the feature representation by highlighting the differences between various crowd categories. Moreover, our proposed framework can adapt to a novel problem setup called few-example fine-grained crowd counting. This setup, unlike the original fine-grained crowd counting, requires only a few exemplar point annotations instead of dense annotations from predefined categories, making it applicable in a wider range of scenarios. The baseline model for this task can be established by substituting the loss function in our proposed model with a novel hybrid loss function that integrates point-oriented cross-entropy loss and category contrastive loss. Through comprehensive experiments, we present results in both the formulation and application of fine-grained crowd counting.},
  archive      = {J_TMM},
  author       = {Meijing Zhang and Mengxue Chen and Qi Li and Yanchen Chen and Rui Lin and Xiaolian Li and Shengfeng He and Wenxi Liu},
  doi          = {10.1109/TMM.2024.3521823},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {477-488},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Category-contrastive fine-grained crowd counting and beyond},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit and explicit language guidance for diffusion-based
visual perception. <em>TMM</em>, <em>27</em>, 466–476. (<a
href="https://doi.org/10.1109/TMM.2024.3521825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image diffusion models have shown powerful ability on conditional image synthesis. With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich textures and reasonable structures under different text prompts. However, adapting pre-trained diffusion models for visual perception is an open problem. In this paper, we propose an implicit and explicit language guidance framework for diffusion-based visual perception, named IEDP. Our IEDP comprises an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs a frozen CLIP image encoder to directly generate implicit text embeddings that are fed to the diffusion model without explicit text prompts. The explicit branch uses the ground-truth labels of corresponding images as text prompts to condition feature extraction in diffusion model. During training, we jointly train the diffusion model by sharing the model weights of these two branches. As a result, the implicit and explicit branches can jointly guide feature learning. During inference, we employ only implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU$^\text{ss}$ score of 55.9% on ADE20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 11.0%.},
  archive      = {J_TMM},
  author       = {Hefeng Wang and Jiale Cao and Jin Xie and Aiping Yang and Yanwei Pang},
  doi          = {10.1109/TMM.2024.3521825},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {466-476},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Implicit and explicit language guidance for diffusion-based visual perception},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prototype alignment with dedicated experts for test-agnostic
long-tailed recognition. <em>TMM</em>, <em>27</em>, 455–465. (<a
href="https://doi.org/10.1109/TMM.2024.3521665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike vanilla long-tailed recognition trains on imbalanced data but assumes a uniform test class distribution, test-agnostic long-tailed recognition aims to handle arbitrary test class distributions. Existing methods require prior knowledge of test sets for post-adjustment through multi-stage training, resulting in static decisions at the dataset-level. This pipeline overlooks instance diversity and is impractical in real situations. In this work, we introduce Prototype Alignment with Dedicated Experts (PADE), a one-stage framework for test-agnostic long-tailed recognition. PADE tackles unknown test distributions at the instance-level, without depending on test priors. It reformulates the task as a domain detection problem, dynamically adjusting the model for each instance. PADE comprises three main strategies: 1) parameter customization strategy for multi-experts skilled at different categories; 2) normalized target knowledge distillation for mutual guidance among experts while maintaining diversity; 3) re-balanced compactness learning with momentum prototypes, promoting instance alignment with the corresponding class centroid. We evaluate PADE on various long-tailed recognition benchmarks with diverse test distributions. The results verify its effectiveness in both vanilla and test-agnostic long-tailed recognition.},
  archive      = {J_TMM},
  author       = {Chen Guo and Weiling Chen and Aiping Huang and Tiesong Zhao},
  doi          = {10.1109/TMM.2024.3521665},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {455-465},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Prototype alignment with dedicated experts for test-agnostic long-tailed recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDSC-net: Multi-modal discriminative sparse coding driven
RGB-d classification network. <em>TMM</em>, <em>27</em>, 442–454. (<a
href="https://doi.org/10.1109/TMM.2024.3521720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel sparsity-driven deep neural network to solve the RGB-D image classification problem. Different from existing classification networks, our network architecture is designed by drawing inspirations from a new proposed multi-modal discriminative sparse coding (MDSC) model. The key feature of this model is that it can gradually separate the discriminative and non-discriminative features in RGB-D images in a coarse-to-fine manner. Only the discriminative features are integrated and refined for classification, while the non-discriminative features are discarded, to improve the classification accuracy and efficiency. Derived from the MDSC model, the proposed network is composed of three modules, i.e., the shared feature extraction (SFE) module, discriminative feature refinement (DFR) module, and classification module. The architecture of each module is derived from the optimization solution in the MDSC model. To the best of our knowledge, this is the first time a fully sparsity-driven network has been proposed for RGB-D image classification. Extensive results verify the effectiveness of our method on different RGB-D image datasets.},
  archive      = {J_TMM},
  author       = {Jingyi Xu and Xin Deng and Yibing Fu and Mai Xu and Shengxi Li},
  doi          = {10.1109/TMM.2024.3521720},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {442-454},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MDSC-net: Multi-modal discriminative sparse coding driven RGB-D classification network},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-enriched contrastive loss: Enhancing presentation of
inherent sample connections in contrastive learning framework.
<em>TMM</em>, <em>27</em>, 429–441. (<a
href="https://doi.org/10.1109/TMM.2024.3521796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples, similar to self-supervised learning. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.},
  archive      = {J_TMM},
  author       = {Haojin Deng and Yimin Yang},
  doi          = {10.1109/TMM.2024.3521796},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {429-441},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Context-enriched contrastive loss: Enhancing presentation of inherent sample connections in contrastive learning framework},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WHANet: Wavelet-based hybrid asymmetric network for spectral
super-resolution from RGB inputs. <em>TMM</em>, <em>27</em>, 414–428.
(<a href="https://doi.org/10.1109/TMM.2024.3521713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction from three to dozens of spectral bands, known as spectral super resolution (SSR) has achieved remarkable progress with the continuous development of deep learning. However, the reconstructed hyperspectral images (HSIs) still suffer from the spatial degeneration due to the insufficient retention of high-frequency (HF) information during the SSR process. To remedy this issue, a novel Wavelet-based Hybrid Asymmetric Network (WHANet) is proposed to establish a RGB-to-HSI translation in wavelet domain, thus reserving and emphasizing the HF features in hyperspectral space. Basically, the backbone is designed in a hybrid asymmetric structure that learns the exact representations of decomposed wavelet coefficients in hyperspectral domain in a parallel way. Innovatively, a CNN-based HF reconstruction module (HFRM) and a transformer-based low frequency (LF) reconstruction module (LFRM) are delicately devised to perform the SSR process individually, which are able to process the discriminative wavelet coefficients contrapuntally. Furthermore, a hybrid loss function incorporated with the Fast Fourier loss (FFL) is proposed to directly regularize and emphasis the missing HF components. Eventually, experimental results over three benchmark datasets and one remote sensing dataset demonstrate that our WHANet is able to reach the state-of-the-art performance quantitatively and qualitatively.},
  archive      = {J_TMM},
  author       = {Nan Wang and Shaohui Mei and Yi Wang and Yifan Zhang and Duo Zhan},
  doi          = {10.1109/TMM.2024.3521713},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {414-428},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WHANet: Wavelet-based hybrid asymmetric network for spectral super-resolution from RGB inputs},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPT4Ego: Unleashing the potential of pre-trained models for
zero-shot egocentric action recognition. <em>TMM</em>, <em>27</em>,
401–413. (<a href="https://doi.org/10.1109/TMM.2024.3521658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-Language Models (VLMs), pre-trained on large-scale datasets, have shown impressive performance in various visual recognition tasks. This advancement paves the way for notable performance in some egocentric tasks, Zero-Shot Egocentric Action Recognition (ZS-EAR), entailing VLMs zero-shot to recognize actions from first-person videos enriched in more realistic human-environment interactions. Typically, VLMs handle ZS-EAR as a global video-text matching task, which often leads to suboptimal alignment of vision and linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs, emphasizing fine-grained concept-description alignment that capitalizes on the rich semantic and contextual details in egocentric videos. In this work, we introduce a straightforward yet remarkably potent VLM framework, aka GPT4Ego, designed to enhance the fine-grained alignment of concept and description between vision and language. Specifically, we first propose a new Ego-oriented Text Prompting (EgoTP$\spadesuit$) scheme, which effectively prompts action-related text-contextual semantics by evolving word-level class names to sentence-level contextual descriptions by ChatGPT with well-designed chain-of-thought textual prompts. Moreover, we design a new Ego-oriented Visual Parsing (EgoVP$\clubsuit$) strategy that learns action-related vision-contextual semantics by refining global-level images to part-level contextual concepts with the help of SAM. Extensive experiments demonstrate GPT4Ego significantly outperforms existing VLMs on three large-scale egocentric video benchmarks, i.e., EPIC-KITCHENS-100 (33.2%$\uparrow$$_{\bm {+9.4}}$), EGTEA (39.6%$\uparrow$$_{\bm {+5.5}}$), and CharadesEgo (31.5%$\uparrow$$_{\bm {+2.6}}$). In addition, benefiting from the novel mechanism of fine-grained concept and description alignment, GPT4Ego can sustainably evolve with the advancement of ever-growing pre-trained foundational models. We hope this work can encourage the egocentric community to build more investigation into pre-trained vision-language models.},
  archive      = {J_TMM},
  author       = {Guangzhao Dai and Xiangbo Shu and Wenhao Wu and Rui Yan and Jiachao Zhang},
  doi          = {10.1109/TMM.2024.3521658},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {401-413},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GPT4Ego: Unleashing the potential of pre-trained models for zero-shot egocentric action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuromorphic vision-based motion segmentation with graph
transformer neural network. <em>TMM</em>, <em>27</em>, 385–400. (<a
href="https://doi.org/10.1109/TMM.2024.3521662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving object segmentation is critical to interpret scene dynamics for robotic navigation systems in challenging environments. Neuromorphic vision sensors are tailored for motion perception due to their asynchronous nature, high temporal resolution, and reduced power consumption. However, their unconventional output requires novel perception paradigms to leverage their spatially sparse and temporally dense nature. In this work, we propose a novel event-based motion segmentation algorithm using a Graph Transformer Neural Network, dubbed GTNN. Our proposed algorithm processes event streams as 3D graphs by a series of nonlinear transformations to unveil local and global spatiotemporal correlations between events. Based on these correlations, events belonging to moving objects are segmented from the background without prior knowledge of the dynamic scene geometry. The algorithm is trained on publicly available datasets including MOD, EV-IMO, and EV-IMO2 using the proposed training scheme to facilitate efficient training on extensive datasets. Moreover, we introduce the Dynamic Object Mask-aware Event Labeling (DOMEL) approach for generating approximate ground-truth labels for event-based motion segmentation datasets. We use DOMEL to label our own recorded Event dataset for Motion Segmentation (EMS-DOMEL), which we release to the public for further research and benchmarking. Rigorous experiments are conducted on several unseen publicly-available datasets where the results revealed that GTNN outperforms state-of-the-art methods in the presence of dynamic background variations, motion patterns, and multiple dynamic objects with varying sizes and velocities. GTNN achieves significant performance gains with an average increase of 9.4% and 4.5% in terms of motion segmentation accuracy (IoU%) and detection rate (DR%), respectively.},
  archive      = {J_TMM},
  author       = {Yusra Alkendi and Rana Azzam and Sajid Javed and Lakmal Seneviratne and Yahya Zweiri},
  doi          = {10.1109/TMM.2024.3521662},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {385-400},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neuromorphic vision-based motion segmentation with graph transformer neural network},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving image inpainting via adversarial collaborative
training. <em>TMM</em>, <em>27</em>, 356–370. (<a
href="https://doi.org/10.1109/TMM.2024.3521800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting aims to restore visually realistic contents from a corrupted image, while inpainting forensic methods focus on locating the inpainted regions to fight against inpainting manipulations. Motivated by these two mutually interdependent tasks, in this paper, we propose a novel image inpainting network called Adversarial Collaborative Network (AdvColabNet), which leverages the contradictory and collaborative information from the two tasks of image inpainting and inpainting forensics to enhance the progress of the inpainting model through adversarial collaborative training. Specifically, the proposed AdvColabNet is a coarse-to-fine two-stage framework. In the coarse training stage, a simple generative adversarial model-based U-Net-style network generates initial coarse inpainting results. In the fine stage, the authenticity of inpainting results is assessed using the estimated forensic mask. A forensics-driven adaptive weighting refinement strategy is developed to emphasize learning from pixels with higher probabilities of being inpainted, which helps the network to focus on the challenging regions, resulting in more plausible inpainting results. Comprehensive evaluations on the CelebA-HQ and Places2 datasets demonstrate that our method achieves state-of-the-art robustness performance in terms of PSNR, SSIM, MAE, FID, and LPIPS metrics. We also show that our method effectively deceives the proposed inpainting forensic method compared to state-of-the-art inpainting methods, further demonstrating the superiority of the proposed method.},
  archive      = {J_TMM},
  author       = {Li Huang and Yaping Huang and Qingji Guan},
  doi          = {10.1109/TMM.2024.3521800},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {356-370},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving image inpainting via adversarial collaborative training},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auxiliary representation guided network for visible-infrared
person re-identification. <em>TMM</em>, <em>27</em>, 340–355. (<a
href="https://doi.org/10.1109/TMM.2024.3521773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-identification aims to retrieve images of specific identities across modalities. To relieve the large cross-modality discrepancy, researchers introduce the auxiliary modality within the image space to assist modality-invariant representation learning. However, the challenge persists in constraining the inherent quality of generated auxiliary images, further leading to a bottleneck in retrieval performance. In this paper, we propose a novel Auxiliary Representation Guided Network (ARGN) to explore the potential of auxiliary representations, which are directly generated within the modality-shared embedding space. In contrast to the original visible and infrared representations, which contain information solely from their respective modalities, these auxiliary representations integrate cross-modality information by fusing both modalities. In our framework, we utilize these auxiliary representations as modality guidance to reduce the cross-modality discrepancy. First, we propose a High-quality Auxiliary Representation Learning (HARL) framework to generate identity-consistent auxiliary representations. The primary objective of our HARL is to ensure that auxiliary representations capture diverse modality information from both modalities while concurrently preserving identity-related discrimination. Second, guided by these auxiliary representations, we design an Auxiliary Representation Guided Constraint (ARGC) to optimize the modality-shared embedding space. By incorporating this constraint, the modality-shared embedding space is optimized to achieve enhanced intra-identity compactness and inter-identity separability, further improving the retrieval performance. In addition, to improve the robustness of our framework against the modality variation, we introduce a Part-based Adaptive Gaussian Module (PAGM) to adaptively extract discriminative information across modalities. Finally, extensive experiments are conducted to demonstrate the superiority of our method over state-of-the-art approaches on three VI-ReID datasets.},
  archive      = {J_TMM},
  author       = {Mengzan Qi and Sixian Chan and Chen Hang and Guixu Zhang and Tieyong Zeng and Zhi Li},
  doi          = {10.1109/TMM.2024.3521773},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {340-355},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Auxiliary representation guided network for visible-infrared person re-identification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointAttention: Rethinking feature representation and
propagation in point cloud. <em>TMM</em>, <em>27</em>, 327–339. (<a
href="https://doi.org/10.1109/TMM.2024.3521745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention mechanisms have revolutionized natural language processing and computer vision. However, in point cloud analysis, most existing methods focus on point convolution operators for feature extraction, but fail to model long-range and hierarchical dependencies. To overcome above issues, in this paper, we present PointAttention, a novel network for point cloud feature representation and propagation. Specifically, this architecture uses a two-stage Learnable Self-attention for long-range attention weights learning, which is more effective than conventional triple attention. Furthermore, it employs a Hierarchical Learnable Attention Mechanism to formulate momentous global prior representation and perform fine-grained context understanding, which enables our framework to break through the limitation of the receptive field and reduce the loss of contexts. Interestingly, we show that the proposed Learnable Self-attention is equivalent to the coupling of two Softmax attention operations while having lower complexity. Extensive experiments demonstrate that our network achieves highly competitive performance on several challenging publicly available benchmarks, including point cloud classification on ScanObjectNN and ModelNet40, and part segmentation on ShapeNet-Part.},
  archive      = {J_TMM},
  author       = {Shichao Zhang and Yibo Ding and Tianxiang Huo and Shukai Duan and Lidan Wang},
  doi          = {10.1109/TMM.2024.3521745},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {327-339},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PointAttention: Rethinking feature representation and propagation in point cloud},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Primary code guided targeted attack against cross-modal
hashing retrieval. <em>TMM</em>, <em>27</em>, 312–326. (<a
href="https://doi.org/10.1109/TMM.2024.3521697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing algorithms have demonstrated considerable success in recent years, particularly in cross-modal retrieval tasks. Although hash-based cross-modal retrieval methods have demonstrated considerable efficacy, the vulnerability of deep networks to adversarial examples represents a significant challenge for the hash retrieval. In the absence of target semantics, previous non-targeted attack methods attempt to attack depth models by adding disturbance to the input data, yielding some positive outcomes. Nevertheless, they still lack specific instance-level hash codes and fail to consider the diversity and semantic association of different modalities, which is insufficient to meet the attacker&#39;s expectations. In response, we present a novel Primary code Guided Targeted Attack (PGTA) against cross-modal hashing retrieval. Specifically, we integrate cross-modal instances and labels to obtain well-fused target semantics, thereby enhancing cross-modal interaction. Secondly, the primary code is designed to generate discriminable information with fine-grained semantics for target labels. Benign samples and target semantics collectively generate adversarial examples under the guidance of primary codes, thereby enhancing the efficacy of targeted attacks. Extensive experiments demonstrate that our PGTA outperforms the most advanced methods on three datasets, achieving State-of-the-Art targeted attack performance.},
  archive      = {J_TMM},
  author       = {Xinru Guo and Huaxiang Zhang and Li Liu and Dongmei Liu and Xu Lu and Hui Meng},
  doi          = {10.1109/TMM.2024.3521697},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {312-326},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Primary code guided targeted attack against cross-modal hashing retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-perspective pseudo-label generation and
confidence-weighted training for semi-supervised semantic segmentation.
<em>TMM</em>, <em>27</em>, 300–311. (<a
href="https://doi.org/10.1109/TMM.2024.3521801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-training has been shown to achieve remarkable gains in semi-supervised semantic segmentation by creating pseudo-labels using unlabeled data. This approach, however, suffers from the quality of the generated pseudo-labels, and generating higher quality pseudo-labels is the main challenge that needs to be addressed. In this paper, we propose a novel method for semi-supervised semantic segmentation based on Multi-perspective pseudo-label Generation and Confidence-weighted Training (MGCT). First, we present a multi-perspective pseudo-label generation strategy that considers both global and local semantic perspectives. This strategy prioritizes pixels in all images by the global and local predictions, and subsequently generates pseudo-labels for different pixels in stages according to the ranking results. Our pseudo-label generation method shows superior suitability for semi-supervised semantic segmentation compared to other approaches. Second, we propose a confidence-weighted training method to alleviate performance degradation caused by unstable pixels. Our training method assigns confident weights to unstable pixels, which reduces the interference of unstable pixels during training and facilitates the efficient training of the model. Finally, we validate our approach on the PASCAL VOC 2012 and Cityscapes datasets, and the results indicate that we achieve new state-of-the-art performance on both datasets in all settings.},
  archive      = {J_TMM},
  author       = {Kai Hu and Xiaobo Chen and Zhineng Chen and Yuan Zhang and Xieping Gao},
  doi          = {10.1109/TMM.2024.3521801},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {300-311},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-perspective pseudo-label generation and confidence-weighted training for semi-supervised semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Focus entirety and perceive environment for arbitrary-shaped
text detection. <em>TMM</em>, <em>27</em>, 287–299. (<a
href="https://doi.org/10.1109/TMM.2024.3521797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the diversity of scene text in aspects such as font, color, shape, and size, accurately and efficiently detecting text is still a formidable challenge. Among the various detection approaches, segmentation-based approaches have emerged as prominent contenders owing to their flexible pixel-level predictions. However, these methods typically model text instances in a bottom-up manner, which is highly susceptible to noise. In addition, the prediction of pixels is isolated without introducing pixel-feature interaction, which also influences the detection performance. To alleviate these problems, we propose a multi-information level arbitrary-shaped text detector consisting of a focus entirety module (FEM) and a perceive environment module (PEM). The former extracts instance-level features and adopts a top-down scheme to model texts to reduce the influence of noises. Specifically, it assigns consistent entirety information to pixels within the same instance to improve their cohesion. In addition, it emphasizes the scale information, enabling the model to distinguish varying scale texts effectively. The latter extracts region-level information and encourages the model to focus on the distribution of positive samples in the vicinity of a pixel, which perceives environment information. It treats the kernel pixels as positive samples and helps the model differentiate text and kernel features. Extensive experiments demonstrate the FEM&#39;s ability to efficiently support the model in handling different scale texts and confirm the PEM can assist in perceiving pixels more accurately by focusing on pixel vicinities. Comparisons show the proposed model outperforms existing state-of-the-art approaches on four public datasets.},
  archive      = {J_TMM},
  author       = {Xu Han and Junyu Gao and Chuang Yang and Yuan Yuan and Qi Wang},
  doi          = {10.1109/TMM.2024.3521797},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {287-299},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Focus entirety and perceive environment for arbitrary-shaped text detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDE2D: Semantic-guided discriminability enhancement feature
detector and descriptor. <em>TMM</em>, <em>27</em>, 275–286. (<a
href="https://doi.org/10.1109/TMM.2024.3521748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local feature detectors and descriptors serve various computer vision tasks, such as image matching, visual localization, and 3D reconstruction. To address the extreme variations of rotation and light in the real world, most detectors and descriptors capture as much invariance as possible. However, these methods ignore feature discriminability and perform poorly in indoor scenes. Indoor scenes have too many weak-textured and even repeatedly textured regions, so it is necessary for the extracted features to possess sufficient discriminability. Therefore, we propose a semantic-guided method (called SDE2D) enhancing feature discriminability to improve the performance of descriptors for indoor scenes. We develop a kind of semantic-guided discriminability enhancement (SDE) loss function that uses semantic information from indoor scenes. To the best of our knowledge, this is the first deep research that applies semantic segmentation to enhance discriminability. In addition, we design a novel framework that allows semantic segmentation network to be well embedded as a module in the overall framework and provides guidance information for training. Besides, we explore the impact of different semantic segmentation models on our method. The experimental results on indoor scenes datasets demonstrate that the proposed SDE2D performs well compared with the state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Jiapeng Li and Ruonan Zhang and Ge Li and Thomas H. Li},
  doi          = {10.1109/TMM.2024.3521748},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {275-286},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SDE2D: Semantic-guided discriminability enhancement feature detector and descriptor},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polarization state attention dehazing network with a
simulated polar-haze dataset. <em>TMM</em>, <em>27</em>, 263–274. (<a
href="https://doi.org/10.1109/TMM.2024.3521827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing under harsh weather conditions remains a challenging and ill-posed problem. In addition, acquiring real-time haze-free counterparts of hazy images poses difficulties. Existing approaches commonly synthesize hazy data by relying on estimated depth information, which is prone to errors due to its physical unreliability. While generative networks can transfer some hazy features to clear images, the resulting hazy images still exhibit an artificial appearance. In this paper, we introduce polarization cues to propose a haze simulation strategy to synthesize hazy data, ensuring visually pleasing results that adhere to physical laws. Leveraging on the simulated Polar-Haze dataset, we present a polarization state attention dehazing network (PSADNet), which consists of a polarization extraction module and a polarization dehazing module. The proposed polarization extraction model incorporates an attention mechanism to capture high-level image features related to polarization and chromaticity. The polarization dehazing module utilizes these features derived from the polarization analysis to enhance image dehazing capabilities while preserving the accuracy of the polarization information. Promising results are observed in both qualitative and quantitative experiments, supporting the effectiveness of the proposed PSADNet and the validity of polarization-based haze simulation strategy.},
  archive      = {J_TMM},
  author       = {Sijia Wen and Yinqiang Zheng and Feng Lu},
  doi          = {10.1109/TMM.2024.3521827},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {263-274},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Polarization state attention dehazing network with a simulated polar-haze dataset},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNP-AUT: Image compression using double-layer non-uniform
partition and adaptive u transform. <em>TMM</em>, <em>27</em>, 249–262.
(<a href="https://doi.org/10.1109/TMM.2024.3521853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To provide an image compression method with better compression performance and lower computational complexity, a new image compression algorithm is proposed in this paper. First, a double-layer non-uniform partition algorithm is proposed, which analyzes the texture complexity of image blocks and performs partitioning and merging of the image blocks at different scales to provide a priori information that helps to reduce the spatial redundancy for subsequent compression against the blocks. Next, by considering the multi-transform cores, we propose an adaptive U transform scheme, which performs more specific coding for different types of image blocks to enhance the coding performance. Finally, in order that the bit allocation can be more flexible and accurate, a fully adaptive quantization technique is proposed. It not only formulates the quantization coefficient relationship between image blocks of different sizes but also further refines the quantization coefficient relationship between image blocks under different topologies. Extensive experiments indicate that the compression performance of the proposed algorithm not only significantly surpasses the JPEG but also surpasses some state-of-the-art compression algorithms with similar computational complexity. In addition, compared with the JPEG2000 compression algorithm, which has greater with higher computational complexity, its compression performance also has certain advantages.},
  archive      = {J_TMM},
  author       = {Yumo Zhang and Zhanchuan Cai},
  doi          = {10.1109/TMM.2024.3521853},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {249-262},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DNP-AUT: Image compression using double-layer non-uniform partition and adaptive u transform},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive region-to-boundary exploration network for
camouflaged object detection. <em>TMM</em>, <em>27</em>, 236–248. (<a
href="https://doi.org/10.1109/TMM.2024.3521761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to segment targeted objects that have similar colors, textures, or shapes to their background environment. Due to the limited ability in distinguishing highly similar patterns, existing COD methods usually produce inaccurate predictions, especially around the boundary areas, when coping with complex scenes. This paper proposes a Progressive Region-to-Boundary Exploration Network (PRBE-Net) to accurately detect camouflaged objects. PRBE-Net follows an encoder-decoder framework and includes three key modules. Specifically, firstly, both high-level and low-level features of the encoder are integrated by a region and boundary exploration module to explore their complementary information for extracting the object&#39;s coarse region and fine boundary cues simultaneously. Secondly, taking the region cues as the guidance information, a Region Enhancement (RE) module is used to adaptively localize and enhance the region information at each layer of the encoder. Subsequently, considering that camouflaged objects usually have blurry boundaries, a Boundary Refinement (BR) decoder is used after the RE module to better detect the boundary areas with the assistance of boundary cues. Through top-down deep supervision, PRBE-Net can progressively refine the prediction. Extensive experiments on four datasets indicate that our PRBE-Net achieves superior results over 21 state-of-the-art COD methods. Additionally, it also shows good results on polyp segmentation, a COD-related task in the medical field.},
  archive      = {J_TMM},
  author       = {Guanghui Yue and Shangjie Wu and Tianwei Zhou and Gang Li and Jie Du and Yu Luo and Qiuping Jiang},
  doi          = {10.1109/TMM.2024.3521761},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {236-248},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive region-to-boundary exploration network for camouflaged object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video instance segmentation without using mask and identity
supervision. <em>TMM</em>, <em>27</em>, 224–235. (<a
href="https://doi.org/10.1109/TMM.2024.3521668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video instance segmentation (VIS) is a challenging vision problem in which the task is to simultaneously detect, segment, and track all the object instances in a video. Most existing VIS approaches rely on pixel-level mask supervision within a frame as well as instance-level identity annotation across frames. However, obtaining these ‘mask and identity’ annotations is time-consuming and expensive. We propose the first mask-identity-free VIS framework that neither utilizes mask annotations nor requires identity supervision. Accordingly, we introduce a query contrast and exchange network (QCEN) comprising instance query contrast and query-exchanged mask learning. The instance query contrast first performs cross-frame instance matching and then conducts query feature contrastive learning. The query-exchanged mask learning exploits both intra-video and inter-video query exchange properties: exchanging queries of an identical instance from different frames within a video results in consistent instance masks, whereas exchanging queries across videos results in all-zero background masks. Extensive experiments on three benchmarks (YouTube-VIS 2019, YouTube-VIS 2021, and OVIS) reveal the merits of the proposed approach, which significantly reduces the performance gap between the identify-free baseline and our mask-identify-free VIS method. On the YouTube-VIS 2019 validation set, our mask-identity-free approach achieves 91.4% of the stronger-supervision-based baseline performance when utilizing the same ImageNet pre-trained model.},
  archive      = {J_TMM},
  author       = {Ge Li and Jiale Cao and Hanqing Sun and Rao Muhammad Anwer and Jin Xie and Fahad Khan and Yanwei Pang},
  doi          = {10.1109/TMM.2024.3521668},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {224-235},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video instance segmentation without using mask and identity supervision},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal cognitive consensus guided audio–visual
segmentation. <em>TMM</em>, <em>27</em>, 209–223. (<a
href="https://doi.org/10.1109/TMM.2024.3521746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-Visual Segmentation (AVS) aims to extract the sounding object from a video frame, which is represented by a pixel-wise segmentation mask for application scenarios such as multi-modal video editing, augmented reality, and intelligent robot systems. The pioneering work conducts this task through dense feature-level audio-visual interaction, which ignores the dimension gap between different modalities. More specifically, the audio clip could only provide a Global semantic label in each sequence, but the video frame covers multiple semantic objects across different Local regions, which leads to mislocalization of the representationally similar but semantically different object. In this paper, we propose a Cross-modal Cognitive Consensus guided Network (C3N) to align the audio-visual semantics from the global dimension and progressively inject them into the local regions via an attention mechanism. Firstly, a Cross-modal Cognitive Consensus Inference Module (C3IM) is developed to extract a unified-modal label by integrating audio/visual classification confidence and similarities of modality-agnostic label embeddings. Then, we feed the unified-modal label back to the visual backbone as the explicit semantic-level guidance via a Cognitive Consensus guided Attention Module (CCAM), which highlights the local features corresponding to the interested object. Extensive experiments on the Single Sound Source Segmentation (S4) setting and Multiple Sound Source Segmentation (MS3) setting of the AVSBench dataset demonstrate the effectiveness of the proposed method, which achieves state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Zhaofeng Shi and Qingbo Wu and Fanman Meng and Linfeng Xu and Hongliang Li},
  doi          = {10.1109/TMM.2024.3521746},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {209-223},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal cognitive consensus guided Audio–Visual segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision transformer with relation exploration for pedestrian
attribute recognition. <em>TMM</em>, <em>27</em>, 198–208. (<a
href="https://doi.org/10.1109/TMM.2024.3521677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian attribute recognition has achieved high accuracy by exploring the relations between image regions and attributes. However, existing methods typically adopt features directly extracted from the backbone or utilize a single structure (e.g., transformer) to explore the relations, leading to inefficient and incomplete relation mining. To overcome these limitations, this paper proposes a comprehensive relationship framework called Vision Transformer with Relation Exploration (ViT-RE) for pedestrian attribute recognition, which includes two novel modules, namely Attribute and Contextual Feature Projection (ACFP) and Relation Exploration Module (REM). In ACFP, attribute-specific features and contextual-aware features are learned individually to capture discriminative information tailored for attributes and image regions, respectively. Then, REM employs Graph Convolutional Network (GCN) Blocks and Transformer Blocks to concurrently explore attribute, contextual, and attribute-contextual relations. To enable fine-grained relation mining, a Dynamic Adjacency Module (DAM) is further proposed to construct instance-wise adjacency matrix for the GCN Block. Equipped with comprehensive relation information, ViT-RE achieves promising performance on three popular benchmarks, including PETA, RAP, and PA-100 K datasets. Moreover, ViT-RE achieves the first place in the WACV 2023 UPAR Challenge.},
  archive      = {J_TMM},
  author       = {Hao Tan and Zichang Tan and Dunfang Weng and Ajian Liu and Jun Wan and Zhen Lei and Stan Z. Li},
  doi          = {10.1109/TMM.2024.3521677},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {198-208},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Vision transformer with relation exploration for pedestrian attribute recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring local and global consistent correlation on
hypergraph for rotation invariant point cloud analysis. <em>TMM</em>,
<em>27</em>, 186–197. (<a
href="https://doi.org/10.1109/TMM.2024.3521678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotation invariant point cloud analysis is essential for many real-world applications where objects can appear in arbitrary orientations. Traditional local rotation-invariant methods rely on lossy region descriptors, limiting the global comprehension of 3D objects. Conversely, global features derived from pose alignment can capture complementary information. To leverage both local and global consistency for enhanced accuracy, we propose the Global-Local-Consistent Hypergraph Cross-Attention Network (GLC-HCAN). This framework includes the Global Consistent Feature (GCF) representation branch, the Local Consistent Feature (LCF) representation branch, and the Hypergraph Cross-Attention (HyperCA) network to model complex correlations through the global-local-consistent hypergraph representation learning. Specifically, the GCF branch employs a multi-pose grouping and aggregation strategy based on PCA for improved global comprehension. Simultaneously, the LCF branch uses local farthest reference point features to enhance local region descriptions. To capture high-order and complex global-local correlations, we construct hypergraphs that integrate both features, mutually enhancing and fusing the representations. The inductive HyperCA module leverages attention techniques to better utilize these high-order relations for comprehensive understanding. Consequently, GLC-HCAN offers an effective and robust rotation-invariant point cloud analysis network, suitable for object classification and shape retrieval tasks in SO(3). Experimental results on both synthetic and scanned point cloud datasets demonstrate that GLC-HCAN outperforms state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yue Dai and Shihui Ying and Yue Gao},
  doi          = {10.1109/TMM.2024.3521678},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {186-197},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring local and global consistent correlation on hypergraph for rotation invariant point cloud analysis},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality-guided skin tone enhancement for portrait
photography. <em>TMM</em>, <em>27</em>, 171–185. (<a
href="https://doi.org/10.1109/TMM.2024.3521829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, learning-based color and tone enhancement methods for photos have become increasingly popular. However, most learning-based image enhancement methods just learn a mapping from one distribution to another based on one dataset, lacking the ability to adjust images continuously and controllably. It is important to enable the learning-based enhancement models to adjust an image continuously, since in many cases we may want to get a slighter or stronger enhancement effect rather than one fixed adjusted result. In this paper, we propose a quality-guided image enhancement paradigm that enables image enhancement models to learn the distribution of images with various quality ratings. By learning this distribution, image enhancement models can associate image features with their corresponding perceptual qualities, which can be used to adjust images continuously according to different quality scores. To validate the effectiveness of our proposed method, a subjective quality assessment experiment is first conducted, focusing on skin tone adjustment in portrait photography. Guided by the subjective quality ratings obtained from this experiment, our method can adjust the skin tone corresponding to different quality requirements. Furthermore, an experiment conducted on 10 natural raw images corroborates the effectiveness of our model in situations with fewer subjects and fewer shots, and also demonstrates its general applicability to natural images.},
  archive      = {J_TMM},
  author       = {Shiqi Gao and Huiyu Duan and Xinyue Li and Kang Fu and Yicong Peng and Qihang Xu and Yuanyuan Chang and Jia Wang and Xiongkuo Min and Guangtao Zhai},
  doi          = {10.1109/TMM.2024.3521829},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {171-185},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Quality-guided skin tone enhancement for portrait photography},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disaggregation distillation for person search. <em>TMM</em>,
<em>27</em>, 158–170. (<a
href="https://doi.org/10.1109/TMM.2024.3521732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search is a challenging task in computer vision and multimedia understanding, which aims at localizing and identifying target individuals in realistic scenes. State-of-the-art models achieve remarkable success but suffer from overloaded computation and inefficient inference, making them impractical in most real-world applications. A promising approach to tackle this dilemma is to compress person search models with knowledge distillation (KD). Previous KD-based person search methods typically distill the knowledge from the re-identification (re-id) branch, completely overlooking the useful knowledge from the detection branch. In addition, we elucidate that the imbalance between person and background regions in feature maps has a negative impact on the distillation process. To this end, we propose a novel KD-based approach, namely Disaggregation Distillation for Person Search (DDPS), which disaggregates the distillation process and feature maps, respectively. Firstly, the distillation process is disaggregated into two task-oriented sub-processes, i.e., detection distillation and re-id distillation, to help the student learn both accurate localization capability and discriminative person embeddings. Secondly, we disaggregate each feature map into person and background regions, and distill these two regions independently to alleviate the imbalance problem. More concretely, three types of distillation modules, i.e., logit distillation (LD), correlation distillation (CD), and disaggregation feature distillation (DFD), are particularly designed to transfer comprehensive information from the teacher to the student. Note that such a simple yet effective distillation scheme can be readily applied to both homogeneous and heterogeneous teacher-student combinations. We conduct extensive experiments on two person search benchmarks, where the results demonstrate that, surprisingly, our DDPS enables the student model to surpass the performance of the corresponding teacher model, even achieving comparable results with general person search models.},
  archive      = {J_TMM},
  author       = {Yizhen Jia and Rong Quan and Haiyan Chen and Jiamei Liu and Yichao Yan and Song Bai and Jie Qin},
  doi          = {10.1109/TMM.2024.3521732},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {158-170},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disaggregation distillation for person search},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRA-det: Anchor-free oriented object detection with polar
radius representation. <em>TMM</em>, <em>27</em>, 145–157. (<a
href="https://doi.org/10.1109/TMM.2024.3521683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented object detection typically adds an additional rotation angle to the regressed horizontal bounding box (HBB) for representing the oriented bounding box (OBB). However, existing oriented object detectors based on regression angles face inconsistency between metric and loss, boundary discontinuity or square-like problems. To solve the above problems, we propose an anchor-free oriented object detector named PRA-Det, which assigns the center region of the object to regress OBBs represented by the polar radius vectors. Specifically, the proposed PRA-Det introduces a diamond-shaped positive region of category-wise attention factor to assign positive sample points to regress polar radius vectors. PRA-Det regresses the polar radius vector of the edges from the assigned sample points as the regression target and suppresses the predicted low-quality polar radius vectors through the category-wise attention factor. The OBBs defined for different protocols are uniformly encoded by the polar radius encoding module into regression targets represented by polar radius vectors. Therefore, the regression target represented by the polar radius vector does not have angle parameters during training, thus solving the angle-sensitive boundary discontinuity and square-like problems. To optimize the predicted polar radius vector, we design a spatial geometry loss to improve the detection accuracy. Furthermore, in the inference stage, the center offset score of the polar radius vector is combined with the classification score as the confidence to alleviate the inconsistency between classification and regression. The extensive experiments on public benchmarks demonstrate that the PRA-Det is highly competitive with state-of-the-art oriented object detectors and outperforms other comparison methods.},
  archive      = {J_TMM},
  author       = {Min Dang and Gang Liu and Hao Li and Di Wang and Rong Pan and Quan Wang},
  doi          = {10.1109/TMM.2024.3521683},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {145-157},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PRA-det: Anchor-free oriented object detection with polar radius representation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D shape segmentation with potential consistency mining and
enhancement. <em>TMM</em>, <em>27</em>, 133–144. (<a
href="https://doi.org/10.1109/TMM.2024.3521674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D shape segmentation is a crucial task in the field of multimedia analysis and processing, and recent years have seen a surge in research on this topic. However, many existing methods only consider geometric features of 3D shapes and fail to explore the potential connections between faces, limiting their segmentation performance. In this paper, we propose a novel segmentation approach that mines and enhances the potential consistency of 3D shapes to overcome this limitation. The key idea is to mine the consistency between different partitions of 3D shapes and to use the unique consistency enhancement strategy to continuously optimize the consistency features for the network. Our method also includes a comprehensive set of network structures to mine and enhance consistent features, enabling more effective feature extraction and better utilization of contextual information around each face when processing complex shapes. We evaluate our approach on public benchmarks through extensive experiments and demonstrate its effectiveness in achieving higher accuracy than existing methods.},
  archive      = {J_TMM},
  author       = {Zhenyu Shu and Shiyang Li and Shiqing Xin and Ligang Liu},
  doi          = {10.1109/TMM.2024.3521674},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {133-144},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3D shape segmentation with potential consistency mining and enhancement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic strategy prompt reasoning for emotional support
conversation. <em>TMM</em>, <em>27</em>, 108–119. (<a
href="https://doi.org/10.1109/TMM.2024.3521669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An emotional support conversation (ESC) system aims to reduce users&#39; emotional distress by engaging in conversation using various reply strategies as guidance. To develop instructive reply strategies for an ESC system, it is essential to consider the dynamic transitions of users&#39; emotional states through the conversational turns. However, existing methods for strategy-guided ESC systems struggle to capture these transitions as they overlook the inference of fine-grained user intentions. This oversight poses a significant obstacle, impeding the model&#39;s ability to derive pertinent strategy information and, consequently, hindering its capacity to generate emotionally supportive responses. To tackle this limitation, we propose a novel dynamic strategy prompt reasoning model (DSR), which leverages sparse context relation deduction to acquire adaptive representation of reply strategies as prompts for guiding the response generation process. Specifically, we first perform turn-level commonsense reasoning with different approaches to extract auxiliary knowledge, which enhances the comprehension of user intention. Then we design a context relation deduction module to dynamically integrate interdependent dialogue information, capturing granular user intentions and generating effective strategy prompts. Finally, we utilize the strategy prompts to guide the generation of more relevant and supportive responses. DSR model is validated through extensive experiments conducted on a benchmark dataset, demonstrating its superior performance compared to the latest competitive methods in the field.},
  archive      = {J_TMM},
  author       = {Yiting Liu and Liang Li and Yunbin Tu and Beichen Zhang and Zheng-Jun Zha and Qingming Huang},
  doi          = {10.1109/TMM.2024.3521669},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {108-119},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic strategy prompt reasoning for emotional support conversation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SQL-net: Semantic query learning for point-supervised
temporal action localization. <em>TMM</em>, <em>27</em>, 84–94. (<a
href="https://doi.org/10.1109/TMM.2024.3521799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-supervised Temporal Action Localization (PS-TAL) detects temporal intervals of actions in untrimmed videos with a label-efficient paradigm. However, most existing methods fail to learn action completeness without instance-level annotations, resulting in fragmentary region predictions. In fact, the semantic information of snippets is crucial for detecting complete actions, meaning that snippets with similar representations should be considered as the same action category. To address this issue, we propose a novel representation refinement framework with a semantic query mechanism to enhance the discriminability of snippet-level features. Concretely, we set a group of learnable queries, each representing a specific action category, and dynamically update them based on the video context. With the assistance of these queries, we expect to search for the optimal action sequence that agrees with their semantics. Besides, we leverage some reliable proposals as pseudo labels and design a refinement and completeness module to refine temporal boundaries further, so that the completeness of action instances is captured. Finally, we demonstrate the superiority of the proposed method over existing state-of-the-art approaches on THUMOS14 and ActivityNet13 benchmarks. Notably, thanks to completeness learning, our algorithm achieves significant improvements under more stringent evaluation metrics.},
  archive      = {J_TMM},
  author       = {Yu Wang and Shengjie Zhao and Shiwei Chen},
  doi          = {10.1109/TMM.2024.3521799},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {84-94},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SQL-net: Semantic query learning for point-supervised temporal action localization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive pitfall: Exploring the effectiveness of adaptation
in skeleton-based action recognition. <em>TMM</em>, <em>27</em>, 56–71.
(<a href="https://doi.org/10.1109/TMM.2024.3521774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolution networks (GCNs) have achieved remarkable performance in skeleton-based action recognition by exploiting the adjacency topology of body representation. However, the adaptive strategy adopted by the previous methods to construct the adjacency matrix is not balanced between the performance and the computational cost. We assume this concept of Adaptive Trap, which can be replaced by multiple autonomous submodules, thereby simultaneously enhancing the dynamic joint representation and effectively reducing network resources. To effectuate the substitution of the adaptive model, we unveil two distinct strategies, both yielding comparable effects. (1) Optimization. Individuality and Commonality GCNs (IC-GCNs) is proposed to specifically optimize the construction method of the associativity adjacency matrix for adaptive processing. The uniqueness and co-occurrence between different joint points and frames in the skeleton topology are effectively captured through methodologies like preferential fusion of physical information, extreme compression of multi-dimensional channels, and simplification of self-attention mechanism. (2) Replacement. Auto-Learning GCNs (AL-GCNs) is proposed to boldly remove popular adaptive modules and cleverly utilize human key points as motion compensation to provide dynamic correlation support. AL-GCNs construct a fully learnable group adjacency matrix in both spatial and temporal dimensions, resulting in an elegant and efficient GCN-based model. In addition, three effective tricks for skeleton-based action recognition (Skip-Block, Bayesian Weight Selection Algorithm, and Simplified Dimensional Attention) are exposed and analyzed in this paper. Finally, we employ the variable channel and grouping method to explore the hardware resource bound of the two proposed models. IC-GCN and AL-GCN exhibit impressive performance across NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA, and UAV-Human datasets, with an exceptional parameter-cost ratio.},
  archive      = {J_TMM},
  author       = {Qiguang Miao and Wentian Xin and Ruyi Liu and Yi Liu and Mengyao Wu and Cheng Shi and Chi-Man Pun},
  doi          = {10.1109/TMM.2024.3521774},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {56-71},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive pitfall: Exploring the effectiveness of adaptation in skeleton-based action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Content-aware tunable selective encryption for HEVC using
sine-modular chaotification model. <em>TMM</em>, <em>27</em>, 41–55. (<a
href="https://doi.org/10.1109/TMM.2024.3521724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing High Efficiency Video Coding (HEVC) selective encryption algorithms only consider the encoding characteristics of syntax elements to keep format compliance, but ignore the semantic features of video content, which may lead to unnecessary computational and bit rate costs. To tackle this problem, we present a content-aware tunable selective encryption (CATSE) scheme for HEVC. First, a deep hashing network is adopted to retrieve groups of pictures (GOPs) containing sensitive objects. Then, the retrieved sensitive GOPs and the remaining insensitive ones are encrypted with different encryption strengths. For the former, multiple syntax elements are encrypted to ensure security, whereas for the latter, only a few bypass-coded syntax elements are encrypted to improve the encryption efficiency and reduce the bit rate overhead. The keystream sequence used is extracted from the time series of a new improved logistic map with complex dynamic behavior, which is generated by our proposed sine-modular chaotification model. Finally, a reversible steganography is applied to embed the flag bits of the GOP type into the encrypted bitstream, so that the decoder can distinguish the encrypted syntax elements that need to be decrypted in different GOPs. Experimental results indicate that the proposed HEVC CATSE scheme not only provides high encryption speed and low bit rate overhead, but also has superior encryption strength than other state-of-the-art HEVC selective encryption algorithms.},
  archive      = {J_TMM},
  author       = {Qingxin Sheng and Chong Fu and Zhaonan Lin and Junxin Chen and Xingwei Wang and Chiu-Wing Sham},
  doi          = {10.1109/TMM.2024.3521724},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {41-55},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Content-aware tunable selective encryption for HEVC using sine-modular chaotification model},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HNR-ISC: Hybrid neural representation for image set
compression. <em>TMM</em>, <em>27</em>, 28–40. (<a
href="https://doi.org/10.1109/TMM.2024.3521715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image set compression (ISC) refers to compressing the sets of semantically similar images. Traditional ISC methods typically aim to eliminate redundancy among images at either signal or frequency domain, but often struggle to handle complex geometric deformations across different images effectively. Here, we propose a new Hybrid Neural Representation for ISC (HNR-ISC), including an implicit neural representation for Semantically Common content Compression (SCC) and an explicit neural representation for Semantically Unique content Compression (SUC). Specifically, SCC enables the conversion of semantically common contents into a small-and-sweet neural representation, along with embeddings that can be conveyed as a bitstream. SUC is composed of invertible modules for removing intra-image redundancies. The feature level combination from SCC and SUC naturally forms the final image set. Experimental results demonstrate the robustness and generalization capability of HNR-ISC in terms of signal and perceptual quality for reconstruction and accuracy for the downstream analysis task.},
  archive      = {J_TMM},
  author       = {Pingping Zhang and Shiqi Wang and Meng Wang and Peilin Chen and Wenhui Wu and Xu Wang and Sam Kwong},
  doi          = {10.1109/TMM.2024.3521715},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {28-40},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HNR-ISC: Hybrid neural representation for image set compression},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LININ: Logic integrated neural inference network for
explanatory visual question answering. <em>TMM</em>, <em>27</em>, 16–27.
(<a href="https://doi.org/10.1109/TMM.2024.3521709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanatory Visual Question Answering (EVQA) is a recently proposed multimodal reasoning task consisting of answering the visual question and generating multimodal explanations for the reasoning processes. Unlike traditional Visual Question Answering (VQA) task that only aims at predicting answers for visual questions, EVQA also aims to generate user-friendly explanations to improve the explainability and credibility of reasoning models. To date, existing methods for VQA and EVQA ignore the prompt in the question and enforce the model to predict the probabilities of all answers. Moreover, existing EVQA methods ignore the complex relationships among question words, visual regions, and explanation tokens. Therefore, in this work, we propose a Logic Integrated Neural Inference Network (LININ) to restrict the range of candidate answers based on first-order-logic (FOL) and capture cross-modal relationships to generate rational explanations. Firstly, we design a FOL-based question analysis program to fetch a small number of candidate answers. Secondly, we utilize a multimodal transformer encoder to extract visual and question features, and conduct the prediction on candidate answers. Finally, we design a multimodal explanation transformer to construct cross-modal relationships and generate rational explanations. Comprehensive experiments on benchmark datasets demonstrate the superiority of LININ compared with the state-of-the-art methods for EVQA.},
  archive      = {J_TMM},
  author       = {Dizhan Xue and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2024.3521709},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {16-27},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LININ: Logic integrated neural inference network for explanatory visual question answering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
