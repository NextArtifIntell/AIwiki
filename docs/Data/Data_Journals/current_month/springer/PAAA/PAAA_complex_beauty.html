<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PAAA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="paaa---35">PAAA - 35</h2>
<ul>
<li><details>
<summary>
(2025). Novel construction methods for picture fuzzy divergence
measures with applications in pattern recognition, MADM, and clustering
analysis. <em>PAAA</em>, <em>28</em>(2), 1–28. (<a
href="https://doi.org/10.1007/s10044-024-01383-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Divergence measure of picture fuzzy sets is a valuable tool to study the problems related to decision-making, pattern classification, and clustering analysis. However, existing divergence/distance measures are sometimes ineffective in capturing the intricacies of uncertainty and imprecision inherent in picture fuzzy sets. In view of the theoretical and experimental weaknesses of existing picture fuzzy divergence/distance measures, this article introduces novel construction methods for deriving picture fuzzy divergence measures. The first one is inductive which utilizes existing intuitionistic fuzzy divergence and the second is based on forming picture fuzzy divergence measures from existing picture fuzzy divergence measures. Additionally, we have shown that restriction on the neutrality degree in picture fuzzy divergence is an intuitionistic fuzzy divergence. Moreover, we suggested a new picture fuzzy divergence measure utilizing the proposed approach and applied it to solve practical problems concerned with decision-making, pattern classification, and clustering analysis. The performance indices “Degree of Confidence” and “Cluster Validity Index” in the picture fuzzy framework further appreciated the advantages of the proposed measures. Comparative studies with existing picture fuzzy distance/divergence measures demonstrated the effectiveness and superiority of the proposed divergence measures.},
  archive      = {J_PAAA},
  author       = {Singh, Surender and Singh, Koushal},
  doi          = {10.1007/s10044-024-01383-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-28},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Novel construction methods for picture fuzzy divergence measures with applications in pattern recognition, MADM, and clustering analysis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated multi-local and global dynamic perception
structure for sign language recognition. <em>PAAA</em>, <em>28</em>(2),
1–14. (<a href="https://doi.org/10.1007/s10044-024-01403-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current sign language recognition methods often focus on whole-body features, neglecting the detailed dynamic information of key body parts. We propose an integrated multi-local and global dynamic perception structure based on a hybrid feature fusion approach that fuses features from multiple local paths into the global path, aiming to benefit from the diverse local information available in videos. First, the multi-local dynamic perception module is designed to extract multiple sign language-related spatial features with fine-grained information on local body dynamics. This module is achieved by expanding multi-local features in the channel dimension, permitting the processing of different perspectives independently for multiple local feature inputs. Moreover, we design a multi-local to global fusion module that generates multi-local fusion representations encompassing both temporal and spatial dimensions. This module integrates the fusion of deep features from multiple local dynamics, to be integrated with shallow features of the global module, achieving a match between the deep features of the multi-local to global fusion module and the shallow features of the global dynamic perception module. Finally, extensive experiments based on several sign language recognition benchmarks demonstrate that our integrated multi-local and global dynamic perception structure effectively improves performance of sign language recognition models, and significantly outperforms a number of competitive baselines.},
  archive      = {J_PAAA},
  author       = {Liang, Siyu and Li, Yunan and Shi, Yuanyuan and Chen, Huizhou and Miao, Qiguang},
  doi          = {10.1007/s10044-024-01403-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Integrated multi-local and global dynamic perception structure for sign language recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing semantic audio-visual representation learning with
supervised multi-scale attention. <em>PAAA</em>, <em>28</em>(2), 1–14.
(<a href="https://doi.org/10.1007/s10044-025-01414-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, breakthroughs in large models such as GPT and Transformers have demonstrated extraordinary versatility and power in various fields and tasks. However, despite significant progress in areas such as natural language processing, these models still face some unique challenges when processing multimodal data. Data from different modalities often contain significantly different characteristics, and the heterogeneity gap between different modalities makes it difficult to fuse these data to extract valuable information. To integrate and align semantic meanings between audio-visual modalities, this paper proposes a novel supervised multi-scale attention for enhancing semantic audio-visual representation learning from multimedia data, utilizing the audio-visual attention mechanism to combine multi-scale features. Specifically, we explore multi-scale feature extraction and audio-visual attention architecture, which computes cross-attention weights based on the correlation between joint feature representations and single-modal representations. In addition, the model is guided to learn powerful discriminative features by minimizing intra-modal and inter-modal discriminative losses and maximizing cross-modal correlations. With the widely used VEGAS and AVE benchmark datasets, our model demonstrates competitive experimental results. Extensive experiments verify that the proposed method significantly outperforms the state-of-the-art cross-modal retrieval methods.},
  archive      = {J_PAAA},
  author       = {Zhang, Jiwei and Yu, Yi and Tang, Suhua and Qi, GuoJun and Wu, Haiyuan and Hachiya, Hirotaka},
  doi          = {10.1007/s10044-025-01414-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing semantic audio-visual representation learning with supervised multi-scale attention},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitask learning of adversarial-contrastive graph for
recommendation. <em>PAAA</em>, <em>28</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-025-01417-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems play a critical role in our daily lives. Despite great progress, existing graph-based recommendation methods still suffer from challenges including skewed data distribution, vulnerability to noises, and sparse supervision signal. We attribute the inferior performance to the limited discriminative ability of the learned representations. To remedy this, in this paper, we develop a framework termed Multi-ACG by introducing self-supervised learning, adversarial learning, and multitask learning to learn representations with higher discrimination. Specifically, self-supervised learning and adversarial learning are first employed to synthesize hard samples for training. Meanwhile, multi-task learning is adopted to balance different loss terms for optimization. Experiments are conducted on benchmark datasets and the results have demonstrated the state-of-the-art performance of the proposed method against previous ones. The code is at https://github.com/xiaoma666123/Multi-ACG.},
  archive      = {J_PAAA},
  author       = {Ma, Xingyu and Wang, Chuanxu},
  doi          = {10.1007/s10044-025-01417-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multitask learning of adversarial-contrastive graph for recommendation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pre-trained noise based unsupervised GAN for fruit disease
classification in imbalanced datasets. <em>PAAA</em>, <em>28</em>(2),
1–19. (<a href="https://doi.org/10.1007/s10044-025-01418-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early disease diagnosis in edible fruits and vegetables is crucial for sustainable economic agricultural production. Recently, deep neural networks have explicitly shown exceptional performance in early disease recognition. However, an insufficient and scarce dataset is a critical issue for training the neural network, which makes dataset acquisition a fundamental obstacle in enhancing the performance of deep network models. A considerable amount of dataset acquisition necessitates an additional, expensive effort owing to time constraints and expert requirements. To mitigate this challenge, a novel data augmentation method, FruitGAN, exploiting the generative adversarial architecture, has been developed. The variational autoencoder transforms the random Gaussian noise into a pre-trained noise vector, which is then input into the proposed FruitGAN method. The FruitGAN method is equipped with a self-attention, residual block, and super-resolution module to maintain tiny lesions, structural integrity, and perceptual quality in the generated fruit images. Moreover, a new real-field eggplant dataset containing the four pathogens and one healthy category, aggregating 1325 samples, has been collected from on-field farms. The proposed FruitGAN method is leveraged to generate real-like synthetic images of the eggplant to avoid class imbalance problems. The effectiveness of FruitGAN is tested on the eggplant dataset in terms of FID and SSIM scores, and the results are compared with seven other State-Of-The-Art GAN models. Furthermore, the classification performance of the FruitGAN-generated dataset has also been tested against nine pre-trained deep networks namely, AlexNet, VGG16, VGG19, ResNet50, ResNet101, DenseNet101, InceptionV3, Xception, and MobileNetV2 and four hybrid networks, namely, InceptionV3 + VGG16, SVM + VGG19, CNN + SVM, and MobileNet + Xception using transfer learning and evaluating the performance by test data. The experimental results affirmed that the developed FruitGAN outperformed all other considered GAN models by achieving 112.88 FID and 0.94 SSIM scores. Moreover, the classification accuracy of the FruitGAN augmented dataset was recorded as 95.74%, which is the highest among other considered GAN models. The code and datasets of the proposed method are available at https://github.com/ersachingupta11/FruitGAN},
  archive      = {J_PAAA},
  author       = {Gupta, Sachin and Tripathi, Ashish Kumar and Lewis, Nkenyereye},
  doi          = {10.1007/s10044-025-01418-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Pre-trained noise based unsupervised GAN for fruit disease classification in imbalanced datasets},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional feature fusion via cross-attention transformer
for chrysanthemum classification. <em>PAAA</em>, <em>28</em>(2), 1–16.
(<a href="https://doi.org/10.1007/s10044-025-01419-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chrysanthemums hold significant ornamental, economic, and medicinal value, with their quality and economic worth heavily influenced by geographic origin. Accurate classification of chrysanthemums is crucial for ensuring product authenticity, boosting consumer trust, and promoting sustainable industry growth. Traditional classification methods, however, suffer from inefficiency and high costs. To address these challenges, we propose a novel chrysanthemum classification method utilizing a bidirectional feature fusion approach via cross-attention and two-stream network fusion. Our method preprocesses front and back images of chrysanthemums from diverse regions, employing the powerful Swin Transformer as the backbone to extract features. The cross-attention mechanism effectively integrates features from both image sides, and a secondary training strategy further enhances the model’s generalization capabilities. Experimental results demonstrate that our method achieves higher accuracy, precision, recall, and F1 score compared to state-of-the-art models, highlighting its potential for accurate chrysanthemum origin tracing. The code and datasets are openly available at https://github.com/dart-into/CCMCAM , ensuring transparency and reproducibility of our findings.},
  archive      = {J_PAAA},
  author       = {Chen, Yifan and Yang, Xichen and Yan, Hui and Liu, Jia and Jiang, Jian and Mao, Zhongyuan and Wang, Tianshu},
  doi          = {10.1007/s10044-025-01419-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Bidirectional feature fusion via cross-attention transformer for chrysanthemum classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Remote sensing image change detection network with
multi-scale feature information mining and fusion. <em>PAAA</em>,
<em>28</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-025-01420-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) aims to predict the pixels that have changed in an image by comparing images from different times. CNN is excellent at local feature extraction, while Transformer is excellent at extracting global features. However, simple CD networks that extract fused local and global feature information have limitations in their discriminative ability. This is due to the underutilization of local and global information for multi-scale features. For this reason, this paper proposes a remote sensing image change detection network for multi-scale feature information mining and fusion (MSFIMF-RSCDNet). Firstly, based on the hierarchical features displaying different levels of information, we design a selective convolutional attention module (SCBAM) to improve the distinguishability of multi-scale features. Subsequently, a cascaded cross-self-attention module (CCSAM) is proposed to refine the global information of the multi-scale features, and finally, a high-level feature-guided multi-scale feature fusion module (HFGFFM) is utilized to improve the discriminability of the model for objects of different sizes. We show through experiments on three public optical remote sensing image CD datasets, LEVIR-CD (Chen and Shi in Remote Sens 12(10):1662, 2020), WHU-CD (Ji et al. in Trans Geosci Remote Sens 57(1):574–586, 2018), and CDD (Lebedev et al. in Int Arch Photogramm Remote Sens Spat Inf Sci 42:565–571, 2018), that stronger change detection CD performance is achieved than other commonly used methods.},
  archive      = {J_PAAA},
  author       = {Xue, Songdong and Zhang, Minming and Qiao, Gangzhu and Zhang, Chaofan and Wang, Bin},
  doi          = {10.1007/s10044-025-01420-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Remote sensing image change detection network with multi-scale feature information mining and fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TabMixer: Advancing tabular data analysis with an enhanced
MLP-mixer approach. <em>PAAA</em>, <em>28</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s10044-025-01423-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular data, prevalent in relational databases and spreadsheets, is fundamental across fields like healthcare, engineering, and finance. Despite significant advances in tabular data learning, critical challenges remain: handling missing values, addressing class imbalance, enabling transfer learning, and facilitating feature incremental learning beyond traditional supervised paradigms. We introduce TabMixer, an innovative model that enhances the multilayer perceptron (MLP) mixer architecture to address these challenges. TabMixer incorporates a self-attention mechanism, making it versatile across various learning scenarios including supervised learning, transfer learning, and feature incremental learning. Extensive experiments on eight public datasets demonstrate TabMixer’s superior performance over existing state-of-the-art methods. Notably, TabMixer achieved substantial improvements in ANOVA AUC across all scenarios: a 4% increase in supervised learning (0.840 to 0.881), 8% in transfer learning (0.803 to 0.872), and 4% in feature incremental learning (0.806 to 0.843). TabMixer demonstrates high computational efficiency and scalability through reduced floating-point operations and learnable parameters. Moreover, it exhibits strong resilience to missing values and class imbalances through both its architectural design and optional preprocessing enhancements. These results establish TabMixer as a promising model for tabular data analysis and a valuable tool for diverse applications.},
  archive      = {J_PAAA},
  author       = {Eslamian, Ali and Cheng, Qiang},
  doi          = {10.1007/s10044-025-01423-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {TabMixer: Advancing tabular data analysis with an enhanced MLP-mixer approach},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMFR-net: Lightweight multi-scale feature refinement network
for retinal vessel segmentation. <em>PAAA</em>, <em>28</em>(2), 1–15.
(<a href="https://doi.org/10.1007/s10044-025-01424-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is a crucial step in analyzing fundus images and plays a vital role in the early detection, diagnosis, and treatment of various diseases. To make the segmentation model more applicable to actual medical scenarios, A Lightweight Multi-scale Feature Refinement Network (LMFR-Net) based on dual-decoding structure is proposed for efficient retinal vessel segmentation. Using a dual-decoding structure to reduce information loss, an Improved Convolution Block (ICB) is proposed to enhance the ability to extract basic features. In addition, a Lightweight Multi-scale Attention Feature Fusion (LMAFF) module is designed to extract the multi-scale spatial structure features. A Feature Refinement Module (FRM) with dense connections is proposed to optimize detailed features and comprehensively improve network segmentation capability. Comparative experiments were conducted on the DRIVE, CHASEDB1, and STARE datasets to verify that LMFR-Net achieved the highest F1-score and Recall of 82.91% and 86.85%, respectively, with only 366kb of parameters. More refined segmentation results have also been achieved in the visualization comparison of segmented images, and the overall segmentation effect is well. This indicates that LMFR-Net achieves efficient retinal vessel segmentation with a significantly reduced computational complexity, making it well-suited for practical medical applications. The code is available at https://github.com/MCloud31/LMFR-Net .},
  archive      = {J_PAAA},
  author       = {Zhang, WenHao and Qu, ShaoJun and Feng, YueWen},
  doi          = {10.1007/s10044-025-01424-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {LMFR-net: Lightweight multi-scale feature refinement network for retinal vessel segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast and accurate 3D lung tumor segmentation algorithm.
<em>PAAA</em>, <em>28</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s10044-025-01425-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a lung tumor segmentation algorithm based on the Allen–Cahn (AC) energy equation. The novelty lies in the fact that, when extracting the energy matrix using the AC energy equation, we employ a sliding window algorithm for feature extraction on the data without neglecting local features. After obtaining the energy matrix, we construct constraint conditions based on the minimum and maximum values in the matrix, forming an arithmetic progression. Due to the flexibility in setting the sliding window size and constraint conditions, we can achieve segmentation results according to different requirements. In the numerical experiments, we conduct segmentation experiments of varying difficulty in both two-dimensional (2D) and three-dimensional (3D) spaces to verify the effectiveness of the proposed method. When addressing the lung tumor segmentation problem, we compare the maximum diameter of 3D lung tumors segmented by our proposed segmentation algorithm with the maximum diameter of lung tumors in the original 2D CT images to validate the segmentation accuracy and significance of the proposed method. By conducting more detailed and precise measurements and segmentations of tumors in 3D space, this approach contributes to advancements in medical science and enhances patient treatment outcomes. We also conduct tumor segmentation experiments on the MSD and LIDC-IDRI datasets, setting up comparison metrics to further verify the method’s effectiveness.},
  archive      = {J_PAAA},
  author       = {Wang, Jian and Han, Ziwei and Chen, Xinlei and Kim, Junseok},
  doi          = {10.1007/s10044-025-01425-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A fast and accurate 3D lung tumor segmentation algorithm},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual emotion analysis using skill-based multi-teacher
knowledge distillation. <em>PAAA</em>, <em>28</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-025-01426-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biggest challenge in visual emotion analysis (VEA) is bridging the affective gap between the features extracted from an image and the emotion it expresses. It is therefore essential to rely on multiple cues to have decent predictions. Recent approaches use deep learning models to extract rich features in an automated manner, through complex frameworks built with multi-branch convolutional neural networks and fusion or attention modules. This paper explores a different approach, by introducing a three-step training scheme and leveraging knowledge distillation (KD), which reconciles effectiveness and simplicity, and thus achieves promising performances despite using a very basic CNN. KD is involved in the first step, where a student model learns to extract the most relevant features on its own, by reproducing those of several teachers specialized in different tasks. The proposed skill-based multi-teacher knowledge distillation (SMKD) loss also ensures that for each instance, the student focuses more or less on the teachers depending on their capacity to obtain a good prediction, i.e. their relevance. The two remaining steps serve respectively to train the student’s classifier and to fine-tune the whole model, both for the VEA task. Experiments on two VEA databases demonstrate the gain in performance offered by our approach, where the students consistently outperform their teachers, and also state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Cladière, Tristan and Alata, Olivier and Ducottet, Christophe and Konik, Hubert and Legrand, Anne-Claire},
  doi          = {10.1007/s10044-025-01426-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Visual emotion analysis using skill-based multi-teacher knowledge distillation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-stage convolutional neural radiance fields.
<em>PAAA</em>, <em>28</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s10044-025-01427-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel view synthesis captured from multiple images is a critical research topic in computer vision and computational photography due to its wide range of applications. Neural radiance fields significantly improve performance by optimizing continuous volumetric scene functions using a multi-layer perceptron. Although neural radiance fields and their modifications provide high-quality scenes, they have limitation in optimizing exact radiance fields due to their hierarchical architecture comprising coarse and fine networks. They also require numerous parameters and generally do not consider local and global relationships between samples on a ray. This paper proposes a unified single-stage paradigm that jointly learns the relative position of three-dimensional rays and their relative color and density for complex scenes using a convolutional neural network to reduce noise and irrelevant features and prevent overfitting. Experimental results including ablation tests verify the proposed approach’s superior robustness to current state-of-the-art models for synthesizing novel views. The code is available at https://github.com/xkdytk/scorf .},
  archive      = {J_PAAA},
  author       = {Lee, Yoonjae and Yoon, Gang-Joon and Song, Jinjoo and Yoon, Sang Min},
  doi          = {10.1007/s10044-025-01427-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Single-stage convolutional neural radiance fields},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tsi-cnn-net: Truly shift-invariant convolutional neural
network for indian sign language recognition system. <em>PAAA</em>,
<em>28</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s10044-025-01428-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of Indian sign language (ISL) recognition systems applied convolutional neural network (CNN) based deep neural networks. However, the output of CNN image classifiers may vary significantly with a little shift in input images. This shortcoming can be partially addressed by data augmentation, anti-aliasing, or blurring that do not work with different input patterns the network trained on and non-linear activation functions like ReLU, respectively. To deal with this short-coming, an ISL recognition approach has been presented using truly shift-invariant CNN. A sub-sampling strategy i.e. adaptive polyphase sampling (APS) has been applied to allow CNN truly shift-invariant. The proposed system is completely consistent to classification task. Furthermore, it offers significantly outstanding classification accuracy not only on Indian sign language datasets but also on datasets of other sign languages.},
  archive      = {J_PAAA},
  author       = {Ghorai, Anudyuti and Nandi, Utpal and Singh, Moirangthem Marjit and Changdar, Chiranjit and Paul, Bachchu and Chowdhuri, Partha and Pal, Pabitra},
  doi          = {10.1007/s10044-025-01428-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Tsi-cnn-net: Truly shift-invariant convolutional neural network for indian sign language recognition system},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale adaptive detail enhancement dehazing network for
autonomous driving perception images. <em>PAAA</em>, <em>28</em>(2),
1–14. (<a href="https://doi.org/10.1007/s10044-025-01430-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hazy weather conditions, a significant accumulation of haze poses a severe challenge to the quality of image capture for autonomous driving systems, thereby heightening safety risks for autonomous vehicles. To solve this problem, we propose the multi-scale adaptive detail enhancement dehazing network, an innovative architecture comprising the initial feature extraction module, the multi-scale adaptive feature module, and the terminal detail enhancement module, specifically designed to eradicate haze with precision. To enhance the extraction of multi-scale features, the multi-scale adaptive feature module employs the squeeze-excitation residual dense block (SRD). It not only learns the intricate multi-scale features of the image but also adaptively recalibrates the feature response of each feature map, ultimately bolstering the network’s performance and resilience. The terminal detail enhancement module, crafted with the dilation refinement block (DRB), serves as a compensatory measure for any detail loss or pseudo-artifacts that might arise from the multi-scale adaptive feature module’s operations. By incorporating the terminal detail enhancement module, the overall dehazing effect is further optimized. Empirical evaluations reveal that the proposed multi-scale adaptive detail enhancement dehazing network achieves impressive results, with a PSNR value of 30.82, an SSIM value of 0.967, and an LPIPS value of 0.033. These figures indicate that the network is adept at removing haze from images while preserving intricate details, ensuring the efficacy and reliability of autonomous driving systems in hazy environments. Code is available at https://github.com/murong-carl/Adaptive-multi-scale-detail-enhancement-dehazing-network .},
  archive      = {J_PAAA},
  author       = {Wang, Juan and Wang, Sheng and Wu, Minghu and Yang, Hao and Cao, Ye and Hu, Shuyao and Shao, Jixiang and Zeng, Chunyan},
  doi          = {10.1007/s10044-025-01430-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-scale adaptive detail enhancement dehazing network for autonomous driving perception images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactive image segmentation combining global seeding and
sparse local reconstruction. <em>PAAA</em>, <em>28</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s10044-025-01432-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seed segmentation methods are highly regarded for their effectiveness in processing complex images, user-friendliness, and compatibility with graph-based representations. However, these methods often depend on intricate computational tools, leading to issues such as poor image contour adherence and incomplete seed propagation. To address these limitations, this paper proposes an interactive framework that integrates global seed information with sparse local linear reconstruction regularization (GSSR). In this framework, a Gaussian mixture model is firstly employed to construct a flow of global seed information, establishing connections between pixel points and yielding more complete segmented objects. Additionally, the $$L_{p}(0 &lt; p \le 1)$$ norm is utilized to constrain the sparse local reconstruction term, facilitating the generation of sparse boundaries. An iterative process based on the Alternating Direction Method of Multipliers (ADMM) is developed to solve the $$L_1$$ regularization term, which is then generalized for the $$L_p$$ problem through reweighting. We conduct a comprehensive comparison on the BSD dataset, CVC-ClinicDB datasets and two publicly available MSRC datasets with different labeling schemes. Extensive experimental validation demonstrates that the proposed method outperforms existing results.The source code and datasets are openly available at: https://github.com/choppy-water/GSSR .},
  archive      = {J_PAAA},
  author       = {Long, Jianwu and Liu, Yuanqin and Zhang, Kaixin and Chen, Shuang and Luo, Qi},
  doi          = {10.1007/s10044-025-01432-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Interactive image segmentation combining global seeding and sparse local reconstruction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSDBPN: Multi-column smoothed dilated convolution based back
projection network for stereo image super-resolution. <em>PAAA</em>,
<em>28</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s10044-025-01433-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully exploiting the parallax information of stereo images for super-resolution (SR) can obtain remarkable performance. The most challenging issue for stereo image SR is how to capture complementary correlation information between the stereo image pair to accurately guide reconstruction. In this paper, we propose a multi-column smoothed dilated convolution based back projection network (MSDBPN) for stereo SR by explicitly learning and exploiting the parallax information. In particular, we incorporate adaptive weighted multi-column smoothed dilated convolutions to rapidly expand the receptive field while maintaining excellent inter-pixel correlation. Meanwhile, we reweight different column feature with adaptive learnable parameter to distinguish contributions. Furthermore, we employ a deep back projection mechanism to calculate projection error and implement self-correction to guide precise reconstruction. Extensive experiments on benchmark datasets demonstrate that our proposed method outperforms other state-of-the-art approaches on both quantitative and qualitative evaluations.},
  archive      = {J_PAAA},
  author       = {Zhou, Zihao and Wang, Yongfang and Lian, Junjie},
  doi          = {10.1007/s10044-025-01433-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSDBPN: Multi-column smoothed dilated convolution based back projection network for stereo image super-resolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic prototype-guided structural information maintaining
for unsupervised domain adaptation. <em>PAAA</em>, <em>28</em>(2), 1–14.
(<a href="https://doi.org/10.1007/s10044-025-01435-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) intends to transfer the knowledge learned from labeled source domain to unlabeled target domain. Most existing methods employ domain adversarial training to align the feature space distributions of two domains. However, these methods may destroy the discriminative structural information. In this paper, we propose a Dynamic Prototype-guided Structural Information Maintaining (DPSIM) approach to preserve the structural information of the target domain based on pairwise semantic similarity. Specifically, we propose a dynamic prototype learning module to learn the categorical intrinsic representation of the source domain and then to predict the similarity of pairwise samples of the target domain. Finally, a structural information maintaining module is proposed to restrict the target domain by discriminating structural information. Extensive experiments on both image classification and object detection tasks demonstrate the effectiveness of our method.},
  archive      = {J_PAAA},
  author       = {Li, Deng and Li, Peng and Liu, Jian and Han, Yahong},
  doi          = {10.1007/s10044-025-01435-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dynamic prototype-guided structural information maintaining for unsupervised domain adaptation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source screen identification using difference image mask
obtained from images recaptured through screenshots based on spatial
rich features. <em>PAAA</em>, <em>28</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s10044-025-01442-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As images are integral to many sectors in the digital age, it is essential to ensure their authenticity and integrity. However, the ease of digital image creation and sharing also exposes them to manipulation and misrepresentation, heightening concerns about privacy and misinformation. The process of recapturing, a prevalent anti-forensic technique, poses challenges to tampering detection methods, necessitating effective countermeasures to uphold image credibility. Monitor-screenshots, facilitated by the simplicity of capturing screenshots of Original images displayed on monitors, pose unique challenges in source identification. Addressing this, we propose a novel approach to unveil the screen fingerprint, capturing distinctive irregularities associated with blur exist in Monitor-screenshots for accurate source identification. Leveraging image registration, difference image masking, and sophisticated feature extraction techniques, our method enables precise identification of specific screens, enhancing the authentication of digital content. By scrutinizing screen-specific characteristics and artifacts left during recapture, proposed model can verify the claimed origin of Screenshots, tested on a Screenshot dataset using SVM classifier, offers a robust framework to authenticate digital content and trace its source with precision and reliability, mitigating risks associated with image manipulation and misrepresentation in the digital domain.},
  archive      = {J_PAAA},
  author       = {Anjum, Areesha and Islam, Saiful and Saleem, Mahreen and Siddiqui, Nadia},
  doi          = {10.1007/s10044-025-01442-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Source screen identification using difference image mask obtained from images recaptured through screenshots based on spatial rich features},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCT: Image super-resolution restoration using hierarchical
convolution transformer networks. <em>PAAA</em>, <em>28</em>(2), 1–11.
(<a href="https://doi.org/10.1007/s10044-025-01413-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the computer vision domain, image super-resolution (SR) technology, which restores high-resolution details from low-resolution images, plays a vital role in practical applications such as medical imaging, public safety, and remote sensing. Traditional methods employ convolutional neural networks to address these issues, while Visual Transformers show potential performance in high-level vision tasks. However, compared to typical CNN architecture networks, Visual Transformers exhibit weaker reliance on high-frequency information in images, leading to blurred details and residual artifacts. To solve this issue, we use a hierarchical network structure, which allows for a more flexible feeling field for our approach. Firstly, our method complements lost spatial features using a Convolutional Swin Transformer Layer incorporating a Convolutional Feed Forward Network. This allows for the retrieval of missing spatial information and enhances the model’s representational capabilities. Next, deep feature extraction is performed by combining multiple layers into a Residual Convolutional Swin Transformer Block. Finally, we employ a hierarchical-type structure to combine the features of each branch. Experiments validate the effectiveness of the proposed method in generating images with greater detail aligned with human perception. Based on the experiments, our method is effective on SR tasks with magnification factors of 2, 3, and 4. Our method can reconstruct a clear and complete edge structure. We provide code at https://github.com/Q88392/HCT .},
  archive      = {J_PAAA},
  author       = {Guo, Ying and Tian, Chang and Wang, Han and Liu, Jie and Di, Chong and Ning, Keqing},
  doi          = {10.1007/s10044-025-01413-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {HCT: Image super-resolution restoration using hierarchical convolution transformer networks},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCDPose: Enhancing the lightweight 2D human pose estimation
model with pose-enhancing attention and context broadcasting.
<em>PAAA</em>, <em>28</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s10044-025-01431-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {2D human pose estimation is an important domain in computer vision. In recent years, the lightweight 2D human pose estimation (2DTLHPE) models based on vision transformer (ViT) have attracted extensive attention due to the fewer parameters and the lower computational requirements. However, these models are also facing the challenges of cluttered and occluded background. It results in the errors of locating keypoints. Therefore, this paper proposes the pose-enhanced contextual distillation for pose estimation model (PCDPose) to alleviate the influence of the challenges. Firstly, PCDPose introduces the pose-enhancing attention (PEA) module which highlights the foreground information in the feature map. It alleviates the influence caused by the cluttered background. Moreover, PCDPose introduces the context broadcasting (CB) module, which builds the long-range dependencies between the keypoints in occluded regions and the neighboring keypoints by broadcasting the context to each vision token (VT). It alleviates the influence caused by the occluded background. Experimental results show that PCDPose achieves a 73.5% average precision (AP) on the COCO2017 dataset, and it has a 1% performance improvement over the state-of-the-art (SOTA) model. On the CrowdPose dataset, PCDPose achieves a 71.3% AP, and it has a 5.9% performance improvement over the SOTA model.},
  archive      = {J_PAAA},
  author       = {Tian, Zhenyuan and Fu, Weina and Woźniak, Marcin and Liu, Shuai},
  doi          = {10.1007/s10044-025-01431-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PCDPose: Enhancing the lightweight 2D human pose estimation model with pose-enhancing attention and context broadcasting},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPFCNet: Multi-scale parallel feature fusion convolutional
network for 3D knee segmentation from MR images. <em>PAAA</em>,
<em>28</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-025-01437-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and automatic segmentation of knee magnetic resonance (MR) images plays a vital role in the diagnosis of osteoarthritis and knee bone diseases. However, the anatomical structure of the knee joint is complex, it is difficult to segment knee joints accurately and efficiently. This paper proposes a knee joint segmentation model from MR image, which is named a multi-scale parallel feature fusion convolutional network (MPFCNet). A Large Kernel Attention (LKA) module is coined in the MPFCNet, which effectively increases the receptive field and preserves detail textures, resulting in better feature extraction. To further utilize complementary information at various scales in both spatial and channel dimensions, a Multi-Scale Fusion (MSF) module is established. A Hybrid Feedforward Attention (HFA) module is proposed to establish long-range dependencies. Experiments and comparisons with state-of-the-art methods were conducted on the publicly available dataset OAI-ZIB. The results show that the MPFCNet achieved excellent segmentation results on the knee joint segmentation task, improving the average dice similarity coefficient.},
  archive      = {J_PAAA},
  author       = {Zhang, Hanzheng and Wu, Qing and Zhao, Xing and Wang, Yuanquan and Zhou, Shoujun and Zhang, Lei and Zhang, Tao},
  doi          = {10.1007/s10044-025-01437-6},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MPFCNet: Multi-scale parallel feature fusion convolutional network for 3D knee segmentation from MR images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fedpartwhole: Federated domain generalization via consistent
part-whole hierarchies. <em>PAAA</em>, <em>28</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-025-01439-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Domain Generalization (FedDG) aims to address the challenge of generalizing to unseen domains at test time while adhering to data privacy constraints that prevent centralized data storage from various client domains. Existing approaches can be broadly classified into domain alignment, data manipulation, learning strategies, and optimization of model aggregation weights. This paper introduces a novel approach to FedDG that focuses on the backbone model architecture. The key insight is that objects, even under substantial domain shifts and appearance variations, retain a consistent hierarchical structure of parts and wholes. For example, a photograph and a sketch of a dog share the same structural organization, comprising a head, body, limbs, and so on. Our architecture explicitly integrates a feature representation for the image parse tree, enabling robust generalization across domains. To the best of our knowledge, this is the first work to approach FedDG from a model architecture perspective. We compared the performance of our proposed backbone against a comparable-sized CNN-based backbone (MobileNet) for 5 different algorithms on standard benchmark datasets (PACS and VLCS), and the results showed an average improved performance of up to 17.3%. Additionally, our approach marginally outperforms the Vision Transformer (ViT-Small) on average, despite utilizing approximately 5x fewer parameters. Unlike conventional convolutional neural networks, our method is inherently interpretable, fostering trust in its predictions-a critical asset in federated learning scenarios.},
  archive      = {J_PAAA},
  author       = {Radwan, Ahmed and Shehata, Mohamed},
  doi          = {10.1007/s10044-025-01439-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Fedpartwhole: Federated domain generalization via consistent part-whole hierarchies},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic metric memory network for long-term tracking with
spatial-temporal region proposal method. <em>PAAA</em>, <em>28</em>(2),
1–18. (<a href="https://doi.org/10.1007/s10044-025-01441-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully mining target information is critical to cope with the recovery of lost targets in long-term tracking scenarios. However, most existing trackers focus on either temporal or spatial information during tracking and do not utilize this information effectively simultaneously. Therefore, we propose a dynamic metric memory network for long-term tracking with spatial-temporal region proposals. First, we present a spatio-temporal region proposal method, in which temporal memory is utilized to construct dynamic templates that represent the variations in the historical appearance of the target. Meanwhile, spatial attention focuses on the geometric information of the target to enhance the perceptual capabilities of the model. This interactive use of spatio-temporal information makes the Regional Proposal Network (RPN) generate higher-quality object-oriented proposals. Second, a dynamic metric memory network encompassing writing and reading mechanisms is designed. The former includes a metric learning judgment strategy to maintain temporal consistency and dynamically memorize significant variations. The latter reads out the entire memory to verify the quality of the candidate region and infer the optimal candidate, in which the short-term memory is used to update the template. The designed network enhances the tracker’s adaptive capability to target changes. Finally, we employ an online refinement network to rectify the prediction results to further improve the tracking performance, which updates the memory pool and switches the local–global search strategy. our experimental results on benchmarks such as VOT-LT2018 and others demonstrate that our proposed tracker is on par with the current state-of-the-art tracking algorithms.},
  archive      = {J_PAAA},
  author       = {Zhang, Huanlong and Fu, Weiqiang and Yang, Xiangbo and Qi, Rui and Wang, Xin and Zhang, Chunjie},
  doi          = {10.1007/s10044-025-01441-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dynamic metric memory network for long-term tracking with spatial-temporal region proposal method},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle and license plate recognition with novel dataset for
toll collection. <em>PAAA</em>, <em>28</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s10044-025-01443-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an automatic toll tax collection framework designed for challenging conditions, consisting of three sequential steps: vehicle type recognition, license plate localization, and license plate reading. Traditional decorations on vehicle fronts often introduce significant intra-class variations, severe background clutter, and partial occlusions, complicating both license plate detection and reading. In addition, non-uniform license plate positions-particularly on trucks-and variations in font styles, sizes, and partially occluded characters further challenge the process. To address these issues, we leverage advanced deep learning architectures along with a novel dataset of 10k images covering six vehicle types. Each image is manually annotated with the vehicle type and the alphanumeric characters of its license plate. We evaluate our framework using state-of-the-art YOLO models, from the initial to the latest versions: Yolov2, Yolov3, Yolov4, YOLOv5, YOLOv8, and YOLOv11, and assess their lightweight (Nano) variants for real-time deployment on a Raspberry Pi. Our experimental results demonstrate that the large variants of YOLOv5, YOLOv8, and YOLOv11 consistently achieve a top mean average precision (mAP@0.5) of 99% across all tasks, while their Nano versions attain peak mAP values of 98%, 97%, and 98% for vehicle type recognition, license plate detection, and character recognition, respectively. The code, trained models, and test images are available at https://github.com/usama-x930/VT-LPR .},
  archive      = {J_PAAA},
  author       = {Usama, Muhammad and Anwar, Hafeez and Anwar, Saeed},
  doi          = {10.1007/s10044-025-01443-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Vehicle and license plate recognition with novel dataset for toll collection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSGGSA: A multi-strategy-guided gravitational search
algorithm for gene selection in cancer classification. <em>PAAA</em>,
<em>28</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s10044-025-01446-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarray gene expression data, widely employed in the field of cancer research, provides valuable insights for distinguishing between various types of cancers. However, the inherent challenges associated with microarray data, such as limited sample size and high dimensionality, complicate the process of cancer classification. To effectively address these challenges, swarm intelligence optimization techniques have been employed for gene selection from microarray gene expression datasets. Nevertheless, accurately classifying cancer types remains a formidable challenge. The present study presents a gene selection method for cancer classification, employing a multi-strategy-guided gravitational search algorithm (MSGGSA). In MSGGSA, the traditional gravitational search algorithm (GSA) is enhanced by incorporating strategies such as segmented population initialization, inertia stagnation, dynamic update of gravitational constant, and relearning strategy for elite individuals. The proposed enhancements effectively address the limitations associated with excessive randomness in the initial population, premature convergence susceptibility, and vulnerability to local optima encountered by traditional GSA. Moreover, this enhancement effectively achieves a balance between exploration and exploitation within algorithm. The superior performance of the proposed algorithm on high-dimensional data is demonstrated through rigorous testing on 12 publicly available microarray datasets, highlighting its superiority over other popular swarm intelligence algorithms and current state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Li, Min and Jin, Chen and Cai, Yuheng and Deng, Shaobo and Wang, Lei},
  doi          = {10.1007/s10044-025-01446-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSGGSA: A multi-strategy-guided gravitational search algorithm for gene selection in cancer classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conversion-aware forecasting of alzheimer’s disease via
featurewise attention. <em>PAAA</em>, <em>28</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s10044-025-01447-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a neurodegenerative disorder that leads to cerebral atrophy, impacting memory and cognitive abilities. A precursor to AD known as Mild Cognitive Impairment (MCI) shows subtle symptoms that do not overwhelm the patients’ daily activities. MCI patients might eventually progress to AD in later stages. Early detection of the conversion is a vital step in preventative treatment planning. However, conversion detection is challenging due to the rarity of conversion visits in public datasets and the unknown nature of the conversion. This study aims to improve conversion detection with an attention-based architecture designed to encode input biomarkers and time into a shared space where time and attribute embeddings are fused with attention. Temporal information is incorporated as a separate modality with time embeddings to capture the correlation between time and feature significance for the model’s predictions. Experiments with widely used public databases (TADPOLE and NACC) show encouraging performance in conversion detection. In TADPOLE, a conversion recall of 74.3%, significantly outperforming baseline models such as logistic regression (36.9%) and Long Short-Term Memory networks (62.3%), is reported while maintaining an area under the curve (AUC) score of 82.0%. In NACC, our model demonstrates a competitive conversion recall of 71.6% and an AUC of 82.6%. The experimental results highlight the contribution of the attention between time and attributes to MCI-AD conversion recall. The experimental analyses hold promise for assisting physicians in designing targeted preventative treatment strategies for at-risk individuals. The implementation of the proposed method is available at https://github.com/ALLab-Boun/FATE-Net .},
  archive      = {J_PAAA},
  author       = {Karasu, Elvan and Baytaş, İnci M.},
  doi          = {10.1007/s10044-025-01447-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Conversion-aware forecasting of alzheimer’s disease via featurewise attention},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dmvae: A dual-stream multi-modal variational autoencoder for
multi-task fake news detection. <em>PAAA</em>, <em>28</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s10044-025-01412-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news on social media platforms, facilitated by the development of the Internet, has become a pressing social issue, intensifying the urgency of detecting its diverse multi-modal forms. However, current methods are unable to verify the validity of the extracted multimodal features, ignore the problem of interaction between multimodal content, and fail to learn valid cross-modal features. In this paper, we took a look into new multi-modal learning methods for representation and fusion in fake news detection. A two-branch adversarial network is designed to extract different levels of event-irrelevant features, while inter-modal information interaction and intra-modal information enhancement are followed to improve the richness of the features. To improve the interpretability of the model, a multi-task learning methodology based on the variational autoencoder structure is proposed in detail, which redesigns a general loss function to balance competitive submodules, and verifies the effectiveness of the multi-modal features in turn. Finally, by comparing and analyzing the experimental results of different methods, it is demonstrated that the multimodal fake news detection model proposed in this paper can effectively improve the effectiveness of fake news detection.},
  archive      = {J_PAAA},
  author       = {Guo, Ying and Hu, Shuting and Li, Yao and Di, Chong and Liu, Jie},
  doi          = {10.1007/s10044-025-01412-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dmvae: A dual-stream multi-modal variational autoencoder for multi-task fake news detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uieanything: Zero-shot underwater image enhancement via
advanced depth estimation, white balance models, and improved sea-thru.
<em>PAAA</em>, <em>28</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s10044-025-01422-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement is fundamental for marine applications yet remains challenging due to complex light-water interactions that degrade image quality through wavelength-dependent absorption and scattering effects. Existing methods often require extensive paired training data and struggle to generalize across diverse underwater conditions. We propose UIEAnything, a novel zero-shot underwater image enhancement framework that integrates automatic white balance preprocessing, physics-guided depth estimation, and an improved restoration algorithm based on underwater light transport theory. Our approach introduces three key innovations: (1) a domain adaptation strategy that bridges the gap between underwater and natural images via physically motivated white balance correction, enabling effective utilization of pre-trained models; (2) an improved Sea-thru algorithm incorporating nonlinear backscatter modeling and adaptive attenuation estimation, accurately capturing the depth-dependent nature of underwater light propagation; and (3) a unified framework that eliminates the need for task-specific training while maintaining physical consistency. Extensive experiments on seven benchmark datasets demonstrate that UIEAnything consistently outperforms state-of-the-art methods, achieving average improvements of 15.3% in PSNR and 12.8% in SSIM. Furthermore, without additional training, our framework demonstrates remarkable generalization capability by successfully addressing other challenging vision tasks involving scattering media, such as image dehazing and sandstorm removal. These results establish UIEAnything as a significant advancement in physics-guided zero-shot learning for image enhancement in complex optical environments.},
  archive      = {J_PAAA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s10044-025-01422-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Uieanything: Zero-shot underwater image enhancement via advanced depth estimation, white balance models, and improved sea-thru},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accumulating global channel-wise patterns via
deformed-bottleneck recalibration for image classification.
<em>PAAA</em>, <em>28</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s10044-025-01429-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding attention modules into deep convolutional neural networks (CNNs) is currently one of the common deliberations to enhance their learning ability of feature representation. In previous works, the global channel-wise patterns of a given tensor are computed and squeezed into CNN-based models through an attention mechanism. Squeezing different kinds of these features can lead to the less fusion of attentive information due to the independent operations of channel-wise recalibration. To deal with this issue, an efficient attention module of accumulated features ( $$\textrm{MAF}$$ ) is proposed by accumulating these diverse squeezes for a unitary recalibrating perceptron as follows. Firstly, we take advantage of average and deviation calculations to produce correspondingly statistical patterns of a given tensor for aggregating the global channel information. An adaptative perceptron of deformed-bottleneck recalibration ( $$\textrm{DBR}$$ ) is then presented to cohere the resultant features. Finally, the robust $$\textrm{DBR}$$ -based lightweights will be utilized to weight the concerning tensor. Additionally, to exploit more spatial-wise information, we address $$\textrm{MAF}$$ for an effective alternative of the channel-wise component in two critical attention units to form two corresponding modules that will be then inspected to indicate which integration is good for real applications. We adapt the MAF-based modules to MobileNets for further enhancement investigation. Experiments on benchmark datasets for image classification have proved the efficacy of our proposals. The code of the MAF module is available at https://github.com/nttbdrk25/MAFAttention .},
  archive      = {J_PAAA},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong and Nguyen, Vincent},
  doi          = {10.1007/s10044-025-01429-6},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Accumulating global channel-wise patterns via deformed-bottleneck recalibration for image classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPPCN: Density and position-based point convolution network
for point cloud segmentation. <em>PAAA</em>, <em>28</em>(2), 1–10. (<a
href="https://doi.org/10.1007/s10044-025-01436-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A point cloud can usually describe the outline and spatial location of an object. Due to the disorder and uneven density of the point cloud, it is a difficult task to fully obtain the local features and spatial context information of the point cloud. In this paper, we propose a point cloud segmentation network based on the encoding–decoding structure of point convolution, which extracts the local features of point clouds by density-position adaptive convolution, which integrates density information and positional relationships between points. To obtain the density information of center points, we design an auto-adjusted bandwidth and integrate it into adaptive kernel density estimation. In addition, to obtain the context of the point cloud to a greater extent, we design an encoding layer that carries the contextual information. In order to verify the effectiveness of our method, experiments were carried out on S3DIS and a self-built dataset. The experimental results verify the validity of our proposed method.},
  archive      = {J_PAAA},
  author       = {Li, Yaqian and Zhang, Ze and Li, Haibin and Zhang, Wenming},
  doi          = {10.1007/s10044-025-01436-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-10},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DPPCN: Density and position-based point convolution network for point cloud segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive deep CNN: An effective alzheimer’s affected MRI
image registration using heuristic-aided deep learning model and
patch-based level fusion. <em>PAAA</em>, <em>28</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s10044-025-01438-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aligning multiple images is referred to as image registration. Image registration methods aim to find the best adjustment to align the important elements in the images.Registering techniques can be modified to fit specific objectives by balancing rapidity with precision. Magnetic Resonance Imaging (MRI) methods are becoming increasingly significant in Alzheimer’s Disease (AD) in themedical sector. Several intriguing applications for machine learning approaches in clinical imaging exist, including the recognition of AD using MRI scans of the brain. Numerous preprocessing procedures, such as image registration, are often performed on these scans. However, the impact of registration of image approaches on machine learning classification effectiveness is little understood. To address these challenges, a new deep learning approach is suggested for registering images ofAD using MRI images. Originally, the images are garnered from the standard datasets. The patch-based label fusion method is suggested from the collected images, where the patches and their weights are estimated by the similarity measures among the patches using labeling and intensity-based distances. Subsequently, the patches are considered for hippocampal segmentation using anatlas-based segmentation model.The hippocampus is a critical brain structure involved in memory and learning, and it is one of the first regions to exhibit damage in AD. As Alzheimer’s grows, the hippocampus undergoes significant atrophy, leading to the characteristic memory loss and cognitive decline associated with the condition. Hippocampal segmentation, which involves isolating and analyzing the hippocampus from MRI scans, is essential for detecting these changes early. Accurately measuring the volume and structure of the hippocampus helps to identify signs of Alzheimer’s before other symptoms become apparent. Furthermore, precise hippocampal segmentation helps differentiate Alzheimer’s from other neurological conditions, ensuring more accurate diagnoses and tailored treatment strategies.Finally, the “Adaptive Deep Registration” is newly proposed to register the images by utilizing anAdaptive Deep Convolutional Neural Network (DCNN), in which the hyper-parameters are optimized using the Modified Gannet Optimization Algorithm (MGOA). The term “adaptive” in Adaptive DCNN reflects the model’s ability to dynamically optimize its parameters. In the context of image registration, this adaptiveness allows the MGOAto fine-tune DCNN’s parameters, such as hidden neuron count, epoch count, and step per epoch, in response to the complexity and variability of the input images. This ensures that the network can effectively learn from diverse data, improving its accuracy in aligning and registering images. The adaptive nature of the DCNN, further enhanced by the MGOA, allows the network to continuously tune its performance, adjusting hyperparameters on the fly to achieve the best possible outcomes in image registration tasks. This adaptability is crucial in handling the intricate patterns and subtle differences found in medical images, such as those of the hippocampus in Alzheimer’s disease, leading to more accurate and consistent results.At last, the efficacy of the model is examined using divergent measurements. Rather than existing methods, the proposed model acquires impressive results of performance enhancement.},
  archive      = {J_PAAA},
  author       = {Deshmukh, Vaidehi and Chapadgaonkar, Shilpa and Kowdiki, Manisha and Khaparde, Arti},
  doi          = {10.1007/s10044-025-01438-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive deep CNN: An effective alzheimer’s affected MRI image registration using heuristic-aided deep learning model and patch-based level fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured regularization with object size selection using
mathematical morphology. <em>PAAA</em>, <em>28</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s10044-025-01444-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel way to incorporate morphology operators through structured regularization of machine learning models. Specifically, we introduce a feature map in the models that performs structured variable selection. The feature map is automatically processed by approximate morphology operators and is learned together with the model coefficients. Experiments were conducted with linear regression on both synthetic data, demonstrating that the proposed methods are effective in selecting groups of parameters with much less noise than baseline models, and on three-dimensional T1-weighted brain magnetic resonance images (MRI) for age prediction, demonstrating that the proposed methods enforce sparsity and select homogeneous regions of non-zero and relevant regression coefficients. The proposed methods improve interpretability in pattern analysis. The minimum size of features in the structured variable selection can be controlled by adjusting the structuring element in the approximate morphology operator, tailored to the specific study of interest. With these added benefits, the proposed methods still perform on par with commonly used variable selection and structured variable selection methods in terms of the coefficient of determination and the Pearson correlation coefficient.},
  archive      = {J_PAAA},
  author       = {Lin, Disi and Hägg, Linus and Wadbro, Eddie and Berggren, Martin and Löfstedt, Tommy},
  doi          = {10.1007/s10044-025-01444-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Structured regularization with object size selection using mathematical morphology},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel fusion approach with a robust ParallelNet model for
diabetic retinopathy diagnosis. <em>PAAA</em>, <em>28</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s10044-025-01448-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic Retinopathy (DR) is a serious diabetes-related complication that can lead to significant retinal damage and irreversible vision loss if not detected and treated early. While numerous deep learning algorithms have recently been developed for DR diagnosis, however they often focus on specific symptoms like exudates, vessels, or hemorrhages, overlooking a comprehensive analysis of all relevant indicators. Though, previous studies have shown high performance on benchmark public datasets but have struggled with real-time data. This paper introduces a diagnostic system that systematically incorporates all detectable symptoms of diabetic retinopathy and has demonstrated reliable performance on 108 test images from Lahore General Hospital, showcasing its robustness in real-world scenarios. Additionally, a novel algorithm for extracting retinal exudates is proposed, outperforming existing methods. The study categorizes retinal fundus images into both 2-class and multi-class diabetic retinopathy. Evaluation of current models on a local hospital dataset shows significant accuracy improvements. We also present ParallelNet, a model for classifying Diabetic Retinopathy stages: No DR, NPDR, PDR. ParallelNet outperforms established models, achieving 96% accuracy on the APTOS dataset and 90.16% on the local dataset for binary classification. For multi-classification, it achieves 90% accuracy on the APTOS dataset and 87.05% on the local dataset. These results highlight the improved performance achieved by combining our extraction algorithms with the ParallelNet model, demonstrating robustness across both public and local real-time hospital datasets.},
  archive      = {J_PAAA},
  author       = {Mahmood, Haroon and Ather, Saad and Wali, Aamir and Ali, Arshad and Malik, Tayyaba Gul and Kafeel, Wardah},
  doi          = {10.1007/s10044-025-01448-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel fusion approach with a robust ParallelNet model for diabetic retinopathy diagnosis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual stream deep attention networks for annual population
projection. <em>PAAA</em>, <em>28</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s10044-025-01451-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate population projections are essential for local and state governments planning and decision-making processes, as they directly influence the development of local infrastructure and services. Recent advancements in time series forecasting, particularly through machine learning applied to diverse social and economic factors, have improved predictive accuracy, enabling more informed decision-making at local and state levels. Therefore, researchers have explored various statistical, machine learning, and deep learning methods. However, these approaches often rely on standalone models or simple feature integration, which results need to be improved. Furthermore, the existing methods lack hybrid methods with attention mechanisms to effectively capture rich intricate features. To tackle these challenges, we designed a hybrid method, a dual-stream deep attention network namely Deep Population Network (DPNET) for annual population forecasting. The first stream employs convolutional layers to capture spatial patterns while the second stream extracts the temporal information. The output of these two streams is then concatenated and fed to the self-attention module for feature refinement followed by a fully connected layer. The DPNET showcases promising performance in small-area population projection across multiple datasets collected from Korea, Australia, and New Zealand for 10-year and 5-year forecasting periods. Compared to existing approaches, DPNET significantly reduced the error rate, achieving the lowest Mean Absolute Percentage Error (MAPE) of 5.30% and Median Absolute Percentage Error (MedAPE) of 3.03% for the Korean dataset, 6.02% MAPE and 4.09% MedAPE for the Australian dataset, and 3.99% MedAPE for the New Zealand dataset.},
  archive      = {J_PAAA},
  author       = {Hussain, Adnan and Yar, Hikmat and Khan, Noman and Khan, Zulfiqar Ahmad and Kim, Min Je and Baik, Sung Wook},
  doi          = {10.1007/s10044-025-01451-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dual stream deep attention networks for annual population projection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Texture-driven pose-guided human image synthesis.
<em>PAAA</em>, <em>28</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s10044-025-01452-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of computer vision and artificial intelligence, significant breakthroughs have been made in the field of character image synthesis. Although existing methods can synthesize target pose images, there are still limitations in handling complex textures and pose alignment, such as texture distortion, pose misalignment, and missing information. To address this, this paper proposes a pose-guided human image synthesis method called Human Pose Transfer Generative Adversarial Network (HPT-GAN). The model significantly improves the quality and efficiency of synthetic images by introducing ResBlocks module, designing a Texture Transfer Module (TTM) and a ToRGB module. Specifically, ResBlocks enhance gradient stability while preserving context information, TTM efficiently aligns textures through a multi-head attention mechanism, and the ToRGB module optimizes the fusion of multi-resolution features. HPT-GAN has a small number of parameters while achieving faster processing speed than similar methods. Moreover, it has achieved good results on the DeepFashion and Market-1501 datasets.},
  archive      = {J_PAAA},
  author       = {Wei, Wei and Qin, Chao and Duan, Xiaodong},
  doi          = {10.1007/s10044-025-01452-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Texture-driven pose-guided human image synthesis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
