<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip---37">JRTIP - 37</h2>
<ul>
<li><details>
<summary>
(2025). Real-time detection method of intelligent classification and
defect of transmission line insulator based on LightWeight-YOLOv8n
network. <em>JRTIP</em>, <em>22</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-025-01627-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the difficulty in distinguishing different types of insulators in transmission lines and the need for intelligent identification of insulator defects, a real-time detection method for intelligent classification and defect identification of transmission line insulators based on a LightWeight-YOLOv8n network is proposed. This method is used to efficiently detect isolator defects in complex environments and to quickly identify isolator classes. The LightWeight network model MobileNetV3 is used as the backbone network and is based on the YOLOv8n model. The feature extraction ability of the model is enhanced, the redundancy of model parameters is reduced, and the detection speed is improved by adding the SimAM attention mechanism at the end of the backbone network. The LightWeight CSPPC module is used to enhance the feature fusion component of YOLOv8n, reducing the computation load and network complexity while maintaining the accuracy of the model. To further improve the performance of the algorithm, the WIoUv3 bounding box regression loss function is used to replace the original CIoU loss function, and the SlideLoss is used to replace ClsLoss as the category loss function. The experimental results show that the detection accuracy reaches 92.4%, the recall rate reaches 86.6%, and the mAP50 reaches 90.6%. Meanwhile, the training speed is increased by 40.54%, floating-point operation is reduced by 28.57%, and the model parameters are reduced by 34%. Heatmap visualization analysis also showed that the improved models exhibited greater concentration and confidence than the baseline models. Compared to other algorithms, LightWeight-YOLOv8n showed significant advantages in overall performance, accuracy, and real-time target detection. After a random test of 100 images from the test set, the LightWeight-YOLOv8n model exhibited the lowest detection speed and was able to achieve real-time detection.},
  archive      = {J_JRTIP},
  author       = {Tan, Guoguang and Ye, Yongsheng and Chu, Jiawei and Liu, Qiang and Xu, Li and Wen, Bin and Li, Lili},
  doi          = {10.1007/s11554-025-01627-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection method of intelligent classification and defect of transmission line insulator based on LightWeight-YOLOv8n network},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight YOLOv7 for bushing surface defects detection.
<em>JRTIP</em>, <em>22</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01630-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bushings have a wide range of applications in industry. Once the surface of the bushing is defective, it will affect the assembly between bearings resulting in mechanical inefficiency. At present, due to the different target sizes of the different types of defects on the bushing surface, it is difficult to balance inspection accuracy and speed. This paper proposes lightweight You Only Look Once (YOLO) v7 networks to cope with this problem. In this paper, we use a lightweight network, MobileNetv3, as the backbone network, in which a Residual edges CBAM block (RC-block) is designed to retain feature information while focusing on small-scale targets; finally, we use a bi-directional feature pyramid network (BiFPN) to perform feature fusion to further improve the detection accuracy. The experimental results show that the improved model reduces the Mean Average Precision (mAP) by only $$0.7\%$$ compared with the traditional YOLOv7 model, but the detection speed is increased by $$29.4\%$$ , and the model volume is reduced by $$29.9\%$$ , effectively improves the detection accuracy and speed of all kinds of defects on the surface of the bushings. The improved model was trained under the publicly available dataset NEU-DET, and the results showed the generalisability of the model.},
  archive      = {J_JRTIP},
  author       = {Cheng, Wenjun and Zeng, Pengfei and Hao, Yongping},
  doi          = {10.1007/s11554-025-01630-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight YOLOv7 for bushing surface defects detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESF-DETR: A real-time and high-precision detection model for
cigarette appearance. <em>JRTIP</em>, <em>22</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-025-01632-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of cigarettes is greatly affected by appearance defects. Achieving automatic detection of these defects with high precision and speed has been a critical concern for cigarette factories. To meet the needs of manufacturers in detecting appearance defects in cigarettes, this paper proposes a model based on DETR (DEtection TRansformer) for detecting cigarette appearance defects. The model integrates EfficientViT, SENetV2, and FuNet (Full-scale Feature Fusion Network), called ESF-DETR. First, EfficientViT serves as the backbone feature extraction network, substantially reducing model parameters and enhancing feature extraction efficiency. Second, SENetV2 is introduced at the end of the backbone network to improve feature expression accuracy and global information integration capability. Third, the Full-scale Feature Fusion Network (FuNet) is proposed as the encoder, further reducing model parameters while increasing spatial location and high-level semantic information across each feature layer. The proposed ESF-DETR model achieves a mAP of 96.0% with a parameter count of 10.1M. Compared to the original model, the mAP has increased by 4.4%, while the number of parameters has decreased by 49.8%. Additionally, the detection speed reaches 500 FPS, satisfying cigarette production lines’ accuracy and speed requirements.},
  archive      = {J_JRTIP},
  author       = {Ding, Yingchao and Yuan, Guowu and Zhou, Hao and Wu, Hao},
  doi          = {10.1007/s11554-025-01632-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ESF-DETR: A real-time and high-precision detection model for cigarette appearance},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drone object detection incorporating multi-head mixed
self-attention and dynamic regression mapping loss function.
<em>JRTIP</em>, <em>22</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11554-025-01633-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drone object detection is a critical area within computer vision, facing challenges such as low accuracy in detecting large occluded objects and high rates of missed detection for small objects. These issues arise from inadequate emphasis on the distinctive features of the target area during detection and the regression enhancement of low-quality training examples. To tackle these challenges, this paper introduces a novel method for drone object detection that integrates multi-head mixed self-attention with a dynamic regression mAPping loss function. Initially, a Multi-head Mixed Self-Attention mechanism (MMSA) is developed, tailored to the characteristics of occluded object images. This mechanism is embedded into the backbone and neck components of YOLOv8n to bolster feature extraction and fusion. Subsequently, a dedicated layer for small object detection is incorporated into YOLOv8n to enhance its capability in detecting small objects. A new loss function, Focaler-WIoU, is formulated by merging Focaler-IoU and WIoU, aiming to improve detection across various object scales and accelerate the convergence of bounding box regression loss while enhancing localization accuracy. Additionally, Soft NMS is employed to refine candidate bounding boxes, mitigating missed detections in scenarios with overlapping similar targets. Evaluations on the public dataset VisDrone2019 using standard metrics, including ablation and model comparison experiments, reveal an average precision (mAP @0.5) improvement of 10.2% over the baseline YOLOv8n. The proposed method outperforms other algorithms such as Drone-YOLO (nano), YOLOv11n, Faster RCNN, and FE-YOLOv5 in detection accuracy. Further validation on datasets like CityPersons and CrowdHuman underscores the versatility of the improved algorithm. The experimental outcomes confirm that the MMSA attention mechanism significantly enhances the detection of occluded objects, achieving superior accuracy compared to established object detection algorithms. This suggests that the proposed method holds substantial practical and general applicability for drone image detection in natural settings. Detailed code is available at https://github.com/CodeSworder/MMSA .},
  archive      = {J_JRTIP},
  author       = {Su, Qinghua and Mu, Jianhong and Xu, Sheng and Wan, Kaizheng and Qi, Xiangyu and Zhang, Zhichao and Li, Juntao},
  doi          = {10.1007/s11554-025-01633-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Drone object detection incorporating multi-head mixed self-attention and dynamic regression mapping loss function},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time dynamic scale-aware fusion detection network: Take
road damage detection as an example. <em>JRTIP</em>, <em>22</em>(2),
1–12. (<a href="https://doi.org/10.1007/s11554-025-01634-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV)-based Road Damage Detection (RDD) is important for daily maintenance and safety in cities, especially in terms of significantly reducing labor costs. However, current UAV-based RDD research still faces many challenges. For example, the damage with irregular size and direction, the masking of damage by the background, and the difficulty of distinguishing damage from the background significantly affect the ability of UAV to detect road damage in daily inspection. To solve these problems and improve the performance of UAV in real-time road damage detection, we design and propose three corresponding modules: a feature extraction module that flexibly adapts to shape and background; a module that fuses multiscale perception and adapts to shape and background; an efficient downsampling module. Based on these modules, we designed a multi-scale, adaptive road damage detection model with the ability to automatically remove background interference, called Dynamic Scale-Aware Fusion Detection Model (RT-DSAFDet). Experimental results on the UAV-PDD2023 public dataset show that our model RT-DSAFDet achieves a mAP50 of 54.2%, which is 11.1% higher than that of YOLOv10-m, an efficient variant of the latest real-time object detection model YOLOv10, while the amount of parameters is reduced to 1.8M and FLOPs to 4.6G, with a decrease by 88% and 93%, respectively. Furthermore, on the large generalized object detection public dataset MS COCO2017 also shows the superiority of our model with mAP50–95 is the same as YOLOv9-t, but with 0.5% higher mAP50, 10% less parameters volume, and 40% less FLOPs.},
  archive      = {J_JRTIP},
  author       = {Pan, Weichao and Wang, Xu and Huan, Wenqing},
  doi          = {10.1007/s11554-025-01634-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time dynamic scale-aware fusion detection network: Take road damage detection as an example},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced underwater object detection with YOLO-LDFE: A model
for improved accuracy with balanced efficiency. <em>JRTIP</em>,
<em>22</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11554-025-01628-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In underwater image analysis, challenges such as complex environments, low model performance, and slow processing efficiency hinder effective object detection, which is crucial for real-time monitoring. To address these issues, we propose a high-precision underwater object detection model named YOLO-LDFE. To overcome the limitations of existing datasets, we have developed a comprehensive multi-species underwater biological dataset, MCUA, containing 9327 labeled images across 14 categories. The primary contributions of this work are as follows: (1) SPPF_SCSA, which integrates multi-semantic information with channel-space attention mechanisms to enhance performance; (2) the substitution of traditional convolutions with GSConv in C2f, reducing model size while maintaining feature extraction; (3) LEDF, which improves performance through multi-level, dense connections. YOLO-LDFE achieves exceptional results, with an average precision of 93% on the URPC2021 dataset, outperforming existing algorithms while maintaining high detection speed, demonstrating its potential for real-time underwater monitoring.},
  archive      = {J_JRTIP},
  author       = {Liu, JiaXin and Zhou, RiGui and Li, YaoChong and Ren, PengJu},
  doi          = {10.1007/s11554-025-01628-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhanced underwater object detection with YOLO-LDFE: A model for improved accuracy with balanced efficiency},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive receptive field and attention-guided feature
enhancement for ceramic tile surface defect detection. <em>JRTIP</em>,
<em>22</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01636-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of diverse defect shapes, small defect sizes, and real-time detection on ceramic tile surfaces, this paper proposes an efficient anchor-free detector that integrates adaptive receptive fields and feature enhancement. First, the anchor-free YOLOv8 is proposed as the detection framework to eliminate the need for setting anchor-related hyperparameters, thus avoiding their impact on performance. Second, an adaptive receptive field module (ARFM) is introduced, which extracts defect features from multiple scales through constructing several parallel branches and fuses the feature maps from different receptive fields, thereby dynamically adjusting the receptive field to detect various defects. Finally, a feature enhancement module (FEM) is added to the neck part, enhancing feature representation from both global and spatial dimensions to reduce feature information loss and improve defect detection performance. Experiments show that our detector achieves a mean average precision of 70.4%, an improvement of 6.1% over YOLOv8, while maintaining a high detection speed of 121.2 frames per second. This performance surpasses the state-of-the-art detection methods. These results indicate that our proposed model can achieve satisfactory defect detection performance, meeting the industry’s real-time detection needs.},
  archive      = {J_JRTIP},
  author       = {Yu, Songsen and Wang, Zhemeng},
  doi          = {10.1007/s11554-025-01636-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Adaptive receptive field and attention-guided feature enhancement for ceramic tile surface defect detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCR-YOLOv8: An enhanced algorithm for target detection in
sonar images. <em>JRTIP</em>, <em>22</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11554-025-01637-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonar imaging plays a pivotal role in the detection of targets underwater. However, the performance of most sonar detection methods is suboptimal due to the complexity of the underwater environment and its susceptibility to noise interference. To address these issues, this paper proposes a SCR-YOLOv8 algorithm based on an enhanced YOLOv8 architecture. This algorithm aims to overcome the challenges associated with traditional sonar target detection. The proposed approach involves two primary modifications: first, the Conv module in the trunk and neck networks is replaced with the more efficient SPDConv. Second, the CCFM module is introduced to reduce the model size and number of parameters. Subsequently, the Spatial Channel Reconstruction Module (SCRM) is employed. The design of the feature extraction and fusion stage of the model is intended to enhance the model’s efficiency in extracting contextual information from both spatial and channel dimensions. Finally, the Inner-CIoU is employed in lieu of the CIoU to achieve regression results that are both faster and more effective. The experimental results demonstrate that, in comparison with the baseline model, SCR-YOLOv8 enhances precision, recall, and mAP50 by 2.9%, 5.8%, and 2.3%, respectively. Concurrently, the model size and the computational complexity are diminished by 40.2% and 21.4%, respectively.Moreover, a frame rate of 91 FPS is attained, meeting the criteria for real-time detection.},
  archive      = {J_JRTIP},
  author       = {Weng, Youlei and Xiang, Xiaodong and Ma, Linghang},
  doi          = {10.1007/s11554-025-01637-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SCR-YOLOv8: An enhanced algorithm for target detection in sonar images},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight outdoor drowning detection based on improved
YOLOv8. <em>JRTIP</em>, <em>22</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-025-01638-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite abundant global water resources, drowning remains one of the complex challenges to tackle worldwide. To solve the problem of complicated outdoor remote water environments, where people in the water have different morphologies and scale variations, and the existing detection models are highly complex and computationally intensive, we propose a lightweight drowning detection model, YOLOv8-REH, based on YOLOv8n. First, the C2f-RVB-ELA feature extraction module (effective fusion of C2f and RVB-ELA) is designed to improve the C2f module of the YOLOv8n backbone network, reducing the model’s parameters and computation effectively. Second, in the Neck section, we incorporate the ELA-HSFPN feature fusion module, which consists of the Hierarchical Scale-based Feature Pyramid Network (HSFPN) module and the Efficient Local Attention (ELA) mechanism. This helps us gather multi-scale and spatial information perception more comprehensively and efficiently, enhancing the feature fusion capability of the model. Then, we introduce the Powerful-IoUv2 loss function to enhance bounding box regression along the effective path, consequently improving the model’s convergence speed and detection performance. Finally, we use a pruning method based on layer-adaptive magnitude-based pruning scoring to prune and remove unimportant redundant parameters from the improved model, further compressing the model complexity and achieving a better lightweight effect. The final compressed YOLOv8-REH model is compared with the current mainstream algorithms for comparison experiments as well as ablation experiments. The experimental results indicate that the YOLOv8-REH model sustains an average detection accuracy while the computational volume, parameter count, and model size reach 3.7GFLOPs, 0.72 M, and 1.8 MB, and the FPS is improved by 22.9, which achieves a significant improvement in the model’s lightweight performance compared with the existing methods.},
  archive      = {J_JRTIP},
  author       = {Liu, Xiangju and Shuai, Tao and Liu, Dezeng},
  doi          = {10.1007/s11554-025-01638-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight outdoor drowning detection based on improved YOLOv8},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMC-net: A lightweight network for real-time surface defect
segmentation. <em>JRTIP</em>, <em>22</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-025-01639-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial applications, surface defect segmentation is a critical task. However, facing challenges such as diverse defect scales, low contrast between defects and background, high interclass similarity and real-time detection in defect inspection, we propose an efficient lightweight network, named DMC-Net, for real-time surface defect segmentation. The structural optimization of DMC-Net includes the following components: (1) depthwise separable convolution attention module, a lightweight and efficient feature extraction module for extracting multi-scale defect features. (2) Multi-scale feature enhancement module, providing long-range information capture and local information focusing to enhance defect localization capability. (3) Channel shuffle group convolution, enhancing feature interaction and information propagation while reducing the parameter quantity. Based on the experimental results, DMC-Net achieved an mIoU of 73.74% on the NEU-SEG dataset, while achieving an FPS of 211.7. This indicates that we have successfully reduced the complexity and computational cost of the model while improving performance, providing a feasible solution for industrial applications. The relevant code can be obtained at https://github.com/Michaelzyb/DMC-Net.git.},
  archive      = {J_JRTIP},
  author       = {Zuo, Haiqiang and Zheng, Yubo and Huang, Qizhou and Du, Zehao and Wang, Hao},
  doi          = {10.1007/s11554-025-01639-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DMC-net: A lightweight network for real-time surface defect segmentation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time recognition research for an automated egg-picking
robot in free-range duck sheds. <em>JRTIP</em>, <em>22</em>(2), 1–15.
(<a href="https://doi.org/10.1007/s11554-025-01640-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving efficient and accurate detection and localization of duck eggs in the unstructured environment of free-range duck sheds is crucial for developing automated egg-picking robots. This paper proposes an improved YOLOv5s-based model (YOLOv5s-MNKS) designed to enhance detection performance, reduce model complexity, and improve the robot’s adaptability and operational efficiency in complex environments. The model utilizes MobileNetV3 as the backbone network, reducing the number of parameters and increasing detection speed. The Squeeze-and-Excitation Module is replaced with a Normalization-based Attention Module to improve feature extraction capability. Group Shuffle Convolution and Bidirectional Feature Pyramid Network are introduced in the Neck layer, enhancing multi-scale feature fusion while reducing parameter count. A Soft-CIoU-NMS loss function is also designed, which improves detection accuracy in scenarios involving dense stacking and occlusion by lowering the confidence of overlapping bounding boxes instead of directly eliminating them. Experimental results demonstrate that the mAP of YOLOv5s-MNKS reaches 95.6%, representing a 0.3% improvement over the original model, while the model size is reduced to 5.7 MB, approximately 40% of the original size. When deployed on the Jetson Nano embedded platform with TensorRT acceleration, the model achieves a detection frame rate of 22.3 frames per second. In simulated and real-world duck shed scenarios, the improved model accurately and quickly identifies and locates duck eggs in complex environments, including occlusion, stacking, and low lighting, demonstrating strong robustness and applicability. This research provides technical support for the future development of duck egg-picking robots.},
  archive      = {J_JRTIP},
  author       = {Jie, Dengfei and Wang, Jun and Wang, Hao and Lv, Huifang and He, Jincheng and Wei, Xuan},
  doi          = {10.1007/s11554-025-01640-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time recognition research for an automated egg-picking robot in free-range duck sheds},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EF-RT-DETR: A efficient focused real-time DETR model for
pavement distress detection. <em>JRTIP</em>, <em>22</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01641-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification and localization of road distress play a crucial role in intelligent road health monitoring systems. To address the challenges of complex road backgrounds, diverse shapes of distress objects, and high computational resource requirements, this paper proposes an efficient focusing real-time road distress detection model (EF-RT-DETR). Based on RT-DETR, the model designs a new backbone network aimed at accurately capturing fine features and optimizes the attention mechanism to effectively reduce interference from complex backgrounds while enhancing the processing of detailed information. Additionally, an innovative fusion module is introduced in the feature fusion stage to further enhance the interaction between local and global features, while also reducing computational costs. Experiments conducted on the China_Motorbike subset of the RDD2022 dataset include ablation studies to validate the effectiveness of the proposed modules. The experimental results show that EF-RT-DETR reduced background false positives compared to the baseline model, with $${\hbox {mAP}}_{50}$$ and $${\hbox {mAP}}_{50:95}$$ improving by 9.5 and 7.1%, respectively, while reducing computation by 35.4% and the number of parameters by 25.7%.},
  archive      = {J_JRTIP},
  author       = {Han, Tao and Hou, Shuainan and Gao, Can and Xu, Shanyong and Pang, Jiale and Gu, Hai and Huang, Yourui},
  doi          = {10.1007/s11554-025-01641-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EF-RT-DETR: A efficient focused real-time DETR model for pavement distress detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPGA-based 1D-CNN accelerator for real-time arrhythmia
classification. <em>JRTIP</em>, <em>22</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01642-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, cardiovascular diseases are prevalent, real-time arrhythmia detection through electrocardiogram (ECG) is a vital aspect of health monitoring. Consequently, there has been growing interest in wearable edge devices capable of real-time ECG classification. Current convolutional neural networks (CNN) for arrhythmia classification often involve a large number of parameters and have high computational complexity. This work introduces a one-dimensional lightweight convolutional neural network model (LW-CNN), which leverages residual connections and one-dimensional depthwise separable convolution (DSC). The proposed network shows accuracy achieving 99.59% on software-implementation with fewer parameters and lower computational complexity. A model compression method combining unstructured pruning and incremental network quantization (INQ) is implemented to further reduce the model complexity. Additionally, a neural network accelerator based on multiplication-free convolutional processing unit is designed with high level synthesis (HLS) to reduce resource consumption and achieve real-time ECG classification. The entire system is implemented on Xilinx Zynq 7Z020 board leveraging PS-PL synergy design and achieves classification accuracy of 96.55%, a latency of 63 ms under 50-MHZ and a power consumption of 1.78W with resource consumption of 13726 LUT, 9 DSP, and 5.5 BRAM, which improves resource efficiency.},
  archive      = {J_JRTIP},
  author       = {Liu, Zheming and Ling, Xiaofeng and Zhu, Yu and Wang, Nan},
  doi          = {10.1007/s11554-025-01642-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-based 1D-CNN accelerator for real-time arrhythmia classification},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Egc-yolo: Strip steel surface defect detection method based
on edge detail enhancement and multiscale feature fusion.
<em>JRTIP</em>, <em>22</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11554-025-01644-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to imperfect manufacturing crafts and external factors, steel often produces surface defects during manufacturing, seriously influencing its lifespan and availability. It is therefore crucial that surface defects are detected in industrial production. Nevertheless, conventional detection techniques are vulnerable to background interference and feature scale variations when employed to identify defects on strip surfaces. Therefore, we propose an EGC-YOLO model based on YOLOv8 for steel surface defect detection. First, an edge detail enhancement module (EDEM), based on Sobel convolution (SobelConv), is designed and embedded into C2f to capture defective edges and texture better. Second, the generalized dynamic feature pyramid network (GDFPN) is introduced in the neck structure to enhance the multiscale feature fusion. This enables the model to adapt to defects of different sizes and shapes. Finally, the content-guided attention fusion (CGA Fusion) module is employed to optimize the fusion of shallow and deep features for more detection precision. The extensive experimental results illustrate that the accuracy of EGC-YOLO reaches 80.2% mAP on NEU-DET and improves by 3.7% over YOLOv8. The model’s inference speed reached 136.4 frames per second (FPS). EGC-YOLO outperforms other models in accuracy and speed for detecting steel surface defects, showcasing its industrial application potential.},
  archive      = {J_JRTIP},
  author       = {Ni, Yunfeng and Zi, Dexing and Chen, Wei and Wang, Shouhua and Xue, Xinyi},
  doi          = {10.1007/s11554-025-01644-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Egc-yolo: Strip steel surface defect detection method based on edge detail enhancement and multiscale feature fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight real-time object detection method for complex
scenes based on YOLOv4. <em>JRTIP</em>, <em>22</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-025-01645-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object detection network can achieve real-time performance on high-performance computers with ease, but the large number of parameters and limited computational resources of mobile devices pose significant challenges, leading to suboptimal detection performance. As application scenarios expand to embedded devices, such as autonomous vehicles and unmanned aerial vehicles, higher requirements on the real-time performance and resource consumption of object detection algorithms have been proposed. The traditional YOLOv4 model has huge parameters and computations, resulting in low detection efficiency in complex environments. To address it, we propose an improved YOLOv4-lite lightweight network based on depthwise over-parameterized convolutional layer (DO-Conv). Firstly, we replace CSPDarknet53 backbone network in YOLOv4 with MobileNetV3. The parameter quantity of YOLOv4-lite is only 62.4 $$\%$$ of YOLOv4. Secondly, we use DO-Conv to replace the traditional convolution network in YOLOv4 to promote feature extraction effectiveness without extending layers. Meanwhile, we use ReLU6 to replace original Leaky ReLU to improve detection efficiency and obtain good numerical resolution. Our method achieved 70.38 $$\%$$ mean average precision (mAP) on Pascal VOC07+12 dataset and 27.63 $$\%$$ average precision (AP) on MS COCO 2017 dataset. Its model size is only 34MB. The running speed on Titan X reaches 41.82 frames per second (fps), which is 1.7 times that of YOLOv4. The experimental results demonstrate that the proposed method achieves a well-balanced trade-off between speed and accuracy, thereby meeting the real-time requirements for object detection in practical applications.},
  archive      = {J_JRTIP},
  author       = {Ding, Peng and Li, Tong and Qian, Huaming and Ma, Lin and Chen, Zhongfei},
  doi          = {10.1007/s11554-025-01645-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight real-time object detection method for complex scenes based on YOLOv4},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective geometrical feature extraction method for scale
and rotational invariant multi-lingual character recognition.
<em>JRTIP</em>, <em>22</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-025-01646-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new feature extraction technique for Optical Character Recognition (OCR) that achieves state-of-the-art results in the recognition of multilingual characters, overcoming scale, rotation, and distortion challenges. Our method leverages polar coordinates to incorporate two innovative features: Extended Ellipse-Based Features (EEB) and Crossing Count Measure (CCM), which provide inherent scale and rotational invariance. From the benchmark datasets, the proposed technique was tested on character datasets such as ISI Bengali and Chars74K, yielding accuracy rates of 98.82% and 98.69% respectively. These statistics also depict high precision and recall values coupled with a high F1-score, indicating that the method has been sound and robust. Interestingly, our approach exhibits far less computational overhead compared to traditional CNN-based methods and is, therefore, a good candidate to be deployed on resource-constrained edge devices. This work fills the gap between high-performance OCR systems and practical deployment needs by providing a scalable and efficient solution for multilingual character recognition in diverse and challenging contexts.},
  archive      = {J_JRTIP},
  author       = {Mohammed, Sharfuddin Waseem and Murugan, Brindha},
  doi          = {10.1007/s11554-025-01646-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An effective geometrical feature extraction method for scale and rotational invariant multi-lingual character recognition},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating convolutional neural networks on FPGA
platforms: A high-performance design methodology using OpenCL.
<em>JRTIP</em>, <em>22</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11554-025-01647-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) are among the most promising algorithms, outperforming traditional methods in classification tasks with superior accuracy. They have been widely applied across various deep learning domains, including computer vision, speech recognition, image processing, and object detection. However, many CNNs require substantial computational resources, particularly within their convolutional layers. As high-performance CNNs continue to evolve, their processing and memory requirements are also increasing. To address these challenges, this paper proposes an effective design methodology for accelerating CNN algorithms on Field-Programmable Gate Array (FPGA) hardware architectures. The proposed methodology introduces a novel approach for accelerating CNN algorithms using FPGAs, addressing the significant processing and memory demands associated with CNNs. The implementation is based on Open Computing Language (OpenCL), which provides rapid implementation flows. This approach was chosen for its efficiency in reducing development time and eliminating the need to manually write hardware description language (HDL) code. The MNIST and the CIFAR-10 datasets on the Xilinx ZYNQ 7000 device were used to evaluate our approach. Our method achieved a 97% recognition rate on MNIST and an 86% recognition rate on CIFAR-10. We compared the execution time of our accelerated CNN kernel on the FPGA with that of a single-core Central Processing Unit (CPU). The experimental results demonstrate that our proposed design is 10 times faster than a standard CPU, validating its effectiveness. Our model optimizes power consumption and performance, exceeding previous studies in accuracy and efficiency. It is well suited for real-world applications that demand both precision and energy efficiency.},
  archive      = {J_JRTIP},
  author       = {Gdaim, Soufien and Mtibaa, Abdellatif},
  doi          = {10.1007/s11554-025-01647-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accelerating convolutional neural networks on FPGA platforms: A high-performance design methodology using OpenCL},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ship detection algorithm based on structural reparameterize
dilated large-kernel convolution and spatial selective kernel mechanism.
<em>JRTIP</em>, <em>22</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-025-01649-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the issues of large target scale variations and complex background environments in ship detection, this paper presents a single-stage ship detection algorithm named DrbLSK, which is an improvement on YOLOv5. First, to enhance the model’s ability to model the context of targets with different scales, a kernel selection module is introduced into the backbone network. By dynamically selecting the receptive field, environmental interference is reduced. Simultaneously, the combination of large-kernel dilated convolution and structural reparameterization is employed to reduce model parameters and enhance the feature expression capability of the backbone network. Next, a decoupled detection head is utilized to alleviate the conflict between classification and regression tasks in the target detection task. Moreover, CIoU is used in the detection head to replace the original loss function, thereby accelerating the convergence of the network. Experimental results show that, on the ABOships dataset, the improved model reduces the number of parameters while increasing accuracy by 2.5% compared to YOLOv5, with a mean average precision of 63.9.},
  archive      = {J_JRTIP},
  author       = {Shi, Yujing and Wang, Haicheng and Li, Shanqiang},
  doi          = {10.1007/s11554-025-01649-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Ship detection algorithm based on structural reparameterize dilated large-kernel convolution and spatial selective kernel mechanism},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ES-YOLOv8: A real-time defect detection algorithm in
transmission line insulators. <em>JRTIP</em>, <em>22</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-025-01651-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulators are essential components for protecting transmission lines, yet they are vulnerable to defects caused by harsh environments, which can compromise their performance and transmission stability. Regular inspections are therefore crucial. However, traditional manual inspection methods are time-consuming, error-prone, and pose safety risks when working under adverse conditions. In contrast, the combination of drone inspections and image processing technology enables the safe and efficient detection of insulator defects. In this study, the YOLOv8 model was used to detect insulators. Although YOLOv8 exhibits strong detection performance, it encounters challenges in terms of detection speed and computational efficiency in real-time tasks. To address these challenges, this study introduces EfficientViT from the Transformer framework to enhance feature extraction and improve the model’s computational efficiency. Additionally, a spatial context pyramid was incorporated into the model’s header module to bolster its feature analysis capabilities, particularly for small targets and intricate background details, yielding notable results. After several enhancements, the proposed network is named ES-YOLOv8s. The experimental results reveal that compared with YOLOv8, ES-YOLOv8s improved the precision, recall, mean average precision, and harmonic mean of the precision and recall scores on the Insulator Defect Image Dataset by 2%, 3%, 2.7%, and 3%, respectively. Furthermore, the ES-YOLOv8s exhibited notable advantages in terms of giga floating point operations per second and frames per second, highlighting substantial improvements in both detection performance and computational efficiency. The study findings show that the improved model can perform better in real-time detection tasks.},
  archive      = {J_JRTIP},
  author       = {Song, Xiaoyang and Sun, Qianlai and Liu, Jiayao and Liu, Ruizhen},
  doi          = {10.1007/s11554-025-01651-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ES-YOLOv8: A real-time defect detection algorithm in transmission line insulators},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced YOLOv5 for micro-defect detection on KDP crystal
surfaces: A fusion of EfficientNetV2 and normalized wasserstein
distance. <em>JRTIP</em>, <em>22</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01654-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of micro-defects on potassium dihydrogen phosphate (KDP) crystal surfaces is crucial for the efficient operation of high-energy laser equipment. However, traditional defect detection methods suffer from low accuracy and efficiency. To address this, we propose an enhanced YOLOv5 model specifically tailored for micro-defect detection on KDP crystals. Our approach integrates EfficientNetV2 as the backbone for feature extraction, reducing parameters by 12.68% and computational cost by 10.8%. Furthermore, we introduce the XIoU loss function and incorporate the normalized Wasserstein distance (NWD) theory, forming a novel loss function that improves the smoothness of bounding boxes and enhances the model&#39;s ability to learn and distinguish micro-defect features. We created a dataset comprising micro-defect images of KDP crystal surfaces. Experimental results on this dataset show that our method achieves a mean average precision (mAP) of 96.9% and an F1 score of 0.944, outperforming other mainstream models. This study presents a novel and effective approach for micro-defect detection on KDP crystal surfaces, contributing to advancements in ultra-precision machining technology. For further details and to access the dataset and model, please visit our repository: https://github.com/Good-he/EXN-yolo/tree/master .},
  archive      = {J_JRTIP},
  author       = {Feng, Kai and He, Shuhao and Wu, Xinlong and Jiang, Peidong and Huang, Shuai},
  doi          = {10.1007/s11554-025-01654-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhanced YOLOv5 for micro-defect detection on KDP crystal surfaces: A fusion of EfficientNetV2 and normalized wasserstein distance},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low brightness PCB image enhancement algorithm for FPGA.
<em>JRTIP</em>, <em>22</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-025-01635-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of low defect detection rate of PCB images captured by cameras in industrial scenarios under low-light environments, an MGIE (Mean–Gamma Image Enhancement) image brightness enhancement algorithm and the corresponding FPGA design scheme are proposed. Firstly, the RGB image is converted into the YCrCb color space, and the illumination component Y is separated. Then, the illumination component Y is enhanced by the MSR (Multi-Scale Retinex) algorithm based on multi-scale mean filtering, and the Gamma correction algorithm is used to adjust the brightness. Subsequently, the processed Y channel is fused with the Cr and Cb channels to obtain the final output. Secondly, after algorithm research, this paper elaborates on the algorithm design and deployment scheme based on FPGA. The MGIE IP core is designed in the HLS (High-Level Synthesis) environment, and optimization and acceleration are carried out by means of creating look-up tables and constructing PIPELINE. Significantly, this research is capable of real-time processing of images in video. Specifically, images are captured in real time by the OV5640 camera, and the processed images are immediately displayed on the LCD screen. The experimental results show that the MGIE algorithm has remarkable effectiveness in processing low-light PCB images, with a PSNR (Peak Signal-to-Noise Ratio) reaching 17.34 and an SSIM (Structural Similarity Index Measure) reaching 0.79. After the end-to-end deployment, the processing speed of 1280 × 720 and 640 × 640 pixel images reaches 30fps/s and 70fps/s, respectively, meeting the needs of real-time processing.},
  archive      = {J_JRTIP},
  author       = {Han, Jin and Zheng, Meijuan and Dong, Jianye},
  doi          = {10.1007/s11554-025-01635-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low brightness PCB image enhancement algorithm for FPGA},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPGA architecture-based front-end processing for SLAM
applications. <em>JRTIP</em>, <em>22</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-025-01650-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping is intended for robotic and autonomous vehicle applications. These targets require an optimal embedded implementation that respects real-time constraints, limited hardware resources, and energy consumption. SLAM algorithms are computationally intensive to run on embedded targets, and often, the algorithms are deployed on CPUs or CPU–GPGPU architectures. With the growth of embedded heterogeneous computing systems, research work is increasingly interested in the algorithm–architecture mapping of existing SLAM algorithms. The latest trend is pushing processing closer to the sensor. FPGAs constitute the perfect architecture for designing smart sensors by providing low latency suitable for real-time applications, such as video streaming, as they supply data directly into the FPGA without needing a CPU. In this work, we propose the implementation of the HOOFR-SLAM front end on a CPU–FPGA architecture, including both feature extraction and matching processing blocks. A high-level synthesis (HLS) approach based on OpenCL paradigm has been used to design a new system architecture. The performance of the FPGA-based architecture was compared to a high-performance CPU. This innovative architecture delivers superior performance compared to existing state-of-the-art systems.},
  archive      = {J_JRTIP},
  author       = {El Bouazzaoui, Imad and Rodríguez Flórez, Sergio and El Ouardi, Abdelhafid},
  doi          = {10.1007/s11554-025-01650-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA architecture-based front-end processing for SLAM applications},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV inspection insulator defect detection method based on
dynamic adaptation improved YOLOv8. <em>JRTIP</em>, <em>22</em>(2),
1–17. (<a href="https://doi.org/10.1007/s11554-025-01660-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulators play an important role in ensuring the electrical stability of transmission systems, and timely detection of insulator status can effectively guarantee the safe and stable operation of power systems. However, the generalized YOLOv8 model still faces two challenges in the complex application scenario of power inspection by Unmanned Aerial Vehicles (UAVs) for high-altitude insulator defects: one is the standard convolution’s inability to efficiently extract insulator edge features, which leads to a large number of convolution operations and results in parameter redundancy; the other is the distortion of feature spatial information fusion due to complex background interference. To solve these challenges, this paper proposes a lightweight and dynamically adaptive model named DVW-YOLO (Dynamic VoV Wise YOLO, based on YOLOv8). First, a Dynamic Elliptic Convolution is designed to make the model more efficient in focusing on elliptic features, significantly reducing redundant feature extraction and multi-size fusion, and thus significantly reducing the model parameters. Then, a VoV-DE-GSCSP module is designed, and a Dynamic Adaptive Fusion Network is developed in combination with GSConv. This network restricts the cross-fusion of complex background interference information and enhances the effective feature fusion range. Finally, Wise-IoU is used to balance the performance between lightweight and average accuracy. Experimental results demonstrate that, compared with the standard YOLOv8 model, the lightweight performance of the model proposed in this paper has been greatly improved, with a reduction of 11.87% in parameter quantity and a 2.3% increase in $$\hbox {mAP}_{0.75}$$ . Meanwhile, compared with the same level lightweight model (YOLOv5), the F1-score increased by 4%, and $$\hbox {mAP}_{0.75}$$ increased by 7.4%, with a significant improvement in detection performance.},
  archive      = {J_JRTIP},
  author       = {Hu, Cong and Lv, Lingfeng and Zhou, Tian},
  doi          = {10.1007/s11554-025-01660-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {UAV inspection insulator defect detection method based on dynamic adaptation improved YOLOv8},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification of rice disease based on MFAC-YOLOv8.
<em>JRTIP</em>, <em>22</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11554-025-01661-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rice has an important place as food for more than half of the world’s population, but its yield and stability are severely affected by rice diseases. Current rice disease detection methods are inefficient and costly. To address this issue, this study collected 1505 data points and images from rice fields and the web and annotated them for training frameworks. After that, this study developed a new rice detection framework, MFAC-YOLOv8, based on the YOLOv8 network. The framework integrates the MobileNetv4 network and the Focal Modulation module and uses them as the backbone network of the improved YOLOv8 to improve the detection accuracy of the network. In addition, AKConv and the Context Guided block are introduced and used to further improve the neck network of YOLOv8, which simplifies the framework and further enhances the detection. The experimental results show that the MFAC-YOLOv8 framework exhibits excellent performance in all evaluation metrics, with 8.1 $$\%$$ , 2.2 $$\%$$ , and 3.4 $$\%$$ improvements in accuracy, recall, and mean average precision, respectively, compared with the baseline framework. In addition, the framework achieves a high frame rate of 166 frames per second (FPS) and a relatively compact parameter size of 18.4 million. These results suggest that the proposed method has great potential for effective rice disease detection.},
  archive      = {J_JRTIP},
  author       = {Wang, Bingyang and Zhou, Huibo and Xie, Hui and Chen, Ruolan},
  doi          = {10.1007/s11554-025-01661-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Identification of rice disease based on MFAC-YOLOv8},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vaccine-YOLOv10: Real-time QR code detection model for
complex light condition. <em>JRTIP</em>, <em>22</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01631-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {QR code is not only an information storage approach, but also a spatial localization sign. Compared to other spatial localization signs, QR code is more accurate and more efficient to be detected. To achieve spatial localization by QR code, detection is the essential procedure. Existing approaches perform well in regular light condition, but perform badly in complex light condition, because frame quality is extremely damaged by complex light condition. In the real world, complex light condition is very common but always unavoidable. Therefore, it is necessary and worthwhile to improve the under-complex-light QR code detection. In this paper, Vaccine-YOLOv10 (VCY) is proposed to enhance QR code detection capability in complex light condition. First, GhostConv and FasterC2f are introduced to replace the corresponding original modules of YOLOv10n. Second, Simulative Data Augment Algorithm (SDA) is proposed to simulate 5 types of complex light condition. Third, self-built Multi-Scene QR Code Dataset (MSQ) is augmented by SDA for VCY training. Compared to the baseline model YOLOv10n, VCY is improved on both lightweight and accuracy. Specifically, FPS reaches 150; GFLOPs reduces from 8.2 to 5.3; mAP50 increases from 0.877 to 0.905. Code: https://github.com/AlexTraveling/Vaccine-YOLOv10 .},
  archive      = {J_JRTIP},
  author       = {Zhao, Xiaobei and Li, Xiang},
  doi          = {10.1007/s11554-025-01631-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Vaccine-YOLOv10: Real-time QR code detection model for complex light condition},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPGA implementation of double-head SalsaNext: A CNN-based
model for LiDAR point cloud segmentation. <em>JRTIP</em>,
<em>22</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11554-025-01643-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study details the adaptation and deployment of a customized SalsaNext model for semantic segmentation of LiDAR point clouds on edge devices, benchmarked using the SemanticKITTI and Waymo Open datasets. We introduce an innovative multi-dataset training framework designed specifically for range image-based segmentation models. Central to this approach is our double-head SalsaNext model, which features two output heads to facilitate simultaneous training and inference on the Waymo and SemanticKITTI datasets. Following training, the model is streamlined by removing the head dedicated to Waymo, resulting in a compact, single-headed version optimized for SemanticKITTI. This simplified model is then quantized to employ fixed-point arithmetic, significantly enhancing computational efficiency and enabling real-time operation on the Xilinx Kria KV260 board. The quantization process markedly reduces resource consumption while preserving competitive accuracy. Our deployment on this low-power, FPGA-based platform underscores the potential of energy-efficient systems for advanced 3D semantic segmentation, with promising applications in autonomous systems and robotics. Experimental results validate the effectiveness of our training schema and the success of the optimized implementation of the double-head model on resource-constrained hardware.},
  archive      = {J_JRTIP},
  author       = {Adiyaman, Muhammed Yasin and Baskaya, Faik},
  doi          = {10.1007/s11554-025-01643-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA implementation of double-head SalsaNext: A CNN-based model for LiDAR point cloud segmentation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-VG: An efficient real-time recyclable waste detection
network. <em>JRTIP</em>, <em>22</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-025-01655-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of sorting waste using robots, it is necessary to determine the location and category information of waste. The key to the efficient work of the waste sorting robot lies in the accuracy of the target recognition. Traditional waste detection algorithms suffer from high computational cost, low detection accuracy, and poor adaptability, which cannot meet the actual detection needs. Therefore, this paper proposes an efficient and lightweight algorithm YOLO-VG based on the current mainstream algorithm YOLOv8s for waste detection and classification. The algorithm replaces the conventional convolution in the backbone network with GSConv to reduce redundant information and improve the model’s inference process. ODConv replaces the regular convolution in the neck network to enhance the model’s adaptability and generalization ability, thereby reducing the risk of overfitting. Additionally, the VoV-Ghost structure is introduced in the neck network to replace the original C2f, making the model more lightweight and efficient, meeting the requirements of real-time object detection in embedded devices or resource-constrained environments. Finally, the ECA attention mechanism is introduced to improve the ability of information interaction between feature channels in the model, thereby better capturing important information between images or features. Experimental results on the recyclable waste dataset demonstrate that the proposed YOLO-VG achieves a 24.6% improvement in computational efficiency, a 20.4% reduction in model size, and an mAP0.5 of 88.4%, surpassing the performance of the original YOLOv8s. These results indicate that YOLO-VG not only demonstrates excellent detection performance and stability, but also exhibits significant potential for widespread application in the field of waste sorting.},
  archive      = {J_JRTIP},
  author       = {Song, Limei and Yu, Haibo and Yang, Yangang and Tong, Yu and Ren, Siyuan and Ye, Chenchao},
  doi          = {10.1007/s11554-025-01655-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-VG: An efficient real-time recyclable waste detection network},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Study on end-to-end detection method for surface defects of
automotive sheet metal parts. <em>JRTIP</em>, <em>22</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-025-01656-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sheet metal parts account for more than 60% of the total automotive parts, and their defects can seriously affect the safety of automobile operations. Therefore, it is very important to detect defects in sheet metal parts during the production process. Due to the small size of defects in sheet metal parts, and high detection precision required, the traditional detection method cannot meet the requirements. And the factory production speed is fast, if the detection speed is low, it will cause defects to escape. Therefore, we propose an end-to-end detection method for automotive sheet metal parts surface defects. To effectively improve the detection speed, the dual regression classification strategy is proposed, which removes the NMS post-processing. Gradient information branch is added to provide rich gradient information for the model and mitigate the information loss during long convolution. Use the SPD-Conv module, optimized for small-size defects detection, to retain complete space information. Finally, the model is evaluated on the automotive sheet metal parts defect dataset. The experimental results show that the proposed method is superior to the benchmark methods in precision and speed, with mAP of 92.32% and FPS of 39.06, which achieves end-to-end detection.},
  archive      = {J_JRTIP},
  author       = {Dai, Wei and lv, Juncheng and Xiang, Rui and Jin, Sun},
  doi          = {10.1007/s11554-025-01656-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Study on end-to-end detection method for surface defects of automotive sheet metal parts},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LR-DETR: A lightweight real-time traffic sign detection
model based on improved RT-DETR. <em>JRTIP</em>, <em>22</em>(2), 1–16.
(<a href="https://doi.org/10.1007/s11554-025-01659-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time detection of traffic signs is crucial for safe autonomous driving and intelligent transportation systems. The current key challenge in the field of traffic sign detection is to achieve model lightweighting and improve real-time performance while maintaining effectiveness. To address these challenges, this paper proposes a lightweight real-time traffic sign detection model called LR-DETR. LR-DETR is based on the end-to-end object detection model of RT-DETR. By redesigning the core modules in RT-DETR, LR-DETR significantly improves the lightweighting level of the model while maintaining high detection accuracy. A PCIR-Block module based on Partial Convolution and Inverted Residual Structure is proposed to more fully extract multi-scale features in the backbone network. A Context-Guided Feature Fusion Module (CGFFM) is proposed to utilize contextual information between features of different scales to enhance the effectiveness of feature representation and subsequently improve the fusion performance of multi-scale features. In addition, with the help of dilated convolution and reparameterization techniques, LR-DETR designed a DRBC3 module for feature re-extraction to further enhance the model’s ability to capture features at different scales, while effectively reducing the number of parameters and floating-point operations. The experimental results on the CCTSDB 2021 dataset show that compared with the state-of-the-art baseline models, LR-DETR performs better in increasing the value of mAP@0.5 by 0.5%, decreasing the value of FLOPs by 28.1%, decreasing the value of Params by 23.7%, and decreasing the value of Latency by 11.6%. The experimental results on the TT100K traffic sign dataset show that the precision and recall of the LR-DETR model are 87.1% and 81.6%, respectively, outperforming other baseline models. LR-DETR significantly reduces the number of parameters and floating-point operations while maintaining high detection performance, improving the detection speed of the model. This will provide a constructive contribution to achieving real-time detection of traffic signs.},
  archive      = {J_JRTIP},
  author       = {Zhang, Longzhen and Wang, Mingyang and Zhao, Xianhao and Wang, Xianjie},
  doi          = {10.1007/s11554-025-01659-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LR-DETR: A lightweight real-time traffic sign detection model based on improved RT-DETR},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time traffic light detection based on lightweight
improved RT-DETR. <em>JRTIP</em>, <em>22</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11554-025-01652-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing traffic light detection algorithms suffer from high computational overhead and low detection speeds, making it difficult to meet the real-time demands. Therefore, reducing computational overhead and increasing detection speed, while maintaining accuracy, becomes a critical challenge. To tackle these, this paper proposes GAD-DETR, an enhanced RT-DETR-based network. First, inspired by the approach of GhostNet to minimize computational redundancy and integrate reparameterized convolution (RepConv), the GRELAN module is developed to restructure the backbone network which significantly decreases model size and parameters while enhancing detection speed. To improve the recognition of small objects, whose features tend to be diluted as the network deepens, ADown is introduced to replace standard convolution for downsampling. Finally, a lightweight feature fusion module, DGSFM, is designed to further reduce computational costs and enhance efficiency. Experimental results indicate that GAD-DETR achieves a detection precision of 95.9% while significantly optimizing efficiency. The model size is reduced by 50.3%, with parameters and computations decreased by 50.8% and 51.2%, respectively. FPS increases from 76.7 to 117.8, demonstrating that the proposed algorithm achieves lightweight, real-time traffic light detection.},
  archive      = {J_JRTIP},
  author       = {Tang, Chaoli and Li, Yun and Wang, Lei and Li, Wenyan},
  doi          = {10.1007/s11554-025-01652-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time traffic light detection based on lightweight improved RT-DETR},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FDDC-YOLO: An efficient detection algorithm for dense
small-target solder joint defects in PCB inspection. <em>JRTIP</em>,
<em>22</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11554-025-01664-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays printed circuit board plays a vital role in communication, computer, electronics and other industries. Existing PCB welding defect detection algorithms have the problems of low accuracy and poor real-time performance in identifying small or irregular targets and dense solder joints. This is due to the limited receptive field of standard convolutional kernels, which hinder global feature extraction and focus on local details. Moreover, the effects of kernel count and feature extraction dimensions are often overlooked, leading to the loss of important features. Conventional upsampling methods, such as nearest-neighbor interpolation, can further degrade critical information. To address these challenges, we propose FDDC-YOLO, a novel defect detection network. First, we introduce a new full-dimensional dynamic convolution module FDDC, which integrates full-dimensional dynamic convolution with the newly designed od_ottleneck structure to enhance the feature extraction ability by using the four dimensions of the convolution kernel. Secondly, the CECA attention module in the neck improves the ability of the model to detect small defects by enhancing the local interaction between channels. Third, the Dy-Up module is used to improve image resolution and prevent the loss of detailed information during the detection process. Finally, we replace the CIoU loss with IShapeIoU to reduce the overlap of detection boxes in densely packed solder joints, improving both localization accuracy and convergence speed.The mAP of FDDC-YOLO is improved by 5.4% on the PCBSP_dataset, and a Frame Per Second (FPS) of 189. It improves by 3.8% on the public PCB Defect-Augmented dataset, which proves its good generalization ability.},
  archive      = {J_JRTIP},
  author       = {Zheng, Haoyu and Peng, Jinmin and Yu, Xinyi and Wu, Meishun and Huang, Qiufang and Chen, Liangshen},
  doi          = {10.1007/s11554-025-01664-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FDDC-YOLO: An efficient detection algorithm for dense small-target solder joint defects in PCB inspection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved YOLOv8 model for prohibited item detection with
deformable convolution and dynamic head. <em>JRTIP</em>, <em>22</em>(2),
1–17. (<a href="https://doi.org/10.1007/s11554-025-01665-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray security inspection is critical for maintaining public safety and transportation security. However, traditional manual inspection methods are often ineffective due to the challenges posed by complex backgrounds and severe occlusions in X-ray images, resulting in false positives and negatives. This study proposes an enhanced object detection framework based on the YOLOv8 model to address these challenges. Key improvements include the integration of the ADown downsampling module to reduce computational complexity while enhancing detection accuracy and the incorporation of Deformable Convolutional Networks v2 (DCNv2) to improve deformable feature extraction. To strengthen feature representation, the Spatial Pyramid Pooling-Fast with ReLU and Efficient Local Attention (SPPF_RE) module is introduced to effectively integrate global and local features. Additionally, the Dynamic Head (DyHead) module is employed to enhance detection in complex backgrounds, while the Pixels-IOU (PIoU) loss function improves the detection accuracy of rotated objects. Experimental results on the OPIXray and HIXray datasets demonstrate that the proposed framework significantly outperforms the baseline model, achieving notable improvements in detection accuracy. The code can be accessed via the following link: https://github.com/Guanfj2024/x-ray-detection.git},
  archive      = {J_JRTIP},
  author       = {Guan, Fangjing and Zhang, Heng and Wang, Xiaoming},
  doi          = {10.1007/s11554-025-01665-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improved YOLOv8 model for prohibited item detection with deformable convolution and dynamic head},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch evidential framework fusing hard example mining
for abdominal organ segmentation. <em>JRTIP</em>, <em>22</em>(2), 1–14.
(<a href="https://doi.org/10.1007/s11554-025-01648-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {U-Net variants commonly encounter limitations due to overconfidence in predictions, impeding their clinical applicability. Quantifying model uncertainty accurately is vital, but obtaining sufficient and reliable evidence remains challenging. This paper introduces the $$\textbf{D}$$ ual-Branch $$\textbf{Evi}$$ dential Framework Fusing $$\textbf{H}$$ ard $$\textbf{E}$$ xample $$\textbf{M}$$ ining for Abdominal Organ Segmentation (DEvi-HEM). It is a novel dual-branch framework integrating hard example mining (HEM) at region and pixel levels. By applying higher penalty weights to hard examples, HEM improves fine-grained prediction. The dual-branch structure enhances the model’s expressiveness by learning from both region-level and pixel-level representations. Furthermore, the introduction of dual-branch consistency learning and adversarial learning-based variational distributions captures the cognitive variability across branches. This ensures precise segmentation and reliable uncertainty estimation. DEvi-HEM improves segmentation performance, cuts computational cost, and outperforms uncertainty-based methods, with 3.292 GFLOPs on FLARE22 and 2.486 GFLOPs on Synapse.},
  archive      = {J_JRTIP},
  author       = {Yu, Xiangchun and Wu, Tianqi and Zhang, Dingwen and Liang, Miaomiao and Yu, Lingjuan and Zheng, Jian},
  doi          = {10.1007/s11554-025-01648-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dual-branch evidential framework fusing hard example mining for abdominal organ segmentation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-PDC: Algorithm for aluminum surface defect detection
based on multiscale enhanced model of YOLOv7. <em>JRTIP</em>,
<em>22</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11554-025-01658-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address challenges multi-scale variability, category imbalance, and high background similarity in aluminum surface defect detection, this paper proposes a YOLO-PDC model. First, Partial Convolution (PConv) and Deformable ConvNetsv2 (DCNv2) replace traditional convolution in the ELAN and MaxPool modules of the YOLOv7 backbone. This configuration forms the PD-ME module, which mitigates the issue of non-uniform scale variations among different defect types in aluminum dataset. It also reduces computational redundancy and memory access, enabling efficient extraction of spatial features and improving inference speed. Next, a 3D attention module (SimAM) is incorporated into YOLOv7 detection head after two up-sample steps and within two MaxPool structures, creating the Sim-CM Attention Mechanism. This addition enhances detection accuracy without introducing additional parameters. Additionally, during training, the Focal loss function replaces CIoU loss function. Focal loss dynamically decreases the weight of easily distinguishable samples through a scaling factor, allowing the model to focus on hard-to-distinguish samples and addressing low detection accuracy caused by sample imbalance. Experimental results demonstrate that the proposed YOLO-PDC model achieves a high mean Average Precision (mAP) of 87.7% and a real-time detection speed of 114 frames per second. Compared to the original YOLOv7, mAP50 and mAP50:90 improve by 5.2% and 12.2%, respectively, while the number of parameters and computations decrease by 2.18 million and 22.2 billion, respectively. Furthermore, compared to the latest defect detection models DETR, Swin-T, and ConvNeXt-T, the mAP50 of YOLO-PDC is higher by 15.2%, 17.9%, 16.2%, respectively. YOLO-PDC also surpasses existing state-of-the-art detection methods in terms of detection accuracy.},
  archive      = {J_JRTIP},
  author       = {Li, Na and Wang, Zhiwen and Zhao, Runxing and Yang, Kaiqi and Ouyang, Rongyi},
  doi          = {10.1007/s11554-025-01658-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-PDC: Algorithm for aluminum surface defect detection based on multiscale enhanced model of YOLOv7},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-cost real-time traffic situational awareness system
based on modified YOLO v8 and GWO-LSTM for edge deployment.
<em>JRTIP</em>, <em>22</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11554-025-01657-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a traditional traffic situational awareness system (TSAS), its “Road-side unit (RSU) + cloud-based analysis” structure is difficult to meet the demands of rapidly expanding urban areas. Relatively high costs of microwave speed detection modules and bandwidth requirements of information systems significantly increase construction costs. By computer vision (CV) and edge computing technologies, traffic situational awareness tasks can be integrated into cheaper edge devices (roadside surveillance, RSS), effectively addressing such challenges. In this study, we present a low-cost TSAS developed based on YOLO v8 and grey wolf optimizer-long short-term memory (GWO-LSTM) neural network. Proposed system can automatically perform vehicle and license plate recognition, speed measurement, and data recording within the field of view of RSSs. Additionally, it accurately predicts the future traffic conditions of monitored roads using recorded information. Experimental results demonstrate that the proposed TSAS achieves a license plate recognition accuracy of 97.7%, vehicle type recognition accuracy of 98.1%, and speed measurement error of less than 0.45 km/h, with R2 of 0.8971 for GWO-LSTM predictions. This system is sufficiently effective for traffic monitoring and situational awareness tasks but enforcement forensic applications.},
  archive      = {J_JRTIP},
  author       = {Liu, Jianwen and Gong, Ruyue and Gong, Yi and Li, Zeqin and Chen, Zhiwei},
  doi          = {10.1007/s11554-025-01657-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-cost real-time traffic situational awareness system based on modified YOLO v8 and GWO-LSTM for edge deployment},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stme-net: Spatio-temporal motion excitation network for
action recognition. <em>JRTIP</em>, <em>22</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11554-025-01662-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video action recognition, as one of the fundamental tasks in video understanding, relies crucially on accurate temporal modeling. However, accurately modeling the temporal information of videos remains a challenging task. To address this problem, we design two new modules: the Spatial Motion Extraction (SME) module and the Spatio-temporal Motion Excitation (STME) module. The SME module features two branches for extracting motion and spatial features. The motion branch refines pixel differences between neighboring frames through a channel attention module, enhancing detailed motion features. These features are fused with spatial information to yield fine-grained local spatio-temporal features. The STME module, comprising the multi-motion excitation (MME), temporal excitation (TE), and spatio-temporal excitation (STE) sub-modules, efficiently captures long-range motion, temporal, and global spatio-temporal features. The MME introduces a bi-directional, multi-scale structure for effective long-range motion extraction, while the TE module employs a hierarchical pyramid with residual connectivity for fine-grained long-range temporal extraction. The STE module utilizes 3D convolutional layers for global spatio-temporal feature extraction. The seamless integration of these sub-modules within a standard ResNet network forms the Spatio-temporal Motion Excitation Network. Extensive evaluations on Something V1 and V2 and HMDB51 datasets against state-of-the-art methods demonstrate the effectiveness of our approach in achieving accurate recognition of both simple and complex video actions.},
  archive      = {J_JRTIP},
  author       = {Zhao, Qian and Su, Yanxiong and Zhang, Hui},
  doi          = {10.1007/s11554-025-01662-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Stme-net: Spatio-temporal motion excitation network for action recognition},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dilated-convolutional feature modulation network for
efficient image super-resolution. <em>JRTIP</em>, <em>22</em>(2), 1–14.
(<a href="https://doi.org/10.1007/s11554-025-01663-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of image super-resolution (SR), deep learning-based models have achieved remarkable success. However, these models often face compatibility issues with low-power devices due to their computational and memory constraints. To address this challenge, numerous lightweight and efficient models have been proposed. While these models typically employ smaller convolutional kernels and shallower architectures to reduce parameter counts and computational complexity, they often neglect the importance of capturing global receptive fields. In this paper, we propose a simple yet effective deep network, termed the dilated-convolutional feature modulation network (DCFMN), to tackle these limitations. Specifically, we introduce a dilated separable modulation unit (DSMU) to aggregate spatial information from diverse large receptive fields. To complement the DSMU, which processes features from a long-range perspective, we further design a local feature enhancement module (LFEM) to extract local contextual information for effective channel fusion. Additionally, by leveraging reparameterization techniques, we ensure that the model incurs no additional computational overhead during inference. Extensive experimental results demonstrate that our DCFMN achieves competitive performance among existing efficient SR methods, while maintaining a compact model size and low computational complexity.},
  archive      = {J_JRTIP},
  author       = {Wu, Lijun and Li, Shan and Chen, Zhicong},
  doi          = {10.1007/s11554-025-01663-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dilated-convolutional feature modulation network for efficient image super-resolution},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
