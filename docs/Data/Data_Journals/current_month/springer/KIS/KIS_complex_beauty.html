<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kis---32">KIS - 32</h2>
<ul>
<li><details>
<summary>
(2025). Correction: A new hybrid reasoning model based on rules,
cases and processes: Application to care of individuals facing autism
spectrum disorders. <em>KIS</em>, <em>67</em>(3), 3047–3048. (<a
href="https://doi.org/10.1007/s10115-024-02278-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Kaoura, Georgia and Kovas, Konstantinos and Boutsinas, Basilis and Hatzilygeroudis, Ioannis},
  doi          = {10.1007/s10115-024-02278-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {3047-3048},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: a new hybrid reasoning model based on rules, cases and processes: application to care of individuals facing autism spectrum disorders},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDMIX: An entropy-based dissimilarity measure to cluster
mixed data comprising of numerical–nominal–ordinal attributes.
<em>KIS</em>, <em>67</em>(3), 3023–3045. (<a
href="https://doi.org/10.1007/s10115-024-02319-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing (dis)similarity measures for mixed data often ignore data distribution and ordering information along numerical and categorical features, respectively, during distance computation. Additionally, combining numerical and categorical (dis)similarities is often done naively, failing to judiciously assign relative weights. This work introduces a unified dissimilarity measure for mixed data (EDMIX) that addresses these challenges. It employs an entropy-based measure (SEND) for numerical attributes which utilizes intra-attribute inhomogeneity efficiently. A Boltzmann’s entropy-based measure (EDMD) is adopted for categorical features, which effectively handles both nominal and ordinal attributes. Decay of attribute weight is analyzed to determine the threshold weights for numerical and categorical parts. Number of attributes whose weights are above the threshold value in each part decides the mixing proportion of the respective parts. Notably, EDMIX requires no input parameters and is adaptable to diversified mixed data. Experimental results highlight its superiority in terms of cluster quality, accuracy, discrimination ability, and execution time across diverse mixed datasets for clustering applications.},
  archive      = {J_KIS},
  author       = {Kar, Amit Kumar and Mishra, Amaresh Chandra and Mohanty, Sraban Kumar},
  doi          = {10.1007/s10115-024-02319-9},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {3023-3045},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {EDMIX: An entropy-based dissimilarity measure to cluster mixed data comprising of numerical–nominal–ordinal attributes},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models: A survey of their development,
capabilities, and applications. <em>KIS</em>, <em>67</em>(3), 2967–3022.
(<a href="https://doi.org/10.1007/s10115-024-02310-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models can generate text, respond to queries, and translate between languages, as recent research demonstrates. As a new and essential part of computational linguistics, LLMs can understand complex speech patterns and respond appropriately and rationally in the given context. Research contributions have significantly increased as a result of LLMs growing popularity. Nevertheless, the rate of increase has been so quick that evaluating the cumulative impact of these developments is challenging. Keeping abreast with all the LLM research that has surfaced in such a short time and gaining a thorough understanding of the present state of the field’s study has become challenging. The scientific community would gain from a succinct but thorough LLM summary covering their history, architecture, applications, issues, influence, and resources. This paper covers many model types while reviewing LLMs’ fundamental principles and notions. It summarizes earlier research, the evolution of LLMs throughout time, transformer history, and the range of tools and training methods applied to these models. The study also looks at the many fields in which LLMs are used, including business, social work, education, healthcare, and agriculture. Furthermore, it illustrates how LLMs impact society, mold AI’s future, and find valuable uses in problem-solving. This review article aims to provide a comprehensive overview of the history of LLMs, pre-trained architectures, applications, problems, and future directions for practitioners, researchers, and experts.},
  archive      = {J_KIS},
  author       = {Annepaka, Yadagiri and Pakray, Partha},
  doi          = {10.1007/s10115-024-02310-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2967-3022},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Large language models: A survey of their development, capabilities, and applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated cross-domain recommendation system based on bias
eliminator and personalized extractor. <em>KIS</em>, <em>67</em>(3),
2935–2965. (<a
href="https://doi.org/10.1007/s10115-024-02316-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of big data is driven by advancements in Internet of Things technology. The cross-domain recommendation system is a highly successful approach for obtaining user-interested items from massive data. However, implementing a cross-domain recommendation system on distributed IoT devices faces three challenges. On the one hand, item embeddings used in existing cross-domain recommendation models often lead to an unavoidable popularity bias. On the other hand, to convey user preference information, most existing methods utilize common interest channels. However, it stands to reason that the interest channels for different users should be distinct, as each person has their own unique preferences. Furthermore, collecting raw data from distributed IoT devices may lead to user privacy concerns. Given these challenges, we propose a federated cross-domain recommendation system based on bias eliminator and personalized extractor (FedBP) in this paper to achieve precise recommendations in cold-start scenarios. Firstly, we employ a bias eliminator to unfold all embedding directions during training, ensuring that each direction captures only specific features while maintaining neutrality in popularity. Secondly, personalized extractor is utilized to individualize the distinct preference information of each user from the source domain to the target domain. Then, we utilize a federated framework to collaboratively train the cross-domain recommendation system model, where local differential privacy is employed to ensure data privacy. Experimental results on public benchmarks show that FedBP consistently outperforms baseline models across three cold-start cross-domain recommendation scenarios, with improvements of at least 3.02%, 5.08%, and 3.08%.},
  archive      = {J_KIS},
  author       = {Di, Yicheng and Shi, Hongjian and Wang, Qi and Jia, Shunyuan and Bao, Jiayu and Liu, Yuan},
  doi          = {10.1007/s10115-024-02316-y},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2935-2965},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Federated cross-domain recommendation system based on bias eliminator and personalized extractor},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-aware user multi-interest modeling method for news
recommendation. <em>KIS</em>, <em>67</em>(3), 2911–2933. (<a
href="https://doi.org/10.1007/s10115-024-02290-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News recommendation can filter massive news and provide personalized information services, and the key is to better portray news and users. Current news recommendation methods try to integrate the knowledge graph to introduce background knowledge for news features modeling and have achieved good performance. However, when modeling users’ interest features, these methods only consider the personalized information of the interacted news and ignore the fact that some typical entities can help depict multiple topics that users are interested in. In this paper, we propose a method called KMRec (knowledge-aware user multi-interest modeling method for news recommendation) to realize the fine-grained portrayal of users’ multi-interest based on knowledge graph. Specifically, we represent user multi-interest features through four modules: historical text encoder, individual entity encoder, common entity encoder and user encoder. The historical text encoder is for the text modeling of each news and information aggregation. The individual entity encoder achieves the entity feature modeling of each news and information aggregation based on knowledge graph. The common entity encoder realizes the common entity information extraction of users’ historical interacted news based on knowledge graph. The user encoder integrates the text feature, individual entity feature and common entity feature to comprehensively depict the multi-interest of each user. Offline experimental results on both MIND-small and MIND-large datasets show that leveraging KMRec for users’ interest modeling can effectively improve the performance of news recommendation. The idea of introducing the user multi-interest feature is also verified to be effective by comparative experiments with existing news feature representations.},
  archive      = {J_KIS},
  author       = {Zuo, Zong and Lu, Jicang and Tan, Lei and Gong, Daofu and Chen, Jing and Liu, Fenlin},
  doi          = {10.1007/s10115-024-02290-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2911-2933},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge-aware user multi-interest modeling method for news recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential uncertainty quantification with contextual
tensors for social targeting. <em>KIS</em>, <em>67</em>(3), 2881–2910.
(<a href="https://doi.org/10.1007/s10115-024-02304-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common question that an online marketer asks is, given a social network, how one decides which users to target with their limited advertisement resources (such as coupon offers). The key technical task toward answering this question is the estimation of the user–user activation probability that quantifies the influence one user may have on another toward their purchasing decisions. In this paper, we propose a novel targeting strategy with sequential uncertainty quantification via probabilistic tensor regression. The proposed framework is designed to capture the heterogeneity in user preferences, product types, campaign strategies, etc. in the form of contextual tensor. For uncertainty quantification, we derive a closed-form online predictive distribution for the user response score, which is used in a bandit-style sequential decision-making on who to receive marketing offers. We empirically confirm that the proposed algorithm achieves significant improvement in influence maximization tasks over benchmarks, which is attributable to its capability of capturing the user–product heterogeneity.},
  archive      = {J_KIS},
  author       = {Idé, Tsuyoshi and Murugesan, Keerthiram and Bouneffouf, Djallel and Abe, Naoki},
  doi          = {10.1007/s10115-024-02304-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2881-2910},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sequential uncertainty quantification with contextual tensors for social targeting},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient parallel algorithm for finding strongly connected
components based on granulation strategy. <em>KIS</em>, <em>67</em>(3),
2855–2879. (<a
href="https://doi.org/10.1007/s10115-024-02299-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strongly connected components (SCCs) are a significant subgraph structure in digraphs. In the previous work, an algorithm based on rough set theory (RST) called KGRSCC was proposed, which can compute SCCs with high efficiency. Notably, KGRSCC utilized a granulation strategy, which was designed based on SCC correlations between vertices. These SCC correlations are confined to the situations that R-related set or upper approximation set only contains one vertex. However, the situations of ’only one’ cannot fully deduce SCCs correlations, which may limit the computation efficiency of SCCs. In this paper, firstly, the graph concept of SCCs is further analyzed in the framework of RST, and then, four ’not only one’ SCC correlations between vertices can be concluded. Secondly, the four SCC correlations can be divided two classes: trivial and nontrivial. Then, two new granulation strategies are proposed based on the two classes of SCC correlations. They can granulate the vertex set to construct two types of vertex granules. Thirdly, with combination of two types of vertex granules, a parallel algorithm named P@KGS is proposed based on KGRSCC. Finally, experimental results show that the P@KGS algorithm performs higher computational efficiency than compared algorithms.},
  archive      = {J_KIS},
  author       = {Xu, Taihua and He, Huixing and Yang, Xibei and Yang, Jie and Song, Jingjing and Cui, Yun},
  doi          = {10.1007/s10115-024-02299-w},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2855-2879},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient parallel algorithm for finding strongly connected components based on granulation strategy},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Factors influencing open science participation through
research data sharing and reuse among researchers: A systematic
literature review. <em>KIS</em>, <em>67</em>(3), 2801–2853. (<a
href="https://doi.org/10.1007/s10115-024-02284-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This systematic literature review investigates the influential factors guiding researchers’ active engagement in open science through research data sharing and subsequent reuse, spanning various scientific disciplines. The review addresses key objectives and questions, including identifying distinct sample types, data collection methods, critical factors, and existing gaps within the body of literature concerning data sharing and reuse in open science. The methodology employed in the review was detailed, outlining a series of systematic steps. These steps encompass the systematic search and selection of relevant studies, rigorous data extraction and analysis, comprehensive evaluation of selected studies, and transparent reporting of the resulting findings. The review’s evaluation process was governed by well-defined inclusion and exclusion criteria, encompassing publication dates, language, study design, and research outcomes. Furthermore, it adheres to the PRISMA 2020 flow diagram, effectively illustrating the progression of records through the review stages, highlighting the number of records identified, screened, included, and excluded. The findings include a concise tabular representation summarizing data extracted from the 51 carefully selected studies incorporated within the review. The table provides essential details, including study citations, sample sizes, data collection methodologies, and key factors influencing open science data sharing and reuse. Additionally, common themes and categories among these influential factors are identified, shedding light on overarching trends in the field. In conclusion, this systematic literature review offers valuable insights into the multifaceted landscape of open science participation, emphasizing the critical role of research data sharing and reuse. It is a comprehensive resource for researchers and practitioners interested in further understanding the dynamics and factors shaping the open science ecosystem.},
  archive      = {J_KIS},
  author       = {Ahmed, Mahfooz and Othman, Roslina and Noordin, Mohamad Fauzan and Ibrahim, Adamu Abubakar and Al-Hussaini, Abulfathi Ibrahim Saleh},
  doi          = {10.1007/s10115-024-02284-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2801-2853},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Factors influencing open science participation through research data sharing and reuse among researchers: A systematic literature review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sarcasm detection using optimized bi-directional long
short-term memory. <em>KIS</em>, <em>67</em>(3), 2771–2799. (<a
href="https://doi.org/10.1007/s10115-024-02210-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current era, the number of social network users continues to increase day by day due to the vast usage of interactive social networking sites like Twitter, Facebook, Instagram, etc. On these sites, users generate posts, whereas the attitude of followers towards factor utilization like situation, sound, feeling, and so on can be analysed. But most people feel difficult to analyse feelings accurately, which is one of the most difficult problems in natural language processing. Some people expose their opinions with different sole meanings, and this sophisticated form of expressing sentiments through irony or mockery is termed sarcasm. The sarcastic comments, tweets or feedback can mislead data mining activities and may result in inaccurate predictions. Several existing models are used for sarcasm detection, but they have resulted in inaccuracy issues, huge time consumption, less training ability, high overfitting issues, etc. To overcome these limitations, an effective model is introduced in this research to detect sarcasm. Initially, the data are collected from publicly available sarcasmania and Generic sarcasm-Not sarcasm (Gen-Sarc-Notsarc) datasets. The collected data are pre-processed using stemming and stop word removal procedures. The features are extracted using the inverse filtering (IF) model through hash index creation, keyword matching and ranking. The optimal features are selected using adaptive search and rescue (ASAR) optimization algorithm. To enhance the accuracy of sarcasm detection, an optimized Bi-LSTM-based deep learning model is proposed by integrating Bi-directional long short-term memory (Bi-LSTM) with group teaching optimization (GTO). Also, the LSTM + GTO model is proposed to compare its performance with the Bi-LSTM + GTO model. The proposed models are compared with existing classifier approaches to prove the model’s superiority using PYTHON. The accuracy of 98.24% and 98.36% are attained for sarcasmania and Gen-Sarc-Notsarc datasets.},
  archive      = {J_KIS},
  author       = {Sukhavasi, Vidyullatha and Sistla, Venkatrama Phani kumar and Dondeti, Venkatesulu},
  doi          = {10.1007/s10115-024-02210-7},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2771-2799},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sarcasm detection using optimized bi-directional long short-term memory},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new way of search query like knowledge graph and its
interpretability. <em>KIS</em>, <em>67</em>(3), 2745–2770. (<a
href="https://doi.org/10.1007/s10115-024-02242-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online search, it has always been a challenge to provide a way of search query that can help users accurately express search intentions and encourage them to type easily. This paper studies a knowledge graph-like search query and its interpretability. Inspired by the excellent expressive capabilities of the knowledge graph in knowledge reasoning and explainable artificial intelligence, we propose a new way of search query named search query knowledge graph, which can help users express their search requests using concepts such as entities, relationships, and facts. Therefore, people can draw a graph simply and interactively as a search query in a drag-and-drop manner. Compared with traditional search query keyword, the search query knowledge graph is not only fully compatible with it but also enriches and expands it. The search query keyword is a special case of the search query knowledge graph. We take the search query knowledge graph as the search input, obtain the corresponding search results in an online search, and then generate another knowledge graph named search result knowledge graph. According to the structural mapping relationship and the semantic matching relationship between the above two knowledge graphs, we innovatively discuss the structural interpretability and semantic interpretability for the search query knowledge graph. Some judgments or criteria about structural interpretability and semantic interpretability are given, and the sufficiency and necessity of the two interpretabilities are addressed. Extensive experimental results show that the search query knowledge graph proposed in this paper is a more effective and accurate way to express users’ search intentions and can provide help for online search.},
  archive      = {J_KIS},
  author       = {Xie, Ying-jie and Zeng, Guo-sun},
  doi          = {10.1007/s10115-024-02242-z},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2745-2770},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new way of search query like knowledge graph and its interpretability},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-graph-driven environmental monitoring with
cross-regions knowledge transfer. <em>KIS</em>, <em>67</em>(3),
2721–2744. (<a
href="https://doi.org/10.1007/s10115-024-02294-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is a critical task in intelligent transportation systems. However, cross-city data-driven prediction models encounter numerous challenges. A principal issue is data scarcity in underdeveloped cities, resulting from inadequate data collection mechanisms. Additionally, these models frequently rely exclusively on direct spatio-temporal traffic data, often deficient in thorough extraction and lateral exploration of external information from source cities, thereby introducing risks of negative transfer. This paper presents KGD-Transfer, a knowledge graph-driven cross-city traffic flow prediction framework that utilizes the extensive semantic information embedded within knowledge graphs. To address the challenge of data scarcity, we advocate for the construction of Fused Meta-Knowledge ( $${{\textbf {FM}}_k}$$ ) from multiple source cities to facilitate precise traffic prediction. Our knowledge fusion network (Ka-net) deeply integrates heterogeneous knowledge graphs with traffic data. Following this, we apply Neural Controlled Differential Equations (NCDEs) to extract intricate spatio-temporal features from the integrated knowledge. Furthermore, the Model-Agnostic Meta-Learning (MAML) framework is utilized to efficiently minimize the risk of negative transfer in cross-city learning. This approach enables the transfer of fused source knowledge by employing non-shared parameters to perform deep feature matching across cities, capitalizing on the spatio-temporal commonalities within the $${{\textbf {FM}}_k}$$ . Experimental results from four real datasets illustrate that the inclusion of contextual information from knowledge graphs markedly enhances the prediction model’s understanding and reasoning capabilities. KGD-Transfer outperforms advanced baseline methods in both short-term (10 min) and long-term (60 min) predictions, demonstrating superior accuracy and generalizability.},
  archive      = {J_KIS},
  author       = {Liu, Xiuwen and Xiao, Yang and Zhou, Shaoheng},
  doi          = {10.1007/s10115-024-02294-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2721-2744},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge-graph-driven environmental monitoring with cross-regions knowledge transfer},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient rumor detection model based on deep learning
and flower pollination algorithm. <em>KIS</em>, <em>67</em>(3),
2691–2719. (<a
href="https://doi.org/10.1007/s10115-024-02305-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of the growing popularity of social media (Twitter, Facebook, etc.) as a source of news and data, its unfiltered nature often facilitates the spread of rumors or pieces of information that cannot be validated at the time they are shared. It is possible for false or unconfirmed information to spread like wildfire on the internet, influencing public opinion and policy in the same way that reliable news would. Some of the most pervasive examples of incorrect and dubious information are fake news and rumors, both need to be identified as soon as possible to prevent potentially destabilizing outcomes such as loss of life, reputation, or financial loss. This paper presents a pioneering study that integrates the flower pollination algorithm (FPA) with convolutional neural networks (CNNs) for enhanced rumor detection on social media platforms. We develop and test a model that leverages FPA to optimize the architecture and hyperparameters of CNNs, which significantly improves the accuracy and efficiency of detecting rumors. Using data from Twitter, the proposed model achieves a benchmark accuracy of 91.24%, outperforming existing state-of-the-art approaches. The novelty of this research lies in the application of a nature-inspired optimization algorithm to automate the fine-tuning of deep learning models, addressing the challenges of manual parameter selection and model scalability in dynamic information environments. This study contributes to the fields of misinformation detection and machine learning by providing a robust framework for real-time, adaptable rumor analysis.},
  archive      = {J_KIS},
  author       = {Ahsan, Mohammad and Sinha, Bam Bahadur},
  doi          = {10.1007/s10115-024-02305-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2691-2719},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An efficient rumor detection model based on deep learning and flower pollination algorithm},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A clustering algorithm for detecting differential deviations
in the multivariate time-series IoT data based on sensor relationship.
<em>KIS</em>, <em>67</em>(3), 2641–2690. (<a
href="https://doi.org/10.1007/s10115-024-02303-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-of-things (IoT) applications involve a large number of sensors reporting data as a set of time series. Often these data are related to each other based on the relationship of the sensors in the actual application. Any small deviations could indicate a change in the operation of the IoT system and potential problems with the IoT application’s goals. It is often difficult to detect such deviations with respect to the relationship between the sensors. This paper presents the clustering algorithm that can efficiently detect all the deviations small or large in the complex and evolving IoT data streams with the help of sensor relationships. We have demonstrated with the help of experiments that our algorithm can efficiently handle high-dimensional data and accurately detect all the deviations. In this paper, we have presented two more algorithms, anomaly detection and outlier detection, that can efficiently categorize the deviations detected by our proposed clustering algorithm into anomalous or normal deviations. We have evaluated the performance and accuracy of our proposed algorithms on synthetic and real-world datasets. Furthermore, to check the effectiveness of our algorithms in terms of efficiency, we have prepared synthetic datasets in which we have increased the complexity of the deviations to show that our algorithm can handle complex IoT data streams efficiently.},
  archive      = {J_KIS},
  author       = {Idrees, Rabbia and Maiti, Ananda and Garg, Saurabh},
  doi          = {10.1007/s10115-024-02303-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2641-2690},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A clustering algorithm for detecting differential deviations in the multivariate time-series IoT data based on sensor relationship},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusing graph structural information with pre-trained
generative model for knowledge graph-to-text generation. <em>KIS</em>,
<em>67</em>(3), 2619–2640. (<a
href="https://doi.org/10.1007/s10115-024-02235-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph-to-text generation (KG-to-Text) is a task that involves generating accurate textual descriptions based on a given knowledge graph. Previous efforts have often enhanced pre-trained generative models by incorporating additional auxiliary pre-training tasks to supplement the missing graph structural information. However, such tasks not only require substantial computational resources, but also yield limited improvements. To address this issue, we propose a novel approach that effectively combines the graph structural information from knowledge graphs with pre-trained generative models without altering their fundamental architecture. Our approach involves inputting the original knowledge graph data into a graph convolutional network to acquire graph feature representations enriched with node characteristics. Additionally, the linearized sequence of the knowledge graph is inputted into the pre-trained generative model to exploit its inherent semantic richness. After computing multi-head attention mechanisms, we fuse the acquired graph feature representations into the pre-trained generative model to supplement the missing graph structural information. Experimental results on the WebNLG and EventNarrative datasets show that our approach achieves improved performance while reducing computational overhead.},
  archive      = {J_KIS},
  author       = {Shi, Xiayang and Xia, Zhenlin and Li, Yinlin and Wang, Xuhui and Niu, Yufeng},
  doi          = {10.1007/s10115-024-02235-y},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2619-2640},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fusing graph structural information with pre-trained generative model for knowledge graph-to-text generation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stream mining with integrity constraint learning for event
extraction in evolving data streams. <em>KIS</em>, <em>67</em>(3),
2595–2618. (<a
href="https://doi.org/10.1007/s10115-024-02300-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An event is a structured interaction of objects, generally involving an agent (actor) who acts on target entities, possibly with associated instruments. The problem of event extraction is to identify events of interest from a variety of information sources to be stored in a knowledge base for subsequent retrieval and analysis. Two major problems in event extraction are (1) adapting to concept drift and (2) maintaining the consistency of the event knowledge base. In this paper, we address these problems in a stream mining framework, making use of an ontology for events that represents background knowledge and integrity constraints derived empirically from an evolving data stream. We introduce a first-order language FLORE (Formal Language for Ontologies and Representing Events), which is used to encode an ontology of event types and objects, and to represent individual events categorized using concepts from the ontology. We present a multi-layered stream mining method for event extraction, where the first layer consists of a pool of simple learners, and the second layer learns an evolving set of integrity constraints to ensure the ongoing consistency of the extracted events. One intended application of this approach is conflict monitoring, and the domain of the Afghanistan conflict is used to illustrate the approach. Experimental results confirm that our multi-layered approach achieves higher recall and F1 than event extraction baselines on the event extraction task and on the subtasks of event detection and argument detection and classification.},
  archive      = {J_KIS},
  author       = {Calvo Martinez, John and Wobcke, Wayne},
  doi          = {10.1007/s10115-024-02300-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2595-2618},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Stream mining with integrity constraint learning for event extraction in evolving data streams},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer topic interaction propagation model and
simulation considering opinion interaction. <em>KIS</em>,
<em>67</em>(3), 2571–2594. (<a
href="https://doi.org/10.1007/s10115-024-02295-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing research on the dissemination and evolution of online public opinion focuses more on a single original or derivative topic or on treating the evolution process of user opinions and information dissemination as independent research objects. However, the evolution of opinions also affects the process of public opinion dissemination. When multiple derivative topics exist, the dissemination of public opinion will have new dynamic evolution characteristics. Thus, we propose a multi-layer topic interaction propagation model (OI-SEIRI model) considering opinion interaction. When multiple derivative topics appear, we divide the public opinion propagation of multiple topics into multiple independent propagation topic layers and construct opinion interaction rules and topic information transmission mechanisms within and between layers. By dynamically setting the probability of state transition and solving the propagation equilibrium point and propagation threshold, we clarified the impact of different factors on the trend of public opinion dissemination. Finally, the experimental results show that the OI-SEIRI model can effectively and more accurately describe the interactive information dissemination of multiple topics. The number of derivative topics, topic correlation relationships, opinion interaction behavior, and social roles can all impact the dissemination process.},
  archive      = {J_KIS},
  author       = {Yao, Cuiyou and Yu, Lin and Wang, Dong and Fu, Dongpu},
  doi          = {10.1007/s10115-024-02295-0},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2571-2594},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-layer topic interaction propagation model and simulation considering opinion interaction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time topic-based sentiment analysis for movie tweets
using hybrid approach. <em>KIS</em>, <em>67</em>(3), 2543–2569. (<a
href="https://doi.org/10.1007/s10115-024-02298-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to perform sentiment analysis (SA), which deals with dissecting and then extracting the hidden insights from the sentences said or written by a person. The proposed methodology for SA uses a combined and novel framework involving a Lexicon-based approach (LBA) and a deep learning (DL) technique to predict the overall sentiment of the text. Firstly, the LBA segregates the neutral tweets from the polar tweets. Later, a variant of CNN, namely a BERT-based bidirectional long short-term memory-temporal convolutional network (BiLSTM-TCN) grounded by the residual module and the dilated convolutions, is used to identify the type of polarity of the text. The paper also investigates various other BERT-based models like CNN, LSTM, and BiLSTM on the IMDB Movie Review dataset containing 50k movie reviews to show that the suggested model achieves a mean validation accuracy of 0.932 and a mean validation loss of 0.238 for the last three epochs for the polar reviews. After that, the trained model is used to forecast the sentiment of the live-streaming tweets about a particular movie of interest. The Twitter API, Tweepy, fetches tweets crawling over Twitter. The study obtains test accuracy, F1, and ROC–AUC scores of 92.94%, 0.929, and 0.98, respectively, in the least number of epochs, which is better than other models mentioned above, running in the same environment.},
  archive      = {J_KIS},
  author       = {Madan, Anjum and Kumar, Devender},
  doi          = {10.1007/s10115-024-02298-x},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2543-2569},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Real-time topic-based sentiment analysis for movie tweets using hybrid approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel ensemble label propagation with hierarchical
weighting for semi-supervised learning. <em>KIS</em>, <em>67</em>(3),
2521–2542. (<a
href="https://doi.org/10.1007/s10115-024-02245-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, semi-supervised learning is one of the research sports to solve the problem of labeled data. Label propagation (LP) is a popular method to classify graph data by utilizing the characterization of data nodes. However, LP has randomness and cannot effectively employ node attributes. In this paper, to solve the above problem, we propose a novel LP method based on hierarchical weighting (HWLP). Firstly, attribute aggregation and attribute update are executed for each node. Secondly, in the process of LP, for each unlabeled node, the label owned by its neighbors with the highest similarity is selected to avoid arbitrary LP. Finally, maximum voting is adopted to enhance the stability of the results. Experimental results show that the proposed method has better performance and stability than others.},
  archive      = {J_KIS},
  author       = {Zheng, Yifeng and Liu, Yafen and Qing, Depeng and Zhang, Wenjie and Pan, Xueling and Li, Guohe},
  doi          = {10.1007/s10115-024-02245-w},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2521-2542},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel ensemble label propagation with hierarchical weighting for semi-supervised learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-learning app selection multi-criteria group
decision-making problem using einstein operator in linguistic
trapezoidal neutrosophic environment. <em>KIS</em>, <em>67</em>(3),
2481–2519. (<a
href="https://doi.org/10.1007/s10115-024-02265-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we have defined linguistic trapezoidal neutrosophic numbers (LTNNs) which are a combination of trapezoidal neutrosophic numbers and linguistic variable numbers. Then, we introduced the Einstein sum, product and exponentiation operations for LTNNs and discussed the relationship among these operations. Moreover, two new kinds of aggregation operators, namely Einstein weighted-average operator and Einstein weighted-geometric operator for LTNNs, have been constructed. Based on the above aggregation operators, a multi-criteria group decision-making technique has been designed, which is demonstrated through numerical examples. Finally, sensitivity analysis and comparative study have been accomplished to exhibit the effectiveness and feasibility of the newly developed technique in the linguistic environment.},
  archive      = {J_KIS},
  author       = {Haque, Tipu Sultan and Chakraborty, Avishek and Alam, Shariful},
  doi          = {10.1007/s10115-024-02265-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2481-2519},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {E-learning app selection multi-criteria group decision-making problem using einstein operator in linguistic trapezoidal neutrosophic environment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for community architectural
layout generation. <em>KIS</em>, <em>67</em>(3), 2453–2480. (<a
href="https://doi.org/10.1007/s10115-024-02291-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Architectural layout design is of considerable importance in urban design, yet it can be a time-consuming and labor-intensive task for architects. Thus, a highly automated approach would benefit the profession. Different from recently studied methods for indoor floorplan generation, community architectural layout generation faces more challenges due to constraints such as building interval and volume rate while also considering architectural aesthetics. Existing algorithms that rely on predefined rules or heuristic search have difficulties balancing these factors. In addition, a dataset for evaluating the generated plans is lacking. In this paper, we formulate the community architectural layout task as a Markov game by defining the state, action space and reward function. We then propose a multi-agent reinforcement learning method with curriculum learning to generate layouts for the formulation. Besides, we propose a set of metrics for the evaluation of architectural layouts especially for residential area planning. We conduct our experiments on the real-world scene. The results have demonstrated our approach’s superiority in comparison to baseline methods.},
  archive      = {J_KIS},
  author       = {Sheng, Tao and Xiong, Yun and Wang, Haofen and Zhang, Yao and Wang, Siqi and Zhang, Weinan},
  doi          = {10.1007/s10115-024-02291-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2453-2480},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep reinforcement learning for community architectural layout generation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of corn futures prices with decomposition and
hybrid deep learning models. <em>KIS</em>, <em>67</em>(3), 2427–2452.
(<a href="https://doi.org/10.1007/s10115-024-02301-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting corn futures prices is crucial for market participants, policymakers, and agricultural enterprises to manage risks and make informed decisions. Therefore, it is necessary to improve the accuracy and stability of corn futures price predictions. This study proposes a deep mixed model that integrates variational mode decomposition (VMD), particle swarm optimization (PSO), convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and an attention mechanism. First, VMD is used to decompose the corn futures closing price series, thereby reducing data complexity. Then, principal component analysis (PCA) is performed on 21 influencing factors to extract key components affecting price fluctuations. The proposed model combines CNN for local feature extraction, LSTM for time series processing, and an attention mechanism to focus on critical information. In addition, the PSO algorithm optimizes the LSTM hyperparameters, further enhancing the model’s performance. To verify the model’s effectiveness, comprehensive experiments were conducted using historical data on corn futures. The results of four comparative experiments show that, based on multiple metrics such as RMSE, MAE, and $$\text {R}^2$$ , the proposed model significantly outperforms 14 existing models in terms of prediction accuracy and stability. These findings indicate that the model has practical application potential in financial forecasting and decision-making.},
  archive      = {J_KIS},
  author       = {Li, Feng and Tang, Menghe},
  doi          = {10.1007/s10115-024-02301-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2427-2452},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Prediction of corn futures prices with decomposition and hybrid deep learning models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin: Securing IoT networks using integrated ECC
with blockchain for healthcare ecosystem. <em>KIS</em>, <em>67</em>(3),
2395–2426. (<a
href="https://doi.org/10.1007/s10115-024-02273-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twin (DT) innovation is becoming increasingly prevalent in numerous areas. This is often particularly genuine in healthcare, which depends increasingly on Internet of Things (IoT) systems. But this combination brings up enormous issues with safety, security, and being able to develop. Since information is private, it is exceptionally critical to keep it secure from individuals who shouldn&#39;t have gotten to it and from being stolen. This consideration proposes a modern framework that combines elliptic curve cryptography (ECC) with blockchain innovation to bargain with these issues. The system is implied to ensure IoT systems by making demonstrative apparatuses in healthcare more secure. A genetic algorithm-based random forest (GAO-RF) demonstration is additionally utilized to form include choice work way better, which makes it beyond any doubt that information taking care of and analysis go easily. The GAO-RF show includes the body of unused thoughts by progressing the method of choosing highlights, which is exceptionally critical for proficiently overseeing colossal sums of private information. The show that was put into activity works exceptionally well, as appeared by its execution measurements as F1-score of 97.3%, an exactness of 98.4%, an exactness of 97.3%, a review of 97.4%, an MCC of 97.69%, and a kappa measurement (KS) of 97.31%. These results show an exceptionally strong framework that can handle and protect private health data. The safety and security of understanding information in IoT systems have enormously progressed by including ECC and blockchain in the DT system. A genetic algorithm has been shown to work well in the random forest model for feature selection, which has led to better security methods. This study has big effects on the healthcare field because it gives us a strong way to keep patient information safe. This method creates trust in the healthcare system by making sure that private data are handled in an honest and safe way. It could change how patient data are protected in this digital age.},
  archive      = {J_KIS},
  author       = {Sharma, Vikas and Kumar, Akshi and Sharma, Kapil},
  doi          = {10.1007/s10115-024-02273-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2395-2426},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Digital twin: Securing IoT networks using integrated ECC with blockchain for healthcare ecosystem},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High utility itemset mining in data stream using elephant
herding optimization. <em>KIS</em>, <em>67</em>(3), 2357–2394. (<a
href="https://doi.org/10.1007/s10115-024-02288-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining high utility itemsets from data stream within limited time and space is a challenging task. Traditional algorithms typically require multiple scans and complex data structures for data connection, storage and update. Moreover, the evaluation of duplicate itemsets generated by overlapping batches leads to low efficiency of the algorithm in terms of time and space. To address these issues, this paper proposes a heuristic-based data stream high utility itemset mining algorithm, termed SHUIM-EHO, designed to effectively solve limited storage space. The SHUIM-EHO algorithm designs a new clan updating strategy, which effectively enhances the convergence speed and reduces itemset loss. Additionally, a hash storage strategy is proposed to avoid the evaluation of duplicate itemsets, thereby further improving the execution efficiency of the algorithm. Experiments on real and synthetic datasets demonstrate the effectiveness of the algorithm, significantly reducing memory consumption and maintaining strong scalability.},
  archive      = {J_KIS},
  author       = {Han, Meng and He, Feifei and Zhang, Ruihua and Li, Chunpeng and Meng, Fanxing},
  doi          = {10.1007/s10115-024-02288-z},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2357-2394},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {High utility itemset mining in data stream using elephant herding optimization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trustworthy federated learning: Privacy, security, and
beyond. <em>KIS</em>, <em>67</em>(3), 2321–2356. (<a
href="https://doi.org/10.1007/s10115-024-02285-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While recent years have witnessed the advancement in big data and artificial intelligence, it is of much importance to safeguard data privacy and security. As an innovative approach, federated learning (FL) addresses these concerns by facilitating collaborative model training across distributed data sources without transferring raw data. However, the challenges of robust security and privacy across decentralized networks catch significant attention in dealing with the distributed data in FL. In this paper, we conduct an extensive survey of the security and privacy issues prevalent in FL, underscoring the vulnerability of communication links and the potential for cyber threats. We delve into various defensive strategies to mitigate these risks, explore the applications of FL across different sectors, and propose research directions. We identify the intricate security challenges that arise within the FL frameworks, aiming to contribute to the development of secure and efficient FL systems.},
  archive      = {J_KIS},
  author       = {Chen, Chunlu and Liu, Ji and Tan, Haowen and Li, Xingjian and Wang, Kevin I-Kai and Li, Peng and Sakurai, Kouichi and Dou, Dejing},
  doi          = {10.1007/s10115-024-02285-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2321-2356},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Trustworthy federated learning: Privacy, security, and beyond},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially ordered stochastic conformance checking.
<em>KIS</em>, <em>67</em>(3), 2291–2319. (<a
href="https://doi.org/10.1007/s10115-024-02280-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining aids organisations in improving their operational processes by providing visualisations and algorithms that turn event data into insights. How often behaviour occurs in a process—the stochastic perspective—is important for simulation, recommendation, enhancement and other types of analysis. Although the stochastic perspective is important, the focus is often on control flow. Stochastic conformance checking techniques assess the quality of stochastic process models and/or event logs with one another. In this paper, we address three limitations of existing stochastic conformance checking techniques: inability to handle uncertain event data (e.g. events having only a date), exponential blow-up in computation time due to the analysis of all interleavings of concurrent behaviour and the problem that loops that can be unfolded infinitely often. To address these challenges, we provide bounds for conformance measures and use partial orders to encode behaviour. An open-source implementation is provided, which we use to illustrate and evaluate the practical feasibility of the approach.},
  archive      = {J_KIS},
  author       = {Leemans, Sander J. J. and Brockhoff, Tobias and van der Aalst, Wil M. P. and Polyvyanyy, Artem},
  doi          = {10.1007/s10115-024-02280-7},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2291-2319},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Partially ordered stochastic conformance checking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid sampling algorithm for imbalanced and class-overlap
data based on natural neighbors and density estimation. <em>KIS</em>,
<em>67</em>(3), 2259–2290. (<a
href="https://doi.org/10.1007/s10115-024-02281-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification poses a significant challenge in machine learning and data mining, exacerbated by class overlap which adversely affects model performance. Resampling is widely used to tackle imbalanced data. However, most resampling algorithms overlook the complexity of overlapping region samples, resulting in excessive removal of majority class samples and insufficient representation of minority class information. To tackle these issues, this paper proposes a novel approach, namely natural neighbors and density estimation-based hybrid sampling algorithm (NaNDS). NaNDS fully considers the varying impacts of overlapping samples with different characteristics on subsequent classification tasks. First, the density of minority class samples is estimated using a Gaussian kernel function, and high-density minority class samples are selected to construct a set of hyper-spherical structures. These structures facilitate the geometric identification and removal of overlapping majority class samples that negatively impact classification. Then, oversampling weights for minority samples are determined by combining density and information entropy estimates derived from natural neighbors. Afterward, an adaptive oversampling strategy is developed, using differentiation-based generation guided by natural neighbor information. This process appropriately corrects decision boundary by enhancing information in both overlapping and minority class-dominated regions. Experimental validation on the KEEL dataset demonstrates NaNDS outperforming eight state-of-the-art algorithms, highlighting its superior competitiveness and robustness in handling class-overlap imbalanced datasets.},
  archive      = {J_KIS},
  author       = {Li, Xinqi and Liu, Qicheng},
  doi          = {10.1007/s10115-024-02281-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2259-2290},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hybrid sampling algorithm for imbalanced and class-overlap data based on natural neighbors and density estimation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User-based clustering deep model for the sequential
point-of-interest recommendation. <em>KIS</em>, <em>67</em>(3),
2233–2258. (<a
href="https://doi.org/10.1007/s10115-024-02277-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The point-of-interest (POI) recommendation is a key function of location-based social networks that can help users exploit unfamiliar areas. Due to the massive check-in records accumulated in these location-based applications, the sequential POI recommendation has evolved quickly in the research community. Although the existing sequential POI recommendation models have reached an encouraging performance in predicting the next-N POIs to users, the data sparsity problem is still severe in the sequential POI recommendation task. It is challenging to learn the users’ preferences of POIs under the highly sparse dataset. Meanwhile, many sequential models focus on exploiting users’ interests from the entire dataset, ignoring the effect of collaborative information from similar users. To this end, we propose a user-based clustering deep model (UCDM) for the sequential POI recommendation to deal with these issues. UCDM extracts collaborative information via a user-based intent clustering module and uses a binary self-attention layer to both learn the general preference from the entire dataset and the local preference from the collaborative information. In addition, our proposed model uses a POI candidate filter to control the size of the POI candidate set to reduce the sparsity of the dataset. In the model learning phase, we adopt the Bayesian personalized ranking to train our model. The experiment verifies that our proposed UCDM outperforms the selected baseline models for the sequential POI recommendation on two real-world check-in datasets.},
  archive      = {J_KIS},
  author       = {Wang, Tianxing and Wang, Can and Tian, Hui and Liew, Alan Wee-Chung},
  doi          = {10.1007/s10115-024-02277-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2233-2258},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {User-based clustering deep model for the sequential point-of-interest recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio meets text: A loss-enhanced journey with manifold
mixup and re-ranking. <em>KIS</em>, <em>67</em>(3), 2195–2231. (<a
href="https://doi.org/10.1007/s10115-024-02283-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio–text retrieval is the task of aligning natural language and audio such that instances from different modalities can be compared with a similarity metric. Contrastive representation learning is a popular way to address this problem by finetuning a dual encoder backbone. However, finetuning incurs a heavy computational burden and requires multiple graphics processing units. The main objective of this study is thus to drive enhanced retrieval performance in audio–text retrieval with lower computational burden and resources. Thus, a series of plug-and-play modules are introduced to elicit knowledge from pretrained audio and text encoders. From creating a competitive baseline with early and late fusion first, the study endeavors to improve the performance further by adapting deep metric Circle and Ranked List losses. This study utilizes Manifold Mixup as a strong regularizer and a novel post-processing step of re-ranking to refine the results further. These changes work in symphony to obtain superior performance than other models without the need to finetune encoders. Particularly, an improvement of 1–3% is obtained on the AudioCaps dataset, establishing a new state of the art. The results on the Clotho dataset remain competitive with other finetuning approaches utilizing heavy resources. Also, the model emerges as the best among the frozen encoder models across all the metrics. Moreover, the proposed modules are modality agnostic and hold great potential for other retrieval tasks beyond the domain of audio–text. Overall, this study establishes a strong and competitive baseline for future approaches in audio–text retrieval.},
  archive      = {J_KIS},
  author       = {Suryawanshi, Yash and Shah, Vedanshi and Randar, Shyam and Joshi, Amit},
  doi          = {10.1007/s10115-024-02283-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2195-2231},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Audio meets text: A loss-enhanced journey with manifold mixup and re-ranking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuLoR: A multi-graph contrastive network for logical
reasoning. <em>KIS</em>, <em>67</em>(3), 2171–2193. (<a
href="https://doi.org/10.1007/s10115-024-02286-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logical reasoning tasks are more challenging than traditional machine reading comprehension tasks. The machine must recognize the logical relationships implicit in the text and use logical reasoning to derive an answer. Logical reasoning tasks currently face two major challenges. The first challenge is the difficulty of capturing the logical relationships implicit in the text. The second involves connecting the divide between distinct logical structures and the continuous space of text embeddings. In this study, we present a contrastive network based on multiple graphs designed to tackle these issues through the combination of both implicit and explicit logical connections, thereby enhancing reasoning capabilities. We use different construction strategies to create logical relationship graphs and logical hypergraph graphs. These graphs are integrated into a multi-graph contrastive network to learn higher-order logical representations, which are then used as inputs to a decoder for final prediction. The evaluation of our designed models was performed on datasets designed to assess reasoning ability, including ReClor, LogiQA and the extended iteration LogiQA 2.0. The experimental results show that our proposed method outperforms the state-of-the-art models. In particular, the results obtained on the LogiQA 2.0 dataset, which contains a larger number of samples, are particularly outstanding. Our model achieved an accuracy rate of 59.16%, outperforming most of the baseline comparisons by at least one percentage point, demonstrating its superior potential in complex reasoning tasks.},
  archive      = {J_KIS},
  author       = {Xiao, Jing and Lin, Guijin and Xiao, Yu and Li, Ping},
  doi          = {10.1007/s10115-024-02286-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2171-2193},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MuLoR: A multi-graph contrastive network for logical reasoning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource allocation in fog computing: A survey on current
state and research challenges. <em>KIS</em>, <em>67</em>(3), 2091–2170.
(<a href="https://doi.org/10.1007/s10115-024-02274-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With fog computing, new services and applications are enabled on the internet of things by providing computational services at the network edge. Fog computing is emerging as a transformative paradigm, linking edge devices with centralized cloud resources. It improves network efficiency, lowers latency, and increases computing power. Resource allocation and optimization are critical for fog computing to achieve optimal system performance, efficient resource usage, and smooth user experiences. Throughout the presentation, we discussed the architecture, framework, comparison of fog computing with cloud computing, resource allocation strategies, and the relevance of resource allocation to fog computing. Various methods of optimization and allocation are discussed, along with their application to fog-enhanced vehicle services and vehicular fog computing. For the purpose of allocating resources, minimizing latency, and optimizing quality of service (QoS), a variety of techniques have been applied, including game theory, convex optimization, reinforcement learning, and genetic algorithms. Additionally, we discuss how fog computing environment resource allocation works using game theory. The purpose of this paper is to review several articles in the field of fog environments and to provide a detailed comparison of each from a variety of perspectives. An overview of the main features of the reviewed articles was also presented in the form of a table. This study highlights the effectiveness of these strategies for improving system performance, reducing latency, optimizing resources, and reducing energy consumption. Lastly, we highlight future research directions and potential contributions in fog computing. Management of heterogeneity, ensuring real-time optimization, ensuring QoS and security concerns, promoting energy-efficient computing and sustainability, managing mobility, scheduling and self-adaptive scheduling, load balancing, offloading, reliability, sensor lifetime, multiagent reinforcement learning, optimal resource allocation, and quality of experience are discussed. The purpose of this survey is to give readers a detailed understanding of state-of-the-art methods, challenges, and possible future directions in resource allocation and optimization in fog computing. The aim of this research is to synthesize insights from the literature in order to provide valuable insight for researchers, practitioners, and stakeholders interested in advancing the field of fog computing.},
  archive      = {J_KIS},
  author       = {Nemati, Amir Mohammad and Mansouri, Najme},
  doi          = {10.1007/s10115-024-02274-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2091-2170},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Resource allocation in fog computing: A survey on current state and research challenges},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommender systems in smart campus: A systematic mapping.
<em>KIS</em>, <em>67</em>(3), 2063–2089. (<a
href="https://doi.org/10.1007/s10115-024-02240-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are extremely useful tools to provide the user with information that may be of interest. These systems are responsible for performing a series of procedures to filter items from massive databases and return only what the user would be looking for, which can be a product, a song, a movie or series, a website, news, or educational resources. Recommender systems are also intended for educational purposes, returning items such as teaching materials, video classes, books, courses, and short courses, for example. The environments in universities that aggregate these systems are called smart university campus. Sites that make use of multiple technologies, able to relate the virtual environment with the real and provide users with a fully integrated system. From this context, there was a systematic mapping of smart campus areas and recommendation systems. A study was conducted to investigate the relationship between these areas, through the search in four databases, between the years 2017 and 2024, identifying 894 papers, of which 101 were selected for analysis. We also identified some key documents in the area of recommender systems, as well as the technologies applied in each of them. The analysis conducted in this paper identified several research opportunities in the area. However, it was observed that many of the studies do not make clear the information that their applications will be used in conjunction with smart campus.},
  archive      = {J_KIS},
  author       = {Hideki Mensch Maruyama, Martin and Willig Silveira, Luan and da Silva Júnior, Elvandi and Casanova, Gabriel and Palazzo M. de Oliveira, José and Maran, Vinícius},
  doi          = {10.1007/s10115-024-02240-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2063-2089},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Recommender systems in smart campus: A systematic mapping},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum computing and quantum-inspired techniques for
feature subset selection: A review. <em>KIS</em>, <em>67</em>(3),
2019–2061. (<a
href="https://doi.org/10.1007/s10115-024-02282-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature subset selection is essential for identifying relevant and non-redundant features, which enhances classification accuracy and simplifies machine learning models. Given the computational difficulties of determining optimal feature subsets, heuristic and metaheuristic algorithms have been widely used. Recently, the rise of quantum computing has led to the exploration of quantum-inspired metaheuristics and quantum-based approaches for this task. Although various studies have explored quantum-inspired and quantum-based approaches for feature subset selection, a comprehensive review that critically examines their significance, limitations, underlying mechanisms, and future directions remains lacking in the literature. This paper addresses this gap by presenting the first in-depth survey of these approaches. We systematically selected and analyzed relevant studies from prominent research databases, providing a detailed evaluation of quantum-inspired metaheuristics and quantum computing paradigms applied to feature subset selection. Our findings indicate that quantum-inspired metaheuristic approaches often deliver superior performance compared to traditional metaheuristic methods for feature subset selection. Nevertheless, their reliance on classical computing limits their ability to fully realize the advantages offered by quantum computing. The quantum-based feature subset selection methods, on the other hand, show considerable promise but are frequently constrained by the current limitations of quantum hardware, making large-scale feature subset selection challenging. Given the rapid evolution of quantum computing, research on both quantum-inspired and quantum-based feature subset selection remains insufficient to draw definitive conclusions. We are optimistic that this review will provide a foundation for future advancements in feature subset selection as quantum computing resources become more accessible.},
  archive      = {J_KIS},
  author       = {Mandal, Ashis Kumar and Chakraborty, Basabi},
  doi          = {10.1007/s10115-024-02282-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2019-2061},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Quantum computing and quantum-inspired techniques for feature subset selection: A review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
