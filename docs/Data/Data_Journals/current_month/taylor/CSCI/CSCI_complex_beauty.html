<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSCI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="csci---14">CSCI - 14</h2>
<ul>
<li><details>
<summary>
(2025). A novel classification of meditation techniques via
optimised chi-squared 1D-CNN method based on complexity, continuity and
connectivity features. <em>CSCI</em>, <em>37</em>(1), Article: 2467387.
(<a href="https://doi.org/10.1080/09540091.2025.2467387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intricate world of human–computer interaction deeply explores how people gain knowledge and blend technology into daily life. Electroencephalography (EEG) is one of several methods for measuring brain activity, it is non-invasive, portable, inexpensive and time-sensitive. Research shows a strong link between meditation and changes in EEG patterns, spanning various techniques. With machine learning playing a major role, EEG datasets have made comprehensive study possible. This paper investigates the efficacy of 1D-CNN (One-Dimensional Convolutional Neural Network) classification, using complexity, continuity and connectivity features. It remarkably outperforms and achieves 60% training accuracy, showcasing model robustness in meditation classification. This novel methodology enables to differentiates neural oscillations in type of meditator and control. Prior research used power spectrum density, entropy, and connectivity for meditation distinctions. EEG data from practitioners of Himalayan Yoga (HYT), Isha Shoonya (SYN) and Vipassana (VIP) as well as untrained controls (CTR) are examined in this research. Employing chi-square, CNN and hyperparameter models, outcomes reveal distinctive cognitive aspects among meditation styles, allowing effective differentiation.},
  archive      = {J_CSCI},
  author       = {Abhishek Jain and Rohit Raja and Manoj Kumar and Pawan Kumar Verma},
  doi          = {10.1080/09540091.2025.2467387},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2467387},
  shortjournal = {Connect. Sci.},
  title        = {A novel classification of meditation techniques via optimised chi-squared 1D-CNN method based on complexity, continuity and connectivity features},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel CNN architecture for image restoration with implicit
frequency selection. <em>CSCI</em>, <em>37</em>(1), Article: 2465448.
(<a href="https://doi.org/10.1080/09540091.2025.2465448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration aims to recover clear images from degraded ones, with deep neural networks becoming the dominant approach. While earlier methods focused on spatial-domain information, recent models have explored frequency-domain data to improve performance. However, explicit frequency-domain processing introduces significant computational overhead. To address this, we propose the Implicit Frequency Selective Image Restoration Network (IFSR-Net), which implicitly captures frequency information without explicit transformations, achieving high performance with reduced computational cost. Our analysis indicates that the main spectral variations between the clear and degraded images are centred on the high-frequency components in the feature maps; the convolution operator tends to amplify the amplitude and variance of these components. Building on this observation, we designed an Implicit Frequency Selection Module (IFSM) to enrich high-frequency components and an Implicit Frequency Selection Attention (IFSA) mechanism to emphasize and integrate beneficial frequency features. We integrated and optimized design elements from existing image restoration models to further refine the overall architecture of IFSR-Net. Extensive experiments across seven datasets and three tasks demonstrate the effectiveness of our approach. Ablation studies confirm the validity of our design choices, offering insights for future research in image restoration.},
  archive      = {J_CSCI},
  author       = {Jiaxing Hu and Zhibo Wang},
  doi          = {10.1080/09540091.2025.2465448},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2465448},
  shortjournal = {Connect. Sci.},
  title        = {A novel CNN architecture for image restoration with implicit frequency selection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Password region attribute classification based on
multi-granularity cascade fusion. <em>CSCI</em>, <em>37</em>(1),
Article: 2461092. (<a
href="https://doi.org/10.1080/09540091.2025.2461092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The composition of the password is markedly disparate contingent on the configuration strategy and the individual user&#39;s predilections. The objective of this paper is to mine the region attribute information behind the password text through text classification. In contrast to the traditional text classification approach, the classification of password region attribution represents a distinct challenge namely ultra-short text classification. The issue of password regional attribute classification is particularly tricky due to its inherent lexical polysemy, the scarcity of text features, the lack of context and the difficulty in explicitly identifying semantics. To address the aforementioned issues, we propose a multi-granularity cascade fusion approach for password region attribution classification. Firstly, the model employs series of segmentation techniques to split password into multi-dimensional fine-grained subword representations. Subsequently, multiple segmented representations of the same password are fed into a localised feature encoder to mine the private local features. Finally, a multi-level cascade fusion method is designed to integrate different granularity of password features into a unified representation to classification. Our approach can effectively addresses the limitations of scarce information and the challenge of integrating multiple representations for password text. Experiments on a large amount of real password data demonstrate that, our model can converge rapidly and achieve an accuracy of 88.18%, a precision of 88.31%, a recall of 87.73%, and an F1-score of 88.02%, significantly outperforming traditional models.},
  archive      = {J_CSCI},
  author       = {Wei Yu and Cheng Liu and Lvlin Ni and Yu Shi and Qingbing Ji},
  doi          = {10.1080/09540091.2025.2461092},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2461092},
  shortjournal = {Connect. Sci.},
  title        = {Password region attribute classification based on multi-granularity cascade fusion},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved data labelling method for news headlines
classification in cloud environment. <em>CSCI</em>, <em>37</em>(1),
Article: 2461088. (<a
href="https://doi.org/10.1080/09540091.2025.2461088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In several domains, such as computer vision, natural language processing, and speech recognition, automatic data labelling is a critical task. Automatic data labelling is the process of utilising machine learning techniques to automatically label substantial amounts of data. Two largest news datasets like, the Kaggle dataset and the AG News dataset are used as benchmark for the performance analysis of the proposed algorithms. A novel four-dimensional matrix-based input representation that depicts the intra – and inter-word associations is used to obtain the feature vectors. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) helps to remove the outliers which improves the overall accuracy of Convolutional Neural Networks (CNN). The proposed approach also reduces the dependency on human annotators. The DBSCAN algorithm is utilised o automatically cluster similar data points, thereby reducing the need for manual labelling. These clusters are then fed into a CNN with a rethinking mechanism, which allows the network to revise its initial predictions based on additional context. This integration of clustering and deep learning techniques aims to improve the accuracy and efficiency. The cloud computing method is used to achieve high-throughput news headlines classification in order to achieve accurate and efficient data labelling.},
  archive      = {J_CSCI},
  author       = {A. Sherly Alphonse and S. Abinaya and Nirvik Verma},
  doi          = {10.1080/09540091.2025.2461088},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2461088},
  shortjournal = {Connect. Sci.},
  title        = {Improved data labelling method for news headlines classification in cloud environment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software defect prediction using wrapper-based dynamic
arithmetic optimization for feature selection. <em>CSCI</em>,
<em>37</em>(1), Article: 2461080. (<a
href="https://doi.org/10.1080/09540091.2025.2461080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Defect Prediction (SDP) empowers the creators to diagnose and unscramble defects in the introductory legs of the software evolution process to reduce the effort and cost invested in creating high-quality software. Feature Selection (FS) is critical to pinpoint the most pertinent features for defect prediction. This paper intends to employ a peculiar wrapper-based FS mode, dubbed DAOAFS, rooted on the dynamic arithmetic optimization algorithm (DAOA). Subsequently, this work evaluates the competence of the proposed FS mode using ten benchmark NASA datasets on four supervised learning classifiers, namely NB, DT, SVM, and KNN using accuracy and error curve as the standard performance measure metrics. This paper also correlates the proposed FS mode&#39;s conduct with existing FS techniques based on widely utilized meta-heuristic approaches such as GA, PSO, DE, ACO, FA, and SWO. This work employed Friedman and Holm test to ratify the proposed FS mode&#39;s statistical connotation. The investigatory outcomes supported the assertion that the recommended DAOAFS mode was effective in enhancing the efficacy of the defect forecasting model by achieving the highest mean accuracy of 94.76%. The findings also revealed that the proposed approach established its supremacy over the other studied FS techniques with bettered veracity in most instances.},
  archive      = {J_CSCI},
  author       = {Kunal Anand and Ajay Kumar Jena and Himansu Das and S. S. Askar and Mohamed Abouhawwash},
  doi          = {10.1080/09540091.2025.2461080},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2461080},
  shortjournal = {Connect. Sci.},
  title        = {Software defect prediction using wrapper-based dynamic arithmetic optimization for feature selection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative adversarial deep learning model for producing
location-based synthetic trajectory data. <em>CSCI</em>, <em>37</em>(1),
Article: 2458502. (<a
href="https://doi.org/10.1080/09540091.2025.2458502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of location-based services has triggered the acquisition and analysis of various types of individual trajectory recordings. However, the sensitive nature of such kind of data inevitably leads to privacy constraints and regulations on its use and sharing. This paper addresses the problem in a distinctive perspective. Instead of blurring or modifying original trajectory samples, we aim to generate a completely synthetic dataset, whose samples are singularly different from the original ones, but whose collective sets share similar global characteristics and performances. We propose a generative deep learning solution for location-based trajectory formats, with the goal of producing realistic synthetic location sequences: the process relies on a generative adversarial network (GAN) framework, involving long short-term memory (LSTM) recurrent layers to capture trajectory characteristics, and neural embeddings to model mobility relations between places. We leverage multiple metrics to assess the realistic character of synthetic data and their similarity with the original source; moreover, we evaluate downstream performance differences with regard to the next place prediction problem. Tested on a real-world large-scale dataset of long-distance trips, and compared with baselines and traditional geomasking techniques, our approach presents better characteristics, providing novel insights into GeoAI solutions for human mobility analysis.},
  archive      = {J_CSCI},
  author       = {Alessandro Crivellari and Yuhui Shi},
  doi          = {10.1080/09540091.2025.2458502},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2458502},
  shortjournal = {Connect. Sci.},
  title        = {Generative adversarial deep learning model for producing location-based synthetic trajectory data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental learning based two-level multimodal data fusion
model for alzheimer disease prediction on different data modalities.
<em>CSCI</em>, <em>37</em>(1), Article: 2458501. (<a
href="https://doi.org/10.1080/09540091.2025.2458501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is a complex neurodegenerative condition that affects millions of people worldwide, necessitating early and accurate diagnosis for optimal patient care. This study presents a novel Two-Level Multimodal Data Fusion Integrated Incremental Learner Ensemble Classifier (TMDFILE) for Alzheimer&#39;s detection. This method integrates temporal, spatial, spectral, audio, and text data modalities, utilising a gating mechanism to optimise the contribution of each modality. Incremental learning is employed to adjust evolving data patterns and, enhance long-term performance. The proposed TMDFILE was evaluated across five diverse datasets: achieving an accuracy of 94.5%, precision of 93.5%, recall of 95.1%, and F-measure of 94.1% on the ADNI dataset; an accuracy of 94.9%, with precision, recall, and F-measure values of 94.5%, 94.1%, and 94.3%, respectively, on the OASIS dataset; an accuracy of 93.5%, precision of 95.1%, and recall of 94.1% on the EEG Emotion Recognition dataset; an accuracy of 94.5%, precision of 93.5%, and recall of 95.1% on the Aberystwyth Dementia dataset, providing reliable classifications that contribute to early cognitive decline detection; and showed robust performance with an accuracy of 94.5%, precision of 93.5%, and recall of 95.1% on the BRATS dataset, relevant to brain imaging analysis for Alzheimer&#39;s detection. TMDFILE consistently outperformed traditional classifiers, including Support Vector Machines, Random Forest, and Convolutional Neural Networks, achieving an average precision of 93.5%, recall of 95.1%, F-measure of 94.1%, and accuracy of 94.5%. These findings underscore TMDFILE&#39;s effectiveness in diagnostic accuracy and reliability, establishing it as a promising tool for Alzheimer&#39;s disease detection across clinical and research applications.},
  archive      = {J_CSCI},
  author       = {M. Leela and K. Helenprabha},
  doi          = {10.1080/09540091.2025.2458501},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2458501},
  shortjournal = {Connect. Sci.},
  title        = {Incremental learning based two-level multimodal data fusion model for alzheimer disease prediction on different data modalities},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel deep learning model for stock market prediction
using a sentiment analysis system from authoritative financial website’s
data. <em>CSCI</em>, <em>37</em>(1), Article: 2455070. (<a
href="https://doi.org/10.1080/09540091.2025.2455070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of deep learning, specifically time series neural networks, in predicting stock market trends has emerged as a significant use case in financial analysis. However, the complex interrelationships and instability of the stock market have made the timely and accurate prediction of its behaviour as a confronting endeavour. To address this difficulty, in this research work a stock market index prediction model called SenT-In, which combines the with a sentiment awareness model. A sentiment awareness model using Convolutional Neural Networks (CNN) and Gated Recurrent Unit (GRU) is proposed to calculate the sentiment index of a large volume of news articles collected from reputable financial websites. In addition, a sentiment attention method is developed to combine stock data and news sentiment index as the input for training and predicting using the SenT-In network, which is both simple and efficient. The proposed model is evaluated in four different stock market datasets which include FSTE, SSE, Nifty 50 and S&amp;P 500. On comparing the results with conventional deep learning algorithms such as GRU, LSTM, CNN and SVM, proposed SenT-In outperforms existing methods in accuracy with 9%, F1-Score with 7%, AUC-ROC curve with 13% and PR-AUC curve with 9% efficiency (on average).},
  archive      = {J_CSCI},
  author       = {Jitendra Kumar Chauhan and Tanveer Ahmed and Amit Sinha},
  doi          = {10.1080/09540091.2025.2455070},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2455070},
  shortjournal = {Connect. Sci.},
  title        = {A novel deep learning model for stock market prediction using a sentiment analysis system from authoritative financial website’s data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of heart sound signals with whisper model.
<em>CSCI</em>, <em>37</em>(1), Article: 2449943. (<a
href="https://doi.org/10.1080/09540091.2025.2449943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart sounds, or phonocardiograms (PCG), are important for diagnosing cardiovascular conditions, providing a non-invasive means to assess heart function through auscultation. Accurate classification of PCG signals can facilitate early detection of cardiac abnormalities, significantly improving patient outcomes. However, the complexity and variability of heart sound recordings present significant challenges for traditional classification methods, necessitating advanced approaches that can effectively handle the nuances of cardiac acoustics. This paper introduces a novel transfer learning approach that adapts OpenAI&#39;s Whisper model, originally designed for robust speech recognition, to the task of heart sound classification. In particular, we employ Whisper&#39;s encoder architecture to effectively capture acoustic features that generalize to cardiac auscultation, making it a promising candidate for PCG analysis. To tailor the model for this specialized task, we implement a modified encoder architecture optimized for heart sound characteristics. We process the input to the model using a Log-Mel spectrogram pipeline specifically designed to highlight the unique acoustic properties of PCG signals. Experimental results demonstrate that the adapted Whisper model achieves state-of-the-art performance, surpassing existing methods in both accuracy and robustness.},
  archive      = {J_CSCI},
  author       = {Maryam Alotaibi and Yakoub Bazi and Mohamad Mahmoud Al Rahhal and Nassim Ammour and Mansour Zuair},
  doi          = {10.1080/09540091.2025.2449943},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2449943},
  shortjournal = {Connect. Sci.},
  title        = {Classification of heart sound signals with whisper model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising source code vulnerability detection using deep
learning and deep graph network. <em>CSCI</em>, <em>37</em>(1), Article:
2447373. (<a
href="https://doi.org/10.1080/09540091.2024.2447373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the effectiveness of vulnerability detection in software developed using C and C++ programming languages, our study introduces a novel correlation calculation method for analyzing and evaluating Code Property Graphs (CPG). The intelligent computation method proposed in this study comprises three key stages. In the first stage, we present a method for extracting features from the CPG source code. To accomplish this, we integrate three distinct data exploration methods: employing Graph Convolutional Neural (GCN) to extract node features from CPG, utilizing Convolutional Neural Network (CNN) to extract edge features from CPG, and finally employing the Doc2vec natural language processing algorithm to extract source code from CPG nodes. The second stage involves proposing a method for synthesizing CPG source code features. Building on the features acquired in the first stage, our paper introduces a synthesis and construction method to generate feature vectors for the source code. The final stage, stage three, executes the detection of source code vulnerabilities. The experimental results demonstrate that our proposed model in this study achieves higher efficiency compared to other studies, with an improvement ranging from 3% to 4%.},
  archive      = {J_CSCI},
  author       = {Cho Do Xuan and Tran Thi Luong and Ma Cong Thanh},
  doi          = {10.1080/09540091.2024.2447373},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2447373},
  shortjournal = {Connect. Sci.},
  title        = {Optimising source code vulnerability detection using deep learning and deep graph network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selecting hybrids of metaheuristics for resource-constraint
project scheduling problems with discounted cashflows. <em>CSCI</em>,
<em>37</em>(1), Article: 2447365. (<a
href="https://doi.org/10.1080/09540091.2024.2447365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving resource-constrained project scheduling problems with discounted cash flows (RCPSP-DC) is a critical challenge for project and finance managers, as efficient resource allocation can significantly impact a company’s financial success. While prior research addresses this NP-hard problem, most approaches depend on hybrid metaheuristics requiring expertise in hybridisation and lack systematic methods for architecture selection, often relying on a single criterion with trial-and-error parameter tuning. In this paper, we propose a novel collaborative parallel hybridisation framework that integrates Thompson sampling and multicriteria decision analysis (MCDA) to holistically evaluate and identify the best hybrid architecture from a diverse set of options. Unlike conventional approaches, our method employs onboard Taguchi design of experiments (DOE) for structured and efficient parameter tuning. Additionally, Thompson sampling, applied in the form of Bayesian learning, mitigates the stochastic nature of metaheuristics through multiple experiments. This framework was used to select the best architecture from 57 hybrid combinations of six metaheuristics for solving RCPSP-DC. Extensive experiments using standard datasets demonstrate that the proposed framework achieves statistically significant performance improvements, selecting a hybrid architecture that outperforms state-of-the-art methods. The selected architecture’s competitiveness is validated through a Z-test of proportions, underscoring its effectiveness in solving RCPSP-DC problems.},
  archive      = {J_CSCI},
  author       = {Tshewang Phuntsho and Tad Gonsalves},
  doi          = {10.1080/09540091.2024.2447365},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2447365},
  shortjournal = {Connect. Sci.},
  title        = {Selecting hybrids of metaheuristics for resource-constraint project scheduling problems with discounted cashflows},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human body contour extraction method based on human skeleton
key point guidance. <em>CSCI</em>, <em>37</em>(1), Article: 2445805. (<a
href="https://doi.org/10.1080/09540091.2024.2445805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By extracting human contours from 2D images captured by the camera and then obtaining human size data, the cost of garment custom measurement can be effectively reduced and the efficiency of custom measurement can be improved. The extraction of human contours plays an important role in the collection of online human size data. We propose a method to extract human contours by fusing the prior information of human skeleton key points into the salience target detection network. Specifically, the skeleton key point information extracted based on OpenPose is fused into the encoder-decoder network for rough detection of the human body target, and the residual refinement network is used to fine-adjust the human body matting, so as to achieve accurate human contour extraction. In this paper, the accuracy and superiority of the algorithm are verified in the public data set P3M-10K of human body matting and applied to the 2D body measurement WeChat applet on mobile phone and computer website.},
  archive      = {J_CSCI},
  author       = {Zhongwei Hua and Yong Ren and Yulu Wang and Zhuriyao Jin},
  doi          = {10.1080/09540091.2024.2445805},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2445805},
  shortjournal = {Connect. Sci.},
  title        = {Human body contour extraction method based on human skeleton key point guidance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards enhanced assessment question classification: A study
using machine learning, deep learning, and generative AI. <em>CSCI</em>,
<em>37</em>(1), Article: 2445249. (<a
href="https://doi.org/10.1080/09540091.2024.2445249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to benchmark the performance of machine learning (ML), deep learning (DL), and generative AI (GenAI) models in categorising assessment questions based on Bloom’s Taxonomy. Previous studies have lacked comprehensive investigations into the performance of these approaches. Further, the GenAI remains unexplored, offering a promising avenue for groundbreaking explorations. Therefore, we explore the effectiveness of various ML models by incorporating domain-specific term weighting and utilising word embeddings. The study also analyses the performance of Recurrent Neural Networks (RNNs) and Convolutional Neural Network (CNN) with and without bidirectional connections, as well as an approach that combines RNNs and CNN. Furthermore, we evaluate several transformer-based models by fine-tuning them alongside GenAI models text-davinci-003, gpt-3.5-turbo, PaLM2, and Gemini Pro in zero-shot classification settings. The results demonstrate that ML models outperformed DL models, achieving a best accuracy of 0.871 and F1 score of 0.872. Additionally, domain-specific term weighting is found to be superior to word embeddings. Furthermore, most ML and DL models performed better than GenAI models, with GenAI models achieving a best accuracy of 0.618 and a best F1 score of 0.627. Therefore, the outcome suggests considering the ML models with domain-specific term weighting as benchmark models in future research.},
  archive      = {J_CSCI},
  author       = {Mohammed Osman Gani and Ramesh Kumar Ayyasamy and Saadat M. Alhashmi and Khondaker Sajid Alam and Anbuselvan Sangodiah and Khondaker Khaleduzzman and Chinnasamy Ponnusamy},
  doi          = {10.1080/09540091.2024.2445249},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2445249},
  shortjournal = {Connect. Sci.},
  title        = {Towards enhanced assessment question classification: A study using machine learning, deep learning, and generative AI},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating robustness dynamics of shallow machine
learning techniques: Beyond basic natural variations in image
classification. <em>CSCI</em>, <em>37</em>(1), Article: 2435654. (<a
href="https://doi.org/10.1080/09540091.2024.2435654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the widespread adoption of deep learning models, shallow machine learning (SML) algorithms are still used for image classification due to simplicity, interpretability and efficiency. This study aims to bridge this gap by investigating the robustness dynamics of SML techniques under more complex scenarios, such as adversarial perturbations and geometric transformation. Five popular classification algorithms, including k-nearest neighbour and support vector machines, were employed to build classification models. Methodology involves investigating the robustness of proposed  methods, first, on original and corrupted data by utilising benchmark datasets across several image domains. To strengthen the investigation, the models were trained using a new low-rank representation (LRR) strategy. This hybrid model simultaneously addresses two key limitations in classical LRR models: overcoming the sequential learning process and effectively capturing both local and global data structures. By introducing a dual regularisation mechanism, it integrates a k-nearest neighbour graph to preserve local consistency, while a global low-rank constraint ensures coherent data representation. Experimental results reveal significant drops in accuracy of most SML methods, especially under adversarial attacks and geometric transformations, but LRR approach mitigates these effects to a notable extent, boosting performance across data variations. The results also show that the proposed  method outperforms state-of-the-art LRR techniques in most experiments.},
  archive      = {J_CSCI},
  author       = {Mengtong Li},
  doi          = {10.1080/09540091.2024.2435654},
  journal      = {Connection Science},
  month        = {12},
  number       = {1},
  pages        = {Article: 2435654},
  shortjournal = {Connect. Sci.},
  title        = {Investigating robustness dynamics of shallow machine learning techniques: Beyond basic natural variations in image classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
