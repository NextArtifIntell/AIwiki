<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JACM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jacm---10">JACM - 10</h2>
<ul>
<li><details>
<summary>
(2025). Integer programs with bounded subdeterminants and two
nonzeros per row. <em>JACM</em>, <em>72</em>(1), 1–50. (<a
href="https://doi.org/10.1145/3695985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a strongly polynomial-time algorithm for integer linear programs defined by integer coefficient matrices whose subdeterminants are bounded by a constant and that contain at most two nonzero entries in each row. The core of our approach is the first polynomial-time algorithm for the weighted stable set problem on graphs that do not contain more than k vertex-disjoint odd cycles, where k is any constant. Previously, polynomial-time algorithms were only known for k =0 (bipartite graphs) and for k =1. We observe that integer linear programs defined by coefficient matrices with bounded subdeterminants and two nonzeros per column can be also solved in strongly polynomial-time, using a reduction to b -matching.},
  archive      = {J_JACM},
  doi          = {10.1145/3695985},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-50},
  shortjournal = {J. ACM},
  title        = {Integer programs with bounded subdeterminants and two nonzeros per row},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subsampling suffices for adaptive data analysis.
<em>JACM</em>, <em>72</em>(1), 1–45. (<a
href="https://doi.org/10.1145/3698104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst’s query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of adaptive data analysis was formalized in the seminal works of Dwork et al. (STOC 2015) and Hardt and Ullman (FOCS 2014). We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: the only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work. In addition to its simplicity, we demonstrate the utility of this framework by designing mechanisms for two foundational tasks: statistical queries and median finding. In particular, our mechanism for answering the broadly applicable class of statistical queries is both extremely simple and state of the art in many parameter regimes.},
  archive      = {J_JACM},
  doi          = {10.1145/3698104},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-45},
  shortjournal = {J. ACM},
  title        = {Subsampling suffices for adaptive data analysis},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Orbit-finite linear programming. <em>JACM</em>,
<em>72</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3703909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An infinite set is orbit-finite if, up to permutations of atoms, it has only finitely many elements. We study a generalisation of linear programming where constraints are expressed by an orbit-finite system of linear inequalities. As our principal contribution we provide a decision procedure for checking if such a system has a real solution, and for computing the minimal/maximal value of a linear objective function over the solution set. We also show undecidability of these problems in case when only integer solutions are considered. Therefore orbit-finite linear programming is decidable, while orbit-finite integer linear programming is not.},
  archive      = {J_JACM},
  doi          = {10.1145/3703909},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-39},
  shortjournal = {J. ACM},
  title        = {Orbit-finite linear programming},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardness of approximate diameter: Now for undirected graphs.
<em>JACM</em>, <em>72</em>(1), 1–32. (<a
href="https://doi.org/10.1145/3704631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximating the graph diameter is a basic task of both theoretical and practical interest. A simple folklore algorithm can output a 2-approximation to the diameter in linear time by running BFS from an arbitrary vertex. It has been open whether a better approximation is possible in near-linear time. A series of articles on fine-grained complexity have led to strong hardness results for diameter in directed graphs, culminating in a recent tradeoff curve independently discovered by [Li, STOC’21] and [Dalirrooyfard and Wein, STOC’21], showing that under the Strong Exponential Time Hypothesis (SETH), for any integer k ≥ 2 and δ &gt; 0, a \(2-\frac{1}{k}-\delta\) approximation for diameter in directed m -edge graphs requires \(m^{1+1/(k-1)-o(1)}\) time. In particular, the simple linear time 2-approximation algorithm is optimal for directed graphs. In this article, we prove that the same tradeoff lower bound curve is possible for undirected graphs as well, extending results of [Roditty and Vassilevska W., STOC’13], [Li’20] and [Bonnet, ICALP’21] who proved the first few cases of the curve, k =2,3, and 4, respectively. Our result shows in particular that the simple linear time 2-approximation algorithm is conditionally optimal for undirected graphs. To obtain our result, we extract the core ideas in known reductions and introduce a unification and generalization that could be useful for proving SETH-based hardness for other problems in undirected graphs related to distance computation.},
  archive      = {J_JACM},
  doi          = {10.1145/3704631},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-32},
  shortjournal = {J. ACM},
  title        = {Hardness of approximate diameter: Now for undirected graphs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative coding and complexity theory of continuous
data: Part i: Motivation, definition, consequences. <em>JACM</em>,
<em>72</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3705609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When encoding real numbers as (necessarily infinite) bit-strings, the naïve binary/decimal expansion is well-known [ doi:10.1112/plms/s2-43.6.544 ] computably “ un reasonable”, rendering, for example, tripling qualitatively discontinuous on Cantor’s sequence space. Encoding reals as sequences of (finite integer numerators and denominators, in binary, of) rational approximations does make common operations qualitatively computable, yet admits no bounds on their computational complexity/quantitative continuity. Dyadic approximations, on the other hand, are known polynomially, and signed binary expansions even linearly, “reasonable” in a rigorous sense recalled in the introduction of this work. But how to distinguish between un/suitable encodings of spaces common in Calculus beyond the reals, such as Banach or Sobolev? With respect to qualitative computability/continuity on topological spaces, the technical condition of admissibility had been identified [ doi:10.1016/0304-3975(85)90208-7 ] for an encoding over Cantor space (historically called a representation ) to be “reasonable” [ doi:10.1007/978-3-030-59234-9_9 ] . Roughly speaking, admissibility requires the representation to be (i) continuous, and to be (ii) maximal with respect to continuous reduction. Admissible representations exist for a large class of spaces. And for (precisely) these does the Kreitz–Weihrauch—sometimes aka Main —Theorem of Computable Analysis hold, which characterizes continuity of functions by continuity of mappings translating codes, so-called realizers . We refine qualitative computability/continuity on topological spaces to quantitative continuity/complexity on metric spaces by proposing a notion, and investigating the properties, of polynomially/linearly admissible representations. Roughly speaking, these are (i) close to “optimally” continuous, namely linearly/polynomially relative to the space’s entropy, and they are (ii) maximal with respect to relative linear/polynomial quantitatively continuous reductions defined in the main text. Quantitatively admissible representations are closed under composition over generalized ground spaces beyond Cantor’s. Such representations exhibit a quantitative strengthening of the qualitative Main Theorem , namely now characterizing quantitative continuity of functions by quantitative continuity of realizers. A large class of compact metric spaces is shown to admit polynomially admissible representations over compact ultra metric spaces, and some even a generalization of the linearly admissible signed binary encoding. Quantitative admissibility thus provides the desired criterion for complexity-theoretically “reasonable” encodings.},
  archive      = {J_JACM},
  doi          = {10.1145/3705609},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-39},
  shortjournal = {J. ACM},
  title        = {Quantitative coding and complexity theory of continuous data: part i: motivation, definition, consequences},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correct and complete type checking and certified erasure for
coq, in coq. <em>JACM</em>, <em>72</em>(1), 1–74. (<a
href="https://doi.org/10.1145/3706056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coq is built around a well-delimited kernel that performs type checking for definitions in a variant of the Calculus of Inductive Constructions ( CIC ). Although the metatheory of CIC is very stable and reliable, the correctness of its implementation in Coq is less clear. Indeed, implementing an efficient type checker for CIC is a rather complex task, and many parts of the code rely on implicit invariants which can easily be broken by further evolution of the code. Therefore, on average, one critical bug has been found every year in Coq . This article presents the first implementation of a type checker for the kernel of Coq (without the module system, template polymorphism and η-conversion), which is proven sound and complete in Coq with respect to its formal specification. Note that because of Gödel’s second incompleteness theorem, there is no hope to prove completely the soundness of the specification of Coq inside Coq (in particular strong normalization), but it is possible to prove the correctness and completeness of the implementation assuming soundness of the specification, thus moving from a trusted code base (TCB) to a trusted theory base (TTB) paradigm. Our work is based on the MetaCoq project which provides meta-programming facilities to work with terms and declarations at the level of the kernel. We verify a relatively efficient type checker based on the specification of the typing relation of the Polymorphic, Cumulative Calculus of Inductive Constructions ( PCUIC ) at the basis of Coq . It is worth mentioning that during the verification process, we have found a source of incompleteness in Coq ’s official type checker, which has then been fixed in Coq 8.14 thanks to our work. In addition to the kernel implementation, another essential feature of Coq is the so-called extraction mechanism: the production of executable code in functional languages from Coq definitions. We present a verified version of this subtle type and proof erasure step, therefore enabling the verified extraction of a safe type checker for Coq in the future.},
  archive      = {J_JACM},
  doi          = {10.1145/3706056},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-74},
  shortjournal = {J. ACM},
  title        = {Correct and complete type checking and certified erasure for coq, in coq},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow-augmentation i: Directed graphs. <em>JACM</em>,
<em>72</em>(1), 1–38. (<a
href="https://doi.org/10.1145/3706103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show a flow-augmentation algorithm in directed graphs: There exists a randomized polynomial-time algorithm that, given a directed graph G , two vertices s, t ∈ V(G) , and an integer k , adds (randomly) to G a number of arcs such that for every minimal st -cut Z in G of size at most k , with probability 2 −poly( k ) the set Z becomes a minimum st -cut in the resulting graph. We also provide a deterministic counterpart of this procedure. The directed flow-augmentation tool allows us to prove fixed-parameter tractability of a number of problems parameterized by the cardinality of the deletion set whose parameterized complexity status was repeatedly posed as open problems: Chain SAT , defined by Chitnis, Egri, and Marx [ESA’13, Algorithmica’17], a number of weighted variants of classic directed cut problems, such as Weighted st - Cut or Weighted Directed Feedback Vertex Set . By proving that Chain SAT is FPT, we confirm a conjecture of Chitnis, Egri, and Marx that, for any graph H , if the List H - Coloring problem is polynomial-time solvable, then the corresponding vertex-deletion problem is fixed-parameter tractable. Chain SAT , defined by Chitnis, Egri, and Marx [ESA’13, Algorithmica’17], a number of weighted variants of classic directed cut problems, such as Weighted st - Cut or Weighted Directed Feedback Vertex Set .},
  archive      = {J_JACM},
  doi          = {10.1145/3706103},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-38},
  shortjournal = {J. ACM},
  title        = {Flow-augmentation i: Directed graphs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric embeddability of complexes is ∃ℝ-complete.
<em>JACM</em>, <em>72</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3707201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the decision problem of determining whether a given (abstract simplicial) k -complex has a geometric embedding in ℝ d is complete for the Existential Theory of the Reals for all d ≥ 3 and k ∈ { d -1, d } by reducing from pseudoline stretchability. Consequently, the problem is polynomial time equivalent to determining whether a polynomial equation system has a real solution. Moreover, this implies NP-hardness and constitutes the first hardness result for the algorithmic problem of geometrically embedding (abstract simplicial) complexes. This complements recent breakthroughs for the computational complexity of piece-wise linear embeddability [Matoušek, Sedgwick, Tancer, and Wagner, J. ACM 2018, and de Mesmay, Rieck, Sedgwick and Tancer, J. ACM 2020] and establishes connections to computational topology.},
  archive      = {J_JACM},
  doi          = {10.1145/3707201},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. ACM},
  title        = {Geometric embeddability of complexes is ∃ℝ-complete},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient quantum factoring algorithm. <em>JACM</em>,
<em>72</em>(1), 1–13. (<a
href="https://doi.org/10.1145/3708471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that n -bit integers can be factorized by independently running a quantum circuit with \(\tilde{O}(n^{3/2})\) gates for \(\sqrt {n}+4\) times, and then using polynomial-time classical post-processing. The correctness of the algorithm relies on a certain number-theoretic conjecture. It is currently not clear if the algorithm can lead to improved physical implementations in practice.},
  archive      = {J_JACM},
  doi          = {10.1145/3708471},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. ACM},
  title        = {An efficient quantum factoring algorithm},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallelize single-site dynamics up to dobrushin criterion.
<em>JACM</em>, <em>72</em>(1), 1–33. (<a
href="https://doi.org/10.1145/3708558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-site dynamics are canonical Markov chain based algorithms for sampling from high-dimensional distributions, such as the Gibbs distributions of graphical models. We introduce a simple and generic parallel algorithm that faithfully simulates single-site dynamics. Under a much relaxed, asymptotic variant of the ℓ p -Dobrushin’s condition—where the Dobrushin’s influence matrix has a bounded ℓ p -induced operator norm for an arbitrary p ∈ [1, ∞]—our algorithm simulates N steps of single-site updates within a parallel depth of O ( N / n +log n ) on Õ( m ) processors, where n is the number of sites and m is the size of the graphical model. For Boolean-valued random variables, if the ℓ p -Dobrushin’s condition holds—specifically, if the ℓ p -induced operator norm of the Dobrushin’s influence matrix is less than 1—the parallel depth can be further reduced to O (log N + log n ), achieving an exponential speedup. These results suggest that single-site dynamics with near-linear mixing times can be parallelized into RNC sampling algorithms, independent of the maximum degree of the underlying graphical model, as long as the Dobrushin influence matrix maintains a bounded operator norm. We show the effectiveness of this approach with RNC samplers for the hardcore and Ising models within their uniqueness regimes, as well as an RNC SAT sampler for satisfying solutions of conjunctive normal form formulas in a local lemma regime. Furthermore, by employing non-adaptive simulated annealing, these RNC samplers can be transformed into RNC algorithms for approximate counting.},
  archive      = {J_JACM},
  doi          = {10.1145/3708558},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {J. ACM},
  title        = {Parallelize single-site dynamics up to dobrushin criterion},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
