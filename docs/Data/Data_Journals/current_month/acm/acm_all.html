<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>acm_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="acm">ACM</h1>
<h2 id="cacm---20">CACM - 20</h2>
<ul>
<li><details>
<summary>
(2025). Not on the best path. <em>CACM</em>, <em>68</em>(3),
104–ff. (<a href="https://doi.org/10.1145/3707200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3707200},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {104-ff},
  shortjournal = {Commun. ACM},
  title        = {Not on the best path},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). R2T: Instance-optimal truncation for differentially private
query evaluation with foreign keys. <em>CACM</em>, <em>68</em>(3),
93–101. (<a href="https://doi.org/10.1145/3708494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answering SPJA queries under differential privacy (DP), including graph-pattern counting under node-DP as an important special case, has received considerable attention in recent years. The dual challenge of foreign-key constraints and self-joins is particularly tricky to deal with, and no existing DP mechanisms can correctly handle both. For the special case of graph pattern counting under node-DP, the existing mechanisms are correct (that is, satisfy DP), but they do not offer nontrivial utility guarantees or are very complicated and costly. In this paper, we propose the first DP mechanism for answering arbitrary SPJA queries in a database with foreign-key constraints. Meanwhile, it achieves a fairly strong notion of optimality, which can be considered as a small and natural relaxation of instance optimality. Finally, our mechanism is simple enough that it can be easily implemented on top of any RDBMS and an LP solver. Experimental results show that it offers order-of-magnitude improvements in terms of utility over existing techniques, even those specifically designed for graph pattern counting.},
  archive      = {J_CACM},
  doi          = {10.1145/3708494},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {93-101},
  shortjournal = {Commun. ACM},
  title        = {R2T: Instance-optimal truncation for differentially private query evaluation with foreign keys},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical perspective: Toward building a differentially
private DBMS. <em>CACM</em>, <em>68</em>(3), 92. (<a
href="https://doi.org/10.1145/3708551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708551},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {92},
  shortjournal = {Commun. ACM},
  title        = {Technical perspective: Toward building a differentially private DBMS},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New spectral algorithms for refuting smoothed k-SAT.
<em>CACM</em>, <em>68</em>(3), 83–91. (<a
href="https://doi.org/10.1145/3635117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite being a quintessential example of a hard problem, the quest for finding fast algorithms for deciding satisfiability of propositional formulas has occupied computer scientists both in theory and in practice. In this article, we survey recent progress on designing algorithms with strong refutation guarantees for smoothed instances of the k -SAT problem. Smoothed instances are formed by slight random perturbations of arbitrary instances, and their study is a way to bridge the gap between worst-case and average-case models of problem instances. Our methods yield new algorithms for smoothed k -SAT instances with guarantees that match those for the significantly simpler and well-studied model of random formulas. Additionally, they have led to a novel and unexpected line of attack on some longstanding extremal combinatorial problems in graph theory and coding theory. As an example, we will discuss the resolution of a 2008 conjecture of Feige on the existence of short cycles in hypergraphs.},
  archive      = {J_CACM},
  doi          = {10.1145/3635117},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {83-91},
  shortjournal = {Commun. ACM},
  title        = {New spectral algorithms for refuting smoothed k-SAT},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The surprising power of spectral refutation. <em>CACM</em>,
<em>68</em>(3), 82. (<a href="https://doi.org/10.1145/3660530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3660530},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {82},
  shortjournal = {Commun. ACM},
  title        = {The surprising power of spectral refutation},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The sustainability gap for computing: Quo vadis?
<em>CACM</em>, <em>68</em>(3), 70–79. (<a
href="https://doi.org/10.1145/3699595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3699595},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {70-79},
  shortjournal = {Commun. ACM},
  title        = {The sustainability gap for computing: Quo vadis?},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Molecular communications in blood vessels: Models, analysis,
and enabling technologies. <em>CACM</em>, <em>68</em>(3), 60–69. (<a
href="https://doi.org/10.1145/3696205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3696205},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {60-69},
  shortjournal = {Commun. ACM},
  title        = {Molecular communications in blood vessels: Models, analysis, and enabling technologies},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting cross-layer vulnerabilities: Off-path attacks on
the TCP/IP protocol suite. <em>CACM</em>, <em>68</em>(3), 48–59. (<a
href="https://doi.org/10.1145/3689819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3689819},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {48-59},
  shortjournal = {Commun. ACM},
  title        = {Exploiting cross-layer vulnerabilities: Off-path attacks on the TCP/IP protocol suite},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prevalence and prevention of large language model use in
crowd work. <em>CACM</em>, <em>68</em>(3), 42–47. (<a
href="https://doi.org/10.1145/3685527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3685527},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {42-47},
  shortjournal = {Commun. ACM},
  title        = {Prevalence and prevention of large language model use in crowd work},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Program merge: What’s deep learning got to do with it?
<em>CACM</em>, <em>68</em>(3), 34–41. (<a
href="https://doi.org/10.1145/3704254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3704254},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {34-41},
  shortjournal = {Commun. ACM},
  title        = {Program merge: What&#39;s deep learning got to do with it?},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A glimpse into the pandora’s box. <em>CACM</em>,
<em>68</em>(3), 30–32. (<a
href="https://doi.org/10.1145/3714417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3714417},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {30-32},
  shortjournal = {Commun. ACM},
  title        = {A glimpse into the pandora&#39;s box},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence as catalyst for biodiversity
understanding. <em>CACM</em>, <em>68</em>(3), 27–29. (<a
href="https://doi.org/10.1145/3701570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701570},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {27-29},
  shortjournal = {Commun. ACM},
  title        = {Artificial intelligence as catalyst for biodiversity understanding},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The AI alignment paradox. <em>CACM</em>, <em>68</em>(3),
24–26. (<a href="https://doi.org/10.1145/3705294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3705294},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {24-26},
  shortjournal = {Commun. ACM},
  title        = {The AI alignment paradox},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Abstractions. <em>CACM</em>, <em>68</em>(3), 21–23. (<a
href="https://doi.org/10.1145/3710809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3710809},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {21-23},
  shortjournal = {Commun. ACM},
  title        = {Abstractions},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). California’s AI act vetoed. <em>CACM</em>, <em>68</em>(3),
18–20. (<a href="https://doi.org/10.1145/3710808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3710808},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {18-20},
  shortjournal = {Commun. ACM},
  title        = {California’s AI act vetoed},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlling AI’s growing energy needs. <em>CACM</em>,
<em>68</em>(3), 15–17. (<a
href="https://doi.org/10.1145/3703788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3703788},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {15-17},
  shortjournal = {Commun. ACM},
  title        = {Controlling AI’s growing energy needs},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How software bugs led to “one of the greatest miscarriages
of justice” in british history. <em>CACM</em>, <em>68</em>(3), 12–14.
(<a href="https://doi.org/10.1145/3703779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3703779},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {12-14},
  shortjournal = {Commun. ACM},
  title        = {How software bugs led to “One of the greatest miscarriages of justice” in british history},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feedback loops guide AI to proof checking. <em>CACM</em>,
<em>68</em>(3), 9–11. (<a
href="https://doi.org/10.1145/3703778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After decades of promise, techniques and technologies are coming together to make artificial intelligence better at checking mathematicians’ work.},
  archive      = {J_CACM},
  doi          = {10.1145/3703778},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {9-11},
  shortjournal = {Commun. ACM},
  title        = {Feedback loops guide AI to proof checking},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Putting the smarts into robot bodies. <em>CACM</em>,
<em>68</em>(3), 6–8. (<a href="https://doi.org/10.1145/3703761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3703761},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {6-8},
  shortjournal = {Commun. ACM},
  title        = {Putting the smarts into robot bodies},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Homo ratiocinator (reckoning human). <em>CACM</em>,
<em>68</em>(3), 5. (<a href="https://doi.org/10.1145/3714998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3714998},
  journal      = {Communications of the ACM},
  month        = {3},
  number       = {3},
  pages        = {5},
  shortjournal = {Commun. ACM},
  title        = {Homo ratiocinator (Reckoning human)},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="csur---27">CSUR - 27</h2>
<ul>
<li><details>
<summary>
(2025). Explaining the explainers in graph neural networks: A
comparative study. <em>CSUR</em>, <em>57</em>(5), 1–37. (<a
href="https://doi.org/10.1145/3696444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following a fast initial breakthrough in graph-based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process. GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable or which explainer should be preferred in a given setting. In this survey we fill these gaps by devising a systematic experimental study, which tests 12 explainers on eight representative message-passing architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the choice and applicability of GNN explainers, we isolate key components that make them usable and successful and provide recommendations on how to avoid common interpretation pitfalls. We conclude by highlighting open questions and directions of possible future research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3696444},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-37},
  shortjournal = {ACM Comput. Surv.},
  title        = {Explaining the explainers in graph neural networks: A comparative study},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trustworthy AI-based performance diagnosis systems for cloud
applications: A review. <em>CSUR</em>, <em>57</em>(5), 1–37. (<a
href="https://doi.org/10.1145/3701740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance diagnosis systems are defined as detecting abnormal performance phenomena and play a crucial role in cloud applications. An effective performance diagnosis system is often developed based on artificial intelligence (AI) approaches, which can be summarized into a general framework from data to models. However, the AI-based framework has potential hazards that could degrade the user experience and trust. For example, a lack of data privacy may compromise the security of AI models, and low robustness can be hard to apply in complex cloud environments. Therefore, defining the requirements for building a trustworthy AI-based performance diagnosis system has become essential. This article systematically reviews trustworthiness requirements in AI-based performance diagnosis systems. We first introduce trustworthiness requirements and extract six key requirements from a technical perspective, including data privacy, fairness, robustness, explainability, efficiency, and human intervention. We then unify these requirements into a general performance diagnosis framework, ranging from data collection to model development. Next, we comprehensively provide related works for each component and concrete actions to improve trustworthiness in the framework. Finally, we identify possible research directions and challenges for the future development of trustworthy AI-based performance diagnosis systems.},
  archive      = {J_CSUR},
  doi          = {10.1145/3701740},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-37},
  shortjournal = {ACM Comput. Surv.},
  title        = {Trustworthy AI-based performance diagnosis systems for cloud applications: A review},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT authentication protocols: Challenges, and comparative
analysis. <em>CSUR</em>, <em>57</em>(5), 1–43. (<a
href="https://doi.org/10.1145/3703444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the ever-evolving information technology landscape, the Internet of Things (IoT) is a groundbreaking concept that bridges the physical and digital worlds. It is the backbone of an increasingly sophisticated interactive environment, yet it is a subject of intricate security challenges spawned by its multifaceted manifestations. Central to securing IoT infrastructures is the crucial aspect of authentication, necessitating a comprehensive examination of its nuances, including benefits, challenges, opportunities, trends, and societal implications. In this article, we thoroughly review the IoT authentication protocols (Aps), addressing the main challenges such as privacy protection, scalability, and human factors that may impact security. Through exacting analysis, we evaluate the strengths and weaknesses of existing APs and conduct a comparative performance analysis to evaluate their effectiveness and scalability in securing IoT environments and devices. At the end of this study, we summarize the main findings and suggest ways to improve the security of IoT devices in the future.},
  archive      = {J_CSUR},
  doi          = {10.1145/3703444},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-43},
  shortjournal = {ACM Comput. Surv.},
  title        = {IoT authentication protocols: Challenges, and comparative analysis},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The internet of bio-nano things with insulin-glucose,
security and research challenges: A survey. <em>CSUR</em>,
<em>57</em>(5), 1–42. (<a
href="https://doi.org/10.1145/3703448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Bio-Nano Things (IoBNT) is collaborative cell biology and nanodevice technology interacting through Molecular Communication (MC). The IoBNT can be accomplished by using the Information and Communication Theory (ICT) study of biological networks. Various technologies such as the Internet of Nano Things (IoNT), the Internet of Bio-degradable Things (IoBDT), and the Internet of Ingestible Things (IoIT) contribute to the development of IoBNT. Our survey discussed the Bio-Nano network and the role of IoT-based technologies along with a comparative study from various literature. We surveyed the various applications of IoNT in which the drug delivery for Insulin-Glucose system is prominent. Our survey aims to provide information about the Insulin-Glucose system involving the IoBNT and MC. We described the details of various factors for a diabetes analysis. We surveyed the diffusion coefficients of Insulin, Glucose, and the various parameters that influence insulin production in the body. Our survey identifies the security aspects of IoBNT such as attacks in nanonetworks, bio-cyber interface, Insulin-Glucose system, and their possible mitigation techniques. Our survey also provides a hierarchical model of the Bio-Nano network collaborating all the related technologies. Finally, our survey includes the research challenges involved in the proper handling of the IoBNT.},
  archive      = {J_CSUR},
  doi          = {10.1145/3703448},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-42},
  shortjournal = {ACM Comput. Surv.},
  title        = {The internet of bio-nano things with insulin-glucose, security and research challenges: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of AI-generated content (AIGC). <em>CSUR</em>,
<em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3704262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Artificial Intelligence Generated Content (AIGC) has gained significant attention from society, especially with the rise of Generative AI (GAI) techniques such as ChatGPT, GPT-4 [ 165 ], DALL-E-3 [ 184 ], and Sora [ 137 ]. AIGC involves using AI models to create digital content, such as images, music, and natural language, with the goal of making the content creation process more efficient and accessible. Large-scale models have become increasingly important in AIGC as they provide better intent extraction and generation results. This survey provides a comprehensive review of the history of generative models and recent advances in AIGC, focusing on both unimodal and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, the survey discusses the existing open problems and future challenges in AIGC. Overall, this survey serves as a valuable resource for individuals interested in understanding the background and secrets behind the impressive performance of AIGC techniques.},
  archive      = {J_CSUR},
  doi          = {10.1145/3704262},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {A survey of AI-generated content (AIGC)},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent advances of foundation language models-based
continual learning: A survey. <em>CSUR</em>, <em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3705725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing and computer vision. Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich common sense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. Despite these capabilities, LMs still struggle with catastrophic forgetting, hindering their ability to learn continuously like humans. To address this, continual learning (CL) methodologies have been introduced, allowing LMs to adapt to new tasks while retaining learned knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking. In this article, we delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models, large language models, and vision-language models. We divide these studies into offline and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.},
  archive      = {J_CSUR},
  doi          = {10.1145/3705725},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {Recent advances of foundation language models-based continual learning: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wi-fi sensing techniques for human activity recognition:
Brief survey, potential challenges, and research directions.
<em>CSUR</em>, <em>57</em>(5), 1–30. (<a
href="https://doi.org/10.1145/3705893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in wireless communication technologies have made Wi-Fi signals indispensable in both personal and professional settings. The utilization of these signals for Human Activity Recognition (HAR) has emerged as a cutting-edge technology. By leveraging the fluctuations in Wi-Fi signals for HAR, this approach offers enhanced privacy compared to traditional visual surveillance methods. The essence of this technique lies in detecting subtle changes when Wi-Fi signals interact with the human body, which are then captured and interpreted by advanced algorithms. This article initially provides an overview of the key methodologies in HAR and the evolution of non-contact sensing, introducing sensor-based recognition, computer vision, and Wi-Fi signal based approaches, respectively. It then explores tools for Wi-Fi-based HAR signal collection and lists several high-quality datasets. Subsequently, the article reviews various sensing tasks enabled by Wi-Fi signal recognition, highlighting the application of deep learning networks in Wi-Fi signal detection. Experimental results are then presented that assess the capabilities of different networks. The findings indicate significant variability in the generalization capacities of neural networks and notable differences in test accuracy for various motion analyses.},
  archive      = {J_CSUR},
  doi          = {10.1145/3705893},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-30},
  shortjournal = {ACM Comput. Surv.},
  title        = {Wi-fi sensing techniques for human activity recognition: Brief survey, potential challenges, and research directions},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource-efficient algorithms and systems of foundation
models: A survey. <em>CSUR</em>, <em>57</em>(5), 1–39. (<a
href="https://doi.org/10.1145/3706418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large foundation models, including large language models, vision transformers, diffusion, and large language model based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.},
  archive      = {J_CSUR},
  doi          = {10.1145/3706418},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-39},
  shortjournal = {ACM Comput. Surv.},
  title        = {Resource-efficient algorithms and systems of foundation models: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial binaries: AI-guided instrumentation methods for
malware detection evasion. <em>CSUR</em>, <em>57</em>(5), 1–36. (<a
href="https://doi.org/10.1145/3706573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial binaries are executable files that have been altered without loss of function by an AI agent in order to deceive malware detection systems. Progress in this emergent vein of research has been constrained by the complex and rigid structure of executable files. Although prior work has demonstrated that these binaries deceive a variety of malware classification models which rely on disparate feature sets, a consensus as to the best approach has not been reached, either in terms of the optimization algorithms or the instrumentation methods. Although inconsistencies in the data sets, target classifiers, and functionality verification methods make head-to-head comparisons difficult, we extract lessons learned and make recommendations for future research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3706573},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-36},
  shortjournal = {ACM Comput. Surv.},
  title        = {Adversarial binaries: AI-guided instrumentation methods for malware detection evasion},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review of enterprise architecture
evaluation methods. <em>CSUR</em>, <em>57</em>(5), 1–36. (<a
href="https://doi.org/10.1145/3706582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise Architecture (EA) is a systematic and holistic approach to designing and managing an organization&#39;s information systems components, aiding in optimizing resources, managing risk, and facilitating change. It weighs different architectural quality attributes against each other to achieve the most advantageous architecture. However, the evaluation of EA lacks a systematic approach. This study employs a Systematic Literature Review, analyzing, in detail, 109 articles carefully selected from 3644 papers published since 2005. The key outcome of the research reveals that a crucial factor for the extensive worldwide adoption of EA evaluation methods lies in the automation of the assessment and architecture modeling processes, particularly emphasizing the facet of data collection. The automation of EA evaluation will empower organizations to streamline their processes, make data-driven decisions, and respond more effectively to change, ultimately contributing to their competitiveness and long-term success in the global market. The study identifies diverse evaluation methods, determines evaluation criteria, examines the extent to which these methods have been verified in practice, and provides directions for further research and advancement.},
  archive      = {J_CSUR},
  doi          = {10.1145/3706582},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-36},
  shortjournal = {ACM Comput. Surv.},
  title        = {A systematic literature review of enterprise architecture evaluation methods},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on privacy-preserving caching at network edge:
Classification, solutions, and challenges. <em>CSUR</em>,
<em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3706630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caching content at the edge network is a popular and effective technique widely deployed to alleviate the burden of network backhaul, shorten service delay, and improve service quality. However, there has been some controversy over privacy violations in caching content at the edge network. On the one hand, the multi-access open edge network provides an ideal entrance or interface for external attackers to obtain private data from edge caches by extracting sensitive information. On the other hand, privacy can be infringed on by curious edge caching providers through caching trace analysis targeting the achievement of better caching performance or higher profits. Therefore, an in-depth understanding of privacy issues in edge caching networks is vital and indispensable for creating a privacy-preserving caching service at the edge network. In this article, we are among the first to fill this gap by examining privacy-preserving techniques for caching content at the edge network. First, we provide an introduction to the background of privacy-preserving edge caching. Next, we summarize the key privacy issues and present a taxonomy for caching at the edge network from the perspective of private information. Additionally, we conduct a retrospective review of the state-of-the-art countermeasures against privacy leakage from content caching at the edge network. Finally, we conclude the survey and envision challenges for future research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3706630},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {A survey on privacy-preserving caching at network edge: Classification, solutions, and challenges},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly-supervised semantic segmentation with image-level
labels: From traditional models to foundation models. <em>CSUR</em>,
<em>57</em>(5), 1–29. (<a
href="https://doi.org/10.1145/3707447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep learning has driven significant progress in image semantic segmentation—a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time consuming, and labor intensive. Weakly supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully supervised semantic segmentation. In this article, our focus is on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges of deploying visual foundational models for WSSS, facilitating future developments in this exciting research area. Our code is provided at this link: https://github.com/zhaozhengChen/SAM_WSSS .},
  archive      = {J_CSUR},
  doi          = {10.1145/3707447},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-29},
  shortjournal = {ACM Comput. Surv.},
  title        = {Weakly-supervised semantic segmentation with image-level labels: From traditional models to foundation models},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial machine learning attacks and defences in
multi-agent reinforcement learning. <em>CSUR</em>, <em>57</em>(5), 1–35.
(<a href="https://doi.org/10.1145/3708320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Reinforcement Learning (MARL) is susceptible to Adversarial Machine Learning (AML) attacks. Execution-time AML attacks against MARL are complex due to effects that propagate across time and between agents. To understand the interaction between AML and MARL, this survey covers attacks and defences for MARL, Multi-Agent Learning (MAL), and Deep Reinforcement Learning (DRL). This survey proposes a novel perspective on AML attacks based on attack vectors. This survey also proposes a framework that addresses gaps in current modelling frameworks and enables the comparison of different attacks against MARL. Lastly, the survey identifies knowledge gaps and future avenues of research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708320},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Comput. Surv.},
  title        = {Adversarial machine learning attacks and defences in multi-agent reinforcement learning},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed machine learning in edge computing: Challenges,
solutions and future directions. <em>CSUR</em>, <em>57</em>(5), 1–37.
(<a href="https://doi.org/10.1145/3708495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning on edges is widely used in intelligent transportation, smart home, industrial manufacturing, and underground pipe network monitoring to achieve low latency and real time data processing and prediction. However, the presence of a large number of sensing and edge devices with limited computing, storage, and communication capabilities prevents the deployment of huge machine learning models and hinders its application. At the same time, although distributed machine learning on edges forms an emerging and rapidly growing research area, there has not been a systematic survey on this topic. The article begins by detailing the challenges of distributed machine learning in edge environments, such as limited node resources, data heterogeneity, privacy, security issues, and summarizes common metrics for model optimization. We then present a detailed analysis of parallelism patterns, distributed architectures, and model communication and aggregation schemes in edge computing. we subsequently present a comprehensive classification and intensive description of node resource-constrained processing, heterogeneous data processing, attacks and protection of privacy. The article ends by summarizing the applications of distributed machine learning in edge computing and presenting problems and challenges for further research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708495},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-37},
  shortjournal = {ACM Comput. Surv.},
  title        = {Distributed machine learning in edge computing: Challenges, solutions and future directions},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on physical layer authentication
techniques: Categorization and analysis of model-driven and data-driven
approaches. <em>CSUR</em>, <em>57</em>(5), 1–35. (<a
href="https://doi.org/10.1145/3708496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The open and broadcast nature of wireless mediums introduces significant security vulnerabilities, making authentication a critical concern in wireless networks. In recent years, Physical-Layer Authentication (PLA) techniques have garnered considerable research interest due to their advantages over Upper-Layer Authentication (ULA) methods, such as lower complexity, enhanced security, and greater compatibility. The application of signal processing techniques in PLA serves as a crucial link between the extraction of Physical-Layer Features (PLFs) and the authentication of received signals. Different signal processing approaches, even with the same PLF, can result in varying authentication performances and computational demands. Despite this, there remains a shortage of comprehensive overviews on state-of-the-art PLA schemes with a focus on signal processing approaches. This article presents the first thorough survey of signal processing in various PLA schemes, categorizing existing approaches into model-based and Machine Learning (ML)-based schemes. We discuss motivation and address key issues in signal processing for PLA schemes. The applications, challenges, and future research directions of PLA are discussed in Part 3 of the Appendix, which can be found in supplementary materials online.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708496},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Comput. Surv.},
  title        = {A comprehensive survey on physical layer authentication techniques: Categorization and analysis of model-driven and data-driven approaches},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards trustworthy machine learning in production: An
overview of the robustness in MLOps approach. <em>CSUR</em>,
<em>57</em>(5), 1–35. (<a
href="https://doi.org/10.1145/3708497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI), and especially its sub-field of Machine Learning (ML), are impacting the daily lives of everyone with their ubiquitous applications. In recent years, AI researchers and practitioners have introduced principles and guidelines to build systems that make reliable and trustworthy decisions. From a practical perspective, conventional ML systems process historical data to extract the features that are consequently used to train ML models that perform the desired task. However, in practice, a fundamental challenge arises when the system needs to be operationalized and deployed to evolve and operate in real-life environments continuously. To address this challenge, Machine Learning Operations (MLOps) have emerged as a potential recipe for standardizing ML solutions in deployment. Although MLOps demonstrated great success in streamlining ML processes, thoroughly defining the specifications of robust MLOps approaches remains of great interest to researchers and practitioners. In this paper, we provide a comprehensive overview of the trustworthiness property of MLOps systems. Specifically, we highlight technical practices to achieve robust MLOps systems. In addition, we survey the existing research approaches that address the robustness aspects of ML systems in production. We also review the tools and software available to build MLOps systems and summarize their support to handle the robustness aspects. Finally, we present the open challenges and propose possible future directions and opportunities within this emerging field. The aim of this paper is to provide researchers and practitioners working on practical AI applications with a comprehensive view to adopt robust ML solutions in production environments.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708497},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Comput. Surv.},
  title        = {Towards trustworthy machine learning in production: An overview of the robustness in MLOps approach},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of geometric optimization for deep learning: From
euclidean space to riemannian manifold. <em>CSUR</em>, <em>57</em>(5),
1–37. (<a href="https://doi.org/10.1145/3708498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) has achieved remarkable success in tackling complex Artificial Intelligence tasks. The standard training of neural networks employs backpropagation to compute gradients and utilizes various optimization algorithms in the Euclidean space ℛ n . However, this optimization process faces challenges, such as the local optimal issues and the problem of gradient vanishing and exploding. To address these problems, Riemannian optimization offers a powerful extension to solve optimization problems in deep learning. By incorporating the prior constraint structure and the metric information of the underlying geometric information, Riemannian optimization-based DL offers a more stable and reliable optimization process, as well as enhanced adaptability to complex data structures. This article presents a comprehensive survey of applying geometric optimization in DL, including the basic procedure of geometric optimization, various geometric optimizers, and some concepts of the Riemannian manifold. In addition, it investigates various applications of geometric optimization in different DL networks for diverse tasks and discusses typical public toolboxes that implement optimization on the manifold. This article also includes a performance comparison among different deep geometric optimization methods in image recognition scenarios. Finally, this article elaborates on future opportunities and challenges in this field.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708498},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-37},
  shortjournal = {ACM Comput. Surv.},
  title        = {A survey of geometric optimization for deep learning: From euclidean space to riemannian manifold},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent generation of graphical game assets: A
conceptual framework and systematic review of the state of the art.
<em>CSUR</em>, <em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3708499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural content generation (PCG) can be applied to a wide variety of tasks in games, from narratives, levels, and sounds to trees and weapons. A large amount of game content is composed of graphical assets , such as clouds, buildings, or vegetation, that do not require gameplay function considerations. There is also a breadth of literature examining the procedural generation of such elements for purposes outside of games. The body of research, focused on specific methods for generating specific assets, provides a narrow view of the available possibilities. Hence, it is difficult to have a clear picture of all approaches and possibilities, with no guide for interested parties to discover possible methods and approaches for their needs and no facility to guide them through each technique or approach to map out the process of using them. Therefore, a systematic literature review has been conducted, yielding 239 accepted papers. This article explores state-of-the-art approaches to graphical asset generation, examining research from a wide range of applications, inside and outside of games. Informed by the literature, a conceptual framework has been derived to address the aforementioned gaps.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708499},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {Intelligent generation of graphical game assets: A conceptual framework and systematic review of the state of the art},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterization of android malwares and their families.
<em>CSUR</em>, <em>57</em>(5), 1–31. (<a
href="https://doi.org/10.1145/3708500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, smartphones have made our lives easier and have become essential gadgets for us. Apart from calling, mobiles are used for various purposes, such as banking, chatting, data storage, connecting to the internet, and running apps, that make life easier. Therefore, attackers are developing new methods or malware to steal smartphone data. Primarily, the study outlines various types of Android malware families, the evolution of Android malware and its effects on detection techniques over time. We report malware timelines and Android app datasets with their source web links. Data are collected from various recent studies and reported. In this study, we have reported 384 Android malware families and their year of discovery, i.e., from 2001 to 2020. According to the malfunctions they perform on the device, we categorized the families into 11 types. Information about datasets is divided into three categories, along with their source links, and is presented. The categorization and timeline of malware will make it easy for researchers to focus on upcoming trends according to the malware category and activities they perform. Various open issues and future challenges are also addressed for future researchers.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708500},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-31},
  shortjournal = {ACM Comput. Surv.},
  title        = {Characterization of android malwares and their families},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual content privacy protection: A survey. <em>CSUR</em>,
<em>57</em>(5), 1–36. (<a
href="https://doi.org/10.1145/3708501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision is the most important sense for people, and it is also one of the main ways of cognition. As a result, people tend to utilize visual content to capture and share their life experiences, which greatly facilitates the transfer of information. Meanwhile, it also increases the risk of privacy violations, e.g., an image or video can reveal different kinds of privacy-sensitive information. Scholars have persistently pursued the advancement of tailored privacy protection measures. Various surveys attempt to consolidate these efforts from specific viewpoints. Nevertheless, these surveys tend to focus on particular issues, scenarios, or technologies, hindering a comprehensive overview of existing solutions on a broader scale. In this survey, a framework that encompasses various concerns and solutions for visual privacy is proposed, which allows for a macro understanding of privacy concerns from a comprehensive level. It is based on the fact that privacy concerns have corresponding adversaries, and divides privacy protection into three categories, based on computer vision (CV) adversary, based on human vision (HV) adversary, and based on CV &amp; HV adversary. For each category, we analyze the characteristics of the main approaches to privacy protection, and then systematically review representative solutions. Open challenges and future directions for visual privacy protection are also discussed.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708501},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-36},
  shortjournal = {ACM Comput. Surv.},
  title        = {Visual content privacy protection: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISP meets deep learning: A survey on deep learning methods
for image signal processing. <em>CSUR</em>, <em>57</em>(5), 1–44. (<a
href="https://doi.org/10.1145/3708516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The entire Image Signal Processor (ISP) of a camera relies on several processes to transform the data from the Color Filter Array (CFA) sensor, such as demosaicing, denoising, and enhancement. These processes can be executed either by some hardware or via software. In recent years, Deep Learning (DL) has emerged as one solution for some of them or even to replace the entire ISP using a single neural network for the task. In this work, we investigated several recent pieces of research in this area and provide deeper analysis and comparison among them, including results and possible points of improvement for future researchers.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708516},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-44},
  shortjournal = {ACM Comput. Surv.},
  title        = {ISP meets deep learning: A survey on deep learning methods for image signal processing},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserved and responsible recommenders: From
conventional defense to federated learning and blockchain.
<em>CSUR</em>, <em>57</em>(5), 1–35. (<a
href="https://doi.org/10.1145/3708982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RS) play an integral role in many online platforms. Exponential growth and potential commercial interests are raising significant concerns around privacy, security, fairness, and overall responsibility. The existing literature around responsible recommendation services is diverse and multidisciplinary. Most literature reviews cover a specific aspect or a single technology for responsible behavior, such as federated learning or blockchain. This study integrates relevant concepts across disciplines to provide a broader representation of the landscape. We review the latest advancements toward building privacy-preserved and responsible recommendation services for the e-commerce industry. The survey summarizes recent, high-impact works on diverse aspects and technologies that ensure responsible behavior in RS through an interconnected taxonomy. We contextualize potential privacy threats, practical significance, industrial expectations, and research remedies. From the technical viewpoint, we analyze conventional privacy defenses and provide an overview of emerging technologies including differential privacy, federated learning, and blockchain. The methods and concepts across technologies are linked based on their objectives, challenges, and future directions. In addition, we also develop an open source repository that summarizes a wide range of evaluation benchmarks, codebases, and toolkits to aid the further research. The survey offers a holistic perspective on this rapidly evolving landscape by synthesizing insights from both RS and responsible AI literature.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708982},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Comput. Surv.},
  title        = {Privacy-preserved and responsible recommenders: From conventional defense to federated learning and blockchain},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An in-depth analysis of password managers and two-factor
authentication tools. <em>CSUR</em>, <em>57</em>(5), 1–32. (<a
href="https://doi.org/10.1145/3711117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passwords remain the primary authentication method in online services, a domain increasingly crucial in our digital age. However, passwords suffer from several well-documented security and usability issues. Addressing these concerns, password managers and two-factor authentication (2FA) have emerged as key solutions. This article examines these methods with a focus on enhancing password security without compromising usability. We utilize an adapted Bonneau et al. (IEEE S&amp;P 2012) framework tailored to the specific challenges of password managers and 2FA. This allows us to categorize and evaluate prominent solutions from both academic research and industry practice, with a focus on their security, privacy, and usability. A crucial aspect of our study involves evaluating the effectiveness of a combined PM+2FA system in balancing security and usability. This study not only examines current trends but also suggests potential areas for future research, offering valuable insights to both users and developers in the evolving landscape of digital security.},
  archive      = {J_CSUR},
  doi          = {10.1145/3711117},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-32},
  shortjournal = {ACM Comput. Surv.},
  title        = {An in-depth analysis of password managers and two-factor authentication tools},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-centric artificial intelligence: A survey.
<em>CSUR</em>, <em>57</em>(5), 1–42. (<a
href="https://doi.org/10.1145/3711118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI . The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI .},
  archive      = {J_CSUR},
  doi          = {10.1145/3711118},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-42},
  shortjournal = {ACM Comput. Surv.},
  title        = {Data-centric artificial intelligence: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of heuristics for profile and wavefront reductions.
<em>CSUR</em>, <em>57</em>(5), 1–16. (<a
href="https://doi.org/10.1145/3711120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article surveys heuristic methods for profile and wavefront reductions. These graph layout problems represent a challenge for optimization methods and heuristics especially. The article presents the graph layout problems with their formal definition. It provides an ample perspective of techniques for designing heuristic methods for these graph layout problems but concentrates on the approaches and methodologies that yield high-quality solutions. Thus, this work references the most relevant studies in the associated literature and discusses the current state-of-the-art heuristics for these graph layout problems.},
  archive      = {J_CSUR},
  doi          = {10.1145/3711120},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-16},
  shortjournal = {ACM Comput. Surv.},
  title        = {A survey of heuristics for profile and wavefront reductions},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can graph neural networks be adequately explained? A survey.
<em>CSUR</em>, <em>57</em>(5), 1–36. (<a
href="https://doi.org/10.1145/3711122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the barrier caused by the black-box nature of Deep Learning (DL) for practical deployment, eXplainable Artificial Intelligence (XAI) has emerged and is developing rapidly. While significant progress has been made in explanation techniques for DL models targeted to images and texts, research on explaining DL models for graph data is still in its infancy. As Graph Neural Networks (GNNs) have shown superiority over various network analysis tasks, their explainability has also gained attention from both academia and industry. However, despite the increasing number of GNN explanation methods, there is currently neither a fine-grained taxonomy of them, nor a holistic set of evaluation criteria for quantitative and qualitative evaluation. To fill this gap, we conduct a comprehensive survey on existing explanation methods of GNNs in this article. Specifically, we propose a novel four-dimensional taxonomy of GNN explanation methods and summarize evaluation criteria in terms of correctness, robustness, usability, understandability, and computational complexity. Based on the taxonomy and criteria, we thoroughly review the recent advances in GNN explanation methods and analyze their pros and cons. In the end, we identify a series of open issues and put forward future research directions to facilitate XAI research in the field of GNNs.},
  archive      = {J_CSUR},
  doi          = {10.1145/3711122},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-36},
  shortjournal = {ACM Comput. Surv.},
  title        = {Can graph neural networks be adequately explained? a survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regulating information and network security: Review and
challenges. <em>CSUR</em>, <em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3711124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of internet activities in daily life has elevated cyberattacks to a significant global threat. As a result, protecting the networks and systems of industries, organizations, and individuals against cybercrimes has become an increasingly critical challenge. This monograph provides a comprehensive review and analysis of national, international, and industry regulations on cybercrimes. It presents empirical evidence of the effectiveness of these regulatory measures and their impacts at the national, organizational, and individual levels. We also examine the challenges posed by emerging technologies to these regulations. Finally, the monograph identifies limitations in the current regulatory framework and proposes future directions to enhance the cybersecurity ecosystem.},
  archive      = {J_CSUR},
  doi          = {10.1145/3711124},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {Regulating information and network security: Review and challenges},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jacm---10">JACM - 10</h2>
<ul>
<li><details>
<summary>
(2025). Integer programs with bounded subdeterminants and two
nonzeros per row. <em>JACM</em>, <em>72</em>(1), 1–50. (<a
href="https://doi.org/10.1145/3695985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a strongly polynomial-time algorithm for integer linear programs defined by integer coefficient matrices whose subdeterminants are bounded by a constant and that contain at most two nonzero entries in each row. The core of our approach is the first polynomial-time algorithm for the weighted stable set problem on graphs that do not contain more than k vertex-disjoint odd cycles, where k is any constant. Previously, polynomial-time algorithms were only known for k =0 (bipartite graphs) and for k =1. We observe that integer linear programs defined by coefficient matrices with bounded subdeterminants and two nonzeros per column can be also solved in strongly polynomial-time, using a reduction to b -matching.},
  archive      = {J_JACM},
  doi          = {10.1145/3695985},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-50},
  shortjournal = {J. ACM},
  title        = {Integer programs with bounded subdeterminants and two nonzeros per row},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subsampling suffices for adaptive data analysis.
<em>JACM</em>, <em>72</em>(1), 1–45. (<a
href="https://doi.org/10.1145/3698104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst’s query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of adaptive data analysis was formalized in the seminal works of Dwork et al. (STOC 2015) and Hardt and Ullman (FOCS 2014). We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: the only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work. In addition to its simplicity, we demonstrate the utility of this framework by designing mechanisms for two foundational tasks: statistical queries and median finding. In particular, our mechanism for answering the broadly applicable class of statistical queries is both extremely simple and state of the art in many parameter regimes.},
  archive      = {J_JACM},
  doi          = {10.1145/3698104},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-45},
  shortjournal = {J. ACM},
  title        = {Subsampling suffices for adaptive data analysis},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Orbit-finite linear programming. <em>JACM</em>,
<em>72</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3703909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An infinite set is orbit-finite if, up to permutations of atoms, it has only finitely many elements. We study a generalisation of linear programming where constraints are expressed by an orbit-finite system of linear inequalities. As our principal contribution we provide a decision procedure for checking if such a system has a real solution, and for computing the minimal/maximal value of a linear objective function over the solution set. We also show undecidability of these problems in case when only integer solutions are considered. Therefore orbit-finite linear programming is decidable, while orbit-finite integer linear programming is not.},
  archive      = {J_JACM},
  doi          = {10.1145/3703909},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-39},
  shortjournal = {J. ACM},
  title        = {Orbit-finite linear programming},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardness of approximate diameter: Now for undirected graphs.
<em>JACM</em>, <em>72</em>(1), 1–32. (<a
href="https://doi.org/10.1145/3704631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximating the graph diameter is a basic task of both theoretical and practical interest. A simple folklore algorithm can output a 2-approximation to the diameter in linear time by running BFS from an arbitrary vertex. It has been open whether a better approximation is possible in near-linear time. A series of articles on fine-grained complexity have led to strong hardness results for diameter in directed graphs, culminating in a recent tradeoff curve independently discovered by [Li, STOC’21] and [Dalirrooyfard and Wein, STOC’21], showing that under the Strong Exponential Time Hypothesis (SETH), for any integer k ≥ 2 and δ &gt; 0, a \(2-\frac{1}{k}-\delta\) approximation for diameter in directed m -edge graphs requires \(m^{1+1/(k-1)-o(1)}\) time. In particular, the simple linear time 2-approximation algorithm is optimal for directed graphs. In this article, we prove that the same tradeoff lower bound curve is possible for undirected graphs as well, extending results of [Roditty and Vassilevska W., STOC’13], [Li’20] and [Bonnet, ICALP’21] who proved the first few cases of the curve, k =2,3, and 4, respectively. Our result shows in particular that the simple linear time 2-approximation algorithm is conditionally optimal for undirected graphs. To obtain our result, we extract the core ideas in known reductions and introduce a unification and generalization that could be useful for proving SETH-based hardness for other problems in undirected graphs related to distance computation.},
  archive      = {J_JACM},
  doi          = {10.1145/3704631},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-32},
  shortjournal = {J. ACM},
  title        = {Hardness of approximate diameter: Now for undirected graphs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative coding and complexity theory of continuous
data: Part i: Motivation, definition, consequences. <em>JACM</em>,
<em>72</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3705609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When encoding real numbers as (necessarily infinite) bit-strings, the naïve binary/decimal expansion is well-known [ doi:10.1112/plms/s2-43.6.544 ] computably “ un reasonable”, rendering, for example, tripling qualitatively discontinuous on Cantor’s sequence space. Encoding reals as sequences of (finite integer numerators and denominators, in binary, of) rational approximations does make common operations qualitatively computable, yet admits no bounds on their computational complexity/quantitative continuity. Dyadic approximations, on the other hand, are known polynomially, and signed binary expansions even linearly, “reasonable” in a rigorous sense recalled in the introduction of this work. But how to distinguish between un/suitable encodings of spaces common in Calculus beyond the reals, such as Banach or Sobolev? With respect to qualitative computability/continuity on topological spaces, the technical condition of admissibility had been identified [ doi:10.1016/0304-3975(85)90208-7 ] for an encoding over Cantor space (historically called a representation ) to be “reasonable” [ doi:10.1007/978-3-030-59234-9_9 ] . Roughly speaking, admissibility requires the representation to be (i) continuous, and to be (ii) maximal with respect to continuous reduction. Admissible representations exist for a large class of spaces. And for (precisely) these does the Kreitz–Weihrauch—sometimes aka Main —Theorem of Computable Analysis hold, which characterizes continuity of functions by continuity of mappings translating codes, so-called realizers . We refine qualitative computability/continuity on topological spaces to quantitative continuity/complexity on metric spaces by proposing a notion, and investigating the properties, of polynomially/linearly admissible representations. Roughly speaking, these are (i) close to “optimally” continuous, namely linearly/polynomially relative to the space’s entropy, and they are (ii) maximal with respect to relative linear/polynomial quantitatively continuous reductions defined in the main text. Quantitatively admissible representations are closed under composition over generalized ground spaces beyond Cantor’s. Such representations exhibit a quantitative strengthening of the qualitative Main Theorem , namely now characterizing quantitative continuity of functions by quantitative continuity of realizers. A large class of compact metric spaces is shown to admit polynomially admissible representations over compact ultra metric spaces, and some even a generalization of the linearly admissible signed binary encoding. Quantitative admissibility thus provides the desired criterion for complexity-theoretically “reasonable” encodings.},
  archive      = {J_JACM},
  doi          = {10.1145/3705609},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-39},
  shortjournal = {J. ACM},
  title        = {Quantitative coding and complexity theory of continuous data: part i: motivation, definition, consequences},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correct and complete type checking and certified erasure for
coq, in coq. <em>JACM</em>, <em>72</em>(1), 1–74. (<a
href="https://doi.org/10.1145/3706056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coq is built around a well-delimited kernel that performs type checking for definitions in a variant of the Calculus of Inductive Constructions ( CIC ). Although the metatheory of CIC is very stable and reliable, the correctness of its implementation in Coq is less clear. Indeed, implementing an efficient type checker for CIC is a rather complex task, and many parts of the code rely on implicit invariants which can easily be broken by further evolution of the code. Therefore, on average, one critical bug has been found every year in Coq . This article presents the first implementation of a type checker for the kernel of Coq (without the module system, template polymorphism and η-conversion), which is proven sound and complete in Coq with respect to its formal specification. Note that because of Gödel’s second incompleteness theorem, there is no hope to prove completely the soundness of the specification of Coq inside Coq (in particular strong normalization), but it is possible to prove the correctness and completeness of the implementation assuming soundness of the specification, thus moving from a trusted code base (TCB) to a trusted theory base (TTB) paradigm. Our work is based on the MetaCoq project which provides meta-programming facilities to work with terms and declarations at the level of the kernel. We verify a relatively efficient type checker based on the specification of the typing relation of the Polymorphic, Cumulative Calculus of Inductive Constructions ( PCUIC ) at the basis of Coq . It is worth mentioning that during the verification process, we have found a source of incompleteness in Coq ’s official type checker, which has then been fixed in Coq 8.14 thanks to our work. In addition to the kernel implementation, another essential feature of Coq is the so-called extraction mechanism: the production of executable code in functional languages from Coq definitions. We present a verified version of this subtle type and proof erasure step, therefore enabling the verified extraction of a safe type checker for Coq in the future.},
  archive      = {J_JACM},
  doi          = {10.1145/3706056},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-74},
  shortjournal = {J. ACM},
  title        = {Correct and complete type checking and certified erasure for coq, in coq},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow-augmentation i: Directed graphs. <em>JACM</em>,
<em>72</em>(1), 1–38. (<a
href="https://doi.org/10.1145/3706103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show a flow-augmentation algorithm in directed graphs: There exists a randomized polynomial-time algorithm that, given a directed graph G , two vertices s, t ∈ V(G) , and an integer k , adds (randomly) to G a number of arcs such that for every minimal st -cut Z in G of size at most k , with probability 2 −poly( k ) the set Z becomes a minimum st -cut in the resulting graph. We also provide a deterministic counterpart of this procedure. The directed flow-augmentation tool allows us to prove fixed-parameter tractability of a number of problems parameterized by the cardinality of the deletion set whose parameterized complexity status was repeatedly posed as open problems: Chain SAT , defined by Chitnis, Egri, and Marx [ESA’13, Algorithmica’17], a number of weighted variants of classic directed cut problems, such as Weighted st - Cut or Weighted Directed Feedback Vertex Set . By proving that Chain SAT is FPT, we confirm a conjecture of Chitnis, Egri, and Marx that, for any graph H , if the List H - Coloring problem is polynomial-time solvable, then the corresponding vertex-deletion problem is fixed-parameter tractable. Chain SAT , defined by Chitnis, Egri, and Marx [ESA’13, Algorithmica’17], a number of weighted variants of classic directed cut problems, such as Weighted st - Cut or Weighted Directed Feedback Vertex Set .},
  archive      = {J_JACM},
  doi          = {10.1145/3706103},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-38},
  shortjournal = {J. ACM},
  title        = {Flow-augmentation i: Directed graphs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric embeddability of complexes is ∃ℝ-complete.
<em>JACM</em>, <em>72</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3707201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the decision problem of determining whether a given (abstract simplicial) k -complex has a geometric embedding in ℝ d is complete for the Existential Theory of the Reals for all d ≥ 3 and k ∈ { d -1, d } by reducing from pseudoline stretchability. Consequently, the problem is polynomial time equivalent to determining whether a polynomial equation system has a real solution. Moreover, this implies NP-hardness and constitutes the first hardness result for the algorithmic problem of geometrically embedding (abstract simplicial) complexes. This complements recent breakthroughs for the computational complexity of piece-wise linear embeddability [Matoušek, Sedgwick, Tancer, and Wagner, J. ACM 2018, and de Mesmay, Rieck, Sedgwick and Tancer, J. ACM 2020] and establishes connections to computational topology.},
  archive      = {J_JACM},
  doi          = {10.1145/3707201},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. ACM},
  title        = {Geometric embeddability of complexes is ∃ℝ-complete},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient quantum factoring algorithm. <em>JACM</em>,
<em>72</em>(1), 1–13. (<a
href="https://doi.org/10.1145/3708471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that n -bit integers can be factorized by independently running a quantum circuit with \(\tilde{O}(n^{3/2})\) gates for \(\sqrt {n}+4\) times, and then using polynomial-time classical post-processing. The correctness of the algorithm relies on a certain number-theoretic conjecture. It is currently not clear if the algorithm can lead to improved physical implementations in practice.},
  archive      = {J_JACM},
  doi          = {10.1145/3708471},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. ACM},
  title        = {An efficient quantum factoring algorithm},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallelize single-site dynamics up to dobrushin criterion.
<em>JACM</em>, <em>72</em>(1), 1–33. (<a
href="https://doi.org/10.1145/3708558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-site dynamics are canonical Markov chain based algorithms for sampling from high-dimensional distributions, such as the Gibbs distributions of graphical models. We introduce a simple and generic parallel algorithm that faithfully simulates single-site dynamics. Under a much relaxed, asymptotic variant of the ℓ p -Dobrushin’s condition—where the Dobrushin’s influence matrix has a bounded ℓ p -induced operator norm for an arbitrary p ∈ [1, ∞]—our algorithm simulates N steps of single-site updates within a parallel depth of O ( N / n +log n ) on Õ( m ) processors, where n is the number of sites and m is the size of the graphical model. For Boolean-valued random variables, if the ℓ p -Dobrushin’s condition holds—specifically, if the ℓ p -induced operator norm of the Dobrushin’s influence matrix is less than 1—the parallel depth can be further reduced to O (log N + log n ), achieving an exponential speedup. These results suggest that single-site dynamics with near-linear mixing times can be parallelized into RNC sampling algorithms, independent of the maximum degree of the underlying graphical model, as long as the Dobrushin influence matrix maintains a bounded operator norm. We show the effectiveness of this approach with RNC samplers for the hardcore and Ising models within their uniqueness regimes, as well as an RNC SAT sampler for satisfying solutions of conjunctive normal form formulas in a local lemma regime. Furthermore, by employing non-adaptive simulated annealing, these RNC samplers can be transformed into RNC algorithms for approximate counting.},
  archive      = {J_JACM},
  doi          = {10.1145/3708558},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {J. ACM},
  title        = {Parallelize single-site dynamics up to dobrushin criterion},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tap---1">TAP - 1</h2>
<ul>
<li><details>
<summary>
(2025). Personality expression using co-speech gesture.
<em>TAP</em>, <em>22</em>(2), 1–20. (<a
href="https://doi.org/10.1145/3694905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We express our personality through verbal and nonverbal behavior. While verbal cues are mostly related to the semantics of what we say, nonverbal cues include our posture, gestures, and facial expressions. Appropriate expression of these behavioral elements improves conversational virtual agents’ communication capabilities and realism. Although previous studies focus on co-speech gesture generation, they do not consider the personality aspect of the synthesized animations. We show that automatically generated co-speech gestures naturally express personality traits, and heuristics-based adjustments for such animations can further improve personality expression. To this end, we present a framework for enhancing co-speech gestures with the different personalities of the Five-Factor model. Our experiments suggest that users perceive increased realism and improved personality expression when combining heuristics-based motion adjustments with co-speech gestures.},
  archive      = {J_TAP},
  doi          = {10.1145/3694905},
  journal      = {ACM Transactions on Applied Perception},
  month        = {2},
  number       = {2},
  pages        = {1-20},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Personality expression using co-speech gesture},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tist---6">TIST - 6</h2>
<ul>
<li><details>
<summary>
(2025). Fairness and diversity in recommender systems: A survey.
<em>TIST</em>, <em>16</em>(1), 1–28. (<a
href="https://doi.org/10.1145/3664928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RS) are effective tools for mitigating information overload and have seen extensive applications across various domains. However, the single focus on utility goals proves to be inadequate in addressing real-world concerns, leading to increasing attention to fairness-aware and diversity-aware RS. While most existing studies explore fairness and diversity independently, we identify strong connections between these two domains. In this survey, we first discuss each of them individually and then dive into their connections. Additionally, motivated by the concepts of user-level and item-level fairness, we broaden the understanding of diversity to encompass not only the item level but also the user level. With this expanded perspective on user and item-level diversity, we re-interpret fairness studies from the viewpoint of diversity. This fresh perspective enhances our understanding of fairness-related work and paves the way for potential future research directions. Articles discussed in this survey along with public code links are available at: https://github.com/YuyingZhao/Awesome-Fairness-and-Diversity-Papers-in-Recommender-Systems},
  archive      = {J_TIST},
  doi          = {10.1145/3664928},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Fairness and diversity in recommender systems: A survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Misinformation resilient search rankings with webgraph-based
interventions. <em>TIST</em>, <em>16</em>(1), 1–27. (<a
href="https://doi.org/10.1145/3670410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of unreliable news domains on the internet has had wide-reaching negative impacts on society. We introduce and evaluate interventions aimed at reducing traffic to unreliable news domains from search engines while maintaining traffic to reliable domains. We build these interventions on the principles of fairness (penalize sites for what is in their control), generality (label/fact-check agnostic), targeted (increase the cost of adversarial behavior), and scalability (works at webscale). We refine our methods on small-scale webdata as a testbed and then generalize the interventions to a large-scale webgraph containing 93.9M domains and 1.6B edges. We demonstrate that our methods penalize unreliable domains far more than reliable domains in both settings and we explore multiple avenues to mitigate unintended effects on both the small-scale and large-scale webgraph experiments. These results indicate the potential of our approach to reduce the spread of misinformation and foster a more reliable online information ecosystem. This research contributes to the development of targeted strategies to enhance the trustworthiness and quality of search engine results, ultimately benefiting users, and the broader digital community.},
  archive      = {J_TIST},
  doi          = {10.1145/3670410},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Misinformation resilient search rankings with webgraph-based interventions},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommender system-induced eating disorder relapse: Harmful
content and the challenges of responsible recommendation. <em>TIST</em>,
<em>16</em>(1), 1–13. (<a
href="https://doi.org/10.1145/3675404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As users’ social media feeds have become increasingly driven by algorithmically recommended content, there is a need to understand the impact these recommendations have on users. People in recovery from eating disorders (ED) may try to avoid content that features severely underweight bodies or that encourages disordered eating. However, if recommender systems show them this type of content anyway, it may impact their recovery or even lead to relapse. In this study, we take a two-pronged approach to understanding the intersection of recommender systems, ED content, and users in recovery. We performed a content analysis of tweets about recommended ED content and conducted a small-scale study on Pinterest to show that ED content is recommended in response to interaction with posts about ED recovery. We discuss the implications for responsible recommendation and harm prevention.},
  archive      = {J_TIST},
  doi          = {10.1145/3675404},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Recommender system-induced eating disorder relapse: Harmful content and the challenges of responsible recommendation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AVENUE: A novel deepfake detection method based on temporal
convolutional network and rPPG information. <em>TIST</em>,
<em>16</em>(1), 1–16. (<a
href="https://doi.org/10.1145/3702232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Deep Learning (DL), an adversary creates Deepfakes by manipulating facial features to fool someone. The Deepfakes pose a security threat to anyone’s privacy and a primary concern for our society. It can be detected by utilizing the texture and physiological properties of the face, like eye and lip movements; however, such methods are incompetent when Deepfakes are created using recent Generative Adversarial Networks (GAN). Alternatively, Remote Photoplethysmography (rPPG) information can be used for Deepfake detection because GANs neglect human physiological information for Deepfake generation. Such detection can be inaccurate when rPPG signals are affected by the noises induced by facial deformation and illumination variations. Furthermore, the exiting Deepfake detections are usually performed using sequential models, and such models fail to process the long sequence of temporal information. These issues are mitigated by our proposed method AVENUE , that is, \(A\) no \(V\) el d \(E\) epfake detectio \(N\) method based on temporal convol \(U\) tion n \(E\) twork and rPPG information. For mitigating the noise issues in the rPPG signals, the proposed method detects and employs relatively stable clips of the input video for Deepfake detection. The stable clips are those clips that are least affected by facial deformations. Also, we use a modified Temporal convolutional network to model the long sequence of Deepfake information rather than the sequential architectures. We performed the experimental result on publicly available datasets of Deepfake videos. It demonstrates that our proposed method performs better than the existing rPPG-based Deepfake detection methods.},
  archive      = {J_TIST},
  doi          = {10.1145/3702232},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {AVENUE: A novel deepfake detection method based on temporal convolutional network and rPPG information},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BeGin: Extensive benchmark scenarios and an easy-to-use
framework for graph continual learning. <em>TIST</em>, <em>16</em>(1),
1–22. (<a href="https://doi.org/10.1145/3702648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual Learning (CL) is the process of learning ceaselessly a sequence of tasks. Most existing CL methods deal with independent data (e.g., images and text) for which many benchmark frameworks and results under standard experimental settings are available. Compared to them, however, CL methods for graph data (graph CL) are relatively underexplored because of (a) the lack of standard experimental settings, especially regarding how to deal with the dependency between instances, (b) the lack of benchmark datasets and scenarios, and (c) high complexity in implementation and evaluation due to the dependency. In this paper, regarding (a) we define four standard incremental settings (task-, class-, domain-, and time-incremental) for node-, link-, and graph-level problems, extending the previously explored scope. Regarding (b), we provide 35 benchmark scenarios based on 24 real-world graphs. Regarding (c), we develop BeGin , an easy and fool-proof framework for graph CL. BeGin is easily extended since it is modularized with reusable modules for data processing, algorithm design, and evaluation. Especially, the evaluation module is completely separated from user code to eliminate potential mistakes. Regarding benchmark results, we cover \(3\times\) more combinations of incremental settings and levels of problems than the latest benchmark. All assets for the benchmark framework are publicly available at https://github.com/ShinhwanKang/BeGin .},
  archive      = {J_TIST},
  doi          = {10.1145/3702648},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {BeGin: Extensive benchmark scenarios and an easy-to-use framework for graph continual learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain HAR: Few-shot transfer learning for human
activity recognition. <em>TIST</em>, <em>16</em>(1), 1–35. (<a
href="https://doi.org/10.1145/3704921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquitous availability of smartphones and smartwatches with integrated inertial measurement units (IMUs) enables straightforward capturing of human activities through collecting movement data. For specific applications of sensor-based human activity recognition (HAR), however, logistical challenges and burgeoning costs render especially the ground-truth annotation of such data a difficult endeavor, resulting in limited scale and diversity of datasets available for deriving effective HAR systems and less than ideal recognition capabilities. Transfer learning, i.e., leveraging publicly available labeled datasets to first learn useful representations that can then be fine-tuned using limited amounts of labeled data from a target domain, can alleviate some of the performance issues of contemporary HAR systems. Yet they can fail when the differences between source and target conditions are too large and/or only few samples from a target application domain are available—each of which are typical challenges in real-world human activity recognition scenarios. In this article, we present an approach for economic use of publicly available labeled HAR datasets for effective transfer learning. We introduce a novel transfer learning framework—Cross-Domain HAR—which follows the teacher-student self-training paradigm to more effectively recognize activities with very limited label information. It bridges conceptual gaps between source and target domains, including sensor locations and type of activities. Cross-Domain HAR enables substantial performance improvements over the state-of-the-art in sensor-based HAR scenarios. Through our extensive experimental evaluation on a range of benchmark datasets we specifically demonstrate the effectiveness of our approach for practically relevant few-shot activity recognition scenarios. We also present a detailed analysis into how the individual components of our framework affect downstream performance and provide practical suggestions for using the framework in real-world applications.},
  archive      = {J_TIST},
  doi          = {10.1145/3704921},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {2},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Cross-domain HAR: Few-shot transfer learning for human activity recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tkdd---3">TKDD - 3</h2>
<ul>
<li><details>
<summary>
(2025). STORM: A MapReduce framework for symbolic time intervals
series classification. <em>TKDD</em>, <em>19</em>(1), 1–54. (<a
href="https://doi.org/10.1145/3694788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic Time Intervals (STIs) represent events having a non-zero time duration, which are common in various application domains. In this article, we focus on the challenge of STIs series classification (STIC). While in the related problem of time series classification (TSC) Rocket is well-known for its exceptionally fast runtime while achieving accuracy comparable to state-of-the-art, it has only recently been studied in the field of STIC. However, since Rocket as well as its enhanced variants for TSC (e.g., MiniRocket and MultiRocket) solely rely on global features, they might not always fit best for the classification of thousands of time-units long STI series out-of-the-box, which are rather common in STIC. We introduce STORM—a novel, generic MapReduce framework for STIC, which (1) converts raw input STIs series into multivariate time series (MTS) representation; (2) partitions the converted MTS into fixed-sized blocks, each transformed independently into a uniform latent space via a common, desired Rocket variant used as a base transformation in STORM; and (3) performs sequence classification of the blocks’ transformed feature vectors via a deep, lightweight, bidirectional LSTM network. The evaluation demonstrates that STORM significantly improves accuracy over eight state-of-the-art methods for STIC either when applied with MiniRocket and MultiRocket as base transformations, as well as over the baselines of applying the respective Rocket variants directly to the converted MTS representation, that is, while also reporting overall comparable training times, on a benchmark of eight real-world STIC datasets including both extremely long and short STIs series.},
  archive      = {J_TKDD},
  doi          = {10.1145/3694788},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-54},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {STORM: A MapReduce framework for symbolic time intervals series classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on the role of crowds in combating online
misinformation: Annotators, evaluators, and creators. <em>TKDD</em>,
<em>19</em>(1), 1–30. (<a
href="https://doi.org/10.1145/3694980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online misinformation poses a global risk with significant real-world consequences. To combat misinformation, current research relies on professionals like journalists and fact-checkers for annotating and debunking false information while also developing automated machine learning methods for detecting misinformation. Complementary to these approaches, recent research has increasingly concentrated on utilizing the power of ordinary social media users, a.k.a. “the crowd,” who act as eyes-on-the-ground proactively questioning and countering misinformation. Notably, recent studies show that 96% of counter-misinformation responses originate from them. Acknowledging their prominent role, we present the first systematic and comprehensive survey of research papers that actively leverage the crowds to combat misinformation. In this survey, we first identify 88 papers related to crowd-based efforts, 1 following a meticulous annotation process adhering to the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). We then present key statistics related to misinformation, counter-misinformation, and crowd input in different formats and topics. Upon holistic analysis of the papers, we introduce a novel taxonomy of the roles played by the crowds in combating misinformation: (i) crowds as annotators who actively identify misinformation; (ii) crowds as evaluators who assess counter-misinformation effectiveness; (iii) crowds as creators who create counter-misinformation. This taxonomy explores the crowd’s capabilities in misinformation detection, identifies the prerequisites for effective counter-misinformation, and analyzes crowd-generated counter-misinformation. In each assigned role, we conduct a detailed analysis to categorize the specific utilization of the crowd. Particularly, we delve into (i) distinguishing individual, collaborative, and machine-assisted labeling for annotators; (ii) analyzing the effectiveness of counter-misinformation through surveys, interviews, and in-lab experiments for evaluators; and (iii) characterizing creation patterns and creator profiles for creators. Finally, we conclude this survey by outlining potential avenues for future research in this field.},
  archive      = {J_TKDD},
  doi          = {10.1145/3694980},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey on the role of crowds in combating online misinformation: Annotators, evaluators, and creators},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplifying distributed neural network training on massive
graphs: Randomized partitions improve model aggregation. <em>TKDD</em>,
<em>19</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3701563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed graph neural network (GNN) training facilitates learning on massive graphs that surpass the storage and computational capabilities of a single machine. Traditional distributed frameworks strive for performance parity with centralized training by maximally recovering cross-instance node dependencies, relying either on inter-instance communication or periodic fallback to centralized training. However, these processes create overhead and constrain the scalability of the framework. In this work, we propose a streamlined framework for distributed GNN training that eliminates these costly operations, yielding improved scalability, convergence speed, and performance over state-of-the-art approaches. Our framework (1) comprises independent trainers that asynchronously learn local models from locally available parts of the training graph and (2) synchronizes these local models only through periodic (time-based) model aggregation. Contrary to prevailing belief, our theoretical analysis shows that it is not essential to maximize the recovery of cross-instance node dependencies to achieve performance parity with centralized training. Instead, our framework leverages randomized assignment of nodes or super-nodes (i.e., collections of original nodes) to partition the training graph in order to enhance data uniformity and minimize discrepancies in gradient and loss function across instances. Experiments on social and e-commerce networks with up to 1.3 billion edges show that our proposed framework achieves state-of-the-art performance and 2.31 \(\times\) speedup compared to the fastest baseline despite using less training data.},
  archive      = {J_TKDD},
  doi          = {10.1145/3701563},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Simplifying distributed neural network training on massive graphs: Randomized partitions improve model aggregation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="todaes---10">TODAES - 10</h2>
<ul>
<li><details>
<summary>
(2025). HEANA: A hybrid time-amplitude analog optical accelerator
with flexible dataflows for energy-efficient CNN inference.
<em>TODAES</em>, <em>30</em>(2), 1–37. (<a
href="https://doi.org/10.1145/3711845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several photonic microring resonator (MRR)-based analog accelerators have been proposed to accelerate the inference of integer-quantized Convolutional Neural Networks (CNNs) with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (1) severe hampering of wavelength parallelism due to various crosstalk effects, (2) inflexibility of supporting various dataflows with temporal accumulations, and (3) failure in fully leveraging the ability of photodetectors to perform in situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel H ybrid tim E - A mplitude a N alog optical A ccelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical modulators (TAOMs) in a spectrally hitless arrangement, which significantly reduces optical signal losses and crosstalk effects, thereby increasing the wavelength parallelism in HEANA. HEANA employs our invented balanced photo-charge accumulators (BPCAs) that enable buffer-less, in situ, spatio-temporal accumulations to eliminate the need to use reduction networks in HEANA, relieving it from related latency and energy overheads. Moreover, TAOMs and BPCAs increase the flexibility of HEANA to efficiently support spatio-temporal accumulations for various dataflows. Our evaluation for the inference of four modern CNNs indicates that HEANA provides improvements of at least 25× and 32× in frames per second (FPS) and FPS/W (energy efficiency), respectively, for equal-area comparisons on gmean over two MRR-based analog CNN accelerators from prior work.},
  archive      = {J_TODAES},
  doi          = {10.1145/3711845},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-37},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {HEANA: A hybrid time-amplitude analog optical accelerator with flexible dataflows for energy-efficient CNN inference},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Algorithm-hardware co-design for accelerating depthwise
separable CNNs. <em>TODAES</em>, <em>30</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3711846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depthwise separable convolution (DSC) is a popular method for constructing lightweight neural networks. However, the pointwise convolution (PWC) has a much larger number of parameters than the depthwise convolution (DWC), causing the imbalanced parameter ratio of PWC to DWC. In this article, we propose an efficient and hardware-efficiency convolution (Shared Kernel sliding on channel Convolution, SKC) to replace the redundant PWC in DSC for a balanced parameter ratio, where SKC customizes the sharing kernel in the channel dimension to reduce the number of parameters, and the local connection in the channel dimension reduces the computation. Furthermore, the proposed SKC is suitable for Winograd acceleration, and the large kernel decomposition method is introduced to facilitate its use. We implement the first Winograd-based FPGA hardware accelerator for DSCNets. The shared 1D and 2D Winograd convolution computing engine is proposed to compute the proposed DSC consisting of DWC and SKC efficiently. An alternating loading and reusing storage approach is developed to efficiently load SKC input feature maps. Experimental results show our DSC-based accelerator can achieve 20× higher power efficiency at the cost of a small loss of accuracy by algorithm-hardware co-design compared with traditional accelerators.},
  archive      = {J_TODAES},
  doi          = {10.1145/3711846},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Algorithm-hardware co-design for accelerating depthwise separable CNNs},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic hardware pragma insertion in high-level synthesis:
A non-linear programming approach. <em>TODAES</em>, <em>30</em>(2),
1–44. (<a href="https://doi.org/10.1145/3711847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-Level Synthesis enables the rapid prototyping of hardware accelerators, by combining a high-level description of the functional behavior of a kernel with a set of micro-architecture optimizations as inputs. Such optimizations can be described by inserting pragmas e.g., pipelining and replication of units, or even higher level transformations for HLS such as automatic data caching using the AMD/Xilinx Merlin compiler. Selecting the best combination of pragmas, even within a restricted set, remains particularly challenging and the typical state-of-practice uses design-space exploration to navigate this space. But due to the highly irregular performance distribution of pragma configurations, typical DSE approaches are either extremely time consuming, or operating on a severely restricted search space. This work proposes a framework to automatically insert HLS pragmas in regular loop-based programs, supporting pipelining, unit replication, and data caching. We develop an analytical performance and resource model as a function of the input program properties and pragmas inserted, using non-linear constraints and objectives. We prove this model provides a lower bound on the actual performance after HLS. We then encode this model as a Non-Linear Program, by making the pragma configuration unknowns of the system, which is computed optimally by solving this NLP. This approach can also be used during DSE, to quickly prune points with a (possibly partial) pragma configuration, driven by lower bounds on achievable latency. We extensively evaluate our end-to-end, fully implemented system, showing it can effectively manipulate spaces of billions of designs in seconds to minutes for the kernels evaluated.},
  archive      = {J_TODAES},
  doi          = {10.1145/3711847},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-44},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Automatic hardware pragma insertion in high-level synthesis: A non-linear programming approach},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LOGIC: Logic synthesis for digital in-memory computing.
<em>TODAES</em>, <em>30</em>(2), 1–27. (<a
href="https://doi.org/10.1145/3711848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory processing offers a promising solution for enhancing the performance of data-intensive applications. While analog in-memory computing demonstrates remarkable efficiency, its limited precision is suitable only for approximate computing tasks. In contrast, digital in-memory computing delivers the deterministic precision necessary to accelerate high-assurance applications. Current digital in-memory computing methods typically involve manually breaking down arithmetic operations into in-memory compute kernels. In contrast, traditional digital circuits are synthesized through intricate and automated design workflows. In this article, we introduce a logic synthesis framework called LOGIC, which facilitates the translation of high-level applications into digital in-memory compute kernels that can be executed using non-volatile memory. We propose techniques for decomposing element-wise arithmetic operations into in-memory kernels while minimizing the number of in-memory operations. Additionally, we optimize the sequence of in-memory operations to reduce non-volatile memory utilization. To address the NP-hard execution sequencing optimization problem, we have developed two look-ahead algorithms that offer practical solutions. Additionally, we leverage data layout reorganization to efficiently accelerate applications that heavily rely on sparse matrix-vector multiplication operations. Our experimental evaluations demonstrate that our proposed synthesis approach improves the area and latency of fixed-point multiplication by 84% and 20% compared to the state-of-the-art, respectively. Moreover, when applied to scientific computing applications sourced from the SuiteSparse Matrix Collection, our design achieves remarkable improvements in area, latency, and energy efficiency by factors of 4.8×, 2.6×, and 11×, respectively.},
  archive      = {J_TODAES},
  doi          = {10.1145/3711848},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {LOGIC: Logic synthesis for digital in-memory computing},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-preemptive scheduling of periodic tasks with data
dependencies in heterogeneous multiprocessor embedded systems.
<em>TODAES</em>, <em>30</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3711849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous multiprocessor architecture is frequently employed as an economical and efficient means of providing excellent parallel processing capabilities while keeping production cost and power consumption under control. Although this architecture achieves significant performance enhancement and cost reduction, it results in a serious task allocation and scheduling problem, especially for periodic tasks with data dependencies, all of which should be reasonably scheduled and executed in a timely manner such that their deadlines and dependence requirements could be satisfied even if the worst happens. In this article, we concentrate on the non-preemptive scheduling problem of periodic tasks with data dependencies upon heterogeneous multiprocessor platforms. First, with models of data-dependent tasks and heterogeneous processors, we analyze the time, space, precedence, and data dependence constraints of tasks and design an exact formulation based on the mixed integer linear programming to completely explore the solution space and produce the optimal solutions. Then, by constructing a directed acyclic graph to depict the dependence relationship of jobs generated by tasks, we propose an efficient off-line list-based scheduling algorithm to provide a reasonable time and processor allocation for each job, with a view to minimizing the completion time of jobs. Experiments with randomly generated tasks are performed to evaluate the effectiveness and efficiency of the proposed algorithm, and the experimental results show that our algorithm can averagely enhance the scheduling success ratio by 28.5%, and, respectively, reduce the task completion time and the deviation ratio by 23.3% and 17.2%, on average.},
  archive      = {J_TODAES},
  doi          = {10.1145/3711849},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Non-preemptive scheduling of periodic tasks with data dependencies in heterogeneous multiprocessor embedded systems},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-row guiding template design for lamellar directed
self-assembly with self-aligned via process. <em>TODAES</em>,
<em>30</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3711851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed self-assembly (DSA) of block copolymers can generate tiny and dense layout features, holding great potential for patterning vias and contacts at advanced nodes. Existing studies mainly focused on guiding template design for cylindrical DSA, but by leveraging self-aligned via process, lamellar DSA can form vias to be immune to placement errors and free of a uniform pitch between vias, which cylindrical DSA suffers from. The state-of-the-art guiding template design for lamellar DSA can handle only single-row templates, thus limiting the flexibility of via grouping. Therefore, in this article, we explore further and propose a novel and general multi-row guiding template design approach. Experimental results show that our approach outperforms the state-of-the-art work on both mask conflicts and short guiding templates, and requires much less computation time.},
  archive      = {J_TODAES},
  doi          = {10.1145/3711851},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Multi-row guiding template design for lamellar directed self-assembly with self-aligned via process},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPTA 2.0: Enhanced scalable parallel track assignment
algorithm with two-stage partition considering timing delay.
<em>TODAES</em>, <em>30</em>(2), 1–23. (<a
href="https://doi.org/10.1145/3712009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routability has always been a significant challenge in Very Large Scale Integration (VLSI) design. To overcome the potential mismatch between the global routing results and the detailed routing requirements, track assignment is introduced to achieve an efficient routability estimation. Moreover, with the increasing scale of circuits, the intricate interconnections among the components on the chip lead to increased timing delay in signal transmission, thereby significantly impacting the performance and reliability of the circuit. Thus, to further improve the routability of the circuit, it is also critical to realize an accurate estimation of the timing delay within the track assignment stage. Existing heuristic track assignment algorithms, however, are prone to local optimality, and thus fail to provide accurate routability estimations. In this article, we propose an enhanced scalable parallel track assignment algorithm called SPTA 2.0 for VLSI design, employing a two-stage partition strategy and considering timing delay. First, the proposed algorithm achieves efficient assignment of all wires by considering the routing information from both the global and local nets. Second, the overlap cost, the blockage cost, and the wirelength cost can be minimized to significantly improve the routability. Third, a critical wire controlling strategy is proposed to optimize signal timing delays inside nets. Finally, a two-stage partition strategy and a panel-subpanel-level parallelism are designed to further reduce the runtime, improving the scalability of the proposed methodology. Experimental results on multiple benchmarks demonstrate that the proposed method provides better routability estimations, and leads to superior track assignment solutions compared with existing algorithms.},
  archive      = {J_TODAES},
  doi          = {10.1145/3712009},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {SPTA 2.0: Enhanced scalable parallel track assignment algorithm with two-stage partition considering timing delay},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient area and reliability optimization method for
MPRM circuits based on high-dimensional genetic algorithm.
<em>TODAES</em>, <em>30</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3712591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Area and reliability optimization have become the primary constraints in circuits logic synthesis. To address the increasing area and transient fault susceptibility in combinational circuits, we propose a high-dimensional genetic algorithm (HGA). HGA adopts an evolutionary scheme based on ternary tree, and uses adaptive crossover operator and flight operator to jump out of local optimum. Moreover, based on the HGA, we propose an area and reliability optimization method (AROM) for mixed polarity Reed-Muller logic circuits, which searches the best polarity with minimum area and soft error rate. The experimental results confirm that AROM can search for more desirable nondominated solutions in less time compared to existing optimization methods, and can be used as an effective electronic design automation tool for multi-objective optimization.},
  archive      = {J_TODAES},
  doi          = {10.1145/3712591},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {An efficient area and reliability optimization method for MPRM circuits based on high-dimensional genetic algorithm},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling cross-checking opportunities in verilog compilers.
<em>TODAES</em>, <em>30</em>(2), 1–23. (<a
href="https://doi.org/10.1145/3715325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The landscape of Verilog toolchains for electronic design automation (EDA) is diverse, and their reliability is crucial, as errors can lead to significant debugging challenges and delays in development. Methodologies such as testing and formal verification have been applied to identify and eliminate defects in these toolchains. We propose a framework named VeriXmith to interconnect design tools involved in logical synthesis and simulation for cross-checking. These tools process circuit designs and produce outputs in different languages, such as Verilog netlists from synthesizers and C++ programs from simulators. Since these outputs represent the same circuit semantics, we can leverage this semantic consistency to verify the tools that translate one representation into another. Our approach involves creating semantics extractors to extend the range of circuit representations available for semantic equivalence checking by converting them into a canonical and comparable form. Additionally, we develop mutation operators for Verilog designs to introduce new data/control paths and language constructs, enhancing the diversity of circuit designs as test inputs. By validating semantic equivalence, our framework successfully identifies defects in existing Verilog toolchains. An exploratory experiment uncovers 31 previously unknown bugs in well-known open-source Verilog tools, including Verilator and Yosys.},
  archive      = {J_TODAES},
  doi          = {10.1145/3715325},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Unveiling cross-checking opportunities in verilog compilers},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective fault effects evaluation for permanent faults in
GPUs executing DNNs. <em>TODAES</em>, <em>30</em>(2), 1–33. (<a
href="https://doi.org/10.1145/3715327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have permeated multiple applications, including cutting-edge safety-critical domains, which require relevant computational power, often provided by Graphic Processing Units (GPUs). GPUs are manufactured with advanced semiconductor technologies that can be affected by faults during the operational phase (e.g., due to wear-out, aging, or environmental harshness), whose effects possibly reach the DNN outputs, in some cases leading to catastrophic consequences. Hence, hardware-aware reliability assessments of DNNs are crucial to be considered in the context of safety-critical systems (following regulations/standards of specific application domains). Application-level fault injection (FI) techniques (i.e., DNN parameter corruption) are often adopted for the reliability evaluation of DNNs; unfortunately, these approaches hardly represent fault effects from GPU hardware. This work proposes an FI strategy based on Hardware-Injection-Through-Program-Transformation (HITPT) to mimic the effect of permanent faults (PFs) at the GPU instruction level, enabling effective assessment of PFs on DNN’s reliability. Our approach provides a good trade-off between the fault effect evaluation’s accuracy and the required computational time. Using the proposed approach, for the first time, we systematically assessed the effects of PF in GPUs executing some DNN sample cases. The results indicate that the faults injected closer to the hardware, using our evaluation strategy, can produce a higher accuracy degradation than the evaluations performed by the typical application-level FI that modify only the DNN parameters. Furthermore, the proposed FI methodology provides insightful results to identify the most suitable fault-tolerance solutions (e.g., selective hardening or design diversity) for their application at thread levels inside GPU’s kernels.},
  archive      = {J_TODAES},
  doi          = {10.1145/3715327},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {2},
  number       = {2},
  pages        = {1-33},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Effective fault effects evaluation for permanent faults in GPUs executing DNNs},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tosem---3">TOSEM - 3</h2>
<ul>
<li><details>
<summary>
(2025). Don’t complete it! Preventing unhelpful code completion for
productive and sustainable neural code completion systems.
<em>TOSEM</em>, <em>34</em>(1), 1–22. (<a
href="https://doi.org/10.1145/3688831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, large pre-trained language models are widely applied in neural code completion systems. Though large code models significantly outperform their smaller counterparts, around 70% of displayed code completions from Github Copilot are not accepted by developers. Being reviewed but not accepted, their help to developer productivity is considerably limited and may conversely aggravate the workload of developers, as the code completions are automatically and actively generated in state-of-the-art code completion systems as developers type out once the service is enabled. Even worse, considering the high cost of the large code models, it is a huge waste of computing resources and energy, which severely goes against the sustainable development principle of AI technologies. However, such waste has never been realized, not to mention effectively addressed, in the research community for neural code completion. Hence, preventing such unhelpful code completions from happening in a cost-friendly way is of urgent need. To fill this significant gap, we first investigate the prompts of unhelpful code completions, called “low-return prompts.” We empirically identify four observable patterns in low-return prompts, each lacking necessary information, making it difficult to address through enhancements to the model’s accuracy alone. This demonstrates the feasibility of identifying such low-return prompts based on the prompts themselves. Motivated by this finding, we propose an early-rejection mechanism to turn down low-return prompts by foretelling the code completion qualities. The prompts that are estimated to receive unhelpful code completions will not be sent to the model. Furthermore, we investigated five types of estimators to demonstrate the feasibility of the mechanism. The experimental results show that the estimator can reject 20% of code completion requests with a 97.4% precision. To the best of our knowledge, it is the first systemic approach to address the problem of unhelpful code completions and this work also sheds light on an important research direction of large code models.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688831},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Don’t complete it! preventing unhelpful code completion for productive and sustainable neural code completion systems},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). History-driven fuzzing for deep learning libraries.
<em>TOSEM</em>, <em>34</em>(1), 1–29. (<a
href="https://doi.org/10.1145/3688838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many Deep Learning (DL) fuzzers have been proposed for API-level testing of DL libraries. However, they either perform unguided input generation (e.g., not considering the relationship between API arguments when generating inputs) or only support a limited set of corner-case test inputs. Furthermore, many developer APIs crucial for library development remain untested, as they are typically not well documented and lack clear usage guidelines, unlike end-user APIs. This makes them a more challenging target for automated testing. To fill this gap, we propose a novel fuzzer named Orion, which combines guided test input generation and corner-case test input generation based on a set of fuzzing heuristic rules constructed from historical data known to trigger critical issues in the underlying implementation of DL APIs. To extract the fuzzing heuristic rules, we first conduct an empirical study on the root cause analysis of 376 vulnerabilities in two of the most popular DL libraries, PyTorch and TensorFlow. We then construct the fuzzing heuristic rules based on the root causes of the extracted historical vulnerabilities. Using these fuzzing heuristic rules, Orion generates corner-case test inputs for API-level fuzzing. In addition, we extend the seed collection of existing studies to include test inputs for developer APIs. Our evaluation shows that Orion reports 135 vulnerabilities in the latest releases of TensorFlow and PyTorch, 76 of which were confirmed by the library developers. Among the 76 confirmed vulnerabilities, 69 were previously unknown, and 7 have already been fixed. The rest are awaiting further confirmation. For end-user APIs, Orion detected 45.58% and 90% more vulnerabilities in TensorFlow and PyTorch, respectively, compared to the state-of-the-art conventional fuzzer, DeepRel. When compared to the state-of-the-art LLM-based DL fuzzer, AtlasFuz, and Orion detected 13.63% more vulnerabilities in TensorFlow and 18.42% more vulnerabilities in PyTorch. Regarding developer APIs, Orion stands out by detecting 117% more vulnerabilities in TensorFlow and 100% more vulnerabilities in PyTorch compared to the most relevant fuzzer designed for developer APIs, such as FreeFuzz.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3688838},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {1},
  pages        = {1-29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {History-driven fuzzing for deep learning libraries},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatically recommend code updates: Are we there yet?
<em>TOSEM</em>, <em>33</em>(8), 1–27. (<a
href="https://doi.org/10.1145/3678167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large pre-trained Language Models of Code (CodeLMs) have shown promising results on various software engineering tasks. One such task is automatic code update recommendation, which transforms outdated code snippets into their approved and revised counterparts. Although many CodeLM-based approaches have been proposed, claiming high accuracy, their effectiveness and reliability on real-world code update tasks remain questionable. In this article, we present the first extensive evaluation of state-of-the-art CodeLMs for automatically recommending code updates. We assess their performance on two diverse datasets of paired updated methods, considering factors such as temporal evolution, project specificity, method size, and update complexity. Our results reveal that while CodeLMs exhibit higher performance in settings that ignore temporal information, they struggle in more realistic time-wise scenarios and generalize poorly to new projects. Furthermore, CodeLM performance decreases significantly for larger methods and more complex updates. Furthermore, we observe that many CodeLM-generated “updates” are actually null, especially in time-wise settings, and meaningful edits remain challenging. Our findings highlight the significant gap between the perceived and actual effectiveness of CodeLMs for real-world code update recommendation and emphasize the need for more research on improving their practicality, robustness, and generalizability.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3678167},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {2},
  number       = {8},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automatically recommend code updates: Are we there yet?},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
