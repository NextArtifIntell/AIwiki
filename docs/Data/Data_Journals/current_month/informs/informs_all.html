<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>informs_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="informs">INFORMS</h1>
<h2 id="deca---6">DECA - 6</h2>
<ul>
<li><details>
<summary>
(2025). Appreciation to referees, 2024. <em>DECA</em>,
<em>22</em>(1), 87–88. (<a
href="https://doi.org/10.1287/deca.2025.thanks.v22.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vicki Bier, the Editor-in-Chief of Decision Analysis , thanks the referees who generously provide expert counsel and guidance on a voluntary basis. Without them, the journal could not function. The following list acknowledges those individuals who acted as referees for papers considered from September 2023 to September 2024.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2025.thanks.v22.n1},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {87-88},
  shortjournal = {Decis. Anal.},
  title        = {Appreciation to referees, 2024},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision making in information security investments: Impact
of system vulnerability and investment timing on resource-sharing
platforms. <em>DECA</em>, <em>22</em>(1), 70–86. (<a
href="https://doi.org/10.1287/deca.2024.0190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study distinguishes enterprises into high- and low-type categories based on enterprise value and cost efficiency, examining their strategic behaviors in three investment timing games: move simultaneously, the high-type enterprise moves first, and the low-type enterprise moves first. By comparing the three games, we find that both types of enterprises would always exert more effort in the sequential game than in the simultaneous game, and the later-move advantage makes both types of enterprises prefer to become the follower in the game. We also find that the enhanced cost efficiency advantage or enterprise value gap possessed by the high-type enterprise would widen the effort gap between the two types of enterprises, and enhance the low-type enterprise’s incentive to be the follower. Moreover, the existence of system vulnerability not only causes both types of enterprises to reduce their security effort that generates free-riding behaviors but also can first discourage and then encourage enterprises from moving in advance. We further propose a liability-based mechanism to tackle the free-riding problem. We reveal an exact optimal liability coefficient, whether in the simultaneous or sequential game and find that the high-type enterprise should undertake more compensation when its dominant position becomes more obvious and the low-type enterprise could therefore undertake decreased liability. Last, we extend the model to multiple enterprises and show that the results are robust. Funding: This work was supported by the Shanghai Science and Technology Development Foundation [Grant 24692107700]; the Humanities and Social Science Fund of Ministry of Education of China [Grant 23YJA630100]; Fundamental Research Funds for the Central Universities; DHU Distinguished Young Professor Program; and the Shanghai Social Science Foundation [Grant 2022ZGL009]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/deca.2024.0190 .},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0190},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {70-86},
  shortjournal = {Decis. Anal.},
  title        = {Decision making in information security investments: Impact of system vulnerability and investment timing on resource-sharing platforms},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on ecological restoration strategies for abandoned
mines based on ecology-oriented development. <em>DECA</em>,
<em>22</em>(1), 44–69. (<a
href="https://doi.org/10.1287/deca.2023.0132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ecological restoration of abandoned mines poses a significant challenge within China’s broader efforts toward the protection and restoration of the nation’s ecosystems. Based on the ecology-oriented development (EOD) model, a differential game framework for the ecological restoration of abandoned mines was constructed in this study, involving both government and corporate entities as players. Our analysis focused on various factors, including investment levels in ecological restoration quality, the quality of ecological restoration itself, investment levels in environmentally sensitive industry development, the quality of environmentally sensitive industries, environmentally sensitive product pricing strategies, and profit dynamics for both players across four distinct modes. Our findings reveal several key insights. First, extending the government’s concession period incentivizes enterprises to increase investment in both the quality of ecological restoration of abandoned mines and the development of environmentally sensitive industries. Second, the government should prefer the cooperative mode to introduce social capital for the ecology-oriented development of abandoned mines. Third, in noncooperative mode, for technology-intensive industries, the government’s industrial support policy is more mutually beneficial; conversely, for labor-intensive industries, the ecological restoration compensation policy yields higher mutual gains. Lastly, differentiated incentive policies implemented by the government across various modes prompt the input of enterprises in ecological restoration quality and environmentally sensitive industry development. Based on these findings, we offer specific policy recommendations to guide governmental efforts in promoting EOD for the ecological restoration of abandoned mines. Funding: This work was supported by the Sichuan Office of Philosophy and Social Science [Grant SC22C004] and the National Social Science Fund of China [Grant 22XKS015].},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2023.0132},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {44-69},
  shortjournal = {Decis. Anal.},
  title        = {Research on ecological restoration strategies for abandoned mines based on ecology-oriented development},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Satiation-based theory of frugal materialism. <em>DECA</em>,
<em>22</em>(1), 30–43. (<a
href="https://doi.org/10.1287/deca.2024.0192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frugal materialism is a tendency of consumer demand to become more elastic in product durability in response to a tightening budget constraint. This paper proposes a value function-based model of frugal materialism and establishes a close theoretical link between frugal materialism and the slope of value satiation as defined in the decision analytic literature; for both their absolute and relative versions, frugal materialism and increasing value satiation are nearly equivalent to each other. Only some shapes of the value function are thus compatible with frugal materialism, and the shape restrictions involve concepts familiar from models of decision making under risk, even though the proposed model of frugal materialism does not involve any risk. Under the additional assumption of relative risk neutrality, the demand for risky assets of a frugally materialistic consumer should exhibit well-known behavioral patterns associated with increasing risk aversion, and there is only a narrow possibility for frugal materialism to coexist with precautionary saving behavior.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0192},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {30-43},
  shortjournal = {Decis. Anal.},
  title        = {Satiation-based theory of frugal materialism},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why investors want risk. <em>DECA</em>, <em>22</em>(1),
14–29. (<a href="https://doi.org/10.1287/deca.2024.0206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We imagine investors taking shares in an exogenous lognormal cash payoff with known parameters. Using a power utility (constant relative risk aversion (CRRA)) replica of Lintner’s static payoffs -based constant absolute risk aversion-normal capital asset pricing model, we examine how an investor’s expected utility is affected by the payoff parameters and surrounding market conditions. The market clearing asset price falls as a proportion of wealth when market wealth is higher, implying that CRRA investors hold a lower (rather than fixed) proportion of wealth in the risky asset when they are wealthier. Investors prefer conditions where they can obtain more risk, either because the risky payoff is exogenously riskier or competing investors want less. The equilibrium asset price is “disproportionately” lower when the asset is riskier, or when there are fewer willing buyers, leaving a better opportunity with higher expected utility.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0206},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {14-29},
  shortjournal = {Decis. Anal.},
  title        = {Why investors want risk},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the value of information across decision problems.
<em>DECA</em>, <em>22</em>(1), 1–13. (<a
href="https://doi.org/10.1287/deca.2024.0187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The value of information is an important concept in decision analysis that has been quantified as the buying price ( BPI) for the information and the expected utility increase ( EUI) obtainable by using the information. These two measures rank information sources identically in a scalar-valued decision problem only when the utility function is linear or exponential. In contrast, this paper focuses on the value of information across scalar-valued decision problems sharing the same utility function such as different divisions within an organization exploring various information sources for their decisions using the same organizational utility function. In this context, it still makes sense to ask which sources are more informative. We show that BPI and EUI rank information sources identically in this context only when the utility function is linear . However, if the certainty equivalent increase is used instead of EUI , then identical ranking with BPI across problems is maintained for the broader class of linear or exponential utility functions. We discuss the importance of these results for distributed decision-making settings, where different departments within an organization may calculate the value of information separately. Our results advise against using EUI to measure information value in this context when risk attitude is important.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0187},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Decis. Anal.},
  title        = {On the value of information across decision problems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijaa---6">IJAA - 6</h2>
<ul>
<li><details>
<summary>
(2025). Part feeding and internal transportation decision making for
a machinery manufacturer. <em>INFORMS Journal on Applied Analytics</em>,
<em>55</em>(2), 154–177. (<a
href="https://doi.org/10.1287/inte.2023.0039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Because of a constant increase in the variety and number of parts required for assembly, part provision to the border of line requires careful orchestration. Therefore, every part must be assigned to the most suitable feeding policy, determining logistical in-house processes, presorting degree, and load carrier sizing. This assignment allows effective space management at the border of line while avoiding unnecessary intralogistics activities. Furthermore, timely part delivery necessitates decisions on vehicle type selection and delivery frequencies. In this research, we adapt an existing optimization model to the specific case of a machinery manufacturing company, validating the model for practical usage. The model aims to achieve cost minimization through optimal part feeding policy and vehicle type selection decisions, considering various constraints related to space availability at the border of line and the capacity of transportation vehicles. Our results reveal that the optimal assignment reduces costs by 56% compared with the company’s current assignment. Although the optimal assignment requires additional capacity investments for the company, we also test various restrictive solutions by adding practically relevant constraints. The latter still results in minimum cost savings of 23%, compared with the current assignment. Besides the cost benefits, the number of different line feeding policies is reduced. This streamlines the process, making it more conducive to automation. History: This paper was refereed. Funding: Financial support from Universiteit Gent is gratefully acknowledged.},
  archive  = {J},
  doi      = {10.1287/inte.2023.0039},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {3-4},
  number   = {2},
  pages    = {154-177},
  title    = {Part feeding and internal transportation decision making for a machinery manufacturer},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating road construction costs with explainable machine
learning. <em>INFORMS Journal on Applied Analytics</em>, <em>55</em>(2),
137–153. (<a href="https://doi.org/10.1287/inte.2023.0027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A preliminary estimation of construction costs is a crucial operation of any project related to civil engineering. An accurate estimation ensures a proper management of the available funds and helps the project managers in their decision-making processes. For instance, it is common that specific subtasks of the project are delegated to private subcontractors. Through a call for tenders, each eventual subcontractor has the opportunity to propose a bid with a price for supplying the service. Because the call is generally public, a competition may arise between subcontractors. This impacts the price proposed by the competitors to get the contract. In order to select a subcontractor, the project manager needs to have an accurate idea of a reasonable price for the subtask given. A price higher than expected is undesirable, but a price significantly lower than expected may also result in poor quality of service. The project manager must also be able to explain to stakeholders why a price is suited and justify why a specific subcontractor has been selected. Providing an estimation that is both accurate and transparent is a hard problem for the project manager. A growing trend is to leverage machine learning for this estimation, but designing a model that is both accurate and explainable is still a challenge. Another difficulty is that an approach that is accurate for estimating the cost of a subtask may not be efficient for another one. Based on this context, this paper introduces a framework for estimating construction costs while tackling both challenges. It is based on six machine learning models and on Shapley additive explanations. This project was commissioned by the Ministry of Transport and Sustainable Mobility, a public agency responsible for transport infrastructure in Quebec, Canada. Experiments were carried out on real data, covering historical road construction costs of 11,646 contracts and eight subtasks from 2014 to 2021. Results show that the framework is able to surpass the accuracy of human estimations by up to 31.56% while being able to adequately explain how the estimations have been obtained. History: This paper was refereed. Funding: This project was commissioned by the Ministry of Transport and Sustainable Mobility, a public agency responsible for transport infrastructure in Quebec, Canada. Supplemental Material: The online appendix is available at https://doi.org/10.1287/inte.2023.0027 .},
  archive  = {J},
  doi      = {10.1287/inte.2023.0027},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {3-4},
  number   = {2},
  pages    = {137-153},
  title    = {Estimating road construction costs with explainable machine learning},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing delhivery’s midmile logistics network using a
hybrid evolutionary search algorithm. <em>INFORMS Journal on Applied
Analytics</em>, <em>55</em>(2), 121–136. (<a
href="https://doi.org/10.1287/inte.2023.0049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study demonstrates the application of operations research techniques to enhance the operational efficiency of a large logistics network. The focus is on addressing a variant of the vehicle routing problem faced by Delhivery, a leading logistics company in India, to improve its midmile operations. The problem involves enhancing a complex distribution network with more than 3,000 locations, which can be closely described as a multidepot fleet size and mix site-dependent asymmetric distance-constrained vehicle routing problem with time windows. The study also considers the geographic scope of the network, which has not been previously explored in the literature. A novel mixed integer linear programming formulation is presented along with a powerful hybrid evolutionary search algorithm that has been tested in many real-world routing problems. Additionally, a novel insertion algorithm that significantly reduces computational time is introduced. Furthermore, the capabilities of the algorithm are expanded to determine the optimal locations for new hubs within Delhivery’s network. The proposed algorithm achieves significant cost savings of nearly 7.3% and offers various managerial advantages. The algorithm converges rapidly and automates the entire planning and operations process, resulting in improved overall efficiency. History: This paper was refereed.},
  archive  = {J},
  doi      = {10.1287/inte.2023.0049},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {3-4},
  number   = {2},
  pages    = {121-136},
  title    = {Optimizing delhivery’s midmile logistics network using a hybrid evolutionary search algorithm},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Saving millions in government procurement through data
science and market design. <em>INFORMS Journal on Applied
Analytics</em>, <em>55</em>(2), 101–120. (<a
href="https://doi.org/10.1287/inte.2023.0002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Framework agreements (FAs) are procurement mechanisms used in private and public organizations by which a central procurement agency selects an assortment of products, typically through auctions, and then affiliated organizations can purchase from the selected assortment as needs arise. In Chile’s central procurement agency (ChileCompra), FAs accounted for 23% of the procurement expenditures during 2018–2019. However, descriptive analysis of purchase transaction data suggests that some FAs exhibited low levels of competition in the auctions used to select the suppliers, which could potentially result in larger government expenditures. We collaborated with ChileCompra to redesign FAs to enhance competition introducing two important changes: (i) standardize the product catalog using natural language processing algorithms and (ii) use this product standardization to induce more competition in the auctions to select suppliers. These changes were implemented through an experimental approach in a redesigned food FA to measure its impact, showing that inducing more intense competition in the auction stage reduced transaction prices by 8%. This pilot study ultimately led ChileCompra to implement a similar design in all its FAs, and many of the improvements in the design of the FAs were included in the new regulation on government purchases. If we were to extrapolate the savings from our pilot redesign to all these FAs, the total savings would amount to around $74 million in 2022. History: This paper was refereed. Funding: This work was supported by Fondo Nacional de Desarrollo Cientıfico y Tecnologico [Grant 1230416] and the Agencia Nacional de Investigacion y Desarrollo [Grant PIA/APOYO AFB220003]. Weintraub also acknowledges the 2023–2024 Stanford GSB Katherine and David deWilde Faculty Scholarship for financial support. Supplemental Material: The online appendix is available at https://doi.org/10.1287/inte.2023.0002 .},
  archive  = {J},
  doi      = {10.1287/inte.2023.0002},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {3-4},
  number   = {2},
  pages    = {101-120},
  title    = {Saving millions in government procurement through data science and market design},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning by doing: The industrial and applied mathematics
program at eindhoven university of technology. <em>INFORMS Journal on
Applied Analytics</em>, <em>55</em>(2), 85–100. (<a
href="https://doi.org/10.1287/inte.2023.0046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Industrial and Applied Mathematics (IAM) master’s program of the Department of Mathematics and Computer Science at the Eindhoven University of Technology was awarded the INFORMS UPS George D. Smith Prize in 2022. This paper provides an overview of the department’s history, highlights the program’s innovative aspects that led to its recognition, and describes how the program aims at educating operations research (OR) practitioners for today’s society. Eindhoven University of Technology emphasizes collaboration with industry, societal involvement, and scientific excellence. These principles permeate all of its departments and programs. We explain how IAM contributes to this mission, and we explore the educational approaches that are used to prepare skilled OR practitioners. History: This paper was refereed. Funding: This work was partially supported by the INFORMS UPS George D. Smith Prize 2022.},
  archive  = {J},
  doi      = {10.1287/inte.2023.0046},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {3-4},
  number   = {2},
  pages    = {85-100},
  title    = {Learning by doing: The industrial and applied mathematics program at eindhoven university of technology},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In memoriam: Burcu keskin, 1979–2024. <em>INFORMS Journal on
Applied Analytics</em>, <em>55</em>(2), 83–84. (<a
href="https://doi.org/10.1287/inte.2025.memorial.v55.n2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1287/inte.2025.memorial.v55.n2},
  journal = {INFORMS Journal on Applied Analytics},
  month   = {3-4},
  number  = {2},
  pages   = {83-84},
  title   = {In memoriam: Burcu keskin, 1979–2024},
  volume  = {55},
  year    = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijds---6">IJDS - 6</h2>
<ul>
<li><details>
<summary>
(2025). A reduced modeling approach for making predictions with
incomplete data having blockwise missing patterns. <em>IJDS</em>,
<em>4</em>(1), 85–99. (<a
href="https://doi.org/10.1287/ijds.2022.9016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete data with blockwise missing patterns are commonly encountered in analytics, and solutions typically entail listwise deletion or imputation. However, as the proportion of missing values in input features increases, listwise or columnwise deletion leads to information loss, whereas imputation diminishes the integrity of the training data set. We present the blockwise reduced modeling (BRM) method for analyzing blockwise missing patterns, which adapts and improves on the notion of reduced modeling proposed by Friedman, Kohavi, and Yun in 1996 as lazy decision trees. In contrast to the original idea of reduced modeling of delaying model induction until a prediction is required, our method is significantly faster because it exploits the blockwise missing patterns to pretrain ensemble models that require minimum imputation of data. Models are pretrained over the overlapping subsets of an incomplete data set that contain only populated values. During prediction, each test instance is mapped to one of these models based on its feature-missing pattern. BRM can be applied to any supervised learning model for tabular data. We benchmark the predictive performance of BRM using simulations of blockwise missing patterns on three complete data sets from public repositories. Thereafter, we evaluate its utility on three data sets with actual blockwise missing patterns. We demonstrate that BRM is superior to most existing benchmarks in terms of predictive performance for linear and nonlinear models. It also scales well and is more reliable than existing benchmarks for making predictions with blockwise missing pattern data. History: Maytal Saar-Tsechansky served as the senior editor for this article. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/0274716/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.9016 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.9016},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {85-99},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {A reduced modeling approach for making predictions with incomplete data having blockwise missing patterns},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair collaborative learning (FairCL): A method to improve
fairness amid personalization. <em>IJDS</em>, <em>4</em>(1), 67–84. (<a
href="https://doi.org/10.1287/ijds.2024.0029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model personalization has attracted widespread attention in recent years. In an ideal situation, if individuals’ data are sufficient, model personalization can be realized by building models separately for different individuals using their own data. But, in reality, individuals often have data sets of varying sizes and qualities. To overcome this disparity, collaborative learning has emerged as a generic strategy for model personalization, but there is no mechanism to ensure fairness in this framework. In this paper, we develop fair collaborative learning (FairCL) that could potentially integrate a variety of fairness concepts. We further focus on two specific fairness metrics, the bounded individual loss and individual fairness, and develop a self-adaptive algorithm for FairCL and conduct both simulated and real-world case studies. Our study reveals that model fairness and accuracy could be improved simultaneously in the context of model personalization. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: This work was supported by the Breakthrough T1D Award [Grant 2-SRA-2022-1259-S-B]. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/1331847/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2024.0029 ). The real-world data, including the transportation demand management and surgical site infection data sets, are proprietary and not publicly available. Other results are available at https://github.com/ryanlif/FairCL .},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.0029},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {67-84},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Fair collaborative learning (FairCL): A method to improve fairness amid personalization},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multilabel classification for fine-level event
extraction from aviation accident reports. <em>IJDS</em>, <em>4</em>(1),
51–66. (<a href="https://doi.org/10.1287/ijds.2022.0032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large numbers of accident reports are recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we must understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. To make the labeling process more efficient, many researchers have started developing algorithms to automatically identify the underlying events from accident reports. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem to be a hierarchical classification task, where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into the bidirectional encoder representations from transformers model. To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels. The effectiveness of our framework is evaluated using data collected by the National Transportation Safety Board. It has been shown that fine-level prediction accuracy is highly improved and that the regularization term can be beneficial to the rare event identification problem. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: The research reported in this paper was supported by funds from NASA University Leadership Initiative program (Contract No. NNX17AJ86A, Project Officer: Dr. Anupa Bajwa, Principal Investigator: Dr. Yongming Liu) and NSF DMS 1830363. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/9128124/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0032 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0032},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {51-66},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Hierarchical multilabel classification for fine-level event extraction from aviation accident reports},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank robust subspace tensor clustering for metro
passenger flow modeling. <em>IJDS</em>, <em>4</em>(1), 33–50. (<a
href="https://doi.org/10.1287/ijds.2022.0028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor clustering has become an important topic, specifically in spatiotemporal modeling, because of its ability to cluster spatial modes (e.g., stations or road segments) and temporal modes (e.g., time of day or day of the week). Our motivating example is from subway passenger flow modeling, where similarities between stations are commonly found. However, the challenges lie in the innate high-dimensionality of tensors and also the potential existence of anomalies. This is because the three tasks, that is, dimension reduction, clustering, and anomaly decomposition, are intercorrelated with each other, and treating them in a separate manner will render a suboptimal performance. Thus, in this work, we design a tensor-based subspace clustering and anomaly decomposition technique for simultaneous outlier-robust dimension reduction and clustering for high-dimensional tensors. To achieve this, a novel low-rank robust subspace clustering decomposition model is proposed by combining Tucker decomposition, sparse anomaly decomposition, and subspace clustering. An effective algorithm based on Block Coordinate Descent is proposed to update the parameters. Prudent experiments prove the effectiveness of the proposed framework via the simulation study, with a gain of +25% clustering accuracy over benchmark methods in a hard case. The interrelations of the three tasks are also analyzed via ablation studies, validating the interrelation assumption. Moreover, a case study in station clustering based on real passenger flow data is conducted, with quite valuable insights discovered. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: H. Yan is partially funded by DOE [DE-EE0009354] and NSF [CMMI 2316654]. F. Tsung is partially funded with the RGC [GRF 16201718 and 16216119]. The authors appreciate the help from the Hong Kong MTR Co. research, marketing, and customer service teams. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6536164/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0028 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0028},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {33-50},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Low-rank robust subspace tensor clustering for metro passenger flow modeling},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal time series forecasting using an iterative
kernel-based regression. <em>IJDS</em>, <em>4</em>(1), 20–32. (<a
href="https://doi.org/10.1287/ijds.2023.0019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal time series analysis is a growing area of research that includes different types of tasks, such as forecasting, prediction, clustering, and visualization. In many domains, like epidemiology or economics, time series data are collected to describe the observed phenomenon in particular locations over a predefined time slot and predict future behavior. Regression methods provide a simple mechanism for evaluating empirical functions over scattered data points. In particular, kernel-based regressions are suitable for cases in which the relationship between the data points and the function is not linear. In this work, we propose a kernel-based iterative regression model, which fuses data from several spatial locations for improving the forecasting accuracy of a given time series. In more detail, the proposed method approximates and extends a function based on two or more spatial input modalities coded by a series of multiscale kernels, which are averaged as a convex combination. The proposed spatio-temporal regression resembles ideas that are present in deep learning architectures, such as passing information between scales. Nevertheless, the construction is easy to implement, and it is also suitable for modeling data sets of limited size. Experimental results demonstrate the proposed model for solar energy prediction, forecasting epidemiology infections, and future number of fire events. The method is compared with well-known regression techniques and highlights the benefits of the proposed model in terms of accuracy and flexibility. The reliable outcome of the proposed model and its nonparametric nature yield a robust tool to be integrated as a forecasting component in wide range of decision support systems that analyze time series data. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: This research was supported by the Israel Science Foundation [Grant 1144/20] and partly supported by the Ministry of Science and Technology, Israel [Grant 5614]. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6417440/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0019 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0019},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {20-32},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Spatio-temporal time series forecasting using an iterative kernel-based regression},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking cost-sensitive classification in deep learning
via adversarial data augmentation. <em>IJDS</em>, <em>4</em>(1), 1–19.
(<a href="https://doi.org/10.1287/ijds.2022.0033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-sensitive classification is critical in applications where misclassification errors widely vary in cost. However, overparameterization poses fundamental challenges to the cost-sensitive modeling of deep neural networks (DNNs). The ability of a DNN to fully interpolate a training data set can render a DNN, evaluated purely on the training set, ineffective in distinguishing a cost-sensitive solution from its overall accuracy maximization counterpart. This necessitates rethinking cost-sensitive classification in DNNs. To address this challenge, this paper proposes a cost-sensitive adversarial data augmentation (CSADA) framework to make overparameterized models cost sensitive. The overarching idea is to generate targeted adversarial examples that push the decision boundary in cost-aware directions. These targeted adversarial samples are generated by maximizing the probability of critical misclassifications and used to train a model with more conservative decisions on costly pairs. Experiments on well-known data sets and a pharmacy medication image (PMI) data set, made publicly available, show that our method can effectively minimize the overall cost and reduce critical errors while achieving comparable overall accuracy. History: Nick Street served as the senior editor for this article. Funding: Research reported in this publication was supported by the National Library of medicine of the National Institutes of Health in the United States under award number R01LM013624. Data Ethics &amp; Reproducibility Note: This paper abides by data ethics requirements. Data used are publicly available online at https://deepblue.lib.umich.edu/data/concern/data_sets/6d56zw997 . Codes to replicate the results of this paper are available on Code Ocean at https://doi.org/10.24433/CO.2139841.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0033 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0033},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Rethinking cost-sensitive classification in deep learning via adversarial data augmentation},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijoc---11">IJOC - 11</h2>
<ul>
<li><details>
<summary>
(2025). Appreciation to reviewers. <em>IJOC</em>, <em>37</em>(1),
189–196. (<a
href="https://doi.org/10.1287/ijoc.2025.thx.v37.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On behalf of the Editorial Board, I would like to thank the hundreds of people who acted as reviewers for the INFORMS Journal on Computing during the past year. Reviewers are the cornerstone of the peer review system and give their time and expertise unselfishly. IJOC reviewers, please know you are greatly appreciated! Alice Smith, Editor-in-Chief},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2025.thx.v37.n1},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {189-196},
  shortjournal = {INFORMS J. Comput.},
  title        = {Appreciation to reviewers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum-inspired bilevel optimization algorithm for the
first responder network design problem. <em>IJOC</em>, <em>37</em>(1),
172–188. (<a href="https://doi.org/10.1287/ijoc.2024.0574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the aftermath of a sudden catastrophe, first responders (FRs) strive to reach and rescue immobile victims. Simultaneously, civilians use the same roads to evacuate, access medical facilities and shelters, or reunite with their relatives via private vehicles. The escalated traffic congestion can significantly hinder critical FR operations. A proposal from the Türkiye Ministry of Transportation and Infrastructure is to allocate a lane on specific road segments exclusively for FR use, mark them clearly, and precommunicate them publicly. For a successful implementation of this proposal, an FR path should exist from designated entry points to each FR demand point in the network. The reserved FR lanes along these paths will be inaccessible to evacuees, potentially increasing evacuation times. Hence, in this study, we aim to determine a subset of links along which an FR lane should be reserved and analyze the resulting evacuation flow under evacuees’ selfish routing behavior. We introduce this problem as the first responder network design problem (FRNDP) and formulate it as a mixed-integer nonlinear program. To efficiently solve FRNDP, we introduce a novel bilevel nested heuristic, the Graver augmented multiseed algorithm (GAMA) within GAMA, called GAGA. We test GAGA on synthetic graph instances of various sizes as well as scenarios related to a potential Istanbul earthquake. Our comparisons with a state-of-the-art exact algorithm for network design problems demonstrate that GAGA offers a promising alternative approach and highlights the need for further exploration of quantum-inspired computing to tackle complex real-world problems. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: S. Tayur and A. Tenneti acknowledge Raytheon BBN (RTX-BBN) for its support through a Carnegie Mellon University-BBN contract as part of a Defense Advanced Research Projects Agency project on quantum-inspired classical computing. A. Karahalios is supported by the National Science Foundation Graduate Research Fellowship Program [Grants DGE1745016, DGE2140739]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0574 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0574 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0574},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {172-188},
  shortjournal = {INFORMS J. Comput.},
  title        = {A quantum-inspired bilevel optimization algorithm for the first responder network design problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the instance dependence of parameter initialization for
the quantum approximate optimization algorithm: Insights via instance
space analysis. <em>IJOC</em>, <em>37</em>(1), 146–171. (<a
href="https://doi.org/10.1287/ijoc.2024.0564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantum approximate optimization algorithm (QAOA) tackles combinatorial optimization problems in a quantum computing context, where achieving globally optimal and exact solutions is not always feasible because of classical computational constraints or problem complexity. The performance of QAOA generally depends on finding sufficiently good parameters that facilitate competitive approximate solutions. However, this is fraught with challenges, such as “barren plateaus,” making the search for effective parameters a nontrivial endeavor. More recently, the question of whether such an optimal parameter search is even necessary has been posed, with some studies showing that optimal parameters tend to be concentrated on certain values for specific types of problem instances. However, these existing studies have only examined specific instance classes of Maximum Cut, so it is uncertain if the claims of instance independence apply to a diverse range of instances. In this paper, we use instance space analysis to study QAOA parameter initialization strategies for the first time, providing a comprehensive study of how instance characteristics affect the performance of initialization strategies across a diverse set of graph types and weight distributions. Unlike previous studies that focused on specific graph classes (e.g., d -regular or Erdős–Rényi), our work examines a much broader range of instance types, revealing insights about parameter transfer between different graph classes. We introduce and evaluate a new initialization strategy, quantum instance-based parameter initialization, that leverages instance-specific information, demonstrating its effectiveness across various instance types. Our analysis at higher QAOA depths ( p = 15) provides insights into the effectiveness of different initialization strategies beyond the low-depth circuits typically studied. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. This article is accepted for Special Issue. Funding: This research was supported by the Australian Research Council [Grant IC200100009 for the Australian Research Council Training Centre in Optimization Technologies, Integrated Methodologies and Applications]. V. Katial is supported by The University of Melbourne [Research Training Program Scholarship]. The authors gratefully acknowledge the information technology infrastructure support provided by The University of Melbourne’s Research Computing Services and the Petascale Campus Initiative. This research was also supported by The University of Melbourne through the establishment of the IBM Quantum Network Hub at the university. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0564 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0564 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0564},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {146-171},
  shortjournal = {INFORMS J. Comput.},
  title        = {On the instance dependence of parameter initialization for the quantum approximate optimization algorithm: Insights via instance space analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage estimation and variance modeling for
latency-constrained variational quantum algorithms. <em>IJOC</em>,
<em>37</em>(1), 125–145. (<a
href="https://doi.org/10.1287/ijoc.2024.0575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantum approximate optimization algorithm (QAOA) has enjoyed increasing attention in noisy, intermediate-scale quantum computing with its application to combinatorial optimization problems. QAOA has the potential to demonstrate a quantum advantage for NP-hard combinatorial optimization problems. As a hybrid quantum-classical algorithm, the classical component of QAOA resembles a simulation optimization problem in which the simulation outcomes are attainable only through a quantum computer. The simulation that derives from QAOA exhibits two unique features that can have a substantial impact on the optimization process: (i) the variance of the stochastic objective values typically decreases in proportion to the optimality gap, and (ii) querying samples from a quantum computer introduces an additional latency overhead. In this paper, we introduce a novel stochastic trust-region method derived from a derivative-free, adaptive sampling trust-region optimization method intended to efficiently solve the classical optimization problem in QAOA by explicitly taking into account the two mentioned characteristics. The key idea behind the proposed algorithm involves constructing two separate local models in each iteration: a model of the objective function and a model of the variance of the objective function. Exploiting the variance model allows us to restrict the number of communications with the quantum computer and also helps navigate the nonconvex objective landscapes typical in QAOA optimization problems. We numerically demonstrate the superiority of our proposed algorithm using the SimOpt library and Qiskit when we consider a metric of computational burden that explicitly accounts for communication costs. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This material is based upon work supported by the U.S. Department of Energy, Office of Science, National Quantum Information Science Research Centers and the Office of Advanced Scientific Computing Research, Accelerated Research for Quantum Computing program under contract number DE-AC02-06CH11357. Y. Ha and S. Shashaani also gratefully acknowledge the U.S. National Science Foundation Division of Civil, Mechanical and Manufacturing Innovation Grant CMMI-2226347 and the U.S. Office of Naval Research [Grant N000142412398]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0575 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0575 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0575},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {125-145},
  shortjournal = {INFORMS J. Comput.},
  title        = {Two-stage estimation and variance modeling for latency-constrained variational quantum algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QHDOPT: A software for nonlinear optimization with quantum
hamiltonian descent. <em>IJOC</em>, <em>37</em>(1), 107–124. (<a
href="https://doi.org/10.1287/ijoc.2024.0587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an open-source, end-to-end software (named QHDOPT), which can solve nonlinear optimization problems using the quantum Hamiltonian descent (QHD) algorithm. QHDOPT offers an accessible interface and automatically maps tasks to various supported quantum backends (i.e., quantum hardware machines). These features enable users, even those without prior knowledge or experience in quantum computing, to utilize the power of existing quantum devices for nonlinear and nonconvex optimization tasks. In its intermediate compilation layer, QHDOPT employs SimuQ, an efficient interface for Hamiltonian-oriented programming, to facilitate multiple algorithmic specifications and ensure compatible cross-hardware deployment. The detailed documentation of QHDOPT is available at https://github.com/jiaqileng/QHDOPT . History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This work was supported by the U.S. Department of Energy’s Advanced Research Projects Agency–Energy [Grant DE-SC0020273], the Alfred P. Sloan Foundation, the Simons Foundation [Simons Investigator Award 825053], the Simons Quantum Postdoctoral Fellowship, the National Science Foundation [Grants CCF-1816695, CCF-1942837, and ECCS-2045978], the Unitary Fund, and the Air Force Office of Scientific Research [Grant FA95502110051]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0587 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0587 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0587},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {107-124},
  shortjournal = {INFORMS J. Comput.},
  title        = {QHDOPT: A software for nonlinear optimization with quantum hamiltonian descent},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Binary quantum control optimization with uncertain
hamiltonians. <em>IJOC</em>, <em>37</em>(1), 86–106. (<a
href="https://doi.org/10.1287/ijoc.2024.0560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the controls of quantum systems plays a crucial role in advancing quantum technologies. The time-varying noises in quantum systems and the widespread use of inhomogeneous quantum ensembles raise the need for high-quality quantum controls under uncertainties. In this paper, we consider a stochastic discrete optimization formulation of a discretized binary optimal quantum control problem involving Hamiltonians with predictable uncertainties. We propose a sample-based reformulation that optimizes both risk-neutral and risk-averse measurements of control policies, and solve these with two gradient-based algorithms using sum-up-rounding approaches. Furthermore, we discuss the differentiability of the objective function and prove upper bounds of the gaps between the optimal solutions to binary control problems and their continuous relaxations. We conduct numerical simulations on various sized problem instances based on two applications of quantum pulse optimization; we evaluate different strategies to mitigate the impact of uncertainties in quantum systems. We demonstrate that the controls of our stochastic optimization model achieve significantly higher quality and robustness compared with the controls of a deterministic model. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This work was supported by the US Department of Energy, Advanced Scientific Computing Research [Grants DE-AC02-06CH11357, DE-SC0018018]; Defense Sciences Office, DARPA [Grant IAA-8839-annex-130]; the US National Science Foundation, Division of Civil, Mechanical and Manufacturing Innovation [Grant 2041745]; and the US National Aeronautics and Space Administration (NASA) Ames Research Center [Grant 80ARC020D0010]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0560 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0560 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0560},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {86-106},
  shortjournal = {INFORMS J. Comput.},
  title        = {Binary quantum control optimization with uncertain hamiltonians},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel noise-aware classical optimizer for variational
quantum algorithms. <em>IJOC</em>, <em>37</em>(1), 63–85. (<a
href="https://doi.org/10.1287/ijoc.2024.0578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key component of variational quantum algorithms (VQAs) is the choice of classical optimizer employed to update the parameterization of an ansatz. It is well recognized that quantum algorithms will, for the foreseeable future, necessarily be run on noisy devices with limited fidelities. Thus, the evaluation of an objective function (e.g., the guiding function in the quantum approximate optimization algorithm (QAOA) or the expectation of the electronic Hamiltonian in variational quantum eigensolver (VQE)) required by a classical optimizer is subject not only to stochastic error from estimating an expected value but also to error resulting from intermittent hardware noise. Model-based derivative-free optimization methods have emerged as popular choices of a classical optimizer in the noisy VQA setting, based on empirical studies. However, these optimization methods were not explicitly designed with the consideration of noise. In this work we adapt recent developments from the “noise-aware numerical optimization” literature to these commonly used derivative-free model-based methods. We introduce the key defining characteristics of these novel noise-aware derivative-free model-based methods that separate them from standard model-based methods. We study an implementation of such noise-aware derivative-free model-based methods and compare its performance on demonstrative VQA simulations to classical solvers packaged in scikit-quant . History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This material is based upon work supported by the U.S. Department of Energy, Office of Science, National Quantum Information Science Research Centers and the Office of Advanced Scientific Computing Research, Accelerated Research for Quantum Computing program under contract number DE-AC02-06CH11357. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0578 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0578 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0578},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {63-85},
  shortjournal = {INFORMS J. Comput.},
  title        = {A novel noise-aware classical optimizer for variational quantum algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new multicommodity network flow model and branch and cut
for optimal quantum boolean circuit synthesis. <em>IJOC</em>,
<em>37</em>(1), 42–62. (<a
href="https://doi.org/10.1287/ijoc.2024.0562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a new optimization model and a branch-and-cut approach for synthesizing optimal quantum circuits for reversible Boolean functions, which are pivotal components in quantum algorithms. Although heuristic algorithms have been extensively explored for quantum circuit synthesis, research on exact counterparts remains relatively limited. However, the need to design quantum circuits with guaranteed optimality is increasing, especially for improving computational fidelity on noisy intermediate-scale quantum devices. This study presents mathematical optimization as a viable option for optimal synthesis, with the potential to accommodate practical considerations arising in fast-evolving quantum technologies. We set a demonstrative problem to implement reversible Boolean functions using high-level gates known as multiple control Toffoli gates while minimizing a technology-based proxy called quantum cost—the number of low-level gates used to realize each high-level gate. To address this problem, we propose a discrete optimization model based on a multicommodity network and discuss potential future variations at an abstract level to incorporate technical considerations. A customized branch and cut is then developed upon different aspects of our model, including polyhedron integrality, surrogate constraints, and variable prioritization. Our experiments demonstrate the robustness of the proposed approach in finding cost-optimal circuits for all benchmark instances within a two-hour time frame. Furthermore, we present interesting intuitions from these experiments and compare our computational results with relevant studies, highlighting newly discovered circuits with the lowest quantum costs reported in this paper. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing. This paper has been accepted for the INFORMS Journal on Computing Special Issue on Quantum Computing. Funding: This research was supported by the Ministry of Science and Information and Communication Technology, South Korea [Grants 2017R1E1A1A0307098814 and 2020R1A4A307986411]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0562 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0562 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0562},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {42-62},
  shortjournal = {INFORMS J. Comput.},
  title        = {A new multicommodity network flow model and branch and cut for optimal quantum boolean circuit synthesis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized noise suppression for quantum circuits.
<em>IJOC</em>, <em>37</em>(1), 22–41. (<a
href="https://doi.org/10.1287/ijoc.2024.0551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computation promises to advance a wide range of computational tasks. However, current quantum hardware suffers from noise and is too small for error correction. Thus, accurately utilizing noisy quantum computers strongly relies on noise characterization, mitigation, and suppression. Crucially, these methods must also be efficient in terms of their classical and quantum overhead. Here, we efficiently characterize and mitigate crosstalk noise, which is a severe error source in, for example, cross-resonance based superconducting quantum processors. For crosstalk characterization, we develop a simplified measurement experiment. Furthermore, we analyze the problem of optimal experiment scheduling and solve it for common hardware architectures. After characterization, we mitigate noise in quantum circuits by a noise-aware qubit routing algorithm. Our integer programming algorithm extends previous work on optimized qubit routing by swap insertion. We incorporate the measured crosstalk errors in addition to other, more easily accessible noise data in the objective function. Furthermore, we strengthen the underlying integer linear model by proving a convex hull result about an associated class of polytopes, which has applications beyond this work. We evaluate the proposed method by characterizing crosstalk noise for two chips with up to 127 qubits and leverage the resulting data to improve the approximation ratio of the Quantum Approximate Optimization Algorithm by up to 10% compared with other established noise-aware routing methods. Our work clearly demonstrates the gains of including noise data when mapping abstract quantum circuits to hardware native ones. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue on Quantum Computing. Funding: This work was supported by Bavarian state government; Bayerische Staatsministerium für Wirtschaft, Landesentwicklung und Energie. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0551 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0551 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0551},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {22-41},
  shortjournal = {INFORMS J. Comput.},
  title        = {Optimized noise suppression for quantum circuits},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient implementation of interior-point methods for
quantum relative entropy. <em>IJOC</em>, <em>37</em>(1), 3–21. (<a
href="https://doi.org/10.1287/ijoc.2024.0570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum relative entropy (QRE) programming is a recently popular and challenging class of convex optimization problems with significant applications in quantum computing and quantum information theory. We are interested in modern interior-point (IP) methods based on optimal self-concordant barriers for the QRE cone. A range of theoretical and numerical challenges associated with such barrier functions and the QRE cones have hindered the scalability of IP methods. To address these challenges, we propose a series of numerical and linear algebraic techniques and heuristics aimed at enhancing the efficiency of gradient and Hessian computations for the self-concordant barrier function, solving linear systems, and performing matrix-vector products. We also introduce and deliberate about some interesting concepts related to QRE such as symmetric quantum relative entropy. We design a two-phase method for performing facial reduction that can significantly improve the performance of QRE programming. Our new techniques have been implemented in the latest version (DDS 2.2) of the software package Domain-Driven Solver (DDS). In addition to handling QRE constraints, DDS accepts any combination of several other conic and nonconic convex constraints. Our comprehensive numerical experiments encompass several parts, including (1) a comparison of DDS 2.2 with Hypatia for the nearest correlation matrix problem, (2) using DDS 2.2 for combining QRE constraints with various other constraint types, and (3) calculating the key rate for quantum key distribution (QKD) channels and presenting results for several QKD protocols. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This work was supported by the National Science Foundation [Grant CMMI-2347120] and Discovery Grants from the Natural Sciences and Engineering Research Council of Canada. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0570 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0570 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0570},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {3-21},
  shortjournal = {INFORMS J. Comput.},
  title        = {Efficient implementation of interior-point methods for quantum relative entropy},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to the special issue on quantum computing and
operations research. <em>IJOC</em>, <em>37</em>(1), 2. (<a
href="https://doi.org/10.1287/ijoc.2025.ed.v37.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2025.ed.v37.n1},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {2},
  shortjournal = {INFORMS J. Comput.},
  title        = {Introduction to the special issue on quantum computing and operations research},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijoo---4">IJOO - 4</h2>
<ul>
<li><details>
<summary>
(2025). A polyhedral study of multivariate decision trees.
<em>IJOO</em>, <em>7</em>(1), 61–82. (<a
href="https://doi.org/10.1287/ijoo.2023.0017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are a widely used tool for interpretable machine learning. Multivariate decision trees employ hyperplanes at the branch nodes to route datapoints throughout the tree and yield more compact models than univariate trees. Recently, mixed-integer programming (MIP) has been applied to formulate the optimal decision tree problem. A key component of most MIP formulations is a specification of how to route datapoints in the tree from the root to the leaves. Our goal is to provide polyhedral characterizations of realizable routings, that is, routings that can be realized using multivariate branching rules. We first focus on shattering inequalities, a class of valid inequalities that can be used to strengthen almost any MIP formulation and that have been shown to be computationally effective. We prove that if all the feature vectors are in general position, then the shattering inequalities defined for the root of the tree are facet defining for the convex hull of the realizable routings. We then show that every facet-defining inequality of a depth one tree involving at least two variables is also facet defining for trees of arbitrary depth. Finally, we show that facet-defining inequalities characterizing realizable routings are also facet-defining for a complete MIP formulation. We validate our theoretical results by performing numerical experiments that specifically exploit shattering inequalities defined at the root node and we observe an improvement in performance for both numerical and categorical data sets, especially for decision trees of intermediate size. Funding: C. Michini gratefully acknowledges funding from the DoD, Air Force [Award FA9550-23-1-0487].},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2023.0017},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {61-82},
  shortjournal = {INFORMS J. Optim.},
  title        = {A polyhedral study of multivariate decision trees},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonconvex factorization and manifold formulations are almost
equivalent in low-rank matrix optimization. <em>IJOO</em>,
<em>7</em>(1), 38–60. (<a
href="https://doi.org/10.1287/ijoo.2022.0030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the geometric landscape connection of the widely studied manifold and factorization formulations in low-rank positive semidefinite (PSD) and general matrix optimization. We establish a sandwich relation on the spectrum of Riemannian and Euclidean Hessians at first-order stationary points (FOSPs). As a result of that, we obtain an equivalence on the set of FOSPs, second-order stationary points, and strict saddles between the manifold and factorization formulations. In addition, we show that the sandwich relation can be used to transfer more quantitative geometric properties from one formulation to another. Similarities and differences in the landscape connection under the PSD case and the general case are discussed. To the best of our knowledge, this is the first geometric landscape connection between the manifold and factorization formulations for handling rank constraints, and it provides a geometric explanation for the similar empirical performance of factorization and manifold approaches in low-rank matrix optimization observed in the literature. In the general low-rank matrix optimization, the landscape connection of two factorization formulations (unregularized and regularized ones) is also provided. By applying these geometric landscape connections (in particular, the sandwich relation), we are able to solve unanswered questions in the literature and establish stronger results in the applications on geometric analysis of phase retrieval, well-conditioned low-rank matrix optimization, and the role of regularization in factorization arising from machine learning and signal processing. Funding: This work was supported by the National Key R&amp;D Program of China [Grants 2020YFA0711900 and 2020YFA0711901], the National Natural Science Foundation of China [Grants 12271107 and 62141407], and the Shanghai Science and Technology Program [Grant 21JC1400600]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2022.0030 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0030},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {38-60},
  shortjournal = {INFORMS J. Optim.},
  title        = {Nonconvex factorization and manifold formulations are almost equivalent in low-rank matrix optimization},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Function design for improved competitive ratio in online
resource allocation with procurement costs. <em>IJOO</em>,
<em>7</em>(1), 20–37. (<a
href="https://doi.org/10.1287/ijoo.2021.0012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of online resource allocation, where customers arrive sequentially, and the seller must irrevocably allocate resources to each incoming customer while also facing a prespecified procurement cost function over the total allocation. The objective is to maximize the reward obtained from fulfilling the customers’ requests sans the cumulative procurement cost. We analyze the competitive ratio of a primal-dual algorithm in this setting and develop an optimization framework for designing a surrogate function for the procurement cost to be used by the algorithm to improve the competitive ratio of the primal-dual algorithm. We use the optimal surrogate function for polynomial procurement cost functions to improve on previous bounds. For general procurement cost functions, our design method uses quasiconvex optimization to find optimal design parameters. We then implement the design techniques and show the improved performance of the algorithm in numerical examples. Finally, we extend the analysis by devising a posted pricing mechanism in which the algorithm does not require the customers’ preferences to be revealed. Funding: M. Fazel’s work was supported in part by the National Science Foundation [Awards 2023166, 2007036, and 1740551]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2021.0012 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0012},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {20-37},
  shortjournal = {INFORMS J. Optim.},
  title        = {Function design for improved competitive ratio in online resource allocation with procurement costs},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal computing budget allocation for data-driven ranking
and selection. <em>IJOO</em>, <em>7</em>(1), 1–19. (<a
href="https://doi.org/10.1287/ijoo.2024.0035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a fixed-budget ranking and selection (R&amp;S) problem, one aims to identify the best design among a finite number of candidates by efficiently allocating the given computing budget to evaluate design performance. Classical methods for R&amp;S usually assume the distribution of the randomness in the system is exactly known. In this paper, we consider the practical scenario where the true distribution is unknown but can be estimated from streaming input data that arrive in batches over time. We formulate the R&amp;S problem in this dynamic setting as a multistage problem where we adopt the Bayesian approach to estimate the distribution, and we formulate a stagewise optimization problem to allocate the computing budget. We characterize the optimality conditions for the stagewise problem by applying the large deviations theory to maximize the decay rate of the probability of false selection. Based on the optimality conditions and combined with the updating of distribution estimates, we design two sequential budget allocation procedures for R&amp;S under streaming input data. We theoretically guarantee the consistency and asymptotic optimality of the proposed procedures. We demonstrate the practical efficiency through numerical experiments in comparison with the equal allocation policy and an extension of the optimal computing budget allocation algorithm. Funding: The authors gratefully acknowledge the support of the Air Force Office of Scientific Research [Grant FA9550-22-1-0244], the National Science Foundation [Grant NSF-DMS2053489], and the NSF AI Institute for Advances in Optimization under [Grant NSF-2112533]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2024.0035 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2024.0035},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {1-19},
  shortjournal = {INFORMS J. Optim.},
  title        = {Optimal computing budget allocation for data-driven ranking and selection},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="isre---31">ISRE - 31</h2>
<ul>
<li><details>
<summary>
(2025). Research spotlights. <em>ISRE</em>, <em>36</em>(1), iii–xi.
(<a href="https://doi.org/10.1287/isre.2025.resspot.v36.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2025.resspot.v36.n1},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {iii-xi},
  shortjournal = {Inf. Syst. Res.},
  title        = {Research spotlights},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nudge to credible information as a countermeasure to
misinformation: Evidence from twitter. <em>ISRE</em>, <em>36</em>(1),
621–636. (<a href="https://doi.org/10.1287/isre.2021.0491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fueled by social media, health misinformation is spreading rapidly across online platforms. Myths, rumors, and false information on vaccines are flourishing, and the aftermath can be disastrous. A more concerning trend is that people are increasingly relying on social media to obtain healthcare information and tending to believe what they read on social media. Given the serious consequences of misinformation, this study aims to explore the efficacy of a potential cure for the infodemic we face. Specifically, we focus on a countermeasure that Twitter used, which is to nudge users toward credible information when users search topics for which erroneous information is rampant. This Twitter’s policy is unique, in that the intervention is not about censorship but about redirecting users away from false information and toward facts. Our analysis uses 1,468 news articles that contain misinformation about health topics such as measles, vaccines, and cancer. Our analysis reveals that Twitter’s nudging policy reduces misinformation diffusion. After the policy introduction, a news article that contains misinformation is less likely to start a diffusion process on Twitter. In addition, tweets that contain a link to misinformation articles are less likely to be retweeted, quoted, or replied to, which leads to a significant reduction in the aggregated number of tweets each misinformation article attracts. We further uncover that the observed reduction is driven by the decrease both in original tweet posts—those that first introduce misinformation news articles to the Twitter platform—and in those resharing the misinformation, although the reduction is more significant in resharing posts. Last, we find that the effect is driven primarily by a decrease in human-like accounts that share links to unverified claims but not by a decrease in activities by bot-like accounts. Our findings suggest that a misinformation policy that relies on a nudge to a credible source rather than on censorship can suppress misinformation diffusion. History: Anandasivam Gopal, Senior Editor. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2021.0491 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0491},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {621-636},
  shortjournal = {Inf. Syst. Res.},
  title        = {A nudge to credible information as a countermeasure to misinformation: Evidence from twitter},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring and the cold start problem in digital platforms:
Theory and evidence from online labor markets. <em>ISRE</em>,
<em>36</em>(1), 600–620. (<a
href="https://doi.org/10.1287/isre.2021.0146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many online labor platforms employ reputation systems and monitoring systems to mitigate moral hazard. Whereas reputation systems have the potential to reduce moral hazard, they suffer from the cold-start problem, in which new entrants without an established reputation face a high entry barrier as employers predominantly select workers based on their existing reputation. Monitoring systems, providing employers with direct oversight of workers’ actions, offer a different approach. By tracking and reporting workers’ effort levels, monitoring systems reduce ex post information asymmetry and, thus, lower employers’ expected moral hazard risk from workers. However, unlike reputation systems, monitoring systems do not directly address ex ante information asymmetry, failing to assist employers in identifying the right workers. This inherent limitation raises questions about their effectiveness in resolving the cold-start problem. In this paper, we first propose a stylized theoretical model that characterizes worker entry in the presence of reputation and monitoring systems. Based on a unique data set from a leading online labor platform, we then empirically investigate the effect of monitoring systems on the entry barriers by examining the change in workers’ entry behaviors after the introduction of the monitoring system along with associated project outcomes, which include employers’ hiring preferences, hiring prices, and project performance. We exploit the differential availability of the monitoring system across two project types: time-based projects, for which the monitoring system is accessible, and fixed-price projects, for which it is not. Employing a difference-in-differences estimation with a sample including 9,344 fixed-price projects and 3,118 time-based projects, we report that the introduction of the monitoring system increases the number of bids on time-based projects by 27.8%, and the incremental bids predominantly originate from inexperienced workers who lack platform reputation. We further find that, following the introduction of the monitoring system, employers’ preference for experienced workers diminishes, accompanied by an average reduction of 19.5% in labor costs, whereas we observe no significant decrease in project completion and review rating. Our results collectively suggest that monitoring systems alleviate the cold-start problem in online platforms. History: Xiaoquan (Michael) Zhang, Senior Editor; Beibei Li Associate Editor. Funding: This work was supported by the NET Institute Grant and the Robert Wood Johnson Foundation (RWJF) [Grant 74503]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0146 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0146},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {600-620},
  shortjournal = {Inf. Syst. Res.},
  title        = {Monitoring and the cold start problem in digital platforms: Theory and evidence from online labor markets},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KETCH: A knowledge-enhanced transformer-based approach to
suicidal ideation detection from social media content. <em>ISRE</em>,
<em>36</em>(1), 572–599. (<a
href="https://doi.org/10.1287/isre.2021.0619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suicidal ideation (SI), as a psychiatric emergency, requires immediate assistance and intervention. Most people with SI do not actively seek help from mental health professionals, which may result in irreversible consequences. Research has shown that individuals experiencing SI increasingly express their thoughts and emotions on social media platforms, making the latter a viable venue for suicidal ideation detection (SID). This paper proposes, develops, and evaluates a knowledge-enhanced transformer-based approach (KETCH) to SID from social media content. KETCH comprises several key novel design artifacts, including a social media-oriented SI lexicon, a model-level method for integrating domain knowledge (i.e., lexicon) into a state-of-the-art transformer, and aligned dynamic embedding and lexicon-based enhancement that integrate domain relevance and contextual importance of terms to effective SID. We evaluate KETCH’s performance with social media data in two different languages collected from distinct platforms, and further examine its generalizability to user-level models for suicide risk prediction and depression detection. The results demonstrate the superior effectiveness, robustness, and generalizability of KETCH via a series of empirical evaluation and a field study. Our research makes multifold research contributions and opens up practical opportunities for timely detection and proactive intervention of SI, which can have far-reaching impacts on public health, the economy, and society. History: Olivia Liu Sheng, Senior Editor; Huimin Zhao, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0619 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0619},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {572-599},
  shortjournal = {Inf. Syst. Res.},
  title        = {KETCH: A knowledge-enhanced transformer-based approach to suicidal ideation detection from social media content},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast forecasting of unstable data streams for on-demand
service platforms. <em>ISRE</em>, <em>36</em>(1), 552–571. (<a
href="https://doi.org/10.1287/isre.2023.0130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On-demand service platforms face a challenging problem of forecasting a large collection of high-frequency regional demand data streams that exhibit instabilities. This paper develops a novel forecast framework that is fast and scalable and automatically assesses changing environments without human intervention. We empirically test our framework on a large-scale demand data set from a leading on-demand delivery platform in Europe and find strong performance gains from using our framework against several industry benchmarks across all geographical regions, loss functions, and both pre- and post-COVID periods. We translate forecast gains to economic impacts for this on-demand service platform by computing financial gains and reductions in computing costs. History: Olivia Liu Sheng, Senior Editor; Huimin Zhao, Associate Editor. Funding: I. Wilms was financially supported by the Dutch Research Council (NWO) [Grant VI.Vidi.211.032]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2023.0130 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2023.0130},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {552-571},
  shortjournal = {Inf. Syst. Res.},
  title        = {Fast forecasting of unstable data streams for on-demand service platforms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customer acquisition via explainable deep reinforcement
learning. <em>ISRE</em>, <em>36</em>(1), 534–551. (<a
href="https://doi.org/10.1287/isre.2022.0529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective customer acquisition heavily hinges on sequential targeting to ensure that appropriate marketing messages reach customers. Sequential targeting could guide customers through the acquisition process and thus, optimize long-term revenue for the firm. Toward this goal, reinforcement learning (RL) has demonstrated great potential in facilitating sequential targeting during user acquisition. However, decisions made by RL during this process often lack explainability. We introduce the deep recurrent Q-network with attention model, which optimizes the long-term reward of sequential targeting while enhancing the explainability of the decisions. The key idea of the proposed model is to revise Q-learning by adding an attention mechanism to create a bottleneck, forcing the model to focus on features of the next ad exposure that will lead to optimal long-term rewards. We estimate our model using a comprehensive data set from a digital bank. The empirical results show that the proposed model is explainable and also outperforms state-of-the-art methods in terms of long-term revenue optimization. Specifically, the attention mechanism within the model functions as forward planning. The forward planning can spot those features in the next ad exposure that are more likely to lead to the optimal outcome. We further demonstrate how the model makes targeting decisions of advertising channel choices by showing that the model can (1) learn optimal ad channels to target customers from different industries, (2) adjust advertising channels in response to dynamic customer behaviors, and (3) learn the seasonality of the customer’s industry and calibrate the ad channel correspondingly. History: Olivia Liu Sheng, Senior Editor; Huimin Zhao, Associate Editor. Funding: The authors thank General Research Fund [Grant 16500022] of Hong Kong Research Grants Council, [Grant RMGS20EG01-F] from HKUST, and [Grant WEB19EG01-F] for supporting this research. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0529 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0529},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {534-551},
  shortjournal = {Inf. Syst. Res.},
  title        = {Customer acquisition via explainable deep reinforcement learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing online users’ suspicion of sponsored search
results: Effects of informational cues. <em>ISRE</em>, <em>36</em>(1),
508–533. (<a href="https://doi.org/10.1287/isre.2021.0364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sponsored search results (SSRs) that deviate from users’ search queries often raise suspicion despite receiving positive evaluations from previous users. Such a suspicion typically prompts users to avoid SSRs. To address this issue, our study focuses on the role of online informational cues, such as product reviews and ratings from user-generated content, in reducing users’ suspicion when encountering these SSRs. Drawing on the theoretical perspective of state suspicion, we contextualize the dimensions of suspicion in the SSR context, including decision uncertainty , perceived malintent of the search platform , and processing of an SSR . We propose theory-based strategies for reducing the suspicion of users by incorporating informational cues about an SSR in the product search process in e-commerce contexts. Specifically, we theorize that the internalization of an informational cue can reduce users’ decision uncertainty and/or their perceived malintent of the platform, which will increase their processing of an SSR. Our approach also considers contingent factors that trigger users’ internalization. We conducted three laboratory experiments, and the results support our theorization. Our study uncovers the internalization mechanism of informational cues in addressing the suspicion of users in online information search contexts and offers practical implications to e-commerce platforms to facilitate users’ decision-making processes. History: Jason Bennett Thatcher, Senior Editor; J.J. Hsieh, Associate Editor. Funding: The work described in the paper was partially supported by grants from the National Natural Science Foundation of China [Grants 72272109, 72091210, 72091214, and 72071172], the Research Grants Council of Hong Kong S.A.R. [Grant CUHK-11501721], and the Hong Kong Polytechnic University Start-Up [Grant P0044215].},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0364},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {508-533},
  shortjournal = {Inf. Syst. Res.},
  title        = {Addressing online users’ suspicion of sponsored search results: Effects of informational cues},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving students’ argumentation skills using dynamic
machine-learning–based modeling. <em>ISRE</em>, <em>36</em>(1), 474–507.
(<a href="https://doi.org/10.1287/isre.2021.0615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation is an omnipresent rudiment of daily communication and thinking. The ability to form convincing arguments is not only fundamental to persuading an audience of novel ideas but also plays a major role in strategic decision making, negotiation, and constructive, civil discourse. However, humans often struggle to develop argumentation skills, owing to a lack of individual and instant feedback in their learning process, because providing feedback on the individual argumentation skills of learners is time-consuming and not scalable if conducted manually by educators. Grounding our research in social cognitive theory, we investigate whether dynamic technology-mediated argumentation modeling improves students’ argumentation skills in the short and long term. To do so, we built a dynamic machine-learning (ML)–based modeling system. The system provides learners with dynamic writing feedback opportunities based on logical argumentation errors irrespective of instructor, time, and location. We conducted three empirical studies to test whether dynamic modeling improves persuasive writing performance more so than the benchmarks of scripted argumentation modeling (H1) and adaptive support (H2). Moreover, we assess whether, compared with adaptive support, dynamic argumentation modeling leads to better persuasive writing performance on both complex and simple tasks (H3). Finally, we investigate whether dynamic modeling on repeated argumentation tasks (over three months) leads to better learning in comparison with static modeling and no modeling (H4). Our results show that dynamic behavioral modeling significantly improves learners’ objective argumentation skills across domains, outperforming established methods like scripted modeling, adaptive support, and static modeling. The results further indicate that, compared with adaptive support, the effect of the dynamic modeling approach holds across complex (large effect) and simple tasks (medium effect) and supports learners with lower and higher expertise alike. This work provides important empirical findings related to the effects of dynamic modeling and social cognitive theory that inform the design of writing and skill support systems for education. This paper demonstrates that social cognitive theory and dynamic modeling based on ML generalize outside of math and science domains to argumentative writing. History: Gautam Pant, Senior Editor; Hong Guo, Associate Editor. Funding: T. Wambsganss acknowledges funding by a SNSF Doc.Mobility grant [Grant 200207] to collaborate with K. Koedinger from Carnegie Mellon University. J. M. Leimeister [Grant 192718] and A. Janson [Grant 221281] also acknowledge funding from the SNSF. The authors furthermore thank the German Federal Ministry of Education and Research [BMBF, Grant 16DHBKI073] for partly funding this research. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0615 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0615},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {474-507},
  shortjournal = {Inf. Syst. Res.},
  title        = {Improving students’ argumentation skills using dynamic machine-Learning–Based modeling},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of “retail media” on online marketplaces:
Insights from a field experiment. <em>ISRE</em>, <em>36</em>(1),
456–473. (<a href="https://doi.org/10.1287/isre.2022.0188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertising on e-commerce marketplaces, wherein sponsored product listings are interleaved with organic product listings in the search results, is a large and growing phenomenon that falls under the umbrella of “retail media.” In this paper, taking the perspective of the marketplace, we obtain insights into the impact of sponsored listings being shown at the most salient positions in the list of results. To do so, we analyze data from a large-scale field experiment at Flipkart, a leading online marketplace in India. We find nuanced results that substantially vary across categories. In the electronics category, the sponsored listings perform worse (in terms of clicks and conversions) than the organic listings that they replace, whereas the organic listings in the neighborhood of the sponsored listings perform better than in the absence of the sponsored listings. Surprisingly, these effects are reversed in the clothing and footwear categories, in which the ads perform better than the displaced organic listings, suggesting that sponsored listings might help the platform identify new high-relevance products and improve search rankings for these categories. However, at the search level, because of the countervailing impacts on sponsored listings and neighboring organic listings (even though the directions of these effects are different for different categories), we find that increasing the fraction of sponsored listings (by about 10% points) does not affect clicks and conversions in any product category. This implies that ads bring in additional revenue for the marketplace yet do not hurt overall consumer response (in the short run). We theorize that the variation across categories occurs because of differing degrees of information asymmetry on product relevance to a query between the marketplace and the independent sellers of listed products and provide supporting evidence for this mechanism. History: Param Singh, Senior Editor; Yan Huang, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0188 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0188},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {456-473},
  shortjournal = {Inf. Syst. Res.},
  title        = {The impact of “Retail media” on online marketplaces: Insights from a field experiment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conversation analytics: Can machines read between the lines
in real-time strategic conversations? <em>ISRE</em>, <em>36</em>(1),
440–455. (<a href="https://doi.org/10.1287/isre.2022.0415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategic conversations involve one party with an informational advantage and another with an interest in the information. This paper proposes machine learning–based methods to quantify the evasiveness and incoherence of the more-informed party during real-time strategic conversations. To demonstrate the effectiveness of these methods in a real-world setting, we consider the question-and-answer sessions of earnings conference calls, during which managers face scrutinizing questions from analysts. Being reluctant to disclose adverse information, managers may resort to evasive answers and sometimes respond less coherently than they otherwise would. Using data from the earnings calls of S&amp;P 500 companies from 2006 to 2018, we show that the proposed measures predict worse next-quarter earnings. The stock market also perceives incoherence as a negative signal. This paper contributes methodologically to business analytics by developing machine learning methods to extract behavioral cues from real-time strategic conversations. We believe the wide adoption of these tools can increase the efficiency of various markets and institutions where real-time strategic conversations routinely occur, which ultimately benefits business and society. History: Ram Gopal, Senior Editor; Atanu Lahiri, Associate Editor.},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0415},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {440-455},
  shortjournal = {Inf. Syst. Res.},
  title        = {Conversation analytics: Can machines read between the lines in real-time strategic conversations?},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Firm-sponsored online communities: Building alignment
capabilities for participatory governance. <em>ISRE</em>,
<em>36</em>(1), 419–439. (<a
href="https://doi.org/10.1287/isre.2021.0578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firm-sponsored online communities face governance challenges because of differences in the goals and values between the community and the sponsor. This study focuses on better understanding how these communities combine firm-based and community-based governance to achieve participatory governance, balancing firm goals with the autonomy and participation of volunteer members. Through a case study of Mayo Clinic Connect, an online patient community sponsored by Mayo Clinic, we uncover how firm and community stakeholders jointly align firm- and community-based governance. We introduce the concept of a governance alignment capability, which helps avoid and resolve governance tensions, contributing to the success of participatory governance. We specifically uncover two governance alignment mechanisms: informalizing the formal and formalizing the informal. These mechanisms adapt firm-based governance to align more closely with community norms and standardize community-based governance within the firm’s governance structure. The study contributes to both the nascent literature on firm-sponsored online communities and the broader literature on governance by highlighting the importance of participatory governance capabilities in balancing effectiveness and legitimacy in firm-sponsored online communities and emphasizing governance as a dynamic, rather than a static, system. History: Manju K. Ahuja, Senior Editor; Choon-Ling Sia, Associate Editor. Funding: H. Safadi acknowledges financial support from the Terry-Sanford Research Award. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0578 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0578},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {419-439},
  shortjournal = {Inf. Syst. Res.},
  title        = {Firm-sponsored online communities: Building alignment capabilities for participatory governance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 1 + 1 &gt; 2? Information, humans, and machines.
<em>ISRE</em>, <em>36</em>(1), 394–418. (<a
href="https://doi.org/10.1287/isre.2023.0305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of data and the rapid rise of artificial intelligence and automated working processes, humans inevitably fall into increasingly close collaboration with machines as either employees or consumers. Problems in human–machine interaction arise as a consequence, not to mention the dilemmas posed by the need to manage information on ever-expanding scales. Considering the general superiority of machines in this latter respect, compared with human performance, it is essential to explore whether human–machine collaboration is valuable and, if so, why. Recent studies propose diverse explanation methods to uncover machine learning algorithms’ “black boxes,” aiming to reduce human resistance and enhance efficiency. However, the findings of this literature stream have been inconclusive. Little is known about the influential factors involved or the rationale behind their impacts on human decision processes. We aimed to tackle these issues in the present study by specifically examining the joint impact of information complexity and machine explanations. Specifically, we cooperated with a large Asian microloan company to conduct a two-stage field experiment. Drawing upon studies in dual-process theories of reasoning that propose different conditions necessary to arouse humans’ active information processing and systematic thinking, we tailored the treatments to vary the level of information complexity, the presence of collaboration, and the availability of machine explanations. We observed that, with large volumes of information and with machine explanations alone, human evaluators could not add extra value to the final collaborative outcomes. However, when extensive information was coupled with machine explanations, human involvement significantly reduced the default rate compared with machine-only decisions. We disentangled the underlying mechanisms with three-step empirical analyses. We reveal that the coexistence of large-scale information and machine explanations can invoke humans’ active rethinking, which, in turn, shrinks gender gaps and increases prediction accuracy. In particular, we demonstrate that humans can spontaneously associate newly emerging features with others that had been overlooked but had the potential to correct the machine’s mistakes. This capacity not only underscores the necessity of human–machine collaboration, but also offers insights into system designs. Our experiments and empirical findings provide nontrivial implications that are both theoretical and practical. History: Param Vir Singh, Senior Editor; Tianshu Sun, Associate Editor. Funding: This work was supported by the National Natural Science Foundation of China [Grant 72272003]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2023.0305 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2023.0305},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {394-418},
  shortjournal = {Inf. Syst. Res.},
  title        = {1 + 1 &gt; 2? information, humans, and machines},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guided diverse concept miner (GDCM): Uncovering relevant
constructs for managerial insights from text. <em>ISRE</em>,
<em>36</em>(1), 370–393. (<a
href="https://doi.org/10.1287/isre.2020.0494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided Diverse Concept Miner (GDCM) is an interpretable deep learning algorithm to (1) automatically extract corpus-level concepts from text data, (2) focus the discovery of concepts to filter through only the concepts highly correlated to the user-specified managerial outcome, and (3) quantify the concept’s correlational importance to the outcome. GDCM is used to explore and potentially extract previously unknown concepts and insights from the text that may explain the managerial outcome, without the need to provide any human-predefined guidance or labeled data on concepts. GDCM embeds words, documents, and concepts all in the same vector space, enabling easy interpretation of discovered concepts by associating words local to the concept vector. GDCM is explicitly configured to increase recovered-concept diversity, coherence, and relevance to managerial outcomes. We demonstrate GDCM as a “guided exploratory” tool for a hypothetical managerial case involving online purchase journey data connected to consumed reviews. GDCM scalably extracts concepts hidden in customer reviews highly correlated to conversion and provides concept importance in comparison with product ratings. Concepts produced turn out to be product qualities previously theorized to impact conversion in the literature, and correlational importance gauged by GDCM closely matches estimates from a previous causal study run on a similar data set, serving as external validations of GDCM as a “guided exploratory” tool. Additional experiments with other data show that extracted insights are sensitive to guiding managerial variables and sensibly so, further demonstrating the flexibility of GDCM as a managerial tool. History: Ahmed Abbasi, Senior Editor; Huimin Zhao, Associate Editor. Funding: The authors acknowledge funding from the Marketing Science Institute [Grant 4000562] and Nvidia Academic Hardware Grant Program for research. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2020.0494 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2020.0494},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {370-393},
  shortjournal = {Inf. Syst. Res.},
  title        = {Guided diverse concept miner (GDCM): Uncovering relevant constructs for managerial insights from text},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Growing technological relatedness to the ICT industry and
its impacts. <em>ISRE</em>, <em>36</em>(1), 344–369. (<a
href="https://doi.org/10.1287/isre.2020.0627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We create an intersectoral citation network using 1.3 million patents granted between 1981 and 2010 across all industries in the United States. In this network space, we find a significant increase over time in the proximity of non–information and communication technologies (ICT) industries to the ICT industry. Such “ICT-closeness” of an industry relates to a significant increase in the proportion of ICT technologies in the industry’s patent portfolio and complementarity between ICT and non-ICT patents in its portfolio. We hypothesize and find that ICT-Closeness has a significant positive impact on innovation outcomes of the citing industry, including innovative efficiency, recombinant capabilities, and creation of new products, services, and business methods. ICT-closeness also engenders hypercompetition in the citing industries, as reflected in greater winner-take-all dynamics and turbulence. Our results point to a paradigm shift in knowledge production, and in turn, sources of competitive value across diverse industries. History: Ravi Bapna, Senior Editor; Sunil Wattal, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2020.0627 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2020.0627},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {344-369},
  shortjournal = {Inf. Syst. Res.},
  title        = {Growing technological relatedness to the ICT industry and its impacts},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaining a seat at the table: Enhancing the attractiveness of
online lending for institutional investors. <em>ISRE</em>,
<em>36</em>(1), 326–343. (<a
href="https://doi.org/10.1287/isre.2022.0638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although online lending enjoyed explosive growth in the past decade, its market size remains small compared with other financial assets. The risk of losing money, stringent government regulations, and low awareness of the benefits have hampered the realization of the full potential of the online lending market. Because online loans are an emerging asset class, investors may not be aware of the investment performance of online loans compared with other assets, and it remains an open question whether online loans offer sufficiently attractive returns to warrant inclusion in an asset allocation decision. To attract lenders, platforms must provide an appealing investment opportunity which entails construction of portfolios of loans that investors find attractive. We propose general characteristics-based portfolio policy (GCPP), a novel framework to overcome the difficulties associated with portfolio construction of loans. GCPP directly models the portfolio weight of a loan as a flexible function of its characteristics and does not require direct estimation of the distributional properties of loans. Using an extensive data set spanning over one million loans from 2013 to 2020 from LendingClub, we show that GCPP portfolios can achieve an average annualized internal rate of return (IRR) of 8.86%–13.08%, significantly outperforming an equal-weight portfolio of loans. We then address the question of whether online loans can earn competitive rates of return compared with traditional investment vehicles through six market indices covering stocks, bonds, and real estate. The results demonstrate that a portfolio of online loans earns competitive or higher rates of return compared with traditional asset classes. Furthermore, the IRRs of the loan portfolios have small correlations with the benchmark index IRRs, pointing toward significant diversification benefits. Together, we demonstrate that GCPP is an approach that can help platforms better serve both borrowers and lenders en route to growing their business. History: Pei-Yu Chen, Senior Editor; Martin Bichler, Associate Editor. Funding: The work described in this paper was partially supported by the Gillmore Centre for Financial Technology at Warwick Business School, InnoHK initiative, The Government of the HKSAR, and Laboratory for AI-Powered Financial Technologies. X. Qiao acknowledges financial support from the Research Grants Council of the Hong Kong SAR, China [Grant CityU 21500422]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0638 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0638},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {326-343},
  shortjournal = {Inf. Syst. Res.},
  title        = {Gaining a seat at the table: Enhancing the attractiveness of online lending for institutional investors},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On-demand, long-term, or hybrid? An economic analysis of
optimal rental models on sharing platforms. <em>ISRE</em>,
<em>36</em>(1), 307–325. (<a
href="https://doi.org/10.1287/isre.2022.0441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapidly growing sharing economy is characterized by improved resource utilization through peer-to-peer transactions. Sharing platforms usually adopt one of three rental models, long-term, on-demand, and hybrid, where the long-term rental requires full dedication of resources to market over a predefined period, the on-demand rental allows resources to be exchanged for a fraction of the period, and the hybrid rental model provides both options. We develop an analytical model consisting of a monopolistic sharing platform that connects renters with owners who have heterogeneous asset utilization and incur different setup and transaction costs of participation under different rental models. We identify the conditions under which each of the three rental models can emerge as the platform’s optimal rental strategy and further examine their impacts on social welfare and consumer surplus. We find that the relative setup cost and the relative transaction cost between owners and renters play a crucial role in shaping the equilibrium market price and optimality of the three rental models, whereas the total costs determine the equilibrium transaction volume and sustainability of the three rental models. Only when the total cost is low can all three models be sustainable, and each model could emerge to be optimal in the presence of a moderately high relative transaction cost on the owners’ side. Specifically, when the owners’ setup cost is significantly higher than the renters’, the on-demand model is optimal. When the owners’ setup cost is significantly lower than the renters’ and the renter’s setup cost is low, the long-term model is optimal. Only when their setup costs are comparable or when the owners’ setup cost is significantly lower than the renters’ and the renter’s setup cost is high can the hybrid model be optimal. When the platform optimally chooses the long-term or hybrid model, it also generates the highest social welfare, and in a wide range of parameter regions, it also yields the highest consumer surplus, leading to a win–win outcome. Overall, our results offer important new insights into the platform’s optimal rental-model choice in the sharing economy and its implications for consumers and society. History: Juan Feng, Senior Editor; Hong Xu, Associate Editor. Funding: N. Feng acknowledges financial support from the National Natural Science Foundation of China [Grants 72231004 and 72394373]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0441 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0441},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {307-325},
  shortjournal = {Inf. Syst. Res.},
  title        = {On-demand, long-term, or hybrid? an economic analysis of optimal rental models on sharing platforms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Punished for success? A natural experiment of displaying
clinical hospital quality on review platforms. <em>ISRE</em>,
<em>36</em>(1), 285–306. (<a
href="https://doi.org/10.1287/isre.2021.0630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The healthcare market faces severe information asymmetry; patients struggle to evaluate the quality of hospitals and make informed decisions about their healthcare. Review platforms (e.g., Yelp) have begun to display the clinical quality of hospitals (alongside consumer reviews) to inform patients about hospital selection. In 2017 and 2019, Yelp introduced a feature with clinical measures of maternity care for hospitals that deliver babies in select markets. We study how clinical quality measures displayed on Yelp—especially for those (clinically) high-quality hospitals—influence subsequent patients’ ratings of hospitals. Our difference-in-differences estimation shows that when clinical quality measures are displayed, high-quality hospitals are surprisingly punished with lower subsequent ratings on Yelp, especially hospitals with low staffing capacity. This novel finding is consequential for hospitals as patient dissatisfaction can jeopardize the federal funding that hospitals receive ( CMS.gov ). To tease out the underlying mechanism, we queried SafeGraph’s precise foot traffic data, and we observed a significant patient surge for hospitals that have high maternity care clinical scores displayed on Yelp. We used transfer deep learning to show that because of the patient surge, (only) hospitals with high maternity scores that were short staffed received significantly more negative patient reviews and more complaints about key hospital service areas, thus driving patient dissatisfaction and lower ratings. We contribute to theory and practice by elucidating the role of publicly displaying clinical quality measures in patient (dis-)satisfaction with hospitals. History: Eric Zheng, Senior Editor; Atanu Lahiri, Associate Editor. Funding: This research was supported by the Behavioral Research Assistance Grant, generously provided by the C. T. Bauer College of Business, University of Houston. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0630 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0630},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {285-306},
  shortjournal = {Inf. Syst. Res.},
  title        = {Punished for success? a natural experiment of displaying clinical hospital quality on review platforms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blessing or curse? Implications of data brokers for
publisher competition. <em>ISRE</em>, <em>36</em>(1), 261–284. (<a
href="https://doi.org/10.1287/isre.2022.0227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The abundance of consumer data has given rise to a new data broker industry that plays a pivotal role in targeted advertising. Many publishers rely on data brokers to gain (i) individual insights drawn from their own data or (ii) collective insights drawn from both their data and those of their competitors, thus improving their targeting capabilities. As data brokers control large volumes of data, they can govern how data insights are sold to downstream publishers. Despite their importance, prior studies provide little insight into the strategic function of data brokers and their subsequent impact on downstream competition. To address this research gap, we build an analytical model to analyze the interactions among a data broker, two competing publishers, and advertisers. Our analysis yields several novel findings. First, we show that the data broker may strategically set prices in a manner that leads to the exclusive selling of collective insights to one publisher, as opposed to nonexclusive selling involving two publishers. This result implies that it may be advantageous for the data broker to pursue an exclusive strategy that makes data insights scarcer for some firms. Second, we illustrate that competition between publishers can be either facilitated or impeded by the data broker depending on the broker’s interests when selling the data insights. Finally, the data broker may induce both competition-mitigation and value-enhancing effects that affect aggregate welfare. Interestingly, although the data broker’s decision (i.e., exclusive selling) may reduce market competition, the aggregate welfare can still improve because collective insights allow publishers to increase their targeting value for advertisers more efficiently. History: Giri Kumar Tayi, Senior Editor; Atanu Lahiri, Associate Editor. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72301267, 71921001, 72091215/72091210]. This work was supported by Research Grants Council, University Grants Committee, Hong Kong [CityU 11502319]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0227 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0227},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {261-284},
  shortjournal = {Inf. Syst. Res.},
  title        = {Blessing or curse? implications of data brokers for publisher competition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How hospitals differentiate health information technology
portfolios for clinical care efficiency: Insights from the HITECH act.
<em>ISRE</em>, <em>36</em>(1), 239–260. (<a
href="https://doi.org/10.1287/isre.2021.0260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hospitals have implemented health information technology (HIT) for clinical care to address rising operating costs in recent years. We integrate behavioral and institutional perspectives to explain how hospitals differentiate technological search relative to industry peers (i.e., search differentiation) for HIT portfolios. In the context of the U.S. healthcare industry, we theorize that hospitals’ search differentiation for HIT results jointly from idiosyncratic learning in response to cost-based performance shortfalls and isomorphic pressures in relation to changing policy uncertainty as the Health Information Technology for Economic and Clinical Health (HITECH) Act has unfolded. Based on a panel data set from 3,319 hospitals in 2007–2014, we demonstrate that when costs increase relative to aspiration level, a hospital differentiates its search for HIT by exploring more novel technologies for clinical care relative to peers. As policy uncertainty declines from the conceptualization phase to the enactment phase of the HITECH Act, a hospital’s search differentiation for HIT increases to a greater extent in response to cost-based performance shortfalls as lower uncertainty reduces the need to imitate peers’ search. As policy uncertainty further declines from the enactment phase to the enforcement phase of the HITECH Act and reaches its lowest level, however, the hospital’s search differentiation for HIT increases to a smaller extent in response to cost-based performance shortfalls because of policy incentives and professional norms to promote implementation of common technologies. Overall, we provide a more holistic picture of how uncertainty in a dynamic regulatory context intertwines with organizational learning from performance feedback in shaping search differentiation. History: Rajiv Kohli, Senior Editor; Torsten Oliver Salge, Associate Editor. Funding: J. Q. Dong acknowledges the financial support from Nanyang Technological University [Grant SUG 022362-00001]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/isre.2021.0260 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0260},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {239-260},
  shortjournal = {Inf. Syst. Res.},
  title        = {How hospitals differentiate health information technology portfolios for clinical care efficiency: Insights from the HITECH act},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linking clicks to bricks: Understanding the effects of email
advertising on multichannel sales. <em>ISRE</em>, <em>36</em>(1),
225–238. (<a href="https://doi.org/10.1287/isre.2020.0557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Businesses have widely used email ads to directly send promotional information to consumers. Whereas email ads serve as a convenient tool to allow firms to target consumers online, there is little evidence of their multichannel impact on consumer spending in both online and brick-and-mortar stores. In this paper, we utilize a unique high-dimensional data set from one of the world’s largest office supplies retailers to link each consumer’s online behaviors to item-level purchase records in physical stores. We employ a doubly robust estimator that incorporates nonparametric machine learning methods for causal estimation on observational data. Our results show that email ads significantly increase the retailer’s sales across different channels. We also investigate the effects of email ads on diverse consumer behaviors along the purchase funnel and find that increased sales result from increased purchase probability and a wider variety of products purchased by consumers. Further, we examine several moderating factors, such as product types and consumer segments, that influence the multichannel effects of email advertising. Overall, our study provides empirical evidence for the economic impact of email ads on consumer behavior across different channels and the underlying mechanisms thereof. Our findings offer direct implications for multichannel retailers seeking to improve their digital marketing strategies, as well as for policymakers interested in evaluating the economic impact of prevalent email advertising. History: Eric Zheng, Senior Editor; Khim Yong Goh, Associate Editor. Funding: This work was supported by the Marketing Science Institute [Grant 4000086]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2020.0557 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2020.0557},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {225-238},
  shortjournal = {Inf. Syst. Res.},
  title        = {Linking clicks to bricks: Understanding the effects of email advertising on multichannel sales},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Signaling effects under dynamic capacity in online matching
platforms: Evidence from online health consultation communities.
<em>ISRE</em>, <em>36</em>(1), 202–224. (<a
href="https://doi.org/10.1287/isre.2021.0150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Match formation is challenging in online matching platforms where suppliers are subject to dynamic capacity constraints. We provide a theoretical foundation for understanding how online matching platforms support the transmission and triangulation of multisource information for consumers to infer provider service quality and dynamic capacity states, and achieve desirable matching outcomes. Situating this study in the context of an online health consultation community (OHCC) and drawing upon signaling theory, we theorize how physicians’ owned and earned signals influence physicians’ voluntary online consultations with new patients they have not consulted with previously. Importantly, we articulate how these signaling effects are contingent upon physicians’ dynamic capacity in OHCC. We collected longitudinal data from a large OHCC in China and used a hidden Markov model (HMM) to characterize the dynamic physician capacity in the OHCC and test the hypotheses. Our findings reveal that service professionals’ owned and earned signals interactively work together to balance supply and demand dynamically, and thereby facilitating matchmaking. In OHCCs, where physicians provide voluntary service beyond their primary jobs at hospitals, we find that owned and earned signals increase patient consultations in different patterns contingent upon physicians’ capacity states. In addition, we discover the complementary and substitute relationships between owned signals and earned signals change when physicians are in different capacity states. The findings have significant implications for our understanding of online match formation under dynamic capacity constraints and the design of OHCCs. History: Yulin Fang, Senior Editor; Choon Ling Sia, Associate Editor. Funding: X. Guo acknowledges the support from the National Natural Science of China [Grants 72125001, 72071054, 72293584]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0150 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0150},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {202-224},
  shortjournal = {Inf. Syst. Res.},
  title        = {Signaling effects under dynamic capacity in online matching platforms: Evidence from online health consultation communities},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile push vs. Pull targeting and geo-conquesting.
<em>ISRE</em>, <em>36</em>(1), 184–201. (<a
href="https://doi.org/10.1287/isre.2021.0206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the impact of different mobile content delivery mechanisms on consumers’ coupon redemption behavior. Firms have two distinct content delivery options when engaging with consumers’ mobile devices: mobile push and mobile pull . Mobile push delivers firm-initiated (ad) content directly to consumers, whereas mobile pull requires consumers to initiate requests for (ad) content. We hypothesize that mobile push delivery increases the likelihood of coupon redemption due to reduced app-specific search costs compared with mobile pull. We further examine how app-specific use experience and store density influence the heterogeneity of consumer responses. To test our hypotheses, we conducted a large-scale randomized field experiment in a geo-conquesting setting, targeting customers located around competitor retail stores with mobile coupons to drive them to stores of the focal retailer. Our results reveal that mobile push increases coupon redemption rates by 6.0%, with substantial heterogeneity based on app-specific use experience and store density. Notably, app-specific usage experience negatively moderates the effect of mobile push delivery on redemptions, likely because both usage experience and push notifications reduce app-specific search costs, thereby acting as substitutes for one another. In areas with higher store density, the positive effect of mobile push delivery on the redemption likelihood is greater, suggesting that push notifications can highlight the focal coupon among alternative store choices, thereby lowering consumer switching costs. These findings have important implications for retailers and brands in creating competitive mobile targeting campaigns that effectively leverage both mobile push and pull delivery mechanisms. History: Juan Feng, Senior Editor; Khim Yong Goh, Associate Editor. Funding: Financial support was provided by the Deutsche Forschungsgemeinschaft [Project 414986791]. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/isre.2021.0206 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0206},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {184-201},
  shortjournal = {Inf. Syst. Res.},
  title        = {Mobile push vs. pull targeting and geo-conquesting},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing user privacy through ephemeral sharing design:
Experimental evidence from online dating. <em>ISRE</em>, <em>36</em>(1),
162–183. (<a href="https://doi.org/10.1287/isre.2021.0379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users on online dating platforms tend to encounter a cold-start problem, with limited user engagement in the initial stages of the matching process; this is partially due to privacy concerns. In this study, we propose ephemeral sharing as a privacy-enhancing design to strike a balance between users’ privacy concerns and the need for voluntary information disclosure. Ephemeral sharing refers to a digital design in which the information shared (e.g., a personal photo) becomes invisible and irretraceable to the receiver shortly after the receipt of such information. In partnership with an online dating platform, we report a large-scale randomized field experiment with more than 70,000 users to understand how ephemeral sharing influences users’ disclosure of personal photos, match outcome, and receiver engagement. The experiment features a treatment group in which subjects can upload an ephemeral photo along with their matching request and a control group in which subjects can instead upload a persistent photo. We find that users in the treatment group send more personal photos (and ones with human faces) compared with users in the control group. Additionally, the ephemeral sharing treatment leads to a higher number of matches and a higher level of receiver engagement. Further analyses suggest that the treatment effects are more salient for privacy-sensitive senders. Moreover, we find that the treatment effects on match outcome and receiver engagement can be explained by increases in the disclosure of personal photos. Last, through an online experiment, we show that ephemeral sharing increases disclosure intention by reducing privacy concerns related to data collection, dissemination, and identity abuse. Our study contributes to the literature and practice on privacy-enhancing designs for online matching platforms. History: Alessandro Acquisti, Senior Editor; Beibei Li, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0379 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0379},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {162-183},
  shortjournal = {Inf. Syst. Res.},
  title        = {Enhancing user privacy through ephemeral sharing design: Experimental evidence from online dating},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding lenders’ investment behavior in online
peer-to-peer lending: A construal level theory perspective.
<em>ISRE</em>, <em>36</em>(1), 141–161. (<a
href="https://doi.org/10.1287/isre.2020.0428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online peer-to-peer lending (i.e., P2P lending) has grown rapidly in recent years and is a new source of fixed income for investors. However, there is limited understanding of factors affecting individual lenders’ decision making in this context, which is characterized as highly risky. Drawing on construal level theory, we theorize how bidding amounts lenders submit are affected by interest rates and psychological distance caused by the borrower’s demographic attributes (i.e., geographic location, age, educational degree, and marital status) relative to those of the lender. Specifically, we study how psychological distance directly influences and shapes the effects of the duality of interest rates (i.e., as rates of return and as signals of potential risk) on bidding amounts. Using a rich data set from a popular Chinese online P2P lending platform, we apply multiple identification strategies and estimation methods to conduct our analyses. We find that geographic distance decreases the lenders’ bidding amounts (i.e., home bias effect ), whereas social distance increases the bidding amounts (i.e., social distance effect ). In addition, the positive effects of interest rates on bidding amounts are strengthened by the geographic and social distance between the lender and the borrower. Furthermore, we conduct four controlled experiments to explore the causality and mechanisms behind these relationships. Theoretical contributions and practical implications are discussed. History: Yulin Fang, Senior Editor; Wenjing Duan, Associate Editor. Funding: This work was supported by National Natural Science Foundation of China [Grants 72172103, 72231004, 72325002, 72022007, 71802147, and 71872080], Shenzhen Science and Technology Program [Grants JCYJ20220530112800001 and 2021CX020219], and National Key Research and Development Program of China [Grant 2022YFC3303304]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2020.0428 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2020.0428},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {141-161},
  shortjournal = {Inf. Syst. Res.},
  title        = {Understanding lenders’ investment behavior in online peer-to-peer lending: A construal level theory perspective},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regulating powerful platforms: Evidence from commission fee
caps. <em>ISRE</em>, <em>36</em>(1), 126–140. (<a
href="https://doi.org/10.1287/isre.2022.0191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform giants typically possess strong power over other participants on the platforms. Such power asymmetry gives platform owners the edge on setting platform fees to capture the surplus created on their platforms. Although there is a heated debate on regulating these powerful platforms, the lack of empirical studies hinders the progress toward evidence-based policymaking. This research empirically investigates this regulatory issue in the context of on-demand delivery. Delivery platforms (e.g., DoorDash) charge restaurants a commission fee, which can be as high as 30% per order. To support small businesses, recent regulatory scrutiny has started to cap the commission fees for independent restaurants. This research empirically evaluates the effectiveness of platform fee regulation, by investigating recent regulations across 14 cities and states in the United States. Our analyses show that independent restaurants in regulated cities (i.e., those paying reduced commission fees) experience a decline in orders and revenue, whereas chain restaurants (i.e., those paying the original fees) see an increase in orders and revenue. This intriguing finding suggests that chain restaurants, not independent restaurants, benefit from the regulations that were intended to support independent restaurants. We find that platforms’ discriminative responses to the regulation may explain the negative effects on independent restaurants. That is, after cities enact commission fee caps, delivery platforms become less likely to recommend independent restaurants to consumers, and instead turn to promoting chain restaurants. Moreover, delivery platforms increase their delivery fees for consumers in regulated cities, suggesting that these platforms attempt to cover the loss of commission revenue by charging customers more. History: Anandasivam Gopal, Senior Editor; Pallab Sanyal, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0191},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0191},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {126-140},
  shortjournal = {Inf. Syst. Res.},
  title        = {Regulating powerful platforms: Evidence from commission fee caps},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding volunteer crowdsourcing from a multiplex
perspective. <em>ISRE</em>, <em>36</em>(1), 107–125. (<a
href="https://doi.org/10.1287/isre.2022.0290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing is about leveraging information technologies to outsource tasks to a large group of people, who can either be paid workers or nonpaid workers. Differing from monetarily incentivized workers, nonpaid workers are more likely to be affected by coworking relationships. To explore the link between the network and volunteering behavior, we construct dynamic collaboration networks from 827,260 unique volunteers’ participation in 183,445 projects initiated by 74,556 nonprofit organizations over nine years in the capital city of China. Following a multiplex perspective, we allow each type of organization (i.e., school-based, community-based, other-based) to represent a separate network layer. We construct the measures of multiplex ties (i.e., social connections that are linked through multiple layers) and relational pluralism (i.e., involvement diversity in different layers). We find that volunteers with more multiplex ties and a lower level of relational pluralism have higher volunteering continuity and intensity of engagement, guaranteeing the supply of the volunteer labor force. However, they are less likely to explore unfamiliar organizations through interorganization movements. At a macro level, we show how the reduced interorganization movements impeded the development of small and newly established organizations. In addition, we show that incorporating these multiplexity measures improves the prediction of volunteer behavior by 5.390%, 1.624%, and 8.792% for continuity, movement, and engagement, respectively. History: Xiaoquan (Michael) Zhang, Senior Editor; Heng Xu, Associate Editor. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2022.0290 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0290},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {107-125},
  shortjournal = {Inf. Syst. Res.},
  title        = {Understanding volunteer crowdsourcing from a multiplex perspective},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How recommendation affects customer search: A field
experiment. <em>ISRE</em>, <em>36</em>(1), 84–106. (<a
href="https://doi.org/10.1287/isre.2022.0294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product recommendation and search are two technology-mediated channels through which e-commerce platforms can help customers find products. However, the relationship between the two channels and the underlying mechanisms and implications for platform design are not well understood. We leverage a randomized field experiment with 555,800 customers on a large e-commerce platform to investigate how product recommendation affects customer search. We vary the relevance of the recommendation that users experience upon arriving at the home page of the platform and find that a decrease in recommendation relevance leads to a significant increase in consumers’ use of the search channel, indicating a (partial) substitution effect between the two at the aggregate level. We find substantial heterogeneity across product categories, propose a conceptual framework, and theorize how different states of customer demand—demand fulfillment and demand formation—may drive such heterogeneity. The results are aligned with our framework and provide evidence that both demand formation and fulfillment are at work in the channel interactions between recommendation and search. Specifically, when customers receive more product recommendations in a category, they search more in that category with generic query words, which indicates complementarity between recommendation and search. However, when customers receive fewer product recommendations in a category of interest, they compensate for this reduction by searching more in that category with long-tail query words, which indicates a substitution between recommendation and search. This experimental study is among the first to examine the causal relationship between the recommendation channel and search channel and offers implications for the design of e-commerce platforms. History: Bin Gu, Senior Editor; Dokyun Lee, Associate Editor. Funding: Z. Yuan acknowledges financial support from the National Natural Science Foundation of China [Grants 72141305, 72203202 and 72192803] and the Fundamental Research Funds for the Central Universities. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0294 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0294},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {84-106},
  shortjournal = {Inf. Syst. Res.},
  title        = {How recommendation affects customer search: A field experiment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic content generation and monetization in financial
social media. <em>ISRE</em>, <em>36</em>(1), 61–83. (<a
href="https://doi.org/10.1287/isre.2022.0482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial social media platforms, which rely on social media analysts (SMAs) to contribute content to investors, have emerged as a crucial channel for investors to gain access to financial information and for SMAs to monetize their content. However, we still have a limited understanding of the factors that affect how content is generated and monetized in financial social media platforms. This study focuses on the novel role of investors’ preferences for free/paid content and its sentiment and investigates the extent to which SMAs exhibit strategic content generation and monetization behaviors by catering to and trading off the investors’ preferences. We also evaluate the underlying mechanisms and implications of such strategic behaviors. Utilizing a data set from a financial social media platform based in China, we propose a Bayesian empirical model to jointly analyze the investor’s demand and SMAs’ strategic supply of financial social media content. The model estimation results show that SMAs cater to investors’, especially paid subscribers’, preferences in their content generation such that their strategic behaviors account for 46.20% (24.50%) of the variation in SMAs’ generation decision for free (paid) content sentiment. In addition, an SMA is more likely to produce paid content when the expected free readership increases and is less likely to do so when the expected paid subscriptions increase, evidence that SMAs do balance the preferences of different investors when monetizing content. We find that SMAs are strategic in acquiring readers via their content monetization decisions and retaining subscribers via their content generation decisions. Importantly, we uncover that the orientation of an SMA’s strategic catering behavior is driven by the audience composition effect. Our study provides new empirical evidence, associated theoretical explanations for the results, and a practical illustration of an approach to reduce the potential confirmation bias of investors who may favor information from some SMAs that are prone to strategic catering behaviors. History: Eric Zheng, Senior Editor; Tianshu Sun, Associate Editor. Funding: This research is partially supported by the National Research Foundation, Singapore [Project Grant A-0004920-02-00]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0482 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0482},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {61-83},
  shortjournal = {Inf. Syst. Res.},
  title        = {Strategic content generation and monetization in financial social media},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Making lemonade from lemons: A transaction cost economics
perspective on third-party disruptions in a multivendor information
technology service. <em>ISRE</em>, <em>36</em>(1), 41–60. (<a
href="https://doi.org/10.1287/isre.2022.0033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern enterprise computing environments, multiple information technology (IT) services from first and third parties are often integrated to form coherent solutions for business customers. Using transaction cost economics (TCE) as a theoretical foundation, we seek to understand how uncertainties introduced by third-party services shape enterprise customers’ use of various IT services in these multivendor service settings. Specifically, we analyze a case of service disruption caused by a third party that affects the multivendor service but does not directly affect the first-party services. In line with the tenets of TCE, we find a temporary increase in the use of first-party services that can serve as a similar-goal substitute to fulfill the organization’s needs during the disruption; however, on average, we observe a net decline in the total use of services in the long run. We empirically analyze the role of first-party technical support during the disruption. Based on textual data from the first party’s technical support log, we use deep learning to assess what actions the first party can take during such disruptions to turn the challenge into an opportunity. We find that if the first party offers high-quality technical support that specifically addresses issues related to its product, it may be able to make lemonade out of lemons. Such technical support effectively boosts customers’ use of first-party services in the long run. Curiously, however, similar efforts by the first party in the predisruption period are ineffective in achieving the same effect. History: Bin Gu, Senior Editor; Yili (Kevin) Hong, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0033 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0033},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {41-60},
  shortjournal = {Inf. Syst. Res.},
  title        = {Making lemonade from lemons: A transaction cost economics perspective on third-party disruptions in a multivendor information technology service},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mispricing and algorithm trading. <em>ISRE</em>,
<em>36</em>(1), 21–40. (<a
href="https://doi.org/10.1287/isre.2021.0570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of information technology has fundamentally transformed the way information is processed in the financial market. One such technological advancement is algorithm trading, which allows traders to develop sophisticated strategies based on historical price data. This raises important questions: Do these algorithm trading strategies contribute to market instability? When do they yield profits for different market participants? To address these questions, we must move beyond the efficient market hypothesis, as this theory would suggest that such strategies yield no profit due to market efficiency. Instead, we explicitly incorporate initial market mispricing into our analysis and develop a stylized continuous-time model of algorithm feedback trading to investigate market outcomes. Our model yields closed-form solutions, enabling us to assess the degree to which the price diverges from the efficient level. We discover that algorithmic trading, when combined with initial market mispricing, can lead to significant market volatility, resulting in financial bubbles and crashes. However, this scenario only occurs when there is overpricing and the algorithm traders collectively employ a strategy that enlarges the mispricing. Depending on the initial mispricing in the form of underpricing or overpricing, different algorithm trading strategies (positive or negative) have different market impact, profitability, and policy implications. History: Ram Gopal, Senior Editor; Yan Huang, Associate Editor. Funding: L. Zhang received financial support from the National Key Research and Development Program of China [Grant 2007CB814902], the National Natural Science Foundation of China [Grant 70671061], and the Tsinghua University Initiative Scientific Research Program [Grant 2021THZWJC28]. X. (M.). Zhang received financial support from the Hong Kong Research Grant Council [Grants GRF 14500521, 165052947, 14501320, and 14503818; and TRS: T31-604/18-N]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0570 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0570},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {21-40},
  shortjournal = {Inf. Syst. Res.},
  title        = {Mispricing and algorithm trading},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-centric information systems research on the digital
future of healthcare. <em>ISRE</em>, <em>36</em>(1), 1–20. (<a
href="https://doi.org/10.1287/isre.2025.editorial.v36.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Funding: I. Bardhan is grateful for the financial support provided by The University of Texas at Austin [Charles and Elizabeth Prothro Regents Chair]. E. Oborn is supported in part by the National Institute for Health Research Applied Research Centre Collaborations for Leadership in Applied Health Research and CareWest Midlands [Grant WMCLAHRC-2014-1].},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2025.editorial.v36.n1},
  journal      = {Information Systems Research},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Inf. Syst. Res.},
  title        = {Human-centric information systems research on the digital future of healthcare},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ited---10">ITED - 10</h2>
<ul>
<li><details>
<summary>
(2025). Case—prescriptive analytics for entrepreneurial growth:
Data-driven strategic decision making at iParty bangkok co., ltd.
<em>ITED</em>, <em>25</em>(2), 175–178. (<a
href="https://doi.org/10.1287/ited.2023.0028cs">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0028cs},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {175-178},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Prescriptive analytics for entrepreneurial growth: Data-driven strategic decision making at iParty bangkok co., ltd.},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case article—prescriptive analytics for entrepreneurial
growth: Data-driven strategic decision making at iParty bangkok co.,
ltd. <em>ITED</em>, <em>25</em>(2), 169–174. (<a
href="https://doi.org/10.1287/ited.2023.0028ca">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case centers around the dilemma faced by the female owner of a balloon decoration business: whether to outsource transportation of balloon arrangements to third-party providers or handle the transportation of some orders in house. The case illustrates a path to answering the strategic business question through solving a sequence of vehicle routing problems (VRPs) of increasing complexity. The focus of the case is not only on solving VRPs, but also on teaching a practical framework for (1) framing a business problem as a prescriptive analytics problem; (2) researching appropriate concepts, prior publications, and tools; (3) setting up an optimization problem formulation, estimating the necessary inputs, and obtaining a solution; (4) deciding on the appropriate level of formulation complexity needed; and (5) mapping the results from the models to a response to the original business question. Realistic data and Excel models and are provided. The case is appropriate for use in courses in optimization, operations research, and business analytics at either the advanced undergraduate or master’s/MBA level. Supplemental Material: The Teaching Note and supplemental data are available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0028ca},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {169-174},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Prescriptive analytics for entrepreneurial growth: Data-driven strategic decision making at iParty bangkok co., ltd.},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spreadsheet modeling and wrangling with python.
<em>ITED</em>, <em>25</em>(2), 152–168. (<a
href="https://doi.org/10.1287/ited.2023.0047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A staple of many spreadsheet-based management science courses is the use of Excel for activities such as model building, sensitivity analysis, goal seeking, and Monte-Carlo simulation. What might those things look like if carried out using Python? We describe a teaching module in which Python is used to do typical Excel-based modeling and data-wrangling tasks. In addition, students are exposed to basic software engineering principles, including project folder structures, version control, object-oriented programming, and other more advanced Python skills, creating deployable packages and documentation. The module is supported with Jupyter notebooks, Python scripts, course web pages that include numerous screencasts, and a few GitHub repositories. All of the supporting materials are permissively licensed and freely accessible. Supplemental Material: The supplemental files are available at https://doi.org/10.1287/ited.2023.0047 .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0047},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {152-168},
  shortjournal = {Inf. Syst. Res.},
  title        = {Spreadsheet modeling and wrangling with python},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating how social justice framing for assessments
impacts technical learning. <em>ITED</em>, <em>25</em>(2), 136–151. (<a
href="https://doi.org/10.1287/ited.2022.0030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple studies call for engineering education to integrate social justice into classroom instruction. Yet, there is uncertainty regarding whether integrating these social topics into engineering curriculum will support or detract from the learning of technical concepts. This study focuses on evaluating how reframing technical assessments to include social justice concepts impacts student learning and investigates how well students integrate social justice into engineering decision making. Using a within-subject design, in which students were exposed to both conditions (questions with and without social justice context), we evaluate how social justice framing impacts overall student learning of technical topics. Social justice prompts are added to homework questions, and we assess students’ demonstration of knowledge of original technical content of the course, as well as their ability to consider social justice implications of engineering design. In the earlier homework assignment, the experimental group showed a significant decrease in learning when technical concepts were framed to include social justice. As the students became more familiar with social justice considerations, their learning of technical concepts became comparable to that of students who did not have the social justice components in their assignment. Their evaluation of the social implications of technical decisions also improved. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on DEI in ORMS Classrooms. Funding: This work was supported by the Carnegie Mellon University’s Wimmer Faculty Fellowship and the National Science Foundation [Grant 2053856]. D. Nock also acknowledges support from the Wilton E. Scott Institute for Energy Innovation, where she is an energy fellow. Supplemental Material: The online appendices are available at https://doi.org/10.1287/ited.2022.0030 .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2022.0030},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {136-151},
  shortjournal = {Inf. Syst. Res.},
  title        = {Investigating how social justice framing for assessments impacts technical learning},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case—racial bias in automated traffic law enforcement and
the price of unjustness. <em>ITED</em>, <em>25</em>(2), 128–135. (<a
href="https://doi.org/10.1287/ited.2023.0032cs">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case study has been developed for students to practice their data analysis and optimization skills in a contemporary societal issue: that of injustice in automated traffic law enforcement. Specifically, this case study is for students of modern data analysis and statistical modeling courses that focus on hypothesis testing; it also has a component for students in optimization and mathematical modeling courses that focus on linear and network optimization. The case study has been used since Spring 2023 in a combination of two courses from the Industrial Engineering (Analysis of Data, an introduction to probability and statistics) and Civil Engineering (Transportation Systems, an introduction to mathematical modeling and optimization for civil engineers with a focus on transportation) curricula. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on DEI in ORMS Classrooms.},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0032cs},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {128-135},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Racial bias in automated traffic law enforcement and the price of unjustness},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case article—racial bias in automated traffic law
enforcement and the price of unjustness. <em>ITED</em>, <em>25</em>(2),
122–127. (<a href="https://doi.org/10.1287/ited.2023.0032ca">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case study has been developed for students to practice their data analysis and optimization skills in a contemporary societal issue: that of injustice in automated traffic law enforcement. Specifically, this case study is for students of modern data analysis and statistical modeling courses that focus on hypothesis testing; it also has a component for students in optimization and mathematical modeling courses that focus on linear and network optimization. The case study has been used since Spring 2023 in a combination of two courses from the Industrial Engineering (Analysis of Data, an introduction to probability and statistics) and Civil Engineering (Transportation Systems, an introduction to mathematical modeling and optimization for civil engineers with a focus on transportation) curricula. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on DEI in ORMS Classrooms. Supplemental Material: The Teaching Note and data files are available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0032ca},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {122-127},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Racial bias in automated traffic law enforcement and the price of unjustness},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How has the COVID-19 pandemic affected students’ online
social presence? <em>ITED</em>, <em>25</em>(2), 106–121. (<a
href="https://doi.org/10.1287/ited.2022.0054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 pandemic has affected higher education institutions worldwide as they had to switch from face-to-face to online teaching almost overnight. This abrupt change made a huge impact on teaching, learning, and particularly, student engagement. This paper focuses on online social presence as an element of student engagement, which represents how students feel under synchronous online teaching. A survey was conducted among 244 first-year students to evaluate the impact of online social interaction, online collaboration, online contact with staff, online engagement, and online active learning on online social presence. Structural equation modeling was used to test and evaluate these multivariate relationships. Our study illustrates that all variables have a significant positive relationship with online social presence. In particular, online social interaction and online collaboration show a more powerful relationship with student online social presence. Thus, digital technologies should be adopted in a way that encourages students to actively interact with their peers.},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2022.0054},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {106-121},
  shortjournal = {Inf. Syst. Res.},
  title        = {How has the COVID-19 pandemic affected students’ online social presence?},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case—locating a truck terminal in texas. <em>ITED</em>,
<em>25</em>(2), 99–105. (<a
href="https://doi.org/10.1287/ited.2022.0042cs">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ITED},
  doi          = {10.1287/ited.2022.0042cs},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {99-105},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Locating a truck terminal in texas},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case article—locating a truck terminal in texas.
<em>ITED</em>, <em>25</em>(2), 90–98. (<a
href="https://doi.org/10.1287/ited.2022.0042ca">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case study introduces students to the decision-making process of a facility location problem and requires them to apply appropriate methods to recommend a desirable terminal location for CBT, a transportation provider in the United States. In response to the increasing number of drivers in the Texas market, CBT explored different options to better serve the rising demand, including a new and second terminal in a different geographic area. Students employ multi-criteria decision-making framework and optimization modeling to support their analysis using empirical data. While students use provided data to solve the problem in the base case, they need to research various publicly available databases in the case extension. This case can be used in undergraduate courses in logistics, transportation, operations, and supply chain management to apply facility location related concepts, or in a business analytics course to illustrate optimization modeling in a real-world context. Supplemental Material: Data are available at https://doi.org/10.1287/ited.2022.0042ca . The Teaching Note is available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2022.0042ca},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {90-98},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Locating a truck terminal in texas},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Call for papers—INFORMS transactions on education special
issue on generative AI in teaching management science, operations
research, operations management, and analytics. <em>ITED</em>,
<em>25</em>(2), 89. (<a
href="https://doi.org/10.1287/ited.2024.cfp.v25.n2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ITED},
  doi          = {10.1287/ited.2024.cfp.v25.n2},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {89},
  shortjournal = {Inf. Syst. Res.},
  title        = {Call for Papers—INFORMS transactions on education special issue on generative AI in teaching management science, operations research, operations management, and analytics},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mksc---15">MKSC - 15</h2>
<ul>
<li><details>
<summary>
(2025). Focus on authors. <em>MKSC</em>, <em>44</em>(2), 486–489.
(<a href="https://doi.org/10.1287/mksc.2025.focusonaus.v44.2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2025.focusonaus.v44.2},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {486-489},
  shortjournal = {Market. Sci.},
  title        = {Focus on authors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2024 guest associate editors and ad hoc reviewers.
<em>MKSC</em>, <em>44</em>(2), 483–485. (<a
href="https://doi.org/10.1287/mksc.2025.ack.v44.n2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2025.ack.v44.n2},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {483-485},
  shortjournal = {Market. Sci.},
  title        = {2024 guest associate editors and ad hoc reviewers},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2024 marketing science service awards. <em>MKSC</em>,
<em>44</em>(2), 482. (<a
href="https://doi.org/10.1287/mksc.2025.servawards.v44.n2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2025.servawards.v44.n2},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {482},
  shortjournal = {Market. Sci.},
  title        = {2024 marketing science service awards},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practice prize report: The 2024 gary lilien ISMS practice
prize competition. <em>MKSC</em>, <em>44</em>(2), 478–481. (<a
href="https://doi.org/10.1287/mksc.2024.1172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This report summarizes the finalists of the 2024 Gary Lilien INFORMS Society for Marketing Science Practice Prize Competition, designed to identify, encourage, recognize, and reward the application of impactful marketing science to industry and noncommercial settings. These applications aim to showcase innovative and impactful examples of applications demonstrating the best of rigor and relevance that our profession produces. The 2024 winner team developed a deep-learning-based recommender system that helps new salespeople to identify customers with high conversion potential at a major insurance company. The three other finalists include studies of utilizing artificial intelligence and behavior insights to motivate organizations’ sustainable energy consumption, modeling customer lifetime value in the retail banking industry, and designing business policy experiments using fractional factorial designs. This report concludes with reflections of trends and recent developments upon impactful and rigorous marketing science applications above and beyond the typical academic settings. History: Olivier Toubia served as the senior editor.},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2024.1172},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {478-481},
  shortjournal = {Market. Sci.},
  title        = {Practice prize report: The 2024 gary lilien ISMS practice prize competition},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective personalization of marketing interventions.
<em>MKSC</em>, <em>44</em>(2), 457–477. (<a
href="https://doi.org/10.1287/mksc.2023.0122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marketing interventions usually affect multiple outcomes of interest. However, finding an intervention that improves all desired outcomes is often rare, creating a trade-off for managers and decision makers. In this paper, we develop a multiobjective personalization framework that identifies personalized policies to balance multiple objectives at the individual level. We apply our framework to a canonical example of multiobjective conflict between sponsored and organic content consumption outcomes. Partnering with vdo.ai , we conduct a field experiment and randomly assign users to the skippable/long and nonskippable/short versions of the same ad. We document substantial substitution between sponsored and organic content consumption; the version that increases sponsored consumption reduces organic consumption. We find that multiobjective personalized policies can significantly improve both sponsored and organic consumption outcomes over single-objective policies. We show that compared with a single-objective policy optimized for organic consumption, there exists a multiobjective policy that increases sponsored consumption by 61% at the expense of only a 4% decrease in organic consumption. Similarly, compared with the single-objective policy optimized for sponsored consumption, there is a multiobjective policy that increases organic consumption by 53% while decreasing sponsored consumption by just 15%. History: Olivier Toubia served as the senior editor. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0122 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0122},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {457-477},
  shortjournal = {Market. Sci.},
  title        = {Multiobjective personalization of marketing interventions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel experimentation and competitive interference on
online advertising platforms. <em>MKSC</em>, <em>44</em>(2), 437–456.
(<a href="https://doi.org/10.1287/mksc.2022.0194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the measurement of advertising effects on online platforms when parallel experimentation occurs, that is, when multiple advertisers experiment concurrently. It provides a framework that makes precise how parallel experimentation affects the experiment’s value: while ignoring parallel experimentation yields an estimate of the average effect of advertising in place, which has limited value in decision making in an environment with variable advertising competition, accounting for parallel experimentation captures the actual uncertainty advertisers face due to competitive actions. It then implements an experimental design that enables the estimation of these effects on JD.com, a large e-commerce platform that is also a publisher of digital ads. Using traditional and kernel-based estimators, it shows that not accounting for competitive actions can result in the advertiser inaccurately estimating the advertising lift by a factor of two or higher, which can be consequential for decision making. History: Olivier Toubia served as the senior editor for this article. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0194 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0194},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {437-456},
  shortjournal = {Market. Sci.},
  title        = {Parallel experimentation and competitive interference on online advertising platforms},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inventory information frictions explain price rigidity in
perishable groceries. <em>MKSC</em>, <em>44</em>(2), 411–436. (<a
href="https://doi.org/10.1287/mksc.2023.0473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grocery retailers cannot engage in inventory-based pricing without physically auditing their shelves because their inventory records are inaccurate and incomplete. In particular, inventory data do not reflect actual, real-time quantity on the shelf, and standard Universal Product Code barcodes do not contain expiration dates. We argue that these inventory information frictions are much more powerful in explaining price rigidity in perishable groceries than physical menu costs are. We build our argument in two steps. First, we add inventory shrink, deteriorating product quality, and menu costs to [Gallego G, Van Ryzin G (1994) Optimal dynamic pricing of inventories with stochastic demand over finite horizons. Management Sci. 40(8):999–1020] classic revenue-management model, and we derive the model’s optimal pricing policy when the price setter can and cannot condition on real-time inventory information. Within the model, reducing a representative product’s physical menu cost by 90% increases its price-updating frequency by at least 300% more when pricing managers also have real-time inventory information than when they do not. Second, we provide proof of concept for our theory with data from two industry pilots: a UK grocery store that installed electronic shelf labels (ESLs)—a technology that reduces physical menu costs—and an EU grocery store that installed both ESLs and expanded barcodes—the latter being a technology that reduces inventory information frictions. Consistent with our theory, the technology upgrade increased price-updating frequency by 54% in the first pilot and by 853% in the second pilot. Our theory can explain the lack of widespread dynamic pricing of perishable groceries, which has significant consequences for profits, consumer surplus, and food waste. History: Anthony Dukes served as the senior editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.0473 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0473},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {411-436},
  shortjournal = {Market. Sci.},
  title        = {Inventory information frictions explain price rigidity in perishable groceries},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Targeted advertising as implicit recommendation: Strategic
mistargeting and personal data opt-out. <em>MKSC</em>, <em>44</em>(2),
390–410. (<a href="https://doi.org/10.1287/mksc.2023.0117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an advertiser’s targeting strategy and its effects on consumer data privacy choices, both of which determine the advertiser’s targeting accuracy. Targeted ads, serving as implicit recommendations when consumer preferences are uncertain, not only influence the consumer’s beliefs and purchasing decisions, but also amplify the advertiser’s temptation toward strategic mistargeting: sending ads to poorly matched consumers. Our analysis reveals that advertisers may, paradoxically, choose less precise targeting as accuracy improves. Even if prediction is perfect, the advertiser still targets the wrong consumers, leading to strategic mistargeting. Nevertheless, consumer surplus can remain positive because of improved identification of well-matched consumers, thereby reducing the incentive for consumers to withhold information. However, the scenario shifts with endogenous pricing; better prediction leads to more precise targeting although mistargeting persists. To exploit the recommendation effect of advertising, the advertiser raises prices instead of diluting recommendation power, lowering consumer welfare and prompting consumers to opt out of data collection. Furthermore, we investigate the impact of consumer data opt-out decisions under varying privacy policy defaults (opt in versus opt out). These decisions significantly affect equilibrium outcomes, influencing both the advertiser’s targeting strategies and consumer welfare. Our findings highlight the complex relationship between targeting accuracy, privacy choices, and advertisers’ incentives. History: Anthony Dukes served as the senior editor. Funding: This work was supported by the Korea Advanced Institute of Science and Technology [Grant G04230027]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.0117 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0117},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {390-410},
  shortjournal = {Market. Sci.},
  title        = {Targeted advertising as implicit recommendation: Strategic mistargeting and personal data opt-out},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Endogenous costs, market competition, and disclosure.
<em>MKSC</em>, <em>44</em>(2), 374–389. (<a
href="https://doi.org/10.1287/mksc.2022.0346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms must often decide whether to disclose private information regarding their costs to other market participants. Although extant literature has explored firms’ incentives to disclose exogenous and uncertain costs, little is known about when their endogenous costs should be disclosed. This paper studies the cost-disclosure strategies of competing firms whose inputs are sourced from and endogenously priced by upstream suppliers. We find, first, that cost disclosure affects not only market competition but also the motivations of suppliers in setting their input prices. As such, firms can strategize their disclosure decisions to optimize their procurement costs. Second, we find that firms’ disclosure decisions vary depending on both the nature of the competition and the market structure at hand. That is, when competing firms source from the same supplier or compete on price, they never disclose their costs; in such a case, nondisclosure is strictly better for consumers and welfare compared with disclosure. When competing firms source from different suppliers and compete on quantity, they always disclose; in such a case, disclosure is strictly better for consumers and welfare compared with nondisclosure. We also find that whereas manufacturers’ disclosure incentives are misaligned with those of suppliers, they are largely aligned with the goal of maximizing channel profits. Together, our results underscore the distinct role that endogenous costs play in firms’ disclosure decisions. History: Anthony Dukes served as the senior editor for this article. Funding: This work was supported by Research Grants Council of Hong Kong, University Grants Committee [11502221, 21500920]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2022.0346 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0346},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {374-389},
  shortjournal = {Market. Sci.},
  title        = {Endogenous costs, market competition, and disclosure},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overdiagnosis and undertesting for infectious diseases.
<em>MKSC</em>, <em>44</em>(2), 353–373. (<a
href="https://doi.org/10.1287/mksc.2022.0038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 (COVID-19) pandemic brought the availability of diagnostic tests to the forefront of public attention and highlighted the prevalence of undertesting (i.e., insufficient test supply relative to demand). Another important yet little studied systematic issue is overdiagnosis (i.e., positive diagnoses for patients with negligible viral loads); evidence suggests that U.S. laboratories have adopted highly sensitive diagnosis criteria such that up to an estimated 90% of positive diagnoses are for minuscule viral loads. Motivated by this situation, we develop a theory of diagnostic testing for infectious diseases that explains both undertesting and overdiagnosis. We show that a commercial laboratory has an incentive to inflate its diagnosis criterion, which generates a higher diagnosis-driven demand as a result of contact-tracing efforts, albeit while dampening demand from disease transmission. An inflated diagnosis criterion prompts the laboratory to build a higher testing capacity, which may not fully absorb the inflated demand, so undertesting arises. Finally, we examine a social planner’s problem of whether to mandate that the laboratory report the viral load along with its diagnosis so that a physician or contact tracer can make informed triage decisions. Our results show that the social planner may choose not to mandate viral-load reporting initially; this choice induces a higher testing capacity and can help reduce disease transmission. History: Anthony Dukes served as the senior editor. Funding: T. Dai was partially funded by the inaugural Hopkins Business of Health Initiative (HBHI) Seed Grant on “COVID-19 and the Business of Health.”},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0038},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {353-373},
  shortjournal = {Market. Sci.},
  title        = {Overdiagnosis and undertesting for infectious diseases},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalizing ad load to optimize subscription and ad
revenues: Product strategies constructed from experiments on pandora.
<em>MKSC</em>, <em>44</em>(2), 327–352. (<a
href="https://doi.org/10.1287/mksc.2022.0357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of advertising as an implicit price has long been recognized in economics and marketing, yet the effects of personalizing these implicit prices on firm profits and consumer welfare have not been explored. Using a large-scale field experiment on Pandora, we exogenously changed the ad load—the number of ads played per hour—for more than seven million users over 18 months. We find that the impact of ad load on consumption takes more than a year to stabilize, whereas subscription responses stabilize in less than six months. Using a machine-learning model, we analyze the heterogeneous effects of changing ad load on ad and subscription revenues. We then show that reallocating ads across users can enhance subscription profits by 7% without decreasing overall advertising profits. To achieve the same subscription rate with a uniform ad allocation policy, the firm would need to increase ad load by more than 22%. Our results show that, on average, consumer welfare drops by 2% with the proposed personalization strategy, and the effect seems to be more pronounced for users who have a higher willingness to pay. History: Olivier Toubia served as the senior editor for this article. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0357 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0357},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {327-352},
  shortjournal = {Market. Sci.},
  title        = {Personalizing ad load to optimize subscription and ad revenues: Product strategies constructed from experiments on pandora},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the profitability of loyalty. <em>MKSC</em>,
<em>44</em>(2), 306–326. (<a
href="https://doi.org/10.1287/mksc.2022.0109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to enhance our understanding of customer loyalty and its implications on firm profits. Factors such as favorable brand attitudes and switching costs may drive customer loyalty, which is often considered a strong indicator of a firm’s profitability. However, empirical findings on the profitability of customer loyalty are inconsistent, and a comprehensive framework to analyze the connection between customer loyalty and firm profits is still lacking. This paper fills this gap by proposing an analytical framework that characterizes how repeat-purchase loyalty is generated from consumer choice in a dynamic competition environment. It also examines the impact of brand preference and switching costs on firm profits and customer loyalty. Our findings suggest that strong brand preference can either increase or decrease customer loyalty, depending on the stability of consumer preferences over time, and when it increases loyalty, such loyalty is profitable for firms. Conversely, if customer loyalty is primarily driven by habitual buyers who are simply avoiding switching costs, this type of loyalty is not profitable for firms. History: Anthony Dukes served as the senior editor for this article. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2022.0109 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0109},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {306-326},
  shortjournal = {Market. Sci.},
  title        = {On the profitability of loyalty},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Product safety and liability with deceptive advertising and
moral hazard. <em>MKSC</em>, <em>44</em>(2), 287–305. (<a
href="https://doi.org/10.1287/mksc.2023.0173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product safety is jointly determined by product quality and consumers’ precaution efforts when using products. Product failures because of either party’s negligence can cause deaths and injuries. The strict liability rule holds firms fully responsible for damages, whereas the comparative negligence rule allocates responsibilities based on the firm’s negligence in product quality and the consumer’s negligence in precaution efforts. This paper examines how these two common product liability rules affect firms and consumers when firms can advertise product quality deceptively and consumers exert precaution efforts endogenously. Common wisdom suggests that the comparative negligence rule shifts responsibility from firms to consumers, which benefits firms and motivates consumers to exert precautions to improve safety. However, we find that comparative negligence incentivizes firms to use deceptive advertising, whereas strict liability induces firms to advertise low product quality more truthfully. The latter can induce consumers to exert precaution efforts to avoid product failures, thereby improving firm profit and product safety. Moreover, raising firms’ penalties or the amount that firms compensate consumers for product failures can benefit firms but reduce consumer surplus and product safety. These findings caution public policymakers and firms that with asymmetric information about product quality and endogenous precaution efforts, liability policies can lead to unintended consequences. History: Anthony Dukes served as the senior editor. Funding: X. Guan is supported by the National Natural Science Foundation of China [Grants 72325005 and 71821001], H. Cao is supported by the National Natural Science Foundation of China [Grants 72202087 and 72032001], K. J. Li is supported by the Blanche “Peg” Philpott Professorship, and Y. Ding is supported by the National Natural Science Foundation of China [Grant 72103158]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.0173 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0173},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {287-305},
  shortjournal = {Market. Sci.},
  title        = {Product safety and liability with deceptive advertising and moral hazard},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating the value of offsite tracking data to
advertisers: Evidence from meta. <em>MKSC</em>, <em>44</em>(2), 268–286.
(<a href="https://doi.org/10.1287/mksc.2023.0274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Third-party cookies and related “offsite” tracking technologies are frequently used to share user data across applications in support of ad delivery. These data are viewed as highly valuable for online advertisers, but their usage faces increasing headwinds. In this paper, we quantify the benefit to advertisers from using such offsite tracking data in their ad delivery. With this goal in mind, we conduct a large-scale, randomized experiment that includes more than 70,000 advertisers on Facebook and Instagram. We first estimate advertising effectiveness at baseline across our broad sample. We then estimate the change in effectiveness of the same campaigns were advertisers to lose the ability to optimize ad delivery with offsite data. In each of these cases, we use recently developed deconvolution techniques to flexibly estimate the underlying distribution of effects. We find a median cost per incremental customer at baseline of $38.16 that under the median loss in effectiveness would rise to $49.93, a 31% increase. Further, we find ads targeted using offsite data generate more long-term customers per dollar than those without, and losing offsite data disproportionately hurts small scale advertisers. Taken together, our results suggest that offsite data bring large benefits to a wide range of advertisers. History: Catherine Tucker served as the senior editor for this article. Conflict of Interest Statement: N. Wernerfelt and R. Moakler were employees of Meta when this research was conducted and the latter owns stock in the company. Meta was able to review this publication for proprietary, trade-secret, or non-aggregated information that could potentially identify any individual(s), but did not have the right to restrict publication based on the results or content of the findings. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mksc.2023.0274 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0274},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {268-286},
  shortjournal = {Market. Sci.},
  title        = {Estimating the value of offsite tracking data to advertisers: Evidence from meta},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Engagement that sells: Influencer video advertising on
TikTok. <em>MKSC</em>, <em>44</em>(2), 247–267. (<a
href="https://doi.org/10.1287/mksc.2021.0107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many ads are engaging, but what makes them engaging may have little to do with the product. This problem can be particularly relevant to influencer advertising if influencers are motivated to promote themselves, not just the product. We develop an algorithm to measure the degree of effective engagement associated with the product and use it to predict the sales lift of influencer video advertising. We propose the concept of the product engagement score (PE score) to capture how engaging the product itself is as presented in a video. We estimate pixel-level engagement as a saliency map by training a deep three-dimensional convolutional neural network on video-level engagement data. We locate pixel-level product placement with an object detection algorithm. The PE score is computed as the pixel-level, engagement-weighted product placement in a video. We construct and validate the algorithm with influencer video ads on TikTok and product sales data on Taobao. We use variation in video posting time to identify video-specific sales lift and show that the PE score significantly and robustly predicts sales lift. We explore drivers of engagement and discuss how various stakeholders in influencer advertising can use the PE score in a scalable way to manage content, align incentives, and improve efficiency. History: Olivier Toubia served as the senior editor. Supplemental Material: The online appendix and instructions to acquire data are available at https://doi.org/10.1287/mksc.2021.0107 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2021.0107},
  journal      = {Marketing Science},
  month        = {3-4},
  number       = {2},
  pages        = {247-267},
  shortjournal = {Market. Sci.},
  title        = {Engagement that sells: Influencer video advertising on TikTok},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="moor---28">MOOR - 28</h2>
<ul>
<li><details>
<summary>
(2025). Steiner cut dominants. <em>MOOR</em>, <em>50</em>(1),
764–781. (<a href="https://doi.org/10.1287/moor.2022.0280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a subset T of nodes of an undirected graph G , a T-Steiner cut is a cut δ ( S ) with T ∩ S ≠ ø and T \ S ≠ ø . The T-Steiner cut dominant of G is the dominant CUT + ( G , T ) of the convex hull of the incidence vectors of the T -Steiner cuts of G . For T = { s , t } , this is the well-understood s - t -cut dominant. Choosing T as the set of all nodes of G , we obtain the cut dominant for which an outer description in the space of the original variables is still not known. We prove that for each integer τ , there is a finite set of inequalities such that for every pair ( G , T ) with | T | ≤ τ , the nontrivial facet-defining inequalities of CUT + ( G , T ) are the inequalities that can be obtained via iterated applications of two simple operations, starting from that set. In particular, the absolute values of the coefficients and of the right-hand sides in a description of CUT + ( G , T ) by integral inequalities can be bounded from above by a function of | T | . For all | T | ≤ 5 , we provide descriptions of CUT + ( G , T ) by facet-defining inequalities, extending the known descriptions of s - t -cut dominants.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0280},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {764-781},
  shortjournal = {Math. Oper. Res.},
  title        = {Steiner cut dominants},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uniqueness of convex-ranged probabilities and applications
to risk measures and games. <em>MOOR</em>, <em>50</em>(1), 743–763. (<a
href="https://doi.org/10.1287/moor.2023.0015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit Marinacci’s uniqueness theorem for convex-ranged probabilities and its applications. Our approach does away with both the countable additivity and the positivity of the charges involved. In the process, we uncover several new equivalent conditions, which lead to a novel set of applications. These include extensions of the classic Fréchet–Hoeffding bounds as well as of the automatic Fatou property of law-invariant functionals. We also generalize existing results of the “collapse to the mean”-type concerning capacities and α -MEU preferences.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0015},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {743-763},
  shortjournal = {Math. Oper. Res.},
  title        = {Uniqueness of convex-ranged probabilities and applications to risk measures and games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample-path large deviations for unbounded additive
functionals of the reflected random walk. <em>MOOR</em>, <em>50</em>(1),
711–742. (<a href="https://doi.org/10.1287/moor.2020.0094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove a sample-path large deviation principle (LDP) with sublinear speed for unbounded functionals of certain Markov chains induced by the Lindley recursion. The LDP holds in the Skorokhod space D [ 0 , 1 ] equipped with the M 1 ′ topology. Our technique hinges on a suitable decomposition of the Markov chain in terms of regeneration cycles. Each regeneration cycle denotes the area accumulated during the busy period of the reflected random walk. We prove a large deviation principle for the area under the busy period of the Markov random walk, and we show that it exhibits a heavy-tailed behavior. Funding: The research of B. Zwart and M. Bazhba is supported by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek [Grant 639.033.413]. The research of J. Blanchet is supported by the National Science Foundation (NSF) [Grants 1915967, 1820942, and 1838576] as well as the Defense Advanced Research Projects Agency [Grant N660011824028]. The research of C.-H. Rhee is supported by the NSF [Grant CMMI-2146530].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2020.0094},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {711-742},
  shortjournal = {Math. Oper. Res.},
  title        = {Sample-path large deviations for unbounded additive functionals of the reflected random walk},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact characterization of the jointly optimal restocking and
auditing policy in inventory systems with record inaccuracy.
<em>MOOR</em>, <em>50</em>(1), 656–710. (<a
href="https://doi.org/10.1287/moor.2022.0145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a continuous-time stochastic model of an inventory system with record inaccuracy. In this formulation, demand is modeled by a point process and is observable only when it leads to sales. In addition to demand that can reduce the stock, an unobservable stochastic loss process can also reduce the stock. The retailer’s goal is to identify the restocking and auditing policy that minimizes the expected discounted cost of carrying a product over an infinite horizon. We analytically characterize the optimal restocking and jointly optimal auditing policy. We prove that the optimal restocking policy is a threshold policy. Our proof of this result is based on a coupling argument that is valid for any demand and loss model. Unlike the optimal restocking policy, the jointly optimal auditing policy is not of threshold type. We show that a complete proof of this statement cannot be obtained by solely resorting to the first-order stochastic dominance property of the Bayesian shelf stock distribution induced by the demand and loss process. Instead, our characterization of the jointly optimal auditing policy is based on proving that the dynamics of the shelf stock distribution constitute a (strictly) sign-regular kernel. To our knowledge, this is the first paper that characterizes the optimal policy of a complex control problem by establishing sign regularity of its underlying Markovian dynamics. Our theoretical results lead to a fast algorithm for computing the exact jointly optimal auditing/restocking policy over the problem’s entire state space. This enables comparative statics analysis, which allows us to determine how inventory record inaccuracy affects the economic significance of various cost drivers. This, in turn, allows us to determine when or, better, under what conditions auditing can be an effective tool for reducing the total cost.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0145},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {656-710},
  shortjournal = {Math. Oper. Res.},
  title        = {Exact characterization of the jointly optimal restocking and auditing policy in inventory systems with record inaccuracy},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast rates for the regret of offline reinforcement learning.
<em>MOOR</em>, <em>50</em>(1), 633–655. (<a
href="https://doi.org/10.1287/moor.2021.0167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the regret of offline reinforcement learning in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted Q -iteration (FQI), suggest root- n convergence for regret, empirical behavior exhibits much faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function, the regret of the policy it defines converges at a rate given by the exponentiation of the estimate’s pointwise convergence rate, thus speeding up the rate. The level of exponentiation depends on the level of noise in the decision-making problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman residual minimization to establish the correct pointwise convergence guarantees. As specific cases, our results imply one-over- n rates in linear cases and exponential-in- n rates in tabular cases. We extend our findings to general function approximation by extending our results to regret guarantees based on L p -convergence rates for estimating the optimal quality function rather than pointwise rates, where L 2 guarantees for nonparametric estimation can be ensured under mild conditions. Funding: This work was supported by the Division of Information and Intelligent Systems, National Science Foundation [Grant 1846210].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0167},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {633-655},
  shortjournal = {Math. Oper. Res.},
  title        = {Fast rates for the regret of offline reinforcement learning},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal investment strategy for α-robust utility
maximization problem. <em>MOOR</em>, <em>50</em>(1), 606–632. (<a
href="https://doi.org/10.1287/moor.2023.0076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reality, investors are uncertain about the dynamics of risky asset returns. Therefore, investors prefer to make robust investment decisions. In this paper, we propose an α-robust utility maximization problem under uncertain parameters. The investor is allowed to invest in a financial market consisting of a risk-free asset and a risky asset. The uncertainty about the expected return rate is parameterized by a nonempty set. Different from most existing literature on robust utility maximization problems where investors are generally assumed to be extremely ambiguity averse because they tend to consider only expected utility in the worst-case scenario, we pay attention to the investors who are not only ambiguity averse but also ambiguity seeking. Under power utility, we provide the implicit function representations for the precommitted strategy, equilibrium strategy of the open-loop type, and equilibrium strategy of the closed-loop type. Some properties about the optimal trading strategies, the best-case and worst-case parameters under three different kinds of strategies, are provided. Funding: This work was supported by National Natural Science Foundation of China [Grants 12071147, 12171169, 12271171, 12371470, 71721001, 71931004, 72371256], the Shanghai Philosophy Social Science Planning Office Project [Grant 2022ZJB005], Fundamental Research Funds for the Central Universities [Grant 2022QKT001], the Excellent Young Team Project Natural Science Foundation of Guangdong Province of China [Grant 2023B1515040001], the Philosophy and Social Science Programming Foundation of Guangdong Province [Grant GD22CYJ17], the Nature Science Foundation of Guangdong Province of China [Grant 2022A1515011472], and the 111 Project [Grant B14019].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0076},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {606-632},
  shortjournal = {Math. Oper. Res.},
  title        = {Optimal investment strategy for α-robust utility maximization problem},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel langevin pathwise average for gibbs
approximation. <em>MOOR</em>, <em>50</em>(1), 573–605. (<a
href="https://doi.org/10.1287/moor.2021.0243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study a new multilevel method for the numerical approximation of a Gibbs distribution π on R d , based on (overdamped) Langevin diffusions. This method relies on a multilevel occupation measure, that is, on an appropriate combination of R occupation measures of (constant-step) Euler schemes with respective steps γ r = γ 0 2 − r , r = 0 , … , R . We first state a quantitative result under general assumptions that guarantees an ε-approximation (in an L 2 -sense) with a cost of the order ε − 2 or ε − 2 | log ε | 3 under less contractive assumptions. We then apply it to overdamped Langevin diffusions with strongly convex potential U : R d → R and obtain an ε-complexity of the order O ( d ε − 2 log 3 ( d ε − 2 ) ) or O ( d ε − 2 ) under additional assumptions on U . More precisely, up to universal constants, an appropriate choice of the parameters leads to a cost controlled by ( λ ¯ U ∨ 1 ) 2 λ ¯ U − 3 d ε − 2 (where λ ¯ U and λ ¯ U respectively denote the supremum and the infimum of the largest and lowest eigenvalue of D 2 U ). We finally complete these theoretical results with some numerical illustrations, including comparisons to other algorithms in Bayesian learning and opening to the non–strongly convex setting. Funding: The authors are grateful to the SIRIC ILIAD Nantes-Angers program, supported by the French National Cancer Institute [INCA-DGOS-Inserm Grant 12558].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0243},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {573-605},
  shortjournal = {Math. Oper. Res.},
  title        = {Multilevel langevin pathwise average for gibbs approximation},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semidefinite approximations for bicliques and bi-independent
pairs. <em>MOOR</em>, <em>50</em>(1), 537–572. (<a
href="https://doi.org/10.1287/moor.2023.0046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate some graph parameters dealing with bi-independent pairs ( A , B ) in a bipartite graph G = ( V 1 ∪ V 2 , E ) , that is, pairs ( A , B ) where A ⊆ V 1 , B ⊆ V 2 , and A ∪ B are independent. These parameters also allow us to study bicliques in general graphs. When maximizing the cardinality | A ∪ B | , one finds the stability number α ( G ) , well-known to be polynomial-time computable. When maximizing the product | A | · | B | , one finds the parameter g ( G ), shown to be NP-hard by Peeters in 2003, and when maximizing the ratio | A | · | B | / | A ∪ B | , one finds h ( G ), introduced by Vallentin in 2020 for bounding product-free sets in finite groups. We show that h ( G ) is an NP-hard parameter and, as a crucial ingredient, that it is NP-complete to decide whether a bipartite graph G has a balanced maximum independent set. These hardness results motivate introducing semidefinite programming (SDP) bounds for g ( G ), h ( G ), and α bal ( G ) (the maximum cardinality of a balanced independent set). We show that these bounds can be seen as natural variations of the Lovász ϑ-number, a well-known semidefinite bound on α ( G ) . In addition, we formulate closed-form eigenvalue bounds, and we show relationships among them as well as with earlier spectral parameters by Hoffman and Haemers in 2001 and Vallentin in 2020. Funding: This work was supported by H2020 Marie Skłodowska-Curie Actions [Grant 813211 (POEMA)].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0046},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {537-572},
  shortjournal = {Math. Oper. Res.},
  title        = {Semidefinite approximations for bicliques and bi-independent pairs},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-field multiagent reinforcement learning: A
decentralized network approach. <em>MOOR</em>, <em>50</em>(1), 506–536.
(<a href="https://doi.org/10.1287/moor.2022.0055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenges for multiagent reinforcement learning (MARL) is designing efficient learning algorithms for a large system in which each agent has only limited or partial information of the entire system. Whereas exciting progress has been made to analyze decentralized MARL with the network of agents for social networks and team video games, little is known theoretically for decentralized MARL with the network of states for modeling self-driving vehicles, ride-sharing, and data and traffic routing. This paper proposes a framework of localized training and decentralized execution to study MARL with the network of states. Localized training means that agents only need to collect local information in their neighboring states during the training phase; decentralized execution implies that agents can execute afterward the learned decentralized policies, which depend only on agents’ current states. The theoretical analysis consists of three key components: the first is the reformulation of the MARL system as a networked Markov decision process with teams of agents, enabling updating the associated team Q-function in a localized fashion; the second is the Bellman equation for the value function and the appropriate Q-function on the probability measure space; and the third is the exponential decay property of the team Q-function, facilitating its approximation with efficient sample efficiency and controllable error. The theoretical analysis paves the way for a new algorithm LTDE-N eural -AC, in which the actor–critic approach with overparameterized neural networks is proposed. The convergence and sample complexity are established and shown to be scalable with respect to the sizes of both agents and states. To the best of our knowledge, this is the first neural network–based MARL algorithm with network structure and provable convergence guarantee. Funding: X. Wei is partially supported by NSFC no. 12201343. R. Xu is partially supported by the NSF CAREER award DMS-2339240.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0055},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {506-536},
  shortjournal = {Math. Oper. Res.},
  title        = {Mean-field multiagent reinforcement learning: A decentralized network approach},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Marginal values of a stochastic game. <em>MOOR</em>,
<em>50</em>(1), 482–505. (<a
href="https://doi.org/10.1287/moor.2023.0297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-sum stochastic games are parameterized by payoffs, transitions, and possibly a discount rate. In this article, we study how the main solution concepts, the discounted and undiscounted values, vary when these parameters are perturbed. We focus on the marginal values, introduced by Mills in 1956 in the context of matrix games—that is, the directional derivatives of the value along any fixed perturbation. We provide a formula for the marginal values of a discounted stochastic game. Further, under mild assumptions on the perturbation, we provide a formula for their limit as the discount rate vanishes and for the marginal values of an undiscounted stochastic game. We also show, via an example, that the two latter differ in general. Funding: This work was supported by Fondation CFM pour la Recherche; the European Research Council [Grant ERC-CoG-863818 (ForM-SMArt)]; and Agence Nationale de la Recherche [Grant ANR-21-CE40-0020].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0297},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {482-505},
  shortjournal = {Math. Oper. Res.},
  title        = {Marginal values of a stochastic game},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence and stability of coupled belief-strategy
learning dynamics in continuous games. <em>MOOR</em>, <em>50</em>(1),
459–481. (<a href="https://doi.org/10.1287/moor.2022.0161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a learning dynamics to model how strategic agents repeatedly play a continuous game while relying on an information platform to learn an unknown payoff-relevant parameter. In each time step, the platform updates a belief estimate of the parameter based on players’ strategies and realized payoffs using Bayes’ rule. Then, players adopt a generic learning rule to adjust their strategies based on the updated belief. We present results on the convergence of beliefs and strategies and the properties of convergent fixed points of the dynamics. We obtain sufficient and necessary conditions for the existence of globally stable fixed points. We also provide sufficient conditions for the local stability of fixed points. These results provide an approach to analyzing the long-term outcomes that arise from the interplay between Bayesian belief learning and strategy learning in games and enable us to characterize conditions under which learning leads to a complete information equilibrium. Funding: Financial support from the Air Force Office of Scientific Research [Project Building Attack Resilience into Complex Networks], the Simons Institute [research fellowship], and a Michael Hammer Fellowship is gratefully acknowledged.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0161},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {459-481},
  shortjournal = {Math. Oper. Res.},
  title        = {Convergence and stability of coupled belief-strategy learning dynamics in continuous games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A policy gradient algorithm for the risk-sensitive
exponential cost MDP. <em>MOOR</em>, <em>50</em>(1), 431–458. (<a
href="https://doi.org/10.1287/moor.2022.0139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the risk-sensitive exponential cost Markov decision process (MDP) formulation and develop a trajectory-based gradient algorithm to find the stationary point of the cost associated with a set of parameterized policies. We derive a formula that can be used to compute the policy gradient from (state, action, cost) information collected from sample paths of the MDP for each fixed parameterized policy. Unlike the traditional average cost problem, standard stochastic approximation theory cannot be used to exploit this formula. To address the issue, we introduce a truncated and smooth version of the risk-sensitive cost and show that this new cost criterion can be used to approximate the risk-sensitive cost and its gradient uniformly under some mild assumptions. We then develop a trajectory-based gradient algorithm to minimize the smooth truncated estimation of the risk-sensitive cost and derive conditions under which a sequence of truncations can be used to solve the original, untruncated cost problem. Funding: This work was supported by the Office of Naval Research Global [Grant N0001419-1-2566], the Division of Computer and Network Systems [Grant 21-06801], the Army Research Office [Grant W911NF-19-1-0379], and the Division of Computing and Communication Foundations [Grants 17-04970 and 19-34986].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0139},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {431-458},
  shortjournal = {Math. Oper. Res.},
  title        = {A policy gradient algorithm for the risk-sensitive exponential cost MDP},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parametric semidefinite programming: Geometry of the
trajectory of solutions. <em>MOOR</em>, <em>50</em>(1), 410–430. (<a
href="https://doi.org/10.1287/moor.2021.0097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, solutions of convex optimization problems are updated on-line, as functions of time. In this paper, we consider parametric semidefinite programs, which are linear optimization problems in the semidefinite cone whose coefficients (input data) depend on a time parameter . We are interested in the geometry of the solution (output data) trajectory, defined as the set of solutions depending on the parameter . We propose an exhaustive description of the geometry of the solution trajectory. As our main result, we show that only six distinct behaviors can be observed at a neighborhood of a given point along the solution trajectory. Each possible behavior is then illustrated by an example. Funding: This work was supported by OP RDE [Grant CZ.02.1.01/0.0/0.0/16_019/0000765].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0097},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {410-430},
  shortjournal = {Math. Oper. Res.},
  title        = {Parametric semidefinite programming: Geometry of the trajectory of solutions},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the (im-)possibility of representing probability
distributions as a difference of i.i.d. Noise terms. <em>MOOR</em>,
<em>50</em>(1), 390–409. (<a
href="https://doi.org/10.1287/moor.2023.0081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A random variable is difference-form decomposable ( DFD ) if it may be written as the difference of two i.i.d. random terms. We show that densities of such variables exhibit a remarkable degree of structure. Specifically, a DFD density can be neither approximately uniform, nor quasiconvex, nor strictly concave. On the other hand, a DFD density need, in general, be neither unimodal nor logconcave. Regarding smoothness, we show that a compactly supported DFD density cannot be analytic and will often exhibit a kink even if its components are smooth. The analysis highlights the risks for model consistency resulting from the strategy widely adopted in the economics literature of imposing assumptions directly on a difference of noise terms rather than on its components.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0081},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {390-409},
  shortjournal = {Math. Oper. Res.},
  title        = {On the (Im-)Possibility of representing probability distributions as a difference of I.I.D. noise terms},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal consumption and investment with independent
stochastic labor income. <em>MOOR</em>, <em>50</em>(1), 356–389. (<a
href="https://doi.org/10.1287/moor.2023.0119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new dynamic continuous-time model of optimal consumption and investment to include independent stochastic labor income. We reduce the problem of solving the Bellman equation to a problem of solving an integral equation. We then explicitly characterize the optimal consumption and investment strategy as a function of income-to-wealth ratio. We provide some analytical comparative statics associated with the value function and optimal strategies. We also develop a quite general numerical algorithm for control iteration and solve the Bellman equation as a sequence of solutions to ordinary differential equations. This numerical algorithm can be readily applied to many other optimal consumption and investment problems especially with extra nondiversifiable Brownian risks, resulting in nonlinear Bellman equations. Finally, our numerical analysis illustrates how the presence of stochastic labor income affects the optimal consumption and investment strategy. Funding: A. Bensoussan was supported by the National Science Foundation under grant [DMS-2204795]. S. Park was supported by the Ministry of Education of the Republic of Korea and the National Research Foundation of Korea, South Korea [NRF-2022S1A3A2A02089950].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0119},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {356-389},
  shortjournal = {Math. Oper. Res.},
  title        = {Optimal consumption and investment with independent stochastic labor income},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strongly convergent homogeneous approximations to
inhomogeneous markov jump processes and applications. <em>MOOR</em>,
<em>50</em>(1), 334–355. (<a
href="https://doi.org/10.1287/moor.2022.0153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of time-inhomogeneous Markov jump processes is a traditional topic within probability theory that has recently attracted substantial attention in various applications. However, their flexibility also incurs a substantial mathematical burden which is usually circumvented by using well-known generic distributional approximations or simulations. This article provides a novel approximation method that tailors the dynamics of a time-homogeneous Markov jump process to meet those of its time-inhomogeneous counterpart on an increasingly fine Poisson grid. Strong convergence of the processes in terms of the Skorokhod J 1 metric is established, and convergence rates are provided. Under traditional regularity assumptions, distributional convergence is established for unconditional proxies, to the same limit. Special attention is devoted to the case where the target process has one absorbing state and the remaining ones transient, for which the absorption times also converge. Some applications are outlined, such as univariate hazard-rate density estimation, ruin probabilities, and multivariate phase-type density evaluation. Funding: M. Bladt and O. Peralta would like to acknowledge financial support from the Swiss National Science Foundation Project 200021_191984. O. Peralta acknowledges financial support from NSF Award #1653354 and AXA Research Fund Award on “Mitigating risk in the wake of the pandemic”.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0153},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {334-355},
  shortjournal = {Math. Oper. Res.},
  title        = {Strongly convergent homogeneous approximations to inhomogeneous markov jump processes and applications},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk sharing with lambda value at risk. <em>MOOR</em>,
<em>50</em>(1), 313–333. (<a
href="https://doi.org/10.1287/moor.2023.0246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the risk-sharing problem among multiple agents using lambda value at risk ( Λ VaR ) as their preferences via the tool of inf-convolution, where Λ VaR is an extension of value at risk ( VaR ). We obtain explicit formulas of the inf-convolution of multiple Λ VaR with monotone Λ and explicit forms of the corresponding optimal allocations, extending the results of the inf-convolution of VaR . It turns out that the inf-convolution of several Λ VaR is still a Λ VaR under some mild condition. Moreover, we investigate the inf-convolution of one Λ VaR and a general monotone risk measure without cash additivity, including Λ VaR , expected utility, and rank-dependent expected utility as special cases. The expression of the inf-convolution and the explicit forms of the optimal allocation are derived, leading to some partial solution of the risk-sharing problem with multiple Λ VaR for general Λ functions. Finally, we discuss the risk-sharing problem with Λ VaR + , another definition of lambda value at risk. We focus on the inf-convolution of Λ VaR + and a risk measure that is consistent with the second-order stochastic dominance, deriving very different expression of the inf-convolution and the forms of the optimal allocations.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0246},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {313-333},
  shortjournal = {Math. Oper. Res.},
  title        = {Risk sharing with lambda value at risk},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational inequalities on unbounded domains for zero-sum
singular controller vs. Stopper games. <em>MOOR</em>, <em>50</em>(1),
277–312. (<a href="https://doi.org/10.1287/moor.2023.0029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of zero-sum games between a singular controller and a stopper over a finite-time horizon. The underlying process is a multidimensional (locally nondegenerate) controlled stochastic differential equation (SDE) evolving in an unbounded domain. We prove that such games admit a value and provide an optimal strategy for the stopper. The value of the game is shown to be the maximal solution in a suitable Sobolev class of a variational inequality of min-max type with an obstacle constraint and a gradient constraint. Although the variational inequality and the game are solved on an unbounded domain, we do not require boundedness of either the coefficients of the controlled SDE or of the cost functions in the game. Funding: A. Bovo was partially supported by the Doctoral Studentship from the University of Leeds.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0029},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {277-312},
  shortjournal = {Math. Oper. Res.},
  title        = {Variational inequalities on unbounded domains for zero-sum singular controller vs. stopper games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A characterization of simultaneous optimization,
majorization, and (bi-)submodular polyhedra. <em>MOOR</em>,
<em>50</em>(1), 252–276. (<a
href="https://doi.org/10.1287/moor.2023.0054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by resource allocation problems (RAPs) in power management applications, we investigate the existence of solutions to optimization problems that simultaneously minimize the class of Schur-convex functions, also called least-majorized elements. For this, we introduce a generalization of majorization and least-majorized elements, called ( a , b )-majorization and least ( a , b )-majorized elements, and characterize the feasible sets of problems that have such elements in terms of base and (bi-)submodular polyhedra. Hereby, we also obtain new characterizations of these polyhedra that extend classical characterizations in terms of optimal greedy algorithms from the 1970s. We discuss the implications of our results for RAPs in power management applications and derive a new characterization of convex cooperative games and new properties of optimal estimators of specific regularized regression problems. In general, our results highlight the combinatorial nature of simultaneously optimizing solutions and provide a theoretical explanation for why such solutions generally do not exist.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0054},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {252-276},
  shortjournal = {Math. Oper. Res.},
  title        = {A characterization of simultaneous optimization, majorization, and (Bi-)Submodular polyhedra},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stationary points of a shallow neural network with quadratic
activations and the global optimality of the gradient descent algorithm.
<em>MOOR</em>, <em>50</em>(1), 209–251. (<a
href="https://doi.org/10.1287/moor.2021.0082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of training a shallow neural network with quadratic activation functions and the generalization power of such trained networks. Assuming that the samples are generated by a full rank matrix W * of the hidden network node weights, we obtain the following results. We establish that all full-rank approximately stationary solutions of the risk minimization problem are also approximate global optimums of the risk (in-sample and population). As a consequence, we establish that, when trained on polynomially many samples, the gradient descent algorithm converges to the global optimum of the risk minimization problem regardless of the width of the network when it is initialized at some value ν * , which we compute. Furthermore, the network produced by the gradient descent has a near zero generalization error. Next, we establish that initializing the gradient descent algorithm below ν * is easily achieved when the weights of the ground truth matrix W * are randomly generated and the matrix is sufficiently overparameterized. Finally, we identify a simple necessary and sufficient geometric condition on the size of the training set under which any global minimizer of the empirical risk has necessarily zero generalization error. Funding: The research of E. C. Kizildag is supported by Columbia University, with the Distinguished Postdoctoral Fellowship in Statistics. Support from the National Science Foundation [Grant DMS-2015517] is gratefully acknowledged.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0082},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {209-251},
  shortjournal = {Math. Oper. Res.},
  title        = {Stationary points of a shallow neural network with quadratic activations and the global optimality of the gradient descent algorithm},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Submodular functions and perfect graphs. <em>MOOR</em>,
<em>50</em>(1), 189–208. (<a
href="https://doi.org/10.1287/moor.2021.0302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a combinatorial polynomial-time algorithm to find a maximum weight independent set in perfect graphs of bounded degree that do not contain a prism or a hole of length four as an induced subgraph. An even pair in a graph is a pair of vertices all induced paths between which are even. An even set is a set of vertices every two of which are an even pair. We show that every perfect graph that does not contain a prism or a hole of length four as an induced subgraph has a balanced separator which is the union of a bounded number of even sets, where the bound depends only on the maximum degree of the graph. This allows us to solve the maximum weight independent set problem using the well-known submodular function minimization algorithm. Funding: This work was supported by the Engineering and Physical Sciences Research Council [Grant EP/V002813/1]; the National Science Foundation [Grants DMS-1763817, DMS-2120644, and DMS-2303251]; and Alexander von Humboldt-Stiftung.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0302},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {189-208},
  shortjournal = {Math. Oper. Res.},
  title        = {Submodular functions and perfect graphs},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluctuation theory of continuous-time, skip-free downward
markov chains with applications to branching processes with immigration.
<em>MOOR</em>, <em>50</em>(1), 169–188. (<a
href="https://doi.org/10.1287/moor.2022.0246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a comprehensive methodology for the fluctuation theory of continuous-time, skip-free Markov chains, extending and improving the recent work of Choi and Patie for discrete-time, skip-free Markov chains. As a significant application, we use it to derive a full set of fluctuation identities regarding exiting a finite or infinite interval for Markov branching processes with immigration, thereby uncovering many new results for this classic family of continuous-time Markov chains. The theory also allows us to recover in a simple manner fluctuation identities for skip-free downward compound Poisson processes.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0246},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {169-188},
  shortjournal = {Math. Oper. Res.},
  title        = {Fluctuation theory of continuous-time, skip-free downward markov chains with applications to branching processes with immigration},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alternating and parallel proximal gradient methods for
nonsmooth, nonconvex minimax: A unified convergence analysis.
<em>MOOR</em>, <em>50</em>(1), 141–168. (<a
href="https://doi.org/10.1287/moor.2022.0294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in nonconvex minimax problems that is driven by an abundance of applications. Our focus is on nonsmooth, nonconvex-strongly concave minimax, thus departing from the more common weakly convex and smooth models assumed in the recent literature. We present proximal gradient schemes with either parallel or alternating steps. We show that both methods can be analyzed through a single scheme within a unified analysis that relies on expanding a general convergence mechanism used for analyzing nonconvex, nonsmooth optimization problems. In contrast to the current literature, which focuses on the complexity of obtaining nearly approximate stationary solutions, we prove subsequence convergence to a critical point of the primal objective and global convergence when the latter is semialgebraic. Furthermore, the complexity results we provide are with respect to approximate stationary solutions. Lastly, we expand the scope of problems that can be addressed by generalizing one of the steps with a Bregman proximal gradient update, and together with a few adjustments to the analysis, this allows us to extend the convergence and complexity results to this broader setting. Funding: The research of E. Cohen was partially supported by a doctoral fellowship from the Israel Science Foundation [Grant 2619-20] and Deutsche Forschungsgemeinschaft [Grant 800240]. The research of M. Teboulle was partially supported by the Israel Science Foundation [Grant 2619-20] and Deutsche Forschungsgemeinschaft [Grant 800240].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0294},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {141-168},
  shortjournal = {Math. Oper. Res.},
  title        = {Alternating and parallel proximal gradient methods for nonsmooth, nonconvex minimax: A unified convergence analysis},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling in the high-uncertainty heavy traffic regime.
<em>MOOR</em>, <em>50</em>(1), 107–140. (<a
href="https://doi.org/10.1287/moor.2022.0100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model uncertainty approach to heavy traffic asymptotics that allows for a high level of uncertainty. That is, the uncertainty classes of underlying distributions accommodate disturbances that are of order 1 at the usual diffusion scale as opposed to asymptotically vanishing disturbances studied previously in relation to heavy traffic. A main advantage of the approach is that the invariance principle underlying diffusion limits makes it possible to define uncertainty classes in terms of the first two moments only. The model we consider is a single-server queue with multiple job types. The problem is formulated as a zero sum stochastic game played between the system controller, who determines scheduling and attempts to minimize an expected linear holding cost, and an adversary, who dynamically controls the service time distributions of arriving jobs and attempts to maximize the cost. The heavy traffic asymptotics of the game are fully solved. It is shown that an asymptotically optimal policy for the system controller is to prioritize according to an index rule, and for the adversary, it is to select distributions based on the system’s current workload. The workload-to-distribution feedback mapping is determined by a Hamilton–Jacobi–Bellman equation, which also characterizes the game’s limit value. Unlike in the vast majority of results in the heavy traffic theory and as a direct consequence of the diffusive size disturbances, the limiting dynamics under asymptotically optimal play are captured by a stochastic differential equation where both the drift and the diffusion coefficients may be discontinuous. Funding: R. Atar is supported by the Israeli Science Foundation [Grant 1035/20].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0100},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {107-140},
  shortjournal = {Math. Oper. Res.},
  title        = {Scheduling in the high-uncertainty heavy traffic regime},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial voting rules. <em>MOOR</em>, <em>50</em>(1),
90–106. (<a href="https://doi.org/10.1287/moor.2023.0080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study a new class of polynomial voting rules for a general decentralized decision/consensus system, and more specifically for the proof-of-stake protocol. The main idea, inspired by the Penrose square-root law and the more recent quadratic voting rule, is to differentiate a voter’s voting power and the voter’s share (fraction of the total in the system). We show that, whereas voter shares form a martingale process that converges to a Dirichlet distribution, their voting powers follow a supermartingale process that decays to zero over time. This prevents any voter from controlling the voting process and, thus, enhances security. For both limiting results, we also provide explicit rates of convergence. When the initial total volume of votes (or stakes) is large, we show a phase transition in share stability (or the lack thereof), corresponding to the voter’s initial share relative to the total. We also study the scenario in which trading (of votes/stakes) among the voters is allowed and quantify the level of risk sensitivity (or risk aversion) in three categories, corresponding to the voter’s utility being a supermartingale, a submartingale, and a martingale. For each category, we identify the voter’s best strategy in terms of participation and trading. Funding: W. Tang gratefully acknowledges financial support through the National Science Foundation [Grants DMS-2113779 and DMS-2206038] and through a start-up grant at Columbia University. D. D. Yao’s work is part of a Columbia–City University/Hong Kong collaborative project that is supported by InnoHK Initiative, the Government of Hong Kong Special Administrative Region, and the Laboratory for AI-Powered Financial Technologies.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0080},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {90-106},
  shortjournal = {Math. Oper. Res.},
  title        = {Polynomial voting rules},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow allocation games. <em>MOOR</em>, <em>50</em>(1), 68–89.
(<a href="https://doi.org/10.1287/moor.2022.0355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a game-theoretic variant of the maximum circulation problem. In a flow allocation game , we are given a directed flow network. Each node is a rational agent and can strategically allocate any incoming flow to the outgoing edges. Given the strategy choices of all agents, a maximal circulation that adheres to the chosen allocation strategies evolves in the network. Each agent wants to maximize the amount of flow through his or her node. Flow allocation games can be used to express strategic incentives of clearing in financial networks. We provide a cumulative set of results on the existence and computational complexity of pure Nash and strong equilibria as well as tight bounds on the (strong) prices of anarchy and stability. Our results show an interesting dichotomy. Ranking strategies over individual flow units allows us to obtain optimal strong equilibria for many objective functions. In contrast, more intuitive ranking strategies over edges can give rise to unfavorable incentive properties. Funding: This work was supported by Deutsche Forschungsgemeinschaft Research Group ADYN [411362735].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0355},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {68-89},
  shortjournal = {Math. Oper. Res.},
  title        = {Flow allocation games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards optimal problem dependent generalization error
bounds in statistical learning theory. <em>MOOR</em>, <em>50</em>(1),
40–67. (<a href="https://doi.org/10.1287/moor.2021.0076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study problem-dependent rates, that is, generalization errors that scale near-optimally with the variance, effective loss, or gradient norms evaluated at the “best hypothesis.” We introduce a principled framework dubbed “uniform localized convergence” and characterize sharp problem-dependent rates for central statistical learning problems. From a methodological viewpoint, our framework resolves several fundamental limitations of existing uniform convergence and localization analysis approaches. It also provides improvements and some level of unification in the study of localized complexities, one-sided uniform inequalities, and sample-based iterative algorithms. In the so-called “slow rate” regime, we provide the first (moment-penalized) estimator that achieves the optimal variance-dependent rate for general “rich” classes; we also establish an improved loss-dependent rate for standard empirical risk minimization. In the “fast rate” regime, we establish finite-sample, problem-dependent bounds that are comparable to precise asymptotics. In addition, we show that iterative algorithms such as gradient descent and first order expectation maximization can achieve optimal generalization error in several representative problems across the areas of nonconvex learning, stochastic optimization, and learning with missing data. Supplemental Material: The online appendix is available at https://doi.org/10.1287/moor.2021.0076 .},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0076},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {40-67},
  shortjournal = {Math. Oper. Res.},
  title        = {Towards optimal problem dependent generalization error bounds in statistical learning theory},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The online saddle point problem and online convex
optimization with knapsacks. <em>MOOR</em>, <em>50</em>(1), 1–39. (<a
href="https://doi.org/10.1287/moor.2018.0330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the online saddle point problem, an online learning problem where at each iteration, a pair of actions needs to be chosen without knowledge of the current and future (convex-concave) payoff functions. The objective is to minimize the gap between the cumulative payoffs and the saddle point value of the aggregate payoff function, which we measure using a metric called saddle point regret (SP-Regret). The problem generalizes the online convex optimization framework, but here, we must ensure that both players incur cumulative payoffs close to that of the Nash equilibrium of the sum of the games. We propose an algorithm that achieves SP-Regret proportional to ln ( T ) T in the general case, and log ( T ) SP-Regret for the strongly convex-concave case. We also consider the special case where the payoff functions are bilinear and the decision sets are the probability simplex. In this setting, we are able to design algorithms that reduce the bounds on SP-Regret from a linear dependence in the dimension of the problem to a logarithmic one. We also study the problem under bandit feedback and provide an algorithm that achieves sublinear SP-Regret. We then consider an online convex optimization with knapsacks problem motivated by a wide variety of applications, such as dynamic pricing, auctions, and crowdsourcing. We relate this problem to the online saddle point problem and establish O ( T ) regret using a primal-dual algorithm.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2018.0330},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {1-39},
  shortjournal = {Math. Oper. Res.},
  title        = {The online saddle point problem and online convex optimization with knapsacks},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ms---39">MS - 39</h2>
<ul>
<li><details>
<summary>
(2025). Signaling targeting cost through list price. <em>MS</em>,
<em>71</em>(3), 2733–2750. (<a
href="https://doi.org/10.1287/mnsc.2023.02031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data analytics enable firms to offer personalized prices to targeted consumers but at a cost. We study a competitive personalized pricing game where the entrant is uncertain about the incumbent’s targeting cost. We demonstrate that implementing personalized pricing through a “list price-discount” scheme allows the incumbent to signal its targeting cost via the list price. This signaling mechanism is effective because the list price serves as a price ceiling, which limits the incumbent’s ability to extract consumer surplus through personalized discounts. The high-cost incumbent can strategically set its list price below the full-information level to separate itself from the low-cost incumbent. Interestingly, the high-cost incumbent prefers separating over pooling only when there is a moderate variation in the incumbents’ targeting costs. Personalized pricing can affect firms differently, benefiting the incumbent but hurting the entrant. Asymmetric information about targeting costs weakens the high-cost incumbent’s incentive to offer personalized discounts, resulting in lower total targeting costs and potentially increasing social surplus. These findings shed light on government regulations and transparency policies regarding personalized pricing. This paper was accepted by Dmitri Kuksov, marketing. Funding: P. Yu was supported by the National Natural Science Foundation of China [Grants 72371038 and 72033003]. J. Zhang was supported by the National Natural Science Foundation of China [Grants 72371061 and 72232001]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/mnsc.2023.02031 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02031},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2733-2750},
  shortjournal = {Manag. Sci.},
  title        = {Signaling targeting cost through list price},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vaccine progress, stock prices, and the value of ending the
pandemic. <em>MS</em>, <em>71</em>(3), 2714–2732. (<a
href="https://doi.org/10.1287/mnsc.2023.00863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One measure of the ex ante cost of disasters is the welfare gain from shortening their expected duration. We introduce a stochastic clock into a standard disaster model that summarizes information about progress (positive or negative) toward disaster resolution. We show that the stock market response to duration news is essentially a sufficient statistic to identify the welfare gain to interventions that alter the state. Using information on clinical trial progress during 2020, we build contemporaneous forecasts of the time to vaccine deployment, which provide a measure of the anticipated length of the COVID-19 pandemic. The model can thus be calibrated from market reactions to vaccine news, which we estimate. The estimates imply that ending the pandemic would have been worth from 5% to 15% of total wealth as the expected duration varied in this period. This paper was accepted by Lukas Schmid, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00863 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00863},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2714-2732},
  shortjournal = {Manag. Sci.},
  title        = {Vaccine progress, stock prices, and the value of ending the pandemic},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness to dependency in influence maximization.
<em>MS</em>, <em>71</em>(3), 2696–2713. (<a
href="https://doi.org/10.1287/mnsc.2021.03445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we pursue a correlation-robust study of the influence maximization problem. Departing from the classic independent cascade model, we study a diffusion process adversarially adapted to the choice of seed set. More precisely, rather than the independent coupling of known individual edge probabilities, we now evaluate a seed set’s expected influence under all possible correlations, specifically, the one that presents the worst case. We find that the worst case expected influence can be efficiently computed, its NP-hard optimization done approximately ( 1 − 1 / e ) with greedy construction, and we provide complete, efficient characterizations of the adversarial coupling, the random graph, and the random number of influenced nodes. But, most importantly, upon mixing the independent cascade with the worst case, we attain a tunable and more comprehensive model better suited for real-world diffusion phenomena than the independent cascade alone and without increased computational complexity. Extensions to the correlation-robust study of risk follow along with numerical experiments on network data sets with demonstration of how our models can be tuned. This paper was accepted by George Shanthikumar, data science. Funding: This work was supported by the Air Force Office of Scientific Research (Mathematical Optimization Program) under the grant: “Optimal Decision Making under Tight Performance Requirements in Adversarial and Uncertain Environments: Insight from Rockafellian Functions”. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03445 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03445},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2696-2713},
  shortjournal = {Manag. Sci.},
  title        = {Robustness to dependency in influence maximization},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving tensions between heterogeneous investors in a
startup. <em>MS</em>, <em>71</em>(3), 2678–2695. (<a
href="https://doi.org/10.1287/mnsc.2022.01724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legal scholars highlight the tensions that exist between different classes of shareholders in startups. We model a startup owned by undiversified investors with heterogeneous capital contributions and risk preferences. A social planner runs the firm on behalf of all investors. We compare investors’ expected utility with a hypothetical first-best decentralized benchmark. The startup’s optimal investment policy is procyclical and a time-varying weighted average of shareholders’ optimal investment policies. The optimal contracts issued to investors are tailor-made, interdependent, and include equity claims resembling preferred stock with heterogeneous payout caps, leading to a complex capitalization table as more investors join the startup. This paper was accepted by Will Cong, finance. Funding: This work was supported by the Cambridge Endowment for Research in Finance and Keynes Fellowship. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2022.01724 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01724},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2678-2695},
  shortjournal = {Manag. Sci.},
  title        = {Resolving tensions between heterogeneous investors in a startup},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revealed privacy preferences: Are privacy choices rational?
<em>MS</em>, <em>71</em>(3), 2657–2677. (<a
href="https://doi.org/10.1287/mnsc.2022.00807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the extent to which trade-offs involving the sharing of personal information exhibit consistency with an underlying rational preference for privacy. In an experiment, people engage in trade-offs across two domains of personal information sharing, allowing us to classify whether their choices satisfy the generalized axiom of revealed preference. They also subsequently price the sharing of their personal information. Sixty-three percent of subjects act consistently with a rational preference ordering when allocating privacy levels, a level comparable to that observed in other domains. Individuals who are inconsistent when engaging in simple privacy trade-offs exhibit substantially more costly preference reversals when pricing personal information sharing. Only one-third of subjects exhibit consistency across both the selection of personal information to share and the pricing of such information sharing. Our results imply that monetizing personal information can have distinct welfare consequences for people with different degrees of rationality in their underlying ability to make sensible trade-offs involving personal information. We also provide evidence that preferences elicited over choices in our experiment correlate with real-world privacy behaviors. This paper was accepted by Yan Chen, behavioral economics and decision analysis. Funding: This work was supported by Forschungskredit (University of Zurich) [Grant FK-17-017] and Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung [Grant 174630] to Y.S. Lee. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00807 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00807},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2657-2677},
  shortjournal = {Manag. Sci.},
  title        = {Revealed privacy preferences: Are privacy choices rational?},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The co-production of service: Modeling services in contact
centers using hawkes processes. <em>MS</em>, <em>71</em>(3), 2635–2656.
(<a href="https://doi.org/10.1287/mnsc.2021.04060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In customer support contact centers, every service interaction involves a messaging dialogue between a customer and an agent; together, they exchange information, solve problems, and collectively co-produce the service. Because the service progression is shaped by the history of conversation thus far, we propose a bivariate marked Hawkes process cluster model of the customer-agent interaction. To evaluate our stochastic model of service, we apply it to an industry contact center data set containing nearly 5 million messages. Through both a novel residual analysis comparison and several Monte Carlo goodness-of-fit tests, we show that the Hawkes cluster model indeed captures dynamics at the heart of the service and surpasses classic models that do not incorporate the service history. Furthermore, in an entirely data-driven simulation, we demonstrate how this history-dependent model can be leveraged operationally to inform a prediction-based routing policy. We show that widely used and well-studied customer routing policies can be outperformed with simple modifications according to the Hawkes model. Through analysis of a stylized model proposed in the contact center literature, we prove that service heterogeneity can cause this underperformance and, moreover, that such heterogeneity will occur if service closures are not carefully managed. This paper was accepted by Elena Katok, operations management. Funding: The authors are grateful for the generous support of this work by the National Science Foundation Division of Graduate Education [Grant DGE-1650441] (A. Daw), the Israel Science Foundation [Grant 336/19] (G. B. Yom-Tov), and the United States-Israel Binational Science Foundation [Grant 2022095] (A. Daw, G. B. Yom-Tov). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.04060 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.04060},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2635-2656},
  shortjournal = {Manag. Sci.},
  title        = {The co-production of service: Modeling services in contact centers using hawkes processes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consumption of values. <em>MS</em>, <em>71</em>(3),
2623–2634. (<a href="https://doi.org/10.1287/mnsc.2023.01632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumption decisions are partly influenced by values and ideologies. Consumers care about global warming, child labor, fair trade, etc. We develop an axiomatic model of intrinsic values—those that are carriers of meaning in and of themselves—and argue that they often introduce discontinuities near zero. For example, a vegetarian’s preferences would be discontinuous near zero amount of animal meat. We distinguish intrinsic values from instrumental ones, which are means rather than ends and serve as proxies for intrinsic values. We illustrate the relevance of our value-based model in different contexts, including equity concerns and prosocial behavior. This paper was accepted by Manel Baucells, behavioral economics and decision analysis. Funding: The authors acknowledge support from the Agence Nationale de la Recherche [Grants ANR-11-IDEX-0003 and 11-LABX-0047], the Israel Science Foundation [Grant 1443/20], the AXA Research Fund [Chair for Decision Sciences], the Foerder Institute at Tel-Aviv University, and the Sapir Center for Economic Development (Gilboa). Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2023.01632 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01632},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2623-2634},
  shortjournal = {Manag. Sci.},
  title        = {Consumption of values},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risks to human capital. <em>MS</em>, <em>71</em>(3),
2583–2622. (<a href="https://doi.org/10.1287/mnsc.2022.03068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build a model with inalienable human capital, in which investors finance individuals who can potentially become skilled. Although investment in skill is always optimal, it does not take place in some states of the world, due to moral hazard. In intermediate states of the world, individuals acquire skill; however outside investors and individuals inefficiently share risk. We show that this simple moral hazard problem, combined with risk aversion of individuals and outside investors, amplifies the equity premium, lowers the risk-free rate, and leads to disaster states that fall especially heavily on some agents but not on others. We show that the possibility of disaster states distorts risk prices and affects wealth inequality, even under calibrations in which they never occur in equilibrium. This paper was accepted by Lukas Schmid, finance. Funding: M. Ebrahimian acknowledges funding from the Swedish House of Finance. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2022.03068 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.03068},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2583-2622},
  shortjournal = {Manag. Sci.},
  title        = {Risks to human capital},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the robustness of idiosyncratic volatility effect.
<em>MS</em>, <em>71</em>(3), 2565–2582. (<a
href="https://doi.org/10.1287/mnsc.2022.04140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idiosyncratic volatility (IVol) effect is robust to restricting the sample to New York Stock Exchange (NYSE) firms (once the proper listing indicator is used) and to excluding from the sample small, illiquid, and low-price stocks. The idiosyncratic volatility effect is also unlikely to stem from the short-run reversal, as the IVol effect stays significant for about six months and seems stronger for high turnover firms, which do not exhibit short-term reversal. The IVol effect also does not seem to weaken postpublication. This paper was accepted by Lukas Schmid, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.04140 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.04140},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2565-2582},
  shortjournal = {Manag. Sci.},
  title        = {On the robustness of idiosyncratic volatility effect},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selling bonus actions in video games. <em>MS</em>,
<em>71</em>(3), 2544–2564. (<a
href="https://doi.org/10.1287/mnsc.2022.02348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mobile video games industry, a common in-app purchase is for additional “moves” or “time” in single-player puzzle games. We call these in-app purchases bonus actions . In some games, bonus actions can only be purchased in advance of attempting a level of the game (pure advance sales (PAS)), yet in other games, bonus actions can only be purchased in a “spot” market that appears when an initial attempt to pass the level fails (pure spot sales). Some games offer both advance and spot purchases (hybrid advance sales). This paper studies these selling strategies for bonus actions in video games. Such a question is novel to in-app tools selling in video games, and it cannot be answered by previous advance selling studies focusing on end goods. We model the selling of bonus actions as a stochastic extensive form game. We show how the distribution of skill among players (i.e., their inherent ability to pass the level) and the inherent randomness of the game influence selling strategies. For casual games, where low-skill players have a sufficiently high probability of success in each attempt, if the proportion of high-skill players is either sufficiently large or sufficiently small, firms should adopt PAS and shut down the “spot” market. Furthermore, the player welfare-maximizing selling strategy is to sell only in the spot market. Hence, no “win-win” strategy exists for casual games. However, PAS can be a win-win for hardcore games, where low-skill players have a sufficiently low success probability for each attempt. This paper was accepted by Hemant Bhargava, information systems. Funding: C. T. Ryan received funding from NSERC Discovery [Grant RGPIN-2020-06488]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2022.02348 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02348},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2544-2564},
  shortjournal = {Manag. Sci.},
  title        = {Selling bonus actions in video games},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human in the loop automation: Ride-hailing with remote
(tele-)drivers. <em>MS</em>, <em>71</em>(3), 2527–2543. (<a
href="https://doi.org/10.1287/mnsc.2022.01687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tele-driving refers to a novel concept by which drivers can remotely operate vehicles (without being physically in the vehicle). By putting the human back in the loop, tele-driving has emerged recently as a more viable alternative to fully automated vehicles with ride-hailing (and other on-demand transportation-enabled services) being an important application. Because remote drivers can be operated as a shared resource (any driver can be assigned to any customer regardless of trip origin or destination), it may be possible for such services to deploy fewer drivers than vehicles without significantly reducing service quality. In this paper, we examine the extent to which this is possible. Using a spatial queueing model that captures the dynamics of both pickup and trip times, we show that the impact of reducing the number of drivers depends crucially on system workload relative to the number of vehicles. In particular, when workload is sufficiently high relative to the number of vehicles, we show that, perhaps surprisingly, reducing the number of drivers relative to the number of vehicles can actually improve service level (e.g., as measured by the amount of demand fulfilled in the case of impatient customers). Having fewer drivers than vehicles ensures that there are always idle vehicles; the fewer the drivers, the likelier it is for there to be more idle vehicles. Consequently, the fewer the drivers, the likelier it is for the pickup times to be shorter (making overall shorter service times likelier). The impact of shorter service time is particularly significant when the workload is high, and in this case, it is enough to overcome the loss in driver capacity. When workload is sufficiently low relative to the number of vehicles, we show that it is possible to significantly reduce the number of drivers without significantly reducing service level. In systems in which customers are patient and willing to queue up for the service, we show that reducing the number of drivers can also reduce delay, including stabilizing a system that may otherwise be unstable. In general, relative to a system in which the number of vehicles equals the number of drivers (as in a system with in-vehicle drivers), a system with remote drivers can result in savings in the number of drivers either without significantly degrading performance or actually improving performance. We discuss how these results can, in part, be explained by the interplay of two counteracting forces: (1) having fewer drivers increasing service rate and (2) having fewer drivers reducing the number of servers with the relative strength of these forces depending on system workload. This paper was accepted by Baris Ata, stochastic models and simulation. Funding: This work was supported by the US National Science Foundation [Grant SCC-1831140], and the Guangdong (China) Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [2023B1212010001]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01687 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01687},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2527-2543},
  shortjournal = {Manag. Sci.},
  title        = {Human in the loop automation: Ride-hailing with remote (Tele-)Drivers},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stable matching on the job? Theory and evidence on internal
talent markets. <em>MS</em>, <em>71</em>(3), 2508–2526. (<a
href="https://doi.org/10.1287/mnsc.2023.01373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A principal often needs to match agents to perform coordinated tasks, but agents can quit or slack off if they dislike their match. We study two prevalent approaches for matching within organizations: centralized assignment by firm leaders and self-organization through market-like mechanisms. We provide a formal model of the strengths and weaknesses of both methods under different settings, incentives, and production technologies. The model highlights trade-offs between match-specific productivity and job satisfaction. We then measure these trade-offs with data from a large organization’s internal talent market. Firm-dictated matches are 33% more valuable than randomly assigned matches within job categories (using the firm’s preferred metric of quality). By contrast, preference-based matches (using deferred acceptance) are only 5% better than random but are ranked (on average) about 38 percentiles higher by the workforce. The self-organized match is positively assortative and helps workers grow new skills; the firm’s preferred match is negatively assortative and harvests existing expertise. This paper was accepted by Joshua Gans, business strategy. Funding: This work was supported by the Ewing Marion Kauffman Foundation (Emerging Scholar Award). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01373 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01373},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2508-2526},
  shortjournal = {Manag. Sci.},
  title        = {Stable matching on the job? theory and evidence on internal talent markets},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do firms proactively build internal markets for
redeployment? Evidence from a natural experiment. <em>MS</em>,
<em>71</em>(3), 2487–2507. (<a
href="https://doi.org/10.1287/mnsc.2021.02396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of related diversification is of fundamental interest to corporate strategy research. However, the understanding of how the potential for resource redeployment might encourage related diversification is limited. This paper explores the impact of the 2006 German real estate transfer tax (RETT) reform, which led to higher RETT rates but retained the exemption from RETT for corporate restructurings. We propose that firms might have reacted strategically to the threat of higher transfer taxes by combining related businesses into a portfolio, providing an efficient internal market for reallocating property assets in the event of business-specific shocks. Consistent with a dynamic perspective on relatedness, difference-in-differences analyses of the European retail market show that the probability of related diversification increased significantly for firms that were affected by the reform, relative to control firms. This finding contributes to the literature that links firm boundary choices to intertemporal economies of scope and offers new insights into the real effects of transfer tax policies. This paper was accepted by Alfonso Gambardella, business strategy. Funding: This work was supported by the Ministerio de Ciencia, Innovación y Universidades [Grant PID2020-115660GB-I00]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.02396 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.02396},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2487-2507},
  shortjournal = {Manag. Sci.},
  title        = {Do firms proactively build internal markets for redeployment? evidence from a natural experiment},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The operational data analytics (ODA) for service speed
design. <em>MS</em>, <em>71</em>(3), 2467–2486. (<a
href="https://doi.org/10.1287/mnsc.2023.00655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop the operational data analytics (ODA) framework for the classical service design problem of G / G / c / k systems. The customer arrival rate is unknown. Instead, some historical data of interarrival times are collected. The data-integration model, specifying the mapping from the arrival data to the service rate, is formulated based on the time-scaling property of the stochastic service process. Validating the data-integration model against the long-run average service reward leads to a uniformly optimal service rate for any given sample size. We further derive the ODA-predicted reward function based on the data-integration model, which gives a consistent estimate of the underlying reward function. Our numerical experiments show that the ODA framework can lead to an efficient design of service rate and service capacity, which is insensitive to model specification. The ODA solution exhibits superior performance compared with the conventional estimation-and-then-optimization solutions in the small sample regime. This paper was accepted by David Simchi-Levi, operations management. Funding: Z. Jiang’s research is supported by the National Natural Science Foundation of China [Grant 71931007]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2023.00655 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00655},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2467-2486},
  shortjournal = {Manag. Sci.},
  title        = {The operational data analytics (ODA) for service speed design},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating career benefits from online community leadership:
Evidence from stack exchange moderators. <em>MS</em>, <em>71</em>(3),
2443–2466. (<a href="https://doi.org/10.1287/mnsc.2019.03252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many IT professionals seek to improve their job prospects by engaging as leaders of online communities, for example, by serving as a moderator or admin. We investigate whether such community leadership leads to (causal) improvements in individuals’ careers. We assemble a data set, including job histories of IT professionals who have sought election as moderators (mods) in Stack Exchange question-and-answer communities, between 2010 and 2020. We estimate the career benefits of moderatorship under two complementary identification strategies: difference-in-differences (DID) and regression discontinuity (RD). We observe qualitatively consistent results under each design, finding that election to a moderator role has a significant, causal, positive effect on job mobility. We estimate that moderatorship increases the probability of a job change by between 4.7 and 12.3 percentage points over the two years following the election. We also report a series of secondary analyses that speak to associated salary increases and show evidence consistent with the notion that social capital and signaling play a role. Our findings help us understand the benefits of online community leadership, and they extend our understanding of the motivations for online community engagement. This paper was accepted by Chris Forman, information systems. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2019.03252 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2019.03252},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2443-2466},
  shortjournal = {Manag. Sci.},
  title        = {Estimating career benefits from online community leadership: Evidence from stack exchange moderators},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Number processing constraints and earnings news.
<em>MS</em>, <em>71</em>(3), 2413–2442. (<a
href="https://doi.org/10.1287/mnsc.2023.01722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroscience shows that human brains are neurologically constrained to process small numbers linearly and large numbers logarithmically, leading to underreactions to larger numbers as their perceived difference becomes smaller. We test this hypothesis in the context of earnings announcements and find that investors respond less in the short term to earnings news for stocks with high earnings per share magnitudes, exacerbating postearnings announcement drift for these stocks. These findings are distinct from and incremental to several risk-based and behavioral explanations, attenuated by robot presence and present in a quasi-experimental design using stock splits. Our evidence suggests that number processing constraints have implications for stock price efficiency. This paper was accepted by Will Cong, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01722 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01722},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2413-2442},
  shortjournal = {Manag. Sci.},
  title        = {Number processing constraints and earnings news},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Insider trading restrictions and informed trading in peer
stocks. <em>MS</em>, <em>71</em>(3), 2390–2412. (<a
href="https://doi.org/10.1287/mnsc.2022.02907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a uniquely constructed data set of trades by corporate insiders in all stocks, we find that, after insider trading regulations become stricter, insiders are 20% more likely to trade in peer stocks and that such trades become more profitable. The increase in both the probability and profitability of peer-stock trades is driven by the insider’s information that is fungible to industry peers. Stricter insider trading laws are designed to improve liquidity and price informativeness in capital markets. We show that peer trading dampens these intended benefits of the insider trading regulation. This paper was accepted by Ranjani Krishnan, accounting. Funding: This work was supported by NSE-NYU Stern Initiative on the Study of Indian Financial Markets. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02907 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02907},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2390-2412},
  shortjournal = {Manag. Sci.},
  title        = {Insider trading restrictions and informed trading in peer stocks},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovation strategy after IPO: How AI analytics spurs
innovation after IPO. <em>MS</em>, <em>71</em>(3), 2360–2389. (<a
href="https://doi.org/10.1287/mnsc.2022.01559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the role of AI analytics in facilitating innovation in firms that have gone through IPO. Using patent data on over 1,000 publicly traded firms, we find that firms acquiring AI analytics capability post-IPO experience less of a decline in innovation quality compared with similar firms that have not acquired that capability. This effect is greater when only machine learning capabilities are considered. Moreover, we find this sustained rate of innovation is driven principally by the continued development of innovations that combine existing technologies into new ones—a form of innovation that is especially well supported by analytics. By examining three main mechanisms that hampered post-IPO innovation, we find that AI analytics can ameliorate the pressure to meet short-term financial goals and disclosure requirements. However, it has limited effect in addressing managerial incentives. For firms with long product cycles, the disclosure effect is reduced to a greater extent than it is for those with short cycles. Overall, our results show the importance of examining technology as a critical input factor in innovation. We show that the increased deployment of AI analytics may reduce some of the innovative penalties suffered by IPOs and that investors and managers can potentially mitigate post-IPO reductions in innovative output by directing capital acquired in the IPO process to the acquisition of AI analytics capabilities. This paper was accepted by D. J. Wu, information systems. Funding: The authors appreciate the generous financial support from Wharton Dean’s Research Fund and Mack Institute for Innovation Management. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2022.01559 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01559},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2360-2389},
  shortjournal = {Manag. Sci.},
  title        = {Innovation strategy after IPO: How AI analytics spurs innovation after IPO},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information search within a web page: Modeling the full
sequence of eye movement decisions, subjective value updating, and first
clicks. <em>MS</em>, <em>71</em>(3), 2332–2359. (<a
href="https://doi.org/10.1287/mnsc.2022.02983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online retail settings often present shoppers with large, complex choice sets where they need to quickly and dynamically weigh the benefits and costs of search within each web page. We build a model of information search within a web page using eye-tracking data collected during two incentive-compatible online shopping experiments, in which participants browsed the websites of two different clothing retailers (Experiments 1 and 2), as well as previously reported data from a laboratory experiment involving choices among snack food assortments (Experiment 3). Our model incorporates features that build upon recent advances in descriptive and normative models of information sampling and search in psychology and economics. First, our model captures how people decide where to look by treating eye fixations on clickable options as a series of “split-second” decisions that depend on estimates of option attractiveness and navigation effort. Second, our model assumes that the value of each option is learned via Bayesian updating. Third, the choice to end search on the web page depends on a dynamic decision threshold. Our model outperforms benchmarks that assume random search, instant learning, fixed thresholds, nonheterogeneous thresholds, and stochastic accumulator stopping rules. Explicitly modeling the sequence of eye fixation decisions results in accurate counterfactual simulations of the effects of hypothetical product orderings on search duration and quality as verified using experimental manipulation, and it can be applied flexibly to a wide range of web-page layouts. Systematic differences across experiments highlight the importance of accounting for product familiarity, choice-set size, and the role of category outside options. This paper was accepted by Ilia Tsetlin, behavioral economics and decision analysis. Funding: This work was supported by the Jay H. Baker Retailing Center and the Wharton Behavioral Lab at the University of Pennsylvania. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02983 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02983},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2332-2359},
  shortjournal = {Manag. Sci.},
  title        = {Information search within a web page: Modeling the full sequence of eye movement decisions, subjective value updating, and first clicks},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forgetful consumers and consumption tracking. <em>MS</em>,
<em>71</em>(3), 2311–2331. (<a
href="https://doi.org/10.1287/mnsc.2023.00522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the market consequences of advances in consumption tracking technologies—such as mobile banking apps that help consumers monitor their spending and avoid overdrawn accounts—using a two-period consumption model. In the model, consumers pay a penalty fee if they consume in both periods. In the second period, consumers may be forgetful of their first-period consumption, although the use of consumption tracking can remind them. According to our analysis, the availability of consumption tracking often helps consumers at the expense of the firm; such benefits may be direct , where consumers make use of the technology to avoid penalty fees, or indirect , where the mere availability of consumption tracking forces the firm to lower its penalty fee. If consumers are partially sophisticated regarding their forgetfulness, however, the availability of consumption tracking may instill a false sense of security in that consumers expect to use consumption tracking to avoid penalty fees but ultimately, decide not to bother, making them especially susceptible to penalty fees. In some cases, the availability of consumption tracking may actually compel a firm to impose a penalty fee that would not otherwise be viable, leading to higher profits and lower consumer surplus. As we show, this scenario is attained within an intermediate range of forgetfulness and at a level of (partial) sophistication for which consumers overestimate their demand for the technology. This paper was accepted by Dmitri Kuksov, marketing. Funding: M. Shi appreciates financial support from HKUST Yuk-Shee Chan Professorship Fund and China NSF [Grant 72272036]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2023.00522 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00522},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2311-2331},
  shortjournal = {Manag. Sci.},
  title        = {Forgetful consumers and consumption tracking},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Who lends before banking crises? Evidence from the
international syndicated loan market. <em>MS</em>, <em>71</em>(3),
2289–2310. (<a href="https://doi.org/10.1287/mnsc.2022.03505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies assume that all lenders have similar incentives to take on risks during different phases of the lending cycle. We show that foreign lenders and low market share lenders extend more credit in comparison with other lenders during lending booms leading to banking crises but not during other credit expansions. These less established lenders also shorten loan maturity and increase the amount of credit they extend to riskier borrowers without asking for collateral or imposing covenants and higher interest rates. Our results suggest that foreign lenders and low market share lenders contribute disproportionately to credit misallocation and risk accumulation in precrisis periods. This paper was accepted by Victoria Ivashina, finance. Funding: M. Giannetti acknowledges financial support from the Bank of Sweden Tercentenary Foundation [P16-0328] and Jan Wallanders och Tom Hedelius stiftelse [P20-0127]. Supplemental Material: The internet appendix and data files are available at https://doi.org/10.1287/mnsc.2022.03505 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.03505},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2289-2310},
  shortjournal = {Manag. Sci.},
  title        = {Who lends before banking crises? evidence from the international syndicated loan market},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of dispersion on the informativeness of consensus
analyst target prices. <em>MS</em>, <em>71</em>(3), 2264–2288. (<a
href="https://doi.org/10.1287/mnsc.2021.03549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus analyst target prices are widely available online at no cost to investors. In this paper, we examine how the amount of dispersion in the individual target prices comprising the consensus affects the predictive association between the consensus target price and future returns. We find that returns implied by consensus target prices and realized future returns are positively correlated when dispersion is low, but they become highly negatively correlated when dispersion is high. Further analyses suggest that the differing effect of dispersion stems from incentive-driven staleness in price targets by some analysts after bad news. As a stock performs poorly and some analysts are slow to update their target prices, dispersion increases, and the consensus target price becomes too high. This has important implications for how consensus analyst target prices should inform investment decisions. We show that a hedge strategy taking a long (short) position in stocks with the highest predicted returns among stocks with the lowest (highest) dispersion earns more than 11% annually. Finally, we show that the negative correlation between consensus-based predicted returns and future realized returns for high-dispersion stocks exists mainly for stocks with high retail interest, suggesting that unsophisticated investors are misled by inflated target prices that are available freely online. This paper was accepted by Suraj Srinivasan, accounting. Funding: The authors acknowledge financial support from Indiana University and Yale University. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2021.03549 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03549},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2264-2288},
  shortjournal = {Manag. Sci.},
  title        = {The effect of dispersion on the informativeness of consensus analyst target prices},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic behavior with tight, loose, and polarized norms.
<em>MS</em>, <em>71</em>(3), 2245–2263. (<a
href="https://doi.org/10.1287/mnsc.2023.01022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Descriptive norms, the behavior of other individuals in one’s reference group, play a key role in shaping individual decisions in managerial contexts and beyond. Organizations are increasingly using information about descriptive norms to nudge positive behavior change. When characterizing peer decisions, a standard approach in the literature is to focus on average behavior. In this paper, we argue both theoretically and empirically that not only averages but also the shape of the whole distribution of behavior can play a crucial role in how people react to descriptive norms. Using a representative sample of the U.S. population, we experimentally investigate how individuals react to strategic environments that are characterized by different distributions of behavior, focusing on the distinction between tight (i.e., characterized by low behavioral variance), loose (i.e., characterized by high behavioral variance), and polarized (i.e., characterized by u-shaped behavior) environments. We find that individuals indeed strongly respond to differences in the variance and shape of the descriptive norm they are facing: Loose norms generate greater behavioral variance and polarization generates polarized responses. In polarized environments, most individuals prefer extreme actions, which expose them to considerable strategic risk, to intermediate actions that minimize such risk. Furthermore, in polarized and loose environments, personal traits and values play a larger role in determining actual behavior. These nuances of how individuals react to different types of descriptive norms have important implications for company culture, productivity, and organizational effectiveness alike. This paper was accepted by Dorothea Kübler, behavioral economics and decision analysis. Funding: This work was supported by the German Research Foundation under Germany’s Excellence Strategy [EXC 2126/1–390838866]. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2023.01022 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01022},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2245-2263},
  shortjournal = {Manag. Sci.},
  title        = {Strategic behavior with tight, loose, and polarized norms},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The international commonality of idiosyncratic variances.
<em>MS</em>, <em>71</em>(3), 2216–2244. (<a
href="https://doi.org/10.1287/mnsc.2022.01398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We document strong global commonality in country idiosyncratic return variances across 23 developed markets, which is stronger than international return commonality. The global common factor of idiosyncratic return variances is highly correlated with that of idiosyncratic cash flow variances and is also significantly related to variables capturing aggregate discount rate variation and the conditional market variance. Furthermore, aggregate idiosyncratic return and cash flow variances are mostly but not always countercyclical. This paper was accepted by Kay Giesecke, finance. Funding: X. Zhang acknowledges financial support from the National Natural Science Foundation of China [Grant 72350710220] and the Beijing Natural Science Foundation [Grant IS23127]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01398 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01398},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2216-2244},
  shortjournal = {Manag. Sci.},
  title        = {The international commonality of idiosyncratic variances},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technological changes and countries’ tax policy design:
Evidence from anti–tax avoidance rules. <em>MS</em>, <em>71</em>(3),
2192–2215. (<a href="https://doi.org/10.1287/mnsc.2021.03955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the association between technological changes and corporate tax policies in 34 OECD countries between 1996 and 2016. We use a shift-share design to capture the differential exposure of countries to U.S. technological advancements. Our study shows that countries’ antitax avoidance rules are tightened as their exposure to U.S. technological advancements increases. The tightening is particularly concentrated in countries that are larger, more exposed to intangibles, and have higher profit shifting incentives. Our findings have important implications for corporate executives as a country’s anti–tax avoidance rules are associated with foreign technological advancements. This paper was accepted by Eric So, accounting. Funding: A. I. Brühne and M. Jacob acknowledge funding by the Deutsche Forschungsgemeinschaft [Project-ID 403041268–TRR 266 Accounting for Transparency]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03955 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03955},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2192-2215},
  shortjournal = {Manag. Sci.},
  title        = {Technological changes and countries’ tax policy design: Evidence from Anti–Tax avoidance rules},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stars in their constellations: Great person or great team?
<em>MS</em>, <em>71</em>(3), 2170–2191. (<a
href="https://doi.org/10.1287/mnsc.2021.01969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although much attention is accorded to star performers, this paper considers the extent to which stars, themselves, benefit from the contribution of their collaborators (the constellation). By considering stars, constellations, and the synergies between them, we address a key question: To what extent is collaboration performance driven by the great individual or by great constellations? We introduce a novel approach that uses a matching model to uncover the complementarities driving collaboration formation. We use formal value-capture theory to estimate the relative contribution of stars and constellations to joint value creation. Analyzing a sample of academic research collaborations, we document that stars’ relative contribution exceeds that of their constellations in less than 15% of collaborations, although constellations provide a greater relative contribution in 9%. In most collaborations, neither party dominates: Innovation is a collective endeavor driven equally by the star and the constellation. Joint value creation and relative contribution are explained by the subtle interplay between complementarities in joint work and the substitutability of collaborative parties in the market. Joint value creation increases with the strength of complementarities between parties in a match. Relative value creation, and hence dominance, increases with the substitutability of one’s collaborative partner. Interestingly, joint value creation is greatest in collaborations where both stars and constellations offer bundles of rare attributes and where neither the star nor the constellation dominates. This paper was accepted by Olav Sorenson, organizations. Funding: D. Mindruta gratefully acknowledges funding from the HEC Foundation and from the French National Research Agency (ANR) “Investissements d’Avenir” (LabEx Ecodec/ANR-11-LABX-0047). J. Bercovitz and M. Feldman gratefully acknowledge funding from the Science of Science Approach to Analyzing and Innovating the Biomedical Research Enterprise (SCISIPBIO) program of the U.S. National Science Foundation (NSF) [Grant 1934875]. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2021.01969 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01969},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2170-2191},
  shortjournal = {Manag. Sci.},
  title        = {Stars in their constellations: Great person or great team?},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investor-paid credit ratings and managerial information
disclosure. <em>MS</em>, <em>71</em>(3), 2142–2169. (<a
href="https://doi.org/10.1287/mnsc.2021.01914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike issuer-paid credit rating agencies (CRAs), investor-paid CRAs are compensated by investors for providing rating services. Exploiting the staggered timing of rating initiation by an investor-paid rating agency (the Egan Jones Ratings (EJR)), I document that the coverage by EJR increases rated firm managers’ voluntary disclosure of negative news. Consistent with EJR’s rating coverage deterring managerial bad news hoarding by informing investors of downside risks, I find that the effect of EJR coverage is more pronounced when issuer-paid CRAs tend to assign inflated ratings and when rated firms’ managers have a stronger incentive to conceal bad news. I also document that firms unwind upward earnings management after being covered by EJR. In contrast, coverage by an issuer-paid CRA (Standard &amp; Poor’s) is not associated with changes in managerial information disclosure. I conclude that investor-paid CRAs function as a type of effective information intermediary to discipline firm managers and improve corporate transparency. This paper was accepted by Brian Bushee, accounting. Funding: The author acknowledge financial support from the MOE Project of the Key Research Institute of Humanities and Social Science in University [Grants 22JJD790093 and 22JJD790094], the Program for Innovative Research Team of Shanghai University of Finance and Economics, and the 111 Project [Grant B18033]. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2021.01914 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01914},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2142-2169},
  shortjournal = {Manag. Sci.},
  title        = {Investor-paid credit ratings and managerial information disclosure},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gender promotion gaps and career aspirations. <em>MS</em>,
<em>71</em>(3), 2127–2141. (<a
href="https://doi.org/10.1287/mnsc.2023.00715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a representative survey of U.S. lawyers, we document a sizeable gender gap in early partnership aspirations, which explains half of the later gender promotion gap. We further document that the correlation between aspirations and effort provides a “mechanical link” between aspirations and promotion. Early workplace experiences, such as harassment and demeaning comments, are linked to promotion aspirations. Moreover, early aspirations provide insight into eventual promotion outcomes that goes beyond what can be drawn only from expectations. Our study highlights that measuring aspirations and adapting the corporate culture that shapes them are key components for firms to improve workplace environments. This paper was accepted by Joshua Gans, business strategy. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00715 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00715},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2127-2141},
  shortjournal = {Manag. Sci.},
  title        = {Gender promotion gaps and career aspirations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction-driven surge planning with application to
emergency department nurse staffing. <em>MS</em>, <em>71</em>(3),
2079–2126. (<a href="https://doi.org/10.1287/mnsc.2021.02781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining emergency department (ED) nurse staffing decisions to balance quality of service and staffing costs can be extremely challenging, especially when there is a high level of uncertainty in patient demand. Increasing data availability and continuing advancements in predictive analytics provide an opportunity to mitigate demand uncertainty by using demand forecasts. In this work, we study a two-stage prediction-driven staffing framework where the prediction models are integrated with the base (made weeks in advance) and surge (made nearly real-time) nurse staffing decisions in the ED. We quantify the benefit of having the ability to use the more expensive surge staffing and identify the importance of balancing demand uncertainty versus system stochasticity. We also propose a near-optimal two-stage staffing policy that is straightforward to interpret and implement. Last, we develop a unified framework that combines parameter estimation, real-time demand forecasts, and nurse staffing in the ED. High-fidelity simulation experiments for the ED demonstrate that the proposed framework has the potential to reduce annual staffing costs by 10%–16% ($2 M–$3 M) while guaranteeing timely access to care. This paper was accepted by David Simchi-Levi, healthcare management. Funding: J. Dong was partially supported by the Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-1944209]. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2021.02781 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.02781},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2079-2126},
  shortjournal = {Manag. Sci.},
  title        = {Prediction-driven surge planning with application to emergency department nurse staffing},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Like stars: How firms learn at scientific conferences.
<em>MS</em>, <em>71</em>(3), 2056–2078. (<a
href="https://doi.org/10.1287/mnsc.2022.01373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific conferences are an underexplored channel by which firms can learn from science. We provide empirical evidence that firms learn from scientific conferences in which they participate but also that this is conditional on intense participation. Using data from conference papers in computer science since the 1990s, we show that corporate investments in participation are both frequent and highly skewed, with some firms contributing to a given conference scientifically, some as sponsors, and some doing both. We use direct flights as an instrumental variable for the probability that other scientists participate in the same conference as a firm, altering the knowledge set to which the firm is exposed. We find that a firm’s use of scientists’ knowledge increases when they participate in the same conferences. Greater participation efforts, where the firm seeks the spotlight by both sponsoring the conference and contributing to its scientific discourse, foretell research collaborations and a stronger learning effect. Such learning is disproportionately concentrated among the most prominent firms and scientists rather than benefitting those without alternative interaction channels. Therefore, on average, firms learn from scientists that they encounter at conferences, but the substantial heterogeneity of the effect reflects the influence of reputation mechanisms in social interactions. This paper was accepted by Ashish Arora, entrepreneurship and innovation. Funding: S. Baruffaldi acknowledges financial support from the Swiss Science National Foundation (Reference No.: P2ELP1-161847). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01373 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01373},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2056-2078},
  shortjournal = {Manag. Sci.},
  title        = {Like stars: How firms learn at scientific conferences},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital vs. Traditional advertising and the recognition of
brand intangible assets. <em>MS</em>, <em>71</em>(3), 2035–2055. (<a
href="https://doi.org/10.1287/mnsc.2022.00387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines how different advertising categories are associated with attributes of brand asset recognition arising in the context of acquisitions. Prior research documents that aggregate advertising is positively associated with firm sales and brand values, but it fails to consider major recent shifts in advertising from traditional channels (such as TV) to digital advertising (such as paid search and online display). Using proprietary data, I decompose advertising expenditures into the categories of traditional, online display, and paid search. Consistent with expectations, results confirm that target firms’ traditional and online display advertising exhibit a higher likelihood of brand asset recognition and higher recognized brand values as compared with paid search advertising. Confirming the economic substance of the recognized brands, additional results reveal positive investor equity market reactions to brand value, which are driven by firms investing more in traditional and online display advertising. Overall, these results confirm that certain advertising channels exhibit stronger associations with the recognition and characteristics of the underlying brand asset, consistent with their heterogeneous properties. This paper was accepted by Suraj Srinivasan, accounting. Funding: I acknowledge the financial support from my advisor, Eddie Riedl, as well as from the UNH Paul College of Business and Economics Summer Research Support Award. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00387 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00387},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2035-2055},
  shortjournal = {Manag. Sci.},
  title        = {Digital vs. traditional advertising and the recognition of brand intangible assets},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accessing the untapped brand leverage potential: A strategic
framework from a capital market view. <em>MS</em>, <em>71</em>(3),
2011–2034. (<a href="https://doi.org/10.1287/mnsc.2022.00670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Do firms fully capitalize on the financial strength of their customer-based brand equity (CBBE)? If not, how large is the gap to a benchmark condition where CBBE is financially fully leveraged? Which market strategies, if any, should the firm emphasize to close the gap? This research addresses these questions. The authors introduce a framework and methodology that enables firms to identify and measure their “brand capabilities gap,” which captures the firm’s potential for leveraging CBBE into firm value when the most effective and efficient brand leverage strategy is in place, and, if necessary, take corrective action to realize their brand leverage potential. The authors further examine three market strategy moderators that firms can steer via their marketing programs and tactics to close the brand capabilities gap. The application of the framework and methodology to a wide-ranging sample of firms from 2005 to 2013 reveals that, on average, firms only realize 35% of their financial brand leverage potential. This translates into an average brand capabilities gap worth $2.1 bn per brand, which represents 4.3% of firm value. These findings have important implications for strategic brand analysis and planning and can help firms design a more effective financial brand leverage strategy. This paper was accepted by Raphael Thomadsen, marketing. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2022.00670 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00670},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {2011-2034},
  shortjournal = {Manag. Sci.},
  title        = {Accessing the untapped brand leverage potential: A strategic framework from a capital market view},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thompson sampling with information relaxation penalties.
<em>MS</em>, <em>71</em>(3), 1988–2010. (<a
href="https://doi.org/10.1287/mnsc.2020.01396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a finite-horizon multiarmed bandit (MAB) problem in a Bayesian setting, for which we propose an information relaxation sampling framework. With this framework, we define an intuitive family of control policies that include Thompson sampling (TS) and the Bayesian optimal policy as endpoints. Analogous to TS, which at each decision epoch pulls an arm that is best with respect to the randomly sampled parameters, our algorithms sample entire future reward realizations and take the corresponding best action. However, this is done in the presence of “penalties” that seek to compensate for the availability of future information. We develop several novel policies and performance bounds for MAB problems that vary in terms of improving performance and increasing computational complexity between the two endpoints. Our policies can be viewed as natural generalizations of TS that simultaneously incorporate knowledge of the time horizon and explicitly consider the exploration-exploitation trade-off. We prove associated structural results on performance bounds and suboptimality gaps. Numerical experiments suggest that this new class of policies perform well, in particular, in settings where the finite time horizon introduces significant exploration-exploitation tension into the problem. Finally, inspired by the finite-horizon Gittins index, we propose an index policy that builds on our framework that particularly outperforms the state-of-the-art algorithms in our numerical experiments. This paper was accepted by Hamid Nazerzadeh, data science. Funding: This research was supported by the National Research Foundation of Korea [NRF-2022R1C1C1013402]. Supplemental Material: The electronic companion and data files are available at https://doi.org/10.1287/mnsc.2020.01396 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2020.01396},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {1988-2010},
  shortjournal = {Manag. Sci.},
  title        = {Thompson sampling with information relaxation penalties},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Credit market conditions and mental health. <em>MS</em>,
<em>71</em>(3), 1967–1987. (<a
href="https://doi.org/10.1287/mnsc.2023.00194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research offers conflicting predictions about the impact of credit conditions on mental health. We first assess how bank regulatory reforms that improved credit conditions, for example, by enhancing the efficiency of credit allocation and lowering lending rates, impacted mental health. We discover that among low-income individuals, these regulatory reforms reduced mental depression, boosted labor market outcomes, eased access to mortgage debt, and reduced the ranks of the “unbanked.” We also find that mergers of large regional banks that led to branch closures and tighter credit constraints in affected counties harmed the mental health of lower-income individuals in treated counties. This paper was accepted by Kay Giesecke, finance. Funding: C. Lin and M. Tai acknowledge financial support from the National Natural Science Foundation of China [Grant 72192841] and the Research Grants Council of the Hong Kong Special Administration Region, China [Project T35/710/20R]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00194 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00194},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {1967-1987},
  shortjournal = {Manag. Sci.},
  title        = {Credit market conditions and mental health},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A benchmark for collateralized loan obligations.
<em>MS</em>, <em>71</em>(3), 1944–1966. (<a
href="https://doi.org/10.1287/mnsc.2022.00097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build a benchmark for AAA-rated tranches of collateralized loan obligations (CLOs) using business development companies (BDCs), which hold a diversified portfolio of loans as CLOs do. BDCs are publicly listed, and their share price, equity volatility, and borrowing cost can be easily obtained. Applying a structural model to BDCs, we extract market-implied correlation in their loan portfolio, compare spreads on CLO tranches and BDC-implied benchmark, and find that observed large credit spreads on CLO senior tranches after the financial crisis are a fair reflection of the systematic risk of correlated loan defaults. This paper was accepted by Lukas Schmid, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00097 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00097},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {1944-1966},
  shortjournal = {Manag. Sci.},
  title        = {A benchmark for collateralized loan obligations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive three-dimensional printing of spare parts with
internet of things. <em>MS</em>, <em>71</em>(3), 1925–1943. (<a
href="https://doi.org/10.1287/mnsc.2023.00978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industry 4.0 integrates digital and physical technologies to transform work management, where two core enablers are the internet of things (IoT) and three-dimensional printing (3DP). IoT monitors complex systems in real time, whereas 3DP enables agile manufacturing that can respond to real-time information. However, the details of how these two can be integrated are not yet clear. To gain insights, we consider a scenario where a three-dimensional (3D) printer supplies a critical part to multiple machines that are embedded with sensors and connected through IoT. Although the public perception indicates that this integration would enable on-demand printing, our research suggests that this is not necessarily the case. Instead, the true benefit is the ability to print predictively. In particular, it is typically more effective for the 3D printer to predictively print to stock based on a threshold that depends on the system’s status. We also identify a printing mode called predictive print on demand that allows for minimal inventory, and we find the speed of 3DP to be the primary factor that influences its optimality. Furthermore, we assess the value of IoT in cost reductions by separately analyzing the impact of advance information from embedded sensors and the real-time information fusion through IoT. We find that IoT provides significant value in general. However, the conventional wisdom that IoT’s value scales up for larger systems is suitable only when the expansion is paired with appropriate 3DP capacity. Our framework can help inform investment decisions regarding IoT/embedded sensors and support the development of scheduling tools for predictive 3DP. This paper was accepted by David Simchi-Levi, operations management. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00978 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00978},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {1925-1943},
  shortjournal = {Manag. Sci.},
  title        = {Predictive three-dimensional printing of spare parts with internet of things},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time pressure preferences. <em>MS</em>, <em>71</em>(3),
1909–1924. (<a href="https://doi.org/10.1287/mnsc.2023.02078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many professional and educational settings require individuals to be willing and able to perform under time pressure. We use a laboratory experiment and survey data to study preferences for working under time pressure. We make three main contributions. First, we develop an incentivized method to measure preferences for working under time pressure and document that participants in our laboratory experiment are averse to working under time pressure on average. Second, we show that there is substantial heterogeneity in the degree of time pressure aversion across individuals and that these individual preferences can be partially captured by simple survey questions. Third, we include these questions in a survey of bachelor’s degree students and a nationally representative survey panel and show that time pressure preferences predict career choices and income. Our results indicate that individual differences in time pressure aversion could be an influential factor in determining labor market outcomes. This paper was accepted by Yan Chen, behavioral economics and decision analysis. Funding: This work was supported by the Jan Wallander and Tom Hedelius Foundation, European Research Council under the European Union’s Horizon 2020 Research and Innovation Programme [Grant 850590]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02078 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02078},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {1909-1924},
  shortjournal = {Manag. Sci.},
  title        = {Time pressure preferences},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pigeonhole design: Balancing sequential experiments from an
online matching perspective. <em>MS</em>, <em>71</em>(3), 1889–1908. (<a
href="https://doi.org/10.1287/mnsc.2023.02184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practitioners and academics have long appreciated the benefits of covariate balancing when they conduct randomized experiments. For web-facing firms running online A/B tests, however, it still remains challenging in balancing covariate information when experimental subjects arrive sequentially. In this paper, we study an online experimental design problem, which we refer to as the online blocking problem . In this problem, experimental subjects with heterogeneous covariate information arrive sequentially and must be immediately assigned into either the control or the treated group. The objective is to minimize the total discrepancy, which is defined as the minimum weight perfect matching between the two groups. To solve this problem, we propose a randomized design of experiment, which we refer to as the pigeonhole design. The pigeonhole design first partitions the covariate space into smaller spaces, which we refer to as pigeonholes, and then, when the experimental subjects arrive at each pigeonhole, balances the number of control and treated subjects for each pigeonhole. We analyze the theoretical performance of the pigeonhole design and show its effectiveness by comparing against two well-known benchmark designs: the matched-pair design and the completely randomized design. We identify scenarios when the pigeonhole design demonstrates more benefits over the benchmark design. To conclude, we conduct extensive simulations using Yahoo! data to show a 10.2% reduction in variance if we use the pigeonhole design to estimate the average treatment effect. This paper was accepted by George Shanthikumar, data science. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02184 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02184},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {1889-1908},
  shortjournal = {Manag. Sci.},
  title        = {Pigeonhole design: Balancing sequential experiments from an online matching perspective},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does mining fuel bubbles? An experimental study on
cryptocurrency markets. <em>MS</em>, <em>71</em>(3), 1865–1888. (<a
href="https://doi.org/10.1287/mnsc.2022.01238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how key features associated with the Proof-of-Work consensus mechanism of Bitcoin (commonly referred to as mining) affect pricing. In a controlled laboratory experiment, we observe that price bubble formation can be attributed to mining. Moreover, overpricing is more pronounced if the mining capacity is centralized to a small group of individuals. The order book data reveal that miners seem to play a crucial role in bubble formation. Further probing the mechanism in a second study, we find that both mining costs and decisions jointly with the sluggish rate of supply of the asset contribute to the bubble formation. Our results demonstrate that erratic pricing is an inherent feature of cryptocurrencies based on a mining protocol, thus seriously limiting any prospects for such assets becoming a medium of exchange. This paper was accepted by Yan Chen, behavioral economics and decision analysis. Funding: The funding provided by the University of Heidelberg, Hanken Foundation [Grant 271-6250], and Durham University is gratefully acknowledged. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01238 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01238},
  journal      = {Management Science},
  month        = {3},
  number       = {3},
  pages        = {1865-1888},
  shortjournal = {Manag. Sci.},
  title        = {Does mining fuel bubbles? an experimental study on cryptocurrency markets},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="msom---19">MSOM - 19</h2>
<ul>
<li><details>
<summary>
(2025). Budget disclosure in crowdfunding: Information asymmetry and
cost transparency. <em>MSOM</em>, <em>27</em>(2), 659–678. (<a
href="https://doi.org/10.1287/msom.2022.0374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper investigates how a crowdfunding campaign’s voluntary disclosure of sensitive cost information affects its funding performance on Kickstarter—a mainstream, primarily reward-based crowdfunding platform. Instead of enhancing a campaign’s marketing traits, project budget as a novel information-provision tool reminds the crowd of the behind-the-scenes operations for developing and executing a project. This resembles the radical practice of cost transparency recently observed in the retail industry and studied experimentally in the literature. We also separate the disclosure effect on reward seeking versus altruistic pledges and examine how it varies with the structure of the cost items (fixed versus variable costs). Methodology/results : Our data collection follows Kickstarter’s rollout of the Project Budget tool in 2019. The project-level raw data are preprocessed via coarsened exact matching to match projects with and without budgets. We show that the main channel through which budget provision improves funding performance is via attracting additional funding, not by cannibalizing competing projects. Because of the voluntary nature of budget provision, we address the resulting endogeneity issue using peer average-style instruments. Budget provision on average increases funding performance by over 100%, which is more effective than many information-provision tools studied. Interestingly, this funding improvement comes from an increased number of backers (width), not from a higher pledge per backer (depth), and from a considerably higher number of altruistic pledges, not from more reward-seeking pledges. Further, a higher fixed-cost (over total cost) ratio, which is indicative of a higher profit margin, significantly reduces the benefit but does not make disclosure backfire. Managerial implications : Showing the costs to customers can be highly effective in business settings where trust between stakeholders is critical, which implies a promising generalization to donation-based crowdfunding. Our results can also be used to guide a sequential rollout of the Project Budget feature across campaign categories and promote adoption rates. Funding: X. Tong was supported by funding from the Department of Operations and the Research School of the Faculty of Economics and Business at the University of Groningen [Projects 147215070 and 147120310]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0374 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0374},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {659-678},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Budget disclosure in crowdfunding: Information asymmetry and cost transparency},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-armed bandits with endogenous learning curves: An
application to split liver transplantation. <em>MSOM</em>,
<em>27</em>(2), 640–658. (<a
href="https://doi.org/10.1287/msom.2022.0412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem Definition: Proficiency in many sophisticated tasks is attained through experience-based learning, in other words, learning by doing. For example, transplant centers’ surgical teams need to practice difficult surgeries to master the skills required. Meanwhile, this experience-based learning may affect other stakeholders, such as patients eligible for transplant surgeries, and require resources, including scarce organs and continual efforts. To ensure that patients have excellent outcomes and equitable access to organs, the organ allocation authority needs to quickly identify and develop medical teams with high aptitudes. This entails striking a balance between exploring surgical combinations with initially unknown full potential and exploiting existing knowledge based on observed outcomes. Methodology/results: We formulate a multi-armed bandit (MAB) model in which parametric learning curves are embedded in the reward functions to capture endogenous experience-based learning. In addition, our model includes provisions ensuring that the choices of arms are subject to fairness constraints to guarantee equity. To solve our MAB problem, we propose the L-UCB and FL-UCB algorithms, variants of the upper confidence bound (UCB) algorithm that attain the optimal O ( log t ) regret on problems enhanced with experience-based learning and fairness concerns. We demonstrate our model and algorithms on the split liver transplantation (SLT) allocation problem, showing that our algorithms have superior numerical performance compared with standard bandit algorithms in a setting where experience-based learning and fairness concerns exist. Managerial implications: From a methodological point of view, our proposed MAB model and algorithms are generic and have broad application prospects. From an application standpoint, our algorithms could be applied to help evaluate potential strategies to increase the proliferation of SLT and other technically difficult procedures. Funding: The authors acknowledge the support of CMU Tepper’s Health Care Initiative Funding. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/msom.2022.0412 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0412},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {640-658},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Multi-armed bandits with endogenous learning curves: An application to split liver transplantation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nurse staffing under absenteeism: A distributionally robust
optimization approach. <em>MSOM</em>, <em>27</em>(2), 624–639. (<a
href="https://doi.org/10.1287/msom.2023.0398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We study a nurse staffing problem under random nurse demand and absenteeism. Although the demand uncertainty is exogenous, the absenteeism uncertainty is decision-dependent , that is, the number of nurses who show up for work partially depends on the nurse staffing level. For quality of care, hospitals develop float pools of hospital units and train nurses to be able to work in multiple units (termed cross-training) in response to potential nurse shortages. Methodology/results : We study a distributionally robust nurse staffing (DRNS) model that considers both exogenous and decision-dependent uncertainties. We derive a separation algorithm to solve this model under a general structure of float pools. In addition, we identify several pool structures that often arise in practice and recast the corresponding DRNS model as a mixed-integer linear program, which facilitates off-the-shelf commercial solvers. Managerial implications : Through the numerical case studies, based on the data of a collaborating hospital, we found that modeling decision-dependent absenteeism improves the out-of-sample performance of staffing decisions, and such improvement is positively correlated with the value of flexibility arising from fully utilizing float pools. Funding: R. Jiang was supported in part by the National Science Foundation [Grant ECCS-1845980]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0398 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0398},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {624-639},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Nurse staffing under absenteeism: A distributionally robust optimization approach},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Going the distance: The impact of commute on gender
diversity in public service. <em>MSOM</em>, <em>27</em>(2), 607–623. (<a
href="https://doi.org/10.1287/msom.2024.0927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Women have been shown to prefer jobs with a better work-life balance across many fields. Given that serving as a state political representative requires a significant amount of travel between one’s home district and the state capitol, this suggests that long commute distances may reduce the number of women seeking political office in the United States. At the same time, state political positions vary in their degree of flexibility (e.g., full-time versus part-time) and commensurate compensation, which could make them more or less desirable to women. We analyze the extent to which longer commute distances deter female political participation and whether this effect varies according to the degree of flexibility and commensurate compensation provided by the position. Furthermore, we investigate policies that can lower the barrier to entry for women in politics. Methodology/results : Leveraging differences in distance to the state capitol among state legislative districts, we show that districts located further from the state capitol in states with full-time legislatures have a lower percentage of female candidates, whereas in states with part-time legislatures, the opposite is true. The effect in hybrid states, that is, those state legislatures with a workload between part-time and full-time, appears to be slightly negative or neutral. We then conduct two conjoint survey experiments administered to a pool of college students and past political candidates to understand how policies allowing for paid parental leave, remote work/proxy voting, or daycare benefits in political positions could help to close the gender gap in politics. We find that paid parental leave might motivate women at the beginning of their political careers with longer commutes to run for office while a remote work/proxy voting policy could help to sustain the political careers of these types of women. Managerial implications : For organizers and policymakers seeking to encourage more women to run for office, our work shows that commute distance is a barrier for would-be candidates in states with full-time legislatures and that making paid parental leave and remote work/proxy voting available may help. Funding: This work was supported by Gies College of Business (Junior Faculty Council Grant). Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2024.0927 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.0927},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {607-623},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Going the distance: The impact of commute on gender diversity in public service},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laissez-faire vs. Government intervention: Implications of
regulation preventing nonauthorized remanufacturing. <em>MSOM</em>,
<em>27</em>(2), 588–606. (<a
href="https://doi.org/10.1287/msom.2023.0128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : In this paper, we compare laissez-faire and mandatory authorization policy regimes for third-party remanufacturing. Under a laissez-faire policy, an independent remanufacturer (IR) chooses whether to get the original equipment manufacturer (OEM) authorization. Under a mandatory authorization policy, the IR is required to get OEM authorization and to pay the OEM a fee for every item remanufactured. Motivated by China’s regulatory journey that first mandated authorized remanufacturing and then moved to a laissez-faire policy, our goal is to understand which policy is better from the perspectives of different stakeholders. Methodology/results : We use a game-theoretic approach and consider a supply chain consisting of a supplier, an OEM, and an IR under the two policy regimes. Conventional wisdom suggests that the IR would be better off under the laissez-faire policy, but the OEM and the supplier would be better off under the mandatory authorization policy. However, we show that this conventional wisdom may not hold. For products with a low remanufacturing cost, all firms benefit from the mandatory authorization policy, whereas for products with a moderately high remanufacturing cost, all firms are better off under the laissez-faire policy. Further, mandatory authorization may outperform the laissez-faire policy in both economic and environmental dimensions. Managerial implications : Our findings reveal that seemingly advantageous policy regimes may backfire for firms. Therefore, before supporting such policies, the firms need to assess the strategic reactions of other firms and the potential impacts on their profits. Furthermore, a mandatory authorization policy can be beneficial in fostering the development of the remanufacturing sector for products with low remanufacturing costs. Nevertheless, it may also lead to an increase in the total environmental impact. Funding: The work of M. Jin was supported by the National Natural Science Foundation of China [Grants 72071020 and 72471038]. The work of Y. Zhou was supported by the National Natural Science Foundation of China [Grants 71971033 and 72371040] and the Fundamental Research Funds for the Central Universities [Grant 2024CDJSKPT14]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0128 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0128},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {588-606},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Laissez-faire vs. government intervention: Implications of regulation preventing nonauthorized remanufacturing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How does flexibility affect environmental performance?
Evidence from the power generation industry. <em>MSOM</em>,
<em>27</em>(2), 569–587. (<a
href="https://doi.org/10.1287/msom.2023.0522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : It is well known that the power sector requires flexibility, especially from fossil fuel–based power-generating units, to balance demand side variability and supply side fluctuations. However, the environmental consequences related to possessing and exercising such flexibility remain relatively unexplored. In this study, we examine the environmental impact of two forms of flexibility pertinent to power generation: fuel flexibility —that is, the ability to utilize multiple fuel types, and volume flexibility —that is, the ability to alter production volumes quickly. Methodology/results : We assembled a data set that spans 1998–2016 and includes details on fuel usage, power generation, generating unit characteristics, and carbon dioxide (i.e., CO 2 ) emissions for 3,135 fossil fuel power-generating units that account for more than 92% of the United States’ fossil fuel–generating capacity. Our empirical analysis reveals that power-generating units that possess fuel flexibility or volume flexibility generate greater CO 2 emissions than comparable nonflexible generating units. Additionally, when power-generating units exercise fuel flexibility (i.e., they use multiple fuel types in a period) or volume flexibility (i.e., they vary production to a greater degree in a period), they generate greater CO 2 emissions. By contrast, when power-generating units exercise both fuel and volume flexibility, we find that they diminish the aggregate emissions increases expected from exercising fuel or volume flexibility alone. Managerial implications : We add to the literature by exploring how flexibility affects environmental performance and by disambiguating the effects of possessing flexibility and exercising flexibility. These results are important when considering the penetration of renewables, the adoption of utility-grade storage, and demand response as each of these paths can significantly affect the flexibility burden placed on conventional sources of power. Thus, our results are relevant to policy makers and practitioners because they crystallize the environmental tradeoffs involved with deploying flexibility in fossil fuel–based power-generating units. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0522 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0522},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {569-587},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {How does flexibility affect environmental performance? evidence from the power generation industry},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Experience minimizes the pull-to-center effect in newsvendor
decisions. <em>MSOM</em>, <em>27</em>(2), 554–568. (<a
href="https://doi.org/10.1287/msom.2021.0439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The asymmetric pull-to-center effect for newsvendors is a robust finding in operations, and understanding why newsvendors make suboptimal decisions is key for identifying ways to improve decision-making quality in this critical task. Although prior studies have indicated that experience does not substantially mitigate the pull-to-center effect, the experience levels achieved in those previous studies are limited in comparison with the experience level that a near-continuous time environment can provide. Methodology/results : We conduct a set of laboratory experiments using a near-continuous time environment to determine the effect that extensive experience has on newsvendor behavior and the extent to which resultant learning is transferable across conditions. Observed behavior clearly demonstrates that the pull-to-center effect is substantially reduced, if not eliminated, with sufficient experience and that this learning can have positive spillovers to more traditional settings. Further, the experiments suggest that it is the repeated feedback regarding a given inventory decision rather than the ability to explore many strategies that drives improved decision making. Managerial implications : Experience can mitigate the pull-to-center effect, and near-continuous time environments can be an effective training tool for gaining such experience. Funding: This work was supported by the Ministerstvo školstva, vedy, výskumu a športu Slovenskej republiky [Grants VEGA 1/0660/23 and VEGA 1/0721/24] and the Slovak Research and Development Agency [Grant APVV-19-0573]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2021.0439 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0439},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {554-568},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Experience minimizes the pull-to-center effect in newsvendor decisions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offline feature-based pricing under censored demand: A
causal inference approach. <em>MSOM</em>, <em>27</em>(2), 535–553. (<a
href="https://doi.org/10.1287/msom.2024.1061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We study a feature-based pricing problem with demand censoring in an offline, data-driven setting. In this problem, a firm is endowed with a finite amount of inventory and faces a random demand that is dependent on the offered price and the features (from products, customers, or both). Any unsatisfied demand that exceeds the inventory level is lost and unobservable. The firm does not know the demand function but has access to an offline data set consisting of quadruplets of historical features, inventory, price, and potentially censored sales quantity. Our objective is to use the offline data set to find the optimal feature-based pricing rule so as to maximize the expected profit. Methodology/results : Through the lens of causal inference, we propose a novel data-driven algorithm that is motivated by survival analysis and doubly robust estimation. We derive a finite sample regret bound to justify the proposed offline learning algorithm and prove its robustness. Numerical experiments demonstrate the robust performance of our proposed algorithm in accurately estimating optimal prices on both training and testing data. Managerial implications : The work provides practitioners with an innovative modeling and algorithmic framework for the feature-based pricing problem with demand censoring through the lens of causal inference. Our numerical experiments underscore the value of considering demand censoring in the context of feature-based pricing. Funding: The research of E. Fang is partially supported by the National Science Foundation [Grants NSF DMS-2346292, NSF DMS-2434666] and the Whitehead Scholarship. The research of C. Shi is partially supported by the Amazon Research Award. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2024.1061 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.1061},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {535-553},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Offline feature-based pricing under censored demand: A causal inference approach},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural estimation of attrition in a last-mile delivery
platform: The role of driver heterogeneity, compensation, and
experience. <em>MSOM</em>, <em>27</em>(2), 516–534. (<a
href="https://doi.org/10.1287/msom.2021.0367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We examine how to manage turnover among drivers delivering parcels for last-mile platforms. Although driver attrition in these platforms is both commonplace and costly, there is little understanding of the processes responsible for this phenomenon. Methodology/results : We collaborate with a platform to build a structural model to estimate the effects of key predictors of drivers’ decisions to leave or remain at the platform. For this estimation, we apply a dynamic discrete-choice framework in a two-step procedure that accounts for unobserved heterogeneity among drivers while circumventing the use of approximation or reduction methods commonly used to solve dynamic choice problems in the operations management domain. Drivers are compensated using a combination of regular payments that reward their productivity and subsidy payments that support them as they gain experience on the job. We find that regular pay has a greater effect on drivers’ retention. Furthermore, the marginal effects of both regular and subsidy pay diminish with drivers’ tenure at the platform, but the latter diminishes faster than the former. Additionally, we find significant heterogeneity among drivers in their unobserved nonpecuniary taste for the jobs at the platform and a significantly greater probability of retention among drivers with greater taste for these jobs. Managerial implications : Platforms can leverage our results to improve driver retention and design more profitable payment policies. We perform counterfactual analyses and develop a modeling framework to guide platforms toward this goal. Funding: This study was partially funded by a grant from TForce Logistics. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2021.0367 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0367},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {516-534},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Structural estimation of attrition in a last-mile delivery platform: The role of driver heterogeneity, compensation, and experience},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Choice overload and the long tail: Consideration sets and
purchases in online platforms. <em>MSOM</em>, <em>27</em>(2), 496–515.
(<a href="https://doi.org/10.1287/msom.2021.0318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper examines frictions in the shopping funnel using empirical clickstream data from an online travel platform. We analyze (a) customers’ heterogeneous search and purchase behaviors and (b) their reactions to changes in assortment size. We then develop a consider-then-choose model to generalize our findings. Methodology/results : We characterize the online customer journey as a two-stage consider-then-choose framework. In the consider stage, we analyze the consideration set formation and show that heterogeneity—familiarity with the assortment—amplifies the number of options; in the purchase stage, it drives preferences for niche versus popular choices. A real-world high-stakes field experiment reveals that shrinking the menu produces mixed results: highlighting the market for the long-tail for some customers and reflecting choice overload for others. Finally, we build a psychologically rich consider-then-choose model with (a) heterogeneous preferences for product features and (b) heterogeneous search costs moderated by search fatigue, theoretically characterizing the impact on consideration sets and conversion rates. Managerial implications : Identifying frictions in the shopping funnel is critical for online platforms, especially when pain points hurt click-through or conversion rates. Which options matter to which users? What is the right assortment size? Although online platforms can offer virtually unlimited assortments, managers may assume frictionless environments—which is not always the case. Our findings offer insights into improving the customer journey by considering heterogeneous preferences and boundedly rational heuristics. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2021.0318 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0318},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {496-515},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Choice overload and the long tail: Consideration sets and purchases in online platforms},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frame by fame: Content creation on short video-format
platforms. <em>MSOM</em>, <em>27</em>(2), 479–495. (<a
href="https://doi.org/10.1287/msom.2021.0332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Short video format platforms like NetEase and TikTok are attention economies that host user-generated content, in the form of combined video and audio elements, and operate on principles of network virality. This study explores user content creation strategies by focusing on decisions around content release frequency and the incorporation of adopted content in newly generated creations. Methodology/results : We theoretically explore the role of the following three mechanisms influencing these decisions: (a) social hierarchies in the platform’s network structure, (b) virality of content on the platform, and (c) algorithmic interventions through the ranking of content recommendations. To empirically study these relationships, we use a detailed log of user activity on NetEase Cloud Village registered during the month of November 2019 and carry out regression-based analyses. We find that high-status users, measured through their follower count, adapt their content release frequency strategically, slowing down after successful content to avoid dilution of potential virality. We also observe that high-status users generally deviate from prevailing viral trends in their content creation. However, following exceptionally successful releases, they tend to conform more to viral content, suggesting a risk-averse approach. Finally, contrary to algorithm aversion, users prefer content recommended by the platform. However, high-status users disregard algorithms to retain agency in decision making. Managerial implications : Our findings provide a holistic understanding of the content creation process and suggest that platforms could strategically adjust algorithmic ranking policies to foster content creation diversity while catering to the preferences of users with different status levels. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2021.0332 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0332},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {479-495},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Frame by fame: Content creation on short video-format platforms},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of route-level decisions in the efficiency and
resilience of airline operations: Evidence from the wright amendment
repeal. <em>MSOM</em>, <em>27</em>(2), 460–478. (<a
href="https://doi.org/10.1287/msom.2023.0070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We study the impact of an airline’s route-level decisions on the efficiency and resilience of its operations. Methodology/results : Our study uses a proprietary passenger-level data set provided by Southwest Airlines and takes advantage of a regulatory change, the Wright Amendment repeal. The Wright Amendment restricted nonstop service between Dallas Love Field Airport (DAL) and most major destinations. Its repeal led to significant changes in Southwest’s service offerings at DAL. We use a difference-in-differences framework to quantify the impact of the repeal on the scheduled travel times and long travel delays at the passenger level, which serve as proxies for efficiency and resilience, respectively. We show that the repeal’s impact varies widely across destinations. Although Southwest introduced nonstop service to some destinations but not others after the repeal, this decision alone does not sufficiently explain the observed heterogeneity. As such, we develop two route-level metrics—route inefficiency and route resilience—to explain the heterogeneous impact of the repeal on different destinations. Moreover, our counterfactual analysis shows that reallocating connecting passengers with small itinerary buffers to alternative itineraries can result in major resilience improvements without significantly deteriorating efficiency. Managerial implications : Our findings are robust to alternative empirical specifications and have important implications for practitioners and regulators. Specifically, our study not only sheds light on the operational consequences of airlines’ route-level decisions but also demonstrates how managers can effectively manage the efficiency-resilience tradeoff for connecting passengers. Our study also implies that regulators can add more transparency to airline operations by providing public access to passenger-level data. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0070 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0070},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {460-478},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {The role of route-level decisions in the efficiency and resilience of airline operations: Evidence from the wright amendment repeal},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Buying cheap: Brand switching during economic distress and
its disparate impact on consumers. <em>MSOM</em>, <em>27</em>(2),
441–459. (<a href="https://doi.org/10.1287/msom.2022.0380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Improving the surplus of low-income consumers during economic distress is of primary concern for many governments. This paper uses an economic model to investigate consumers’ brand switching during economic distress and highlights its disparate impact on low-income consumers. Our modeling framework also captures how retailers and national-brand manufacturers strategically adjust the market prices in response to brand switching and shelf space constraints. To generate prescriptive insights, we also analyze the effectiveness of commonly observed government interventions that fall into two major categories: (i) consumer-focused (e.g., cash subsidy) and (ii) retailer-focused (e.g., price control). Methodology/results : Our analysis indicates that market access for low-income consumers can decline significantly because of the brand switching behavior. Furthermore, not all government interventions are equally effective in increasing the welfare of low-income consumers. In fact, retailer-focused schemes such as price control (PC) can backfire and decrease access for low-income consumers. Although cash subsidy (CS) can increase the surplus of low-income consumers, it is always at the expense of high-income consumers. Managerial implications : We study an important but understudied challenge that highlights how strategic behavior by retailers can exacerbate affordability and accessibility concerns during economic distress. Model calibration using NielsenIQ Homescan Panel data shows that our model captures the data well and generates practical insights for policymakers. These results suggest that the success of government interventions depends critically on whether they account for the strategic behavior of different stakeholders in the supply chain. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0380 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0380},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {441-459},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Buying cheap: Brand switching during economic distress and its disparate impact on consumers},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient cloud server deployment under demand uncertainty.
<em>MSOM</em>, <em>27</em>(2), 425–440. (<a
href="https://doi.org/10.1287/msom.2023.0372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Cloud computing is a multibillion-dollar business that draws substantial capital investments from large companies such as Amazon, Microsoft, and Google. Large cloud providers need to accommodate the growing demand for computing resources while avoiding unnecessary overprovisioning of hardware and operational costs. The underlying decision processes are challenging, as they involve long-term hardware and infrastructure investments under future demand uncertainty. In this paper, we introduce the cloud server deployment problem . One important aspect of the problem is that the infrastructure preparation work has to be planned for before server deployments can take place. Furthermore, a combination of temporal constraints has to be considered together with a variety of physical constraints. Methodology/results : We formulate the underlying optimization problem as a two-stage stochastic program. After carefully examining the demand data and on-the-ground deployment operations, we distill two structural properties on deployment throughput constraints and provide tightness results on a convex relaxation of the second stage. Based on that, we develop efficient cutting-plane methods that exploit the special structure of the problem and can accommodate different risk measures. We test our algorithms with real production traces from Microsoft Azure and demonstrate sizeable cost reductions. We show empirically that the algorithms remain optimal even when the two properties are not fully satisfied. Managerial implications : Cloud supply chain operations were largely executed manually due to their complexity and dynamic nature. In this paper, we show that the key decision processes can be systematically optimized. In particular, we demonstrate that accounting for the stochastic nature of demands results in substantial cost reductions in cloud server deployments. Another benefit of our stochastic optimization approach is the ability to seamlessly integrate configurable risk preferences of cloud providers. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0372 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0372},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {425-440},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Efficient cloud server deployment under demand uncertainty},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of digitization on print book sales: Analysis
using genre exposure heterogeneity. <em>MSOM</em>, <em>27</em>(2),
408–424. (<a href="https://doi.org/10.1287/msom.2022.0594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The rise of digital channels has led to significant media market transformations. This paper studies whether and how digitization, sparked by the launch of Amazon Kindle in late 2007, affected print book sales. Methodology/results: To estimate the impact, we exploit the quasiexperimental variation in the popularity of digital books across different genres or subgenres. We employ difference-in-differences and other identification strategies, and we use print sales data on a large representative sample of book titles published in the United States from 2004 to 2015 across a variety of genres. Using various empirical specifications, we find that digitization significantly reduced print sales of adult fiction (the most popular genre in the e-book format) while having a much smaller impact on adult nonfiction and juvenile fiction. We further find that the effect for adult fiction is higher after the launch of the iPad (in 2010) and stronger for smaller publishers, the paperback version, and low-selling books. Managerial implications : Our findings provide book publishers with key inputs for optimal pricing and release strategies of different book formats. Our study also extends the vast yet growing academic literature on the impact of digital distribution channels on existing sales channels and can inform the policy debate concerning third-party digitization and its effect on physical sales. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0594 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0594},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {408-424},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {The impact of digitization on print book sales: Analysis using genre exposure heterogeneity},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient frontier and applications in product offering and
pricing. <em>MSOM</em>, <em>27</em>(2), 389–407. (<a
href="https://doi.org/10.1287/msom.2022.0164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The joint assortment and pricing problem is notoriously difficult to solve, especially for large-scale issues subject to various operational constraints arising from business practices. In this paper, we delve into the joint optimization challenge under constrained logit choice models, accounting for product-differentiated price sensitivities across static, randomized, and dynamic contexts. Methodology/results : We develop a unified optimization methodology that applies efficient frontier and dimensional reduction and transforms the joint optimization problem into a single-variable one with respect to the total choice probability. The efficient frontier is employed to transform the nonconcave objective function equivalently into its concave counterpart. We show that the optimal prices are uniquely determined by the common target adjusted markup. The complexity of the mixed combinatorial optimization problem can be reduced by searching over efficient sets of polynomial size. In the randomized assortment and pricing problem in which the product offer sets and prices follow certain distribution, we show that randomization has the potential to elevate the total revenue to the exact efficient frontier, a feat that the static problem may fail to achieve (with a given total choice probability). For dynamic joint product selection and pricing, we find that the optimal policy adopts a simple time-threshold structure that can be precomputed. Furthermore, we illustrate the robustness of our methodology by extending the analysis to a variety of general settings, including the nested logit model. Managerial implications : The proposed methodology contributes to the increasingly popular topic in retail management by significantly reducing the computational complexity of joint product offering and pricing under different constraints. Our results regarding the effects of product set constraints and price bounds provide valuable insights and guidance to practitioners on product offering and pricing in various business scenarios. Funding: C. Ke acknowledges financial support from the National Natural Science Foundation of China [Grant 72101113]. L. Lu acknowledges financial support from the Hong Kong Research Grants Council [Grants 26501021 and 16502423]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0164 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0164},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {389-407},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Efficient frontier and applications in product offering and pricing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep policy iteration with integer programming for inventory
management. <em>MSOM</em>, <em>27</em>(2), 369–388. (<a
href="https://doi.org/10.1287/msom.2022.0617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : In this paper, we present a reinforcement learning (RL)-based framework for optimizing long-term discounted reward problems with large combinatorial action space and state dependent constraints. These characteristics are common to many operations management problems, for example, network inventory replenishment, where managers have to deal with uncertain demand, lost sales, and capacity constraints that results in more complex feasible action spaces. Our proposed programmable actor RL (PARL) uses a deep-policy iteration method that leverages neural networks to approximate the value function and combines it with mathematical programming and sample average approximation to solve the per-step-action optimally while accounting for combinatorial action spaces and state-dependent constraint sets. Methodology/results : We then show how the proposed methodology can be applied to complex inventory replenishment problems where analytical solutions are intractable. We also benchmark the proposed algorithm against state-of-the-art RL algorithms and commonly used replenishment heuristics and find that the proposed algorithm considerably outperforms existing methods by as much as 14.7% on average in various complex supply chain settings. Managerial implications : We find that this improvement in performance of PARL over benchmark algorithms can be directly attributed to better inventory cost management, especially in inventory constrained settings. Furthermore, in the simpler setting where optimal replenishment policy is tractable or known near optimal heuristics exist, we find that the RL-based policies can learn near optimal policies. Finally, to make RL algorithms more accessible for inventory management researchers, we also discuss the development of a modular Python library that can be used to test the performance of RL algorithms with various supply chain structures. This library can spur future research in developing practical and near-optimal algorithms for inventory management problems. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0617 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0617},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {369-388},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Deep policy iteration with integer programming for inventory management},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A manager and an AI walk into a bar: Does ChatGPT make
biased decisions like we do? <em>MSOM</em>, <em>27</em>(2), 354–368. (<a
href="https://doi.org/10.1287/msom.2023.0279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Large language models (LLMs) are being increasingly leveraged in business and consumer decision-making processes. Because LLMs learn from human data and feedback, which can be biased, determining whether LLMs exhibit human-like behavioral decision biases (e.g., base-rate neglect, risk aversion, confirmation bias, etc.) is crucial prior to implementing LLMs into decision-making contexts and workflows. To understand this, we examine 18 common human biases that are important in operations management (OM) using the dominant LLM, ChatGPT. Methodology/results : We perform experiments where GPT-3.5 and GPT-4 act as participants to test these biases using vignettes adapted from the literature (“standard context”) and variants reframed in inventory and general OM contexts. In almost half of the experiments, Generative Pre-trained Transformer (GPT) mirrors human biases, diverging from prototypical human responses in the remaining experiments. We also observe that GPT models have a notable level of consistency between the standard and OM-specific experiments as well as across temporal versions of the GPT-3.5 model. Our comparative analysis between GPT-3.5 and GPT-4 reveals a dual-edged progression of GPT’s decision making, wherein GPT-4 advances in decision-making accuracy for problems with well-defined mathematical solutions while simultaneously displaying increased behavioral biases for preference-based problems. Managerial implications : First, our results highlight that managers will obtain the greatest benefits from deploying GPT to workflows leveraging established formulas. Second, that GPT displayed a high level of response consistency across the standard, inventory, and non-inventory operational contexts provides optimism that LLMs can offer reliable support even when details of the decision and problem contexts change. Third, although selecting between models, like GPT-3.5 and GPT-4, represents a trade-off in cost and performance, our results suggest that managers should invest in higher-performing models, particularly for solving problems with objective solutions. Funding: This work was supported by the Social Sciences and Humanities Research Council of Canada [Grant SSHRC 430-2019-00505]. The authors also gratefully acknowledge the Smith School of Business at Queen’s University for providing funding to support Y. Chen’s postdoctoral appointment. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0279 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0279},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {354-368},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {A manager and an AI walk into a bar: Does ChatGPT make biased decisions like we do?},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OM forum—barriers to implementing diversity, equity, and
inclusion (DEI) programs in supply chains: Lessons from comparing public
and private firms. <em>MSOM</em>, <em>27</em>(2), 339–353. (<a
href="https://doi.org/10.1287/msom.2023.0621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This OM Forum article explores the barriers hindering the implementation of diversity, equity, and inclusion (DEI) programs in supply chains by analyzing the DEI performance of publicly traded and privately held companies. Whereas the importance of DEI in supply chain management is widely acknowledged, publicly traded and privately held companies exhibit a notable gap in achieving DEI goals. Methodology/results : This article explores the factors that enabled publicly traded companies to be relatively more successful with DEI implementation and provides insights into how privately held companies can improve their DEI performance. We highlight the role of transparency in driving positive DEI change in publicly traded companies, which are also more likely to benefit from DEI initiatives because of their greater access to the resources required for implementation and a higher degree of customization. We describe additional barriers to DEI implementation, by which the differences between company types are less certain. These barriers include unawareness of or skepticism about the benefits of DEI programs in supply chains, internal opposition from advantaged groups, and the geographical scope of operations. Managerial implications : By addressing these barriers, privately held companies can create more inclusive and equitable supply chains that benefit all stakeholders.},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0621},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {3-4},
  number       = {2},
  pages        = {339-353},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {OM Forum—Barriers to implementing diversity, equity, and inclusion (DEI) programs in supply chains: Lessons from comparing public and private firms},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="or---33">OR - 33</h2>
<ul>
<li><details>
<summary>
(2025). Technical note—on the convergence rate of stochastic
approximation for gradient-based stochastic optimization. <em>OR</em>,
<em>73</em>(2), 1143–1150. (<a
href="https://doi.org/10.1287/opre.2023.0055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider stochastic optimization via gradient-based search. Under a stochastic approximation framework, we apply a recently developed convergence rate analysis to provide a new finite-time error bound for a class of problems with convex differentiable structures. For noisy black-box functions, our main result allows us to derive finite-time bounds in the setting where the gradients are estimated via finite-difference estimators, including those based on randomized directions such as the simultaneous perturbation stochastic approximation algorithm. In particular, the convergence rate analysis sheds light on when it may be advantageous to use such randomized gradient estimates in terms of problem dimension and noise levels. Funding: This work was supported by the Air Force Office of Scientific Research [Grant FA95502010211] and the National Science Foundation [Grants IIS-2123684 and CMMI-2027527].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0055},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1143-1150},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On the convergence rate of stochastic approximation for gradient-based stochastic optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recovering dantzig–wolfe bounds by cutting planes.
<em>OR</em>, <em>73</em>(2), 1128–1142. (<a
href="https://doi.org/10.1287/opre.2023.0048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dantzig–Wolfe (DW) decomposition is a well-known technique in mixed-integer programming (MIP) for decomposing and convexifying constraints to obtain potentially strong dual bounds. We investigate cutting planes that can be derived using the DW decomposition algorithm and show that these cuts can provide the same dual bounds as DW decomposition. More precisely, we generate one cut for each DW block, and when combined with the constraints in the original formulation, these cuts imply the objective function cut one can simply write using the DW bound. This approach typically leads to a formulation with lower dual degeneracy that consequently has a better computational performance when solved by standard MIP solvers in the original space. We also discuss how to strengthen these cuts to improve the computational performance further. We test our approach on the multiple knapsack assignment problem and the temporal knapsack problem, and we show that the proposed cuts are helpful in accelerating the solution time without the need to implement branch and price. Funding: This work was supported by the Office of Naval Research [Grant N00014-21-1-2575]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.0048 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0048},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1128-1142},
  shortjournal = {Oper. Res.},
  title        = {Recovering Dantzig–Wolfe bounds by cutting planes},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing bayes–nash equilibrium strategies in auction games
via simultaneous online dual averaging. <em>OR</em>, <em>73</em>(2),
1102–1127. (<a href="https://doi.org/10.1287/opre.2022.0287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auctions are modeled as Bayesian games with continuous type and action spaces. Determining equilibria in auction games is computationally hard in general, and no exact solution theory is known. We introduce an algorithmic framework in which we discretize type and action space and then learn distributional strategies via online optimization algorithms. One advantage of distributional strategies is that we do not have to make any assumptions on the shape of the bid function. Besides, the expected utility of agents is linear in the strategies. It follows that, if our optimization algorithms converge to a pure strategy, then they converge to an approximate equilibrium of the discretized game with high precision. Importantly, we show that the equilibrium of the discretized game approximates an equilibrium in the continuous game. In a wide variety of auction games, we provide empirical evidence that the approach approximates the analytical (pure) Bayes–Nash equilibrium closely. This speed and precision are remarkable because, in many finite games, learning dynamics do not converge or are even chaotic. In standard models in which agents are symmetric, we find equilibrium in seconds. Whereas we focus on dual averaging, we show that the overall approach converges independent of the regularizer, and alternative online convex optimization methods achieve similar results even though the discretized game satisfies neither monotonicity nor variational stability globally. The method allows for interdependent valuations and different types of utility functions, and it can be used to find equilibrium in auction markets and beyond. Funding: M. Bichler was supported by the Deutsche Forschungsgemeinschaft (DFG) (German Research Foundation) [Grant BI 1057/9]. M. Fichtl and M. Oberlechner were funded by the DFG [Grant GRK 2201/2 - Projektnummer 277991500].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0287},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1102-1127},
  shortjournal = {Oper. Res.},
  title        = {Computing Bayes–Nash equilibrium strategies in auction games via simultaneous online dual averaging},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear classifiers under infinite imbalance. <em>OR</em>,
<em>73</em>(2), 1075–1101. (<a
href="https://doi.org/10.1287/opre.2021.0376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the behavior of linear discriminant functions for binary classification in the infinite-imbalance limit, where the sample size of one class grows without bound while the sample size of the other remains fixed. The coefficients of the classifier minimize an empirical loss specified through a weight function. We show that for a broad class of weight functions, the intercept diverges but the rest of the coefficient vector has a finite almost sure limit under infinite imbalance, extending prior work on logistic regression. The limit depends on the left-tail growth rate of the weight function, for which we distinguish two cases: subexponential and exponential. The limiting coefficient vectors reflect robustness or conservatism properties in the sense that they optimize against certain worst-case alternatives. In the subexponential case, the limit is equivalent to an implicit choice of upsampling distribution for the minority class. We apply these ideas in a credit risk setting, with particular emphasis on performance in the high-sensitivity and high-specificity regions. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://pubsonline.informs.org/doi/suppl/10.1287/opre.2021.0376 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0376},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1075-1101},
  shortjournal = {Oper. Res.},
  title        = {Linear classifiers under infinite imbalance},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse optimization: Theory and applications. <em>OR</em>,
<em>73</em>(2), 1046–1074. (<a
href="https://doi.org/10.1287/opre.2022.0382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse optimization describes a process that is the “reverse” of traditional mathematical optimization. Unlike traditional optimization, which seeks to compute optimal decisions given an objective and constraints, inverse optimization takes decisions as input and determines objective and/or constraint parameters that render these decisions approximately or exactly optimal. In recent years, there has been an explosion of interest in the mathematics and applications of inverse optimization. This paper provides a comprehensive review of both the methodological and application-oriented literature in this field. Funding: T. C. Y. Chan received funding support from the Natural Science and Engineering Research Council of Canada. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0382 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0382},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1046-1074},
  shortjournal = {Oper. Res.},
  title        = {Inverse optimization: Theory and applications},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluid policies, reoptimization, and performance guarantees
in dynamic resource allocation. <em>OR</em>, <em>73</em>(2), 1029–1045.
(<a href="https://doi.org/10.1287/opre.2022.0601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many sequential decision problems involve deciding how to allocate shared resources across a set of independent systems at each point in time. A classic example is the restless bandit problem, in which a budget constraint limits the selection of arms. Fluid relaxations provide a natural approximation technique for this broad class of problems. A recent stream of research has established strong performance guarantees for feasible policies based on fluid relaxations. In this paper, we generalize and improve these recent performance results. First, we provide easy-to-implement feasible fluid policies that achieve performance within O ( N ) of optimal, where N is the number of subproblems. This result holds for a general class of dynamic resource allocation problems with heterogeneous subproblems and multiple shared resource constraints. Second, we show using a novel proof technique that a feasible fluid policy that chooses actions using a reoptimized fluid value function achieves performance within O ( N ) of optimal as well. To the best of our knowledge, this performance guarantee is the first one for reoptimization for the general dynamic resource allocation problems that we consider. The scaling of the constants with respect to time in these results implies similar results in the infinite horizon setting. Finally, we develop and analyze a class of feasible fluid-budget balancing policies that stay “close” to actions selected by an optimal fluid policy while simultaneously using as much of the shared resources as possible. We show that this policy achieves performance within O (1) of optimal under particular nondegeneracy assumptions. This result generalizes recent advances for restless bandit problems by considering (a) any finite number of actions for each subproblem and (b) heterogeneous subproblems with a fixed number of types. We demonstrate the use of these techniques on dynamic multiwarehouse inventory problems and find empirically that these fluid-based policies achieve excellent performance, as our theory suggests. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0601 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0601},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1029-1045},
  shortjournal = {Oper. Res.},
  title        = {Fluid policies, reoptimization, and performance guarantees in dynamic resource allocation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed-integer optimization with constraint learning.
<em>OR</em>, <em>73</em>(2), 1011–1028. (<a
href="https://doi.org/10.1287/opre.2021.0707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish a broad methodological foundation for mixed-integer optimization with learned constraints. We propose an end-to-end pipeline for data-driven decision making in which constraints and objectives are directly learned from data using machine learning, and the trained models are embedded in an optimization formulation. We exploit the mixed-integer optimization representability of many machine learning methods, including linear models, decision trees, ensembles, and multilayer perceptrons, which allows us to capture various underlying relationships between decisions, contextual variables, and outcomes. We also introduce two approaches for handling the inherent uncertainty of learning from data. First, we characterize a decision trust region using the convex hull of the observations to ensure credible recommendations and avoid extrapolation. We efficiently incorporate this representation using column generation and propose a more flexible formulation to deal with low-density regions and high-dimensional data sets. Then, we propose an ensemble learning approach that enforces constraint satisfaction over multiple bootstrapped estimators or multiple algorithms. In combination with domain-driven components, the embedded models and trust region define a mixed-integer optimization problem for prescription generation. We implement this framework as a Python package ( OptiCL ) for practitioners. We demonstrate the method in both World Food Programme planning and chemotherapy optimization. The case studies illustrate the framework’s ability to generate high-quality prescriptions and the value added by the trust region, the use of ensembles to control model robustness, the consideration of multiple machine learning methods, and the inclusion of multiple learned constraints. Funding: This work was supported by the Dutch Scientific Council [Grant OCENW.GROOT.2019.015] and the National Science Foundation [Grant 174530]. Additionally, H. Wiberg was supported by the National Science Foundation Graduate Research Fellowship [Grant 174530]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0707 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0707},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1011-1028},
  shortjournal = {Oper. Res.},
  title        = {Mixed-integer optimization with constraint learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online matching frameworks under stochastic rewards, product
ranking, and unknown patience. <em>OR</em>, <em>73</em>(2), 995–1010.
(<a href="https://doi.org/10.1287/opre.2021.0371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study generalizations of online bipartite matching in which each arriving vertex (customer) views a ranked list of offline vertices (products) and matches to (purchases) the first one they deem acceptable. The number of products that the customer has patience to view can be stochastic and dependent on the products seen. We develop a framework that views the interaction with each customer as an abstract resource consumption process and derive new results for these online matching problems under the adversarial, nonstationary, and independent and identically-distributed arrival models, assuming we can (approximately) solve the product ranking problem for each single customer. To that end, we show new results for product ranking under two cascade-click models: an optimal algorithm when each item has its own hazard rate for making the customer depart and a 1/2-approximate algorithm when the customer has a general item-independent patience distribution. We also present a constant-factor 0.027-approximate algorithm in a new model where items are not initially available and arrive over time. We complement these positive results by presenting three additional negative results relating to these problems. Funding: N. Grammel was supported in part by NSF award [CCF-1918749] and by research awards from Amazon and Google. A. Srinivasan was supported in part by NSF awards [CCF-1422569, CCF-1749864, and CCF-1918749], as well as research awards from Adobe, Amazon, and Google. W. Ma was supported in part by a research award from Amazon. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0371 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0371},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {995-1010},
  shortjournal = {Oper. Res.},
  title        = {Online matching frameworks under stochastic rewards, product ranking, and unknown patience},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—improved sample-complexity bounds in
stochastic optimization. <em>OR</em>, <em>73</em>(2), 986–994. (<a
href="https://doi.org/10.1287/opre.2018.0340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world network-optimization problems often involve uncertain parameters during the optimization phase. Stochastic optimization is a key approach introduced in the 1950s to address such uncertainty. This paper presents improved upper bounds on the number of samples required for the sample-average approximation method in stochastic optimization. It enhances the sample complexity of existing approaches in this setting, providing faster approximation algorithms for any method that employs this framework. This work is particularly relevant for solving problems like the stochastic Steiner tree problem. Funding: The research of A. Baveja is partially supported by the United States Department of Transportation (via the University Transportation Research Center Program) [Grant 49198-25-26] and the British Council [the UKIERI Research Program]. The work of A. Chavan was done while he was a graduate student at the University of Maryland. The research of A. Srinivasan is partially supported by the National Science Foundation [Awards CNS 1010789, CCF-1422569, CCF-1749864, and CCF-1918749]; Adobe, Inc. [research awards]; Amazon, Inc.; and Google, Inc. The research of P. Xu was supported in part by the National Science Foundation [Awards CNS 1010789 and CCF-1422569 (when he was a graduate student)] and is partially funded by the National Science Foundation [CRII Award IIS-1948157]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2018.0340 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2018.0340},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {986-994},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Improved sample-complexity bounds in stochastic optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust optimization under distorted
expectations. <em>OR</em>, <em>73</em>(2), 969–985. (<a
href="https://doi.org/10.1287/opre.2020.0685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributionally robust optimization (DRO) has arisen as an important paradigm for addressing the issue of distributional ambiguity in decision optimization. In the case in which a decision maker is not risk neutral, the most common scheme applied in DRO for capturing the risk attitude is to employ an expected utility functional. In this paper, we propose to address a decision maker’s risk attitude in DRO by following an alternative scheme known as dual expected utility. In this scheme, a distortion function is applied to convert physical probabilities into subjective probabilities so that the resulting expectation, called a distorted expectation, captures the decision maker’s risk attitude. Unlike an expected utility functional, which is linear in probability, in the dual scheme, the distorted expectation is generally nonlinear in probability. We distinguish DRO based on distorted expectations by coining the term “distributionally robust distortion risk optimization” (DRDRO) and show that DRDRO problems can be equally, if not more, tractable to solve as DRO problems based on utility functionals. Our tractability results hold for any distortion function, and hence, our scheme provides more flexibility in capturing more realistic forms of risk attitudes. These include, as an important example, the inverse S-shaped distortion functionals in cumulative prospect theory. We demonstrate through a numerical example that a production manager who overly weights “very good” and “very bad” outcomes may act as if the manager is risk averse when distributional ambiguity is considered. Funding: J. Cai gratefully acknowledges financial support from the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2022-03354]. J. Y.-M. Li gratefully acknowledges financial support from the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2014-05602]. T. Mao gratefully acknowledges financial support from the National Natural Science Foundation of China [Grants 12371476, 71671176, and 71921001]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0685 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0685},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {969-985},
  shortjournal = {Oper. Res.},
  title        = {Distributionally robust optimization under distorted expectations},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customer scheduling in large service systems under model
uncertainty. <em>OR</em>, <em>73</em>(2), 949–968. (<a
href="https://doi.org/10.1287/opre.2022.0144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling in the context of many-server queues has received considerable attention. When there are multiple customer classes and many servers, it is common to make simplifying assumptions that result in a “low-fidelity” model, potentially leading to model misspecification. However, empirical evidence suggests that these assumptions may not accurately reflect real-world scenarios. Although relaxing these assumptions can yield a more accurate “high-fidelity” model, it often becomes complex and challenging, if not impossible, to solve. In this paper, we introduce a novel approach for decision makers to generate high-quality scheduling policies for large service systems based on a simple and tractable low-fidelity model instead of its complex and intractable high-fidelity counterpart. At the core of our approach is a robust control formulation, wherein optimization is conducted against an imaginary adversary. This adversary optimally exploits the potential weaknesses of a scheduling rule within prescribed limits defined by an uncertainty set by dynamically perturbing the low-fidelity model. This process assists decision-makers in assessing the vulnerability of a given scheduling policy to model errors stemming from the low-fidelity model. Moreover, our proposed robust control framework is complemented by practical data-driven schemes for uncertainty set selection. Extensive numerical experiments, including a case study based on a U.S. call center data set, substantiate the effectiveness of our framework by revealing scheduling policies that can significantly reduce the system’s costs in comparison with established benchmarks in the literature. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0144 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0144},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {949-968},
  shortjournal = {Oper. Res.},
  title        = {Customer scheduling in large service systems under model uncertainty},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic localization methods for convex discrete
optimization via simulation. <em>OR</em>, <em>73</em>(2), 927–948. (<a
href="https://doi.org/10.1287/opre.2022.0030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and analyze a set of new sequential simulation-optimization algorithms for large-scale multidimensional discrete optimization via simulation problems with a convexity structure. The “large-scale” notion refers to that the discrete decision variable has a large number of values from which to choose on each dimension of the decision variable. The proposed algorithms are targeted to identify a solution that is close to the optimal solution given any precision level with any given probability. To achieve this target, utilizing the convexity structure, our algorithm design does not need to scan all the choices of the decision variable, but instead sequentially draws a subset of choices of the decision variable and uses them to “localize” potentially near-optimal solutions to an adaptively shrinking region. To show the power of the proposed methods based on the localization idea, we first consider one-dimensional large-scale problems. We develop the shrinking uniform sampling algorithm, which is proved to achieve the target with an optimal expected simulation cost under an asymptotic criterion. For multidimensional problems, we combine the idea of localization with subgradient information and propose a framework to design stochastic cutting-plane methods, whose expected simulation costs have a low dependence on the scale and the dimension of the problems. In addition, utilizing the discrete nature of the problems, we propose a stochastic dimension-reduction algorithm, which does not require prior information about the Lipschitz constant of the objective function, and its simulation costs are upper bounded by a value that is independent of the Lipschitz constant. We implement the proposed algorithms on synthetic problems and queueing simulation-optimization problems and demonstrate better performances compared with benchmark methods especially for large-scale examples. Funding: H. Zhang and J. Lavaei were funded by grants from Army Research Office (ARO), Office of Naval Research (ONR), Air Force Office of Scientific Research(AFOSR), and National Science Foundation (NSF). H. Zhang was partially supported by the Two Sigma PhD Fellowship. Z. Zheng was partially supported by the Hellman Fellows Fund and NSF. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0030 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0030},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {927-948},
  shortjournal = {Oper. Res.},
  title        = {Stochastic localization methods for convex discrete optimization via simulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A doubly stochastic simulator with applications in arrivals
modeling and simulation. <em>OR</em>, <em>73</em>(2), 910–926. (<a
href="https://doi.org/10.1287/opre.2021.0597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework that integrates classic Monte Carlo simulators and Wasserstein generative adversarial networks to model, estimate, and simulate a broad class of arrival processes with general nonstationary and multidimensional random arrival rates. Classic Monte Carlo simulators have advantages in capturing the interpretable “physics” of a stochastic object, whereas neural network–based simulators have advantages in capturing less interpretable complicated dependence within a high-dimensional distribution. We propose a doubly stochastic simulator that integrates a stochastic generative neural network and a classic Monte Carlo Poisson simulator to utilize the advantages of both. Such integration brings challenges to both theoretical reliability and computational tractability for the estimation of the simulator given real data, in which the estimation is done through minimizing the Wasserstein distance between the distribution of the simulation output and of real data. Regarding theoretical properties, we prove consistency and convergence rate for the estimated simulator under a nonparametric smoothness assumption. Regarding computational efficiency and tractability for the estimation procedure, we address a challenge in gradient evaluation that arises from the discontinuity in the Monte Carlo Poisson simulator. Numerical experiments with synthetic and real data sets are implemented to illustrate the performance of the proposed framework. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2021.0597 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0597},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {910-926},
  shortjournal = {Oper. Res.},
  title        = {A doubly stochastic simulator with applications in arrivals modeling and simulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning and optimization with seasonal patterns.
<em>OR</em>, <em>73</em>(2), 894–909. (<a
href="https://doi.org/10.1287/opre.2023.0017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A standard assumption adopted in the multiarmed bandit (MAB) framework is that the mean rewards are constant over time. This assumption can be restrictive in the business world as decision makers often face an evolving environment in which the mean rewards are time-varying. In this paper, we consider a nonstationary MAB model with K arms whose mean rewards vary over time in a periodic manner. The unknown periods can be different across arms and scale with the length of the horizon T polynomially. We propose a two-stage policy that combines the Fourier analysis with a confidence bound–based learning procedure to learn the periods and minimize the regret. In stage one, the policy correctly estimates the periods of all arms with high probability. In stage two, the policy explores the periodic mean rewards of arms using the periods estimated in stage one and exploits the optimal arm in the long run. We show that our learning policy incurs a regret upper bound O ˜ ( T ∑ k = 1 K T k ) , where T k is the period of arm k . Moreover, we establish a general lower bound Ω ( T max k { T k } ) for any policy. Therefore, our policy is near optimal up to a factor of K . Funding: The research of N. Chen is partly supported by the Natural Sciences and Engineering Research Council of Canada [Discovery Grant RGPIN-2020-04038]. C. Wang acknowledges support from the National Natural Science Foundation of China [Grants 72293561 and 71802115] and the Tsinghua University Initiative Scientific Research Program. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0017 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0017},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {894-909},
  shortjournal = {Oper. Res.},
  title        = {Learning and optimization with seasonal patterns},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate almost stochastic dominance: Transfer
characterizations and sufficient conditions under dependence
uncertainty. <em>OR</em>, <em>73</em>(2), 879–893. (<a
href="https://doi.org/10.1287/opre.2022.0596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most often, important decisions involve several unknown attributes. This produces a double challenge in the sense that both assessing the individual multiattribute preferences and assessing the joint distribution of the attributes can be extremely hard. To handle the first challenge, we suggest multivariate almost stochastic dominance, a relation based on bounding marginal utilities. We provide necessary and sufficient characterizations in terms of simple transfers, which are easily communicated to decision makers and, thus, can be used for preference elicitation. To handle the second challenge, we develop sufficient conditions that do not consider the dependence structure and are based on either marginal distributions of the attributes or just their means and variances. We apply the theoretical results to a case study of comparing the efficiency of photovoltaic plants. Funding: M. Scarsini is a member of Gruppo Nazionale per l’Analisi Matematica, la Probabilità, e le loro Applicazioni-Istituto Nazionale di Alta Matematica Francesco Severi (GNAMPA-INdAM). His work was partially supported by GNAMPA-INdAM (Project CUP_E53C22001930001 “Limiting behavior of stochastic dynamics in the Schelling segregation model”) and the Italian Ministry of Education, Universities, and Research (Research Projects of National Interest 2017 Project ALGADIMAR “Algorithms, Games, and Digital Markets”). Supplemental Material: The online companion is available at https://doi.org/10.1287/opre.2022.0596 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0596},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {879-893},
  shortjournal = {Oper. Res.},
  title        = {Multivariate almost stochastic dominance: Transfer characterizations and sufficient conditions under dependence uncertainty},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified theory of robust and distributionally robust
optimization via the primal-worst-equals-dual-best principle.
<em>OR</em>, <em>73</em>(2), 862–878. (<a
href="https://doi.org/10.1287/opre.2021.0268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust optimization and distributionally robust optimization are modeling paradigms for decision making under uncertainty where the uncertain parameters are only known to reside in an uncertainty set or are governed by any probability distribution from within an ambiguity set, respectively, and a decision is sought that minimizes a cost function under the most adverse outcome of the uncertainty. In this paper, we develop a rigorous and general theory of robust and distributionally robust nonlinear optimization using the language of convex analysis. Our framework is based on a generalized “primal-worst-equals-dual-best” principle that establishes strong duality between a semi-infinite primal worst and a nonconvex dual best formulation, both of which admit finite convex reformulations. This principle offers an alternative formulation for robust optimization problems that obviates the need to mobilize the machinery of abstract semi-infinite duality theory to prove strong duality in distributionally robust optimization. We illustrate the modeling power of our approach through convex reformulations for distributionally robust optimization problems whose ambiguity sets are defined through general optimal transport distances, which generalize earlier results for Wasserstein ambiguity sets. Funding: This research was supported by the Swiss National Science Foundation [NCCR Automation Grant 51NF40_180545] and the Engineering and Physical Sciences Research Council [Grant EP/R045518/1]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0268 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0268},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {862-878},
  shortjournal = {Oper. Res.},
  title        = {A unified theory of robust and distributionally robust optimization via the primal-worst-equals-dual-best principle},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved decision rule approximations for multistage robust
optimization via copositive programming. <em>OR</em>, <em>73</em>(2),
842–861. (<a href="https://doi.org/10.1287/opre.2018.0505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study decision rule approximations for generic multistage robust linear optimization problems. We examine linear decision rules for the case when the objective coefficients, the recourse matrices, and the right-hand sides are uncertain, and we explore quadratic decision rules for the case when only the right-hand sides are uncertain. The resulting optimization problems are NP hard but amenable to copositive programming reformulations that give rise to tight, tractable semidefinite programming solution approaches. We further enhance these approximations through new piecewise decision rule schemes. Finally, we prove that our proposed approximations are tighter than the state-of-the-art schemes and demonstrate their superiority through numerical experiments. Funding: G. A. Hanasusanto was supported by the National Science Foundation [Grants 1752125 and 2153606]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2018.0505 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2018.0505},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {842-861},
  shortjournal = {Oper. Res.},
  title        = {Improved decision rule approximations for multistage robust optimization via copositive programming},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—a data-driven approach to beating SAA out of
sample. <em>OR</em>, <em>73</em>(2), 829–841. (<a
href="https://doi.org/10.1287/opre.2021.0393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whereas solutions of distributionally robust optimization (DRO) problems can sometimes have a higher out-of-sample expected reward than the sample average approximation (SAA), there is no guarantee. In this paper, we introduce a class of distributionally optimistic optimization (DOO) models and show that it is always possible to “beat” SAA out-of-sample if we consider not just worst case (DRO) models but also best case (DOO) ones. We also show, however, that this comes at a cost: optimistic solutions are more sensitive to model error than either worst case or SAA optimizers and, hence, are less robust, and calibrating the worst or best case model to outperform SAA may be difficult when data are limited. Funding: J. Gotoh is supported in part by the Japan Society for the Promotion of Science [Grant 20H00285]. M. J. Kim is supported in part by the Natural Sciences and Engineering Research Council of Canada [Discovery Grant RGPIN-2015-04019]. A. E. B. Lim is supported by the Ministry of Education, Singapore, under its 2021 Academic Research Fund Tier 2 grant call [Grant MOE-T2EP20121-0014]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0393 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0393},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {829-841},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A data-driven approach to beating SAA out of sample},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—on adaptivity in nonstationary stochastic
optimization with bandit feedback. <em>OR</em>, <em>73</em>(2), 819–828.
(<a href="https://doi.org/10.1287/opre.2022.0576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the nonstationary stochastic optimization problem with bandit feedback and dynamic regret measures. The seminal work of Besbes et al. (2015) shows that, when aggregated function changes are known a priori, a simple restarting algorithm attains the optimal dynamic regret. In this work, we design a stochastic optimization algorithm with fixed step sizes, which, combined with the multiscale sampling framework in existing research, achieves the optimal dynamic regret in nonstationary stochastic optimization without prior knowledge of function changing budget, thereby closing a question that has been open for a while. We also establish an additional result showing that any algorithm achieving good regret against stationary benchmarks with high probability could be automatically converted to an algorithm that achieves good regret against dynamic benchmarks (for problems that admit O ˜ ( T ) regret against stationary benchmarks in fully adversarial settings, a dynamic regret of O ˜ ( V T 1 / 3 T 2 / 3 ) is expected), which is potentially applicable to a wide class of bandit convex optimization and other types of bandit algorithms.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0576},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {819-828},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On adaptivity in nonstationary stochastic optimization with bandit feedback},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic online fisher markets: Static pricing limits and
adaptive enhancements. <em>OR</em>, <em>73</em>(2), 798–818. (<a
href="https://doi.org/10.1287/opre.2023.0636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher markets are one of the most fundamental models for resource allocation. However, the problem of computing equilibrium prices in Fisher markets typically relies on complete knowledge of users’ budgets and utility functions and requires transactions to happen in a static market where all users are present simultaneously. Motivated by these practical considerations, we study an online variant of Fisher markets, wherein users with privately known utility and budget parameters, drawn independently and identically (i.i.d.) from a distribution, arrive sequentially. In this setting, we first study the limitations of static pricing algorithms, which set uniform prices for all users, along two performance metrics: (i) regret, that is, the optimality gap in the objective of the Eisenberg-Gale program between an online algorithm and an oracle with complete information, and (ii) capacity violations, that is, the overconsumption of goods relative to their capacities. Given the limitations of static pricing, we design adaptive posted-pricing algorithms, one with knowledge of the distribution of users’ budget and utility parameters and another that adjusts prices solely based on past observations of user consumption, that is, revealed preference feedback, with improved performance guarantees. Finally, we present numerical experiments to compare our revealed preference algorithm’s performance to several benchmarks. Funding: This work was supported by the Stanford Interdisciplinary Graduate Fellowship. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0636 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0636},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {798-818},
  shortjournal = {Oper. Res.},
  title        = {Stochastic online fisher markets: Static pricing limits and adaptive enhancements},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal fairness in learning and earning: Price protection
guarantee and phase transitions. <em>OR</em>, <em>73</em>(2), 775–797.
(<a href="https://doi.org/10.1287/opre.2022.0629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the prevalence of price protection guarantee which helps to promote temporal fairness in dynamic pricing, we study the impact of such policy on the design of online learning algorithms for data-driven dynamic pricing with initially unknown customer demand. Under the price protection guarantee, a customer who purchased a product in the past can receive a refund from the seller during the so-called price protection period (typically defined as a certain time window after the purchase date) in case the seller decides to lower the price. We consider a setting where a firm sells a product over a horizon of T time steps. For this setting, we characterize how the value of M , the length of the price protection period, can affect the optimal regret of the learning process. We derive the optimal regret by first establishing a fundamental impossible regime with the novel refund-aware regret lower bound analysis. Then, we propose LEAP , a phased exploration type algorithm for Learning and EArning under Price Protection, to match this lower bound up to logarithmic factors or even doubly logarithmic factors (when there are only two prices available to the seller). Our results reveal the surprising phase transitions of the optimal regret with respect to M . Specifically, when M is not too large, the optimal regret has no major difference when compared with that of the classic setting with no price protection guarantee. In addition, there also exists an upper limit on how much the optimal regret can deteriorate when M grows large. Finally, we conduct extensive numerical simulations with both synthetic and real-world data sets to show the benefit of LEAP over other heuristic methods for this problem. The numerical results suggest that under certain realistic assumptions, it is indeed beneficial for the seller to set a longer price protection period. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0629 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0629},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {775-797},
  shortjournal = {Oper. Res.},
  title        = {Temporal fairness in learning and earning: Price protection guarantee and phase transitions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Political districting to minimize county splits.
<em>OR</em>, <em>73</em>(2), 752–774. (<a
href="https://doi.org/10.1287/opre.2023.0094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When partitioning a state into political districts, a common criterion is that political subdivisions, like counties, should not be split across multiple districts. This criterion is encoded into most state constitutions and is sometimes enforced quite strictly by the courts. However, map drawers, courts, and the public typically do not know what amount of splitting is truly necessary, even to satisfy basic criteria, like contiguity and population balance. In this paper, we provide answers for all congressional, state senate, and state house districts in the United States using 2020 census data. Our approach is based on integer programming. The associated codes and experimental results are available on GitHub. Funding: This material is based upon work supported by the National Science Foundation [Grant 1942065]. Supplementary Material: All supplemental materials, including the computer code that supports the findings of this study, are available at https://doi.org/10.1287/opre.2023.0094 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0094},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {752-774},
  shortjournal = {Oper. Res.},
  title        = {Political districting to minimize county splits},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EFX: A simpler approach and an (almost) optimal guarantee
via rainbow cycle number. <em>OR</em>, <em>73</em>(2), 738–751. (<a
href="https://doi.org/10.1287/opre.2023.0433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of envy-freeness up to any good (EFX) allocations is a fundamental open problem in discrete fair division. The goal is to determine the existence of an allocation of a set of indivisible goods among n agents for which no agent envies another, following the removal of any single good from the other agent’s bundle. Because the general problem has been elusive, progress is made on two fronts: (i) proving existence when n is small and (ii) proving the existence of relaxations of EFX. In this paper, we improve and simplify the state-of-the-art results on both fronts with new techniques. For the case of three agents, the existence of EFX was first shown with additive valuations and then extended to nice-cancelable valuations. As our first main result, we simplify and improve this result by showing the existence of EFX allocations when two of the agents have general monotone valuations and one has a maximin share (MMS)–feasible valuation (a strict generalization of nice-cancelable valuation functions). Our approach is significantly simpler than the previous ones, and it also avoids using the standard concepts of envy graph and champion graph and may find use in other fair-division problems. Second, we consider approximate EFX allocations with few unallocated goods (charity). Through a promising new method using a problem in extremal combinatorics called rainbow cycle number (RCN), the existence of ( 1 − ϵ ) -EFX allocation with O ( ( n / ϵ ) 4 5 ) charity was established. This is done by upper bounding the RCN by O ( d 4 ) in d -dimension. They conjecture RCN to be O ( d ) . We almost settle this conjecture by improving the upper bound to O ( d log d ) and thereby get (almost) optimal charity of O ˜ ( ( n / ϵ ) 1 2 ) that is possible through this method. Our technique is much simpler than the previous ones and is based on the probabilistic method. Funding: This work was supported by the Division of Computing and Communication Foundations (B. R. Chaudhury, R. Mehta, J. Garg) [Grants CCF-1750436, CCF-1942321, CCF-2334461], the National Science Foundation (N. Alon) [Grant DMS-2154082], and the United States–Israel Binational Science Foundation (N. Alon) [Grant 2018267].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0433},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {738-751},
  shortjournal = {Oper. Res.},
  title        = {EFX: A simpler approach and an (Almost) optimal guarantee via rainbow cycle number},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural estimation of markov decision processes in
high-dimensional state space with finite-time guarantees. <em>OR</em>,
<em>73</em>(2), 720–737. (<a
href="https://doi.org/10.1287/opre.2022.0511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of estimating a structural model of dynamic decisions by a human agent based on the observable history of implemented actions and visited states. This problem has an inherent nested structure: In the inner problem, an optimal policy for a given reward function is identified, whereas in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper, we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising reward estimation accuracy. In the proposed algorithm, each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show the proposed algorithm converges to a stationary solution with a finite-time guarantee. Further, if the reward is parameterized linearly, the algorithm approximates the maximum likelihood estimator sublinearly. Funding: M. Hong and S. Zeng are supported by the National Science Foundation [Grants EPCN-2311007 and CCF-1910385]. This work is also part of AI-CLIMATE: “AI Institute for Climate-Land Interactions, Mitigation, Adaptation, Tradeoffs and Economy” and is supported by the U.S. Department of Agriculture National Institute of Food and Agriculture and the National Science Foundation National AI Research Institutes [Competitive Award 2023-67021-39829]. A. Garcia is partially supported by the Army Research Office [Grant W911NF-22-1-0213]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0511 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0511},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {720-737},
  shortjournal = {Oper. Res.},
  title        = {Structural estimation of markov decision processes in high-dimensional state space with finite-time guarantees},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient algorithms for a class of stochastic hidden convex
optimization and its applications in network revenue management.
<em>OR</em>, <em>73</em>(2), 704–719. (<a
href="https://doi.org/10.1287/opre.2022.0216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of stochastic nonconvex optimization in the form of min x ∈ X F ( x ) ≔ E ξ [ f ( ϕ ( x , ξ ) ) ] , that is, F is a composition of a convex function f and a random function ϕ . Leveraging an (implicit) convex reformulation via a variable transformation u = E [ ϕ ( x , ξ ) ] , we develop stochastic gradient-based algorithms and establish their sample and gradient complexities for achieving an ϵ -global optimal solution. Interestingly, our proposed Mirror Stochastic Gradient (MSG) method operates only in the original x -space using gradient estimators of the original nonconvex objective F and achieves O ˜ ( ϵ − 2 ) complexities, matching the lower bounds for solving stochastic convex optimization problems. Under booking limits control, we formulate the air-cargo network revenue management (NRM) problem with random two-dimensional capacity, random consumption, and routing flexibility as a special case of the stochastic nonconvex optimization, where the random function ϕ ( x , ξ ) = x ∧ ξ , that is, the random demand ξ truncates the booking limit decision x . Extensive numerical experiments demonstrate the superior performance of our proposed MSG algorithm for booking limit control with higher revenue and lower computation cost than state-of-the-art bid-price-based control policies, especially when the variance of random capacity is large. Funding: This work was partly supported by the National Science Foundation [Grants CMMI-1761699, CRII-1755829], the ZJU-UIUC Institute Research Program, and NCCR Automation in Switzerland. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0216 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0216},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {704-719},
  shortjournal = {Oper. Res.},
  title        = {Efficient algorithms for a class of stochastic hidden convex optimization and its applications in network revenue management},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On (random-order) online contention resolution schemes for
the matching polytope of (bipartite) graphs. <em>OR</em>,
<em>73</em>(2), 689–703. (<a
href="https://doi.org/10.1287/opre.2023.0339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Contention Resolution Schemes (OCRSs) represent a modern tool for selecting a subset of elements, subject to resource constraints, when the elements are presented to the algorithm sequentially. OCRSs have led to some of the best-known competitive ratio guarantees for online resource allocation problems, with the added benefit of treating different online decisions—accept/reject, probing, pricing—in a unified manner. This paper analyzes OCRSs for resource constraints defined by matchings in graphs, a fundamental structure in combinatorial optimization. We consider two dimensions of variants: the elements being presented in adversarial or random order; and the graph being bipartite or general. We improve the state of the art for all combinations of variants, both in terms of algorithmic guarantees and impossibility results. Some of our algorithmic guarantees are best-known, even compared with Contention Resolution Schemes that can choose the order of arrival or are offline. All in all, our results for OCRSs directly improve the best-known competitive ratios for online accept/reject, probing, and pricing problems on graphs in a unified manner. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0339 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0339},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {689-703},
  shortjournal = {Oper. Res.},
  title        = {On (Random-order) online contention resolution schemes for the matching polytope of (Bipartite) graphs},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ordinary and prophet planning under uncertainty in bernoulli
congestion games. <em>OR</em>, <em>73</em>(2), 672–688. (<a
href="https://doi.org/10.1287/opre.2023.0252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an atomic congestion game in which each player i participates in the game with an exogenous and known probability p i ∈ ( 0 , 1 ] , independently of everybody else, or stays out and incurs no cost. We compute the parameterized price of anarchy to characterize the impact of demand uncertainty on the efficiency of selfish behavior, considering two different notions of a social planner. A prophet planner knows the realization of the random participation in the game; the ordinary planner does not. As a consequence, a prophet planner can compute an adaptive social optimum that selects different solutions depending on the players who turn out to be active, whereas an ordinary planner faces the same uncertainty as the players and can only minimize the expected social cost according to the player participation distribution. For both types of planners, we obtain tight bounds for the price of anarchy by solving suitable optimization problems parameterized by the maximum participation probability q = max i p i . In the case of affine costs, we find an analytic expression for the corresponding bounds. Funding: The research of R. Cominetti was supported by Fondo Nacional de Desarrollo Científico y Tecnológico [Grants 1171501 and ANID/PIA/ACT192094]. M. Scarsini gratefully acknowledges the support and hospitality of Fondo Nacional de Desarrollo Científico y Tecnológico [Grant 1130564] and Núcleo Milenio “Información y Coordinación en Redes.” The research of M. Scarsini was supported by the Gruppo Nazionale per l’Analisi Matematica, la Probabilità e le loro Applicazioni [Grant CUP_E53C22001930001], the Ministero dell’Università e della Ricerca [Grants PRIN 2017 ALGADIMAR and PRIN 2022 2022EKNE5K], and the European Union–Next Generation EU, Component M4C2, Investment 1.1 [Grant PRIN PNRR P2022XT8C8]. This work was also supported by European Cooperation in Science and Technology [Grant CA16228 GAMENET]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0252 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0252},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {672-688},
  shortjournal = {Oper. Res.},
  title        = {Ordinary and prophet planning under uncertainty in bernoulli congestion games},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically tight bounds on the optimal pricing strategy
with patient customers. <em>OR</em>, <em>73</em>(2), 664–671. (<a
href="https://doi.org/10.1287/opre.2021.0459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers a monopolist seller facing both patient and impatient customers. Given the current price, the impatient customers will either purchase or leave immediately, depending on the relative magnitude between this price and their valuation of the product. In comparison, the patient customers will wait for some periods to see if the price will drop to their valuation, and if that occurs, they will purchase immediately. The monopolist designs the pricing strategy to maximize the long-run average revenue from them. We give tight bounds on both the optimal strategy’s cycle period and the optimal revenue when the patient customers possess a high patience level. This result answers the open question of the optimal cycle period raised by extant work. Later we also extend our theoretical result to the general case with multiple patience levels. Funding: C. Zhou is supported in part by NSFC [Grant 11871364] and IoTeX Foundation Industry [Grant A-8001180-00-00]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0459 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0459},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {664-671},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically tight bounds on the optimal pricing strategy with patient customers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization guarantees for multi-item profit
maximization: Pricing, auctions, and randomized mechanisms. <em>OR</em>,
<em>73</em>(2), 648–663. (<a
href="https://doi.org/10.1287/opre.2021.0026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study multi-item profit maximization when there is an underlying distribution over buyers’ values. In practice, a full description of the distribution is typically unavailable, so we study the setting where the mechanism designer only has samples from the distribution. If the designer uses the samples to optimize over a complex mechanism class—such as the set of all multi-item, multibuyer mechanisms—a mechanism may have high average profit over the samples, but low expected profit. This raises the central question of this paper: How many samples are sufficient to ensure that a mechanism’s average profit is close to its expected profit? To answer this question, we uncover structure shared by many pricing, auction, and lottery mechanisms: For any set of buyers’ values, profit is piecewise linear in the mechanism’s parameters. Using this structure, we prove new bounds for mechanism classes not yet studied in the sample-based mechanism design literature and match or improve over the best-known guarantees for many classes. Finally, we provide tools for optimizing an important tradeoff: More complex mechanisms typically have higher average profit over the samples than simpler mechanisms, but more samples are required to ensure that average profit nearly matches expected profit. Funding: This material is based on work supported by the National Science Foundation [Grants CCF-1422910, CCF-1535967, CCF-1733556, CCF-1910321, IIS-1617590, IIS-1618714, IIS-1718457, IIS-1901403, RI-2312342, SES-1919453 and a Graduate Research Fellowship]; the Army Research Office [Awards W911NF2010081, W911NF1710082, and W911NF2210266]; Office of Naval Research [Award N00014-23-1-2876]; the Defense Advanced Research Projects Agency [Cooperative Agreement HR00112020003]; Vannevar Bush Faculty Fellowship; an Amazon [Research Award]; a Microsoft Research [Faculty Fellowship]; an Amazon Web Services [Machine Learning Research Award]; a Bloomberg [Data Science research grant]; an International Business Machines Corporation [PhD Fellowship]; and a fellowship from Carnegie Mellon University&#39;s Center for Machine Learning and Health. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0026 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0026},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {648-663},
  shortjournal = {Oper. Res.},
  title        = {Generalization guarantees for multi-item profit maximization: Pricing, auctions, and randomized mechanisms},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monetizing positive externalities to mitigate the
infrastructure underinvestment problem. <em>OR</em>, <em>73</em>(2),
632–647. (<a href="https://doi.org/10.1287/opre.2023.0075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many cities face challenges in financing their infrastructure. If a decision maker cannot capture all the benefits of its investment, there is a risk of underinvestment. Hong Kong’s transit operator designed a scheme in which it not only receives fare revenues, but also participates in a property management business, exploiting the positive externalities of public transport on nearby property prices. We develop a stochastic Stackelberg game of timing to explore the rationale of this scheme. The underlying problem is nontrivial because the operator faces a two-dimensional optimal stopping problem that cannot be reduced by a change of numéraire. We determine the operator’s optimal investment policy via the intermediation of a “penalized problem” and derive comparative statics. We determine the circumstances under which monetizing positive externalities effectively favors infrastructure investment. Other management problems have similar structures. Funding: This work was supported by the National Science Foundation [Grant NSF-DMS 220 4795]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0075 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0075},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {632-647},
  shortjournal = {Oper. Res.},
  title        = {Monetizing positive externalities to mitigate the infrastructure underinvestment problem},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Budget-driven multiperiod hub location: A robust time-series
approach. <em>OR</em>, <em>73</em>(2), 613–631. (<a
href="https://doi.org/10.1287/opre.2022.0319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the (un)capacitated multiperiod hub location problem with uncertain periodic demands. With a distributionally robust approach that considers time series, we build a model driven by budgets on periodic costs. In particular, we construct a nested ambiguity set that characterizes uncertain periodic demands via a general multivariate time-series model, and to ensure stable periodic costs, we propose to constrain each expected periodic cost within a budget whereas optimizing the robustness level by maximizing the size of the nested ambiguity set. Statistically, the nested ambiguity set ensures that the model’s solution enjoys finite-sample performance guarantees under certain regularity conditions on the underlying VAR( p ) or VARMA( p , q ) process of the stochastic demand. Operationally, we show that our budget-driven model in the uncapacitated case essentially optimizes a “Sharpe ratio”–type criterion over the worst case among all periods, and we discuss how cost budgets would affect the optimal robustness level. Computationally, the uncapacitated model can be efficiently solved via a bisection search algorithm that solves (in each iteration) a mixed-integer conic program, whereas the capacitated model can be approximated by using decision rules. Finally, numerical experiments demonstrate the attractiveness and competitiveness of our proposed model. Funding: Z. Chen is funded in part by the Hong Kong Research Grants Council General Research Fund [CUHK-11507223] and the National Natural Science Foundation of China [72394395, 72422002]. S. Wang is supported by the National Natural Science Foundation of China [Grants 72471224, 72171221, 71922020, and 71988101], the Fundamental Research Funds for the Central Universities [Grant UCAS-E2ET0808X2], and a grant from MOE Social Science Laboratory of Digital Economic Forecasts and Policy Simulation at UCAS. J. Hu is supported by the National Natural Science Foundation of China [Grants 72192843 and 71872171]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, were reviewed and are available at https://doi.org/10.1287/opre.2022.0319 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0319},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {613-631},
  shortjournal = {Oper. Res.},
  title        = {Budget-driven multiperiod hub location: A robust time-series approach},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust workforce management with crowdsourced delivery.
<em>OR</em>, <em>73</em>(2), 595–612. (<a
href="https://doi.org/10.1287/opre.2023.0125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how crowdsourced delivery platforms with both contracted and ad hoc couriers can effectively manage their workforce to meet delivery demands amidst uncertainties. Our objective is to minimize the hiring costs of contracted couriers and the crowdsourcing costs of ad hoc couriers, while considering the uncertain availability and behavior of the latter. Because of the complication of calibrating these uncertainties through data-driven approaches, we instead introduce a basic reduced information model to estimate the upper bound of the crowdsourcing cost and a generalized reduced information model to obtain a tighter bound. Subsequently, we formulate a robust satisficing model associated with the generalized reduced information model and show that a binary search algorithm can tackle the model exactly by solving a modest number of convex optimization problems. Our numerical tests using Solomon’s data sets show that reduced information models provide decent approximations for practical delivery scenarios. Simulation tests further demonstrate that the robust satisficing model has better out-of-sample performance than the empirical optimization model that minimizes the total cost under historical scenarios. Funding: C. Cheng was supported by the National Natural Science Foundation of China [Grants 72471042, 72101049, and 72232001] and the Fundamental Research Funds for the Central Universities [Grant DUT23RC(3)045]. M. Sim and Y. Zhao were supported by the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0125 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0125},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {595-612},
  shortjournal = {Oper. Res.},
  title        = {Robust workforce management with crowdsourced delivery},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Search for an immobile hider on a binary tree with
unreliable locational information. <em>OR</em>, <em>73</em>(2), 583–594.
(<a href="https://doi.org/10.1287/opre.2023.0128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial search of a network for an immobile Hider (or target) was introduced and solved for rooted trees by Shmuel Gal in 1979. In this zero-sum game, a Hider picks a point to hide on the tree and a Searcher picks a unit speed trajectory starting at the root. The payoff (to the Hider) is the search time. In Gal’s model (and many subsequent investigations), the Searcher receives no additional information after the Hider chooses his location. In reality, the Searcher will often receive such locational information. For homeland security, mobile sensors on vehicles have been used to locate radioactive material stashed in an urban environment. In a military setting, mobile sensors can detect chemical signatures from land mines. In predator-prey search, the predator often has specially attuned senses (hearing for wolves, vision for eagles, smell for dogs, sonar for bats, pressure sensors for sharks) that may help it locate the prey. How can such noisy locational information be used by the Searcher to modify her route? We model such information as signals which indicate which of two branches of a binary tree should be searched first, where all signals have a known accuracy p &lt; 1. Our solution calculates which branch (at every branch node) is favored , meaning it should always be searched first when the signal is in that direction. When the signal is in the other direction, we calculate the probability the signal should be followed. Compared with the optimal Hider strategy in the classic search game of Gal, the Hider’s optimal distribution for this model is more skewed toward leaf nodes that are further from the root. Funding: This work was supported by the Air Force Office of Scientific Research [Grant FA9550-23-1-0556]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0128 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0128},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {583-594},
  shortjournal = {Oper. Res.},
  title        = {Search for an immobile hider on a binary tree with unreliable locational information},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="orsc---22">ORSC - 22</h2>
<ul>
<li><details>
<summary>
(2025). Timing is everything: An imprinting framework for the
implications of leader emotional expressions for team member social
worth and performance. <em>ORSC</em>, <em>36</em>(1), 514–546. (<a
href="https://doi.org/10.1287/orsc.2023.17390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leader emotional expressions have profound implications for team members. Research has established that how frequently leaders express positive and negative emotional expressions shapes team member performance through conveying critical social-functional information about team member social worth. Yet, this social-functional approach to emotions has not fully considered how the timing of leader emotional expressions during a team’s lifecycle can also shape the information conveyed to individual team members about their social worth. In this paper, we integrate the social-functional approach to emotions with imprinting theory to propose that the temporal context of leader emotional expressions has performance implications for individual team members through two distinct facets of social worth: respect and status. Specifically, our imprinting framework explains how positive leader emotional expressions during the early team phase have the most beneficial performance implications through imprinting respect in individual team members. We then propose that these positive implications are amplified by more-frequent-than-average negative leader emotional expressions during the midpoint phase. When filtered through earlier positive expressions, negative emotional expressions during the midpoint phase may signal opportunities for respect and status gains rather than respect and status losses. We find general support for our model in a preregistered four-wave longitudinal archival study of 9,968 team members on 234 consulting teams at a leading professional services company and a four-wave longitudinal field study at a NCAA Division 1 sports program including 245 student-athletes and 86 coaches on 20 varsity teams. Our work highlights that the temporal context of leader emotional expressions is an important performance predictor through social worth. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17390 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17390},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {514-546},
  shortjournal = {Organ. Sci.},
  title        = {Timing is everything: An imprinting framework for the implications of leader emotional expressions for team member social worth and performance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trailblazing motivation and marginalized group members:
Changing expectations to pave the way for others. <em>ORSC</em>,
<em>36</em>(1), 477–513. (<a
href="https://doi.org/10.1287/orsc.2021.15624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employees from marginalized groups frequently face low performance expectations based on group membership. Although past research shows several different reactions to these types of expectations, such as stereotype threat and stereotype reactance, scholars still know little about when and why low expectations spur individuals to not only try to prove themselves, but to seek to change expectations and opportunities for others like them. Addressing this discrepancy, I introduce trailblazing motivation, which captures the desire to set new precedents that open doors for others. I integrate self-determination theory and regulatory focus theory to identify group-based low expectations, moderated by a sense of belonging with one’s broad marginalized group, and core self-evaluations as key antecedents of trailblazing motivation. I hypothesize that trailblazing motivation will lead to not only greater persistence in one’s work—as with stereotype reactance—but also potentially riskier behaviors aimed at changing expectations for one’s broad group on a larger scale, including advocacy for other marginalized group members and diversity, equity, and inclusion–related issue selling. I test and find support for these hypotheses across a time-lagged survey study and a preregistered experiment. I also establish discriminant validity for trailblazing motivation from other responses to group-based low performance expectations. This research advances our understanding of the behavior of marginalized individuals at work by helping to explain (1) when and why people facing group-based low expectations go beyond seeking to prove their own abilities and also strive to effect change for their marginalized group as a whole and (2) how a closer connection to one’s marginalized group can drive people to increase opportunities for that broad group. Funding: This research was funded by a grant from The Leadership Center at The Wharton School, University of Pennsylvania [Grant 2000-0267] as well as through Post-doctoral Fellowship research funds allocated by The Tuck School, Dartmouth College.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15624},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {477-513},
  shortjournal = {Organ. Sci.},
  title        = {Trailblazing motivation and marginalized group members: Changing expectations to pave the way for others},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The indirect effect of entrepreneurship on pay dispersion:
Entry cost reduction, mobility threat, and wage redistribution within
incumbent firms. <em>ORSC</em>, <em>36</em>(1), 452–476. (<a
href="https://doi.org/10.1287/orsc.2022.16201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Past research has examined the link between initiatives promoting entrepreneurship and compensation, but scholars have predominantly focused on earnings of individuals directly engaged in the founding process, such as founders, cofounders, and start-up employees. Shifting our focus to incumbent workers, we instead propose that a decline in the cost of entrepreneurship increases the variance in pay among incumbent workers who are not involved in entrepreneurial activities. We posit that, as entrepreneurship becomes a more attractive career option, due to institutional changes, the outside option value of entrepreneurship increases. The resulting increase in mobility threat will disproportionately benefit high earners or those employees who are more difficult to replace: As their bargaining power increases, incumbents will disproportionately reward these workers, especially when they are systematically more inclined to leave for entrepreneurship. We explore these arguments using a difference-in-differences methodology, based on the enactment of an entry reform that reduced the cost of entry in Portugal between 1995 and 2009. We find that an exogenous decrease in the administrative costs of establishing a new venture led to high earners capturing disproportionate rewards relative to low earners. We further show that this relationship was especially pronounced among high earners who (a) exhibited a higher ex ante propensity to transition into entrepreneurship; (b) had fewer credible outside options in paid employment; and (c) operated in industries with decentralized wage bargaining arrangements. By documenting the impact of institutional changes that promote entrepreneurship on incumbent workers’ pay, our study contributes to recent debates about the impact of entrepreneurship on individual earnings. Funding: This work was supported by the CY Initiative, the FCT—Portuguese Foundation of Science and Technology, and the Knowledge, Technology and Organization Research Center at Skema. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16201 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16201},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {452-476},
  shortjournal = {Organ. Sci.},
  title        = {The indirect effect of entrepreneurship on pay dispersion: Entry cost reduction, mobility threat, and wage redistribution within incumbent firms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving virtual team collaboration paradox management: A
field experiment. <em>ORSC</em>, <em>36</em>(1), 429–451. (<a
href="https://doi.org/10.1287/orsc.2023.17952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual teams are ubiquitous in the workplace, yet they experience frequent collaboration challenges. Successfully managing the team collaboration paradox, in terms of maintaining a unified team perspective and diverse individual perspectives, presents a potentially important lever to improve virtual team performance. However, scholars have conflicting opinions regarding whether such improvement is possible. We argue that team collaboration paradox management will positively relate to team performance over time and can be improved via a theory-based intervention. This intervention draws from theory on paradoxes for its content (paradoxical thinking) and team development interventions for its structure (general content knowledge, team-specific feedback, action-focused planning). Given the complexity of paradoxes, it is unclear whether a single training session could substantively improve their management; therefore, one intervention condition was comprised of a single training session and the other condition included a follow-up session. Analyzing two waves of multisource quantitative data from a sample of 76 virtual teams from 37 organizations, we find a positive relationship between team collaboration paradox management and team performance at both time periods. We also find that only the intervention condition with the follow-up session, as compared with untreated control teams, significantly improved how well teams managed the collaboration paradox and thereby facilitated subsequent changes in team performance. Supplementary qualitative insights from the intervention sessions illuminate the actions virtual teams took to improve their collaboration paradox management. These results have important implications for the paradox and teams literatures, as well as the managers and members of virtual teams. Funding: This work was supported by the SHRM Foundation [Project 166].},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17952},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {429-451},
  shortjournal = {Organ. Sci.},
  title        = {Improving virtual team collaboration paradox management: A field experiment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cultural spawning: Founders bringing organizational cultures
to their startup. <em>ORSC</em>, <em>36</em>(1), 411–428. (<a
href="https://doi.org/10.1287/orsc.2023.17771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In searching for the sources of heterogeneity in organizational cultures, one possibility is that new ventures show a cultural spawning of incorporating elements from the culture of the organization that the founder left (the parent) when starting the new venture. Such a genealogical effect would explain why some organizational cultures remain different, despite the opportunities to learn cultural elements from other organizations. This study investigates whether and under what circumstances such cultural spawning occurs. We argue that cultural spawning occurs, but with varying strength, depending on the founder and the culture of the parent organization. Applying the cultural toolkit perspective, we predict that it is stronger when founders have a longer tenure in the parent organization, the parent culture is more internally coherent, and the parent culture is more atypical, compared with other organizations. These ideas were tested with a sample of U.S. technology startups in Crunchbase. Natural language processing of Glassdoor employee reviews was used to identify the cultural elements of the technology companies. The analysis demonstrates that cultural spawning occurs and reveals previously unexplored contingencies of cultural transmission through congruency and atypicality. It contributes to research on new venture formation and organizational culture. Funding: Financial support from the Rudolf and Valeria Maag Endowment is gratefully acknowledged. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17771 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17771},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {411-428},
  shortjournal = {Organ. Sci.},
  title        = {Cultural spawning: Founders bringing organizational cultures to their startup},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Organizational selection of innovation. <em>ORSC</em>,
<em>36</em>(1), 387–410. (<a
href="https://doi.org/10.1287/orsc.2023.17357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Budgetary constraints force organizations to pursue only a subset of possible innovation projects. Identifying which subset is most promising is an error-prone exercise, and involving multiple decision makers may be prudent. This raises the question of how to most effectively aggregate their collective nous. Our model of organizational portfolio selection provides some first answers. We show that portfolio performance can vary widely. Delegating evaluation makes sense when organizations employ the relevant experts and can assign projects to them. In most other settings, aggregating the impressions of multiple agents leads to better performance than delegation. In particular, letting agents rank projects often outperforms alternative aggregation rules—including averaging agents’ project scores and counting their approval votes—especially when organizations have tight budgets and can select only a few project alternatives out of many. Funding: L. Böttcher acknowledges financial support from hessian.AI and the Army Research Office [Grant W911NF-23-1-0129]. Supplemental Material: The data files are available at https://doi.org/10.1287/orsc.2023.17357 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17357},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {387-410},
  shortjournal = {Organ. Sci.},
  title        = {Organizational selection of innovation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A buddhist mindfulness view of paradox: Silence and
skepticism of language to dismantle paradoxes. <em>ORSC</em>,
<em>36</em>(1), 361–386. (<a
href="https://doi.org/10.1287/orsc.2023.17606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores how Buddhist mindfulness as a self-reflective practice helps individuals respond to a paradox and ultimately dismantle it. To deeply immerse myself into this context, I conducted a nine-month ethnographic fieldwork in three Korean Buddhist temples that confront the paradox between the need for financial resources and spiritual values that disavow money. The findings show a series of cognitive mechanisms that reveal multiple roles of mindfulness, manifested as silence and skepticism of language. First, the monastic environment enables monks to become familiar with a life of silence that turns their attention to the inner mind from the external-empirical world. The silence serves as a mental buffer when monks switch between their sacred role and their business role. Over time, deep silence directs them to skepticism of language that triggers doubt on preexisting linguistic categories, boundaries, and separations. When the preexisting linguistic categories finally disappear in their mind, monks no longer rely on any differentiating or integrating tactic to navigate their paradox. In other words, they no longer perceive a paradox, which means the paradox has disappeared from their life. These cognitive mechanisms construct the monks’ worldview on contradictions, conflicts, and dualities, leading them from the experience of paradox to a unique mental state, the nonexperience of paradox. Integrating this mental state and the worldview of Buddhist monks with paradox research, this study theorizes a Buddhist mindfulness view of paradox. Funding: This work was supported by Chulalongkorn University.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17606},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {361-386},
  shortjournal = {Organ. Sci.},
  title        = {A buddhist mindfulness view of paradox: Silence and skepticism of language to dismantle paradoxes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Through the narrows: The meaning and enactment of
interpersonal holding. <em>ORSC</em>, <em>36</em>(1), 340–360. (<a
href="https://doi.org/10.1287/orsc.2023.17661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Helping practices that focus on returning members to their work tasks amid significant distress do little to attend to how they may be severely hindered by their own self-limiting responses to that distress. The focus of this study is on helping that attends to individuals limited in their responses to distress—individuals who inhabit, metaphorically, the narrows, a place of diminishment in which individuals are made smaller by accessing only specific parts of their selves. Through in-depth interviews with professionals working with individuals suffering from various distressing conditions, events, and situations, I develop generalizable theoretical insights about interpersonal holding as a specific form of helping dedicated to surfacing, expanding, and integrating aspects of individuals’ selves that have receded amid distress. The findings indicate a sequence of holding behaviors marked by three overlapping phases: holders coming alongside, linking up with, and guiding individuals in distress through narrowed intrapsychic spaces. This sequence is enabled, first, by the availability of individuals to interpersonal holding and second, by aspects of the holders that stabilize them during the complicated work of attending to the agentic selves of others. The study contributes to both the evolution of scholarship on distress helping in the context of resilience, loss, and role distress and to theory about interpersonal holding.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17661},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {340-360},
  shortjournal = {Organ. Sci.},
  title        = {Through the narrows: The meaning and enactment of interpersonal holding},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do investors value workforce gender diversity?
<em>ORSC</em>, <em>36</em>(1), 313–339. (<a
href="https://doi.org/10.1287/orsc.2022.17098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine whether investors value workforce gender diversity. Consistent with the view that investors believe workforce gender diversity can be valuable in major firms, we use event studies to demonstrate that U.S. technology firms and financial firms experience more positive stock price reactions when it is revealed that they have relatively higher (versus lower) workforce gender diversity numbers. For instance, we find that Google’s revelation of relatively low workforce gender diversity numbers triggered a negative stock price reaction, whereas eBay’s revelation of relatively high workforce gender diversity numbers triggered a positive stock price reaction. These stock price reactions are both economically and statistically significant; for example, we estimate that if a technology firm had revealed gender diversity numbers that were one standard deviation higher, its market valuation would have increased by $1.11 billion. Corroborating this plausibly causal field evidence, we also find positive investor reactions to workforce gender diversity in randomized experiments using Prolific participants with investing experience; these reactions seem to be underpinned by investors’ beliefs about potential upsides of diversity for the firm (e.g., reduced legal risks; increased creativity) but not by investors’ beliefs about potential downsides of diversity for the firm (e.g., increased conflict). Our findings highlight the importance of understanding investors’ intuitions or beliefs about major organizational phenomena such as workforce gender diversity. Our results also point toward a new type of business case for diversity, driven by investors: if major firms had more workforce gender diversity, investors may “reward” them with substantially higher valuations. Funding: This research is supported by NUS (National University of Singapore) Startup Grant WBS A-0003900-00-00 to D. P. Daniels.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.17098},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {313-339},
  shortjournal = {Organ. Sci.},
  title        = {Do investors value workforce gender diversity?},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal miscoupling: The challenges and consequences of
enacting a practice in decline. <em>ORSC</em>, <em>36</em>(1), 288–312.
(<a href="https://doi.org/10.1287/orsc.2021.16026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A practice firmly entrenched in one period can experience decline or even deinstitutionalization in another. However, at the organization level, practice abandonment can be a slow process. Organizations may continue to use a practice that is in decline or out of date, coupling asynchronously with a prior institutional environment. In this paper we develop the concept of temporal miscoupling and examine the challenges of performing a practice under this condition. Drawing on a field study of a labor strike in Canada, we examine the difficulties of inhabiting the practice of striking—picketing a workplace—with eroding political, legal, and cultural support; declining familiarity with the practice; and fading narratives to motivate and justify the practice. We show how strikers developed extemporaneous resources that gave local meaning and form to the practice. These improvised resources supported the practice but distorted historical meanings and performances. Our study expands the analytical repertoire of inhabited institutionalism by problematizing the temporal lag between institutional conditions and organizational practices for the on-the-ground enactment of practices. The extemporaneous resources generated in the context of temporal miscoupling threaten future enactments, indicating an important role for practice enactment in processes of decline and revitalization. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.16026 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.16026},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {288-312},
  shortjournal = {Organ. Sci.},
  title        = {Temporal miscoupling: The challenges and consequences of enacting a practice in decline},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging the chasm between intentions and behaviors:
Developing and testing a construal level theory of internal
whistle-blowing. <em>ORSC</em>, <em>36</em>(1), 261–287. (<a
href="https://doi.org/10.1287/orsc.2021.15292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent wave of corporate scandals has necessitated a more systematic investigation of internal whistle-blowing as a potential way to prevent wrongdoing. Our understanding of whistle-blowing, however, has been hampered by a deep chasm that exists between employees’ intent to blow the whistle and their whistle-blowing behaviors. We argue that to fully bridge this gap, we need to consider employees’ cognitive states at the time of whistle-blowing intentions versus behaviors and to link these cognitive states to the ethical systems within the organization’s ethical infrastructure to understand which systems are more effective in cultivating whistle-blowing intentions and which systems help translate those intentions into behaviors. Across one multisource field study and one multiwave experiment, we found support for our arguments that top management values-based communication systems, which are more high construal (abstract), affect whistle-blowing intentions whereas ethical accountability systems and ethical retaliatory systems, which are more low construal (concrete), moderate the relationship between whistle-blowing intentions and behaviors. By linking ethical systems within the organization’s ethical infrastructure to the two stages (intentions and behaviors) of the whistle-blowing process and the accompanying cognitive states, we develop and empirically test a construal level theory of internal whistle-blowing. Funding: This work was supported by the Ministry of Education, Singapore, under its Academic Research Fund Grant Calls [MOE 2019-T2-1-192 and MSS16B012].},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15292},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {261-287},
  shortjournal = {Organ. Sci.},
  title        = {Bridging the chasm between intentions and behaviors: Developing and testing a construal level theory of internal whistle-blowing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regulation and innovation revisited: How restrictive
environments can promote destabilizing new technologies. <em>ORSC</em>,
<em>36</em>(1), 240–260. (<a
href="https://doi.org/10.1287/orsc.2022.16770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous literature offers conflicting findings on how the restrictiveness of the regulatory environment—the amount of rules that prohibit specific activities—affects innovation of firms. One camp suggests that restrictiveness circumscribes the range of available technological components and therefore decreases innovation. The other camp believes that restrictiveness can lead firms to seek new alternative technological components, which could increase innovation. In this article, we develop a new theory on regulation and innovation to reconcile these views, which we test using novel data on federal regulations and the patents of 1,242 firms, from 1994 to 2013. We find that restrictiveness can have both a negative and positive relationship with innovation output depending on the level of regulatory uncertainty and the innovation type in question. Funding: This work was supported by the Mercatus Center, George Mason University, and the Strategy Research Foundation [Grant SRF-2021-DRG-8365]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16770 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16770},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {240-260},
  shortjournal = {Organ. Sci.},
  title        = {Regulation and innovation revisited: How restrictive environments can promote destabilizing new technologies},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The relative effects of a scandal on member engagement in
rites of integration and rites of passage: Evidence from a child abuse
scandal in the catholic archdiocese of philadelphia. <em>ORSC</em>,
<em>36</em>(1), 213–239. (<a
href="https://doi.org/10.1287/orsc.2022.16682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizational research has documented that scandals lead to negative aggregate stakeholder reactions. There is little reason to believe, however, that the effects of a scandal are homogenous across different types of engagement. We therefore compare the effects of a scandal on member engagement in two types of rites at normative organizations: rites of integration and rites of passage. Rites of integration focus on the community, celebrate organizational values, and help strengthen organizational identification; they are thus enacted more by core members. Rites of passage focus on the individual, celebrate transition between social roles, and require only occasional engagement; they are thus enacted by core and peripheral members. Because of these differences, we hypothesize that a normative organization’s implication in a scandal affects rites of passage more negatively than rites of integration, but that this effect depends on scandal prevalence among neighboring organizations, organizational age, and organizational size. We test our hypotheses in the context of a child abuse scandal in the Catholic Archdiocese of Philadelphia. Using yearly parish-level data from 1990 to 2010, we find that a parish’s implication in the scandal was associated with a larger decline in rites of passage (marriages, baptisms, and funerals) than in rites of integration (mass attendance). This difference was reversed with the increase in scandal prevalence. Furthermore, rites of integration were more resilient than rites of passage at older and larger parishes. To help rule in the plausibility of our organization-level theory, we present a simulation grounded in individual-level polling data from the context. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16682 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16682},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {213-239},
  shortjournal = {Organ. Sci.},
  title        = {The relative effects of a scandal on member engagement in rites of integration and rites of passage: Evidence from a child abuse scandal in the catholic archdiocese of philadelphia},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Location-specificity and relocation incentive programs for
remote workers. <em>ORSC</em>, <em>36</em>(1), 186–212. (<a
href="https://doi.org/10.1287/orsc.2023.17712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precipitous growth of remote work has given rise to a new phenomenon: the emergence of relocation incentive programs that localities use to compete for the physical presence of remote workers. Remote workers with high general human capital may create value for their new destinations and reverse net talent outflow from smaller cities in middle America and globally. However, localities seeking to attract, retain, and create value from remote workers face significant challenges because such workers may have a low attachment to their new destination. Analogizing these challenges to the problem of creating and capturing value from workers with general human capital, we argue that localities can benefit from using relocation incentive program by leveraging location-specific attributes that create value for the individual and the locality. We examined these ideas in the context of Tulsa Remote, a program that provides relocation incentives and a bundle of services to increase engagement and embeddedness in Tulsa, Oklahoma. We found that Tulsa Remote increased community engagement, real income, and entrepreneurship of remote workers, benefiting both the community and the individual. Tulsa Remote increased the worker’s willingness to stay, and local community engagement is a key driver of this relationship. This work thus suggests that location specificity enables localities to both create and capture value from remote workers. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17712 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17712},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {186-212},
  shortjournal = {Organ. Sci.},
  title        = {Location-specificity and relocation incentive programs for remote workers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How mixed performance feedback shapes exploration: The
moderating role of self-enhancement. <em>ORSC</em>, <em>36</em>(1),
166–185. (<a href="https://doi.org/10.1287/orsc.2021.15676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct an experiment to examine how providing decision makers with high versus low peer performance information influences choices between exploration and exploitation. Previous work on organization-level learning suggests that a high-performing peer would fuel exploration, whereas a low-performing peer would dampen it. In line with this, we find that individuals who receive information about a high-performing peer explore more than those who receive information about a low-performing peer. However, we also find that compared to individuals with a low tendency to self-enhance, individuals with a high tendency to self-enhance are less likely to explore when receiving information about a high-performing peer. In fact, these individuals explore at levels comparable to those who receive information about a low-performing peer. We explain this behavioral pattern by demonstrating that as individuals learn and improve, information about a high-performing peer increasingly results in mixed performance feedback; under these conditions of relative interpretive flexibility, exploration is moderated by decision makers’ tendency to self-enhance. When these individual dynamics are aggregated, our data suggest that an organization that provides peer performance information may experience either the same or less exploration than an organization that does not, with the exact difference depending on its proportion of high self-enhancers. These insights into the contingencies and aggregate effects of how individuals interpret and respond to peer performance information are particularly relevant given recent interest in designing organizations that shape employee behavior through the provision of feedback rather than through traditional instruments of coordination and control, such as incentives or hierarchy. Funding: This work was supported by Danmarks Frie Forskningsfond [Grant 25194]. Additionally, this research was supported by a grant from the HEC Paris Foundation.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15676},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {166-185},
  shortjournal = {Organ. Sci.},
  title        = {How mixed performance feedback shapes exploration: The moderating role of self-enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of financial resources on misconduct: Evidence
from lottery ticket sales. <em>ORSC</em>, <em>36</em>(1), 145–165. (<a
href="https://doi.org/10.1287/orsc.2023.17541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the influence of financial resources on a firm’s propensity for misconduct. Previous studies offer conflicting predictions regarding the relationship, and much of the empirical evidence suffers from issues like selection, measurement error, reverse causality, and omitted variable bias. Leveraging a difference-in-differences design, we first examine quasirandom fluctuations in retailers’ financial resources resulting from large windfalls from selling winning lottery tickets. Our results suggest that an increase in financial resources from selling a large winning lottery ticket reduces retailers’ tobacco sales to minors. Next, to rule in strain theory and to rule out alternative explanations, we leverage a second natural experiment, heterogeneity analysis, an alternate measure of misconduct, and an online randomized experiment. In doing so, we provide plausibly causal evidence on the relationship between a firm’s financial resources and its propensity for misconduct and provide potentially useful findings for policymakers and regulators. Funding: This work was supported by the Blake Family Fund for Ethics, Leadership, and Governance at Purdue University. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17541 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17541},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {145-165},
  shortjournal = {Organ. Sci.},
  title        = {The effect of financial resources on misconduct: Evidence from lottery ticket sales},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The double-edged sword of exemplar similarity.
<em>ORSC</em>, <em>36</em>(1), 121–144. (<a
href="https://doi.org/10.1287/orsc.2022.16855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how a firm’s positioning relative to category exemplars shapes security analysts’ evaluations. Using a two-stage model of evaluation (initial screening and subsequent assessment), we propose that exemplar similarity enhances a firm’s recognizability and legitimacy, increasing the likelihood that it passes the initial screening stage and attracts analyst coverage. However, exemplar similarity may also prompt unfavorable comparisons with exemplar firms, leading to lower analyst recommendations in the assessment stage. We further argue that category coherence, distinctiveness, and exemplar typicality influence the impact of exemplar similarity on firm evaluation. Leveraging natural language processing (NLP) techniques to analyze a sample of 7,603 U.S. public firms from 1997 to 2022, we find robust support for our predictions. By highlighting the intricate role of strategic positioning vis-à-vis category exemplars in shaping audience evaluations, our findings have important implications for research on positioning relative to category exemplars, category viability, optimal distinctiveness, and security analysts. Supplemental Material: The online appendices are available at https://doi.org/10.1287/orsc.2022.16855 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16855},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {121-144},
  shortjournal = {Organ. Sci.},
  title        = {The double-edged sword of exemplar similarity},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measure twice, cut once: Unit profitability, scalability,
and the exceptional growth of new firms. <em>ORSC</em>, <em>36</em>(1),
88–120. (<a href="https://doi.org/10.1287/orsc.2021.15970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By studying three promising venture pairs with different business models in different nascent markets, we explore how some new firms grow into mature firms and some promising peers do not. The successful firms first use a learning process that begins with (1) focused attention on unit profitability (not growth), then broad (not narrow) learning, and finally active delay (not acceleration) of growth. They then use a capability-building process that (2) shifts to focused attention on unit-profitable growth, then makes long-term investments in profit-oriented capabilities, and finally leverages these capabilities into new products. In contrast, peers with focused attention on growth ironically fail to achieve it. Broadly, we contribute a process theory of exceptional growth that fills the gap between the growth of new firms in the entrepreneurship literature and the growth of mature firms in the strategy and organization theory literatures. Further, we add the importance of learning content —not just learning processes—to the entrepreneurship literature on new firm growth and a rare, novel, and grounded account of the origin of capabilities to the strategy literature on the resource-based view and mature firm growth. Finally, we add to practice the significance of product–market–profit fit and a precise definition of premature scaling . Funding: This work was supported by the Strategy Research Foundation [Grant SRF-2018-DP-9167].},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15970},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {88-120},
  shortjournal = {Organ. Sci.},
  title        = {Measure twice, cut once: Unit profitability, scalability, and the exceptional growth of new firms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CEO initial contract duration and corporate acquisitions.
<em>ORSC</em>, <em>36</em>(1), 65–87. (<a
href="https://doi.org/10.1287/orsc.2022.16493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the organizational impact of CEO initial contract duration on corporate acquisitions. We argue that CEOs with shorter initial contract durations are more likely to experience time pressure. Consequently, they are more likely to manage time by engaging in corporate mergers and acquisitions (M&amp;As) to achieve quick growth. In addition, these CEOs are more likely to engage in straightforward deals, acquiring targets that are private, divested, related, small, and using cash payment, because these types of transactions are quicker to complete, carry less risk, and generally come with good performance prospects. Using a sample of firms that underwent new CEO appointments between 1990 and 2017 and detailed employment contract data collected from SEC filings, we find strong support for our hypotheses. In addition, we apply UK corporate governance reform to CEO contract duration as an exogenous shock to show causal evidence of such relations. This study contributes to the literature on CEO contracts, corporate acquisitions, time management and strategic leadership. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16493 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16493},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {65-87},
  shortjournal = {Organ. Sci.},
  title        = {CEO initial contract duration and corporate acquisitions},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opportunistic change during a punctuation: How and when the
front lines can drive bursts of incremental change. <em>ORSC</em>,
<em>36</em>(1), 40–64. (<a
href="https://doi.org/10.1287/orsc.2021.15120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental jolts can trigger more conducive conditions for driving change in organizations. However, punctuated equilibrium theories of organizational change concentrate on top managers’ implementation of de novo radical changes after jolts. Existing research has not examined frontline-driven, incremental change efforts during these periods of disrupted stasis, despite the value of frontline change ideas. We develop a process model to explain how and when those on an organization’s front lines can leverage a jolt to opportunistically implement long-desired change ideas in ways that promote their retention. We conducted a two-year qualitative field study at a hospital during the Covid-19 pandemic, examining the trajectories of 33 premeditated change ideas raised by frontline staff. By comparing ideas that persisted to become part of normal operations with those that failed to be selected or retained, we identified practices and conditions that promoted the selection and retention of frontline change ideas. Our study suggests that frontline change advocates can seed the long-term retention of “shovel-ready” ideas—as opposed to de novo ideas—after a jolt by rapidly and opportunistically deploying a novel set of practices before the brief window of opportunity created by lessened constraints and increased managerial receptivity closes. Prior theories of change largely assume frontline-driven change to be slow and continuous, proceeding in a one-off fashion; we explain how and when frontline change can instead occur in rapid, opportunistic bursts. This study advances theories of punctuated equilibrium and bottom-up change in organizations by unearthing an alternative way that change can be intentionally accomplished in organizations. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.15120 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15120},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {40-64},
  shortjournal = {Organ. Sci.},
  title        = {Opportunistic change during a punctuation: How and when the front lines can drive bursts of incremental change},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention focus and new opportunities: The moderating role
of managerial attention to alternative issues. <em>ORSC</em>,
<em>36</em>(1), 21–39. (<a
href="https://doi.org/10.1287/orsc.2021.15861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The strategy literature highlights the critical role that managerial attention plays in shaping firms’ responses to industry change. However, attention to change alone is not sufficient to precipitate action. The context in which this attention is paid is also critical in determining the translation of attention to action as managers generally pay attention to multiple issues within their strategic agendas. We argue that attention to a competing issue and the breadth of managerial strategic attention impair the relationship between attention to a focal issue and related organizational action as these attentional constraints may lead to hesitancy or confusion at an individual level and reduced coordination at an organizational level. Realizing that at an organizational level, firms vary tremendously with respect to their sizes, resources, and by implication their overall attentional capacities, we argue that the negative moderation impacts of attention to a competing issue and the breadth of strategic attention on the attention to a focal issue–subsequent action relationship are larger for smaller firms. Studying the U.S. electric utility industry and the emerging new distributed electricity generation business model, we find broad support for our arguments and also important nuance. By melding theory, important details of our empirical setting, and intriguing empirical results, we develop greater insights into when and how the other strategic issues to which managers pay attention can shape the translation of managerial attention to a new business model into subsequent organizational action. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.15861 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15861},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {21-39},
  shortjournal = {Organ. Sci.},
  title        = {Attention focus and new opportunities: The moderating role of managerial attention to alternative issues},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). More to lose: The adverse effect of high performance ranking
on employees’ preimplementation attitudes toward the integration of
powerful AI aids. <em>ORSC</em>, <em>36</em>(1), 1–20. (<a
href="https://doi.org/10.1287/orsc.2023.17515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing availability of algorithm-augmented work, algorithm aversion is prevalent among employees, hindering successful implementations of powerful artificial intelligence (AI) aids. Applying a social comparison perspective, this article examines the adverse effect of employees’ high performance ranking on their preimplementation attitudes toward the integration of powerful AI aids within their area of advantage. Five studies, using a weight estimation simulation (Studies 1–3), recall of actual job tasks (Study 4), and a workplace scenario (Study 5), provided consistent causal evidence for this effect by manipulating performance ranking (performance advantage compared with peers versus no advantage). Studies 3–4 revealed that this effect was driven in part by employees’ perceived potential loss of standing compared with peers, a novel social-based mechanism complementing the extant explanation operating via one’s confidence in own (versus AI) ability. Stronger causal evidence for this mechanism was provided in Study 5 using a “moderation-of-process” design. It showed that the adverse effect of high performance ranking on preimplementation AI attitudes was reversed when bolstering the stability of future performance rankings (presumably counteracting one’s concern with potential loss of standing). Finally, pointing to the power of symbolic threats, this adverse effect was evident both in the absence of financial incentives for high performance (Study 1) and in various incentive-based settings (Studies 2–3). Implications for understanding and managing high performers’ aversion toward the integration of powerful algorithmic aids are discussed. Funding: This work was supported by the Coller Foundation. Supplemental Material: The supplemental material is available at https://doi.org/10.1287/orsc.2023.17515 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17515},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Organ. Sci.},
  title        = {More to lose: The adverse effect of high performance ranking on employees’ preimplementation attitudes toward the integration of powerful AI aids},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="serv---5">SERV - 5</h2>
<ul>
<li><details>
<summary>
(2025). Call for papers—service science special issue on
sustainability-driven service innovations. <em>SERV</em>,
<em>17</em>(1), 71–72. (<a
href="https://doi.org/10.1287/serv.2025.cfp.v17.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SERV},
  doi          = {10.1287/serv.2025.cfp.v17.n1},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {71-72},
  shortjournal = {Serv. Sci.},
  title        = {Call for Papers—Service science special issue on sustainability-driven service innovations},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does negative publicity change tourists’ advocacy intention
on online hotel websites? Searching for answers from an online travel
agency study. <em>SERV</em>, <em>17</em>(1), 58–70. (<a
href="https://doi.org/10.1287/serv.2023.0055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Travel virtual communities have reshaped trip planning and experience-sharing amid COVID-19. Postpandemic, information technology in tourism must adjust to changes in traveler behavior, including online booking trends and risk perceptions. Based on the online platform Trivago, the research objectives of this study seek to explore the relationships among electronic service quality, perceived severity of negative publicity, perceived risk, and consumers’ advocacy intention to embrace after the impact of Trivago’s negative publicity. Further, this study aims to explore the moderating effects of electronic service quality and mediating effects of perceived risk. This research collected a total of 468 valid responses and verified the hypothesis by regression analysis and Sobel test. The research results are summarized as follows: the perceived severity of negative publicity negatively affects advocacy intention and positively affects perceived risk, which also negatively affects advocacy intention. These effects are all moderated by e-service quality, which negatively affects perceived risk; perceived risk is the mediator between perceived severity of negative publicity and advocacy intention. The results enrich literature on online hotel websites, offering valuable insights into how managers can effectively use online travel agencies to enhance their online presence and increase revenue. Funding: This study was supported by a grant from the National Science and Technology Council, Taiwan [Grant 111-2410-H-227-009].},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0055},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {58-70},
  shortjournal = {Serv. Sci.},
  title        = {Does negative publicity change tourists’ advocacy intention on online hotel websites? searching for answers from an online travel agency study},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Could auto dealers benefit from vertical media platforms’
encroachment? <em>SERV</em>, <em>17</em>(1), 35–57. (<a
href="https://doi.org/10.1287/serv.2023.0070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating sales leads for auto dealers is the core business to auto vertical media platforms (VMPs). However, in recent years, many auto VMPs have started directly selling vehicles to consumers. Anecdotal evidence suggests that the primary purpose of auto VMPs’ business-to-consumer (B2C) endeavors is not solely B2C profitability, but rather the acquisition of sales data to calculate the leads-to-sales conversion ratio. This ratio is crucial in determining the pricing strategy for sales leads. To understand this, we consider an asymmetric information game in which the auto dealer possesses information about the leads-to-sales conversion ratio, whereas the VMP does not. We find that, when the VMP solely focuses on profiting from the sales lead business, if the market potential is large, the auto dealer prefers the cost-per-lead scheme, whereas the VMP always tends to the cost-per-sale scheme. However, when the VMP encroaches on the consumer market as a reseller, this divergence in contract-type preference can potentially be resolved. We recognize that VMPs need to conduct a comprehensive assessment of factors such as market potential, fixed cost related to market encroachment, and the correlation between market uncertainty and leads-to-sales conversion ratio when encroachment. Moreover, the encroachment of the VMP has the potential to create a win–win outcome. It is essential to highlight that engaging in B2C business enables the VMP to expand its market sample, thereby enhancing the value of leveraging information technology to optimize the leads-to-sales ratio and achieve superior performance. Funding: This work is partly supported by the Major program of the National Social Science Foundation of China [Grant 22&amp;ZD082], the National Natural Science Foundation of China [Grants 72321001, 72371104, 72071080], and the Guangdong Basic and Applied Basic Research Foundation, China [Grants 2023A1515012529, 2023A1515012435, 2024A1515010940].},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0070},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {35-57},
  shortjournal = {Serv. Sci.},
  title        = {Could auto dealers benefit from vertical media platforms’ encroachment?},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Earnings pressure and corporate social responsibility
impression management. <em>SERV</em>, <em>17</em>(1), 18–34. (<a
href="https://doi.org/10.1287/serv.2023.0052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive research has investigated the buffering effect of corporate social responsibility (CSR) when firms have poor financial performance. However, few studies have examined the possible consequences of this effect. By analyzing data on listed firms in China from 2010 to 2018, this study investigates the buffering effect of CSR under conditions of earnings pressure, and then discusses its impact on CSR disclosure impression management behavior. The results show that CSR can buffer the negative impact of earnings pressure on stock prices, chief executive officer stability, and top management team stability. This buffering effect is mainly exerted by technical CSR. Moreover, earnings pressure leads to CSR disclosure impression management without resulting in improved CSR performance. These findings have implications for cultivating CSR buffering awareness among firms and managers. The results also have practical implications for future research on the antecedents and motivations of CSR disclosure impression management. Funding: This work was supported by the National Natural Science Foundation of China [Grant 72372104].},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0052},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {18-34},
  shortjournal = {Serv. Sci.},
  title        = {Earnings pressure and corporate social responsibility impression management},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic exception points for fair liver allocation.
<em>SERV</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1287/serv.2023.0092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are disparities in access to livers based on transplant patients’ height, which disproportionately affects Hispanics, Asians, and women (across all ethnicities), because short patients can receive transplants from a smaller pool of available deceased donors for medical reasons. Reduced likelihood of transplantation leads to higher mortality rates and longer waiting times. We analyze fairness within the current U.S. liver allocation system where patients receive priority dynamically, based on their model for end-stage liver disease (MELD) scores, which reflect the severity of liver disease. We propose a simple adjustment, providing additional (exception) points based on height and MELD score, that can be easily implemented in practice, which materially reduces the disparity without sacrificing overall efficiency. We model the liver allocation system as a multiclass fluid model of overloaded queues with heterogeneous servers. We impose explicit equity constraints for all static patient classes, that is, height. We characterize the optimal solution under the objective of minimizing pretransplant mortality. The discretized version of the optimal policy is numerically solved using estimates from clinical data and a detailed simulation study demonstrates its effectiveness. The optimal policy, called the equity adjusted mortality risk policy, advocates ranking patients based on their short-term mortality risk adjusted for equity among height classes. Interpretation of the shadow prices of equity constraints in the optimal control problem as MELD exception points is novel in the transplant context since they can be seamlessly mapped into the existing system. Our simulations show that for women, the disparity can be almost completely eliminated. Hispanics and Asians greatly benefit from receiving these MELD exception points also. Our work provides a remedy to reduce the disparities in access to liver transplantation within the MELD-based allocation. Our approach can help the on-going analysis of the continuous distribution model for livers because it also considers aspects of candidate biology, notably height and body surface area. Funding: M. Akan was supported by the National Science Foundation [Grant CMMI-1334194] and the Carnegie Mellon University (CMU) [Onetto Fellowship in Operations Management]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/serv.2023.0092 .},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0092},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Serv. Sci.},
  title        = {Dynamic exception points for fair liver allocation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="stsc---3">STSC - 3</h2>
<ul>
<li><details>
<summary>
(2025). Innovation disclosures and the design of technology
acquisition contracts: Evidence from the american inventors protection
act. <em>STSC</em>, <em>10</em>(1), 68–92. (<a
href="https://doi.org/10.1287/stsc.2022.0069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material adverse change (MAC) clauses and contingent earnouts are important contractual mechanisms used to protect acquirers from the risk of adverse selection. Yet, the extant literature has not sufficiently explored the antecedents of their use, in particular within the context of technology acquisitions. In this study, we take advantage of the passage of the American Inventors Protection Act (AIPA), which disseminated information through the publication of patent applications, to explore the impact of innovation disclosures on the design of technology acquisition contracts. Consistent with the view that an increase in the availability of information related to the broader technological landscape reduces the need for contractual protections in acquisition contracts, our analysis demonstrates that deals disproportionately affected by AIPA have less expansive MAC clauses and are less likely to feature contingent earnouts. These results provide new evidence linking the use of MAC clauses and earnouts with acquisitions subject to information frictions.},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2022.0069},
  journal      = {Strategy Science},
  month        = {3},
  number       = {1},
  pages        = {68-92},
  shortjournal = {Strat. Sci.},
  title        = {Innovation disclosures and the design of technology acquisition contracts: Evidence from the american inventors protection act},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource redeployment and the pursuit of the new best use:
Economic logic and organizational challenges. <em>STSC</em>,
<em>10</em>(1), 32–47. (<a
href="https://doi.org/10.1287/stsc.2022.0105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shifting economic circumstances may suggest some degree of resource redeployment of capacity-constrained capabilities. However, a separate question remains as to how corporate actors operate within the space of this economic logic. While prior literature on diversification has tended to highlight the fungibility of resources as a basis for their redeployment, we point to an additional consideration of the complementarity of resources that makes allocation within the firm attractive, what we term a “new best use.” Further, we point to a potential tension as the economic logic suggested by this property of complementarity, and more generally interdependence with the firm’s other resources and activities, may make assessing what constitutes what we term the “new best use” of a resource challenging. The latent fungibility of a resource may differ from the range of reliable assessment of relative value creation. The allocation of resources and capabilities across a firm of any scale or scope is generally guided by the organizational and task structure of the enterprise. Building on these arguments, we consider two classes of factors that might impact the realization of the latent opportunities of resource redeployment. One is the “pipes” of organizational structures and grouping of business units and allocation of decision rights of specific managers to those units, and the other is the “prisms” of the various metrics of performance and value creation that may make resource use across different domains of the business more or less comparable.},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2022.0105},
  journal      = {Strategy Science},
  month        = {3},
  number       = {1},
  pages        = {32-47},
  shortjournal = {Strat. Sci.},
  title        = {Resource redeployment and the pursuit of the new best use: Economic logic and organizational challenges},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How demand shocks “jumpstart” technological ecosystems and
commercialization: Evidence from the global electric vehicle industry.
<em>STSC</em>, <em>10</em>(1), 1–31. (<a
href="https://doi.org/10.1287/stsc.2022.0075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine how exogenous demand shocks overcome ecosystem bottlenecks in the commercialization of an emergent technology. We argue that demand shocks that spur new technology adoption by niche users pull “hub” firms into country technology markets, despite ecosystem bottlenecks, thereby serving to “jumpstart” the process of ecosystem development and technology commercialization. By analyzing global electric vehicle markets over the period 2008–2017, we find that extreme weather events such as abnormal heat-related events spur adoption of electric vehicles by end users, thereby propelling automotive or hub firms’ entry into country technology markets, and the subsequent shift of their electric vehicle product portfolios toward the more radical version of the technology. Notably, such demand shocks propel firms’ commercialization strategy, despite ecosystem bottlenecks such as the lack of regulatory and economic inducements for adoption, relative absence of complements, and product market differences. After entry, entrant firms’ electric vehicle product portfolios transition from hybrids toward radical technology products and investments in complements, albeit contingent on their competitive market position in the legacy technology. We discuss the implications of these findings concerning the uptake of demand shocks, and their robustness to modeling choices, technological generations across extended timeframes, potential mediating forces, and boundary conditions owing to firm and country-market heterogeneity. Supplemental Material: The online appendices are available at https://doi.org/10.1287/stsc.2022.0075 .},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2022.0075},
  journal      = {Strategy Science},
  month        = {3},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Strat. Sci.},
  title        = {How demand shocks “Jumpstart” technological ecosystems and commercialization: Evidence from the global electric vehicle industry},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="stsy---3">STSY - 3</h2>
<ul>
<li><details>
<summary>
(2025). Normal approximation of random gaussian neural networks.
<em>STSY</em>, <em>15</em>(1), 88–110. (<a
href="https://doi.org/10.1287/stsy.2023.0033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide explicit upper bounds on some distances between the (law of the) output of a random Gaussian neural network and (the law of) a random Gaussian vector. Our main results concern deep random Gaussian neural networks with a rather general activation function. The upper bounds show how the widths of the layers, the activation function, and other architecture parameters affect the Gaussian approximation of the output. Our techniques, relying on Stein’s method and integration by parts formulas for the Gaussian law, yield estimates on distances that are indeed integral probability metrics and include the convex distance. This latter metric is defined by testing against indicator functions of measurable convex sets and so allows for accurate estimates of the probability that the output is localized in some region of the space, which is an aspect of a significant interest both from a practitioner’s and a theorist’s perspective. We illustrated our results by some numerical examples. Funding: This research was supported by the European Union’s Horizon 2020 research project WARIFA under grant agreement no. 101017385, by the PRIN project 2022 “Variational Analysis of Complex Systems in Materials Science, Physics and Biology” (CUP B53D23009290006), and by the INdAM project “Modelli ed Algoritmi per dati ad elevata dimensionalità” (CUP E53C23001670001).},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2023.0033},
  journal      = {Stochastic Systems},
  month        = {3},
  number       = {1},
  pages        = {88-110},
  shortjournal = {Stoch. Syst.},
  title        = {Normal approximation of random gaussian neural networks},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast exact simulation of the first passage of a tempered
stable subordinator across a non-increasing function. <em>STSY</em>,
<em>15</em>(1), 50–87. (<a
href="https://doi.org/10.1287/stsy.2023.0014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a fast exact algorithm for the simulation of the first-passage time, jointly with the undershoot and overshoot, of a tempered stable subordinator over an arbitrary, nonincreasing, absolutely continuous function. We prove that the running time of our algorithm has finite exponential moments and provide bounds on its expected running time, with explicit dependence on the characteristics of the process and the initial value of the function. The expected running time grows most cubically in the stability parameter (as it approaches either 0 or 1) and is linear in the tempering parameter and the initial value of the function. Numerical performance, based on the implementation in the dedicated GitHub repository, exhibits a good agreement with our theoretical bounds. We provide numerical examples to illustrate the performance of our algorithm in Monte Carlo estimation. Funding: J. I. González Cázares and A. Mijatović are supported by the EPSRC Grant EP/V009478/1 and by The Alan Turing Institute under the EPSRC grant EP/X03870X/1. A. Mijatović is also supported by the EPSRC grant EP/W006227/1. F. Lin is funded by The China Scholarship Council and The University of Warwick PhD scholarship.},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2023.0014},
  journal      = {Stochastic Systems},
  month        = {3},
  number       = {1},
  pages        = {50-87},
  shortjournal = {Stoch. Syst.},
  title        = {Fast exact simulation of the first passage of a tempered stable subordinator across a non-increasing function},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The BAR approach for multiclass queueing networks with SBP
service policies. <em>STSY</em>, <em>15</em>(1), 1–49. (<a
href="https://doi.org/10.1287/stsy.2023.0011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The basic adjoint relationship (BAR) approach is an analysis technique based on the stationary equation of a Markov process. This approach was introduced to study heavy-traffic, steady-state convergence of generalized Jackson networks in which each service station has a single job class. We extend it to multiclass queueing networks operating under static-buffer-priority (SBP) service disciplines. Our extension makes a connection with Palm distributions that allows one to attack a difficulty arising from queue-length truncation, which appears to be unavoidable in the multiclass setting. For multiclass queueing networks operating under SBP service disciplines, our BAR approach provides an alternative to the “interchange of limits” approach that has dominated the literature in the last twenty years. The BAR approach can produce sharp results and allows one to establish steady-state convergence under three additional conditions: stability, state space collapse (SSC) and a certain matrix being “tight.” These three conditions do not appear to depend on the interarrival and service-time distributions beyond their means, and their verification can be studied as three separate modules. In particular, they can be studied in a simpler, continuous-time Markov chain setting when all distributions are exponential. As an example, these three conditions are shown to hold in reentrant lines operating under last-buffer-first-serve discipline. In a two-station, five-class reentrant line, under the heavy-traffic condition, the tight-matrix condition implies both the stability condition and the SSC condition. Whether such a relationship holds generally is an open problem.},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2023.0011},
  journal      = {Stochastic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-49},
  shortjournal = {Stoch. Syst.},
  title        = {The BAR approach for multiclass queueing networks with SBP service policies},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="trsc---12">TRSC - 12</h2>
<ul>
<li><details>
<summary>
(2025). The restaurant meal delivery problem with ghost kitchens.
<em>TRSC</em>, <em>59</em>(2), 433–450. (<a
href="https://doi.org/10.1287/trsc.2024.0510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restaurant meal delivery has been rapidly growing in the last few years. The main operational challenges are the temporally and spatially dispersed stochastic demand that arrives from customers all over town as well as the customers’ expectation of timely and fresh delivery. To overcome these challenges, a new business concept emerged: ghost kitchens. This concept proposes synchronized food preparation of several restaurants in a central facility. Ghost kitchens can bring several advantages, such as fresher food because of the synchronization of food preparation and delivery and less delay because of the consolidated delivery of orders. Exploiting these advantages requires effective operational strategies for the dynamic scheduling of food preparation and delivery. The goal of this paper is providing these strategies and investigating the value of ghost kitchens. We model the problem as a sequential decision process. For the complex decision space of scheduling order preparations, consolidating orders to trips, and scheduling trip departures, we propose a large neighborhood search (LNS) procedure based on partial decisions and driven by analytical properties. Within the LNS, decisions are evaluated via a value function approximation, enabling anticipatory and real-time decision making. In a comprehensive computational study, we demonstrate the effectiveness of our method compared with benchmark policies and highlight the advantages of ghost kitchens compared with conventional meal delivery. Funding: G. Neria’s research is partially supported by the Israeli Smart Transportation Research Center, the Council for Higher Education in Israel, and the Shlomo Shmeltzer Institute for Smart Transportation at Tel Aviv University. F. D. Hildebrandt’s research is funded by the Deutsche Forschungsgemeinschaft (DFG) German Research Foundation [Grant 413322447]. M. Tzur’s research is partially supported by the Israeli Smart Transportation Research Center. M. W. Ulmer’s work is funded by the DFG Emmy Noether Programme [Grant 444657906]. We gratefully acknowledge their support. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2024.0510 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0510},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {433-450},
  shortjournal = {Trans. Sci.},
  title        = {The restaurant meal delivery problem with ghost kitchens},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiday user equilibrium with strategic commuters.
<em>TRSC</em>, <em>59</em>(2), 413–432. (<a
href="https://doi.org/10.1287/trsc.2023.0488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of connected and automated mobility, commuters possess strong computation power, enabling them to strategically make sequential travel choices over a planning horizon. This paper investigates the multiday traffic patterns that arise from such decision-making behavior. In doing so, we frame the commute problem as a mean-field Markov game and introduce a novel concept of multiday user equilibrium to capture the steady state of commuters’ interactions. The proposed model is general and can be tailored to various travel choices, such as route or departure time. We explore a range of properties of the multiday user equilibrium under mild conditions. The study reveals the fingerprint of user inertia on network flow patterns, causing between-day variations even at a steady state. Furthermore, our analysis establishes critical connections between the multiday user equilibrium and conventional Wardrop equilibrium. Funding: This work was supported by the National Science Foundation [Grants CMMI-1854684, CMMI-1904575, CMMI-2233057, and CMMI-2240981]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0488 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0488},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {413-432},
  shortjournal = {Trans. Sci.},
  title        = {Multiday user equilibrium with strategic commuters},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing the liner shipping network of tomorrow powered by
alternative fuels. <em>TRSC</em>, <em>59</em>(2), 391–412. (<a
href="https://doi.org/10.1287/trsc.2023.0143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The liner shipping industry is undergoing an extensive decarbonization process to reduce its 275 million tons of CO 2 emissions as of 2018. In this process, the long-term solution is the introduction of new alternative maritime fuels. The introduction of alternative fuels presents a great set of unknowns. Among these are the strategic concerns regarding sourcing of alternative fuels and, operationally, how the new fuels might affect the network of shipping routes. We propose a problem formulation that integrates fuel supply/demand into the liner shipping network design problem. Here, we present a model to determine the production sites and distribution of new alternative fuels—we consider methanol and ammonia. For the network design problem, we apply an adaptive large neighborhood search combined with a delayed column generation process. In addition, we wish to test the effect of designing a robust network under uncertain demand conditions because of the problem’s strategic nature and importance. Therefore, our proposed solution method will have a deterministic and stochastic setup when we apply it to the second-largest multihub instance, WorldSmall , known from LINER-LIB. In the deterministic setting, our proposed solution method finds a new best solution to three instances from LINER-LIB. For the main considered WorldSmall instance, we even noticed a new best solution in all our tested fuel settings. In addition, we note a profit drop of 7.2% between a bunker-powered and pure alternative fuel–powered network. The selected alternative fuel production sites favor a proximity to European ports and have a heavy reliance on wind turbines. The stochastic results clearly showed that the found networks were much more resilient to the demand changes. Neglecting the perspective of uncertain demand leads to highly fluctuating profits. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0143 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0143},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {391-412},
  shortjournal = {Trans. Sci.},
  title        = {Designing the liner shipping network of tomorrow powered by alternative fuels},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The stochastic dynamic postdisaster inventory allocation
problem with trucks and UAVs. <em>TRSC</em>, <em>59</em>(2), 360–390.
(<a href="https://doi.org/10.1287/trsc.2023.0438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humanitarian logistics operations face increasing difficulties due to rising demands for aid in disaster areas. This paper investigates the dynamic allocation of scarce relief supplies across multiple affected districts over time. It introduces a novel stochastic dynamic postdisaster inventory allocation problem (SDPDIAP) with trucks and unmanned aerial vehicles (UAVs) delivering relief goods under uncertain supply and demand. The relevance of this humanitarian logistics problem lies in the importance of considering the intertemporal social impact of deliveries. We achieve this by considering social costs (transportation and deprivation costs) when allocating scarce supplies. Furthermore, we consider the inherent uncertainties of disaster areas and the potential use of cargo UAVs to enhance operational efficiency. This study proposes two anticipatory solution methods based on approximate dynamic programming, specifically decomposed linear value function approximation (DL-VFA) and neural network value function approximation (NN-VFA) to effectively manage uncertainties in the dynamic allocation process. We compare DL-VFA and NN-VFA with various state-of-the-art methods (e.g., exact reoptimization and proximal policy optimization) and results show a 6%–8% improvement compared with the best benchmarks. NN-VFA provides the best performance and captures nonlinearities in the problem, whereas DL-VFA shows excellent scalability against a minor performance loss. From a practical standpoint, the experiments reveal that consideration of social costs results in improved allocation of scarce supplies both across affected districts and over time. Finally, results show that deploying UAVs can play a crucial role in the allocation of relief goods, especially in the first stages after a disaster. The use of UAVs reduces transportation and deprivation costs together by 16%–20% and reduces maximum deprivation times by 19%–40% while maintaining similar levels of demand coverage, showcasing efficient and effective operations. History: This paper has been accepted for the Transportation Science Special Issue on TSL Conference 2023.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0438},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {360-390},
  shortjournal = {Trans. Sci.},
  title        = {The stochastic dynamic postdisaster inventory allocation problem with trucks and UAVs},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CARMA: Fair and efficient bottleneck congestion management
via nontradable karma credits. <em>TRSC</em>, <em>59</em>(2), 340–359.
(<a href="https://doi.org/10.1287/trsc.2023.0323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a nonmonetary traffic demand management scheme, named CARMA , as a fair solution to the morning commute congestion. We consider heterogeneous commuters traveling through a single bottleneck that differ in both the desired arrival time and value of time (VOT). We consider a generalized notion of VOT by allowing it to vary dynamically on each day (e.g., according to trip purpose and urgency) rather than being a static characteristic of each individual. In our CARMA scheme, the bottleneck is divided into a fast lane that is kept in free flow and a slow lane that is subject to congestion. We introduce a nontradable mobility credit, named karma , that is used by commuters to bid for access to the fast lane. Commuters who get outbid or do not participate in the CARMA scheme instead use the slow lane. At the end of each day, karma collected from the bidders is redistributed, and the process repeats day by day. We model the collective commuter behaviors under CARMA as a dynamic population game (DPG), in which a stationary Nash equilibrium (SNE) is guaranteed to exist. Unlike existing monetary schemes, CARMA is demonstrated, both analytically and numerically, to achieve (a) an equitable traffic assignment with respect to heterogeneous income classes and (b) a strong Pareto improvement in the long-term average travel disutility with respect to no policy intervention. With extensive numerical analysis, we show that CARMA is able to retain the same congestion reduction as an optimal monetary tolling scheme under uniform karma redistribution and even outperform tolling under a well-designed redistribution scheme. We also highlight the privacy-preserving feature of CARMA , that is, its ability to tailor to the private preferences of commuters without centrally collecting the information. History: This paper has been accepted for the Transportation Science Special Issue on TSL Conference 2023. Funding: This work was supported by NCCR Automation, a National Centre of Competence in Research, funded by the Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung [Grant 180545]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/trsc.2023.0323 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0323},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {340-359},
  shortjournal = {Trans. Sci.},
  title        = {CARMA: Fair and efficient bottleneck congestion management via nontradable karma credits},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Genetic algorithms with neural cost predictor for solving
hierarchical vehicle routing problems. <em>TRSC</em>, <em>59</em>(2),
322–339. (<a href="https://doi.org/10.1287/trsc.2023.0369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When vehicle routing decisions are intertwined with higher-level decisions, the resulting optimization problems pose significant challenges for computation. Examples are the multi-depot vehicle routing problem (MDVRP), where customers are assigned to depots before delivery, and the capacitated location routing problem (CLRP), where the locations of depots should be determined first. A simple and straightforward approach for such hierarchical problems would be to separate the higher-level decisions from the complicated vehicle routing decisions. For each higher-level decision candidate, we may evaluate the underlying vehicle routing problems to assess the candidate. As this approach requires solving vehicle routing problems multiple times, it has been regarded as impractical in most cases. We propose a novel deep learning-based approach called the genetic algorithm with neural cost predictor to tackle the challenge and simplify algorithm developments. For each higher-level decision candidate, we predict the objective function values of the underlying vehicle routing problems using a pretrained graph neural network without actually solving the routing problems. In particular, our proposed neural network learns the objective values of the HGS-CVRP open-source package that solves capacitated vehicle routing problems. Our numerical experiments show that this simplified approach is effective and efficient in generating high-quality solutions for both MDVRP and CLRP and that it has the potential to expedite algorithm developments for complicated hierarchical problems. We provide computational results evaluated in the standard benchmark instances used in the literature. History: This paper has been accepted for the Transportation Science Special Issue on TSL Conference 2023. Funding: This research was funded by the National Research Foundation of Korea [Grant RS-2023-00259550]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0369 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0369},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {322-339},
  shortjournal = {Trans. Sci.},
  title        = {Genetic algorithms with neural cost predictor for solving hierarchical vehicle routing problems},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse optimization for routing problems. <em>TRSC</em>,
<em>59</em>(2), 301–321. (<a
href="https://doi.org/10.1287/trsc.2023.0241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for learning decision makers’ behavior in routing problems using inverse optimization (IO). The IO framework falls into the supervised learning category and builds on the premise that the target behavior is an optimizer of an unknown cost function. This cost function is to be learned through historical data, and in the context of routing problems, can be interpreted as the routing preferences of the decision makers. In this view, the main contributions of this study are to propose an IO methodology with a hypothesis function, loss function, and stochastic first-order algorithm tailored to routing problems. We further test our IO approach in the Amazon Last Mile Routing Research Challenge, where the goal is to learn models that replicate the routing preferences of human drivers, using thousands of real-world routing examples. Our final IO-learned routing model achieves a score that ranks second compared with the 48 models that qualified for the final round of the challenge. Our examples and results showcase the flexibility and real-world potential of the proposed IO methodology to learn from decision-makers’ decisions in routing problems. History: This paper has been accepted for the Transportation Science Special Issue on TSL Conference 2023. Funding: This work was supported by the European Research Council [TRUST-949796].},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0241},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {301-321},
  shortjournal = {Trans. Sci.},
  title        = {Inverse optimization for routing problems},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pricing and demand management for integrated same-day and
next-day delivery systems. <em>TRSC</em>, <em>59</em>(2), 279–300. (<a
href="https://doi.org/10.1287/trsc.2023.0381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a system in which a common delivery fleet provides service to both same-day delivery (SDD) and next-day delivery (NDD) orders placed by e-retail customers who are sensitive to delivery prices. We develop a model of the system and optimize with respect to two separate objectives. First, empirical research suggests that fulfilling e-retail orders ahead of promised delivery days increases a firm’s long-run market share. Motivated by this phenomenon, we optimize for customer satisfaction by maximizing the quantity of NDD orders fulfilled one day early given fixed prices. Next, we optimize for total profit; we optimize for a single SDD price, and we then set SDD prices in a two-level scheme with discounts for early-ordering customers. Our analysis relies on continuous approximation techniques to capture the interplay between NDD and SDD orders and particularly the effect one day’s operations have on the next, a novel modeling component not present in SDD-only models; a key technical result is establishing the model’s convergence to a steady state using dynamical systems theory. We derive structural insights and efficient algorithms for both objectives. In particular, we show that, under certain conditions, the total profit is a piecewise-convex function with polynomially many breakpoints that can be efficiently enumerated. In a case study set in metropolitan Denver, Colorado, approximately 10% of NDD orders can be fulfilled one day early at optimality, and profit is increased by 1% to 3% in a two-level pricing scheme versus a one-level scheme. We conduct operational simulations for validation of solutions and analysis of initial conditions. History: This paper has been accepted for the Transportation Science Special Issue on TSL Conference 2023. Funding: This work was supported by the National Science Foundation [Grant DGE-1650044]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0381 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0381},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {279-300},
  shortjournal = {Trans. Sci.},
  title        = {Pricing and demand management for integrated same-day and next-day delivery systems},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning dynamic selection and pricing of out-of-home
deliveries. <em>TRSC</em>, <em>59</em>(2), 250–278. (<a
href="https://doi.org/10.1287/trsc.2023.0434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Home delivery failures, traffic congestion, and relatively large handling times have a negative impact on the profitability of last-mile logistics. A potential solution is the delivery to parcel lockers or parcel shops, denoted by out-of-home (OOH) delivery. In the academic literature, models for OOH delivery are so far limited to static settings, contrasting with the sequential nature of the problem. We model the sequential decision-making problem of which OOH location to offer against what incentive for each incoming customer, taking into account future customer arrivals and choices. We propose dynamic selection and pricing of OOH (DSPO), an algorithmic pipeline that uses a novel spatial-temporal state encoding as input to a convolutional neural network. We demonstrate the performance of our method by benchmarking it against two state-of-the-art approaches. Our extensive numerical study, guided by real-world data, reveals that DSPO can save 19.9 percentage points (%pt) in costs compared with a situation without OOH locations, 7%pt compared with a static selection and pricing policy, and 3.8%pt compared with a state-of-the-art demand management benchmark. We provide comprehensive insights into the complex interplay between OOH delivery dynamics and customer behavior influenced by pricing strategies. The implications of our findings suggest that practitioners adopt dynamic selection and pricing policies. History: This paper has been accepted for the Transportation Science special issue on TSL Conference 2023. Funding: This work was supported by TKI DINALOG.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0434},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {250-278},
  shortjournal = {Trans. Sci.},
  title        = {Learning dynamic selection and pricing of out-of-home deliveries},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Branch and price for the stochastic traveling salesman
problem with generalized latency. <em>TRSC</em>, <em>59</em>(2),
229–249. (<a href="https://doi.org/10.1287/trsc.2023.0417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an extension of the symmetric traveling salesman problem with generalized latency that explicitly models uncertainty. The stochastic traveling salesman problem with generalized latency (STSP-GL) aims to choose a subset of nodes of an undirected graph and determines a Hamiltonian tour among those nodes, minimizing an objective function that is a weighted combination of route design and passenger routing costs. These nodes are selected to ensure that a predefined percentage of uncertain passenger demand is served with a given probability. We formulate the STSP-GL as a stochastic program and propose a branch-and-price algorithm for solving its deterministic equivalent. We also develop a local search approach with which we improve the performance of the branch-and-price approach. We assess the efficiency of the proposed methods on a set of instances from the literature. We demonstrate that the proposed methods outperform a known benchmark, improving upper bounds by up to 85% and lower bounds by up to 55%. Finally, we show that solutions of the stochastic model are both more cost-effective and robust than those of the deterministic model. History: This paper has been accepted for the Transportation Science Special Issue on TSL Conference 2023. Funding: This work was supported by the Bundesministerium für Bildung und Forschung [Grant 03ZU1105FA]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0417 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0417},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {229-249},
  shortjournal = {Trans. Sci.},
  title        = {Branch and price for the stochastic traveling salesman problem with generalized latency},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact two-step benders decomposition for the time window
assignment traveling salesperson problem. <em>TRSC</em>, <em>59</em>(2),
210–228. (<a href="https://doi.org/10.1287/trsc.2024.0750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-day delivery logistics services are redefining the industry by increasingly focusing on customer service. Each logistics service provider’s challenge is jointly optimizing time window assignment and vehicle routing for such next-day delivery services. To do so in a cost-efficient and customer-centric fashion, real-life uncertainty, such as stochastic travel times, needs to be incorporated into the optimization process. This paper focuses on the canonical optimization problem within this context: the time window assignment traveling salesperson problem with stochastic travel times (TWATSP-ST). It belongs to two-stage, stochastic, mixed-integer programming problems with continuous recourse. We introduce two-step Benders decomposition with scenario clustering (TBDS) as an exact solution methodology for solving such stochastic programs. The method utilizes a new two-step decomposition along the binary and continuous first stage decisions and introduces a new scenario-retention strategy that combines and generalizes state-of-the-art Benders approaches and scenario-clustering techniques. Extensive experiments show that TBDS is superior to state-of-the-art approaches in the literature. It solves TWATSP-ST instances with up to 25 customers to optimality. It provides better lower and upper bounds that lead to faster convergence than existing state-of-the-art methods. We use TBDS to analyze the structure of the optimal solutions. By increasing routing costs only slightly, customer service can be improved tremendously driven by smartly alternating between high- and low-variance travel arcs to reduce the impact of delay propagation throughout the executed vehicle route. Funding: A. H. Schrotenboer has received support from the Dutch Science Foundation [Grant VI.Veni.211E.043]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2024.0750 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0750},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {210-228},
  shortjournal = {Trans. Sci.},
  title        = {Exact two-step benders decomposition for the time window assignment traveling salesperson problem},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on TSL conference 2023. <em>TRSC</em>,
<em>59</em>(2), 207–209. (<a
href="https://doi.org/10.1287/trsc.2023.intro.v59.n2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.intro.v59.n2},
  journal      = {Transportation Science},
  month        = {3-4},
  number       = {2},
  pages        = {207-209},
  shortjournal = {Trans. Sci.},
  title        = {Special issue on TSL conference 2023},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
