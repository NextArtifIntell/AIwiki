<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SW_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sw---20">SW - 20</h2>
<ul>
<li><details>
<summary>
(2025). Temporal relevance for representing learning over temporal
knowledge graphs. <em>SW</em>, <em>15</em>(6), 2695–2711. (<a
href="https://doi.org/10.3233/SW-243699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning for link prediction is one of the leading approaches to deal with incompleteness problem of real world knowledge graphs. Such methods are often called knowledge graph embedding models which represent entities and relationships in knowledge graphs in continuous vector spaces. By doing this, semantic relationships and patterns can be captured in the form of compact vectors. In temporal knowledge graphs, the connection of temporal and relational information is crucial for representing facts accurately. Relations provide the semantic context for facts, while timestamps indicate the temporal validity of facts. The importance of time is different for the semantics of different facts. Some relations in some temporal facts are time-insensitive, while others are highly time-dependent. However, existing embedding models often overlook the time sensitivity of different facts in temporal knowledge graphs. These models tend to focus on effectively representing connection between individual components of quadruples, consequently capturing only a fraction of the overall knowledge. Ignoring importance of temporal properties reduces the ability of temporal knowledge graph embedding models in accurately capturing these characteristics. To address these challenges, we propose a novel embedding model based on temporal relevance, which can effectively capture the time sensitivity of semantics and better represent facts. This model operates within a complex space with real and imaginary parts to effectively embed temporal knowledge graphs. Specifically, the real part of the final embedding of our proposed model captures semantic characteristic with temporal sensitivity by learning the relational information and temporal information through transformation and attention mechanism. Simultaneously, the imaginary part of the embeddings learns the connections between different elements in the fact without predefined weights. Our approach is evaluated through extensive experiments on the link prediction task, where it majorly outperforms state-of-the-art models. The proposed model also demonstrates remarkable effectiveness in capturing the complexities of temporal knowledge graphs.},
  archive      = {J_SW},
  author       = {Song, Bowen and Amouzouvi, Kossi and Xu, Chengjin and Wang, Maocai and Lehmann, Jens and Vahdati, Sahar},
  doi          = {10.3233/SW-243699},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2695-2711},
  shortjournal = {Semantic Web},
  title        = {Temporal relevance for representing learning over temporal knowledge graphs},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Background knowledge in ontology matching: A survey.
<em>SW</em>, <em>15</em>(6), 2639–2693. (<a
href="https://doi.org/10.3233/SW-223085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology matching is an integral part for establishing semantic interoperability. One of the main challenges within the ontology matching operation is semantic heterogeneity, i.e. modeling differences between the two ontologies that are to be integrated. The semantics within most ontologies or schemas are, however, typically incomplete because they are designed within a certain context which is not explicitly modeled. Therefore, external background knowledge plays a major role in the task of (semi-) automated ontology and schema matching. In this survey, we introduce the reader to the general ontology matching problem. We review the background knowledge sources as well as the approaches applied to make use of external knowledge. Our survey covers all ontology matching systems that have been presented within the years 2004–2021 at a well-known ontology matching competition together with systematically selected publications in the research field. We present a classification system for external background knowledge, concept linking strategies, as well as for background knowledge exploitation approaches. We provide extensive examples and classify all ontology matching systems under review in a resource/strategy matrix obtained by coalescing the two classification systems. Lastly, we outline interesting and yet underexplored research directions of applying external knowledge within the ontology matching process.},
  archive      = {J_SW},
  author       = {Portisch, Jan and Hladik, Michael and Paulheim, Heiko},
  doi          = {10.3233/SW-223085},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2639-2693},
  shortjournal = {Semantic Web},
  title        = {Background knowledge in ontology matching: A survey},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTab4D: Semantic annotation of tabular data with DBpedia.
<em>SW</em>, <em>15</em>(6), 2613–2637. (<a
href="https://doi.org/10.3233/SW-223098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic annotation of tabular data is the process of matching table elements with knowledge graphs. As a result, the table contents could be interpreted or inferred using knowledge graph concepts, enabling them to be useful in downstream applications such as data analytics and management. Nevertheless, semantic annotation tasks are challenging due to insufficient tabular data descriptions, heterogeneous schema, and vocabulary issues. This paper presents an automatic semantic annotation system for tabular data, called MTab4D, to generate annotations with DBpedia in three annotation tasks: 1) matching table cells to entities, 2) matching columns to entity types, and 3) matching pairs of columns to properties. In particular, we propose an annotation pipeline that combines multiple matching signals from different table elements to address schema heterogeneity, data ambiguity, and noisiness. Additionally, this paper provides insightful analysis and extra resources on benchmarking semantic annotation with knowledge graphs. Experimental results on the original and adapted datasets of the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab 2019) show that our system achieves an impressive performance for the three annotation tasks. MTab4D’s repository is publicly available at https://github.com/phucty/mtab4dbpedia .},
  archive      = {J_SW},
  author       = {Nguyen, Phuc and Kertkeidkachorn, Natthawut and Ichise, Ryutaro and Takeda, Hideaki},
  doi          = {10.3233/SW-223098},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2613-2637},
  shortjournal = {Semantic Web},
  title        = {MTab4D: Semantic annotation of tabular data with DBpedia},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The community solid server: Supporting research &amp;
development in an evolving ecosystem. <em>SW</em>, <em>15</em>(6),
2597–2611. (<a href="https://doi.org/10.3233/SW-243726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Solid project aims to empower people with control over their own data through the separation of data, identity, and applications. The goal is an environment with clear interoperability between all servers and clients that adhere to the specification. Solid is a standards-driven way to extend the Linked Data vision from public to private data, and everything in between. Multiple implementations of the Solid Protocol exist, but due to the evolving nature of the ecosystem, there is a strong need for an implementation that enables qualitative and quantitative research into new features and allows developers to quickly set up varying development environments. To meet these demands, we created the Community Solid Server, a modular server that can be configured to suit the needs of researchers and developers. In this article, we provide an overview of the server architecture and how it is positioned within the Solid ecosystem. The server supports many orthogonal feature combinations on axes such as authorization, authentication, and data storage. The Community Solid Server comes with several predefined configurations that allow researchers and developers to quickly set up servers with different content and backends, and can easily be modified to change many of its features. The server will help evolve the specification, and support further research into Solid and its possibilities.},
  archive      = {J_SW},
  author       = {Van Herwegen, Joachim and Verborgh, Ruben},
  doi          = {10.3233/SW-243726},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2597-2611},
  shortjournal = {Semantic Web},
  title        = {The community solid server: Supporting research &amp;amp; development in an evolving ecosystem},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRSC: From PG to RDF and back, using schemas. <em>SW</em>,
<em>15</em>(6), 2555–2595. (<a
href="https://doi.org/10.3233/SW-243675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Property graphs (PG) and RDF graphs are two popular database graph models, but they are not interoperable: data modeled in PG cannot be directly integrated with other data modeled in RDF. This lack of interoperability also impedes the use of the tools of one model when data are modeled in the other. In this paper, we propose PRSC, a configurable conversion to transform a PG into an RDF graph. This conversion relies on PG schemas and user-defined mappings called PRSC contexts. We also formally prove that a subset of PRSC contexts, called well-behaved contexts, can be used to reverse back to the original PG, and provide the related algorithm. Algorithms for conversion and reversion are available as open-source implementations.},
  archive      = {J_SW},
  author       = {Bruyat, Julian and Champin, Pierre-Antoine and Médini, Lionel and Laforest, Frédérique},
  doi          = {10.3233/SW-243675},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2555-2595},
  shortjournal = {Semantic Web},
  title        = {PRSC: From PG to RDF and back, using schemas},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on SPARQL query relaxation under the lens of RDF
reification. <em>SW</em>, <em>15</em>(6), 2507–2554. (<a
href="https://doi.org/10.3233/SW-243621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query relaxation has been proposed to cope with the problem of queries that produce none or insufficient answers. The goal is to modify these queries to be able to produce alternative results close to those expected in the original query. Existing approaches querying RDF datasets generally relax the SPARQL query constraints based on logical relaxations through RDFS entailment and RDFS ontologies. Techniques also exist that use the similarity of instances based on resource descriptions. These relaxation approaches defined for SPARQL queries over RDF triples have proved their efficiency. Nevertheless, significant challenges arise for query relaxation techniques in the presence of statement-level annotations, i.e., RDF reification. In this survey, we overview SPARQL query relaxation works with a particular focus on issues and challenges posed by representative RDF reification models, namely, standard reification, named graphs, n-ary relations, singleton properties, and RDF-Star.},
  archive      = {J_SW},
  author       = {Fakih, Ginwa and Serrano-Alvarado, Patricia},
  doi          = {10.3233/SW-243621},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2507-2554},
  shortjournal = {Semantic Web},
  title        = {A survey on SPARQL query relaxation under the lens of RDF reification},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path-based and triplification approaches to mapping data
into RDF: User behaviours and recommendations. <em>SW</em>,
<em>15</em>(6), 2479–2505. (<a
href="https://doi.org/10.3233/SW-243585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping complex structured data to RDF, e.g. for the creation of linked data, requires a clear understanding of the data, but also a clear understanding of the paradigm used by the mapping tool. We illustrate this with an empirical study comparing two different mapping tools, in particular considering the likelihood of user error. One tool uses path descriptions, e.g. JSONPath or XPath, to access data elements; the other uses a default triplification which can be queried, e.g. with SPARQL. As an example of the former, the study used YARRRML , to map from CSV, JSON and XML to RDF. As an example of the latter, the study used an extension of SPARQL, SPARQL Anything , to query the same data and CONSTRUCT a set of triples. Our study was a qualitative one, based on observing the kinds of errors made by participants using the two tools with identical mapping tasks, and using a grounded approach to categorize these errors. Whilst there are difficulties common to the two tools, there are also difficulties specific to each tool. For each tool, we present recommendations which help ensure that the mapping code is consistent with the data and the desired RDF. We propose future developments to reduce the difficulty users experience with YARRRML and SPARQL Anything. We also make some general recommendations about the future development of mapping tools and techniques. Finally, we propose some research questions for future investigation.},
  archive      = {J_SW},
  author       = {Warren, Paul and Mulholland, Paul and Daga, Enrico and Asprino, Luigi},
  doi          = {10.3233/SW-243585},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2479-2505},
  shortjournal = {Semantic Web},
  title        = {Path-based and triplification approaches to mapping data into RDF: User behaviours and recommendations},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RQSS: Referencing quality scoring system for wikidata.
<em>SW</em>, <em>15</em>(6), 2419–2475. (<a
href="https://doi.org/10.3233/SW-243695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wikidata is a collaborative multi-purpose Knowledge Graph (KG) with the unique feature of adding provenance data to the statements of items as a reference. More than 73% of Wikidata statements have provenance metadata; however, few studies exist on the referencing quality in this KG, focusing only on the relevancy and trustworthiness of external sources. While there are existing frameworks to assess the quality of Linked Data, and in some aspects their metrics investigate provenance, there are none focused on reference quality. We define a comprehensive referencing quality assessment framework based on Linked Data quality dimensions, such as completeness and understandability. We implement the objective metrics of the assessment framework as the Referencing Quality Scoring System – RQSS. The system provides quantified scores by which the referencing quality can be analyzed and compared. RQSS scripts can also be reused to monitor the referencing quality regularly. Due to the scale of Wikidata, we have used well-defined subsets to evaluate the quality of references in Wikidata using RQSS. We evaluate RQSS over three topical subsets: Gene Wiki, Music, and Ships, corresponding to three Wikidata WikiProjects, along with four random subsets of various sizes. The evaluation shows that RQSS is practical and provides valuable information, which can be used by Wikidata contributors and project holders to identify the quality gaps. Based on RQSS, the average referencing quality in Wikidata subsets is 0.58 out of 1. Random subsets (representative of Wikidata) have higher overall scores than topical subsets by 0.05, with Gene Wiki having the highest scores amongst topical subsets. Regarding referencing quality dimensions, all subsets have high scores in accuracy, availability, security, and understandability, but have weaker scores in completeness, verifiability, objectivity, and versatility. Although RQSS is developed based on the Wikidata RDF model, its referencing quality assessment framework can be applied to KGs in general.},
  archive      = {J_SW},
  author       = {Hosseini Beghaeiraveri, Seyed Amir and Gray, Alasdair and McNeill, Fiona},
  doi          = {10.3233/SW-243695},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2419-2475},
  shortjournal = {Semantic Web},
  title        = {RQSS: Referencing quality scoring system for wikidata},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On assessing weaker logical status claims in wikidata
cultural heritage records. <em>SW</em>, <em>15</em>(6), 2395–2417. (<a
href="https://doi.org/10.3233/SW-243686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work analyses the usage of different approaches adopted in Wikidata to represent information with weaker logical status (WLS, e.g., uncertain information, competing hypotheses, temporally evolving information). The study examines four main approaches: non-asserted statements, ranked statements, non-existing valued objects, and statements qualified with properties P5102 : nature of statement , P1480 : sourcing circumstances , and P2241 : reason for deprecated rank . We analyse their prevalence, success, and clarity in Wikidata. The analysis is performed over Cultural Heritage artefacts stored in Wikidata, divided into three subsets (i.e., visual heritage, textual heritage, and audio-visual heritage), and compared with astronomical data (stars and galaxies entities). Our findings indicate that (1) the presence of weaker logical status information is limited, with only a small proportion of items reporting such information, (2) the usage of WLS claims varies significantly between the two datasets in terms of prevalence and success of such approaches, and (3) precise assessment of WLS statements is made complicated by the ambiguities and overlappings between WLS and non-WLS claims allowed by the chosen representations. Finally, we list a few proposals to simplify and standardise this information representation in Wikidata, hoping to increase its clarity, accuracy and richness.},
  archive      = {J_SW},
  author       = {Di Pasquale, Alessio and Pasqual, Valentina and Tomasi, Francesca and Vitali, Fabio},
  doi          = {10.3233/SW-243686},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2395-2417},
  shortjournal = {Semantic Web},
  title        = {On assessing weaker logical status claims in wikidata cultural heritage records},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InteractOA: Showcasing the representation of knowledge from
scientific literature in wikidata. <em>SW</em>, <em>15</em>(6),
2381–2393. (<a href="https://doi.org/10.3233/SW-243685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge generated during the scientific process is still mostly stored in the form of scholarly articles. This lack of machine-readability hampers efforts to find, query, and reuse such findings efficiently and contributes to today’s information overload. While attempts have been made to semantify journal articles, widespread adoption of such approaches is still a long way off. One way to demonstrate the usefulness of such approaches to the scientific community is by showcasing the use of freely available, open-access knowledge graphs such as Wikidata as sustainable storage and representation solutions. Here we present an example from the life sciences in which knowledge items from scholarly literature are represented in Wikidata, linked to their exact position in open-access articles. In this way, they become part of a rich knowledge graph while maintaining clear ties to their origins. As example entities, we chose small regulatory RNAs (sRNAs) that play an important role in bacterial and archaeal gene regulation. These post-transcriptional regulators can influence the activities of multiple genes in various manners, forming complex interaction networks. We stored the information on sRNA molecule interaction taken from open-access articles in Wikidata and built an intuitive web interface called InteractOA , which makes it easy to visualize, edit, and query information. The tool also links information on small RNAs to their reference articles from PubMed Central on the statement level. InteractOA encourages researchers to contribute, save, and curate their own similar findings. InteractOA is hosted at https://interactoa.zbmed.de and its code is available under a permissive open source licence. In principle, the approach presented here can be applied to any other field of research.},
  archive      = {J_SW},
  author       = {Elhossary, Muhammad and Förstner, Konrad U.},
  doi          = {10.3233/SW-243685},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2381-2393},
  shortjournal = {Semantic Web},
  title        = {InteractOA: Showcasing the representation of knowledge from scientific literature in wikidata},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formalizing and validating wikidata’s property constraints
using SHACL and SPARQL. <em>SW</em>, <em>15</em>(6), 2333–2380. (<a
href="https://doi.org/10.3233/SW-243611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we delve into the crucial role of constraints in maintaining data integrity in knowledge graphs with a specific focus on Wikidata, one of the most extensive collaboratively maintained open data knowledge graphs on the Web. The World Wide Web Consortium (W3C) recommends the Shapes Constraint Language (SHACL) as the constraint language for validating Knowledge Graphs, which comes in two different levels of expressivity, SHACL-Core, as well as SHACL-SPARQL. Despite the availability of SHACL, Wikidata currently represents its property constraints through its own RDF data model, which relies on Wikidata’s specific reification mechanism based on authoritative namespaces, and – partially ambiguous – natural language definitions. In the present paper, we investigate whether and how the semantics of Wikidata property constraints, can be formalized using SHACL-Core, SHACL-SPARQL, as well as directly as SPARQL queries. While the expressivity of SHACL-Core turns out to be insufficient for expressing all Wikidata property constraint types, we present SPARQL queries to identify violations for all 32 current Wikidata constraint types. We compare the semantics of this unambiguous SPARQL formalization with Wikidata’s violation reporting system and discuss limitations in terms of evaluation via Wikidata’s public SPARQL query endpoint, due to its current scalability. Our study, on the one hand, sheds light on the unique characteristics of constraints defined by the Wikidata community, in order to improve the quality and accuracy of data in this collaborative knowledge graph. On the other hand, as a “byproduct”, our formalization extends existing benchmarks for both SHACL and SPARQL with a challenging, large-scale real-world use case.},
  archive      = {J_SW},
  author       = {Ferranti, Nicolas and De Souza, Jairo Francisco and Ahmetaj, Shqiponja and Polleres, Axel},
  doi          = {10.3233/SW-243611},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2333-2380},
  shortjournal = {Semantic Web},
  title        = {Formalizing and validating wikidata’s property constraints using SHACL and SPARQL},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using wikidata lexemes and items to generate text from
abstract representations. <em>SW</em>, <em>15</em>(6), 2319–2332. (<a
href="https://doi.org/10.3233/SW-243564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ninai/Udiron, a living function-based natural language generation system, uses knowledge in Wikidata lexemes and items to transform abstract representations of factual statements into human-readable text. The combined system first produces syntax trees based on those abstract representations (Ninai) and then yields sentences from those syntax trees (Udiron). The system relies on information about individual lexical units and links to the concepts those units represent, as well as rules encoded in various types of functions to which users may contribute, to make decisions about words, phrases, and other morphemes to use and how to arrange them. Various system design choices work toward using the information in Wikidata lexemes and items efficiently and effectively, making different components individually contributable and extensible, and making the overall resultant outputs from the system expectable and analyzable. These targets accompany the intentions for Ninai/Udiron to ultimately power the Abstract Wikipedia project as well as be hosted on the Wikifunctions project.},
  archive      = {J_SW},
  author       = {Morshed, Mahir},
  doi          = {10.3233/SW-243564},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2319-2332},
  shortjournal = {Semantic Web},
  title        = {Using wikidata lexemes and items to generate text from abstract representations},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical ontology design patterns and shapes from wikidata.
<em>SW</em>, <em>15</em>(6), 2293–2317. (<a
href="https://doi.org/10.3233/SW-243613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ontology underlying the Wikidata knowledge graph (KG) has not been formalized. Instead, its semantics emerges bottom-up from the use of its classes and properties. Flexible guidelines and rules have been defined by the Wikidata project for the use of its ontology, however, it is still often difficult to reuse the ontology’s constructs. Based on the assumption that identifying ontology design patterns from a knowledge graph contributes to making its (possibly) implicit ontology emerge, in this paper we present a method for extracting what we term empirical ontology design patterns (EODPs) from a knowledge graph. This method takes as input a knowledge graph and extracts EODPs as sets of axioms/constraints involving the classes instantiated in the KG. These EODPs include data about the probability of such axioms/constraints happening . We apply our method on two domain-specific portions of Wikidata, addressing the music and art, architecture, and archaeology domains, and we compare the empirical ontology design patterns we extract with the current support present in Wikidata. We show how these patterns can provide guidance for the use of the Wikidata ontology and its potential improvement, and can give insight into the content of (domain-specific portions of) the Wikidata knowledge graph.},
  archive      = {J_SW},
  author       = {Carriero, Valentina Anita and Groth, Paul and Presutti, Valentina},
  doi          = {10.3233/SW-243613},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2293-2317},
  shortjournal = {Semantic Web},
  title        = {Empirical ontology design patterns and shapes from wikidata},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can you trust wikidata? <em>SW</em>, <em>15</em>(6),
2271–2292. (<a href="https://doi.org/10.3233/SW-243577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to use a value retrieved from a Knowledge Graph (KG) for some computation, the user should, in principle, ensure that s/he trusts the veracity of the claim, i.e., considers the statement as a fact. Crowd-sourced KGs, or KGs constructed by integrating several different information sources of varying quality, must be used via a trust layer. The veracity of each claim in the underlying KG should be evaluated, considering what is relevant to carrying out some action that motivates the information seeking. The present work aims to assess how well Wikidata (WD) supports the trust decision process implied when using its data. WD provides several mechanisms that can support this trust decision, and our KG Profiling, based on WD claims and schema, elaborates an analysis of how multiple points of view, controversies, and potentially incomplete or incongruent content are presented and represented.},
  archive      = {J_SW},
  author       = {Santos, Veronica and Schwabe, Daniel and Lifschitz, Sérgio},
  doi          = {10.3233/SW-243577},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2271-2292},
  shortjournal = {Semantic Web},
  title        = {Can you trust wikidata?},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evidence of large-scale conceptual disarray in multi-level
taxonomies in wikidata. <em>SW</em>, <em>15</em>(6), 2253–2270. (<a
href="https://doi.org/10.3233/SW-243562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distinction between types and individuals is key to most conceptual modeling techniques and knowledge representation languages. Despite that, there are a number of situations in which modelers navigate this distinction inadequately, leading to problematic models. We show evidence of a large number of representation mistakes associated with the failure to employ this distinction in the Wikidata knowledge graph, which can be identified with the incorrect use of instantiation , which is a relation between an instance and a type, and specialization (or subtyping ), which is a relation between two types. The prevalence of the problems in Wikidata’s taxonomies suggests that methodological and computational tools are required to mitigate the issues identified, which occur in many settings when individuals, types, and their metatypes are included in the domain of interest. We conduct a conceptual analysis of entities involved in recurrent erroneous cases identified in this empirical data, and present a tool that supports users in identifying some of these mistakes.},
  archive      = {J_SW},
  author       = {Dadalto, Atílio A. and Almeida, João Paulo A. and Fonseca, Claudenir M. and Guizzardi, Giancarlo},
  doi          = {10.3233/SW-243562},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2253-2270},
  shortjournal = {Semantic Web},
  title        = {Evidence of large-scale conceptual disarray in multi-level taxonomies in wikidata},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dura-europos stories: Developing interactive storytelling
applications using knowledge graphs for cultural heritage exploration.
<em>SW</em>, <em>15</em>(6), 2237–2251. (<a
href="https://doi.org/10.3233/SW-243552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Dura-Europos Stories, a multimedia application for viewing artifacts and places related to the Dura-Europos archaeological excavation. We describe the process of mapping data to the Wikidata data model as well as the process of contributing data to Wikidata. We provide an overview of the functionality of an interactive application for viewing images of the artifacts in the context of their metadata. We contextualize this project as an example of using knowledge graphs in research projects in order to leverage technologies of the Semantic Web in such a way that data related to the project can be easily combined with other data on the web. Presenting artifacts in this story-based application allows users to explore these objects visually, and provides pathways for further exploration of related information.},
  archive      = {J_SW},
  author       = {Thornton, Katherine and Seals-Nutt, Kenneth and Chen, Anne},
  doi          = {10.3233/SW-243552},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2237-2251},
  shortjournal = {Semantic Web},
  title        = {Dura-europos stories: Developing interactive storytelling applications using knowledge graphs for cultural heritage exploration},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wikidata subsetting: Approaches, tools, and evaluation.
<em>SW</em>, <em>15</em>(6), 2209–2235. (<a
href="https://doi.org/10.3233/SW-233491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wikidata is a massive Knowledge Graph (KG), including more than 100 million data items and nearly 1.5 billion statements, covering a wide range of topics such as geography, history, scholarly articles, and life science data. The large volume of Wikidata is difficult to handle for research purposes; many researchers cannot afford the costs of hosting 100 GB of data. While Wikidata provides a public SPARQL endpoint, it can only be used for short-running queries. Often, researchers only require a limited range of data from Wikidata focusing on a particular topic for their use case. Subsetting is the process of defining and extracting the required data range from the KG; this process has received increasing attention in recent years. Specific tools and several approaches have been developed for subsetting, which have not been evaluated yet. In this paper, we survey the available subsetting approaches, introducing their general strengths and weaknesses, and evaluate four practical tools specific for Wikidata subsetting – WDSub, KGTK, WDumper, and WDF – in terms of execution performance, extraction accuracy, and flexibility in defining the subsets. Results show that all four tools have a minimum of 99.96% accuracy in extracting defined items and 99.25% in extracting statements. The fastest tool in extraction is WDF, while the most flexible tool is WDSub. During the experiments, multiple subset use cases have been defined and the extracted subsets have been analyzed, obtaining valuable information about the variety and quality of Wikidata, which would otherwise not be possible through the public Wikidata SPARQL endpoint.},
  archive      = {J_SW},
  author       = {Hosseini Beghaeiraveri, Seyed Amir and Labra Gayo, Jose Emilio and Waagmeester, Andra and Ammar, Ammar and Gonzalez, Carolina and Slenter, Denise and Ul-Hasan, Sabah and Willighagen, Egon and McNeill, Fiona and Gray, Alasdair J.G.},
  doi          = {10.3233/SW-233491},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2209-2235},
  shortjournal = {Semantic Web},
  title        = {Wikidata subsetting: Approaches, tools, and evaluation},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QALD-10 – the 10th challenge on question answering over
linked data. <em>SW</em>, <em>15</em>(6), 2193–2207. (<a
href="https://doi.org/10.3233/SW-233471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Question Answering (KGQA) has gained attention from both industry and academia over the past decade. Researchers proposed a substantial amount of benchmarking datasets with different properties, pushing the development in this field forward. Many of these benchmarks depend on Freebase, DBpedia, or Wikidata. However, KGQA benchmarks that depend on Freebase and DBpedia are gradually less studied and used, because Freebase is defunct and DBpedia lacks the structural validity of Wikidata. Therefore, research is gravitating toward Wikidata-based benchmarks. That is, new KGQA benchmarks are created on the basis of Wikidata and existing ones are migrated. We present a new, multilingual, complex KGQA benchmarking dataset as the 10th part of the Question Answering over Linked Data (QALD) benchmark series. This corpus formerly depended on DBpedia. Since QALD serves as a base for many machine-generated benchmarks, we increased the size and adjusted the benchmark to Wikidata and its ranking mechanism of properties. These measures foster novel KGQA developments by more demanding benchmarks. Creating a benchmark from scratch or migrating it from DBpedia to Wikidata is non-trivial due to the complexity of the Wikidata knowledge graph, mapping issues between different languages, and the ranking mechanism of properties using qualifiers. We present our creation strategy and the challenges we faced that will assist other researchers in their future work. Our case study, in the form of a conference challenge, is accompanied by an in-depth analysis of the created benchmark.},
  archive      = {J_SW},
  author       = {Usbeck, Ricardo and Yan, Xi and Perevalov, Aleksandr and Jiang, Longquan and Schulz, Julius and Kraft, Angelie and Möller, Cedric and Huang, Junbo and Reineke, Jan and Ngonga Ngomo, Axel-Cyrille and Saleem, Muhammad and Both, Andreas},
  doi          = {10.3233/SW-233471},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2193-2207},
  shortjournal = {Semantic Web},
  title        = {QALD-10 – the 10th challenge on question answering over linked data},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ProVe: A pipeline for automated provenance verification of
knowledge graphs against textual sources. <em>SW</em>, <em>15</em>(6),
2159–2192. (<a href="https://doi.org/10.3233/SW-233467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs are repositories of information that gather data from a multitude of domains and sources in the form of semantic triples, serving as a source of structured data for various crucial applications in the modern web landscape, from Wikipedia infoboxes to search engines. Such graphs mainly serve as secondary sources of information and depend on well-documented and verifiable provenance to ensure their trustworthiness and usability. However, their ability to systematically assess and assure the quality of this provenance, most crucially whether it properly supports the graph’s information, relies mainly on manual processes that do not scale with size. ProVe aims at remedying this, consisting of a pipelined approach that automatically verifies whether a Knowledge Graph triple is supported by text extracted from its documented provenance. ProVe is intended to assist information curators and consists of four main steps involving rule-based methods and machine learning models: text extraction, triple verbalisation, sentence selection, and claim verification. ProVe is evaluated on a Wikidata dataset, achieving promising results overall and excellent performance on the binary classification task of detecting support from provenance, with 87.5 % accuracy and 82.9 % F1-macro on text-rich sources. The evaluation data and scripts used in this paper are available in GitHub and Figshare.},
  archive      = {J_SW},
  author       = {Amaral, Gabriel and Rodrigues, Odinaldo and Simperl, Elena},
  doi          = {10.3233/SW-233467},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2159-2192},
  shortjournal = {Semantic Web},
  title        = {ProVe: A pipeline for automated provenance verification of knowledge graphs against textual sources},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Psychiq and wwwyzzerdd: Wikidata completion using wikipedia.
<em>SW</em>, <em>15</em>(6), 2145–2158. (<a
href="https://doi.org/10.3233/SW-233450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its size, Wikidata remains incomplete and inaccurate in many areas. Hundreds of thousands of articles on English Wikipedia have zero or limited meaningful structure on Wikidata. Much work has been done in the literature to partially or fully automate the process of completing knowledge graphs, but little of it has been practically applied to Wikidata. This paper presents two interconnected practical approaches to speeding up the Wikidata completion task. The first is Wwwyzzerdd, a browser extension that allows users to quickly import statements from Wikipedia to Wikidata. Wwwyzzerdd has been used to make over 100 thousand edits to Wikidata. The second is Psychiq, a new model for predicting instance and subclass statements based on English Wikipedia articles. Psychiq’s performance and characteristics make it well suited to solving a variety of problems for the Wikidata community. One initial use is integrating the Psychiq model into the Wwwyzzerdd browser extension.},
  archive      = {J_SW},
  author       = {Erenrich, Daniel},
  doi          = {10.3233/SW-233450},
  journal      = {Semantic Web},
  month        = {1},
  number       = {6},
  pages        = {2145-2158},
  shortjournal = {Semantic Web},
  title        = {Psychiq and wwwyzzerdd: Wikidata completion using wikipedia},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
