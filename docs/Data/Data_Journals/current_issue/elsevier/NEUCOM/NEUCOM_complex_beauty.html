<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NEUCOM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neucom---22">NEUCOM - 22</h2>
<ul>
<li><details>
<summary>
(2025). Echo state network with a non-convex penalty for nonlinear
time series prediction. <em>NEUCOM</em>, <em>637</em>, 130084. (<a
href="https://doi.org/10.1016/j.neucom.2025.130084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo state networks (ESNs) with large reservoirs have been widely used in nonlinear time series prediction. However, over-large reservoirs will lead to ill-conditioned solutions when the output weights of ESNs are calculated by solving a linear regression problem. To address this issue, we propose an improved ESN with a non-convex penalty (NCP-ESN) for nonlinear time series prediction. The main idea of NCP-ESN is that an adjustable log penalty with nonconvex characteristics is introduced to the loss function for generating unbiased and sparse solutions when optimizing the output weights of the network. Meanwhile, a learning method with two-stage optimization is developed for the optimal output weights by combining the coordinate descent algorithm with the generalized inverse method. Finally, two simulation sequences and two real sequences are used to test the performance of the proposed NCP-ESN on time series prediction. Experimental results have shown the better performance of the proposed NCP-ESN compared with some regularized ESNs.},
  archive      = {J_NEUCOM},
  author       = {Wenting Wang and Fanjun Li and Qianwen Liu},
  doi          = {10.1016/j.neucom.2025.130084},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130084},
  shortjournal = {Neurocomputing},
  title        = {Echo state network with a non-convex penalty for nonlinear time series prediction},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PUAL: A classifier on trifurcate positive-unlabelled data.
<em>NEUCOM</em>, <em>637</em>, 130080. (<a
href="https://doi.org/10.1016/j.neucom.2025.130080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive-unlabelled (PU) learning aims to train a classifier using the data containing only labelled-positive instances and unlabelled instances. However, existing PU learning methods are generally hard to achieve satisfactory performance on trifurcate data, where the positive instances distribute on both sides of the negative instances. To address this issue, firstly we propose a PU classifier with asymmetric loss (PUAL), by introducing a structure of asymmetric loss on positive instances into the objective function of the global and local learning classifier. Then we develop a kernel-based algorithm to enable PUAL to obtain non-linear decision boundary. We show that, through experiments on both simulated and real-world datasets, PUAL can achieve satisfactory classification on trifurcate data.},
  archive      = {J_NEUCOM},
  author       = {Xiaoke Wang and Xiaochen Yang and Rui Zhu and Jing-Hao Xue},
  doi          = {10.1016/j.neucom.2025.130080},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130080},
  shortjournal = {Neurocomputing},
  title        = {PUAL: A classifier on trifurcate positive-unlabelled data},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HB-net: Holistic bursting cell cluster integrated network
for occluded multi-objects recognition. <em>NEUCOM</em>, <em>637</em>,
130071. (<a href="https://doi.org/10.1016/j.neucom.2025.130071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the realm of image recognition, a specific category of multi-label classification (MLC) challenges arises when objects within the visual field may occlude one another, demanding simultaneous identification of both occluded and occluding objects. While traditional convolutional neural networks (CNNs) address these tasks, they are often bulky and achieve only moderate accuracy. To overcome this limitation, this paper introduces HB-net, a novel integrated network framework inspired by the Holistic Bursting (HB) cell from cutting-edge neural science research. Built upon the foundation of HB cell clusters, HB-net is designed to address the intricate task of simultaneously recognizing multiple occluded objects within images. The framework incorporates various Bursting cell cluster structures along with an evidence accumulation mechanism to enhance performance. Testing on multiple datasets, including digits and letters, shows that models incorporating the HB framework achieve a significant 2.98% improvement in recognition accuracy compared to models without the HB framework (1.0298 times, p=0.0499). Although in high-noise settings, standard CNNs exhibit slightly greater robustness when compared to HB-net models, the models that combine the HB framework and EA mechanism achieve a comparable level of accuracy and resilience to ResNet50, despite having only three convolutional layers and approximately 1 / 30 of the parameters. These findings of this study offer valuable insights for improving computer vision algorithms. The essential code is provided at https://github.com/d-lab438/hb-net.git .},
  archive      = {J_NEUCOM},
  author       = {Xudong Gao and Xiaoguang Gao and Jia Rong and Xiaowei Chen and Xiang Liao and Jun Chen},
  doi          = {10.1016/j.neucom.2025.130071},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130071},
  shortjournal = {Neurocomputing},
  title        = {HB-net: Holistic bursting cell cluster integrated network for occluded multi-objects recognition},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adversarial contrastive learning based cross-modality
zero-watermarking scheme for DIBR 3D video copyright protection.
<em>NEUCOM</em>, <em>637</em>, 130068. (<a
href="https://doi.org/10.1016/j.neucom.2025.130068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copyright protection of depth image-based rendering (DIBR) videos has raised significant concerns due to their increasing popularity. Zero-watermarking, emerging as a powerful tool to protect the copyright of DIBR 3D videos, mainly relies on traditional feature extraction methods, thus necessitating improvements in robustness against complex geometric attacks and its ability to strike a balance between robustness and distinguishability. This paper presents a novel zero-watermarking scheme based on cross-modality feature fusion within a contrastive learning framework. Our approach integrates complementary information from 2D frames and depth maps using a cross-modality attention feature fusion mechanism to obtain discriminative features. Moreover, our features achieve a better trade-off between robustness and distinguishability by leveraging a designed contrastive learning strategy with an adversarial distortion simulator. Experimental results demonstrate our remarkable performance by reducing the false negative rates to around 0.2% when the false positive rate is equal to 0.5%, which is superior to the state-of-the-art zero-watermarking methods.},
  archive      = {J_NEUCOM},
  author       = {Xiyao Liu and Qingyu Dang and Huiyi Wang and Xiaoheng Deng and Xunli Fan and Cundian Yang and Zhihong Chen and Hui Fang},
  doi          = {10.1016/j.neucom.2025.130068},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130068},
  shortjournal = {Neurocomputing},
  title        = {An adversarial contrastive learning based cross-modality zero-watermarking scheme for DIBR 3D video copyright protection},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dstsa-gcn: Advancing skeleton-based gesture recognition with
semantic-aware spatio-temporal topology modeling. <em>NEUCOM</em>,
<em>637</em>, 130066. (<a
href="https://doi.org/10.1016/j.neucom.2025.130066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have emerged as a powerful tool for skeleton-based action and gesture recognition, thanks to their ability to model spatial and temporal dependencies in skeleton data. However, existing GCN-based methods face critical limitations: (1) they lack effective spatio-temporal topology modeling that captures dynamic variations in skeletal motion, and (2) they struggle to model multiscale structural relationships beyond local joint connectivity. To address these issues, we propose a novel framework called Dynamic Spatial-Temporal Semantic Awareness Graph Convolutional Network (DSTSA-GCN). DSTSA-GCN introduces three key modules: Group Channel-wise Graph Convolution (GC-GC), Group Temporal-wise Graph Convolution (GT-GC), and Multi-Scale Temporal Convolution (MS-TCN). GC-GC and GT-GC operate in parallel to independently model channel-specific and frame-specific correlations, enabling robust topology learning that accounts for temporal variations. Additionally, both modules employ a grouping strategy to adaptively capture multiscale structural relationships. Complementing this, MS-TCN enhances temporal modeling through group-wise temporal convolutions with diverse receptive fields. Extensive experiments demonstrate that DSTSA-GCN significantly improves the topology modeling capabilities of GCNs, achieving state-of-the-art performance on benchmark datasets for gesture and action recognition, including SHREC’17 Track, DHG-14/28, NTU-RGB+D, NTU-RGB+D-120 and NW-ULCA. The code will be publicly available https://hucui2022.github.io/dstsa_gcn/ .},
  archive      = {J_NEUCOM},
  author       = {Hu Cui and Renjing Huang and Ruoyu Zhang and Tessai Hayama},
  doi          = {10.1016/j.neucom.2025.130066},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130066},
  shortjournal = {Neurocomputing},
  title        = {Dstsa-gcn: Advancing skeleton-based gesture recognition with semantic-aware spatio-temporal topology modeling},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GlobalLight: Exploring global influence in multi-agent deep
reinforcement learning for large-scale traffic signal control.
<em>NEUCOM</em>, <em>637</em>, 130065. (<a
href="https://doi.org/10.1016/j.neucom.2025.130065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By treating each intersection as an intelligent agent, multi-agent deep reinforcement learning (MADRL) offers a promising solution to adaptive traffic signal control (ATSC) in complex urban environments. However, existing approaches often emphasize the interactions between adjacent intersections while overlooking the global influence of distant relationships. This oversight limits their scalability to small-scale traffic networks, reducing their effectiveness in real-world urban transportation systems. In this paper, we propose GlobalLight , a novel MADRL-based traffic signal control method that addresses these challenges by exploring and exploiting global influence in traffic networks. We first propose a multidimensional feature extraction module via a multi-head graph attention network, which captures the mutual influence among locally adjacent intersections. Then we design a similarity mining module with two loss functions to analyze node embeddings in the representation space, uncovering latent relationships across distant intersections in the global traffic network. Finally, GlobalLight enables similar intersections to share policy parameters for decision-making within an effective MADRL framework. Our method simultaneously considers local dependencies between adjacent intersections and global traffic flow influence, enhancing scalability and decision efficiency for ATSC in city-level larger-scale traffic systems. Experimental evaluations on both synthetic and real-world traffic networks, encompassing up to 1000 of intersections, demonstrate that our method significantly outperforms SOTA approaches across multiple performance metrics, particularly in large-scale traffic scenarios.},
  archive      = {J_NEUCOM},
  author       = {Yilin Liu and Jintao Liang and Yifeng Zhang and Ping Gong and Guiyang Luo and Quan Yuan and Jinglin Li},
  doi          = {10.1016/j.neucom.2025.130065},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130065},
  shortjournal = {Neurocomputing},
  title        = {GlobalLight: Exploring global influence in multi-agent deep reinforcement learning for large-scale traffic signal control},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fraud detection in multi-relation graph: Contrastive
learning on feature and structural levels. <em>NEUCOM</em>,
<em>637</em>, 130063. (<a
href="https://doi.org/10.1016/j.neucom.2025.130063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraud detection has emerged as a significant area of study, primarily due to its considerable impact on real-world applications. Despite the effectiveness of existing methods for fraud detection, they have not adequately addressed two key challenges: fraudulent camouflage and class imbalance. To tackle these challenges, we propose a novel model called Contrastive Learning on Feature and Structural Levels in Graph Neural Networks (CLFS-GNN) to effectively tackle these challenges. Our model incorporates an innovative neighbor nodes selection module that considers both feature and structural similarity between central nodes and their neighbor nodes, effectively reducing interference from fraudulent nodes by selecting highly similar neighbor nodes. Additionally, it employs an intra- and inter-graph message aggregation module with attention mechanisms to enhance the value of aggregated neighbor node information, thereby improving fraud detection performance. Furthermore, the algorithm incorporates contrastive learning to pull similar nodes closer and push dissimilar nodes further apart, mitigating class imbalance effects and achieving superior performance. Extensive experimental results show that this model outperforms the state-of-the-art GNN-based fraud detection on the Yelp and Amazon benchmark datasets.},
  archive      = {J_NEUCOM},
  author       = {Jiangnan Tang and Huanhuan Gu and Darko B. Vuković and Guandong Xu and Youquan Wang and Haicheng Tao and Jie Cao},
  doi          = {10.1016/j.neucom.2025.130063},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130063},
  shortjournal = {Neurocomputing},
  title        = {Fraud detection in multi-relation graph: Contrastive learning on feature and structural levels},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGQB-ViT: Adaptive granularity quantizer with bias for
post-training quantization of vision transformers. <em>NEUCOM</em>,
<em>637</em>, 130061. (<a
href="https://doi.org/10.1016/j.neucom.2025.130061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant achievements of Vision Transformers (ViTs) in various computer vision tasks, these models are often large and computationally complex, making them unsuitable for resource-constrained devices. Quantizing ViTs converts model parameters from high-precision to low-precision formats, which significantly reduces computational complexity and memory requirements, thereby enhancing the performance and efficiency of ViTs in limited-resource environments. In the post-training quantization (PTQ) process of ViTs, the logarithmic quantizer is an effective method for components with power-law distribution characteristics, such as Softmax and GELU. However, when further exploring quantization scenarios at 4 bits or lower, the traditional log-based quantizer suffers from significant accuracy loss due to inflexibility in base selection and suboptimal quantization efficiency. To address this issue, we propose an adaptive granularity quantizer with bias, termed AGQB. It conducts adaptive granularity optimization for both the logarithmic base and bias according to different activation distributions and quantization bit-width requirements. Furthermore, we implemented a three-stage optimization process, setting block reconstruction as the learning objective to minimize the error before and after quantization. Extensive experimental results show that AGQB-ViT can effectively quantize various ViT models and outperforms previous methods on multiple computer vision tasks. In particular, when the ViTs model is quantized to 3 bits, we achieve an average accuracy improvement of 2.13% in image classification tasks relative to existing state-of-the-art PTQ methods. The related code is available at https://github.com/kkkyq/AGQB-ViT .},
  archive      = {J_NEUCOM},
  author       = {Ying Huo and Yongqiang Kang and Dawei Yang and Jiahao Zhu},
  doi          = {10.1016/j.neucom.2025.130061},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130061},
  shortjournal = {Neurocomputing},
  title        = {AGQB-ViT: Adaptive granularity quantizer with bias for post-training quantization of vision transformers},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure modeling activation free fourier network for
spacecraft image denoising. <em>NEUCOM</em>, <em>637</em>, 130058. (<a
href="https://doi.org/10.1016/j.neucom.2025.130058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spacecraft image denoising is a crucial fundamental technology closely related to aerospace research. However, the existing deep learning-based image denoising methods are primarily designed for natural image and fail to adequately consider the characteristics of spacecraft image (e.g. low-light conditions, repetitive periodic structures), resulting in suboptimal performance in the spacecraft image denoising task. To address the aforementioned problems, we propose a Structure modeling Activation Free Fourier Network (SAFFN), which is an efficient spacecraft image denoising method including Structure Modeling Block (SMB) and Activation Free Fourier Block (AFFB). We present SMB to effectively extract edge information and model the structure for better identification of spacecraft components from dark regions in spacecraft noise image. We present AFFB and utilize an improved Fast Fourier block to extract repetitive periodic features and long-range information in noisy spacecraft image. Extensive experimental results demonstrate that our SAFFN performs competitively compared to the state-of-the-art methods on spacecraft noise image datasets. The codes are available at: https://github.com/shenduke/SAFFN .},
  archive      = {J_NEUCOM},
  author       = {Jingfan Yang and Hu Gao and Ying Zhang and Bowen Ma and Depeng Dang},
  doi          = {10.1016/j.neucom.2025.130058},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130058},
  shortjournal = {Neurocomputing},
  title        = {Structure modeling activation free fourier network for spacecraft image denoising},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAC: Collaborative learning of structure and content
features for android malware detection framework. <em>NEUCOM</em>,
<em>637</em>, 130053. (<a
href="https://doi.org/10.1016/j.neucom.2025.130053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Internet of Things (IoT) technology, Android devices have increasingly become primary targets for malware attacks. Although significant research has been conducted in the field of malware detection, existing methods still face challenges when dealing with complex samples. In particular, a more comprehensive analysis is required in the domain of feature extraction. To enhance the accuracy of malware detection, we propose the SAC framework. This method utilizes Dalvik Executable (DEX) files as the data source and achieves deep integration of multi-view features by collaboratively modeling image and graph data types. Specifically, to accurately capture the local features of malware and improve the identification of critical behavioral patterns, we designed a task-oriented convolutional neural network (CNN) named IFNeXt, which integrates visualization analysis with an inverted bottleneck structure. Furthermore, we introduced a dual-channel graph convolutional network (GCN) that models the hierarchical structure of bytecode as a directed graph, capturing the co-occurrence relationships and semantic similarities between method calls. This approach enables a deeper exploration of the global structural features of malware. The SAC framework fully leverages the complementary advantages of image and graph data structures, providing a more comprehensive characterization of malware features from both content and structural perspectives. Experimental results demonstrate that our method achieves a detection accuracy of 99.43% on multiple real-world public datasets, significantly outperforming existing state-of-the-art detection techniques. This indicates the potential and innovation of our approach in enhancing the security of the Android platform.},
  archive      = {J_NEUCOM},
  author       = {Jin Yang and Huijia Liang and Hang Ren and Dongqing Jia and Xin Wang},
  doi          = {10.1016/j.neucom.2025.130053},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130053},
  shortjournal = {Neurocomputing},
  title        = {SAC: Collaborative learning of structure and content features for android malware detection framework},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DASC: Learning discriminative latent space for video
clustering. <em>NEUCOM</em>, <em>637</em>, 130050. (<a
href="https://doi.org/10.1016/j.neucom.2025.130050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant advancements have been made in video analysis technologies. However, most existing methods are primarily designed for supervised learning, particularly in video classification. Accurately labeling video data is often time-consuming and labor-intensive, making large-scale annotation challenging. As a result, most of the available video data remain in an unsupervised or weakly supervised state. This situation underscores the urgent need to develop efficient methods for unsupervised video data analysis, with a particular emphasis on video clustering techniques, which can effectively alleviate the high cost and labor intensity of video data annotation by automatically grouping similar videos, thus reducing the reliance on manual labeling. This significantly enhances the efficiency and scalability of video analysis. In this paper, we propose a deep aggregation subspace clustering (DASC) network, designed to learn a video-level self-representation matrix in an end-to-end manner, without the need for any labeled data, thus operating in an unsupervised learning environment. Specifically, DASC consists of four main components: auto-encoder backbone, video modeling module (VMM), self-representation module (SrM) and feature recovered module (FRM). A frame-level latent space is first established by utilizing the auto-encoder backbone. Then, a video-level latent space is established by constructing the VMM. Next, the video-level self-representation matrix is learned in the latent space by using the SrM. Finally, the video-level latent feature will be restored to frame-level features using the FRM. Experimental results on multiple benchmark datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_NEUCOM},
  author       = {Jiaxin Lin and Xizhan Gao and Zhihan Zhang and Haotian Deng},
  doi          = {10.1016/j.neucom.2025.130050},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130050},
  shortjournal = {Neurocomputing},
  title        = {DASC: Learning discriminative latent space for video clustering},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive symbiotic graph convolutional network.
<em>NEUCOM</em>, <em>637</em>, 130049. (<a
href="https://doi.org/10.1016/j.neucom.2025.130049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the intrinsic fabric of graph and network data, the latent reciprocity between network nodes forms a profound symbiotic relationship. Traditional graph neural networks struggle to adaptively parse and integrate multi-source features when learning these symbiotic relationships. To address this challenge, we propose a novel Adaptive Symbiotic Graph Convolutional Network (ASGCN) for semi-supervised node classification tasks. First, The initial contribution of this investigation is the introduction of a multi-scale feature convolution module, which enables the extraction of hierarchical features at varying scales and the construction of k-nearest neighbor graphs. This facilitates the deepening of the symbiotic features of nodes. Second, a symbiotic co-convolution module is put forth as a means of reinforcing the profound interdependence inherent to symbiotic relationships. Finally, an adaptive dynamic feature selection mechanism is introduced to flexibly respond to data characteristics, effectively identifying and fusing the most influential features in the processing information flow. Experimental results demonstrate that ASGCN exhibits significant advantages in deeply analyzing and integrating the intrinsic attributes of nodes with graph structural relationships, thereby improving performance in node classification tasks.},
  archive      = {J_NEUCOM},
  author       = {Lin Zhou and Yuzhi Xiao and Zhonglin Ye and Haixing Zhao and Zhen Liu},
  doi          = {10.1016/j.neucom.2025.130049},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130049},
  shortjournal = {Neurocomputing},
  title        = {Adaptive symbiotic graph convolutional network},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaGen: A framework for metaheuristic development and
hyperparameter optimization in machine and deep learning.
<em>NEUCOM</em>, <em>637</em>, 130046. (<a
href="https://doi.org/10.1016/j.neucom.2025.130046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter optimization is a pivotal step in enhancing model performance within machine learning. Traditionally, this challenge is addressed through metaheuristics, which efficiently explore large search spaces to uncover optimal solutions. However, implementing these techniques can be complex without adequate development tools, which is the primary focus of this paper. Hence, we introduce MetaGen , a novel Python package designed to provide a comprehensive framework for developing and evaluating metaheuristic algorithms. MetaGen follows best practices in Python design, ensuring minimalistic code implementation, intuitive comprehension, and full flexibility in solution representation. The package defines two distinct user roles: Developers, responsible for algorithm implementation for hyperparameter optimization, and Solvers, who leverage pre-implemented metaheuristics to address optimization problems. Beyond algorithm implementation, MetaGen facilitates benchmarking through built-in test functions, ensuring standardized performance comparisons. It also provides automated reporting and visualization tools to analyze optimization progress and outcomes effectively. Furthermore, its modular design allows distribution and integration into existing machine learning workflows. Several illustrative use cases are presented to demonstrate its adaptability and efficacy. The package, along with code, a user manual, and supplementary materials, is available at: https://github.com/Data-Science-Big-Data-Research-Lab/MetaGen .},
  archive      = {J_NEUCOM},
  author       = {David Gutiérrez-Avilés and Manuel Jesús Jiménez-Navarro and José Francisco Torres and Francisco Martínez-Álvarez},
  doi          = {10.1016/j.neucom.2025.130046},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130046},
  shortjournal = {Neurocomputing},
  title        = {MetaGen: A framework for metaheuristic development and hyperparameter optimization in machine and deep learning},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prescribed performance event-triggered optimal control of
nonlinear multi-input systems. <em>NEUCOM</em>, <em>637</em>, 130044.
(<a href="https://doi.org/10.1016/j.neucom.2025.130044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a prescribed performance reinforcement learning control (PPRLC) based on event-triggered mechanism for nonlinear multi-input systems, where target error is constrained to a bounded set. Firstly, the constrained optimal control problem is reformulated as an unconstrained stationary optimal problem by using prescribed performance function. Then, the event-triggered mechanism (ETM) is integrated to save communication resources and reduce data transmission volume. In order to study the solution of the Hamilton-Jacobi-Bellman equation (HJB), we use a reinforcement learning (RL) algorithm based on the single-critic neural network (NN) and introduce a new adaptive law to update the NN weights. Based on the Lyapunov function, the convergence of weights and the closed-loop stability of the system are confirmed. Finally, the correctness and effectiveness of the method are proved by a numerical simulation example.},
  archive      = {J_NEUCOM},
  author       = {Yu Tang and Yongfeng Lv and Jun Zhao and Long Jian and Linwei Li},
  doi          = {10.1016/j.neucom.2025.130044},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130044},
  shortjournal = {Neurocomputing},
  title        = {Prescribed performance event-triggered optimal control of nonlinear multi-input systems},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trend-aware spatio-temporal fusion graph convolutional
network with self-attention for traffic prediction. <em>NEUCOM</em>,
<em>637</em>, 130040. (<a
href="https://doi.org/10.1016/j.neucom.2025.130040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current traffic prediction methods often extract insufficient road network information, have difficulty in mining long-term temporal dependencies, and cause model performance decline due to uneven data distribution. To address the issues above, we propose a novel Spatio-Temporal Fusion Graph Convolutional Network with Trend-Aware(STFTGCN) for traffic prediction. It consists of Spatio Temporal Embedding, Spatio-Temporal Synchronous Graph Convolution, Temporal-Attention Module, and Trend-Aware Forecasting. By constructing and thus aggregating Spatial Distance Graph, Road Connection Graph, Geographic Correlation Graph, and the proposed Lagged Correlation Graph, the hidden information in the road network is fully extracted. Then, Multi-layer Spatio-Temporal Synchronous Graph Convolution captures local spatio-temporal correlations, while a sandwich structure combined Temporal Self-Attention and Temporal Trend-Aware Multi-Head Self-Attention effectively extracts long-term dependencies and responds to local traffic fluctuations. The Trend-Aware transformations method overcome uneven data distribution, improving node relationship matching and capturing dynamic traffic changes. Experiments results on real-world datasets (PEMS03, PEMS04, PEMS07, PEMS08, PEMS-BAY and METR-LA) demonstrate that the proposed STFTGCN outperforms baseline models, validating its effectiveness and practicality.},
  archive      = {J_NEUCOM},
  author       = {Xiongtao Zhang and Lijie Pan and Qing Shen and Zhenfang Liu and Jungang Lou and Yunliang Jiang},
  doi          = {10.1016/j.neucom.2025.130040},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130040},
  shortjournal = {Neurocomputing},
  title        = {Trend-aware spatio-temporal fusion graph convolutional network with self-attention for traffic prediction},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class-view graph knowledge distillation: A new idea for
learning MLPs on graphs. <em>NEUCOM</em>, <em>637</em>, 130035. (<a
href="https://doi.org/10.1016/j.neucom.2025.130035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs), while effective for processing non-Euclidean structured data, suffer from computationally intensive neighbor fetching, which hinders their deployment in low-latency applications. Cross-architecture graph knowledge distillation (KD), which trains high-performance Multi-layer Perceptrons (MLPs) to emulate teacher GNNs, offers a promising solution. However, existing GNN-MLP distillation methods rely on the sample-view KD paradigm, where student models directly mimic the teacher’s parameter space. Given the fundamentally different architectures and parameter spaces of GNNs and MLPs, this direct mimicry approach limits effective knowledge transfer. Inspired by the inherent properties of MLPs, we propose a novel class-view KD paradigm for GNN-MLP distillation. Unlike sample-view KD, our method guides student MLPs to generate more discriminative parameter configurations within their own parameter space while preserving the similarity of prediction distributions with the teacher, rather than directly imitating the teacher’s parameter configurations. Extensive experiments on public benchmark datasets demonstrate that class-view KD outperforms sample-view KD across various evaluation metrics and can be seamlessly integrated into existing GNN-MLP distillation methods to improve performance without additional computational cost. The code is available at https://github.com/xsk160/Class-View-Graph-KD .},
  archive      = {J_NEUCOM},
  author       = {Yingjie Tian and Shaokai Xu and Muyang Li},
  doi          = {10.1016/j.neucom.2025.130035},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130035},
  shortjournal = {Neurocomputing},
  title        = {Class-view graph knowledge distillation: A new idea for learning MLPs on graphs},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counterfactual experience augmented off-policy reinforcement
learning. <em>NEUCOM</em>, <em>637</em>, 130017. (<a
href="https://doi.org/10.1016/j.neucom.2025.130017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning control algorithms face significant challenges due to out-of-distribution and inefficient exploration problems. While model-based reinforcement learning enhances the agent’s reasoning and planning capabilities by constructing virtual environments, training such virtual environments can be very complex. In order to build an efficient inference model and enhance the representativeness of learning data, we propose the Counterfactual Experience Augmentation (CEA) algorithm. CEA leverages variational autoencoders to model the dynamic patterns of state transitions and introduces randomness to model non-stationarity. This approach focuses on expanding the learning data in the experience pool through counterfactual inference and performs exceptionally well in environments that follow the bisimulation assumption. Environments with bisimulation properties are usually represented by discrete observation and action spaces, we propose a sampling method based on maximum kernel density estimation entropy to extend CEA to various environments. By providing reward signals for counterfactual state transitions based on real information, CEA constructs a complete counterfactual experience to alleviate the out-of-distribution problem of the learning data, and outperforms general SOTA algorithms in environments with difference properties. Finally, we discuss the similarities, differences and properties of generated counterfactual experiences and real experiences. The code is available at https://github.com/Aegis1863/CEA .},
  archive      = {J_NEUCOM},
  author       = {Sunbowen Lee and Yicheng Gong and Chao Deng},
  doi          = {10.1016/j.neucom.2025.130017},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130017},
  shortjournal = {Neurocomputing},
  title        = {Counterfactual experience augmented off-policy reinforcement learning},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Centroid-based contrastive consistency learning for
transferable deepfake detection. <em>NEUCOM</em>, <em>637</em>, 130009.
(<a href="https://doi.org/10.1016/j.neucom.2025.130009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research efforts in deepfake detection mainly concentrated on identifying and differentiating artifacts discernible to humans. Those methods have left a bias in learned models, as they tend to concentrate on the disparities between forged and natural regions from the perspective of a single sample while overlooking consistency within categories from the perspective of the entire sample set, which remains crucial across various real-world applications. Therefore, inspired by contrastive learning, we tackle the deepfake detection problem by learning the invariant representations of both categories. Our proposed method, termed Centroid-based Contrastive Consistency Learning (C3L) method, integrates constraints on representations at both the data preprocessing and feature extraction stages. Specifically, during data preprocessing, we consider both temporal relationships within videos and the latent relationships within synthesis data. We introduce a novel Positive Enhancement Module (PEM) designed to characterize natural and forged samples in a facial semantically irrelevant way, thereby guiding a task-oriented positive pair contrasting strategy. In addition, at the feature extraction stage, we introduce the Margin Feature Simulation Module (MFSM), which leverages the centroid of the natural category to simulate marginal features for both categories. Subsequently, we employ the Supervised Contrastive Margin Loss (SCML), utilizing simulated features to emphasize differences at decision boundaries and optimize the learning process. The effectiveness and robustness of the proposed method have been demonstrated through extensive experiments.},
  archive      = {J_NEUCOM},
  author       = {Ruiqi Zha and Zhichao Lian and Qianmu Li},
  doi          = {10.1016/j.neucom.2025.130009},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130009},
  shortjournal = {Neurocomputing},
  title        = {Centroid-based contrastive consistency learning for transferable deepfake detection},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CITAL: Counterfactual intervention for temporal action
localization with point-level annotation. <em>NEUCOM</em>, <em>637</em>,
130006. (<a href="https://doi.org/10.1016/j.neucom.2025.130006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-supervised temporal action localization (PTAL) requires only a timestamp annotated on each action instance for training. Most existing PTAL methods use multiple instances learning (MIL) paradigm that localize actions according to contributions of the snippets to the classification results. The gap between classification and localization tasks causes the models to focus more on clues than pure actions. And the models are prone to localize fake actions when there are biased clues between training and test datasets. Inspired by earlier efforts on causal inference, we propose a counterfactual intervention framework for PTAL, CITAL for short. Counterfactual intervention simulates how models respond to counterfactual inputs that contains the same clues without action instances. Intuitively, we can obtain the real response to the pure actions by comparing responses to the inputs before and after counterfactual intervention. Specifically, we propose a background suppression (BS) block to suppresses the background response by guiding the model pay more attention to action instances rather than clues. To fuse the output scores of the various inputs, we propose a fusing by imitation (FI) strategy that further modifies the scores to have a high response to actions and low response to the background segments, generating more accurate proposals. Besides, we propose a counterfactual example generation (CEG) block to generate counterfactual examples with only clues and background contents based on the point labels and snippet-level action scores. Our approach achieves significant mAP gains on THUMOS14, BEOID and GTEA benchmarks comparing to various CAS-based methods without introducing additional parameters.},
  archive      = {J_NEUCOM},
  author       = {Yongxiang Hu and Ziying Xia and Zichong Chen and Thupten Tsering and Jian Cheng and Tashi Nyima},
  doi          = {10.1016/j.neucom.2025.130006},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {130006},
  shortjournal = {Neurocomputing},
  title        = {CITAL: Counterfactual intervention for temporal action localization with point-level annotation},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting accuracy of student models via masked adaptive
self-distillation. <em>NEUCOM</em>, <em>637</em>, 129988. (<a
href="https://doi.org/10.1016/j.neucom.2025.129988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has achieved impressive success, yet conventional KD approaches are time-consuming and computationally costly. In contrast, self-distillation methods provide an efficient alternative. However, existing self-distillation methods mostly suffer from information redundancy due to the same network architecture from the teacher and student models. Additionally, they simultaneously face the inherent limitation of lacking a high-capacity teacher model. To cope with the above challenges, we propose a novel and efficient method named Masked Adaptive Self-Distillation (MASD). Specifically, we first introduce the Mask Generation Module, which masks random pixels of the feature maps and force it to reconstruct and refine more valuable features on different layers. Moreover, the Adaptive Weighting Mechanism is designed to dynamically adjust and optimize the weights of supervisory signals utilizing the probabilities from the mutual masked supervisory signals, thereby compensating the absence of high-capacity teacher model. We demonstrate the effectiveness of our MASD method on conventional image classification datasets and fine-grained datasets using state-of-the-art CNN architectures, and show that MASD significantly enhances the generalization of various backbone networks. For instance, on the CIFAR-100 classification benchmark, the proposed MASD method achieves an accuracy of 80.40% with the ResNet-18 architecture, surpassing the baseline with a 4.16% margin in Top-1 accuracy.},
  archive      = {J_NEUCOM},
  author       = {Haoran Zhao and Shuwen Tian and Jinlong Wang and Zhaopeng Deng and Xin Sun and Junyu Dong},
  doi          = {10.1016/j.neucom.2025.129988},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {129988},
  shortjournal = {Neurocomputing},
  title        = {Boosting accuracy of student models via masked adaptive self-distillation},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature calibration and feature separation for long-tailed
visual recognition. <em>NEUCOM</em>, <em>637</em>, 129983. (<a
href="https://doi.org/10.1016/j.neucom.2025.129983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tailed data classification is prevalent in real-world scenarios, but training on such datasets can lead to biased classifications and poor performance. We address this challenge by focusing on improving feature representation for tail classes, which is often lower in quality due to their closer proximity to other distinct classes. Inspired by the similarity between head and tail classes, we propose Class-wise Knowledge Distillation (CKD) to help tail classes learn prediction distributions from head classes, thus calibrating their features. Additionally, we introduce Hard Negative Samples Sampling (HNSS) to enhance feature separation by selecting challenging negative examples for contrastive learning. Our Feature Calibration and Feature Separation (FCFS) method achieves competitive results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks, demonstrating effective feature learning for long-tailed classification problems. This approach leverages both knowledge distillation and hard negative sampling to improve model performance.},
  archive      = {J_NEUCOM},
  author       = {Qianqian Wang and Fangyu Zhou and Xiangge Zhao and Yangtao Lin and Haibo Ye},
  doi          = {10.1016/j.neucom.2025.129983},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {129983},
  shortjournal = {Neurocomputing},
  title        = {Feature calibration and feature separation for long-tailed visual recognition},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPEC: Dual-path error compensation for low-light image
enhancement. <em>NEUCOM</em>, <em>637</em>, 129980. (<a
href="https://doi.org/10.1016/j.neucom.2025.129980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the task of low-light image enhancement, deep learning-based algorithms have demonstrated superiority and effectiveness compared to traditional methods. However, these methods, primarily based on Retinex theory, tend to overlook the noise and color distortions in input images, leading to significant noise amplification and local color distortions in enhanced results. To address these issues, we propose the Dual-Path Error Compensation (DPEC) method, designed to improve image quality under low-light conditions by preserving local texture details while restoring global image brightness without amplifying noise. DPEC incorporates precise pixel-level error estimation to capture subtle differences and an independent denoising mechanism to prevent noise amplification. We introduce the HIS-Retinex loss to guide DPEC’s training, ensuring the brightness distribution of enhanced images closely aligns with real-world conditions. To balance computational speed and resource efficiency while training DPEC for a comprehensive understanding of the global context, we integrated the VMamba architecture into its backbone. Comprehensive quantitative and qualitative experimental results demonstrate that our algorithm significantly outperforms state-of-the-art methods in low-light image enhancement. The code is publicly available online at https://github.com/wangshuang233/DPEC .},
  archive      = {J_NEUCOM},
  author       = {Shuang Wang and Qianwen Lu and Boxing Peng and Yihe Nie and Qingchuan Tao},
  doi          = {10.1016/j.neucom.2025.129980},
  journal      = {Neurocomputing},
  month        = {7},
  pages        = {129980},
  shortjournal = {Neurocomputing},
  title        = {DPEC: Dual-path error compensation for low-light image enhancement},
  volume       = {637},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
