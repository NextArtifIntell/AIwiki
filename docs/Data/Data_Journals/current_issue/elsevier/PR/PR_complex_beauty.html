<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr---31">PR - 31</h2>
<ul>
<li><details>
<summary>
(2025). Video summarization with temporal-channel visual
transformer. <em>PR</em>, <em>165</em>, 111631. (<a
href="https://doi.org/10.1016/j.patcog.2025.111631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization task has gained widespread interest, benefiting from its valuable capabilities for efficient video browsing. Existing approaches generally focus on inter-frame temporal correlations, which may not be sufficient to identify crucial content because of the limited useful details that can be gleaned. To resolve these issues, we propose a novel transformer-based approach for video summarization, called Temporal-Channel Visual Transformer (TCVT). The proposed TCVT consists of three components, including a dual-stream embedding module, an inter-frame encoder, and an intra-segment encoder. The dual-stream embedding module creates the fusion embedding sequence by extracting visual features and short-range optical features, preserving appearance and motion details. The temporal-channel inter-frame correlations are learned by the inter-frame encoder with multiple temporal and channel attention modules. Meanwhile, the intra-segment representations are captured by the intra-segment encoder for the local temporal context modeling. Finally, we fuse the frame-level and segment-level representations for the frame-wise importance score prediction. Our network outperforms state-of-the-art methods on two benchmark datasets, with improvements from 55.3% to 56.9% on the SumMe dataset and from 69.3% to 70.4% on the TVSum dataset.},
  archive      = {J_PR},
  author       = {Xiaoyan Tian and Ye Jin and Zhao Zhang and Peng Liu and Xianglong Tang},
  doi          = {10.1016/j.patcog.2025.111631},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111631},
  shortjournal = {Pattern Recognition},
  title        = {Video summarization with temporal-channel visual transformer},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniHDSA: A unified relation prediction approach for
hierarchical document structure analysis. <em>PR</em>, <em>165</em>,
111617. (<a href="https://doi.org/10.1016/j.patcog.2025.111617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to restore the hierarchical structure of documents created using authoring software with hierarchical schemas. Previous research has primarily followed two approaches: one focuses on tackling specific subtasks of HDSA in isolation, such as table detection or reading order prediction, while the other adopts a unified framework that uses multiple branches or modules, each designed to address a distinct task. In this work, we propose a unified relation prediction approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as relation prediction problems and consolidates relation prediction labels into a unified label space. This allows a single relation prediction module to handle multiple tasks simultaneously, whether at a page-level or document-level structure analysis. By doing so, our approach significantly reduces the risk of cascading errors and enhances systemâ€™s efficiency, scalability, and adaptability. To validate the effectiveness of UniHDSA, we develop a multimodal end-to-end system based on Transformer architectures. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance on a hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on a large-scale document layout analysis dataset, DocLayNet, effectively illustrating the superiority of our method across all sub-tasks.},
  archive      = {J_PR},
  author       = {Jiawei Wang and Kai Hu and Qiang Huo},
  doi          = {10.1016/j.patcog.2025.111617},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111617},
  shortjournal = {Pattern Recognition},
  title        = {UniHDSA: A unified relation prediction approach for hierarchical document structure analysis},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cascade residual learning based adaptive feature aggregation
for light field super-resolution. <em>PR</em>, <em>165</em>, 111616. (<a
href="https://doi.org/10.1016/j.patcog.2025.111616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) super-resolution aims to enhance the spatial or angular resolutions of LF images. Most existing methods tend to decompose 4D LF images into multiple 2D subspaces such as spatial, angular, and epipolar plane image (EPI) domains, and devote efforts to designing various feature extractors for each subspace domain. However, it remains challenging to select an effective multi-domain feature fusion strategy, including the fusion order and structure. To this end, this paper proposes an adaptive feature aggregation framework based on cascade residual learning, which can adaptively select feature aggregation strategies through learning rather than designed artificially. Specifically, we first employ three types of 2D feature extractors for spatial, angular, and EPI feature extraction, respectively. Then, an adaptive feature aggregation (AFA) module is designed to cascade these feature extractors through multi-level residual connections. This design enables the network to flexibly aggregate various subspace features without introducing additional parameters. We conduct comprehensive experiments on both real-world and synthetic LF datasets for light field spatial super-resolution (LFSSR) and light field angular super-resolution (LFASR). Quantitative and visual comparisons demonstrate that our model achieves state-of-the-art super-resolution (SR) performance. The code is available at https://github.com/haozhang25/AFA-LFSR .},
  archive      = {J_PR},
  author       = {Hao Zhang and Wenhui Zhou and Lili Lin and Andrew Lumsdaine},
  doi          = {10.1016/j.patcog.2025.111616},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111616},
  shortjournal = {Pattern Recognition},
  title        = {Cascade residual learning based adaptive feature aggregation for light field super-resolution},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised polarization image dehazing method via
frequency domain generative adversarial networks. <em>PR</em>,
<em>165</em>, 111615. (<a
href="https://doi.org/10.1016/j.patcog.2025.111615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze significantly hinders the application of autonomous driving, traffic surveillance, and remote sensing. Image dehazing serves as a key technology to enhance the clarity of images captured in hazy conditions. However, the lack of paired annotated training data significantly limits the performance of deep learning-based dehazing methods in real-world scenarios. In this work, we propose a self-supervised polarization image dehazing framework based on frequency domain generative adversarial networks. By incorporating a polarization calculation module into the generator, the Stokes parameters of airlight are accurately estimated, which are used to reconstruct the synthesized hazy image by combining the dehazed image generated via a densely connected encoder-decoder. Furthermore, we optimize the discriminator with frequency domain features extracted by frequency decomposition module and introduce a pseudo airlight coefficient supervision loss to enhance the self-supervised training. By discriminating between synthetic hazy images and real hazy images, we achieve adversarial training without the need for paired data. Simultaneously, supervised by the atmospheric scattering model, our network can iteratively generate more realistic dehazed images. Extensive experiments conducted on the constructed multi-view polarization datasets demonstrate that our method achieves state-of-the-art performance without requiring real-world ground truth.},
  archive      = {J_PR},
  author       = {Rui Sun and Long Chen and Tanbin Liao and Zhiguo Fan},
  doi          = {10.1016/j.patcog.2025.111615},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111615},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised polarization image dehazing method via frequency domain generative adversarial networks},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual fidelity and full-scale interaction driven network
for infrared and visible image fusion. <em>PR</em>, <em>165</em>,
111612. (<a href="https://doi.org/10.1016/j.patcog.2025.111612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of infrared and visible image fusion is to combine the unique strengths of source images into a single image that serves human visual perception and machine detection. The existing fusion networks are still lacking in the effective characterization and retention of source image features. To counter these deficiencies, we propose a visual fidelity and full-scale interaction driven network for infrared and visible image fusion, named VFFusion. First, a multi-scale feature encoder based on BiFormer is constructed, and a feature cascade interaction module is designed to perform full-scale interaction on features distributed across different scales. In addition, a visual fidelity branch is built to process multi-scale features in parallel with the fusion branch. Specifically, the visual fidelity branch uses blurred images for self-supervised training in the constructed auxiliary task, thereby obtaining an effective representation of the source image information. By exploring the complementary representational features of infrared and visible images as supervisory information, it constrains the fusion branch to retain the source image features in the fused image. Notably, the visual fidelity branch employs a multi-scale joint reconstruction loss, utilizing the rich supervisory signals provided by multi-scale original images to enhance the feature representation of targets at different scales, resulting in clear fusion of the targets. Extensive qualitative and quantitative comparative experiments are conducted on four datasets against nine advanced methods, demonstrating the superiority of our approach. The source code is available at https://github.com/XingLongH/VFFusion .},
  archive      = {J_PR},
  author       = {Liye Mei and Xinglong Hu and Zhaoyi Ye and Zhiwei Ye and Chuan Xu and Sheng Liu and Cheng Lei},
  doi          = {10.1016/j.patcog.2025.111612},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111612},
  shortjournal = {Pattern Recognition},
  title        = {Visual fidelity and full-scale interaction driven network for infrared and visible image fusion},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rank-revealing fully-connected tensor network decomposition
and its application to tensor completion. <em>PR</em>, <em>165</em>,
111610. (<a href="https://doi.org/10.1016/j.patcog.2025.111610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully-connected tensor network (FCTN) decomposition has become a powerful tool for handling high-dimensional data. However, for a given N th-order data, N ( N âˆ’ 1 ) / 2 tuning parameters (i.e., FCTN rank) in FCTN decomposition is a tricky challenge, which hinders its wide deployments. Although many recent works have emerged to adaptively search for a (near)-optimal FCTN rank, these methods suffer from expensive computational costs since they require too many search and evaluation processes, significantly limiting their applications to high-dimensional data. To tackle the above challenges, we develop a rank-revealing FCTN (revealFCTN) decomposition, whose FCTN rank is adaptively and efficiently inferred. More specifically, by analyzing the sizes of the sub-network tensors in the FCTN decomposition, we establish the equivalent relationships between the FCTN rank and the ranks of single-mode and double-mode unfolding matrices of the given data. The FCTN rank can be directly revealed through the ranks of these unfolding matrices, which does not require any search and evaluation process, making the computational cost almost negligible compared to the search-based methods. To evaluate the performance of the developed revealFCTN decomposition, we test its performance on a representative task: tensor completion (TC). Comprehensive experimental results demonstrate that our method outperforms several state-of-the-art methods, achieving a MPSNR gain of around 1 dB in most cases compared to the original FCTN decomposition.},
  archive      = {J_PR},
  author       = {Yun-Yang Liu and Xi-Le Zhao and Gemine Vivone},
  doi          = {10.1016/j.patcog.2025.111610},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111610},
  shortjournal = {Pattern Recognition},
  title        = {Rank-revealing fully-connected tensor network decomposition and its application to tensor completion},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online asymmetric supervised discrete cross-modal hashing
for streaming multimedia data. <em>PR</em>, <em>165</em>, 111604. (<a
href="https://doi.org/10.1016/j.patcog.2025.111604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal online hashing, which uses freshly received data to retrain the hash function gradually, has become a research hotspot as a means of handling the massive amounts of streaming data that have been brought about by the fast growth of multimedia technology and the popularity of portable devices. However, in the process of processing stream data in most methods, on the one hand, the relationship between modal classes and the common features between label vectors and binary codes is not fully explored. On the other hand, the semantic information in the old and new data modes is not fully utilized. In this post, we offer Online Asymmetric Supervised Discrete Cross-Modal Hashing for Streaming Multimedia Data (OASCH) as a solution. This study integrates the concept cognition mechanism of dynamic incremental samples and an asymmetric knowledge guidance mechanism into the online hash learning framework. The proposed algorithmic model takes into account the knowledge similarity between newly arriving data and the existing dataset, as well as the knowledge similarity within the new data itself. It projects the hash codes associated with new incoming sample data into the potential space of concept cognition. By doing so, the model maximizes the mining of implicit semantic similarities within streaming data across different time points, resulting in the generation of compact hash codes with enhanced discriminative power, we further propose an adaptive edge regression strategy. Our method surpasses several current sophisticated cross-modal hashing techniques regarding both retrieval efficiency and search accuracy, according to studies on three publicly available multimedia retrieval datasets.},
  archive      = {J_PR},
  author       = {Fan Yang and Xinqi Liu and Fumin Ma and Xiaojian Ding and Kaixiang Wang},
  doi          = {10.1016/j.patcog.2025.111604},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111604},
  shortjournal = {Pattern Recognition},
  title        = {Online asymmetric supervised discrete cross-modal hashing for streaming multimedia data},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain anatomy prior modeling to forecast clinical
progression of cognitive impairment with structural MRI. <em>PR</em>,
<em>165</em>, 111603. (<a
href="https://doi.org/10.1016/j.patcog.2025.111603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain structural MRI has been widely used to assess the future progression of cognitive impairment (CI). Previous learning-based studies usually suffer from the issue of small-sized labeled training data, while a huge amount of structural MRIs exist in large-scale public databases. Intuitively, brain anatomical structures derived from these public MRIs (even without task-specific label information) can boost CI progression trajectory prediction. However, previous studies seldom use such brain anatomy structure information as priors. To this end, this paper proposes a brain anatomy prior modeling (BAPM) framework to forecast the clinical progression of cognitive impairment with small-sized target MRIs by exploring anatomical brain structures. Specifically, the BAPM consists of a pretext model and a downstream model , with a shared brain anatomy-guided encoder to model brain anatomy prior using auxiliary tasks explicitly. Besides the encoder, the pretext model also contains two decoders for two auxiliary tasks ( i.e. , MRI reconstruction and brain tissue segmentation), while the downstream model relies on a predictor for classification. The brain anatomy-guided encoder is pre-trained with the pretext model on 9,344 auxiliary MRIs without diagnostic labels for anatomy prior modeling. With this encoder frozen, the downstream model is then fine-tuned on limited target MRIs for prediction. We validate BAPM on two CI-related studies with T1-weighted MRIs from 448 subjects. Experimental results suggest the effectiveness of BAPM in (1) four CI progression prediction tasks, (2) MR image reconstruction, and (3) brain tissue segmentation, compared with several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Lintao Zhang and Jinjian Wu and Lihong Wang and Li Wang and David C. Steffens and Shijun Qiu and Guy G. Potter and Mingxia Liu},
  doi          = {10.1016/j.patcog.2025.111603},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111603},
  shortjournal = {Pattern Recognition},
  title        = {Brain anatomy prior modeling to forecast clinical progression of cognitive impairment with structural MRI},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSKA: Multi-stream keypoint attention network for sign
language recognition and translation. <em>PR</em>, <em>165</em>, 111602.
(<a href="https://doi.org/10.1016/j.patcog.2025.111602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language serves as a non-vocal means of communication, transmitting information and significance through gestures, facial expressions, and bodily movements. The majority of current approaches for sign language recognition (SLR) and translation rely on RGB video inputs, which are vulnerable to fluctuations in the background. Employing a keypoint-based strategy not only mitigates the effects of background alterations but also substantially diminishes the computational demands of the model. Nevertheless, contemporary keypoint-based methodologies fail to fully harness the implicit knowledge embedded in keypoint sequences. To tackle this challenge, our inspiration is derived from the human cognition mechanism, which discerns sign language by analyzing the interplay between gesture configurations and supplementary elements. We propose a multi-stream keypoint attention network to depict a sequence of keypoints produced by a readily available keypoint estimator. In order to facilitate interaction across multiple streams, we investigate diverse methodologies such as keypoint fusion strategies, head fusion, and self-distillation. The resulting framework is denoted as MSKA-SLR, which is expanded into a sign language translation (SLT) model through the straightforward addition of an extra translation network. We carry out comprehensive experiments on well-known benchmarks like Phoenix-2014, Phoenix-2014T, and CSL-Daily to showcase the efficacy of our methodology. Notably, we have attained a novel state-of-the-art performance in the sign language translation task of Phoenix-2014T. The code and models can be accessed at: https://github.com/sutwangyan/MSKA .},
  archive      = {J_PR},
  author       = {Mo Guan and Yan Wang and Guangkun Ma and Jiarui Liu and Mingzu Sun},
  doi          = {10.1016/j.patcog.2025.111602},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111602},
  shortjournal = {Pattern Recognition},
  title        = {MSKA: Multi-stream keypoint attention network for sign language recognition and translation},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptively robust high-order tensor factorization for
low-rank tensor reconstruction. <em>PR</em>, <em>165</em>, 111600. (<a
href="https://doi.org/10.1016/j.patcog.2025.111600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, various approaches have been proposed for tensor reconstruction from incomplete and contaminated data. However, most algorithms focus on third-order tensors, neglecting higher-order tensors that are common in real-world applications. Additionally, many studies use LASSO-type penalties or second-order statistics to capture noise patterns, which may not perform well with dense and gross outliers. To address these challenges, we propose a novel robust high-order tensor recovery model that simultaneously removes complex noise and completes missing entries. We introduce a factor Frobenius norm for the low-rank structures of high-order tensors and derive a nonconvex function via the L 2 criterion. An estimation algorithm is developed using the alternating minimization method. Our method jointly estimates tensor terms of interest and precision parameters, adapting to noise patterns for data-driven robustness. We analyze the convergence properties of our algorithm, and numerical experiments validate its superiority in natural image reconstruction, video restoration, and background modeling compared to state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zihao Song and Yongyong Chen and Zhao Weihua},
  doi          = {10.1016/j.patcog.2025.111600},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111600},
  shortjournal = {Pattern Recognition},
  title        = {Adaptively robust high-order tensor factorization for low-rank tensor reconstruction},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional trained tree-structured decoder for
handwritten mathematical expression recognition. <em>PR</em>,
<em>165</em>, 111599. (<a
href="https://doi.org/10.1016/j.patcog.2025.111599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Handwritten Mathematical Expression Recognition (HMER) task is a critical branch in the field of Optical Character Recognition (OCR). Recent studies have demonstrated that incorporating bidirectional context information significantly improves the performance of HMER models. However, existing methods fail to effectively utilize bidirectional context information during the inference stage. Furthermore, current bidirectional training methods are primarily designed for string decoders and cannot adequately generalize to tree decoders, which offer superior generalization capabilities and structural analysis capacity. To overcome these limitations, we propose the Mirror-Flipped Symbol Layout Tree (MF-SLT) and Bidirectional Asynchronous Training (BAT) structure. Our method extends the bidirectional training strategy to the tree decoder, enabling more effective training by leveraging bidirectional information. Additionally, we analyze the impact of the visual and linguistic perception of the HMER model separately and introduce the Shared Language Modeling (SLM) mechanism. Through the SLM, we enhance the modelâ€™s robustness and generalization when dealing with visual ambiguity, especially in scenarios with abundant training data. Our approach has been validated through extensive experiments, demonstrating its ability to achieve new state-of-the-art results on the CROHME 2014, 2016, and 2019 datasets, as well as the HME100K dataset. The code used in our experiments will be publicly available at https://github.com/Hanbo-Cheng/BAT.git .},
  archive      = {J_PR},
  author       = {Hanbo Cheng and Chenyu Liu and Pengfei Hu and Zhenrong Zhang and Jiefeng Ma and Jun Du},
  doi          = {10.1016/j.patcog.2025.111599},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111599},
  shortjournal = {Pattern Recognition},
  title        = {Bidirectional trained tree-structured decoder for handwritten mathematical expression recognition},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentanglement and codebook learning-induced feature match
network to diagnose neurodegenerative diseases on incomplete multimodal
data. <em>PR</em>, <em>165</em>, 111597. (<a
href="https://doi.org/10.1016/j.patcog.2025.111597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal data can provide complementary information to diagnose neurodegenerative diseases (NDs). However, image quality variations and high costs can result in the missing data problem. Although incomplete multimodal data can be projected onto a common space, the traditional projection process may increase alignment errors and lose some modality-specific information. A disentanglement and codebook learning-induced feature match network (DCFMnet) is proposed in this study to solve the aforementioned issues. First, multimodal data are disentangled into latent modality-common and -specific features to help preserve modality-specific information in the subsequent alignment of multimodal data. Second, the latent modal features of all available data are aligned into a common space to reduce alignment errors and fused to achieve ND diagnosis. Moreover, the latent modal features of the modality with missing data are explored in online updated feature codebooks. Last, DCFMnet is tested on two publicly available datasets to illustrate its excellent performance in ND diagnosis.},
  archive      = {J_PR},
  author       = {Wei Xiong and Tao Wang and Xiumei Chen and Yue Zhang and Wencong Zhang and Qianjin Feng and Meiyan Huang and Alzheimerâ€™s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.patcog.2025.111597},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111597},
  shortjournal = {Pattern Recognition},
  title        = {Disentanglement and codebook learning-induced feature match network to diagnose neurodegenerative diseases on incomplete multimodal data},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-world nighttime image dehazing using contrastive and
adversarial learning. <em>PR</em>, <em>165</em>, 111596. (<a
href="https://doi.org/10.1016/j.patcog.2025.111596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nighttime image dehazing is a challenging task due to the scarcity of real hazy images and the domain gap between synthetic and real data. To address these challenges, we propose a novel deep learning framework that integrates contrastive and adversarial learning. In the initial training phase, the dehazing generator is trained on synthetic data to produce dehazed images that closely match the ground truths while maintaining a significant distance from the original hazy images through contrastive learning. Simultaneously, the contrastive learning encoder is updated to enhance its ability to distinguish between the dehazed images and ground truths, thereby increasing the difficulty of the dehazing task and pushing the generator to fully exploit feature information for improved results. To bridge the gap between synthetic and real data, the model is fine-tuned using a small set of real hazy images. To mitigate bias from the limited amount of real data, an additional constraint is applied to regulate model adjustments during fine-tuning. Empirical evaluation on multiple benchmark datasets demonstrates that our model outperforms state-of-the-art methods, providing an effective solution for improving visibility in hazy nighttime images by effectively leveraging both synthetic and real data.},
  archive      = {J_PR},
  author       = {Jingwen Deng and Patrick P.K. Chan and Daniel S. Yeung},
  doi          = {10.1016/j.patcog.2025.111596},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111596},
  shortjournal = {Pattern Recognition},
  title        = {Real-world nighttime image dehazing using contrastive and adversarial learning},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSDC-NET: Semi-supervised superficial OCTA vessel
segmentation for false positive reduction. <em>PR</em>, <em>165</em>,
111592. (<a href="https://doi.org/10.1016/j.patcog.2025.111592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate vessel segmentation in Optical Coherence Tomography Angiography (OCTA) is essential for ocular disease diagnosis, monitoring, and treatment assessment. However, most current automatic segmentation methods overlook false positives in the segmentation results, leading to potential misdiagnosis and delayed treatment. To address this issue, we propose a Dynamic Spatial Semi-Supervised Vessel Segmentation with Dual Topological Consistency (DSDC-NET) for retinal superficial OCTA images. The network integrates a Dynamic Spatial Attention Mechanism that combines snake-shaped convolution, which captures tubular fine structures, with spatial attention to suppress background noise and artefacts. This design enhances vessel region responses while accurately capturing complex local structures, thereby reducing false positives arising from inaccurate localisation of vessel details. Furthermore, Dual Topological Consistency Loss integrates the Persistent Homology features of the vessel system with the topological skeleton features of major vessels, enhancing branching pattern recognition. A Warm-up mechanism balances the focus of the network between major and branch vessels across training phases, mitigating false positives from inadequate branching structure learning. Comprehensive evaluations on ROSE-1, OCTA-500, and ROSSA datasets demonstrate the superiority of DSDC-NET over existing methods. Notably, DSDC-NET effectively reduces the false discovery rate and improves segmentation accuracy, validating its effectiveness in reducing false positives.},
  archive      = {J_PR},
  author       = {Xinyi Liu and Hailan Shen and Wenyan Zhong and Wanqing Xiong and Zailiang Chen},
  doi          = {10.1016/j.patcog.2025.111592},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111592},
  shortjournal = {Pattern Recognition},
  title        = {DSDC-NET: Semi-supervised superficial OCTA vessel segmentation for false positive reduction},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizable person re-identification method using
bi-stream interactive learning with feature reconstruction. <em>PR</em>,
<em>165</em>, 111591. (<a
href="https://doi.org/10.1016/j.patcog.2025.111591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that metric learning and representation learning are two main methods to improve the generalization ability of pedestrian re-identification models. However, their relationship has not been fully explored. Unlike GANsâ€™ emphasis on adversarial learning, our objective is to develop an interactive and synergistic learning framework for them. To achieve this, we propose a generalized pedestrian re-identification method using bi-stream interactive learning. One of the learning streams is the correlation graph sampler (CGS) for metric learning, and the other learning stream is the global sparse attention network (GSANet) for representation learning. We establish an intrinsic connection between these two learning streams. Unlike many existing methods that have high memory and computation costs or lack learning ability, CGS provides a more efficient and effective solution. CGS uses local sensitive hashing and feature metrics to construct the nearest neighbor graph for all categories at the beginning of training, which ensures that each batch of training samples contains randomly selected base categories and their nearest neighbor categories, providing strong similarity and challenging learning examples. As CGS sampling performance is affected by the quality of the feature map, we propose a global feature sparse reconstruction module to enhance the global self-correlation of the feature map extracted by the backbone network. Additionally, we extensively evaluate our method on large-scale datasets, including CUHK03, Market-1501, and MSMT17, and our method outperforms current state-of-the-art methods. These results confirm the effectiveness of our method and demonstrate its potential in pedestrian re-identification applications.},
  archive      = {J_PR},
  author       = {Feng Min and Yuhui Liu and Yixin Mao},
  doi          = {10.1016/j.patcog.2025.111591},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111591},
  shortjournal = {Pattern Recognition},
  title        = {Generalizable person re-identification method using bi-stream interactive learning with feature reconstruction},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eye-SCAN: Eye-movement-attention-based spatial channel
adaptive network for traffic accident prediction. <em>PR</em>,
<em>165</em>, 111590. (<a
href="https://doi.org/10.1016/j.patcog.2025.111590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of using visual cues extracted from DashCam video data to predict future accidents, understanding the dynamic spatio-temporal interactions in driving scenarios poses a major challenge. Given that the gaze attention information of experienced drivers during the driving process involves complex spatio-temporal interactions, this information can provide valuable guidance for training accident prediction models. Therefore, we propose an Eye-Movement-Attention-based Spatial Channel Adaptive Network (Eye-SCAN) for traffic accident prediction, which can efficiently learn multi-scale spatial channel information from driver gaze data. To integrate potential guidance information from driver eye movement information (EyeInfo) into Eye-SCAN, we propose two sub-modules in our model: the Spatial Adaptive Module (SAM), which helps Eye-SCAN adaptively learn low-dimensional spatial features of EyeInfo; and the Channel Adaptive Module (CAM), which aids Eye-SCAN to adaptively learning high-dimensional channel features of EyeInfo. Additionally, we introduce a novel recursive transmission strategy for temporal information to mitigate the impact of varying past results on the modelâ€™s current inferences. Experimental results demonstrate that our model outperforms state-of-the-art methods on two benchmark datasets, highlighting the contributions of each component and offering an effective solution for enhancing the safety of intelligent vehicles.},
  archive      = {J_PR},
  author       = {Xiaohui Yang and Yu Qiao and Tongzhen Si and Jing Wang and Tao Xu},
  doi          = {10.1016/j.patcog.2025.111590},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111590},
  shortjournal = {Pattern Recognition},
  title        = {Eye-SCAN: Eye-movement-attention-based spatial channel adaptive network for traffic accident prediction},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated subset selection and bandwidth estimation
algorithm for geographically weighted regression. <em>PR</em>,
<em>165</em>, 111589. (<a
href="https://doi.org/10.1016/j.patcog.2025.111589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a mathematical programming-based algorithm for the integrated selection of variable subsets and bandwidth estimation in geographically weighted regression, a local regression method that allows the kernel bandwidth and regression coefficients to vary across study areas. Unlike standard approaches in the literature, in which bandwidth and regression parameters are estimated separately for each focal point on the basis of different criteria, our model uses a single objective function for the integrated estimation of regression and bandwidth parameters across all focal points, based on the regression likelihood function and variance modeling. The proposed model further integrates a procedure to select a single subset of independent variables for all focal points, whereas existing approaches may return heterogeneous subsets across focal points. We then propose an alternative direction method to solve the nonconvex mathematical model and show that it converges to a partial minimum. The computational experiment indicates that the proposed algorithm provides competitive explanatory power with stable spatially varying patterns, with the ability to select the best subset and account for additional constraints.},
  archive      = {J_PR},
  author       = {Hyunwoo Lee and Young Woong Park},
  doi          = {10.1016/j.patcog.2025.111589},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111589},
  shortjournal = {Pattern Recognition},
  title        = {Integrated subset selection and bandwidth estimation algorithm for geographically weighted regression},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Granular-ball computing-based random walk for anomaly
detection. <em>PR</em>, <em>165</em>, 111588. (<a
href="https://doi.org/10.1016/j.patcog.2025.111588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a key task in data mining, which has been successfully employed in many practical scenarios. However, most existing methods usually analyze the anomalous characteristics of samples at a single and finest granularity, which leads to high computational cost and low efficiency. As one of the significant mathematical models in the theory of granular computing, granular-ball computing can portray the distributional characteristics of data from a multi-granularity perspective. For this reason, this paper proposes an unsupervised anomaly detection method based on granular-ball computing. Firstly, the samples are covered by generating adaptive granular-balls, and the multi-granularity information represented by granular-balls with different sizes can reflect the data distribution characteristics of the corresponding region. Secondly, the granular-balls are used to fit the samples for constructing a state transfer matrix in Random walk. Then, the steady-state distribution is generated using iterative computation and is normalized as the degree of anomaly for each granular-ball. Finally, the anomaly score for each sample is computed by relating the anomaly degree of each granular-ball to the samples it covers. Comparative experiments show that the proposed anomaly detection method performs well on multiple datasets, demonstrating its feasibility and superiority in practical applications. The code is publicly available online at https://github.com/optimusprimeyy/GBRAD .},
  archive      = {J_PR},
  author       = {Sihan Wang and Zhong Yuan and Shitong Cheng and Hongmei Chen and Dezhong Peng},
  doi          = {10.1016/j.patcog.2025.111588},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111588},
  shortjournal = {Pattern Recognition},
  title        = {Granular-ball computing-based random walk for anomaly detection},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain consistency learning for continual test-time
adaptation in image semantic segmentation. <em>PR</em>, <em>165</em>,
111585. (<a href="https://doi.org/10.1016/j.patcog.2025.111585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the open-world scenario, the challenge of distribution shift persists. Test-time adaptation adjusts the model during test-time to fit the target domainâ€™s data, addressing the distribution shift between the source and target domains. However, test-time adaptation methods still face significant challenges with continuously changing data distributions, especially since there are few methods applicable to continual test-time adaptation in image semantic segmentation. Furthermore, inconsistent semantic representations across different domains result in catastrophic forgetting in continual test-time adaptation. This paper focuses on the problem of continual test-time adaptation in semantic segmentation tasks and proposes a method named domain consistency learning for continual test-time adaptation. We mitigate catastrophic forgetting through feature-level and prediction-level consistency learning. Specifically, we propose domain feature consistency learning and class awareness consistency learning to guide model learning, enabling the target domain model to extract generalized knowledge. Additionally, to mitigate error accumulation, we propose a novel value-based sample selection method that jointly considers the pseudo-label confidence and style representativeness of the test images. Extensive experiments on widely-used semantic segmentation benchmarks demonstrate that our approach achieves satisfactory performance compared to state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yanyu Ye and Wei Wei and Lei Zhang and Chen Ding and Yanning Zhang},
  doi          = {10.1016/j.patcog.2025.111585},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111585},
  shortjournal = {Pattern Recognition},
  title        = {Domain consistency learning for continual test-time adaptation in image semantic segmentation},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Layerlink: Bridging remote sensing object detection and
large vision models with efficient fine-tuning. <em>PR</em>,
<em>165</em>, 111583. (<a
href="https://doi.org/10.1016/j.patcog.2025.111583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Vision Models (LVMs) exhibit robust feature extraction capabilities, offering significant potential to address performance bottlenecks in remote sensing object detection (RSOD). However, fine-tuning LVMs for RSOD remains challenging due to the high computational cost of large-resolution imagery, the complexity of densely packed objects and backgrounds, and the susceptibility to over-fitting on limited RSOD datasets. To address these challenges, we propose Layerlink , a parameter-efficient fine-tuning (PEFT) framework. Layerlink introduces the Conductor Adapter (CA) , a lightweight module that fine-tunes only a minimal set of parameters, enabling precise adaptation to complex object layouts while ensuring both computational and storage efficiency. Building on the strengths of CA, the Layerlink strategy integrates shared CA modules across hierarchical layers of the LVM, leveraging inter-layer feature similarities to enhance generalization and reduce redundancy. To validate our approach, we adapt state-of-the-art PEFT techniques originally developed for large language models to the RSOD domain, benchmarking them to establish a new standard for future research. Experiments on widely used RSOD datasets demonstrate that Layerlink achieves state-of-the-art performance while fine-tuning less than 8% of the entire network parameters. This innovation opens new avenues for efficient LVM utilization in RSOD. Code and models will be made publicly available.},
  archive      = {J_PR},
  author       = {Xingkui Zhu and Dingkang Liang and Xingyu Jiang and Yiran Guan and Yuliang Liu and Yingying Zhu and Xiang Bai},
  doi          = {10.1016/j.patcog.2025.111583},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111583},
  shortjournal = {Pattern Recognition},
  title        = {Layerlink: Bridging remote sensing object detection and large vision models with efficient fine-tuning},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual structure-aware consensus graph learning for incomplete
multi-view clustering. <em>PR</em>, <em>165</em>, 111582. (<a
href="https://doi.org/10.1016/j.patcog.2025.111582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to single-view data, multi-view data encompasses both additional complementary information and redundancies. The discriminative information presented in these aligned multiple views is helpful for enhancing the performance of clustering tasks. In reality, data views are frequently incomplete, which poses a significant challenge to the clustering task. In this paper, we introduce a new method, which we called Structured-aware Consensus Graph Learning for Incomplete Multi-View Clustering (SWCGLIMVC) to tackle the problem of incomplete multi-view clustering (IMVC). Specifically, considering that the neighbor relationships between samples are of utmost importance in unsupervised clustering tasks, SWCGLIMVC leverages the intrinsic geometry structure information of all samples and preserves their neighbor relationships through the graph Laplacian regularization constraint. Moreover, to reduce the adverse effects of the imbalanced useful information contained in different views, SWCGLIMVC incorporates a dynamically learnable vector to constrain the learning models of different views. This allows the method to effectively explore the information from all incomplete views for data clustering tasks. The effectiveness of SWCGLIMVC is evaluated by conducting experiments on six widely known datasets with the comparison of several state-of-the-art clustering methods. The experimental results show that the superior performance of SWCGLIMVC on IMVC tasks.},
  archive      = {J_PR},
  author       = {Lilei Sun and Wai Keung Wong and Yusen Fu and Jie Wen and Mu Li and Yuwu Lu and Lunke Fei},
  doi          = {10.1016/j.patcog.2025.111582},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111582},
  shortjournal = {Pattern Recognition},
  title        = {Dual structure-aware consensus graph learning for incomplete multi-view clustering},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized quaternion tensor UTV decompositions for color
image and color video processing. <em>PR</em>, <em>165</em>, 111580. (<a
href="https://doi.org/10.1016/j.patcog.2025.111580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose novel quaternion matrix UTV (QUTV) and quaternion tensor UTV (QTUTV) decomposition methods, specifically designed for color image and video processing. We begin by defining both QUTV and QTUTV decompositions and provide detailed algorithmic descriptions. To enhance computational efficiency, we introduce randomized versions of these decompositions using random sampling from the quaternion normal distribution, which results in cost-effective and interpretable solutions. Extensive numerical experiments demonstrate that the proposed algorithms significantly improve computational efficiency while maintaining relative errors comparable to existing decomposition methods. These results underscore the strong potential of quaternion-based decompositions for real-world color image and video processing applications. Theoretical findings further support the robustness of the proposed methods, providing a solid foundation for their widespread use in practice.},
  archive      = {J_PR},
  author       = {Liqiao Yang and Jifei Miao and Tai-Xiang Jiang and Yanlin Zhang and Kit Ian Kou},
  doi          = {10.1016/j.patcog.2025.111580},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111580},
  shortjournal = {Pattern Recognition},
  title        = {Randomized quaternion tensor UTV decompositions for color image and color video processing},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identity-aware infrared person image generation and
re-identification via controllable diffusion model. <em>PR</em>,
<em>165</em>, 111561. (<a
href="https://doi.org/10.1016/j.patcog.2025.111561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visibleâ€“infrared person re-identification (VI-ReID) aims to learn the identity-aware features between visible and infrared person images. However, most works rely on two publicly available datasets, i . e . , SYSU-MM01 and RegDB, which is limited by the limited amount of training data and the lack of rich scenes and perspectives. In this paper, we propose a controllable diffusion framework for infrared person image generation and re-identification. Our approach is beyond the existing diffusion model in two perspectives: (1) we use LoRA to fine-tune the existing diffusion models with VI-ReID dataset and therefore it helps the diffusion model understand the infrared modality. A text adapter is then utilized to transfer the semantic understanding ability of Large Language Model (LLMs) to our generation models; (2) we design a controllable generation module to make the generated person images, from the same textual description, identity-aware. After meticulous post-processing operations, our approach is capable of producing diverse visible and infrared person images, allowing for improving the discrimination of existing VI-ReID model without any annotations. We expand the VI-ReID dataset with our generated images, and conduct extensive experiments on VI-ReID models. Experimental results demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Xizhuo Yu and Chaojie Fan and Zhizhong Zhang and Yongbo Wang and Chunyang Chen and Tianjian Yu and Yong Peng},
  doi          = {10.1016/j.patcog.2025.111561},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111561},
  shortjournal = {Pattern Recognition},
  title        = {Identity-aware infrared person image generation and re-identification via controllable diffusion model},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FILP-3D: Enhancing 3D few-shot class-incremental learning
with pre-trained vision-language models. <em>PR</em>, <em>165</em>,
111558. (<a href="https://doi.org/10.1016/j.patcog.2025.111558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot class-incremental learning (FSCIL) aims to mitigate the catastrophic forgetting issue when a model is incrementally trained on limited data. However, many of these works lack effective exploration of prior knowledge, rendering them unable to effectively address the domain gap issue in the context of 3D FSCIL, thereby leading to catastrophic forgetting. The Contrastive Vision-Language Pre-Training (CLIP) model serves as a highly suitable backbone for addressing the challenges of 3D FSCIL due to its abundant shape-related prior knowledge. Unfortunately, its direct application to 3D FSCIL still faces the incompatibility between 3D data representation and the 2D features, primarily manifested as feature space misalignment and significant noise. To address the above challenges, we introduce the FILP-3D framework with two novel components: the Redundant Feature Eliminator (RFE) for feature space misalignment and the Spatial Noise Compensator (SNC) for significant noise. RFE aligns the feature spaces of input point clouds and their embeddings by performing a unique dimensionality reduction on the feature space of pre-trained models (PTMs), effectively eliminating redundant information without compromising semantic integrity. On the other hand, SNC is a graph-based 3D model designed to capture robust geometric information within point clouds, thereby augmenting the knowledge lost due to projection, particularly when processing real-world scanned data. Moreover, traditional accuracy metrics are proven to be biased due to the imbalance in existing 3D datasets. Therefore we propose 3D FSCIL benchmark FSCIL3D-XL and novel evaluation metrics that offer a more nuanced assessment of a 3D FSCIL model. Experimental results on both established and our proposed benchmarks demonstrate that our approach significantly outperforms existing state-of-the-art methods. Code is available at: https://github.com/HIT-leaderone/FILP-3D},
  archive      = {J_PR},
  author       = {Wan Xu and Tianyu Huang and Tianyuan Qu and Guanglei Yang and Yiwen Guo and Wangmeng Zuo},
  doi          = {10.1016/j.patcog.2025.111558},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111558},
  shortjournal = {Pattern Recognition},
  title        = {FILP-3D: Enhancing 3D few-shot class-incremental learning with pre-trained vision-language models},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised multiplex graph representation learning via
maximizing coding rate reduction. <em>PR</em>, <em>165</em>, 111557. (<a
href="https://doi.org/10.1016/j.patcog.2025.111557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multiplex graph representation learning (UMGRL) has gained increasing attention for its effectiveness to extract discriminative and consistent representations without labels. However, previous methods ignore the diversity of extracted representations, leading to sub-optimal results. To address the aforementioned limitations, in this paper, we propose a unified framework to extract discriminative, diverse and consistent representations simultaneously for UMGRL. To do this, we first employ the Multi-Layer Perceptron encoder with the local preserve loss to extract high-quality representations, and then employ two constraints based on the coding rate to constrain representationsâ€™ diversity, discrimination, and consistency. Comprehensive experiments are conducted to verify the effectiveness of the proposed model. The results show that our method outperforms fourteen existing methods on four public benchmark datasets for three different downstream tasks. The code is available at https://github.com/OllieWangx/D2CMG .},
  archive      = {J_PR},
  author       = {Xin Wang and Liang Peng and Rongyao Hu and Ping Hu and Xiaofeng Zhu},
  doi          = {10.1016/j.patcog.2025.111557},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111557},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised multiplex graph representation learning via maximizing coding rate reduction},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchy-based diagram-sentence matching on dual-modal
graphs. <em>PR</em>, <em>165</em>, 111556. (<a
href="https://doi.org/10.1016/j.patcog.2025.111556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagram is a special kind of image drawn by domain experts, which mainly consists of graphic symbols and abstract drawings. It is essential to combine the diagrams with other modalities (e.g. textual descriptions and subtitles in teaching videos) for in-depth understanding of the knowledge concepts. Diagram-sentence matching, a novel task proposed to bridge abstract diagram representation and explicit natural language, is significant to textbook question answering (TQA) and diagram understanding but remains challenging. Existing vision-language matching works mainly focus on the field of natural images and are not applicable to diagrams due to the following two characteristics: (1) the relation in diagrams has diversified representation forms; (2) the knowledge concepts conveyed in diagrams are key to fine-grained diagram-sentence matching. In this paper, we propose the Hierarchy-Based Diagram-Sentence Matching (HBDSM) model and transfer this problem into a cross-modal knowledge concept matching task at multiple levels. To achieve this, the HBDSM first encodes the diagram and sentence as symmetrical dual-modal graphs. For diagram, a novel Visual Relation Structure Learning (VRSL) method is designed to explore the structural relations between objects, which constitute the edges. For sentence, words are fused into object and relation chunks as nodes, associated by edges according to their semantic dependencies. Motivated by the human cognitive process, the fine-grained correspondence between diagram and sentence is modeled based on the hierarchy of dual-modal graphs progressively, using from low-order to high-order information. Node-level matching establishes alignment of object nodes, based on which structure-level matching compares the internal structures of both graphs. Further, concept-level matching includes relation semantics to match the cross-modal concepts based on structure alignment. Extensive experiments demonstrate the effectiveness of HBDSM in diagram-sentence matching, achieving new state-of-the-art results with relative improvement of 20.0% at rSum on AI2D#. Competitive performances of image-sentence matching on Flickr30K and MSCOCO also verify certain applicability of HBDSM for natural images.},
  archive      = {J_PR},
  author       = {Wenjun Wu and Lingling Zhang and Jun Liu and Ming Ren and Xin Hu and Jiaxin Wang and Qianying Wang},
  doi          = {10.1016/j.patcog.2025.111556},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111556},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchy-based diagram-sentence matching on dual-modal graphs},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal hypergraph contrastive learning for medical
image segmentation. <em>PR</em>, <em>165</em>, 111544. (<a
href="https://doi.org/10.1016/j.patcog.2025.111544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) has become a dominant approach in multi-modal medical image segmentation. However, existing methods, such as Seq SSL and Joint SSL, suffer from catastrophic forgetting and conflicts in representation learning across different modalities. To address these challenges, we propose a two-stage SSL framework, HyCon, for multi-modal medical image segmentation. It combines the advantages of Seq and Joint SSL using knowledge distillation to align similar topological samples across modalities. In the first stage, cross-modal features are learned through adversarial learning. Inspired by the Graph Foundation Models and further adapted to our task, the Hypergraph Contrastive Learning Network (HCLN) with a teacher-student architecture is subsequently introduced to capture high-order relationships across modalities by integrating hypergraphs with contrastive learning. The Topology Hybrid Distillation (THD) module distills topological information, contextual features, and relational knowledge into the student model. We evaluated HyCon on two organs, lung and brain. Our framework outperformed state-of-the-art SSL methods, achieving significant improvements in segmentation with limited labeled data. Both quantitative and qualitative experiments validate the effectiveness of the design of our framework. Code is available at: https://github.com/reeive/HyCon .},
  archive      = {J_PR},
  author       = {Weipeng Jing and Junze Wang and Donglin Di and Dandan Li and Yang Song and Lei Fan},
  doi          = {10.1016/j.patcog.2025.111544},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111544},
  shortjournal = {Pattern Recognition},
  title        = {Multi-modal hypergraph contrastive learning for medical image segmentation},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep wavelet temporal-frequency attention for nonlinear fMRI
factorization in ASD. <em>PR</em>, <em>165</em>, 111543. (<a
href="https://doi.org/10.1016/j.patcog.2025.111543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal-frequency characteristics in fMRI data are key to distinguishing Autism Spectrum Disorder (ASD) from neurotypical individuals. However, the non-linearity and multidimensionality of fMRI data pose significant challenges. To address these, we introduce a Deep Non-linear Factorization method with a Wavelet Temporal-Frequency Attention module (Deep WTFAF) tailored for multidimensional fMRI analysis. By leveraging the wavelet domain, our approach applies temporal-frequency attention to assign weights to significant features, enhancing critical data while reconstructing incomplete fMRI data. This method enables deep non-linear factorization and effective feature representation for subsequent classification tasks. Validated on ASD-related fMRI datasets, Deep WTFAF outperforms traditional methods, maintaining essential information and ensuring robustness against high-dimensional and incomplete data. Stability theory proof further confirms the modelâ€™s reliability, crucial for clinical applications like neurological disorder classification.},
  archive      = {J_PR},
  author       = {Fengqin Wang and Hengjin Ke and Hongyin Ma and Yunbo Tang},
  doi          = {10.1016/j.patcog.2025.111543},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111543},
  shortjournal = {Pattern Recognition},
  title        = {Deep wavelet temporal-frequency attention for nonlinear fMRI factorization in ASD},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust trajectory forecasting in autonomous systems using
mixtures of studentâ€™s t-distributions with t-DistNet. <em>PR</em>,
<em>165</em>, 111524. (<a
href="https://doi.org/10.1016/j.patcog.2025.111524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of predicting future trajectories of agents in complex traffic scenes, emphasizing the need for reliable predictions that are robust to various sources of uncertainty. Current methods for trajectory prediction often overlook the uncertainty aspect, although typically relying on deep neural networks (DNNs) trained to predict mixtures of Gaussian and Laplace distributions. In our study, we evaluate the significance of distribution choice for achieving reliable and robust predictions in uncertain environments and introduce T-DistNet, which employs a mixture of Studentâ€™s T-distributions for superior uncertainty modeling. This approach enables more accurate performance in scenarios with varying levels of uncertainty compared to other mixed distributions. Our analysis demonstrates that T-DistNet effectively models uncertainty, facilitating efficient and precise predictions.},
  archive      = {J_PR},
  author       = {Adrien Lafage and Gianni Franchi and Mathieu Barbier and David Filliat},
  doi          = {10.1016/j.patcog.2025.111524},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111524},
  shortjournal = {Pattern Recognition},
  title        = {Robust trajectory forecasting in autonomous systems using mixtures of studentâ€™s T-distributions with T-DistNet},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D microvascular reconstruction in retinal OCT angiography
images via domain-adaptive learning. <em>PR</em>, <em>165</em>, 111494.
(<a href="https://doi.org/10.1016/j.patcog.2025.111494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical Coherence Tomography Angiography (OCTA) is a non-invasive imaging technique that enables the acquisition of 3D depth-resolved information with micrometer resolution, facilitating the diagnosis of various eye-related diseases. In OCTA-based image analysis, 2D en face projected images are commonly used for quantifying microvascular changes, while the 3D images with rich depth information remains largely unexplored. This is mainly due to that direct 3D vessel reconstruction faces several challenges, including projection artifacts, complex vessel topology, and high computational cost. These limitations hinder comprehensive microvascular analysis and may obscure potentially vital 3D vessel biomarkers. In this study, we propose a novel method for 3D reconstruction of retinal microvasculature using 2D en face images. Our approach capitalizes on a elaborately generated 2D OCTA depth map for vessel reconstruction, thus eliminating the need for unavailable 3D volumetric data in certain retinal imaging devices. More specifically, we first build a structure-guided depth prediction network which incorporates a domain adaptation module to evaluate the depth maps obtained from different OCTA imaging devices. A point-cloud-to-surface reconstruction method is then utilized to reconstruct the corresponding 3D retinal vessels, based on the predicted depth maps and 2D vascular information. Experimental results demonstrate the superior performance of our method in comparison to existing state-of-the-art techniques. Furthermore, we extract 3D vessel-related features to assess disease correlation and classification, effectively evaluating the potential of our method for guiding subsequent clinical analysis. The results show promise of exploring 3D microvascular analysis for early diagnosis of various eye-related diseases.},
  archive      = {J_PR},
  author       = {Jiong Zhang and Shuai Yu and Yonghuai Liu and Dan Zhang and Jianyang Xie and Tao Chen and Yalin Zheng and Huazhu Fu and Yitian Zhao},
  doi          = {10.1016/j.patcog.2025.111494},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111494},
  shortjournal = {Pattern Recognition},
  title        = {3D microvascular reconstruction in retinal OCT angiography images via domain-adaptive learning},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual and uncertainty-aware approach for multi-person
pose estimation. <em>PR</em>, <em>165</em>, 111454. (<a
href="https://doi.org/10.1016/j.patcog.2025.111454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating human poses for multiple individuals in an image presents a significant challenge due to the requirement of identifying key body points for each person concurrently. Traditional methods struggle with occlusion, scale, and accuracy issues in 2D coordinate and heatmap-based approaches. Also, many approaches focus on single-person detection, which is impractical in real-world scenarios. Furthermore, traditional deep neural networks for pose estimation often neglect modeling the uncertainty associated with predicted poses. They often require additional parameters to estimate pose variance, increasing computational complexity and training challenges. This research addresses these challenges in multi-person pose estimation by proposing a novel approach, the Context-aware Bayesian Lightweight Capsule Network (CBLCapsNet). The key contributions of this work encompass the introduction of a highly effective cascaded Context Integration Block (CIB), allowing the acquisition of analyzing relative location and feature data to reconstruct the human body and challenging keypoints. Capsule blocks enhance spatial relationship capture between body parts, leading to more structured and hierarchical representations and improved accuracy in the proposed CBLCapsNet approach. A novel method is presented for estimating predictive uncertainty in 2D pose predictions directly from images, without introducing additional variability in the final layer. This method decomposes uncertainty into aleatoric and epistemic components, enhancing accuracy. Comprehensive experiments conducted on both COCO and MPII datasets confirm that the proposed approach enhances the performance of bottom-up pose estimation methods.},
  archive      = {J_PR},
  author       = {Pham Thanh Huu and Nguyen Thai An and Nguyen Ngoc Trung},
  doi          = {10.1016/j.patcog.2025.111454},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {111454},
  shortjournal = {Pattern Recognition},
  title        = {Contextual and uncertainty-aware approach for multi-person pose estimation},
  volume       = {165},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
