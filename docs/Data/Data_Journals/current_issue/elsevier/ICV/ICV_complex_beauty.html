<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv---2">ICV - 2</h2>
<ul>
<li><details>
<summary>
(2025). Image–text feature learning for unsupervised
visible–infrared person re-identification. <em>ICV</em>, <em>158</em>,
105520. (<a href="https://doi.org/10.1016/j.imavis.2025.105520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible–infrared person re-identification (VI-ReID) focuses on matching infrared and visible images of the same person. To reduce labeling costs, unsupervised VI-ReID (UVI-ReID) methods typically use clustering algorithms to generate pseudo-labels and iteratively optimize the model based on these pseudo-labels. Although existing UVI-ReID methods have achieved promising performance, they often overlook the effectiveness of text semantics in inter-modality matching and modality-invariant feature learning. In this paper, we propose an image–text feature learning (ITFL) method, which not only leverages text semantics to enhance intra-modality identity-related learning but also incorporates text semantics into inter-modality matching and modality-invariant feature learning. Specifically, ITFL first performs modality-aware feature learning to generate pseudo-labels within each modality. Then, ITFL employs modality-invariant text modeling (MTM) to learn a text feature for each cluster in the visible modality, and utilizes inter-modality dual-semantics matching (IDM) to match inter-modality positive clusters. To obtain modality-invariant and identity-related image features, we not only introduce a cross-modality contrastive loss in ITFL to mitigate the impact of modality gaps, but also develop a text semantic consistency loss to further promote modality-invariant feature learning. Extensive experimental results on VI-ReID datasets demonstrate that ITFL not only outperforms existing unsupervised methods but also competes with some supervised approaches.},
  archive      = {J_ICV},
  author       = {Jifeng Guo and Zhiqi Pang},
  doi          = {10.1016/j.imavis.2025.105520},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105520},
  shortjournal = {Image Vis. Comput.},
  title        = {Image–text feature learning for unsupervised visible–infrared person re-identification},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MrgaNet: Multi-scale recursive gated aggregation network for
tracheoscopy images. <em>ICV</em>, <em>158</em>, 105503. (<a
href="https://doi.org/10.1016/j.imavis.2025.105503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is a potentially fatal disease worldwide, and improving the accuracy of diagnosis plays a key role in enhancing patient outcomes. In this study, we extended computer-aided work to the task of assisting tracheoscopy in predicting lung cancer subtypes. To solve the problem of information fusion in different spatial scales and channels, we proposed MrgaNet. The network enhances classification performance by expanding interactions from low to high orders, dynamically adjusting feature weights, and incorporating a channel competition operator for efficient feature selection. Our network achieved a precision of 0.87 in the endobronchial dataset. In addition, the accuracy of 89.25% and 96.76% was achieved in the Kvasir-v2 dataset and the Kvasir-Capsule dataset, respectively. The results demonstrate that MrgaNet achieves superior performance compared to existing excellent methods.},
  archive      = {J_ICV},
  author       = {Ying Wang and Yun Tie and Dalong Zhang and Fenghui Liu and Lin Qi},
  doi          = {10.1016/j.imavis.2025.105503},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105503},
  shortjournal = {Image Vis. Comput.},
  title        = {MrgaNet: Multi-scale recursive gated aggregation network for tracheoscopy images},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
