<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJAR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijar---10">IJAR - 10</h2>
<ul>
<li><details>
<summary>
(2025). Modeling and updating uncertain evidence within belief
function theory. <em>IJAR</em>, <em>182</em>, 109428. (<a
href="https://doi.org/10.1016/j.ijar.2025.109428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework that enhances the expressiveness of the evidential and credal interpretations of Belief Function Theory while remaining within its scope. It allows uncertain evidence to be represented “as is” by associating meaningful intervals of N or R to focal elements, providing an intrinsic justification for belief values. This improves the modeling and manipulation of knowledge. From a credal perspective, the framework enables the accurate representation of non-maximal credal sets, when their extrema are belief and plausibility functions. We introduce three update operations that extend Dempster&#39;s, geometric, and Bayesian conditioning to uncertain evidence. These updates are expressed in terms of transfer of evidence, ensuring linear complexity relative to the number of focal elements. This approach provides clear evidential semantics to Bayesian conditioning, resolves several of its anomalies by making it tractable and commutative, and explains its apparent dilation effect. Most importantly, it accurately yields the updated credal set, rather than merely providing its bounds.},
  archive      = {J_IJAR},
  author       = {Pierre Pomeret-Coquot},
  doi          = {10.1016/j.ijar.2025.109428},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109428},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {Modeling and updating uncertain evidence within belief function theory},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel active learning approach to label one million
unknown malware variants. <em>IJAR</em>, <em>182</em>, 109426. (<a
href="https://doi.org/10.1016/j.ijar.2025.109426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning for classification seeks to reduce the cost of labeling samples by finding unlabeled examples about which the current model is least certain and sending them to an annotator/expert to label. Bayesian theory can provide a probabilistic view of deep neural network models by asserting a prior distribution over model parameters and estimating the uncertainties by posterior distribution over these parameters. This paper proposes two novel active learning approaches to label one million malware examples belonging to different unknown modern malware families. The first model is Inception-V4+PCA combined with several support vector machine (SVM) algorithms (UTSVM, PSVM, SVM-GSU, TBSVM). The second model is Vision Transformer based Bayesian Neural Networks ViT-BNN. Our proposed ViT-BNN is a state-of-the-art active learning approach that differs from current methods and can apply to any particular task. The experiments demonstrate that the ViT-BNN is more stable and robust in handling uncertainty.},
  archive      = {J_IJAR},
  author       = {Ahmed Bensaoud and Jugal Kalita},
  doi          = {10.1016/j.ijar.2025.109426},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109426},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {A novel active learning approach to label one million unknown malware variants},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong consistency and robustness of fuzzy medoids.
<em>IJAR</em>, <em>182</em>, 109425. (<a
href="https://doi.org/10.1016/j.ijar.2025.109425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Central tendency of fuzzy number-valued data can be robustly summarized with different proposals from the literature, namely, fuzzy-valued medians, trimmed means and M-estimators of location. In many applications, fuzzy numbers of a specific shape, such as trapezoidal or triangular, are considered, since the chosen shape scarcely affects the value of these summary measures, whenever the ‘meaning’ is basically preserved. Whereas, irrespective of the considered data shape, M-estimators of location under the conditions of the representer theorem and trimmed means would share the same shape, fuzzy medians do not have to. Fuzzy medians must be frequently approximated through the computation of some of their α -levels, whence methods based on them become more complex computationally. All this might discourage users from choosing these measures to describe central tendency. Fuzzy medoids have been recently introduced as an alternative that keeps both the shape of the data and the idea inspiring fuzzy medians, by focusing on the minimization of the mean distance to sample observations, but constrained to the set of fuzzy-valued data. Consequently, it is guaranteed that they always coincide with a sample observation, like it happens (or can be assumed, by convention, to happen) with the median in real-valued scenarios. This work shows the strong consistency of fuzzy medoids as estimators of the corresponding population median (with respect to the same distance) and their robustness in terms of the finite sample breakdown point. Furthermore, some simulation studies have been developed to compare the finite-sample behaviour of fuzzy medoids and other robust central tendency measures.},
  archive      = {J_IJAR},
  author       = {Beatriz Sinova and Sergio Palacio-Vega and María Ángeles Gil},
  doi          = {10.1016/j.ijar.2025.109425},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109425},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {Strong consistency and robustness of fuzzy medoids},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval three-way decision model based on data envelopment
analysis and prospect theory. <em>IJAR</em>, <em>182</em>, 109424. (<a
href="https://doi.org/10.1016/j.ijar.2025.109424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the three-way decision theory was proposed, two paradigms, “narrow sense” and “broad sense”, have gradually evolved, each demonstrating unique advantages in handling multi-granularity information and uncertainty analysis. These approaches provide a systematic theoretical framework for solving complex decision-making problems. However, the traditional three-way decision model has limitations in multi-input-output scenarios, and the current method is still insufficient in characterizing the risk attitudes and psychological characteristics of decision-makers. To address these challenges, this paper proposes an interval three-way decision model based on Data Envelopment Analysis (DEA) and Prospect theory for multi-input-output decision problems. First, we define a many-valued decision information system based on DEA, using benefit scores from various orientations as decision attributes to formulate decision strategies. Second, to mitigate the subjective bias introduced by reference points in Prospect theory, we introduce triangular fuzzy reference points that account for interval uncertainty. Additionally, we propose a calculation method for the multi-input-output interval membership function of Decision-Making Units (DMUs) and a construction method for the value function. Comprehensive decision rules are derived by calculating the overall prospect value. Finally, the effectiveness of the proposed model is validated using a series of experiments across multiple datasets, with comparisons to over ten existing methods. The results indicate that the model achieves competitive performance in terms of classification accuracy and decision-making efficiency, demonstrating its strengths in addressing uncertain multi-input-output decision problems while incorporating decision-makers&#39; risk preferences in an interval environment.},
  archive      = {J_IJAR},
  author       = {Xianwei Xin and Xiao Yu and Tao Li and Zhanao Xue},
  doi          = {10.1016/j.ijar.2025.109424},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109424},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {Interval three-way decision model based on data envelopment analysis and prospect theory},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification in regression neural networks
using evidential likelihood-based inference. <em>IJAR</em>,
<em>182</em>, 109423. (<a
href="https://doi.org/10.1016/j.ijar.2025.109423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new method for quantifying prediction uncertainty in regression neural networks using evidential likelihood-based inference. The method is based on the Gaussian approximation of the likelihood function and the linearization of the network output with respect to the weights. Prediction uncertainty is described by a random fuzzy set inducing a predictive belief function. Two models are considered: a simple one with constant conditional variance and a more complex one in which the conditional variance is predicted by an auxiliary neural network. Both models are trained by regularized log-likelihood maximization using a standard optimization algorithm. The postprocessing required for uncertainty quantification only consists of one computation and inversion of the Hessian matrix after convergence. Numerical experiments show that the approximations are quite accurate and that the method allows for conservative uncertainty-aware predictions.},
  archive      = {J_IJAR},
  author       = {Thierry Denœux},
  doi          = {10.1016/j.ijar.2025.109423},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109423},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {Uncertainty quantification in regression neural networks using evidential likelihood-based inference},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way clustering based on the graph of local density
trend. <em>IJAR</em>, <em>182</em>, 109422. (<a
href="https://doi.org/10.1016/j.ijar.2025.109422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way clustering demonstrates its unique advantages in dealing with the issues of information ambiguity and unclear boundaries present in real-world datasets. The core and boundary region in the data are identified as key features of cluster analysis. Typically, data is segmented into three regions based on a set of predetermined global thresholds, a common practice in three-way clustering. However, this method, which relies on global thresholds, often overlooks the intrinsic distribution patterns within the dataset and determining these thresholds a priori can be quite challenging. In this paper, we propose a three-way clustering method based on the graph of local density trend (3W-GLDT). Specifically, the algorithm first uses a density-decreasing strategy to build subgraphs and divide the core region data. Then, the unreasonable connection is corrected by using isolated forest, which increases the number of core points and enlarges the distribution range of core points. Next, a three-way allocation strategy is proposed, which fully considers the degree of local aggregation of subgraphs and the natural domain information of each data object to ensure the correct allocation. Finally, the proposed algorithm is compared with 8 different clustering methods on 8 synthetic datasets and 10 UCI real datasets. The experimental results show that the 3W-GLDT algorithm has good performance and clustering results.},
  archive      = {J_IJAR},
  author       = {Haifeng Yang and Weiqi Wang and Jianghui Cai and Jie Wang and Yating Li and Yaling Xun and Xujun Zhao},
  doi          = {10.1016/j.ijar.2025.109422},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109422},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {Three-way clustering based on the graph of local density trend},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Superior scoring rules for probabilistic evaluation of
single-label multi-class classification tasks. <em>IJAR</em>,
<em>182</em>, 109421. (<a
href="https://doi.org/10.1016/j.ijar.2025.109421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces novel superior scoring rules called Penalized Brier Score ( PBS ) and Penalized Logarithmic Loss ( PLL ) to improve model evaluation for probabilistic classification. Traditional scoring rules like Brier Score and Logarithmic Loss sometimes assign better scores to misclassifications in comparison with correct classifications. This discrepancy from the actual preference for rewarding correct classifications can lead to suboptimal model selection. By integrating penalties for misclassifications, PBS and PLL modify traditional proper scoring rules to consistently assign better scores to correct predictions. Formal proofs demonstrate that PBS and PLL satisfy strictly proper scoring rule properties while also preferentially rewarding accurate classifications. Experiments showcase the benefits of using PBS and PLL for model selection, model checkpointing, and early stopping. PBS exhibits a higher negative correlation with the F1 score compared to the Brier Score during training. Thus, PBS more effectively identifies optimal checkpoints and early stopping points, leading to improved F1 scores. Comparative analysis verifies models selected by PBS and PLL achieve superior F1 scores. Therefore, PBS and PLL address the gap between uncertainty quantification and accuracy maximization by encapsulating both proper scoring principles and explicit preference for true classifications. The proposed metrics can enhance model evaluation and selection for reliable probabilistic classification.},
  archive      = {J_IJAR},
  author       = {Rouhollah Ahmadian and Mehdi Ghatee and Johan Wahlström},
  doi          = {10.1016/j.ijar.2025.109421},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109421},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {Superior scoring rules for probabilistic evaluation of single-label multi-class classification tasks},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel covering rough set model based on granular-ball
computing for data with label noise. <em>IJAR</em>, <em>182</em>,
109420. (<a href="https://doi.org/10.1016/j.ijar.2025.109420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a novel granular computing model, granular-ball computing (GBC) has a notable advantage of robustness. Inspired by GBC, a granular-ball covering rough set (GBCRS) model whose covering is made up of granular-balls (GBs) is proposed. GBCRS is the first covering rough set that fits the data distribution well. Inheriting the robustness of GBC, GBCRS can work in label noise environments. First, the optimization objective function of GBs in GBCRS is given. In order to ensure the quality of generated GBs, this function is subject to three constraints. Second, the GBCRS model is proposed. The purity threshold is used to relax the related notions so that GBCRS can be used in label noise environments. Subsequently, GBCRS is applied to the covering granular reduction and attribute reduction in label noise environments. In covering granular reduction, we propose an intuitive, understandable and anti-noise GBCRS-based granular reduction (GBCRS-GR) algorithm, which also solves the optimization objective function of GBs. Based on GBCRS-GR, a GBCRS-based attribute reduction (GBCRS-AR) algorithm is proposed with the classification ability of the attribute subset as the evaluation. The experiments on UCI datasets illustrate that proposed algorithm is more robust against label noise than the comparison ones.},
  archive      = {J_IJAR},
  author       = {Xiaoli Peng and Yuanlin Gong and Xiang Hou and Zhan Tang and Yabin Shao},
  doi          = {10.1016/j.ijar.2025.109420},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109420},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {A novel covering rough set model based on granular-ball computing for data with label noise},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surprisingly popular-based multivariate conceptual knowledge
acquisition method. <em>IJAR</em>, <em>182</em>, 109419. (<a
href="https://doi.org/10.1016/j.ijar.2025.109419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal Concept Analysis (FCA) plays a crucial role in uncertain artificial intelligence by revealing specialization and generalization relationships among concepts. Unlike artificial neural network methods, concepts are fundamental to human cognition and learning, making knowledge acquisition and decision-making methods highly significant, especially when influenced by data and cognition. However, existing FCA models and extensions predominantly emphasize data-driven approaches, often disregarding the cognitive attributes of individual learners. This paper introduces a novel method for acquiring diverse conceptual knowledge and a cognition-enhanced decision-making approach based on the Surprisingly Popular (SP) method. Initially, it defines a new multivariate relational formal context and its corresponding decision-making method to facilitate a more sophisticated exploration of uncertain information. Additionally, it presents a method for quantifying the similarity of multivariate concepts. The SP approach is then integrated to identify the core and peripheral multivariate concepts within the multivariate concept lattice. Furthermore, the paper develops a multivariate cognitive decision-making method and presents the corresponding algorithm. Finally, instance analysis is conducted on the UCI dataset to compare the proposed method with state-of-the-art models. The results indicate that the proposed model effectively uncovers core and peripheral concepts within uncertain information by incorporating human cognitive decision processes.},
  archive      = {J_IJAR},
  author       = {Xianwei Xin and Shiting Yuan and Tao Li and Zhanao Xue and Chenyang Wang},
  doi          = {10.1016/j.ijar.2025.109419},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109419},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {Surprisingly popular-based multivariate conceptual knowledge acquisition method},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weightedness measures from inequality systems.
<em>IJAR</em>, <em>182</em>, 109418. (<a
href="https://doi.org/10.1016/j.ijar.2025.109418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A simple game is a cooperative game where some coalitions among players or voters became the (monotonic) set of winning coalitions, and the other ones form the set of losing coalitions. It is well-known that weighted voting games form a strict subclass of simple games, where each player has a voting weight so that a coalition wins if and only if the sum of weights of their members exceeds a given quota, otherwise it loses. This work studies how far away a simple game is for being representable as a weighted voting game, which allows for a more compact representation. There are several measures that determine the weightedness of a simple game, such as the dimension, the trade-robustness, the critical threshold value associated with the α -roughly weightedness property, etc. In this work we propose some new weightedness measures, all based on linear programming. In general terms, for a given simple game, a linear program is used to identify its weightedness: (i) the ϵ -roughly value ( μ ϵ ), (ii) the Z + -roughly value ( μ Z + ), (iii) the Δ-roughly value ( μ Δ ), and (iv) the outlier value ( Ψ M ). We show a close relation between the known critical threshold value of weightedness and the new measure μ Δ . Finally, we also present an exhaustive comparison of weightedness measures for simple games with up to six players.},
  archive      = {J_IJAR},
  author       = {Maria Albareda-Sambola and Xavier Molinero and Salvador Roura},
  doi          = {10.1016/j.ijar.2025.109418},
  journal      = {International Journal of Approximate Reasoning},
  month        = {7},
  pages        = {109418},
  shortjournal = {Int. J. Approx. Reasoning},
  title        = {Weightedness measures from inequality systems},
  volume       = {182},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
