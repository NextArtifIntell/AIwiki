<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJUFKS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijufks---7">IJUFKS - 7</h2>
<ul>
<li><details>
<summary>
(2025). Interval methods in knowledge representation.
<em>IJUFKS</em>, <em>33</em>(2), 255–256. (<a
href="https://doi.org/10.1142/S0218488525970037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJUFKS},
  doi          = {10.1142/S0218488525970037},
  journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  number       = {2},
  pages        = {255-256},
  shortjournal = {Int. J. Uncertain. Fuzziness Knowl.-Based. Syst.},
  title        = {Interval methods in knowledge representation},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acknowledgements to the referees. <em>IJUFKS</em>,
<em>33</em>(2), 253–254. (<a
href="https://doi.org/10.1142/S0218488525970025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJUFKS},
  doi          = {10.1142/S0218488525970025},
  journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  number       = {2},
  pages        = {253-254},
  shortjournal = {Int. J. Uncertain. Fuzziness Knowl.-Based. Syst.},
  title        = {Acknowledgements to the referees},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). V-SVR with imprecise observations. <em>IJUFKS</em>,
<em>33</em>(2), 235–252. (<a
href="https://doi.org/10.1142/S0218488525500102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector regression (SVR) has been widely used in academia and industry with excellent performance. Crisp data always be trained by classic SVR and its varieties. However, classic SVR is feeble if data are imprecise or low-quality. Hence, the uncertainty theory emerged as the times require, which can process the imprecise observations well. In this study, a novel SVR model be introduced into uncertainty theory, termed v -SVR with imprecise observations, designed to handle imprecise or low-quality data. Unlike the conventional ε -SVR with imprecise observations approach, v -SVR offers an automated computation of the accuracy parameter ε , thereby eliminating the need for manual selection. This results in improved performance with simplified parameter tuning. The effectiveness of the approach in this paper be demonstrated through a numerical example.},
  archive      = {J_IJUFKS},
  author       = {Hao Zhang and Yuhong Sheng},
  doi          = {10.1142/S0218488525500102},
  journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  number       = {2},
  pages        = {235-252},
  shortjournal = {Int. J. Uncertain. Fuzziness Knowl.-Based. Syst.},
  title        = {V-SVR with imprecise observations},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entropic semi-supervised dimensionality reduction for
distance metric learning. <em>IJUFKS</em>, <em>33</em>(2), 219–234. (<a
href="https://doi.org/10.1142/S0218488525500096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance metric learning and nonlinear dimensionality reduction are intrinsically related, since they are both different perspectives of the same fundamental problem: to learn compact and meaningful data representations for classification and visualization. In this paper, we propose a graph-based generalization of Semi-Supervised Dimensionality Reduction (SSDR) algorithm that uses stochastic distances (Kullback-Leibler, Bhattacharyya and Cauchy-Schwarz divergences) to compute the similarity between local multivariate Gaussian distributions along the K Nearest Neighbors (KNN) graph build from the samples in the input high-dimensional space. In summary, there are two variants of the proposed method: one which uses only a fraction of the labeled samples (10%) and another that also uses a clustering method (Gaussian Mixture Models) to estimate the labels of the minimum spanning tree of the KNN graph, incorporating more information into the process. Experimental results with several real datasets show that the proposed method is able to improve the classification accuracy of several supervised classifiers and also the quality of the obtained clusters (Silhouette Coefficients) in comparison to the regular SSDR algorithm, making it a viable alternative for pattern classification problems.},
  archive      = {J_IJUFKS},
  author       = {Alexandre L. M. Levada},
  doi          = {10.1142/S0218488525500096},
  journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  number       = {2},
  pages        = {219-234},
  shortjournal = {Int. J. Uncertain. Fuzziness Knowl.-Based. Syst.},
  title        = {Entropic semi-supervised dimensionality reduction for distance metric learning},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bi-level alternating meta-learning for recurring concept
on evolving data streams instructions for typing. <em>IJUFKS</em>,
<em>33</em>(2), 193–217. (<a
href="https://doi.org/10.1142/S0218488525500084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans’ learning involves remembering patterns of the past to better understand recurring concepts as their knowledge grows. However, a key issue that arises from these cases is that previous knowledge in deep neural networks could be gradually forgotten when they are trained for a new concept. We address this problem by learning a general representation that can remember the previous information and promote future learning. In this pursuit, a new controller is introduced by the meta-learning strategy that guides the network to keep the balance between the previously learned concepts and the new concept, hence it avoids catastrophic forgetting. Compared to previous online incremental learning for evolving data streams, our approach is dedicated to handling recurring concepts. When encountering recurring concepts, the model can remember and recall the previous knowledge and can quickly adapt to this change. In this paper, we propose a Bi-level Alternating Meta-learning approach for recurring concepts (BLAML), which emphasizes the hidden representation learning of different concepts in model-level learning, and obtains a set of shared parameters through the global meta-learning strategy. Through extensive experiments, the effectiveness of the proposed method is proved.},
  archive      = {J_IJUFKS},
  author       = {Jian-Wei Liu and Si-Si Zhang and Zhong-Lin Bao},
  doi          = {10.1142/S0218488525500084},
  journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  number       = {2},
  pages        = {193-217},
  shortjournal = {Int. J. Uncertain. Fuzziness Knowl.-Based. Syst.},
  title        = {A bi-level alternating meta-learning for recurring concept on evolving data streams instructions for typing},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An unconstrained primal based twin parametric insensitive
support vector regression. <em>IJUFKS</em>, <em>33</em>(2), 173–192. (<a
href="https://doi.org/10.1142/S0218488525500072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient regression algorithm based on primal formulation of twin support vector machine. This is an efficient approach to solve the optimization problem leading to reduced computation time. The proposed method is termed as twin parametric insensitive support vector regression (UPTPISVR). The optimization problems of the proposed (UPTPISVR) are a pair of unconstrained convex minimization problems. Moreover, the objective functions of UPTPISVR are strongly convex, differentiable and piecewise quadratic. Therefore, an approximate solution is obtained in primal variables instead of solving the dual formulation. Further, an absolute value equation problem is solved by using a functional iterative algorithm for UPTPISVR, termed as FUPTPISVR. The objective function of the proposed formulation involves the plus function which is non-smooth and therefore, smooth approximation functions are used to replace the plus function, termed as SUPTPISVR. The Newton-Armijo algorithm is then used to iteratively obtain the solutions, thus eliminates the requirement of any optimization toolbox. Various numerical experiments on synthetic and benchmark real-world datasets are presented for justifying the applicability and effectiveness of the proposed UPTPISVR. The results clearly indicate that the proposed algorithms outperform the existing algorithms in terms of root mean square error (RMSE) on most datasets.},
  archive      = {J_IJUFKS},
  author       = {Deepak Gupta and Bharat Richhariya and Parashjyoti Borah},
  doi          = {10.1142/S0218488525500072},
  journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  number       = {2},
  pages        = {173-192},
  shortjournal = {Int. J. Uncertain. Fuzziness Knowl.-Based. Syst.},
  title        = {An unconstrained primal based twin parametric insensitive support vector regression},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple item support constraints based frequent pattern
mining using dynamic prefix tree. <em>IJUFKS</em>, <em>33</em>(2),
143–172. (<a href="https://doi.org/10.1142/S0218488525500060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structural complexity of the pattern mining algorithm depends on the types of datasets. Even though they are highly connected, it can be intriguing to identify patterns in some application areas where they do not usually occur. FP tree construction practice with traversal of conditional pattern base with conditional FP Tree, with path traversing, to address the issue of massive memory and time usage. The creation of a non-recursive single-label dynamic prefix tree with a rule generation method utilizing multiple-item support restrictions is the paper’s significant contribution. The effectiveness of our proposed method is also compared to the FP tree and state-of-the-art TIS tree on various datasets in terms of time and memory complexity.},
  archive      = {J_IJUFKS},
  author       = {Sudarsan Biswas and Diganta Saha and Rajat Pandit},
  doi          = {10.1142/S0218488525500060},
  journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  number       = {2},
  pages        = {143-172},
  shortjournal = {Int. J. Uncertain. Fuzziness Knowl.-Based. Syst.},
  title        = {Multiple item support constraints based frequent pattern mining using dynamic prefix tree},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
