<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>iop_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="iop">IOP</h1>
<h2 id="mlst---18">MLST - 18</h2>
<ul>
<li><details>
<summary>
(2025). Uncertainty quantification with bayesian higher order
ReLU-KANs. <em>MLST</em>, <em>6</em>(1), 015073. (<a
href="https://doi.org/10.1088/2632-2153/adbeb7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the first method of uncertainty quantification in the domain of Kolmogorov–Arnold Networks, specifically focusing on (Higher Order) ReLU-KANs to enhance computational efficiency given the computational demands of Bayesian methods. The method we propose is general in nature, providing access to both epistemic and aleatoric uncertainties. It is also capable of generalization to other various basis functions. We validate our method through a series of closure tests, commonly found in the KAN literature, including simple one-dimensional functions and application to the domain of (stochastic) partial differential equations. Referring to the latter, we demonstrate the method&#39;s ability to correctly identify functional dependencies introduced through the inclusion of a stochastic term.},
  archive      = {J_MLST},
  author       = {J Giroux and C Fanelli},
  doi          = {10.1088/2632-2153/adbeb7},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015073},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Uncertainty quantification with bayesian higher order ReLU-KANs},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving early detection of gravitational waves from binary
neutron stars using CNNs and FPGAs. <em>MLST</em>, <em>6</em>(1),
015072. (<a href="https://doi.org/10.1088/2632-2153/adbf66">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of gravitational waves (GWs) from binary neutron stars (BNSs) with possible telescope follow-ups opens a window to ground-breaking discoveries in the field of multi-messenger astronomy. With the improved sensitivity of current and future GW detectors, more BNS detections are expected in the future. Therefore, enhancing low-latency GW search algorithms to achieve rapid speed, high accuracy, and low computational cost is essential. One innovative solution to reduce latency is the use of machine learning (ML) methods embedded in field-programmable gate arrays (FPGAs). In this work, we present a novel WaveNet -based method, leveraging the state-of-the-art ML model, to produce early-warning alerts for BNS systems. Using simulated GW signals embedded in Gaussian noise from the Advanced LIGO and Advanced Virgo detectors&#39; third observing run (O3) as a proof-of-concept dataset, we demonstrate significant performance improvements. Compared to the current leading ML-based early-warning system, our approach enhances detection accuracy from 66.81% to 76.22% at a 1% false alarm probability. Furthermore, we evaluate the time, energy, and economical cost of our model across CPU, GPU, and FPGA platforms, showcasing its potential for deployment in real-time GW detection pipelines.},
  archive      = {J_MLST},
  author       = {Ana Martins and Melissa Lopez and Gregory Baltus and Quirijn Meijer and Marc van der Sluys and Chris Van Den Broeck and Sarah Caudill},
  doi          = {10.1088/2632-2153/adbf66},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015072},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving early detection of gravitational waves from binary neutron stars using CNNs and FPGAs},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-fidelity transfer learning for quantum chemical data
using a robust density functional tight binding baseline. <em>MLST</em>,
<em>6</em>(1), 015071. (<a
href="https://doi.org/10.1088/2632-2153/adc222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has revolutionized the development of interatomic potentials over the past decade, offering unparalleled computational speed without compromising accuracy. However, the performance of these models is highly dependent on the quality and amount of training data. Consequently, the current scarcity of high-fidelity datasets (i.e. beyond semilocal density functional theory) represents a significant challenge for further improvement. To address this, this study investigates the performance of transfer learning (TL) across multiple fidelities for both molecules and materials. Crucially, we disentangle the effects of multiple fidelities and different configuration/chemical spaces for pre-training and fine-tuning, in order to gain a deeper understanding of TL for chemical applications. This reveals that negative transfer, driven by noise from low-fidelity methods such as a density functional tight binding baseline, can significantly impact fine-tuned models. Despite this, the multi-fidelity approach demonstrates superior performance compared to single-fidelity learning. Interestingly, it even outperforms TL based on foundation models in some cases, by leveraging an optimal overlap of pre-training and fine-tuning chemical spaces.},
  archive      = {J_MLST},
  author       = {Mengnan Cui and Karsten Reuter and Johannes T Margraf},
  doi          = {10.1088/2632-2153/adc222},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015071},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-fidelity transfer learning for quantum chemical data using a robust density functional tight binding baseline},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LatentDE: Latent-based directed evolution for protein
sequence design. <em>MLST</em>, <em>6</em>(1), 015070. (<a
href="https://doi.org/10.1088/2632-2153/adc2e2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed evolution (DE) has been the most effective method for protein engineering that optimizes biological functionalities through a resource-intensive process of screening or selecting among a vast range of mutations. To mitigate this extensive procedure, recent advancements in machine learning-guided methodologies center around the establishment of a surrogate sequence-function model. In this paper, we propose latent-based DE (LDE), an evolutionary algorithm designed to prioritize the exploration of high-fitness mutants in the latent space. At its core, LDE is a regularized variational autoencoder (VAE), harnessing the capabilities of the state-of-the-art protein language model, ESM-2, to construct a meaningful latent space of sequences. From this encoded representation, we present a novel approach for efficient traversal on the fitness landscape, employing a combination of gradient-based methods and DE. Experimental evaluations conducted on eight protein sequence design tasks demonstrate the superior performance of our proposed LDE over previous baseline algorithms. Our implementation is publicly available at https://github.com/HySonLab/LatentDE .},
  archive      = {J_MLST},
  author       = {Thanh V T Tran and Nhat Khang Ngo and Viet Thanh Duy Nguyen and Truong-Son Hy},
  doi          = {10.1088/2632-2153/adc2e2},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015070},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {LatentDE: Latent-based directed evolution for protein sequence design},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-precision prediction of non-resonant high-order
harmonics energetic particle modes via stacking ensemble strategies.
<em>MLST</em>, <em>6</em>(1), 015069. (<a
href="https://doi.org/10.1088/2632-2153/adbfdb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the prediction accuracy and stability of non-resonant high-order harmonics energetic particle modes (EPMs) is crucial for achieving tokamak plasmas steady state operation. This study adopts a stacking ensemble learning model for the EPMs prediction. This ensemble model has a 2-layer structure with the base learner and the meta-learner: the first layer, base learners include K-nearest neighbor regression, Extreme Gradient Boosting, gradient boosting regression (GBR), decision tree (DT) and support vector regression (SVR); the second layer, meta-learners output the final result via GBR. The developed stacking model is designed for multi-objective prediction, including the grow rate ( γ ) and mode frequency ( ω ) of the EPMs. The evaluation results indicate that the performance of the proposed model surpasses most supervised learning algorithms. Specifically, in comparison with the SVR and Bagging algorithms, the growth rate predictions of stacking model reduces Root mean squared error (RMSE) by 45% and 33%, mean absolute error (MAE) by 47% and 32%, and increases the R -squared coefficient ( R 2 ) by 5% and 3%, respectively. Similarly, for the mode frequency predictions, RMSE decreases by 34% and 65%, MAE by 23% and 50%, while R 2 improves by 2% and 7%, respectively. In terms of computational cost, the stacking model can conserve more time and expense than kinetic-hybrid simulations of EPMs. Therefore, the proposed model can be well applied to the prediction of the high-energy particle instability phenomenon in tokamak reactors.},
  archive      = {J_MLST},
  author       = {Sheng Liu and Zhenzhen Ren and Weihua Wang and Kai Zhong and Jinhong Yang and Hongwei Ning},
  doi          = {10.1088/2632-2153/adbfdb},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015069},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {High-precision prediction of non-resonant high-order harmonics energetic particle modes via stacking ensemble strategies},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating quantum reservoir state representations with
random matrices. <em>MLST</em>, <em>6</em>(1), 015068. (<a
href="https://doi.org/10.1088/2632-2153/adc0e2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate a novel approach to reservoir computation measurements using random matrices. We do so to motivate how atomic-scale devices could be used for real-world computational applications. Our approach uses random matrices to construct reservoir measurements, introducing a simple, scalable means of generating state representations. In our studies, two reservoirs, a five-atom Heisenberg spin chain and a five-qubit quantum circuit, perform time series prediction and data interpolation. The performance of the measurement technique and current limitations are discussed in detail, along with an exploration of the diversity of measurements provided by the random matrices. In addition, we explore the role of reservoir parameters such as coupling strength and measurement dimension, providing insight into how these learning machines could be automatically tuned for different problems. This research highlights the use of random matrices to measure simple quantum reservoirs for natural learning devices, and outlines a path forward for improving their performance and experimental realization.},
  archive      = {J_MLST},
  author       = {Samuel Tovey and Tobias Fellner and Christian Holm and Michael Spannowsky},
  doi          = {10.1088/2632-2153/adc0e2},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015068},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Generating quantum reservoir state representations with random matrices},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CNN-based vortex detection in atomic 2D bose gases in the
presence of a phononic background. <em>MLST</em>, <em>6</em>(1), 015067.
(<a href="https://doi.org/10.1088/2632-2153/adbfdc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum vortices play a crucial role in both equilibrium and dynamical phenomena in two-dimensional (2D) superfluid systems. Experimental detection of these excitations in 2D ultracold atomic gases typically involves examining density depletions in absorption images, however the presence of a significant phononic background renders the problem challenging, beyond the capability of simple algorithms or the human eye. Here, we utilize a convolutional neural network to detect vortices in the presence of strong long- and intermediate-length scale density modulations in finite-temperature 2D Bose gases. We train the model on datasets obtained from ab initio Monte Carlo simulations using the classical-field method for density and phase fluctuations, and Gross–Pitaevskii simulation of realistic expansion dynamics. We use the model to analyze experimental images and benchmark its performance by comparing the results to the matter-wave interferometric detection of vortices, confirming the observed scaling of vortex density across the Berezinskii–Kosterlitz–Thouless critical point. The combination of a relevant simulation pipeline with machine-learning methods is a key development towards the comprehensive understanding of complex vortex-phonon dynamics in out-of-equilibrium 2D quantum systems.},
  archive      = {J_MLST},
  author       = {M Sesodia and S Sunami and E Chang and E Rydow and C J Foot and A Beregi},
  doi          = {10.1088/2632-2153/adbfdc},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015067},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {CNN-based vortex detection in atomic 2D bose gases in the presence of a phononic background},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid vision transformer framework for efficient and
explainable SEM image-based nanomaterial classification. <em>MLST</em>,
<em>6</em>(1), 015066. (<a
href="https://doi.org/10.1088/2632-2153/adc072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scanning electron microscopy images, with their high potential to reveal detailed microstructural and compositional information across various fields, are challenging to label and process due to the large volumes being generated, the presence of noise and artifacts, and the reliance on domain expertise. Moreover, the lack of scalable, automated, and interpretable methods for analyzing scanning electron microscopy images has prompted this research, which focuses on three primary objectives. First, the use of semi-supervised learning techniques, including pseudo-labeling and consistency regularization, aims to utilize both labeled and unlabeled scanning electron microscopy data by generating pseudo-labels for the unlabeled data and enforcing consistency in predictions for perturbed inputs. Second, this study introduces a hybrid Vision Transformer (ViT-ResNet50) model, which combines the representational power of ViT with the feature extraction capabilities of ResNet50. Lastly, the use of SHapley Additive exPlanations enhances the model&#39;s interpretability, revealing critical image regions that contribute to predictions. To evaluate performance, the model is assessed using confusion matrices, test accuracy, precision, recall, F1 scores, receiver operating characteristic—area under the curve scores, model fit duration, and trainable parameters, along with a comparative analysis to demonstrate its competitiveness against state-of-the-art models in both semi-supervised and supervised (completely labeled data) settings. As a result, the semi-supervised based ViT-ResNet50 model achieved accuracies of 93.65% and 84.76% on the scanning electron microscopy Aversa and UltraHigh Carbon Steel Database, respectively, with notable interpretability, surpassing baseline models like ResNet101, InceptionV3, InceptionResNetV2, and InceptionV4. The findings highlight the potential of semi-supervised to improve model performance in scenarios with limited labeled data, though challenges such as class imbalance and increased computational cost suggest areas for further optimization.},
  archive      = {J_MLST},
  author       = {Manpreet Kaur and Camilo E Valderrama and Qian Liu},
  doi          = {10.1088/2632-2153/adc072},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015066},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Hybrid vision transformer framework for efficient and explainable SEM image-based nanomaterial classification},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian optimization of hybrid quantum LSTM in a mixed
model for precipitation forecasting. <em>MLST</em>, <em>6</em>(1),
015065. (<a href="https://doi.org/10.1088/2632-2153/adbbad">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precipitation forecasting has important applications in meteorological research. Accurate forecasting is of great significance for reducing the impact of floods, optimizing crop planting plans, rationally allocating water resources, and ensuring traffic safety. However, the factors affecting precipitation are complex and nonlinear, and have spatiotemporal variability, making rainfall forecasting extremely challenging. In response to these challenges, this paper proposes a hybrid model based on temporal convolutional network, quantum long short-term memory network (QLSTM), and random forest regression (RFR) to achieve more accurate rainfall forecasting. The hyperparameters of the model are optimized using the Bayesian optimization algorithm to obtain the best performance. Experiments are conducted on meteorological datasets from Seattle and Ukraine, and the results are verified using mean absolute error (MAE), root mean square error (RMSE), and bias evaluation indicators. The results show that the proposed hybrid model outperforms traditional models such as RFR, support vector machine, K-nearest neighbor, LSTM, and QLSTM in terms of MAE, RMSE, and bias. The proposed model achieves improvements of 1.89 \% MAE, 2.65 \% RMSE, and 31 \% Bias, respectively. These results highlight the improved forecast accuracy and robustness of the proposed hybrid model. This research provides a new approach to weather forecasting and demonstrates the potential of combining quantum computing with traditional machine learning techniques.},
  archive      = {J_MLST},
  author       = {Yumin Dong and Huanxin Ding},
  doi          = {10.1088/2632-2153/adbbad},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015065},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Bayesian optimization of hybrid quantum LSTM in a mixed model for precipitation forecasting},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-powered data cleaning for LEGEND: A
semi-supervised approach using affinity propagation and support vector
machines. <em>MLST</em>, <em>6</em>(1), 015064. (<a
href="https://doi.org/10.1088/2632-2153/adbb37">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neutrinoless double-beta decay ( 0\nu\beta\beta ) is a rare nuclear process that, if observed, will provide insight into the nature of neutrinos and help explain the matter-antimatter asymmetry in the Universe. The large enriched germanium experiment for neutrinoless double-beta decay (LEGEND) will operate in two phases to search for 0\nu\beta\beta . The first (second) stage will employ 200 (1000) kg of High-Purity Germanium (HPGe) enriched in 76 Ge to achieve a half-life sensitivity of 10 27 (10 28 ) years. In this study, we present a semi-supervised data-driven approach to remove non-physical events captured by HPGe detectors powered by a novel artificial intelligence model. We utilize affinity propagation to cluster waveform signals based on their shape and a support vector machine to classify them into different categories. We train, optimize, and test our model on data taken from a natural abundance HPGe detector installed in the Full Chain Test experimental stand at the University of North Carolina at Chapel Hill. We demonstrate that our model yields a maximum sacrifice of physics events of 0.024 ^{+0.004}_{-0.003} \% after data cleaning. Our model is being used to accelerate data cleaning development for LEGEND-200 and will serve to improve data cleaning procedures for LEGEND-1000.},
  archive      = {J_MLST},
  author       = {E León and A Li and M A Bahena Schott and B Bos and M Busch and J R Chapman and G L Duran and J Gruszko and R Henning and E L Martin and J F Wilkerson},
  doi          = {10.1088/2632-2153/adbb37},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015064},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning-powered data cleaning for LEGEND: A semi-supervised approach using affinity propagation and support vector machines},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for structure-preserving universal stable
koopman-inspired embeddings for nonlinear canonical hamiltonian
dynamics. <em>MLST</em>, <em>6</em>(1), 015063. (<a
href="https://doi.org/10.1088/2632-2153/adb9b5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering a suitable coordinate transformation for nonlinear systems enables the construction of simpler models, facilitating prediction, control, and optimization for complex nonlinear systems. To that end, Koopman operator theory offers a framework for global linearization of nonlinear systems, thereby allowing the usage of linear tools for design studies. In this work, we focus on the identification of global linearized embeddings for canonical nonlinear Hamiltonian systems through a symplectic transformation. While this task is often challenging, we leverage the power of deep learning to discover the desired embeddings. Furthermore, to overcome the shortcomings of Koopman operators for systems with continuous spectra, we apply the lifting principle and learn global cubicized embeddings. Additionally, a key emphasis is given to enforce the bounded stability for the dynamics of the discovered embeddings. We demonstrate the capabilities of deep learning in acquiring compact symplectic coordinate transformations and the corresponding simple dynamical models, fostering data-driven learning of nonlinear canonical Hamiltonian systems, even those with continuous spectra.},
  archive      = {J_MLST},
  author       = {Pawan Goyal and Süleyman Yıldız and Peter Benner},
  doi          = {10.1088/2632-2153/adb9b5},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015063},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning for structure-preserving universal stable koopman-inspired embeddings for nonlinear canonical hamiltonian dynamics},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic resetting mitigates latent gradient bias of SGD
from label noise. <em>MLST</em>, <em>6</em>(1), 015062. (<a
href="https://doi.org/10.1088/2632-2153/adbc46">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Giving up and starting over may seem wasteful in many situations such as searching for a target or training deep neural networks (DNNs). Our study, though, demonstrates that resetting from a checkpoint can significantly improve generalization performance when training DNNs with noisy labels. In the presence of noisy labels, DNNs initially learn the general patterns of the data but then gradually memorize the corrupted data, leading to overfitting. By deconstructing the dynamics of stochastic gradient descent (SGD), we identify the behavior of a latent gradient bias induced by noisy labels, which harms generalization. To mitigate this negative effect, we apply the stochastic resetting method to SGD, inspired by recent developments in the field of statistical physics achieving efficient target searches. We first theoretically identify the conditions where resetting becomes beneficial, and then we empirically validate our theory, confirming the significant improvements achieved by resetting. We further demonstrate that our method is both easy to implement and compatible with other methods for handling noisy labels. Additionally, this work offers insights into the learning dynamics of DNNs from an interpretability perspective, expanding the potential to analyze training methods through the lens of statistical physics.},
  archive      = {J_MLST},
  author       = {Youngkyoung Bae and Yeongwoo Song and Hawoong Jeong},
  doi          = {10.1088/2632-2153/adbc46},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015062},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Stochastic resetting mitigates latent gradient bias of SGD from label noise},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear gaussian process tomography with imposed
non-negativity constraints on physical quantities for plasma
diagnostics. <em>MLST</em>, <em>6</em>(1), 015061. (<a
href="https://doi.org/10.1088/2632-2153/adbbae">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel tomographic method, nonlinear Gaussian process tomography (nonlinear GPT), that uses the Laplace approximation to impose constraints on non-negative physical quantities, such as the emissivity in plasma optical diagnostics. While positive-valued posteriors have previously been introduced through sampling-based approaches in the original GPT method, our alternative approach implements a logarithmic Gaussian process (log-GP) for faster computation and more natural enforcement of non-negativity. The effectiveness of the proposed log-GP tomography is demonstrated through a case study using the Ring Trap 1 device, where log-GPT outperforms existing methods, standard GPT, and the minimum Fisher information methods in terms of reconstruction accuracy. The results highlight the effectiveness of nonlinear GPT for imposing physical constraints in applications to an inverse problem.},
  archive      = {J_MLST},
  author       = {Kenji Ueda and Masaki Nishiura},
  doi          = {10.1088/2632-2153/adbbae},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015061},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Nonlinear gaussian process tomography with imposed non-negativity constraints on physical quantities for plasma diagnostics},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic deep learning based super-resolution for the shallow
water equations. <em>MLST</em>, <em>6</em>(1), 015060. (<a
href="https://doi.org/10.1088/2632-2153/ada19f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correctly capturing the transition to turbulence in a barotropic instability requires fine spatial resolution. To reduce computational cost, we propose a dynamic super-resolution approach where a transient simulation on a coarse mesh is frequently corrected using a U-net-type neural network. For the nonlinear shallow water equations, we demonstrate that a simulation with the Icosahedral Nonhydrostatic ocean model with a 20 km resolution plus dynamic super-resolution trained on a 2.5km resolution achieves discretization errors comparable to a simulation with 10 km resolution. The neural network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh solution every 12 h. We show that the ML-corrected coarse solution correctly maintains a balanced flow and captures the transition to turbulence in line with the higher resolution simulation. After an 8 d simulation, the L 2 -error of the corrected run is similar to a simulation run on a finer mesh. While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy.},
  archive      = {J_MLST},
  author       = {Maximilian Witte and Fabrício R Lapolli and Philip Freese and Sebastian Götschel and Daniel Ruprecht and Peter Korn and Christopher Kadow},
  doi          = {10.1088/2632-2153/ada19f},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015060},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Dynamic deep learning based super-resolution for the shallow water equations},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal equivariant architectures from the symmetries of
matrix-element likelihoods. <em>MLST</em>, <em>6</em>(1), 015059. (<a
href="https://doi.org/10.1088/2632-2153/adbab1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Matrix-Element Method (MEM) has long been a cornerstone of data analysis in high-energy physics. It leverages theoretical knowledge of parton-level processes and symmetries to evaluate the likelihood of observed events. In parallel, the advent of geometric deep learning has enabled neural network architectures that incorporate known symmetries directly into their design, leading to more efficient learning. This paper presents a novel approach that combines MEM-inspired symmetry considerations with equivariant neural network design for particle physics analysis. Even though Lorentz invariance and permutation invariance over all reconstructed objects are the largest and most natural symmetry in the input domain, we find that they are sub-optimal in most practical search scenarios. We propose a longitudinal boost-equivariant message-passing neural network architecture that preserves relevant discrete symmetries. We present numerical studies demonstrating MEM-inspired architectures achieve new state-of-the-art performance in distinguishing di-Higgs decays to four bottom quarks from the QCD background, with enhanced sample and parameter efficiencies. This synergy between MEM and equivariant deep learning opens new directions for physics-informed architecture design, promising more powerful tools for probing physics beyond the Standard Model.},
  archive      = {J_MLST},
  author       = {Daniel Maître and Vishal S Ngairangbam and Michael Spannowsky},
  doi          = {10.1088/2632-2153/adbab1},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015059},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Optimal equivariant architectures from the symmetries of matrix-element likelihoods},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards virtual painting recolouring using vision
transformer on x-ray fluorescence datacubes. <em>MLST</em>,
<em>6</em>(1), 015058. (<a
href="https://doi.org/10.1088/2632-2153/adb937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contribution, we define (and test) a pipeline to perform virtual painting recolouring using raw data of x-ray Fluorescence (XRF) analysis on pictorial artworks. To circumvent the small dataset size, we generate a synthetic dataset, starting from a database of XRF spectra; furthermore, to ensure a better generalisation capacity (and to tackle the issue of in-memory size and inference time), we define a Deep Variational Embedding network to embed the XRF spectra into a lower dimensional, K-Means friendly, metric space. We thus train a set of models to assign coloured images to embedded XRF images. We report here the devised pipeline performances in terms of visual quality metrics, and we close on a discussion on the results.},
  archive      = {J_MLST},
  author       = {Alessandro Bombini and Fernando García-Avello Bofías and Francesca Giambi and Chiara Ruberto},
  doi          = {10.1088/2632-2153/adb937},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015058},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards virtual painting recolouring using vision transformer on x-ray fluorescence datacubes},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepAir: Deep learning and satellite imagery to estimate
high-resolution PM2.5 at scale. <em>MLST</em>, <em>6</em>(1), 015057.
(<a href="https://doi.org/10.1088/2632-2153/adb67a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution, specifically PM 2.5 , has become a significant global concern owing to its detrimental impacts on public health. Even so, the high-resolution monitoring of air pollution is still a challenge on a global scale. To cope with this, machine learning (ML) techniques have been utilized to infer the concentration of air pollutants at a fine scale. In this study, we propose DeepAir , a learning framework for estimating PM 2.5 concentrations at a fine scale with sparsely distributed observations. DeepAir integrates a pre-trained convolutional neural network with the LightGBM method. This framework estimates the PM 2.5 concentration of a given patch, utilizing a synergy of geographical information, meteorological conditions, and satellite observations. We select California as the focal region and train the model with data from 2014 to 2017 provided by 130 PM 2.5 observation stations in the state. Upon training, the model can be applied to estimate the daily PM 2.5 concentrations at 1 km resolution across California. Our methodology meticulously incorporates meteorological variables, with a particular emphasis on wildfire propagation, and contemplates the complex interplay of various features. To ascertain the efficacy of our model, we employ the 10-fold cross-validation technique, which confirms that our model surpasses traditional ML and standalone deep learning methods.},
  archive      = {J_MLST},
  author       = {Wenxuan Guo and Zhaoping Hu and Ling Jin and Yanyan Xu and Marta C Gonzalez},
  doi          = {10.1088/2632-2153/adb67a},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015057},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {DeepAir: Deep learning and satellite imagery to estimate high-resolution PM2.5 at scale},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse explanations from data-driven and domain-driven
perspectives in the physical sciences. <em>MLST</em>, <em>6</em>(1),
013002. (<a href="https://doi.org/10.1088/2632-2153/ad9137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning methods have been remarkably successful in material science, providing novel scientific insights, guiding future laboratory experiments, and accelerating materials discovery. Despite the promising performance of these models, understanding the decisions they make is also essential to ensure the scientific value of their outcomes. However, there is a recent and ongoing debate about the diversity of explanations, which potentially leads to scientific inconsistency. This Perspective explores the sources and implications of these diverse explanations in ML applications for physical sciences. Through three case studies in materials science and molecular property prediction, we examine how different models, explanation methods, levels of feature attribution, and stakeholder needs can result in varying interpretations of ML outputs. Our analysis underscores the importance of considering multiple perspectives when interpreting ML models in scientific contexts and highlights the critical need for scientists to maintain control over the interpretation process, balancing data-driven insights with domain expertise to meet specific scientific needs. By fostering a comprehensive understanding of these inconsistencies, we aim to contribute to the responsible integration of eXplainable artificial intelligence into physical sciences and improve the trustworthiness of ML applications in scientific discovery.},
  archive      = {J_MLST},
  author       = {Sichao Li and Xin Wang and Amanda Barnard},
  doi          = {10.1088/2632-2153/ad9137},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {013002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Diverse explanations from data-driven and domain-driven perspectives in the physical sciences},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
