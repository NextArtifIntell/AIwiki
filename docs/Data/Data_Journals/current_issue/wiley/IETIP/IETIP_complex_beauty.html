<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietip---88">IETIP - 88</h2>
<ul>
<li><details>
<summary>
(2025). EANet: Integrate edge features and attention mechanisms
multi-scale networks for vessel segmentation in retinal images.
<em>IETIP</em>, <em>19</em>(1), e70056. (<a
href="https://doi.org/10.1049/ipr2.70056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately extracting blood vessel structures from retinal fundus images is critical for the early diagnosis and treatment of various ocular and systemic diseases. However, retinal vessel segmentation continues to face significant challenges. Firstly, capturing the boundary information of small vessels is particularly difficult. Secondly, uneven vessel thickness and irregular distribution further complicate the multi-scale feature modelling. Lastly, low-contrast images lead to increased background noise, further affecting the segmentation accuracy. To tackle these challenges, this article presents a multi-scale segmentation network that combines edge features and attention mechanisms, referred to as EANet. It demonstrates significant advantages over existing methods. Specifically, EANet consists of three key modules: the edge feature enhancement module, the multi-scale information interaction encoding module, and the multi-class attention mechanism decoding module. Experimental results validate the effectiveness of the method. Specifically, EANet outperforms existing advanced methods in the precise segmentation of small and multi-scale vessels and in effectively filtering background noise to maintain segmentationÂ continuity.},
  archive      = {J_IETIP},
  author       = {Jiangyi Zhang and Yuxin Tan and Duantengchuan Li and Guanghui Xu and Fuling Zhou},
  doi          = {10.1049/ipr2.70056},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70056},
  shortjournal = {IET Image Process.},
  title        = {EANet: Integrate edge features and attention mechanisms multi-scale networks for vessel segmentation in retinal images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-shadow scenarios tennis ball detection by an improved
RTMdet-light model. <em>IETIP</em>, <em>19</em>(1), e70054. (<a
href="https://doi.org/10.1049/ipr2.70054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time and rapid recording of sport sensor data related to tennis ball trajectories facilitates the analysis of this information and the development of intelligent training regimes. However, there are three essential challenges in the task of tennis ball recognition using sport vision sensors: the small size of the ball, its high speed, and the complex match scenarios. As a result, this paper considers a lightweight object detection model named improved RTMDet-light to deal with these challenges. Specifically, it has compatible capacities in the backbone and neck, constructed by a basic building block that consists of large-kernel depth-wise convolutions. Furthermore, GhosNet and ShuffleNet are used to replace the CSPLayers which reduce the parameters of our model. The lightweight model proposed addresses the inherent challenges of detecting small objects and muti scenarios in the match. After training, the proposed model performed better on four scenarios with different shades of tennis ball match, with results visualized through heatmaps and performance metrics tabulated for detailed analysis. The recall, FLOPs and number of parameters of the improved RTMDet-light are 71.4%, 12.543G, and 4.874M, respectively. The results demonstrate robustness and effectiveness of our model in accurate tennis ball detecting across various scales. In conclusion, our model for real-time detection in tennis ball detection offers a lightweight and faster solution for sport sensors.},
  archive      = {J_IETIP},
  author       = {Yukun Zhu and Yanxia Peng and Cong Yu},
  doi          = {10.1049/ipr2.70054},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70054},
  shortjournal = {IET Image Process.},
  title        = {Multi-shadow scenarios tennis ball detection by an improved RTMdet-light model},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human behaviour recognition method based on SME-net.
<em>IETIP</em>, <em>19</em>(1), e70053. (<a
href="https://doi.org/10.1049/ipr2.70053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal, motion and channel information are pivotal in video-based behaviour recognition. Traditional 2D CNNs demonstrate low computational complexity but fail to capture temporal dynamics effectively. Conversely, 3D CNNs excel in recognising temporal patterns but at the cost of significantly higher computational demands. To address these challenges, we propose a generic and effective SME module composed of three parallel sub-modules, namely Spatio-Temporal Excitation (STE), Motion Excitation (ME) and Efficient Channel Excitation (ECE). Specifically, the STE module enhances the spatiotemporal representation using a single-channel 3D convolution, enabling the model to focus on both temporal and spatial features. The ME module emphasises motion-sensitive channels by calculating feature map differences at adjacent time steps, guiding the model toward motion-centric regions. The ECE module efficiently captures cross-channel interactions without dimensionality reduction, ensuring robust performance while significantly reducing model complexity. Pre-trained on the ImageNet dataset, the proposed method achieved Top-1 accuracy of 49.0% on the Something-Something V1 (Sth-Sth V1) dataset and 40.8% on the Diving48 dataset. Extensive ablation studies and comparative experiments further demonstrate that the proposed method strikes an optimal balance between recognition accuracy and computational efficiency.},
  archive      = {J_IETIP},
  author       = {Ruimin Li and Yajuan Jia and Dan Yao and Fuquan Pan},
  doi          = {10.1049/ipr2.70053},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70053},
  shortjournal = {IET Image Process.},
  title        = {Human behaviour recognition method based on SME-net},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMFNet: A three-stage feature matching network with
geometric consistency and attentional enhancement. <em>IETIP</em>,
<em>19</em>(1), e70050. (<a
href="https://doi.org/10.1049/ipr2.70050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current feature matching methods typically employ a two-stage process, consisting of coarse and fine matching. However, the transition from the coarse to the fine stage often lacks an effective intermediate state, leading to abrupt changes in the matching process. This can hinder smooth transitions and precise localization. To address these limitations, this study introduces Coarse-Mid-Fine Match Net (CMFNet), a novel three-stage image feature matching method. CMFNet incorporates an intermediate-grained matching phase between the coarse and fine stages to facilitate a more gradual and seamless transition. In the proposed method, the intermediate-grained matching refines the correspondences obtained from the coarse-grained stage using Adaptive-random sample consensus (RANSAC). Subsequently, the midtransformer, which integrates sparse self-attention (SSA) mechanisms with local-feature-based cross-attention, is employed for feature extraction. This approach enhances the feature extraction capabilities and improves the adaptability to various types of image data, thereby boosting overall matching performance. Additionally, a cross-attention mechanism based on local region features is introduced. The network undergoes fully self-supervised training, aiming to minimize a match loss that is autonomously generated from the training data using a multi-scale cross-entropy method. A series of thorough experiments was carried out on diverse real-world datasets, including both unaltered and extensively processed images.The results demonstrate that the proposed method outperforms state-of-the-art approaches, achieving 0.776 mAUC on the HPatches dataset and 0.442 mAUC on the ISC-HE dataset.},
  archive      = {J_IETIP},
  author       = {RenKai Xiao and ShengZhi Yuan and Kai Jin and Min Li and Yan Tang and Sen Shen},
  doi          = {10.1049/ipr2.70050},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70050},
  shortjournal = {IET Image Process.},
  title        = {CMFNet: A three-stage feature matching network with geometric consistency and attentional enhancement},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frame extraction person retrieval framework based on
improved YOLOv8s and the stage-wise clustering person re-identification.
<em>IETIP</em>, <em>19</em>(1), e70046. (<a
href="https://doi.org/10.1049/ipr2.70046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID), a crucial research area in smart city security, faces challenges due to person posture changes, object occlusion and other factors, making it difficult for existing methods to accurately retrieving target person in video surveillance. To resolve this problem, we propose a person retrieval framework that integrates YOLOv8s and person Re-ID. Improved YOLOv8s is employed to extract person categories from the video on a frame-by-frame basis, and when combined with the stage-wise clustering person Re-ID network (SCPN), it enables collaborative person retrieval across multiple cameras. Notably, a feature precision (FP) module is added in the YOLOv8s network to form FP-YOLOv8s, and SCPN incorporates innovative enhancements including the stage-wise learning rate scheduler, centralized clustering loss and adaptive representation joint attention module into the person Re-ID baseline model. Comprehensive experiments on COCO, Market-1501 and DukeMTMC-ReID datasets demonstrate that our proposed framework outperforms several other leading methods. Given the scarcity of image-video person Re-ID datasets, we also provide an extended image-video person (EIVP) dataset, which contains 102 videos and 814 bounding boxes of 57 identities captured by 8 cameras. The video reasoning detection score of this framework reaches 78.8% on this dataset, indicating a 3.2% increase compared to conventional models.},
  archive      = {J_IETIP},
  author       = {Jianjun Zhuang and Nan Wang and Yuchen Zhuang and Yong Hao},
  doi          = {10.1049/ipr2.70046},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70046},
  shortjournal = {IET Image Process.},
  title        = {Frame extraction person retrieval framework based on improved YOLOv8s and the stage-wise clustering person re-identification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic uncertainty-awared for semantic segmentation of
remote sensing images. <em>IETIP</em>, <em>19</em>(1), e70045. (<a
href="https://doi.org/10.1049/ipr2.70045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image segmentation is crucial for applications ranging from urban planning to environmental monitoring. However, traditional approaches struggle with the unique challenges of aerial imagery, including complex boundary delineation and intricate spatial relationships. To address these limitations, we introduce the semantic uncertainty-aware segmentation (SUAS) method, an innovative plug-and-play solution designed specifically for remote sensing image analysis. SUAS builds upon the rotated multi-scale interaction network (RMSIN) architecture and introduces the prompt refinement and uncertainty adjustment module (PRUAM). This novel component transforms original textual prompts into semantic uncertainty-aware descriptions, particularly focusing on the ambiguous boundaries prevalent in remote sensing imagery. By incorporating semantic uncertainty, SUAS directly tackles the inherent complexities in boundary delineation, enabling more refined segmentations. Experimental results demonstrate SUAS&#39;s effectiveness, showing improvements over existing methods across multiple metrics. SUAS achieves consistent enhancements in mean intersection-over-union (mIoU) and precision at various thresholds, with notable performance in handling objects with irregular and complex boundariesâa persistent challenge in aerial imagery analysis. The results indicate that SUAS&#39;s plug-and-play design, which leverages semantic uncertainty to guide the segmentation task, contributes to improved boundary delineation accuracy in remote sensing imageÂ analysis.},
  archive      = {J_IETIP},
  author       = {Xiangfeng Qiu and Zhilin Zhang and Xin Luo and Xiang Zhang and Youcheng Yang and Yundong Wu and Jinhe Su},
  doi          = {10.1049/ipr2.70045},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70045},
  shortjournal = {IET Image Process.},
  title        = {Semantic uncertainty-awared for semantic segmentation of remote sensing images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tri-plane dynamic neural radiance fields for high-fidelity
talking portrait synthesis. <em>IETIP</em>, <em>19</em>(1), e70044. (<a
href="https://doi.org/10.1049/ipr2.70044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural radiation field (NeRF) has been widely used in the field of talking portrait synthesis. However, the inadequate utilisation of audio information and spatial position leads to the inability to generate images with high audio-lip consistency and realism. This paper proposes a novel tri-plane dynamic neural radiation field (Tri-NeRF) that employs an implicit radiation field to study the impacts of audio on facial movements. Specifically, Tri-NeRF propose tri-plane offset network (TPO-Net) to offset spatial positions in three 2D planes guided by audio. This allows for sufficient learning of audio features from image features in a low dimensional state to generate more accurate lip movements. In order to better preserve facial texture details, we innovatively propose a new gated attention fusion module (GAF) to dynamically fuse features based on strong and weak correlation of cross-modal features. Extensive experiments have demonstrated that Tri-NeRF can generate talking portraits with audio-lip consistency andÂ realism.},
  archive      = {J_IETIP},
  author       = {Xueping Wang and Xueni Guo and Jun Xu and Yuchen Wu and Feihu Yan and Guangzhe Zhao},
  doi          = {10.1049/ipr2.70044},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70044},
  shortjournal = {IET Image Process.},
  title        = {Tri-plane dynamic neural radiance fields for high-fidelity talking portrait synthesis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A face quality assessment system for unattended face
recognition: Design and implementation. <em>IETIP</em>, <em>19</em>(1),
e70042. (<a href="https://doi.org/10.1049/ipr2.70042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a face quality assessment approach that selects the highest-quality face image using a two-stage process from video streaming. In high-traffic environments, traditional face recognition methods can cause crowd congestion, emphasizing the need for unconscious face recognition, which requires no active cooperation from individuals. Due to the nature of unconscious face recognition, it is necessary to capture high-quality face images. In this paper, the FSA-Net head pose estimation network is enhanced to FSA-Shared_Nadam by replacing the Adam optimizer with Nadam and improving stage fusion. In the first stage, FSA-Shared_Nadam estimates head pose angles, MediaPipe detects facial landmarks to calculate eye distance and aspect ratios, and sharpness is calculated using the Laplacian operator. Images are considered valid if they meet the criteria. A model trains a face quality scoring formula, learning how different head pose angles affect face recognition accuracy. In the second stage, face images are clustered, and the formula is applied to select the highest-scoring face within each cluster. The approach was tested across multiple datasets, and a simulated security checkpoint scenario was created for practical testing. The results demonstrate the effectiveness of the FSA-Shared_Nadam head pose estimation algorithm and the proposed face quality assessment approach.},
  archive      = {J_IETIP},
  author       = {Dunli Hu and Xin Bi and Wei Zhao and Xiaoping Zhang and Xingchen Duan},
  doi          = {10.1049/ipr2.70042},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70042},
  shortjournal = {IET Image Process.},
  title        = {A face quality assessment system for unattended face recognition: Design and implementation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retinal fundus image enhancement with detail highlighting
and brightness equalizing based on image decomposition. <em>IETIP</em>,
<em>19</em>(1), e70041. (<a
href="https://doi.org/10.1049/ipr2.70041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality retinal fundus images are widely used by ophthalmologists for the detection and diagnosis of eye diseases, diabetes, and hypertension. However, in retinal fundus imaging, the reduction in image quality, characterized by poor local contrast and non-uniform brightness, is inevitable. Image enhancement becomes an essential and practical strategy to address these issues. In this paper, we propose a retinal fundus image enhancement method that emphasizes details and equalizes brightness, based on image decomposition. First, the original image is decomposed into three layers using an edge-preserving filter: a base layer, a detail layer, and a noise layer. Second, an adaptive local power-law approach is applied to the base layer for brightness equalization, while detail enhancement is achieved for the detail layer through saliency analysis and blue channel removal. Finally, the base and detail layers are combined, excluding the noise layer, to synthesize the final image. The proposed method is evaluated and compared with both classical and recent approaches using two widely adopted datasets. According to the experimental results, both subjective and objective assessments demonstrate that the proposed method effectively enhances retinal fundus images by highlighting details, equalizing brightness, and suppressing noise and artifacts, all without causing color distortion.},
  archive      = {J_IETIP},
  author       = {Zhiyi Wu and Lucy J. Kessler and Xiang Chen and Yiguo Pan and Xiaoxia Yang and Ling Zhao and Jufeng Zhao and Gerd U. Auffarth},
  doi          = {10.1049/ipr2.70041},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70041},
  shortjournal = {IET Image Process.},
  title        = {Retinal fundus image enhancement with detail highlighting and brightness equalizing based on image decomposition},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian process-driven semi-supervised single-image rain
removal: Enhancing real-scene generalizability. <em>IETIP</em>,
<em>19</em>(1), e70040. (<a
href="https://doi.org/10.1049/ipr2.70040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a semi-supervised single-image rain removal method using Gaussian processes to decouple rain components and background features. Existing methods often fail to generalize to real scenes due to synthetic data&#39;s limited diversity in rain direction and density. To address this, we integrate synthetic and real rainy images, where Gaussian processes model synthetic intermediate features to generate pseudo-labels for real image supervision. A two-stage encoderâdecoder architecture with squeeze-and-excitation residual and context feature fusion modules enhances feature disentanglement. Experimental results on both synthetic and real datasets demonstrate superior performance, achieving a peak signal-to-noise ratio of 26.11Â dB and structural similarity of 0.89 on synthetic images, while preserving more background details and effectively supporting downstream tasks like object segmentation.},
  archive      = {J_IETIP},
  author       = {Lisha Liu and Peiquan Xiong and Fei Liu},
  doi          = {10.1049/ipr2.70040},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70040},
  shortjournal = {IET Image Process.},
  title        = {Gaussian process-driven semi-supervised single-image rain removal: Enhancing real-scene generalizability},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial blemish detection based on YOLOv8n optimised with
space-to-depth and GCNet attention mechanisms. <em>IETIP</em>,
<em>19</em>(1), e70039. (<a
href="https://doi.org/10.1049/ipr2.70039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial blemishes are small and often similar in colour to the surrounding skin, making detection even more challenging. This paper proposes an improved algorithm based on YOLOv8 to address the limitations of the original YOLOv8n in facial blemish detection. First, we introduce space-to-depth-convolution (SPD-Conv), which replaces traditional downsampling methods in convolutional neural networks, preserving spatial details without reducing the image resolution. This enhances the model&#39;s ability to detect small imperfections. Additionally, the integration of GCNet helps detect blemishes that closely resemble surrounding skin tones by leveraging global context modelling. The improved model better understands the overall structure and features of the face. Experimental results show that our model achieves a 5.3% and 5.6% improvement in mAP50 and mAP50-95, respectively, over YOLOv8n. Furthermore, it outperforms the latest YOLOv11n model by 6.9% and 7.2% in mAP50 and mAP50-95.},
  archive      = {J_IETIP},
  author       = {Shuxi Zhou and Lijun Liang},
  doi          = {10.1049/ipr2.70039},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70039},
  shortjournal = {IET Image Process.},
  title        = {Facial blemish detection based on YOLOv8n optimised with space-to-depth and GCNet attention mechanisms},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved few-shot object detection method based on faster
r-CNN. <em>IETIP</em>, <em>19</em>(1), e70038. (<a
href="https://doi.org/10.1049/ipr2.70038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uneven distribution of object features and insufficient feature learning significantly affect the accuracy and generalizability of existing detection methods. This paper proposes an improved two-stage few-shot object detection method that builds upon the faster region-based convolutional neural network framework to enhance its performance in detecting objects with limited training data. First, a modified data augmentation method for optical images is introduced, and a Gaussian optimization module of sample feature distribution is constructed to enhance the model&#39;s generalizability. Second, a parameter-less 3D space attention module without additional parameters, is added to enhance the space features of a sample, where a neuron linear separability measurement and feature optimization module based on mathematical operations are used to adjust the feature distribution and reduce data distribution bias. Finally, a class feature vector extractor based on meta-learning is provided to reconstruct the feature map by overlaying a class feature vector from the target domain onto the query image. This process improves accuracy and generalization performance, and multiple experiments on the PASCAL VOC dataset show that the proposed method has higher detection accuracy and stronger generalizability than other methods. Especially, the experiment using practical images under complicated environments indicates its potential effectiveness in real-world scenarios.},
  archive      = {J_IETIP},
  author       = {YangJie Wei and Shangwei Long and Yutong Wang},
  doi          = {10.1049/ipr2.70038},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70038},
  shortjournal = {IET Image Process.},
  title        = {Improved few-shot object detection method based on faster R-CNN},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater image enhancement based on depth and light
attenuation estimation. <em>IETIP</em>, <em>19</em>(1), e70037. (<a
href="https://doi.org/10.1049/ipr2.70037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light attenuation and complex water environments seriously deteriorate underwater imaging quality. Current underwater image restoration algorithms cannot handle low-quality colour-distorted images in aquatic environments. This study proposed a novel underwater image processing algorithm based on a light attenuation estimation model and a depth estimation network. First, a pseudo-depth map strategy was proposed to train the underwater image depth estimation network to realise underwater image depth estimation. Second, the attenuation coefficient of the current image was estimated based on the background light using a light attenuation model. Finally, the images were restored using an underwater imaging model. The proposed algorithm is superior to state-of-the-art underwater image processing algorithms regarding subjective and objective qualities.},
  archive      = {J_IETIP},
  author       = {Lianjun Zhang and Tingna Liu and Qichao Shi and Fen Chen},
  doi          = {10.1049/ipr2.70037},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70037},
  shortjournal = {IET Image Process.},
  title        = {Underwater image enhancement based on depth and light attenuation estimation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale frequency enhancement network for blind image
deblurring. <em>IETIP</em>, <em>19</em>(1), e70036. (<a
href="https://doi.org/10.1049/ipr2.70036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deblurring is a fundamental preprocessing technique aimed at recovering clear and detailed images from blurry inputs. However, existing methods often struggle to effectively integrate multi-scale feature extraction with frequency enhancement, limiting their ability to reconstruct fine textures, especially in the presence of non-uniform blur. To address these challenges, we propose a multi-scale frequency enhancement network (MFENet) for blind image deblurring. MFENet introduces a multi-scale feature extraction module (MS-FE) based on depth-wise separable convolutions to capture rich multi-scale spatial and channel information. Furthermore, the proposed method employs a frequency enhanced blur perception module (FEBP) that utilizes wavelet transforms to extract high-frequency details and multi-strip pooling to perceive non-uniform blur. Experimental results on the GoPro and HIDE datasets demonstrate that our method achieves superior deblurring performance in both visual quality and objective evaluation metrics. Notably, in downstream object detection tasks, our blind image deblurring algorithm significantly improves detection accuracy, further validating its effectiveness and robustness in practicalÂ applications.},
  archive      = {J_IETIP},
  author       = {YaWen Xiang and Heng Zhou and Xi Zhang and ChengYang Li and ZhongBo Li and YongQiang Xie},
  doi          = {10.1049/ipr2.70036},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70036},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale frequency enhancement network for blind image deblurring},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing self-supervised monocular depth estimation in
endoscopy via feature-based perceptual loss. <em>IETIP</em>,
<em>19</em>(1), e70035. (<a
href="https://doi.org/10.1049/ipr2.70035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, self-supervised learning methods for monocular depth estimation have garnered significant attention due to their ability to learn from large amounts of unlabelled data. In this study, we propose further improvements for endoscopic scenes based on existing self-supervised monocular depth estimation methods. The previous method introduce an appearance flow to address brightness inconsistencies caused by lighting changes and uses a unified self-supervised framework to estimate both depth and camera motion simultaneously. However, to further enhance the model&#39;s supervisory signals, we introduce a new feature-based perceptual loss. This module utilizes a pre-trained encoder to extract features from both the synthesized and target frames and calculates their cosine dissimilarity as an additional source of supervision. In this way, we aim to improve the model&#39;s robustness in handling complex lighting and surface reflection conditions in endoscopic scenes. We compare the performance of using two pre-trained CNN-based models and four foundational models as encoder. Experimental results show that our improve method further enhances the accuracy of depth estimation in medical imaging. Additionally, it demonstrates that features extracted by CNN-based models, which are sensitive to local details, outperform foundation models. This suggests that encoders for extracting medical image features may not require extensive pre-training, and relatively simple traditional convolutional neural networks canÂ suffice.},
  archive      = {J_IETIP},
  author       = {Kejin Zhu and Li Cui},
  doi          = {10.1049/ipr2.70035},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70035},
  shortjournal = {IET Image Process.},
  title        = {Enhancing self-supervised monocular depth estimation in endoscopy via feature-based perceptual loss},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-channel deep pulse-coupled net: A novel bearing fault
diagnosis framework. <em>IETIP</em>, <em>19</em>(1), e70033. (<a
href="https://doi.org/10.1049/ipr2.70033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bearings are a critical part of various industrial equipment. Existing bearing fault detection methods face challenges such as complicated data preprocessing, difficulty in analysing time series data, and inability to learn multi-dimensional features, resulting in insufficient accuracy. To address these issues, this study proposes a novel bearing fault diagnosis model called multi-channel deep pulse-coupled net (MC-DPCN) inspired by the mechanisms of image processing in the primary visual cortex of the brain. Initially, the data are transformed into greyscale spectrograms, allowing the model to handle time series data effectively. The method introduces a convolutional coupling mechanism between multiple channels, enabling the framework can learn the features on all channels well. This study conducted experiments using the bearing fault dataset from Case Western Reserve University. On this dataset, a 6-channel (adjustable to specific tasks) MC-DPCN was utilized to analyse one normal class and three fault classes. Compared to state-of-the-art bearing fault diagnosis methods, our model demonstrates one of the highest diagnostic accuracies. This method achieved an accuracy of 99.96% in normal vs. fault discrimination and 99.89% in fault type diagnosis (average result of ten-fold cross-validation).},
  archive      = {J_IETIP},
  author       = {Yanxi Wu and Yalin Yang and Zhuoran Yang and Zhizhuo Yu and Jing Lian and Bin Li and Jizhao Liu and Kaiyuan Yang},
  doi          = {10.1049/ipr2.70033},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70033},
  shortjournal = {IET Image Process.},
  title        = {Multi-channel deep pulse-coupled net: A novel bearing fault diagnosis framework},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-aware transformer for shadow detection.
<em>IETIP</em>, <em>19</em>(1), e70031. (<a
href="https://doi.org/10.1049/ipr2.70031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection helps reduce ambiguity in object detection and tracking. However, existing shadow detection methods tend to misidentify complex shadows and their similar patterns, such as soft shadow regions and shadow-like regions, since they treat all cases equally, leading to an incomplete structure of the detected shadow regions. To alleviate this issue, we propose a structure-aware transformer network (STNet) for robust shadow detection. Specifically, we first develop a transformer-based shadow detection network to learn significant contextual information interactions. To this end, a context-aware enhancement (CaE) block is also introduced into the backbone to expand the receptive field, thus enhancing semantic interaction. Then, we design an edge-guided multi-task learning framework to produce intermediate and main predictions with a rich structure. By fusing these two complementary predictions, we can obtain an edge-preserving refined shadow map. Finally, we introduce an auxiliary semantic-aware learning to overcome the interference from complex scenes, which facilitates the model to perceive shadow and non-shadow regions using a semantic affinity loss. By doing these, we can predict high-quality shadow maps in different scenarios. Experimental results demonstrate that our method reduces the balance error rate (BER) by 4.53%, 2.54%, and 3.49% compared to state-of-the-art (SOTA) methods on the benchmark datasets SBU, ISTD, and UCF,Â respectively.},
  archive      = {J_IETIP},
  author       = {Wanlu Sun and Liyun Xiang and Wei Zhao},
  doi          = {10.1049/ipr2.70031},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70031},
  shortjournal = {IET Image Process.},
  title        = {Structure-aware transformer for shadow detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new genetic algorithm-based network for text localization
in degraded social media images. <em>IETIP</em>, <em>19</em>(1), e70030.
(<a href="https://doi.org/10.1049/ipr2.70030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel model for understanding social image content through text localization. For text localization, we explore maximally stable extremal regions (MSER) for detecting components that work by clustering pixels with similar properties. The output of component detection includes several non-text components due to the degradations of social media images. To select the best components among many, we explore the genetic algorithm by convolving different kernels with components, which results in a feature matrix that is further fed to EfficientNet for choosing actual text components. Therefore, the proposed model is called genetic algorithm based network for text localization in degraded social media images (TLDSMI). For evaluating text localization, we consider the images of the standard dataset of natural scenes by uploading and downloading from different social media platforms, namely, WhatsApp, Telegram, and Instagram. The effectiveness of our method is shown by testing on original and degraded standard datasets. For example, for the degraded images of different complexities including degradations caused by social media platforms, the proposed method performs well in almost all situations. In addition, the proposed model achieves the best F1-Score, 0.76, 0.77, 0.70, and 0.78 for the degraded images of CUTE, ICDAR 2013, Total-Text, and CTW1500, respectively, compared to the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Shivakumara Palaiahnakote and Chandrahas Pavan Kumar and Pranjal Aggarwal and Shubham Sharma and Pasupuleti Chandana and Mahadveppa Basavanna and Umapada Pal},
  doi          = {10.1049/ipr2.70030},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70030},
  shortjournal = {IET Image Process.},
  title        = {A new genetic algorithm-based network for text localization in degraded social media images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning discriminative palmprint anti-spoofing features via
high-frequency spoofing regions adaptation. <em>IETIP</em>,
<em>19</em>(1), e70029. (<a
href="https://doi.org/10.1049/ipr2.70029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the majority of palmprint recognition studies have focused on feature extraction while neglecting security issues. Among the various attack types, spoofing attack poses a significant threat due to high success rates and minimal technical requirements. In this study, we explore the differences between real and fake palmprint images. Based on these differences, we propose the concept of âhigh-frequency spoofing regionsâ to capture key discriminative spoofing clues. Specifically, the high-frequency spoofing regions adaptation ( HFSRA ) model is proposed to address palmprint anti-spoofing. The HFSRA consists of two key modules: the texture analysis module (TAM) and the spoofing attention module (SAM). In particular, the TAM divides the input feature map into several patches and evaluates the texture distribution within each patch. Next, the SAM dynamically constructs an attention map by mapping the texture distribution to an attention weight matrix. This adaptive structure forces the model to focus on high-frequency spoofing regions, which improves the model&#39;s ability to extract meaningful spoofing clues effectively. Furthermore, we establish three experimental protocols for evaluating the performance of palmprint anti-spoofing models. These protocols provide a standardized evaluation framework for future studies. Extensive experiments conducted under these protocols demonstrate the effectiveness and competitiveness ofÂ HFSRA.},
  archive      = {J_IETIP},
  author       = {Chengcheng Liu and Huikai Shao and Dexing Zhong},
  doi          = {10.1049/ipr2.70029},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70029},
  shortjournal = {IET Image Process.},
  title        = {Learning discriminative palmprint anti-spoofing features via high-frequency spoofing regions adaptation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN-based super-resolution with enhanced multi-scale
laplacian pyramid and frequency domain loss. <em>IETIP</em>,
<em>19</em>(1), e70028. (<a
href="https://doi.org/10.1049/ipr2.70028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution techniques play an important role in the fields of image processing and computer vision. However, existing super-resolution methods based on generative adversarial networks still exhibit significant shortcomings in recovering high-frequency details and effectively utilising multi-scale information. To address these issues, this paper proposes an improved generative adversarial network. Specifically, an enhanced multi-scale Laplacian pyramid structure is designed to capture and process image details at different scales. Then, convolutional operations are added to each layer of the pyramid to further improve the recovery of multi-scale details. Additionally, a frequency domain loss is introduced, where the generated and real images are transformed into the frequency domain using Fourier transforms for comparison. This method enhances the reconstruction of high-frequency details. The experiments are validated on four publicly available datasets and the results show that the proposed network significantly outperforms existing methods in both reconstruction quality and visual performance.},
  archive      = {J_IETIP},
  author       = {Hao Chen and Xi Lu and Jixining Zhu},
  doi          = {10.1049/ipr2.70028},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70028},
  shortjournal = {IET Image Process.},
  title        = {GAN-based super-resolution with enhanced multi-scale laplacian pyramid and frequency domain loss},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SF-YOLO: A novel YOLO framework for small object detection
in aerial scenes. <em>IETIP</em>, <em>19</em>(1), e70027. (<a
href="https://doi.org/10.1049/ipr2.70027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection models are widely applied in the fields such as video surveillance and unmanned aerial vehicles to enable the identification and monitoring of various objects on a diversity of backgrounds. The general CNN-based object detectors primarily rely on downsampling and pooling operations, often struggling with small objects that have low resolution and failing to fully leverage contextual information that can differentiate objects from complex background. To address the problems, we propose a novel YOLO framework called SF-YOLO for small object detection. Firstly, we present a spatial information perception (SIP) module to extract contextual features for different objects through the integration of space to depth operation and large selective kernel module, which dynamically adjusts receptive field of the backbone and obtains the enhanced features for richer understanding of differentiation between objects and background. Furthermore, we design a novel multi-scale feature weighted fusion strategy, which performs weighted fusion on feature maps by combining fast normalized fusion method and CARAFE operation, accurately assessing the importance of each feature and enhancing the representation of small objects. The extensive experiments conducted on VisDrone2019, Tiny-Person and PESMOD datasets demonstrate that our proposed method enables comparable detection performance to state-of-the-art detectors.},
  archive      = {J_IETIP},
  author       = {Meng Sun and Le Wang and Wangyu Jiang and Fayaz Ali Dharejo and Guojun Mao and Radu Timofte},
  doi          = {10.1049/ipr2.70027},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70027},
  shortjournal = {IET Image Process.},
  title        = {SF-YOLO: A novel YOLO framework for small object detection in aerial scenes},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deformable attention network for efficient space-time video
super-resolution. <em>IETIP</em>, <em>19</em>(1), e70026. (<a
href="https://doi.org/10.1049/ipr2.70026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-time video super-resolution (STVSR) aims to construct high space-time resolution video sequences from low frame rate and low-resolution video sequences. While recent STVSR works combine temporal interpolation and spatial super-resolution in a unified framework, they face challenges in computational complexity across both temporal and spatial dimensions, particularly in achieving accurate intermediate frame interpolation and efficient temporal information utilisation. To address these, we propose a deformable attention network for efficient STVSR. Specifically, we introduce a deformable interpolation block that employs hierarchical feature fusion to effectively handle complex inter-frame motions at multiple scales, enabling more accurate intermediate frame generation. To fully utilise temporal information, we design a temporal feature shuffle block (TFSB) to efficiently exchange complementary information across multiple frames. Additionally, we develop a motion feature enhancement block incorporating channel attention mechanism to selectively emphasise motion-related features, further boosting TFSB&#39;s effectiveness. Experimental results on benchmark datasets definitively demonstrate that our proposed method achieves competitive performance in STVSRÂ tasks.},
  archive      = {J_IETIP},
  author       = {Hua Wang and Rapeeporn Chamchong and Phatthanaphong Chomphuwiset and Pornntiwa Pawara},
  doi          = {10.1049/ipr2.70026},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70026},
  shortjournal = {IET Image Process.},
  title        = {Deformable attention network for efficient space-time video super-resolution},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust text watermarking based on modifying the stroke
components of chinese characters. <em>IETIP</em>, <em>19</em>(1),
e70025. (<a href="https://doi.org/10.1049/ipr2.70025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional codebooks used for tracing information leakage in text documents often suffer from limitations in embedding capacity, robustness, and efficiency due to their manual generation process. This paper proposes a robust text watermarking method based on the stroke components of Chinese characters. By designing an innovative approach, Chinese character strokes are divided into several distinct components, with only specific ones being selectively modified to generate new glyphs, thus forming a unique codebook. The watermark signals are embedded by substituting the carrier glyph with the newly generated one, and the signals are extracted using a template matching method. Experimental results demonstrate that, compared to traditional manually designed codebooks, the proposed method significantly reduces human labor and computational overhead while maintaining high visual quality. Moreover, it exhibits superior robustness and adaptability across various challenging scenarios, including digital noise attacks, print-scanning attacks, and print-camera capture, making it a highly effective solution for protecting textualÂ information.},
  archive      = {J_IETIP},
  author       = {Hai Chen and Yanli Chen and Zhicheng Dong and Yongrong Wang and Asad Malik and Hanzhou Wu},
  doi          = {10.1049/ipr2.70025},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70025},
  shortjournal = {IET Image Process.},
  title        = {Robust text watermarking based on modifying the stroke components of chinese characters},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight accelerated unfolding network with collaborative
attention for snapshot spectral compressive imaging. <em>IETIP</em>,
<em>19</em>(1), e70024. (<a
href="https://doi.org/10.1049/ipr2.70024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In coded aperture snapshot spectral imaging (CASSI) systems, deep unfolding networks (DUNs) have made significant strides in recovering 3D hyperspectral images (HSIs) from a single 2D measurement. However, the inherent nonlinearity and ill-posed nature of HSI reconstruction continue to challenge existing methods in terms of accuracy and stability. To address these challenges, we propose a lightweight collaborative attention-enhanced accelerated unfolding network ( ), which integrates a DUN framework with a streamlined prior extractor. Our integrated approach introduces a generically accelerated half-quadratic splitting algorithm (A-HQS) for degradation estimation, overcoming the limitations of first-order optimization and enabling effective long-range dependency modeling. Within the prior extractor, we introduce cross-convergence attention, facilitating iterative information exchange between local and non-local Transformers to capture holistic features and enhance inductive capacity. Notably, the concept of collaborative cross-convergence is embedded throughout all submodules, ensuring effective information flow. The proposed not only accelerates the convergence of spectral reconstruction, but also fully exploits compressed spatial-spectral information. Numerical and visual comparisons on both synthetic and real datasets demonstrate the superior performance of this approach. Comparisons on both synthetic and real datasets illustrate the superiority of this approach. The source code is available at https://github.com/Mengjie-s/CA2UN .},
  archive      = {J_IETIP},
  author       = {Mengjie Qin and Yuchao Feng},
  doi          = {10.1049/ipr2.70024},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70024},
  shortjournal = {IET Image Process.},
  title        = {Lightweight accelerated unfolding network with collaborative attention for snapshot spectral compressive imaging},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flood-MATE: A flood segmentation model in urban regions
through adaptation of mean teacher and ensemble approach.
<em>IETIP</em>, <em>19</em>(1), e70023. (<a
href="https://doi.org/10.1049/ipr2.70023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flood disasters remain one of the most recurring natural phenomena worldwide, resulting from excessive water flow submerging land for an extended period of time. The escalating occurrences of floods, particularly in urban areas, can be attributed to climate change, extreme weather patterns, uncontrolled urbanization, and complex geographical conditions. To mitigate the destructive impacts, such as loss of life and economic ramifications, automatic flood analysis and remote-sensing imagery segmentation offer valuable decision-making insights. However, the segmentation process for flood detection faces challenges due to the scarcity of labelled data and diverse resolutions, including medium resolution data. In response, the authors propose Flood-MATE, a novel semi-supervised learning approach based on the mean-teacher model. Our approach leverages the deep learning architecture and introduces a new loss function scenario for training. The dataset utilized in this study comprises SAR images of Sentinel-1 C-band that have undergone thorough processing. Promisingly, the results demonstrate a 4% improvement in the IoU metric compared to the baseline method employingÂ pseudo-labelling.},
  archive      = {J_IETIP},
  author       = {Bella Septina Ika Hartanti and Adila Alfa Krisnadhi and Laksmita Rahadianti and Wiwiek Dwi Susanti and Achmad Fakhrus Shomim},
  doi          = {10.1049/ipr2.70023},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70023},
  shortjournal = {IET Image Process.},
  title        = {Flood-MATE: A flood segmentation model in urban regions through adaptation of mean teacher and ensemble approach},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRSE-YOLO: Efficient and lightweight architecture for
accurate waste detection. <em>IETIP</em>, <em>19</em>(1), e70022. (<a
href="https://doi.org/10.1049/ipr2.70022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces DRSE-YOLO, an efficient waste detection model designed to address detection accuracy and lightweight design challenges. The RCCA module in the model&#39;s neck enhances multi-scale feature representation, thereby improving detection performance. The DySample module optimizes upsampling through adaptive point-sampling, reducing computational demands and improving resource efficiency. The Slim-Neck module is applied to select convolutional layers and C2f modules to streamline the model and enhance computational efficiency. The ECC-Head integrates asymmetric depth convolution, point convolution, and an attention mechanism, balancing accuracy with reduced parameters and computational load. Evaluated on a custom dataset comprising 46 waste classes and approximately 25,000 images, DRSE-YOLO achieves significant improvements over YOLOv8n, including a higher mAP@0.5 (+1.59%) and mAP@0.5:95 (+2.08%), alongside a reduced parameter count (2.43Â M vs. 3.2Â M) and GFLOPs (5.8Â vs. 8.2, a 24.4% reduction). These results underscore DRSE-YOLO&#39;s efficiency and accuracy.},
  archive      = {J_IETIP},
  author       = {Guangling Sun and Fenqi Zhang},
  doi          = {10.1049/ipr2.70022},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70022},
  shortjournal = {IET Image Process.},
  title        = {DRSE-YOLO: Efficient and lightweight architecture for accurate waste detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibration method for ultra-wide FOV fisheye cameras based
on improved camera model and SE(3) image pre-correction. <em>IETIP</em>,
<em>19</em>(1), e70021. (<a
href="https://doi.org/10.1049/ipr2.70021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The severe radial distortion of ultra-wide field of view (FOV) fisheye camera results in poor model fitting and challenges in calibration board detection. In this paper, a novel calibration method for ultra-wide FOV fisheye cameras is proposed based on improved camera model and SE(3) image pre-correction. Initially, a method to extend the maximum fitting FOV of the camera model to over 180 degrees is proposed. Subsequently, a calibration board detection approach is proposed using SE(3) image pre-correction. Specifically, image pre-correction is incorporated into the camera calibration process, utilizing SE(3) to define the pre-correction plane. Calibration boards are detected within the pre-corrected images, enhancing the reliability, accuracy and speed of board detection in distorted images, consequently increasing the control point&#39;s maximum FOV. Lastly, the improved camera model and SE(3) image pre-correction are integrated into a feedback-based camera calibration system for ultra-wide FOV fisheye cameras. Operating with real-time or offline video streams as input, this system autonomously selects calibration key frames, optimizes camera parameters and calibration board poses in real-time. Simulation and real-world experiments verify the effectiveness of the proposed method, leading to a 62% increase in the achievable maximum FOV.},
  archive      = {J_IETIP},
  author       = {Rui Xing and Fenghua He and Yu Yao},
  doi          = {10.1049/ipr2.70021},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70021},
  shortjournal = {IET Image Process.},
  title        = {Calibration method for ultra-wide FOV fisheye cameras based on improved camera model and SE(3) image pre-correction},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of overlay targetâs centre positioning
algorithms using customizable shape fitting for high-precision wafer
bonding. <em>IETIP</em>, <em>19</em>(1), e70020. (<a
href="https://doi.org/10.1049/ipr2.70020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wafer bonding is a critical process in 3D integration, and overlay (OVL) metrology is essential for its success. Accurately positioning the centre of OVL targets is fundamental for effective metrology. However, the identification and localization of target centres become challenging due to complex shapes and unexpected features, such as rounded corners, that can arise during manufacturing. An algorithm is proposed to tackle this challenge by employing customizable shape fitting. This method begins with the extraction of sub-pixel edge points, followed by applying a Hough transform to group and smooth these points, thereby enhancing contour quality. By parameterizing the target shape based on specific points, the algorithm integrates sub-pixel traversal techniques with an optimization objective, achieving sub-pixel accuracy in centre positioning. Simulation results indicate that the algorithm can achieve a positioning accuracy of Â±0.03 pixels and demonstrates robustness against noise and blur. Finally, the proposed algorithm was used to test the OVL target pair arrays fabricated by electron beam etching, confirming an accuracy of Â±0.04 pixels (Â±6.9Â nm). These results validate the algorithm&#39;s capability to meet high precision requirements for OVL target centre positioning in wafer applications.},
  archive      = {J_IETIP},
  author       = {Rui Wang and Yixian Zhu and Sen Lu and Kaiming Yang and Yu Zhu},
  doi          = {10.1049/ipr2.70020},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70020},
  shortjournal = {IET Image Process.},
  title        = {Development of overlay target&#39;s centre positioning algorithms using customizable shape fitting for high-precision wafer bonding},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RBS-YOLO: A lightweight YOLOv5-based surface defect
detection model for castings. <em>IETIP</em>, <em>19</em>(1), e70018.
(<a href="https://doi.org/10.1049/ipr2.70018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure precise and rapid identification of casting surface defects and to support the subsequent realisation of high-precision grinding, this study introduces a method for detecting casting surface defects using a lightweight YOLOv5 framework. The enhanced model integrates the ShuffleNetV2 high-efficiency CNN architecture into the YOLOv5 foundation, substantially reducing network parameters to achieve a lightweight model. Additionally, the Convolutional Block Attention Module (CBAM) attention mechanism is incorporated to enhance the model&#39;s capability to detect defects. The ReLU activation function replaces the SiLU function in the convolutional layer, decreasing the computational load and boosting efficiency. Subsequently, the optimised model is quantised and implemented on the RV1126 embedded development board, successfully performing image inference. To validate the effectiveness of the proposed method, a dataset of casting surface defects was designed and constructed. The optimised model has a file size of 7.6Â MB, representing 55.4% of the original model, with about 50.6% of the original model&#39;s parameters. The onboard inference speed of the improved model is 50Â ms per image, which is 9.1% faster than the traditional YOLOv5 model. These results offer valuable insights for future casting surface defect detection technologies.},
  archive      = {J_IETIP},
  author       = {KeZhu Wu and ShaoMing Sun and YiNing Sun and CunYi Wang and YiFan Wei},
  doi          = {10.1049/ipr2.70018},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70018},
  shortjournal = {IET Image Process.},
  title        = {RBS-YOLO: A lightweight YOLOv5-based surface defect detection model for castings},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image segmentation refinement based on region expansion and
minor contour adjustments. <em>IETIP</em>, <em>19</em>(1), e70017. (<a
href="https://doi.org/10.1049/ipr2.70017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-precision image segmentation tasks, even slight deviations in the segmentation results can bring about significant consequences, especially in certain application areas such as medical imaging and remote sensing image classification. The precision of segmentation has become the main factor limiting its development. Researchers typically refine image segmentation algorithms to enhance accuracy, but it is challenging for any improvement strategy to be effectively applied to images of different objects and scenes. To address this issue, we propose a two-step refinement method for image segmentation, comprising region expansion and minor contour adjustments. First, we design an adaptive gradient thresholding module to provide gradient-based constraints for the refinement process. Next, the region expansion module iteratively refines each segmented region based on colour differences and gradient thresholds. Finally, the minor contour adjustments module leverages local strong gradient features to refine the contour positions further. This method integrates region-level and pixel-level information to refine various image segmentation results. This method was applied to the BSDS500, Cells, and WHU Building datasets. The results demonstrate that the refined closed contours align more closely with the ground truth, with the most notable improvement observed at contour inflection points (corner points). Among the results, the Cells dataset showed the most significant improvement in segmentation accuracy, with the F-score increasing from 87.51% to 89.73% and IoU from 86.83% to 88.40%.},
  archive      = {J_IETIP},
  author       = {Li-yue Yan and Xing Zhang and Kafeng Wang and Siting Xiong and De-jin Zhang},
  doi          = {10.1049/ipr2.70017},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70017},
  shortjournal = {IET Image Process.},
  title        = {Image segmentation refinement based on region expansion and minor contour adjustments},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task collaboration for cross-modal generation and
multi-modal ophthalmic diseases diagnosis. <em>IETIP</em>,
<em>19</em>(1), e70016. (<a
href="https://doi.org/10.1049/ipr2.70016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal diagnosis of ophthalmic disease is becoming increasingly important because combining multi-modal data allows for more accurate diagnosis. Color fundus photograph (CFP) and optical coherence tomography (OCT) are commonly used as two non-invasive modalities for ophthalmic examination. However, the diagnosis of each modality is not entirely accurate. Compounding the challenge is the difficulty in acquiring multi-modal data, with existing datasets frequently lacking paired multi-modal data. To solve these problems, we propose multi-modal distribution fusion diagnostic algorithm and cross-modal generation algorithm. The multi-modal distribution fusion diagnostic algorithm first calculates the mean and variance separately for each modality, and then generates multi-modal diagnostic results in a distribution fusion manner. In order to generate the absent modality (mainly OCT data), three sub-networks are designed in the cross-modal generation algorithm: cross-modal alignment network, conditional deformable autoencoder and latent consistency diffusion model (LCDM). Finally, we propose multi-task collaboration strategy where diagnosis and generation tasks are mutually reinforcing to achieve optimal performance. Experimental results demonstrate that our proposed method yield superior results compared toÂ state-of-the-arts.},
  archive      = {J_IETIP},
  author       = {Yang Yu and Hongqing Zhu and Tianwei Qian and Tong Hou and Bingcang Huang},
  doi          = {10.1049/ipr2.70016},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70016},
  shortjournal = {IET Image Process.},
  title        = {Multi-task collaboration for cross-modal generation and multi-modal ophthalmic diseases diagnosis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TLBP: Tomography-aided local binary patterns with high
discrimination for image classification. <em>IETIP</em>, <em>19</em>(1),
e70015. (<a href="https://doi.org/10.1049/ipr2.70015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local binary patterns (LBP) play a vital role in image classification as a computationally efficient feature descriptor. A crucial reason for its limitation of discriminability is the lack of neighbourhood information description from a global perspective. Previous research has attempted to improve its performance by introducing global thresholds, but such threshold selection is not optimal. To address this issue, we propose a novel tomography-aided local binary patterns (TLBP), inspired by the tomographic process of sample separation. TLBP considers constructing visual feature representations under multi-level non-local information to compensate for the lack of LBP possessing only a single shallow feature. In addition to the basic LBP features from local visual context, TLBP captures refined neighbourhood greyscale information through multi-quantile thresholds from a global visual perspective, thereby greatly enhancing discriminability. Experimental results in texture classification, face recognition, and hyperspectral pixel-wise classification demonstrate that the proposed TLBP descriptor outperforms the competitors, achieving 94.39% (KTH-TIPS), 81.22% (KTH-TIPS-ROT), 93.81% (Indian Pines), 99.85% (Salinas), and 99.50% (ORL) accuracy. Furthermore, the performance of the T-variants that apply the tomographic idea to classic LBP descriptors improve significantly, especially for their rotation-invariantÂ versions.},
  archive      = {J_IETIP},
  author       = {Yichen Liu and Xin Zhang and Yanan Jiang and Chunlei Zhang and Hanlin Feng},
  doi          = {10.1049/ipr2.70015},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70015},
  shortjournal = {IET Image Process.},
  title        = {TLBP: Tomography-aided local binary patterns with high discrimination for image classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified you only look once network model for enhanced
traffic scene detection performance for small targets. <em>IETIP</em>,
<em>19</em>(1), e70014. (<a
href="https://doi.org/10.1049/ipr2.70014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to address the challenge of small target recognition in traffic scenes, we propose a model based on you only look once version 8X (Yolov8X) network model, which has been combined with receptive fields block (RFB) and multidimensional collaborative attention (MCA). First, the model employs the RFB to extract reliable and distinctive features, thereby enhancing the precision of small target identification. Furthermore, the MCA structure is introduced to simulate multidimensional attention through three parallel branches, thereby enhancing the feature expression ability of the model. This fragment describes a compression transformation and an excitation transformation that captures the differentiated feature representation of the command. These transformations facilitate the network&#39;s ability to locate and predict the location of small objects more accurately. Utilizing these transformations enhances the expressiveness and diversity of features, thereby improving the detection performance of small objects. Furthermore, data augmentation and hyperparameter optimization techniques are employed to enhance the model&#39;s generalisability. The validation results on the Argoverse 1.1 autonomous driving dataset demonstrate that the enhanced network model outperforms the prevailing detectors, achieving an F1 score of 78.6, an average precision of 55.1, and an average recall of 72.4. The algorithm&#39;s excellent performance for small target detection was demonstrated through visual analysis, proving its high application value and potential for promotion in fields such as autonomousÂ driving.},
  archive      = {J_IETIP},
  author       = {Lei Shi and Shuai Ren and Xing Fan and Ke Wang and Shan Lin and Zhanwen Liu},
  doi          = {10.1049/ipr2.70014},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70014},
  shortjournal = {IET Image Process.},
  title        = {Modified you only look once network model for enhanced traffic scene detection performance for small targets},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight multi-stage holistic attention-based network for
image super-resolution. <em>IETIP</em>, <em>19</em>(1), e70013. (<a
href="https://doi.org/10.1049/ipr2.70013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution images are crucial for many applications, but factors such as environmental conditions can reduce image quality. Super-resolution (SR) techniques address this by generating high-resolution images from low-resolution inputs. While deep learning SR models have made significant progress, they can be computationally expensive and struggle with differentiating between various image scales. Lightweight SR methods, suitable for resource-constrained devices, often compromise image quality. This study introduces a multi-stage holistic attention-based network, using Gaussian Laplacian pyramids to decompose images and apply holistic attention modules at each level. This approach reduces parameters and computational costs while maintaining image quality, achieving a PSNR score of 28 and SSIM of 0.91 with only 29,000 parameters. The model demonstrates the potential for efficient and high-quality image reconstruction. Future work will focus on improving quality while minimizing costs and exploring other advanced techniques. The code will be made available upon request},
  archive      = {J_IETIP},
  author       = {Aatiqa Bint E Ghazali and Ahsan Fiaz and Muhammad Islam},
  doi          = {10.1049/ipr2.70013},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70013},
  shortjournal = {IET Image Process.},
  title        = {Lightweight multi-stage holistic attention-based network for image super-resolution},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient method for detecting targets from remote sensing
images based on global attention mechanism. <em>IETIP</em>,
<em>19</em>(1), e70012. (<a
href="https://doi.org/10.1049/ipr2.70012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image target detection provides an effective and accurate data analysis tool for many application areas. Due to complex backgrounds, large differences in target scales, and missed detection of small targets, remote sensing image target detection is challenging. In order to enhance the model&#39;s understanding of the global information of remote sensing images, this paper proposes the GFA module. This module can establish the global contextual connection of remote sensing images to provide rich context to help understand the complex scene and background in which the target is located, without being limited to local information. Additionally, it focuses on channel information for enhanced target feature extraction. For the purpose of alleviating the serious imbalance in foregroundâbackground samples that is present in single-level target detection models. The loss function is reconstructed based on focal loss by redefining the balance factor Î± and focus factor Î³ , so that it can be dynamically adjusted during network training. Meanwhile, EIoU is used to further enhance the bounding box regression capability. Affine transformations were also used to augment the dataset in order to assist the model in adjusting to real-world situations. The proposed method is experimentally validated on the publicly available HRRSD dataset. In comparison with YOLO v5, the mAP of the detection results improved by 2.7%. Compared with YOLO v8 and YOLO v10, the mAP improved by 3.2% and 3.3%. The model achieves an FPS of 40.1, an optimal balance between speed and accuracy. Further, experiments are conducted using the NWPU VHR-10 dataset and the RSOD dataset, both of which demonstrated that the proposed method outperforms other target detection methods and improves remote sensing target detection performance.},
  archive      = {J_IETIP},
  author       = {Zijun Gao and Jingwen Su and Bo Li and Jue Wang and Zhankui Song},
  doi          = {10.1049/ipr2.70012},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70012},
  shortjournal = {IET Image Process.},
  title        = {Efficient method for detecting targets from remote sensing images based on global attention mechanism},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NewTalker: Exploring frequency domain for speech-driven 3D
facial animation with mamba. <em>IETIP</em>, <em>19</em>(1), e70011. (<a
href="https://doi.org/10.1049/ipr2.70011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current field of speech-driven 3D facial animation, transformer-based methods are limited in practical applications due to their high computational complexity. A new modelâNewTalkerâis proposed, which has core modules consisting of the residual bidirectional Mamba (RBM) and the timeâfrequency domain KolmogorovâArnold networks (TFK). The RBM module incorporates the philosophy of Mamba, enhancing the model&#39;s predictive ability for sequence data by utilizing both past and future contextual information, thereby reducing the computational complexity. The TFK module integrates the temporal and frequency domain information of audio data through KolmogorovâArnold networks, allowing the model to generate 3D facial animations smoothly while learning more detailed features. Extensive experiments and user studies have shown that the proposed NewTalker significantly surpasses current mainstream algorithms in terms of animation quality and inference speed, achieving the state-of-the-art level in this domain.},
  archive      = {J_IETIP},
  author       = {Weiran Niu and Zan Wang and Yi Li and Tangtang Lou},
  doi          = {10.1049/ipr2.70011},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70011},
  shortjournal = {IET Image Process.},
  title        = {NewTalker: Exploring frequency domain for speech-driven 3D facial animation with mamba},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSA-net: Global spatial structure-aware attention network
for liver segmentation in MR images with respiratory artifacts.
<em>IETIP</em>, <em>19</em>(1), e70010. (<a
href="https://doi.org/10.1049/ipr2.70010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic liver segmentation is of great significance for computer-aided treatment and surgery of liver diseases. However, respiratory motion often affects the liver, leading to image artifacts in liver magnetic resonance imaging (MRI) and increasing segmentation difficulty. To overcome this issue, we propose a global spatial structure-aware attention model (GSA-Net), a robust segmentation network developed to overcome the difficulties caused by respiratory motion. The GSA-Net is an encoder-decoder architecture, which extracts spatial structure information from images and identifies different objects using the minimum spanning tree algorithm. The network&#39;s encoder extracts multi-scale image features with the help of an effective and lightweight channel attention module. The decoder then transforms these features bottom-up using tree filter modules. Combined with the boundary detection module, the segmentation performance can be further improved. We evaluate the effectiveness of our method on two liver MRI benchmarks: one with respiratory artifacts and the other without. Numerical evaluations on different benchmarks demonstrate that GSA-Net consistently outperforms previous state-of-the-art models in terms of segmentation precision on our respiratory artifact dataset, and also achieves notable results on high-quality datasets.},
  archive      = {J_IETIP},
  author       = {Jiahuan Jiang and Dongsheng Zhou and Muzhen He and Xiaohan Yue and Shu Zhang},
  doi          = {10.1049/ipr2.70010},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70010},
  shortjournal = {IET Image Process.},
  title        = {GSA-net: Global spatial structure-aware attention network for liver segmentation in MR images with respiratory artifacts},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FMR-YOLO: An improved YOLOv8 algorithm for steel surface
defect detection. <em>IETIP</em>, <em>19</em>(1), e70009. (<a
href="https://doi.org/10.1049/ipr2.70009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the insufficient feature extraction capability for steel surface defects in industrial production, as well as issues such as low detection speed and poor accuracy caused by large model parameters, a metal surface defect detection algorithm named FMR-YOLO, based on an improved YOLOv8n, is proposed. The algorithm incorporates a fast lightweight feature extraction structure, the number of parameters and computation of the model are reduced while preserving the spatial information, thus improving the target detection performance. A multi-scale feature fusion module is introduced, enabling the extraction of more comprehensive and richer features compared to traditional single-scale methods, to better support defect detection tasks. Additionally, a receptive field attention structure, Receptive Field Attention Neck, is designed in the Neck part to expand the model&#39;s receptive field and reduce computational complexity, significantly improving detection accuracy for small defects. This allows the model to effectively capture both global and local features in complex industrial scenarios. The effectiveness of the improved FMR-YOLO algorithm is validated on two industrial surface defect datasets: GC10-DET and NEU-DET. Experimental results show that the mAP@0.5 detection accuracy has increased by 4.5% and 5.1% on the GC10-DET and NEU-DET datasets, respectively, with a parameter size of merely 2.7Â M.},
  archive      = {J_IETIP},
  author       = {Yongjing Ni and Qi Wu and Xiuqing Zhang},
  doi          = {10.1049/ipr2.70009},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70009},
  shortjournal = {IET Image Process.},
  title        = {FMR-YOLO: An improved YOLOv8 algorithm for steel surface defect detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMA-SSR: Optical-guided multi-kernel attention based SAR
image super-resolution reconstruction network. <em>IETIP</em>,
<em>19</em>(1), e70008. (<a
href="https://doi.org/10.1049/ipr2.70008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) has been widely studied and applied in many fields. Although image super-resolution technology has been successfully applied to SAR imaging in recent years, there is less research on large-scale factor SAR image super-resolution methods. A more effective method is to obtain comprehensive information to guide the reconstruction of SAR images. In fact, the co-registered characteristics of high-resolution optical images have been successfully applied to improve the quality of SAR images. Inspired by this, an optical-guided multi-kernel attention based SAR image super-resolution reconstruction network (OMA-SSR) is proposed. The proposed multi-modal mutual attention (MMA) module in this network can effectively establish the dependency between SAR image features and optical image features. This network also designs a deep feature extraction module for SAR images, which includes a channel-splitted multi-kernel attention (CSMA) module and residual connections. CSMA module splits SAR image channels, extracts features in different ranges through multi-kernel convolution, and finally fuses the extracted features between different channels. Experimental results on the Sen1-2 and QXS datasets show that the proposed OMA-SSR performs well in evaluation indicators and visual effects of SAR image super-resolutionÂ reconstruction.},
  archive      = {J_IETIP},
  author       = {Yanshan Li and Fan Xu},
  doi          = {10.1049/ipr2.70008},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70008},
  shortjournal = {IET Image Process.},
  title        = {OMA-SSR: Optical-guided multi-kernel attention based SAR image super-resolution reconstruction network},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiclassification tampering detection algorithm based on
spatial-frequency fusion and swin-t. <em>IETIP</em>, <em>19</em>(1),
e70007. (<a href="https://doi.org/10.1049/ipr2.70007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods for image forgery detection often struggle with compression attack robustness. This paper proposes a novel multi-class forgery detection framework combining spatial-frequency fusion with Swin-Transformer, outperforming existing methods in compression attack scenarios. Our approach integrates a frequency domain perception module with quantization tables, a spatial domain perception module through multi-strategy convolutions, and a dual-attention mechanism combining spatial and channel attention for feature fusion. Experimental results demonstrate superior performance with an F 1 score of 87% under JPEG compression ( q =Â 75), significantly surpassing current state-of-the-art methods by an average of 15% in compression resistance while maintaining high detectionÂ accuracy.},
  archive      = {J_IETIP},
  author       = {Li Li and Kejia Zhang and Jianfeng Lu and ShanQing Zhang and Ning Chu},
  doi          = {10.1049/ipr2.70007},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70007},
  shortjournal = {IET Image Process.},
  title        = {Multiclassification tampering detection algorithm based on spatial-frequency fusion and swin-T},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-cropping contrastive learning and domain consistency
for unsupervised image-to-image translation. <em>IETIP</em>,
<em>19</em>(1), e70006. (<a
href="https://doi.org/10.1049/ipr2.70006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised image-to-image (i2i) translation methods based on contrastive learning have achieved state-of-the-art results. However, in previous works, the negatives are sampled from the input image itself, which inspires us to design a data augmentation method to improve the quality of the selected negatives. Moreover, the previous methods only preserve the content consistency via patch-wise contrastive learning, which ignores the domain consistency between the generated images and the real images of the target domain. This paper proposes a novel unsupervised i2i translation framework based on multi-cropping contrastive learning and domain consistency, called MCDUT. Specifically, the multi-cropping views are obtained with the aim of further generating high-quality negative examples. To constrain the embeddings in the deep feature space, a new domain consistency loss is formulated, which encourages the generated images to be close to the real images.Â In many i2i translation tasks, this method achieves state-of-the-art results, and the advantages of this method have been proven through extensive comparison experiments and ablation research. The code of MCDUT is available at https://github.com/zhihefang/MCDUT .},
  archive      = {J_IETIP},
  author       = {Chen Zhao and Wei-Ling Cai and Zheng Yuan and Cheng-Wei Hu},
  doi          = {10.1049/ipr2.70006},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70006},
  shortjournal = {IET Image Process.},
  title        = {Multi-cropping contrastive learning and domain consistency for unsupervised image-to-image translation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LGS-net: A lightweight convolutional neural network based on
global feature capture for spatial image steganalysis. <em>IETIP</em>,
<em>19</em>(1), e70005. (<a
href="https://doi.org/10.1049/ipr2.70005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of image steganalysis is to detect whether the transmitted images in network communication contain secret messages. Current image steganalysis networks still have some problems such as inappropriate feature selection and easy overfitting. Therefore, this paper proposed a new spatial image steganalysis method based on convolutional neural networks. To extract richer features while reducing useless parameters in the network, this paper introduced the Im SRM filtering kernel into the image preprocessing module. To extract effective steganography noise from images, this paper combined depthwise separable convolution and residual networks for the first time and introduces them into the steganography noise extraction module. In addition, to focus network attention on the image regions where steganography information exists, this paper integrated the coordinate attention mechanism. This module will make the network pay attention to the overall structure and local details of the image during network training, improving the network&#39;s recognition ability for steganography information. Finally, the extracted steganography features are classified through a classification module. This paper conducted a series of experiments on the BOSSBase 1.01 and BOWS2 datasets. The improvement in detection accuracy is between 1.2% and 18.2% compared to classic and recent steganalysis networks.},
  archive      = {J_IETIP},
  author       = {Yuanyuan Ma and Jian Wang and Xinyu Zhang and Guifang Wang and Xianwei Xin and Qianqian Zhang},
  doi          = {10.1049/ipr2.70005},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70005},
  shortjournal = {IET Image Process.},
  title        = {LGS-net: A lightweight convolutional neural network based on global feature capture for spatial image steganalysis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse representation for restoring images by exploiting
topological structure of graph of patches. <em>IETIP</em>,
<em>19</em>(1), e70004. (<a
href="https://doi.org/10.1049/ipr2.70004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration poses a significant challenge, aiming to accurately recover damaged images by delving into their inherent characteristics. Various models and algorithms have been explored by researchers to address different types of image distortions, including sparse representation, grouped sparse representation, and low-rank self-representation. The grouped sparse representation algorithm leverages the prior knowledge of non-local self-similarity and imposes sparsity constraints to maintain texture information within images. To further exploit the intrinsic properties of images, this study proposes a novel low-rank representation-guided grouped sparse representation image restoration algorithm. This algorithm integrates self-representation models and trace optimization techniques to effectively preserve the original image structure, thereby enhancing image restoration performance while retaining the original texture and structural information. The proposed method was evaluated on image denoising and deblocking tasks across several datasets, demonstrating promisingÂ results.},
  archive      = {J_IETIP},
  author       = {Yaxian Gao and Zhaoyuan Cai and Xianghua Xie and Jingjing Deng and Zengfa Dou and Xiaoke Ma},
  doi          = {10.1049/ipr2.70004},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70004},
  shortjournal = {IET Image Process.},
  title        = {Sparse representation for restoring images by exploiting topological structure of graph of patches},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter efficient face frontalization in image sequences
via GAN inversion. <em>IETIP</em>, <em>19</em>(1), e70003. (<a
href="https://doi.org/10.1049/ipr2.70003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing facial images with varying poses is a significant challenge. Most existing face frontalization methods rely on heavy architectures that struggle with small datasets and produce low-quality images. Additionally, although video frames provide richer information, these methods typically use single images due to the lack of suitable multi-image datasets. To address these issues, a parameter-efficient framework for high-quality face frontalization in both single and multi-frame scenarios is proposed. First, a high-quality, diverse dataset is created for single and multi-image face frontalization tasks. Second, a novel single-image face frontalization method is introduced by combining GAN inversion with transfer learning. This approach reduces the number of trainable parameters by over 91% compared to existing GAN inversion methods while achieving far more photorealistic results than GAN-based methods. Finally, this method is extended to sequences of images, using attention mechanisms to merge information from multiple frames. This multi-frame approach reduces artefacts like eye blinks and improves reconstruction quality. Experiments demonstrate that this single-image method outperforms pSp, a state-of-the-art GAN inversion method, with a 0.15 LPIPS improvement and a 0.10 increase in ID similarity. This multi-frame approach further improves identity preservation to 0.87, showcasing its effectiveness for high-quality frontal-viewÂ reconstructions.},
  archive      = {J_IETIP},
  author       = {Mohammadhossein Ahmadi and Nima Kambarani and Mohammad Reza Mohammadi},
  doi          = {10.1049/ipr2.70003},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70003},
  shortjournal = {IET Image Process.},
  title        = {Parameter efficient face frontalization in image sequences via GAN inversion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moving target detection based on improved gaussian mixture
model in dynamic and complex environments. <em>IETIP</em>,
<em>19</em>(1), e70001. (<a
href="https://doi.org/10.1049/ipr2.70001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, background modeling has garnered significant attention for motion target detection in vision and image applications. However, most methods do not achieve satisfactory results because of the influence of background dynamics and other factors. The Gaussian mixture model (GMM) background modeling method is a popular and powerful motion background modeling technology owing to its ability to balance robustness and real-time constraints in various practical environments. However, when the background is complex and the target moves slowly, the traditional GMM cannot accurately detect the target and is prone to misjudging the moving background as a moving target. To address the interference from complex backgrounds, this study proposes a target detection method that combines an adaptive GMM with an improved three-frame difference method, along with an algorithm that combines grayscale statistics with an improved Phong illumination model for illumination compensation and shadow removal. The experimental results demonstrate that the improved method has better robustness, improves target detection accuracy, and reduces noise and background interference.},
  archive      = {J_IETIP},
  author       = {Jiaxin Li and Fajie Duan and Xiao Fu and Guangyue Niu and Rui Wang and Hao Zheng},
  doi          = {10.1049/ipr2.70001},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70001},
  shortjournal = {IET Image Process.},
  title        = {Moving target detection based on improved gaussian mixture model in dynamic and complex environments},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrative survey on indian sign language recognition
and translation. <em>IETIP</em>, <em>19</em>(1), e70000. (<a
href="https://doi.org/10.1049/ipr2.70000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hard of hearing (HoH) people commonly use sign languages (SLs) to communicate. They face major impediments in communicating with hearing individuals, mostly because hearing people are unaware of SLs. Therefore, it is important to promote tools that enable communication between users of sign language and users of spoken languages. The study of sign language recognition and translation (SLRT) is a step forward in this direction, as it tries to create a spoken-language translation of a sign-language video or vice versa. This study aims to survey the Indian sign language (ISL) interpretation literature and gives pertinent information about ISL recognition and translation (ISLRT). It provides an overview of recent advances in ISLRT, including the use of machine learning based, deep learning based, and gesture-based techniques. This work also summarizes the development of ISL datasets and dictionaries. It highlights the gaps in the literature and provides recommendations for future research opportunities for ISLRTÂ development.},
  archive      = {J_IETIP},
  author       = {Rina Damdoo and Praveen Kumar},
  doi          = {10.1049/ipr2.70000},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70000},
  shortjournal = {IET Image Process.},
  title        = {An integrative survey on indian sign language recognition and translation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of crowd density estimation and
counting. <em>IETIP</em>, <em>19</em>(1), e13328. (<a
href="https://doi.org/10.1049/ipr2.13328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is one of the important and challenging research topics in computer vision. In recent years, with the rapid development of deep learning, the model architectures, learning paradigms, and counting accuracy have undergone significant changes. To help researchers quickly understand the research progress in this area, this paper presents a comprehensive survey of crowd density estimation and counting approaches. Initially, the technical challenges and commonly used datasets are intoroduced for crowd counting. Crowd counting approaches is them categorized into two groups based on the feature extraction methods employed: traditional approaches and deep learning-based approaches. A systematic and focused analysis of deep learning-based approaches is proposed. Subsequently, some training and evaluation details are introduced, including labels generation, loss functions, supervised training methods, and evaluation metrics. The accuracy and robustness of selected classical models are further compared. Finally, future prospects, strategies, and challenges are discussed for crowd counting. This review is comprehensive and timely, stemming from the selection of prominent and uniqueÂ works.},
  archive      = {J_IETIP},
  author       = {Mingtao Wang and Xin Zhou and Yuanyuan Chen},
  doi          = {10.1049/ipr2.13328},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13328},
  shortjournal = {IET Image Process.},
  title        = {A comprehensive survey of crowd density estimation and counting},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optical gas imaging and deep learning for quantifying
enteric methane emissions from rumen fermentation in vitro.
<em>IETIP</em>, <em>19</em>(1), e13327. (<a
href="https://doi.org/10.1049/ipr2.13327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated the possibility of using a laser methane detector (LMD) and optical gas imaging (OGI) to detect and quantify enteric methane ( CH 4 ${\rm CH}_4$ ) produced by ruminants in vitro. Four single-flow continuous fermenters were used for rumen culture incubation with four different treatment diets: Control (50:50 forage to concentrate [F:C] ratio), Control + Bromoform (CBR), Low Forage (LF; 20:80), and High Forage (HF; 80:20). After 10 days of incubation, all fermenter contents were transferred and used in a 24 h ANKOM batch culture to measure gas production with LMD and OGI. The authors introduce the Controlled Diet (CD) dataset, a large-scale collection of 4,885 plume images captured using an FLIR GF77 OGI camera under varying dietary conditions. The performance of six semantic segmentation models (FCN, U-Net, Vision Transformer, Swin Transformer, DeepLabv3+, and Gasformer) on the CD dataset is compared. Results showed that LMD data for followed a similar pattern to the gas chromatography (GC) instrument results. The in vitro results showed that different diets and F:C ratios had an impact on gas production and rumen fermentation characteristics. Adding bromoform to the control diet fully inhibited emission. The HF diet produced more compared to all treatments ( ) when measured with GC and LMD. CBR produced the lowest values when measured with GC and LMD. The Gasformer architecture achieved the highest performance with mean IoU of 85.1% and mean F-score of 91.72%. These findings demonstrate that OGI technology combined with advanced semantic segmentation models offers a promising solution for predicting and quantifying emissions in the livestock sector, potentially aiding in the development of mitigation strategies to combat climateÂ change.},
  archive      = {J_IETIP},
  author       = {Mohamed G. Embaby and Toqi Tahamid Sarker and Amer AbuGhazaleh and Khaled R. Ahmed},
  doi          = {10.1049/ipr2.13327},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13327},
  shortjournal = {IET Image Process.},
  title        = {Optical gas imaging and deep learning for quantifying enteric methane emissions from rumen fermentation in vitro},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-learning weight network based on label distribution
training for facial expression recognition. <em>IETIP</em>,
<em>19</em>(1), e13326. (<a
href="https://doi.org/10.1049/ipr2.13326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent widespread utilization of facial expression recognition (FER) has garnered significant attention in the affective computing field. To address the issue of dominant features being suppressed during feature fusion in FER, this study proposes a self-learning weight network based on label distribution training (SLW-LDT). First, based on the ShuffleNet-V2 backbone model, SLW-LDT introduced a local feature extraction branch that highlights specific expression-related features by cropping the facial image into four local regions. Subsequently, the SLW algorithm is devised to allocate learnable weights to global and local features from different branches before their fusion. Moreover, considering the challenge associated with accessing emotional distribution in facial images directly, a label distribution training module (LDT) is introduced during the training phase to generate label distributions for effective training purposes. Experimental results demonstrate that the proposed method achieves accuracies of 89.77% and 64.21% on two in-the-wild datasets (RAF-DB and AffectNet-7), and 98.90% on the lab-controlled CK+ dataset. Comparative analysis against state-of-the-art methods reveals slight improvements in recognition accuracy along with robust performance exhibited by the model.},
  archive      = {J_IETIP},
  author       = {Yangbo Chen and Chunyan Peng and Xuan Wang and Yuhui Zheng},
  doi          = {10.1049/ipr2.13326},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13326},
  shortjournal = {IET Image Process.},
  title        = {Self-learning weight network based on label distribution training for facial expression recognition},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OLODN: An efficient lightweight people detection method for
occlusion and crowding scenarios. <em>IETIP</em>, <em>19</em>(1),
e13325. (<a href="https://doi.org/10.1049/ipr2.13325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the real-time object detection problem for devices with limited computing resources in densely populated and occluded scenarios, a novel occlusion-aware lightweight object detection network (OLODN) is proposed. This network integrates innovative components to significantly enhance detection efficiency while maintaining high accuracy. Firstly, OLODN employs FasterNet blocks and reparameterised generalised-FPN to reduce computational complexity and preserve feature extraction and fusion capabilities. Secondly, a reinforced coordination attention mechanism is designed to strengthen the network&#39;s ability to capture occlusion boundary information. Additionally, a spatial pyramid pooling feature concatenation module is introduced to integrate multi-scale features and enhance the algorithm&#39;s robustness to occlusions. Lastly, OLODN adopts a task-aligned one-stage object detection strategy, optimising the anchor alignment of classification and localisation tasks, effectively improving detection accuracy under occluded conditions. Experiments demonstrate the superiority of the proposed method. On the WiderPerson dataset, OLODN achieved a recall rate of 68.0%, which is 2.8% higher than YOLOv11&#39;s 65.2%, while running at 35.3 frames per second (f/s) on CPU and 76.6 f/s on GPU, faster than YOLOv11&#39;s 34.1 f/s and 74.5 f/s by 1.2 f/s and 2.1 f/s,Â respectively.},
  archive      = {J_IETIP},
  author       = {Wei Sheng and Mingjian Liu and Xiang Li and Mingbao Zhang},
  doi          = {10.1049/ipr2.13325},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13325},
  shortjournal = {IET Image Process.},
  title        = {OLODN: An efficient lightweight people detection method for occlusion and crowding scenarios},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Face de-morphing based on identity feature transfer.
<em>IETIP</em>, <em>19</em>(1), e13324. (<a
href="https://doi.org/10.1049/ipr2.13324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face morphing attacks have emerged as a significant security threat, compromising the reliability of facial recognition systems. Despite extensive research on morphing detection, limited attention has been given to restoring accomplice face images, which is critical for forensic applications. This study aims to address this gap by proposing a novel face de-morphing (FD) method based on identity feature transfer for restoring accomplice face images. The method encodes facial attribute and identity features separately and employs cross-attention mechanisms to extract identity features from morphed faces relative to reference images. This process isolates and enhances the accomplice&#39;s identity features. Additionally, inverse linear interpolation is applied to transfer identity features to attribute features, further refining the restoration process. The enhanced identity features are then integrated with the StyleGAN generator to reconstruct high-quality accomplice facial images. Experimental evaluations on two morphed face datasets demonstrate the effectiveness of the proposed approach, improving the average restoration accuracy by at least 5% compared with other methods. These findings highlight the potential of this approach for advancing forensic and security applications.},
  archive      = {J_IETIP},
  author       = {Le-Bing Zhang and Song Chen and Min Long and Juan Cai},
  doi          = {10.1049/ipr2.13324},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13324},
  shortjournal = {IET Image Process.},
  title        = {Face de-morphing based on identity feature transfer},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-based method for detecting and identifying
surface defects in polyimide foam. <em>IETIP</em>, <em>19</em>(1),
e13323. (<a href="https://doi.org/10.1049/ipr2.13323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the detection and identification of surface defects in polyimide foam products mainly rely on on-site work experience, which has issues such as low detection accuracy, strong subjectivity, and low efficiency. Existing research on foam product defect detection primarily targets internal defects, lacking studies on the detection, identification, and classification of surface defects. Therefore, this article proposes a method for identifying and classifying surface defects in polyimide foam based on an improved GoogLeNet, aiming to quickly and accurately detect and identify surface defects in foam products. By optimizing the Inception blocks, introducing the ECA attention mechanism, and adding an LSTM network module, the model&#39;s recognition accuracy and generalization ability are effectively improved. In experiments, the model proposed in this article performed excellently on the foam surface defect dataset, showing a significant advantage in detection accuracy compared to other convolutional neural network models. The detection accuracy for pits and cracks reached 98.24% and 98.25%, respectively, providing a reliable reference for the detection of surface defects in industrial foam production.},
  archive      = {J_IETIP},
  author       = {Xianhui Song and Guangzhong Hu and Jing Lu and Xianguo Tuo and Yuedong Li},
  doi          = {10.1049/ipr2.13323},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13323},
  shortjournal = {IET Image Process.},
  title        = {A deep learning-based method for detecting and identifying surface defects in polyimide foam},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning optical flow from spiking camera with direction
disassembly. <em>IETIP</em>, <em>19</em>(1), e13322. (<a
href="https://doi.org/10.1049/ipr2.13322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional optical flow estimation methods typically recover two-dimensional motion from RGB image sequences. Recently, due to the rise and widespread use of spike cameras, learning optical flow from spiking cameras has become a hot topic in the field of two-dimensional motion estimation. Although existing methods have been designed to learn optical flow by designing feature processing methods for spike streams, there is still insufficient consideration for flow field post-processing, resulting in limited accuracy of optical flow estimation. To address this problem, an optical flow estimation method based on directional disassembly is proposed. Specifically, the estimated flow fields along the horizontal and vertical directions are disassembled and the motion vectors along the two directions are denoised separately to reduce the burden of post-processing for complex two-dimensional motion information. In addition, contextual information is introduced in the post-processing so that the scene information can effectively contribute to the results of the flow post-processing. Experimental results show that this proposed method is capable of achieving comparable performance on spike-based publicÂ datasets.},
  archive      = {J_IETIP},
  author       = {Mingliang Zhai and Mingming Jiang and Xuezhi Xiang and Kang Ni and Hao Gao},
  doi          = {10.1049/ipr2.13322},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13322},
  shortjournal = {IET Image Process.},
  title        = {Learning optical flow from spiking camera with direction disassembly},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv7-SFWC: A detection algorithm for illegal manned
trucks. <em>IETIP</em>, <em>19</em>(1), e13321. (<a
href="https://doi.org/10.1049/ipr2.13321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic analysis and evidence collection of obvious traffic violations, such as illegal manned trucks, is one of the critical operational challenges of the traffic police department&#39;s business. For the enormous volume of road surveillance images generated daily, traditional manual screening is highly time-intensive and resource-draining. Therefore, this article proposes an improved detection model YOLOv7-SFWC for illegally manned trucks. First of all, the pictures of illegal manned vehicles obtained by relevant departments are expanded and labeled, and the dataset of illegal manned vehicles is created. Building upon the foundational YOLOv7 model, this study replaces the traditional convolution module with the FasterNet convolution module and SCConv module, and introduces the Wise-IoU (WIoU) loss function algorithm and Coordinate Attention (CA) mechanism. The results show that the mAP value of the YOLOv7-SFWC model is improved by 4.15% and FPS by 7.6 compared with the original YOLOv7 model, and the computational complexity is reduced to adapt to the deployment. Moreover, the model&#39;s effectiveness is validated through extensive comparison experiments. Finally, the visual results show the accurate performance of the model and verify the progress of YOLOv7-SFWC. This advancement has the potential to transform traffic violation enforcement by reducing reliance on manual screening, effectively combating traffic violations, and purifying trafficÂ order.},
  archive      = {J_IETIP},
  author       = {Xuan Wu and Yanan Wang and Tengtao Nie and Wenlin Pan},
  doi          = {10.1049/ipr2.13321},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13321},
  shortjournal = {IET Image Process.},
  title        = {YOLOv7-SFWC: A detection algorithm for illegal manned trucks},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCFNet: Research on small target detection of RGB-infrared
under UAV perspective with multi-scale complementary feature fusion.
<em>IETIP</em>, <em>19</em>(1), e13320. (<a
href="https://doi.org/10.1049/ipr2.13320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of single-modal image target detection from a drone&#39;s perspective faces multiple challenges such as large image sizes, small and dense targets, insufficient lighting conditions, and hardware resource constraints, all of which affect the accuracy and real-time performance of algorithms. In response, a small target detection algorithm that fuses multi-scale complementary features, named MCFNet is proposed. Firstly, in order to independently extract the unimodal features of visible light and infrared images and effectively enhance the complementary information between images, a dual-stream backbone network with a terminal fusion mechanism is proposed. Secondly, the complementary feature information residual module is utilized to optimize the integration process of residual features. Subsequently, by designing a multi-scale feature enhancement module the network&#39;s capability to capture multi-scale features is enhanced. Finally, for targets of varying sizes, a lightweight Transformer feature extraction module is proposed to improve the detection accuracy of small targets from a drone&#39;s perspective. Test results on the drone-vehicle dataset show that this method achieved an average detection accuracy of 67.92%, while the detection accuracy on the self-constructed UAV-data dataset reached 96.4%. Additionally, a series of ablation experiments validated the effectiveness of the different modules.},
  archive      = {J_IETIP},
  author       = {Jing Jing and Jian Feng Hu and Zuo Peng Zhao and Ying Liu},
  doi          = {10.1049/ipr2.13320},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13320},
  shortjournal = {IET Image Process.},
  title        = {MCFNet: Research on small target detection of RGB-infrared under UAV perspective with multi-scale complementary feature fusion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on tea buds detection based on optimized YOLOv5s.
<em>IETIP</em>, <em>19</em>(1), e13319. (<a
href="https://doi.org/10.1049/ipr2.13319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the world&#39;s most popular beverages, tea plays a significant role in improving tea production efficiency and quality through the identification of tea shoots during the tea manufacturing process. However, due to the complex morphology, small size, and susceptibility to factors like lighting and obstruction, traditional identification methods suffer from low accuracy and efficiency. In this study, image enhancement techniques such as HSV transformation, horizontal flipping, and vertical flipping were applied to the training dataset to improve model robustness and enhance generalization across varying lighting and angles. To address these challenges in the context of tea buds detection, deep-learning-based object detection methods have emerged as promising solutions. Nevertheless, current object detection technologies still face limitations when detecting tea buds under these conditions. To enhance identification performance, this article proposed an improved YOLOv5s (You Only Look Once version 5 small model) algorithm. In the improved YOLOv5s algorithm, CBAM, SE, and CA attention mechanisms were incorporated into the backbone network to augment feature extraction, and a weighted Bidirectional Feature Pyramid Network (BiFPN) is employed in the neck network to boost performance, resulting in the YOLOv5s_teabuds model. Experimental results indicated that the improved model significantly outperformed the original in terms of precision, recall, mAP and F1-score, with the CA attention mechanism providing the most notable improvementâenhancing precision, recall, mAP and F1-score by 18.119%, 9.633%, 16.496% and 13.524%, respectively. After integrating BiFPN, the YOLOv5s_teabuds model further strengthened performance and robustness, with precision, recall, mAP and F1-score increased by 19.346%, 11.388%, 18.620%, and 15.059%, respectively. Experimental results prove that the optimized YOLOv5s model can provide a real-time, high-precision tea buds detection method for robotic harvesting.},
  archive      = {J_IETIP},
  author       = {Guanli Li and Jianqiang Lu and Dong Zhang and Zhongyi Guo},
  doi          = {10.1049/ipr2.13319},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13319},
  shortjournal = {IET Image Process.},
  title        = {Research on tea buds detection based on optimized YOLOv5s},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image inpainting with aggregated convolution progressive
network. <em>IETIP</em>, <em>19</em>(1), e13318. (<a
href="https://doi.org/10.1049/ipr2.13318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images can be corrupted during capture or transmission due to clouds, overlaps, and other interferences, deviating from their original state. Image inpainting techniques restore such images, but different typesâSynthetic Aperture Radar (SAR), RGB, and infraredârequire varying field-of-view sizes. SAR and infrared images, with less information, need a larger field of view, leading to uncorrelated interference in distant areas. RGB images, richer in information, are constrained by a limited local field of view, hindering access to full semantic details. To address these challenges, an aggregated convolution progressive network is proposed. This model employs a coarse-grained inpainting module for initial restoration, enhanced by an aggregated convolution module to capture contextual information. Local and global details are then used to refine the output, improving restoration quality. Additionally, existing datasets predominantly focus on RGB images, lacking diversity. To bridge this gap, a comprehensive dataset covering SAR, RGB, and infrared images under cloud, overlap, and corruption conditions is constructed. This method achieves superior performance, with MAE of 0.05, SSIM of 0.95, and PSNR of 36.68 within a 20â30% mask size range, outperforming state-of-the-art techniques across diverse image types and size ranges. Experimental results validate its effectiveness in advancing image inpainting.},
  archive      = {J_IETIP},
  author       = {Yang Li and Jia Zhai and Wen Lu and Haipeng Guo and JiaZheng Wen and Huanyu Liu and Junbao Li},
  doi          = {10.1049/ipr2.13318},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13318},
  shortjournal = {IET Image Process.},
  title        = {Image inpainting with aggregated convolution progressive network},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the registration of infrared and visible images
based on phase consistency and edge extreme points. <em>IETIP</em>,
<em>19</em>(1), e13317. (<a
href="https://doi.org/10.1049/ipr2.13317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the significant differences in the imaging principles of infrared images and visible images, the feature points and descriptors between the two cannot be effectively matched directly by traditional feature extraction methods such as SIFT. To solve this problem, this study proposes a registration algorithm for infrared and visible images based on phase information and edge information. The algorithm extracts the feature points of the infrared image and the visible image through the principle of phase agreement and the edge binary map of the image and then calculates the descriptors of the gradient images of the infrared image and the visible image, and the descriptor calculation draws on some SIFT principles. Finally, the cosine similarity was used to match the feature points, and the improved random sample consensus algorithm was used to screen out the correct registration points. Experiments show that this method can effectively register between infrared and visible images and is also suitable for the registration of infrared and visible images with different rotation angles and similar structures.},
  archive      = {J_IETIP},
  author       = {Jie Li and Rougang Zhou and Zhenchao Ruan and Chou Jay Tsai Chien and Junjie Zhu},
  doi          = {10.1049/ipr2.13317},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13317},
  shortjournal = {IET Image Process.},
  title        = {Research on the registration of infrared and visible images based on phase consistency and edge extreme points},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-teacher-guided pseudo label supervision: A
semi-supervised learning framework for muscle and adipose tissue
segmentation on chest CT scans. <em>IETIP</em>, <em>19</em>(1), e13316.
(<a href="https://doi.org/10.1049/ipr2.13316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of muscle and adipose tissue in chest CT scans is essential for the diagnosis, treatment, and prognosis of various diseases. However, this task is hindered by the limited availability of annotated data. This study proposes a novel semi-supervised learning framework, co-teacher-guided pseudo-label supervision (CTGP), to address this challenge. CTGP combines co-training and the Mean-Teacher strategy, where predictions generated by teacher models are filtered and utilized as high-quality pseudo-labels to train the other student models, thus facilitating co-training. Additionally, a medical image-specific augmentation method, MIAugment, is introduced to better adapt to the unique characteristics of medical data. Experiments on a real-life clinical dataset demonstrate that CTGP achieves high segmentation accuracy with minimal labelled data. Using only 10% of labelled data, the framework achieves a mean Dice Similarity Coefficient of 90.03% for four tissue types, a decrease of just 2.85% compared to fully supervised learning. This approach provides a promising solution for automated muscle and adipose tissue segmentation with limited annotations.},
  archive      = {J_IETIP},
  author       = {Jie Yang and Yanli Liu and Xiaoyan Chen and Tianle Chen and Qi Liu},
  doi          = {10.1049/ipr2.13316},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13316},
  shortjournal = {IET Image Process.},
  title        = {Co-teacher-guided pseudo label supervision: A semi-supervised learning framework for muscle and adipose tissue segmentation on chest CT scans},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrapping visionâlanguage transformer for monocular 3D
visual grounding. <em>IETIP</em>, <em>19</em>(1), e13315. (<a
href="https://doi.org/10.1049/ipr2.13315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of 3D visual grounding using monocular RGB images, it is a challenging problem to perceive visual features and accurately predict the localization of 3D objects based on given geometric and appearances descriptions. Traditional text-guided attention-based methods have achieved better results than baselines, but it is argued that there is still potential for improvement in the area of multi-modal fusion. Thus, Mono3DVG-TRv2, an end-to-end transformer-based architecture that employs a visual-text multi-modal encoder for the alignment and fusion of multi-modal features, incorporating an enhanced transformer module proven in 2D detection, is introduced. The depth features predicted by the multi-modal features and the visual-text features are associated with the learnable queries in the decoder, facilitating more efficient and effective acquisition of geometric information in intricate scenes. Following a comprehensive comparison and ablation study on the Mono3DRefer dataset, this method achieves state-of-the-art performance, markedly surpassing the prior approach. The code will be released at https://github.com/Jade-Ray/Mono3DVGv2 .},
  archive      = {J_IETIP},
  author       = {Qi Lei and Shijie Sun and Xiangyu Song and Huansheng Song and Mingtao Feng and Chengzhong Wu},
  doi          = {10.1049/ipr2.13315},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13315},
  shortjournal = {IET Image Process.},
  title        = {Bootstrapping visionâlanguage transformer for monocular 3D visual grounding},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-tiny: A lightweight small object detection algorithm
for UAV aerial imagery. <em>IETIP</em>, <em>19</em>(1), e13314. (<a
href="https://doi.org/10.1049/ipr2.13314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unmanned aerial vehicle (UAV) aerial target detection tasks, two main challenges exist: the limited computational resources of UAV terminals, which are not conducive to running complex models, and the prevalence of small targets, which can easily lead to missed detections and false positives. To address these issues, this study proposes a lightweight and high-accuracy small-object detection algorithm for UAV aerial imagery that is based on YOLOv5s. First, the network structure is optimized by removing the layers in YOLOv5s primarily used for detecting large targets (P4 and P5) and adding layers primarily used for detecting small targets (P2 and P3). This enables the model to focus more on extracting small-object features. A lightweight dynamic convolution is subsequently introduced in the C3 module, and the lightweight LW_C3 and LW_downsampling modules are designed for feature extraction and downsampling operations. This enhances the model&#39;s feature extraction capability while achieving a lightweight design. Finally, the adaptive multi-scale spatial feature fusion (AMSFF) module is designed to adaptively learn the spatial weights of the feature maps at different levels, thereby further strengthening the effective fusion of multi-scale features. Experimental results show that the improved YOLO-Tiny model has higher accuracy and lower complexity, hence validating its excellent performance.},
  archive      = {J_IETIP},
  author       = {Fei Feng and Lu Yang and Quanxing Zhou and Weipeng Li},
  doi          = {10.1049/ipr2.13314},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13314},
  shortjournal = {IET Image Process.},
  title        = {YOLO-tiny: A lightweight small object detection algorithm for UAV aerial imagery},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved conditional diffusion model for image
super-resolution. <em>IETIP</em>, <em>19</em>(1), e13313. (<a
href="https://doi.org/10.1049/ipr2.13313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have achieved remarkable success in image super-resolution by addressing issues such as transition smoothing, insufficient high-frequency information, and training instability encountered in regression-based and GAN-based models. However, challenges persist when applying diffusion models to image super-resolution, including randomness, inadequate conditional information, high computational costs, and network architecture complexities. In this article, the authors introduce a diffusion model based on Mean-Reverting Stochastic Differential EquationsÂ (SDE), and propose the use of ENAFBlocks instead of traditional ResBlocks to enhance model performance in noise prediction. The Mean-Reverting SDE effectively mitigates the randomness of the diffusion model by leveraging low-resolution images as means. Additionally, an LR Encoder is introduced to capture hidden information from LR images, providing a more robust condition for stable result generation by the noise predictor. To efficiently handle high-resolution images within limited GPU memory, the method employs adaptive aggregate sampling, which merges overlapping regions smoothly using weighted averaging. Furthermore, color variations are addressed during diffusion model sampling through color correction. Extensive experiments on CelebA, DIV2K, and Urban100 demonstrate that the method outperforms state-of-the-art diffusion models like IDM, with a PSNR improvement of 0.22 dB, FID reduction of 2.35, and LPIPS reduction of 0.05 on the DIV2K dataset, along with a reduced parameter count and faster inferenceÂ time.},
  archive      = {J_IETIP},
  author       = {Rui Wang and Ningning Zhou},
  doi          = {10.1049/ipr2.13313},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13313},
  shortjournal = {IET Image Process.},
  title        = {Improved conditional diffusion model for image super-resolution},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive lucky imaging method for turbulence-degraded
image restoration. <em>IETIP</em>, <em>19</em>(1), e13312. (<a
href="https://doi.org/10.1049/ipr2.13312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When capturing distant targets, the video sequence images are affected by atmospheric turbulence, resulting in distortion and blur. In order to restore the degraded images due to atmospheric turbulence in video sequences, this article combines lucky imaging with generative adversarial networks for the first time. The idea of lucky imaging is employed to eliminate geometric distortions, followed by the use of generative adversarial networks to address the blur issue. Additionally, an adaptive restoration method targeting turbulence intensity is proposed to improve the computational efficiency of the proposed approach. Experimental results demonstrate that the combined restoration method of lucky imaging and generative adversarial networks outperforms classical lucky imaging. Specifically, compared to classical lucky imaging, the Brenner gradient function, Laplacian gradient function, Spatial Median Difference (SMD), Entropy, Energy gradient function, PIQE, and Brisque indicators improve by 7.7%, 13.1%, 3.6%, 4.1%, 2.1%, 26.6% and 21.54% (all evaluation indicators in the above improvement rates have undergone logarithmic transformation), respectively. Meanwhile, the proposed adaptive restoration method can improve efficiency by 28%, with greater efficiency gains observed with larger datasets.},
  archive      = {J_IETIP},
  author       = {Pin Lv and Tiezhu Shi and Dongping Den and Mengdi Wang and Qian Liu and Guofeng Wu},
  doi          = {10.1049/ipr2.13312},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13312},
  shortjournal = {IET Image Process.},
  title        = {An adaptive lucky imaging method for turbulence-degraded image restoration},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-powered automated analysis of bone scans: A survey.
<em>IETIP</em>, <em>19</em>(1), e13311. (<a
href="https://doi.org/10.1049/ipr2.13311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the key techniques of artificial intelligence, deep learning has emerged as an effective approach for analysing medical images. Various imaging techniques including the planar bone scintigraphy, single photon emission computed tomography and PET can be used to evaluate, in vivo, bone conditions. The introduction of deep learning techniques especially the convolutional neural networks can significantly improve diagnosis accuracy and efficiency of nuclear medicine physicians. Focusing on bone scans acquired by various nuclear medicine imaging techniques, his paper reviews existing work on deep learning-based classification, segmentation and object detection of bone scans. Specifically, an overview of existing work about research objective is presented, deep learning models are adopted, and main results are achieved. Research challenges and directions for developing automated analysis of bone scans with deep learning techniques are then discussed.},
  archive      = {J_IETIP},
  author       = {Qiang Lin and Yang He and Sihan Guo},
  doi          = {10.1049/ipr2.13311},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13311},
  shortjournal = {IET Image Process.},
  title        = {AI-powered automated analysis of bone scans: A survey},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal sentiment analysis based on multi-stage graph
fusion networks under random missing modality conditions.
<em>IETIP</em>, <em>19</em>(1), e13310. (<a
href="https://doi.org/10.1049/ipr2.13310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary challenge of the multimodal sentiment analysis (MSA) task is the modal fusion, and the lack of modalities may exist in the fusion process, which leads to poor prediction results. Most of the previous research on multimodal fusion is single-stage fusion, disregarding how various modality subsets interact, as well as rarely considering relative position relationship of modality sequences, causing the fragmentation of context info. Considering the aforementioned issues, this study introduces an MSA method based on the multi-stage graph fusion network (MSGFN) under random missing modality conditions to improve the robustness of the model to MSA under the random missing modality conditions. Firstly, for each modality, its inter-modal and intra-modal multi-head attention are used to learn robust representation of the modality sequence. Meanwhile, the relative position encoding (RPE) is introduced into mechanism of attention that enables model to perceive and learn the relative position before and after the modality sequence when calculating attention, thereby better understanding the contextual info of the sequence. After that, the transformer encoder receives the learned modality features and uses the pre-trained model to supervise the reconstruction of the missing modality information. Finally, the feature representations of different modalities have effectively fused using multi-stage graph fusion network, and the output is used for the ultimate sentiment classification. Wide experiments are conducted on two publicly available datasets, CMU-MOSI and IEMOCAP, and the findings indicate that the proposed method can better handle the challenges caused by modality fusion and modality missing compared with several baseline methods.},
  archive      = {J_IETIP},
  author       = {Ting Zhang and Bin Song and Zhiyong Zhang and Yajuan Zhang},
  doi          = {10.1049/ipr2.13310},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13310},
  shortjournal = {IET Image Process.},
  title        = {Multimodal sentiment analysis based on multi-stage graph fusion networks under random missing modality conditions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified multi-class anomaly detection model based on
reverse distillation. <em>IETIP</em>, <em>19</em>(1), e13309. (<a
href="https://doi.org/10.1049/ipr2.13309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-class single-model detection paradigm is a prevalent design for industrial anomaly detection, exhibiting suitability for varying industrial classes and dynamic, flexible production scenarios. This approach not only enhances model adaptability but also minimizes maintenance costs. However, the current popular methods are susceptible to the âcopying shortcutâ phenomenon, which constrains their performance on benchmark datasets. To overcome this limitation, this article proposes a multi-class anomaly detection model: UniRD, based on the reverse distillation method. This model creates an image corruptor that expands the dataset and generates ânormal-corruptâ image pairs. During the training process, their correspondence is used to optimize the reverse distillation. This process greatly exploits and releases the potential of the student decoder. Furthermore, a teacher feature adaptation module is devised to enhance the compatibility between the pre-trained model and the anomaly detection task. This has the effect of reducing the discrepancy between teacher and student features while ensuring the consistency of normal sample features. The comprehensive evaluation results in two mainstream datasets, MVTec and VisA, and demonstrates that the proposed method exhibits improvement in all indicators compared to benchmark methods. The proposed method attains the state-of-the-art, substantiating its effectiveness and advancement.},
  archive      = {J_IETIP},
  author       = {Maoli Fu and Zhongliang Fu},
  doi          = {10.1049/ipr2.13309},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13309},
  shortjournal = {IET Image Process.},
  title        = {A unified multi-class anomaly detection model based on reverse distillation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-rigid point cloud registration method based on scene
flow estimation. <em>IETIP</em>, <em>19</em>(1), e13308. (<a
href="https://doi.org/10.1049/ipr2.13308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is an important aspect of computer vision, encompassing various applications. However, many existing algorithms neglect the registration of non-rigid point cloud. Recent studies have focused on scene flow, which can achieve non-rigid point cloud registration by estimating the scene flow of two point clouds. The presence of occlusion in a scene is primarily attributed to variations in viewpoint and access time, thereby posing a significant challenge in accurately predicting scene flow. In the endeavour to mitigate this issue, the focus is on the propagation of scene flow originating from non-occluded points towards occlusion points while concurrently estimating the occlusion map. The proposed network, a non-rigid point cloud registration method based on scene flow estimation, achieves exceptional performance for the EPE3D metric on the FlyingThings3D and KITTI scene flow datasets, and it demonstrates strong generalization on the railroadÂ dataset.},
  archive      = {J_IETIP},
  author       = {Xiaopeng Deng and Kai Yang and Yong Wang and Jinlong Li and Liming Xie},
  doi          = {10.1049/ipr2.13308},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13308},
  shortjournal = {IET Image Process.},
  title        = {A non-rigid point cloud registration method based on scene flow estimation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Salt-and-pepper denoising based on lightweight convolutional
neural networks for flexible AMOLED. <em>IETIP</em>, <em>19</em>(1),
e13307. (<a href="https://doi.org/10.1049/ipr2.13307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the performance of the image preprocessing module in consumer electronics using an active-matrix organic light-emitting diode display panel, the concept of judging before processing for salt-and-pepper denoising is originally proposed. Firstly, a dataset for salt-and-pepper noise image classification is constructed, and a convolutional neural network (CNN) for judging noise image (CNN-J) is trained. Image classified as normal by CNN-J is not processed, while the classified noisy image is denoised. In the denoising process, a marking image and a rough denoised image are generated by CNN for noise mask (CNN-M) and CNN for denoising (CNN-D), respectively. Subsequently, the refined denoised image is output using the proposed refining mechanism. The middle layers of CNN-M and CNN-D are constructed by depth-separable CNN to reduce the network complexity. Experimental results show that the misjudging rate of CNN-M marking is reduced by 19.94% compared with the best existing marking method. Compared with the traditional methods, the peak signal to noise ratio of the proposed method is increased by 2.95% and the information loss is reduced by 21.46%. In addition, the computational complexity is at least 11.18% lower than that of the traditional CNN. Finally, the display of salt-and-pepper denoised images on the flexible AMOLED is realized.},
  archive      = {J_IETIP},
  author       = {Chengqiang Huang and Yanjun Yang and Yinghu He},
  doi          = {10.1049/ipr2.13307},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13307},
  shortjournal = {IET Image Process.},
  title        = {Salt-and-pepper denoising based on lightweight convolutional neural networks for flexible AMOLED},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the object detection in complex scenes using
diagonal distance. <em>IETIP</em>, <em>19</em>(1), e13305. (<a
href="https://doi.org/10.1049/ipr2.13305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection method based on IoU boundary box is an important method of image recognition. This paper proposes a diagonal distance loss function, L DDIoU ${\mathcal {L}_{\text{DDIoU}}}$ , and a combined diagonal distance loss function, L CDIoU ${\mathcal {L}_{\text{CDIoU}}}$ , based IoU. These functions use the distance between diagonals as a penalty, overcoming the degradation phenomenon of prior functions based on traditional IoU. The altered loss functions are also integrated into faster R-CNN and SSD models, and their effectiveness is assessed on PASCAL VOC, MS COCO and SSDD data sets, yielding favourable results. This paper also extends the IoU calculation between two boxes when performing non-maximum suppression to advance position precision. The experiments have demonstrated that the enhanced distance calculation method improves both loss function and non-maximum suppression calculations by around .},
  archive      = {J_IETIP},
  author       = {Dan Guo and Guoliang He},
  doi          = {10.1049/ipr2.13305},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13305},
  shortjournal = {IET Image Process.},
  title        = {Research on the object detection in complex scenes using diagonal distance},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv7-PSAFP: Crop pest and disease detection based on
improved YOLOv7. <em>IETIP</em>, <em>19</em>(1), e13304. (<a
href="https://doi.org/10.1049/ipr2.13304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of pests and diseases in crops is currently a hot topic. The complexity of pest and disease object in the field, combined with inconsistent features across different levels, poses challenges for network detection. Additionally, the complex agricultural production environment tends to generate many interfering negative samples, which significantly complicates pest and disease differentiation. To address these two issues, the YOLOv7-PSAFP network structure was first proposed. Based on YOLOV7, the progressive Spatial Adaptive Feature Pyramid (PSAFP) was introduced. Second, a combination of the Varifocal Loss and Loss Rank Mining loss functions was used for calculating the object loss, which reduces the interference of useless negative examples during training. On the filtered-plant-village-dataset and rice-corn pest dataset, the mAP results of YOLOv7-PSAFP were 84.7 and 93.3 , which are 2.9 and 2.1 higher than the baseline model (YOLOv7), respectively. The code for this paper is located at https://github.com/DuLJ72/PSAFP .},
  archive      = {J_IETIP},
  author       = {Lujia Du and Junlong Zhu and Muhua Liu and Lin Wang},
  doi          = {10.1049/ipr2.13304},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13304},
  shortjournal = {IET Image Process.},
  title        = {YOLOv7-PSAFP: Crop pest and disease detection based on improved YOLOv7},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swin2-MoSE: A new single image supersolution model for
remote sensing. <em>IETIP</em>, <em>19</em>(1), e13303. (<a
href="https://doi.org/10.1049/ipr2.13303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of current optical and sensor technologies and the high cost of updating them, the spectral and spatial resolution of satellites may not always meet desired requirements. For these reasons, Remote-Sensing Single-Image Super-Resolution (RS-SISR) techniques have gained significant interest. In this paper, Swin2-MoSE model is proposed, an enhanced version of Swin2SR. The model introduces MoE-SM, an enhanced Mixture-of-Experts (MoE) to replace the Feed-Forward inside all Transformer block. MoE-SM is designed with Smart-Merger, and new layer for merging the output of individual experts, and with a new way to split the work between experts, defining a new per-example strategy instead of the commonly used per-token one. Furthermore, it is analyzed how positional encodings interact with each other, demonstrating that per-channel bias and per-head bias can positively cooperate. Finally, the authors propose to use a combination of Normalized-Cross-Correlation (NCC) and Structural Similarity Index Measure (SSIM) losses, to avoid typical MSE loss limitations. Experimental results demonstrate that Swin2-MoSE outperforms any Swin derived models by up to 0.377â0.958 dB (PSNR) on task of , and resolution-upscaling ( and OLI2MSI datasets). It also outperforms SOTA models by a good margin, proving to be competitive and with excellent potential, especially for complex tasks. Additionally, an analysis of computational costs is also performed. Finally, the efficacy of Swin2-MoSE is shown, applying it to a semantic segmentation task (SeasoNet dataset). Code and pretrained are available on https://github.com/IMPLabUniPr/swin2-mose/tree/official_code},
  archive      = {J_IETIP},
  author       = {Leonardo Rossi and Vittorio Bernuzzi and Tomaso Fontanini and Massimo Bertozzi and Andrea Prati},
  doi          = {10.1049/ipr2.13303},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13303},
  shortjournal = {IET Image Process.},
  title        = {Swin2-MoSE: A new single image supersolution model for remote sensing},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient no-reference image quality analysis based on
statistical perceptual features. <em>IETIP</em>, <em>19</em>(1), e13302.
(<a href="https://doi.org/10.1049/ipr2.13302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that image quality needs to be measured with human perception in many computer vision applications. However, these approaches are expensive and require more time for image quality analysis. Therefore, this paper proposes a robust and computationally efficient objective-mathematical model based on statistical perceptual features. The structural and textural features are computed using the modified regularized heaviside local binary pattern (RH-LBP) approach and the concept of entropy. The higher-order probability coefficients of images are considered to extract features that are highly correlated to the human visual system features. Further, the additivity property of Renyi entropy is used to show the randomness of the information combining two terms: One extracts the images spatial intensity changes, and therefore their texture qualities, and the other attain structural details. The features in the proposed approach are jointly optimized to achieve better robustness, monotonicity and match human assessments on image quality, while minimizing the computational complexity and run time. Experiments are conducted with three synthetically distorted datasets, KonIQ-10K, BIQ2021, and LIVE (wild), and two intentionally distorted datasets, TID2013 and CSIQ and are used to evaluate performance index. The proposed method offers competitive performance compared with state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {B. N. Al Sameera and Vilas H. Gaidhane},
  doi          = {10.1049/ipr2.13302},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13302},
  shortjournal = {IET Image Process.},
  title        = {An efficient no-reference image quality analysis based on statistical perceptual features},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic detection of rice disease in images of various
leaf sizes. <em>IETIP</em>, <em>19</em>(1), e13301. (<a
href="https://doi.org/10.1049/ipr2.13301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To help farmers manage limited resources, rice disease diagnosis must be accurate, timely, and affordable. This study addresses challenges in rice field images, such as environmental variability and differences in rice leaf sizes. The proposed technique combines convolutional neural network object detection with image tiling, using estimated rice leaf width as a reference for image division. An 18-layer ResNet model was trained using ground truth leaf width values for regression in leaf width estimation. Experiments used a dataset of 4960 images representing 8 rice diseases. The leaf width prediction model achieved a mean absolute percentage error of 11.18% and was used to generate a tiled dataset for training advanced rice disease detection models. The tiling technique was evaluated using YOLOv4, YOLOv8n, YOLOv8l, DINO-5scale Swin-L, and Co-DINO-5scale Swin-L models by comparing detection performance on original and tiled datasets. Mean average precision improved significantly: YOLOv4 increased from 87.56% to91.14%, YOLOv8n from 89.80% to 91.70%, and YOLOv8l from 89.80% to 93.20%. More advanced models, such as DINO5scale Swin-L and Co-DINO-5scale Swin-L, achieved even higher precision, at 93.40% and 94.20%, respectively. In conclusion, the tiling technique improved detection efficiency and addressed object size variability, enhancing rice disease detection accuracy inreal-world scenarios.},
  archive      = {J_IETIP},
  author       = {Kantip Kiratiratanapruk and Pitchayagan Temniranrat and Wasin Sinthupinyo and Sanparith Marukatat and Sujin Patarapuwadol},
  doi          = {10.1049/ipr2.13301},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13301},
  shortjournal = {IET Image Process.},
  title        = {Automatic detection of rice disease in images of various leaf sizes},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pothole detection-you only look once: Deformable convolution
based road pothole detection. <em>IETIP</em>, <em>19</em>(1), e13300.
(<a href="https://doi.org/10.1049/ipr2.13300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of road potholes plays a crucial role in ensuring passenger comfort and the structural safety of vehicles. To address the challenges of pothole detection in complex road environments, this paper proposes a model focusing on shape features (pothole detection you only look once, PD-YOLO). The model aims to overcome the limitations of multi-scale feature learning caused by the use of fixed convolutional kernels in the baseline model, by constructing a feature extraction module that better adapts to variations in the shape of potholes. Subsequently, a cross-stage partial network was designed using a one-time aggregation method, simplifying the model while enabling the network to fuse information between feature maps at different stages. Additionally, a dynamic sparse attention mechanism is introduced to select relevant features, reducing redundancy and suppressing background noise. Experiments conducted on the VOC2007 and GRDDC2020_Pothole datasets reveal that compared to the baseline model YOLOv8, PD-YOLO achieves improvements of 3.9% and 2.8% in mean average precision, with a frame rate of approximately 290 frames per second, effectively meeting the accuracy and real-time requirements for pothole detection. The code and dataset for this paper are located at: https://github.com/woyijiankou/PD-YOLO .},
  archive      = {J_IETIP},
  author       = {Pei Tang and Mao Lv and Zhenyu Ding and Weikai Xu and Minnan Jiang},
  doi          = {10.1049/ipr2.13300},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13300},
  shortjournal = {IET Image Process.},
  title        = {Pothole detection-you only look once: Deformable convolution based road pothole detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot real scenes convolutional anchor clustering twin
network for hyperspectral image classification. <em>IETIP</em>,
<em>19</em>(1), e13299. (<a
href="https://doi.org/10.1049/ipr2.13299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the performance of hyperspectral open set recognition processing with insufficient sample size, the authors propose a new supervised contrastive learning (SCL) framework (FSSCL-OSC) that can achieve open-set classification of hyperspectral images in scenarios with very few sample sizes, which consists of three steps. First, supervised contrastive learning (SCL) with geometric transformations is designed, which uses rotated labels as supervision to obtain lower-level characteristics that more accurately capture various orientations and then makes use of a spectral-spatial characteristic extraction network to maximize the utilization of HSI&#39;s spectrum and spatial information. Next, a class anchor open-set classification module based on a twin clustering network is designed, which uses SCL and FSL to extract more specific personal information by mining the category-invariant characteristics present in the known versus unknown class data. Finally, multiple convolution and open-set recognition (OSC) is performed on the feature blocks. Experimental results on three classical HSI datasets show that FSSCL-OSC provides a significant improvement over existing methods, under a sample size of only 10%, the overall accuracy reached 82.38%, 90.76% and 84.70%, respectively.},
  archive      = {J_IETIP},
  author       = {Xiangshan Zhou and Xiaoyi Tong and Xuchuan Zhou and Lin Wang and Lei Zhang and Jie Zhou and Su Qin},
  doi          = {10.1049/ipr2.13299},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13299},
  shortjournal = {IET Image Process.},
  title        = {Few-shot real scenes convolutional anchor clustering twin network for hyperspectral image classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel selection and local attention transformer model for
semantic segmentation on UAV remote sensing scene. <em>IETIP</em>,
<em>19</em>(1), e13298. (<a
href="https://doi.org/10.1049/ipr2.13298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with common urban landscape semantic segmentation, unmanned aerial vehicle (UAV) image semantic segmentation is more challenging because small targets have very low pixel percentages and multi-scale features due to the influence of flight altitude. Yet, the commonly used successive grid downsampling strategy in the current transformer-based methods omits some important features of small targets. Furthermore, due to the complex background interference, it can lead to even worse results. In reaction to this, existing strategies aim to maintain superior resolution. Nevertheless, the application of this method incurs considerable computational costs, which brings challenges for the practical applications of UAVs. So it is significant to design a novel framework to balance retaining more pixels representing small objects during downsampling and reducing computational costs. For this, the Channel Selection and the Local Attention Transformer Model (CSLFormer) are proposed. During the overlap patch embedding process of feature maps, the model allocates half of the important channels to global attention and local attention. These two types of attention focus on different aspects: one learns the relationships and importance among various patches, while the other emphasizes the features of individual patches. The method shows superior performance on two public datasets: AeroScapes and Vaihingen, achieving mean intersection over union (mIoU) of 75.57% and 78.93%, respectively. The proposed CSLFormer has been released on GitHub: https://github.com/leoda1/CSLFormer .},
  archive      = {J_IETIP},
  author       = {Da Liu and Hao Long and Zhenbao Liu},
  doi          = {10.1049/ipr2.13298},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13298},
  shortjournal = {IET Image Process.},
  title        = {Channel selection and local attention transformer model for semantic segmentation on UAV remote sensing scene},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent retrievable object-tracking system with
real-time edge inference capability. <em>IETIP</em>, <em>19</em>(1),
e13297. (<a href="https://doi.org/10.1049/ipr2.13297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intelligent retrievable object-tracking system assists users in quickly and accurately locating lost objects. However, challenges such as real-time processing on edge devices, low image resolution, and small-object detection significantly impact the accuracy and efficiency of video-stream-based systems, especially in indoor home environments. To overcome these limitations, a novel real-time intelligent retrievable object-tracking system is designed. The system incorporates a retrievable object-tracking algorithm that combines DeepSORT and sliding window techniques to enhance tracking capabilities. Additionally, the YOLOv7-small-scale model is proposed for small-object detection, integrating a specialized detection layer and the convolutional batch normalization LeakyReLU spatial-depth convolution module to enhance feature capture for small objects. TensorRT and INT8 quantization are used for inference acceleration on edge devices, doubling the frames per second. Experiments on a Jetson Nano (4 GB) using YOLOv7-small-scale show an 8.9% improvement in recognition accuracy over YOLOv7-tiny in video stream processing. This advancement significantly boosts the system&#39;s performance in efficiently and accurately locating lost objects in indoor homeÂ settings.},
  archive      = {J_IETIP},
  author       = {Yujie Li and Yifu Wang and Zihang Ma and Xinghe Wang and Benying Tan and Shuxue Ding},
  doi          = {10.1049/ipr2.13297},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13297},
  shortjournal = {IET Image Process.},
  title        = {An intelligent retrievable object-tracking system with real-time edge inference capability},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multispectral image fusion technique based on decoupling of
information. <em>IETIP</em>, <em>19</em>(1), e13296. (<a
href="https://doi.org/10.1049/ipr2.13296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of unremarkable targets and the serious loss of background details that occurs in multispectral image fusion, a multispectral image fusion method based on information decoupling is proposed. In accordance with the U-shaped network structure, multi-scale feature fusion is employed to obtain comprehensive contextual features. Subsequently, taking into account the differences between base and texture information in the images, the recovered image is reconstructed by fusing feature information through the information decoupling branches. Finally, the information decoupling is supervised by a loss function. In addition, a series of comparative experiments are conducted on a self-constructed multispectral dataset and a publicly available dataset. Experimental results demonstrate that our method not only achieves higher fusion accuracy but also has superior visualization results.},
  archive      = {J_IETIP},
  author       = {Wenqi Yu and Huilin Wang and Jilong Liu and Guan Wang},
  doi          = {10.1049/ipr2.13296},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13296},
  shortjournal = {IET Image Process.},
  title        = {Multispectral image fusion technique based on decoupling of information},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fisheye image rectification and restoration based on swin
transformer. <em>IETIP</em>, <em>19</em>(1), e13294. (<a
href="https://doi.org/10.1049/ipr2.13294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisheye cameras are widely used in surveillance, automotive systems, virtual reality, and panoramic photography due to their wide-angle perspective. However, images captured by fisheye cameras suffer from significant geometric distortions, affecting image analysis and understanding. This distortion bends straight lines into curves, resulting in a barrel-shaped appearance of the image. To mitigate these effects and transform fisheye images into a regular perspective, fisheye image correction is necessary, enabling more accurate and reliable performance in applications like object recognition, 3D reconstruction, and visual navigation. While convolutional neural networks based fisheye image correction has progressed, it has not fully utilized the global distribution and local symmetry of distortions due to the limitations of fixed receptive fields. This paper introduces a new model based on the Swin Transformer that effectively utilizes both global and local distortion features to adapt automatically to fisheye lens distortions. It also incorporates image restoration functionality to enhance texture details in the corrected images. A novel approach to synthetic dataset generation is proposed to improve the network&#39;s generalization capabilities.},
  archive      = {J_IETIP},
  author       = {Jian Xu and Dewei Han and Kang Li and Junjie Li and Zhaoyuan Ma},
  doi          = {10.1049/ipr2.13294},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13294},
  shortjournal = {IET Image Process.},
  title        = {Fisheye image rectification and restoration based on swin transformer},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCFA-net: A u-shaped cross-fusion network with attention
mechanism for enhanced polyp segmentation. <em>IETIP</em>,
<em>19</em>(1), e13293. (<a
href="https://doi.org/10.1049/ipr2.13293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing the precision of computer-assisted polyp segmentation and delineation during colonoscopies assists in the removal of potentially precancerous tissue, thus reducing the risk of malignant transformation. Most of the current medical segmentation models use the traditional U-shaped network structure, but they suffer from the problem of information loss during the encoding and decoding of images. To advance towards an autonomous model for detailed polyp segmentation, the authors propose a new framework for polyp segmentation called U-shaped cross-fusion network with attention mechanism (UCFA-Net), which employs a pyramid vision transformer as encoder to extract image features at multiple scales. Furthermore, the multi-scale cross-fusion module cross-fuses the different scale features and then goes through the multi-scale convolutional parallel feedforward transformer module for modelling the global and local information. Finally, progressive attentional up-sampling module acts as a decoder for up-sampling with progressive attention to get the final polyp segmentation result. The authors comprehensive testing demonstrates that their network achieves superior average scores across the five datasets and exhibits greater robustness in the face of diverse and demanding scenarios, when compared to current state-of-the-art approaches.},
  archive      = {J_IETIP},
  author       = {Shuai Wang and Tiejun Zhao and Guocun Wang and Ye Han and Fan Wu},
  doi          = {10.1049/ipr2.13293},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13293},
  shortjournal = {IET Image Process.},
  title        = {UCFA-net: A U-shaped cross-fusion network with attention mechanism for enhanced polyp segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gallbladder cancer detection via ultrasound image analysis:
An end-to-end hierarchical feature-fused model. <em>IETIP</em>,
<em>19</em>(1), e13292. (<a
href="https://doi.org/10.1049/ipr2.13292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gallbladder cancer is a fatal disease, and its early diagnosis can significantly impact patient treatment. Ultrasound imaging is often the initial diagnostic test for gallbladder cancer, making the enhancement of cancer detection accuracy from these images crucial. Despite the promising results of artificial intelligence techniques in disease diagnosis, their black-box nature hinders the reliability of their results and their practical application. Therefore, it is essential not to rely solely on a single model&#39;s output and to further investigate for more reliable outcomes. This study presents a step-by-step structural investigation of forming an end-to-end model, a conjunction of two convolutional neural network based methods, for detecting gallbladder conditions. The final model, leveraging feature fusions and hierarchical classification, achieved a high accuracy of 92.62% for detecting normal, benign, and malignant gallbladders. It also achieved a remarkable accuracy of 98.36% for classifying normal and non-normal instances and 92.22% for classifying benign and malignant cases. Finally, comprehensive post-processing investigations, including cross-validation, temperature scaling, and uncertainty estimation, along with error analysis, are conducted to gain more insights into the model&#39;s output. Among these insights, the model demonstrated resilience of its results to active dropout and augmentation at the inference phase. Furthermore, when applied with test-time data augmentation, uncertainty estimation methods have better distinguishability between the uncertainties of correctly and incorrectly classified instances, which provides additional information about the model&#39;s output. The source code of experiments conducted in this study is available at https://github.com/SaraDadjouy/GBCRet .},
  archive      = {J_IETIP},
  author       = {Sara Dadjouy and Hedieh Sajedi},
  doi          = {10.1049/ipr2.13292},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13292},
  shortjournal = {IET Image Process.},
  title        = {Gallbladder cancer detection via ultrasound image analysis: An end-to-end hierarchical feature-fused model},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer morphological features for segmentation with few
labels on fluorescent mitochondria images. <em>IETIP</em>,
<em>19</em>(1), e13290. (<a
href="https://doi.org/10.1049/ipr2.13290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated segmentation of mitochondria is crucial for statistical analysis in biological research. Existing segmentation techniques often face challenges with fluorescence images. Handcrafted methods have poor segmentation results while deep learning-based methods lack the labeled mitochondrial data. However, although the number of labeled mitochondrial images is limited, the unlabeled fluorescent data is easy to obtain. The authors aim to leverage a large amount of unlabeled data to learn mitochondrial morphological features. The approach begins with self-supervised learning from a vast set of unlabeled images through masked image modeling. This technique involves presenting images with randomly masked patches, prompting the model to predict the content of these masked areas. By doing so, the model learns the distinctive features of mitochondria. In the subsequent phase, the trained encoder is transferred to the segmentation task, replacing the original reconstruction decoder with the Segformer segmentation decoder. The model is then fine-tuned using a small labeled dataset. By reconstructing mitochondria in the masked regions, the model learns features more effectively on unlabeled samples, and improves segmentation performance even with limited labeled data. Empirical results validate the effectiveness of the approach, showing an 11.8% improvement in Intersection over Union metrics compared to existing fluorescence mitochondrial segmentationÂ techniques.},
  archive      = {J_IETIP},
  author       = {Tianyi Zhang and Junchao Fan and Xiuli Bi and Weisheng Li and Bin Xiao and Xiaoshuai Huang},
  doi          = {10.1049/ipr2.13290},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13290},
  shortjournal = {IET Image Process.},
  title        = {Transfer morphological features for segmentation with few labels on fluorescent mitochondria images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised brain tumour segmentation with label
propagation and level set loss. <em>IETIP</em>, <em>19</em>(1), e13289.
(<a href="https://doi.org/10.1049/ipr2.13289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis of brain tumors significantly enhances treatment success. However, accurate detection and segmentation of tumors, essential for diagnosis, rely heavily on costly manual annotation by experts. To mitigate these costs, weakly supervised methods have gained traction. This paper introduces a novel weakly supervised brain tumor segmentation approach utilizing point and scribble supervision. Experts annotate only the slice with the largest tumor area by marking a single point near the tumor center or drawing a scribble within the tumor region. The method operates in two phases. First, labels are propagated to unlabelled pixels, generating a pseudo-ground-truth with three labels: tumor, non-tumor, and marginal pixels (unlabelled pixels surrounding the initial segmentation). Second, a segmentation model is trained using the pseudo-ground-truth and a loss function combining level-set and binary cross-entropy losses. Marginal pixels contribute to level-set loss computation, refining the segmentation process. The approach is validated on 3D magnetic resonance imaging (MRI) volumes from BraTS2020, BraTS2021, and BraTS2023 benchmark datasets. Experimental results show Dice scores comparable to fully supervised methods for whole tumor segmentation, demonstrating the effectiveness of the proposed weakly supervised strategy. This method reduces annotation effort while maintaining competitive segmentation performance, making it valuable for clinical applications.},
  archive      = {J_IETIP},
  author       = {Fatemeh-Sadat Abadian-Zadeh and Mohammad Reza Mohammadi and Mohsen Soryani},
  doi          = {10.1049/ipr2.13289},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13289},
  shortjournal = {IET Image Process.},
  title        = {Weakly supervised brain tumour segmentation with label propagation and level set loss},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corn leaf disease recognition based on improved
EfficientNet. <em>IETIP</em>, <em>19</em>(1), e13288. (<a
href="https://doi.org/10.1049/ipr2.13288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maize leaf disease seriously affects maize yield, a maize leaf disease identification model with an improved lightweight network EfficientNet was proposed in this study. First, the model replaces the SENet module in the MBConv module with the CBAM module, so that the model not only focuses on the correlation between the channels but also adaptively learns the attentional weight of each spatial location. Furthermore, a multi-scale feature fusion layer based on residual connection is introduced to extract more comprehensive and richer disease features at different scales. Finally, by introducing the double pooling method, the overall feature distribution is smoothed while highlighting the important disease features. After the three improvements, the model&#39;s recognition accuracy on the test set increased by 2.34%, 2.16%, and 0.97%, respectively, and the improved model achieved an average recognition accuracy of 98.32%, an average precision of 98.29%, and an average recall of 98.25%. The experimental results compared with other models show that the average recognition accuracy of the proposed model is 5.23%, 3.68%, 1.99%, 1.79%, and 3.2% higher than ResNet34, DenseNet121, MobileNet V2, SqueezeNet, and EfficientNet B0, respectively. Activation heat maps show that the improved model can effectively suppress background interference.},
  archive      = {J_IETIP},
  author       = {Xiaowei Sun and Hua Huo},
  doi          = {10.1049/ipr2.13288},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13288},
  shortjournal = {IET Image Process.},
  title        = {Corn leaf disease recognition based on improved EfficientNet},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDRN: Multi-domain representation network for unsupervised
domain generalization. <em>IETIP</em>, <em>19</em>(1), e13283. (<a
href="https://doi.org/10.1049/ipr2.13283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep neural networks, performance can degrade when test data distributions differ from training data. Unsupervised Domain Generalization (UDG) aims to improve generalization across unseen domains by leveraging multiple source domains without supervision. Traditional methods focus on extracting domain-invariant features, potentially at the expense of feature space integrity and generalization potential. We presents a Multi-Domain Representation Network (MDRN) for unsupervised multi-domain learning. MDRN innovates by disentangling and preserving both domain-invariant and domain-specific features through an unsupervised cross-domain reconstruction task. It employs content encoders for domain-invariant features and multi-domain style encoders for domain-specific characteristics. By merging these features based on domain similarity, MDRN constructs a comprehensive feature space that enhances image reconstruction across domains. Additionally, MDRN integrates domain-specific classifiers, which learn domain classification and provide weighted fusion of domain-specific features. This design facilitates effective inter-domain distance measurement and feature integration. Experiments on PACS and DomainNet show MDRN&#39;s superior performance over existing state-of-the-art UDG approaches, highlighting its effectiveness in handling distribution shifts between source and target domains.},
  archive      = {J_IETIP},
  author       = {Yangyang Zhong and YunFeng Yan and Pengxin Luo and Weizhen He and Yiheng Deng and Donglian Qi},
  doi          = {10.1049/ipr2.13283},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13283},
  shortjournal = {IET Image Process.},
  title        = {MDRN: Multi-domain representation network for unsupervised domain generalization},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From pixels to prognosis: Attention-CNN model for COVID-19
diagnosis using chest CT images. <em>IETIP</em>, <em>19</em>(1), e13249.
(<a href="https://doi.org/10.1049/ipr2.13249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning assisted diagnosis for assessing the severity of various respiratory infections using chest computed tomography (CT) scan images has gained much attention after the COVID-19 pandemic. Major tasks while building such models require an understanding of the characteristic features associated with the disease, patient-to-patient variations and changes associated with disease severity. In this work, an attention-based convolutional neural network (CNN) model with customized bottleneck residual module (Attn-CNN) is proposed for classifying CT images into three classes: COVID-19, normal, and other pneumonia. The efficacy of the model is evaluated by carrying out various experiments, such as effect of class imbalance, impact of attention module, generalizability of the model and providing visualization of model&#39;s prediction for the interpretability of results. Comparative performance evaluation with five state-of-the-art deep architectures such as MobileNet, EfficientNet-B7, Inceptionv3, ResNet-50 and VGG-16, and with published models such as COVIDNet-CT, COVNet, COVID-Net CT2, etc. is discussed.},
  archive      = {J_IETIP},
  author       = {Suba Suseela and Nita Parekh},
  doi          = {10.1049/ipr2.13249},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13249},
  shortjournal = {IET Image Process.},
  title        = {From pixels to prognosis: Attention-CNN model for COVID-19 diagnosis using chest CT images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast intensive crowd counting model of internet of things
based on multi-scale attention mechanism. <em>IETIP</em>,
<em>19</em>(1), e12686. (<a
href="https://doi.org/10.1049/ipr2.12686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection based on deep learning plays an important role in the application of the Internet of Things (IoT). Traditional methods consume a lot of computing resources and cannot be well deployed in the IoT environment. A lightweight object detection method based on attention mechanism is proposed and applied to crowd counting. In view of the low accuracy and poor real-time performance of multi-scale crowd detection, we design a crowd counting model based on YOLO v5, and apply it to the IoT environment. It is proposed to insert the transformer into the YOLO v5 backbone network. Based on the multi-head attention mechanism in the transformer encoder, the global dependency is modelled to make full use of the context information. The CNN is used to realize the fusion of multi-scale feature maps, and the feature enhancement modules concerned by the attention network are further counted. Experiments show that it can not only detect multi-scale targets, but also achieve real-time performance in video surveillance scenes.},
  archive      = {J_IETIP},
  author       = {Dong Liu and Zhiyong Wang and Xiangjia Meng},
  doi          = {10.1049/ipr2.12686},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e12686},
  shortjournal = {IET Image Process.},
  title        = {Fast intensive crowd counting model of internet of things based on multi-scale attention mechanism},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A single-model multi-task method for face recognition and
face attribute recognition in internet of things and visual computing.
<em>IETIP</em>, <em>19</em>(1), e12611. (<a
href="https://doi.org/10.1049/ipr2.12611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of the internet of things (IoT) is steadily increasing in a wide range of applications. Integration of IoT, computer vision, and artificial intelligence can improve people&#39;s daily life in various domains such as smart homes, smart cities, and smart industries. There are a large number of face recognition and face attribute recognition scenarios in reality, and the industry commonly decomposes these tasks, with three models responsible for handling face detection, face recognition, and face attribute recognition. The multi-model approach requires a lot of computational resources for context switching, while training one model with one dataset is not only complex, but also leads to overfitting of the multi-model approach. The authors propose a single-model multi-task approach, which can complete all tasks using only one model, and thus obtains a great improvement in inference speed, especially in scenes with high face density. After an experimental comparison, our approach saves a maximum of 96% of inference time, 49.5% of memory usage, and 59.7% of CPU time, avoids frequent context switching, and simplifies the training steps while improving the generalization performance of the model.},
  archive      = {J_IETIP},
  author       = {Jin Lu and Bo Wu},
  doi          = {10.1049/ipr2.12611},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e12611},
  shortjournal = {IET Image Process.},
  title        = {A single-model multi-task method for face recognition and face attribute recognition in internet of things and visual computing},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
