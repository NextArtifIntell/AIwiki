<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco---7">NECO - 7</h2>
<ul>
<li><details>
<summary>
(2025). Nearly optimal learning using sparse deep ReLU networks in
regularized empirical risk minimization with lipschitz loss.
<em>NECO</em>, <em>37</em>(4), 815–870. (<a
href="https://doi.org/10.1162/neco_a_01742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a sparse deep ReLU network (SDRN) estimator of the regression function obtained from regularized empirical risk minimization with a Lipschitz loss function. Our framework can be applied to a variety of regression and classification problems. We establish novel nonasymptotic excess risk bounds for our SDRN estimator when the regression function belongs to a Sobolev space with mixed derivatives. We obtain a new, nearly optimal, risk rate in the sense that the SDRN estimator can achieve nearly the same optimal minimax convergence rate as one-dimensional nonparametric regression with the dimension involved in a logarithm term only when the feature dimension is fixed. The estimator has a slightly slower rate when the dimension grows with the sample size. We show that the depth of the SDRN estimator grows with the sample size in logarithmic order, and the total number of nodes and weights grows in polynomial order of the sample size to have the nearly optimal risk rate. The proposed SDRN can go deeper with fewer parameters to well estimate the regression and overcome the overfitting problem encountered by conventional feedforward neural networks.},
  archive      = {J_NECO},
  author       = {Huang, Ke and Liu, Mingming and Ma, Shujie},
  doi          = {10.1162/neco_a_01742},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {815-870},
  shortjournal = {Neural Comput.},
  title        = {Nearly optimal learning using sparse deep ReLU networks in regularized empirical risk minimization with lipschitz loss},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced EEG forecasting: A probabilistic deep learning
approach. <em>NECO</em>, <em>37</em>(4), 793–814. (<a
href="https://doi.org/10.1162/neco_a_01743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting electroencephalography (EEG) signals, that is, estimating future values of the time series based on the past ones, is essential in many real-time EEG-based applications, such as brain–computer interfaces and closed-loop brain stimulation. As these applications are becoming more and more common, the importance of a good prediction model has increased. Previously, the autoregressive model (AR) has been employed for this task; however, its prediction accuracy tends to fade quickly as multiple steps are predicted. We aim to improve on this by applying probabilistic deep learning to make robust longer-range forecasts. For this, we applied the probabilistic deep neural network model WaveNet to forecast resting-state EEG in theta- (4–7.5 Hz) and alpha-frequency (8–13 Hz) bands and compared it to the AR model. WaveNet reliably predicted EEG signals in both theta and alpha frequencies 150 ms ahead, with mean absolute errors of 1.0 ± 1.1 µV (theta) and 0.9 ± 1.1 µV (alpha), and outperformed the AR model in estimating the signal amplitude and phase. Furthermore, we found that the probabilistic approach offers a way of forecasting even more accurately while effectively discarding uncertain predictions. We demonstrate for the first time that probabilistic deep learning can be used to forecast resting-state EEG time series. In the future, the developed model can enhance the real-time estimation of brain states in brain–computer interfaces and brain stimulation protocols. It may also be useful for answering neuroscientific questions and for diagnostic purposes.},
  archive      = {J_NECO},
  author       = {Pankka, Hanna and Lehtinen, Jaakko and Ilmoniemi, Risto J. and Roine, Timo},
  doi          = {10.1162/neco_a_01743},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {793-814},
  shortjournal = {Neural Comput.},
  title        = {Enhanced EEG forecasting: A probabilistic deep learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge as a breaking of ergodicity. <em>NECO</em>,
<em>37</em>(4), 742–792. (<a
href="https://doi.org/10.1162/neco_a_01741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a thermodynamic potential that can guide training of a generative model defined on a set of binary degrees of freedom. We argue that upon reduction in description, so as to make the generative model computationally manageable, the potential develops multiple minima. This is mirrored by the emergence of multiple minima in the free energy proper of the generative model itself. The variety of training samples that employ N binary degrees of freedom is ordinarily much lower than the size 2 N of the full phase space. The nonrepresented configurations, we argue, should be thought of as comprising a high-temperature phase separated by an extensive energy gap from the configurations composing the training set. Thus, training amounts to sampling a free energy surface in the form of a library of distinct bound states, each of which breaks ergodicity. The ergodicity breaking prevents escape into the near continuum of states comprising the high-temperature phase; thus, it is necessary for proper functionality. It may, however, have the side effect of limiting access to patterns that were underrepresented in the training set. At the same time, the ergodicity breaking within the library complicates both learning and retrieval. As a remedy, one may concurrently employ multiple generative models—up to one model per free energy minimum.},
  archive      = {J_NECO},
  author       = {He, Yang and Lubchenko, Vassiliy},
  doi          = {10.1162/neco_a_01741},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {742-792},
  shortjournal = {Neural Comput.},
  title        = {Knowledge as a breaking of ergodicity},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in wilson-cowan model for metapopulation.
<em>NECO</em>, <em>37</em>(4), 701–741. (<a
href="https://doi.org/10.1162/neco_a_01744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Wilson-Cowan model for metapopulation, a neural mass network model, treats different subcortical regions of the brain as connected nodes, with connections representing various types of structural, functional, or effective neuronal connectivity between these regions. Each region comprises interacting populations of excitatory and inhibitory cells, consistent with the standard Wilson-Cowan model. In this article, we show how to incorporate stable attractors into such a metapopulation model’s dynamics. By doing so, we transform the neural mass network model into a biologically inspired learning algorithm capable of solving different classification tasks. We test it on MNIST and Fashion MNIST in combination with convolutional neural networks, as well as on CIFAR-10 and TF-FLOWERS, and in combination with a transformer architecture (BERT) on IMDB, consistently achieving high classification accuracy.},
  archive      = {J_NECO},
  author       = {Marino, Raffaele and Buffoni, Lorenzo and Chicchi, Lorenzo and Patti, Francesca Di and Febbe, Diego and Giambagli, Lorenzo and Fanelli, Duccio},
  doi          = {10.1162/neco_a_01744},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {701-741},
  shortjournal = {Neural Comput.},
  title        = {Learning in wilson-cowan model for metapopulation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active inference and intentional behavior. <em>NECO</em>,
<em>37</em>(4), 666–700. (<a
href="https://doi.org/10.1162/neco_a_01738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in theoretical biology suggest that key definitions of basal cognition and sentient behavior may arise as emergent properties of in vitro cell cultures and neuronal networks. Such neuronal networks reorganize activity to demonstrate structured behaviors when embodied in structured information landscapes. In this article, we characterize this kind of self-organization through the lens of the free energy principle, that is, as self-evidencing. We do this by first discussing the definitions of reactive and sentient behavior in the setting of active inference, which describes the behavior of agents that model the consequences of their actions. We then introduce a formal account of intentional behavior that describes agents as driven by a preferred end point or goal in latent state-spaces. We then investigate these forms of (reactive, sentient, and intentional) behavior using simulations. First, we simulate the in vitro experiments, in which neuronal cultures modulated activity to improve gameplay in a simplified version of Pong by implementing nested, free energy minimizing processes. The simulations are then used to deconstruct the ensuing predictive behavior, leading to the distinction between merely reactive, sentient, and intentional behavior with the latter formalized in terms of inductive inference. This distinction is further studied using simple machine learning benchmarks (navigation in a grid world and the Tower of Hanoi problem) that show how quickly and efficiently adaptive behavior emerges under an inductive form of active inference.},
  archive      = {J_NECO},
  author       = {Friston, Karl J. and Salvatori, Tommaso and Isomura, Takuya and Tschantz, Alexander and Kiefer, Alex and Verbelen, Tim and Koudahl, Magnus and Paul, Aswin and Parr, Thomas and Razi, Adeel and Kagan, Brett J. and Buckley, Christopher L. and Ramstead, Maxwell J. D.},
  doi          = {10.1162/neco_a_01738},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {666-700},
  shortjournal = {Neural Comput.},
  title        = {Active inference and intentional behavior},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiking neuron-astrocyte networks for image recognition.
<em>NECO</em>, <em>37</em>(4), 635–665. (<a
href="https://doi.org/10.1162/neco_a_01740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From biological and artificial network perspectives, researchers have started acknowledging astrocytes as computational units mediating neural processes. Here, we propose a novel biologically inspired neuron-astrocyte network model for image recognition, one of the first attempts at implementing astrocytes in spiking neuron networks (SNNs) using a standard data set. The architecture for image recognition has three primary units: the preprocessing unit for converting the image pixels into spiking patterns, the neuron-astrocyte network forming bipartite (neural connections) and tripartite synapses (neural and astrocytic connections), and the classifier unit. In the astrocyte-mediated SNNs, an astrocyte integrates neural signals following the simplified Postnov model. It then modulates the integrate-and-fire (IF) neurons via gliotransmission, thereby strengthening the synaptic connections of the neurons within the astrocytic territory. We develop an architecture derived from a baseline SNN model for unsupervised digit classification. The spiking neuron-astrocyte networks (SNANs) display better network performance with an optimal variance-bias trade-off than SNN alone. We demonstrate that astrocytes promote faster learning, support memory formation and recognition, and provide a simplified network architecture. Our proposed SNAN can serve as a benchmark for future researchers on astrocyte implementation in artificial networks, particularly in neuromorphic systems, for its simplified design.},
  archive      = {J_NECO},
  author       = {Lorenzo, Jhunlyn and Rico-Gallego, Juan-Antonio and Binczak, Stéphane and Jacquir, Sabir},
  doi          = {10.1162/neco_a_01740},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {635-665},
  shortjournal = {Neural Comput.},
  title        = {Spiking neuron-astrocyte networks for image recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-sensitive processing in a model neocortical
pyramidal cell with two sites of input integration. <em>NECO</em>,
<em>37</em>(4), 588–634. (<a
href="https://doi.org/10.1162/neco_a_01739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neocortical layer 5 thick-tufted pyramidal cells are prone to exhibiting burst firing on receipt of coincident basal and apical dendritic inputs. These inputs carry different information, with basal inputs coming from feedforward sensory pathways and apical inputs coming from diverse sources that provide context in the cortical hierarchy. We explore the information processing possibilities of this burst firing using computer simulations of a noisy compartmental cell model. Simulated data on stochastic burst firing due to brief, simultaneously injected basal and apical currents allow estimation of burst firing probability for different stimulus current amplitudes. Information-theory-based partial information decomposition (PID) is used to quantify the contributions of the apical and basal input streams to the information in the cell output bursting probability. Four different operating regimes are apparent, depending on the relative strengths of the input streams, with output burst probability carrying more or less information that is uniquely contributed by either the basal or apical input, or shared and synergistic information due to the combined streams. We derive and fit transfer functions for these different regimes that describe burst probability over the different ranges of basal and apical input amplitudes. The operating regimes can be classified into distinct modes of information processing, depending on the contribution of apical input to output bursting: apical cooperation, in which both basal and apical inputs are required to generate a burst; apical amplification, in which basal input alone can generate a burst but the burst probability is modulated by apical input; apical drive, in which apical input alone can produce a burst; and apical integration, in which strong apical or basal inputs alone, as well as their combination, can generate bursting. In particular, PID and the transfer function clarify that the apical amplification mode has the features required for contextually modulated information processing.},
  archive      = {J_NECO},
  author       = {Graham, Bruce P. and Kay, Jim W. and Phillips, William A.},
  doi          = {10.1162/neco_a_01739},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {588-634},
  shortjournal = {Neural Comput.},
  title        = {Context-sensitive processing in a model neocortical pyramidal cell with two sites of input integration},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
