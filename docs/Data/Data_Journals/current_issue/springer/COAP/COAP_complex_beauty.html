<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coap---10">COAP - 10</h2>
<ul>
<li><details>
<summary>
(2025). On the well-posedness of tracking dirichlet data for
bernoulli free boundary problems. <em>COAP</em>, <em>91</em>(1),
311–349. (<a href="https://doi.org/10.1007/s10589-025-00662-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to study the shape optimization method for solving the Bernoulli free boundary problem, a well-known ill-posed problem that seeks the unknown free boundary through Cauchy data. Different formulations have been proposed in the literature that differ in the choice of the objective functional. Specifically, it was shown respectively in Eppler and Harbrecht (SIAM J Control Optim 48:2901–2916, 2010, J Optim Theory Appl 145:17–35, 2010) that tracking Neumann data is well-posed but tracking Dirichlet data is not. In this paper we propose a new well-posed objective functional that tracks Dirichlet data at the free boundary. By calculating the Euler derivative and the shape Hessian of the objective functional we show that the new formulation is well-posed, i.e., the shape Hessian is coercive at the minima. The coercivity of the shape Hessian may ensure the existence of optimal solutions for the nonlinear Ritz–Galerkin approximation method and its convergence, thus is crucial for the formulation. As a summary, we conclude that tracking Dirichlet or Neumann data in their energy norm is not sufficient, but tracking them in a half an order higher norm will be well-posed. To support our theoretical results we carry out extensive numerical experiments.},
  archive      = {J_COAP},
  author       = {Gong, Wei and Liu, Le},
  doi          = {10.1007/s10589-025-00662-3},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {311-349},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the well-posedness of tracking dirichlet data for bernoulli free boundary problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speeding up l-BFGS by direct approximation of the inverse
hessian matrix. <em>COAP</em>, <em>91</em>(1), 283–310. (<a
href="https://doi.org/10.1007/s10589-025-00665-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {L-BFGS is one of the widely used quasi-Newton methods. Instead of explicitly storing an approximation H of the inverse Hessian, L-BFGS keeps a limited number of vectors that can be used for computing the product of H by the gradient. However, this computation is sequential, each step depending on the outcome of the previous step. To solve this problem, we propose the Direct L-BFGS (DirL-BFGS) method that, seeing H as a linear operator, directly stores a low-rank plus diagonal (LRPD) representation of H. Employing the LRPD representation enables us to leverage the benefits of vector processing, leading to accelerating and parallelizing the calculations in the form of single instruction, multiple data. We evaluate our proposed method on different quadratic optimization problems and several regression and classification tasks with neural networks. Numerical results show that DirL-BFGS is faster overall than L-BFGS.},
  archive      = {J_COAP},
  author       = {Sadeghi-Lotfabadi, Ashkan and Ghiasi-Shirazi, Kamaledin},
  doi          = {10.1007/s10589-025-00665-0},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {283-310},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Speeding up L-BFGS by direct approximation of the inverse hessian matrix},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The <span class="math display"><em>ω</em></span> -condition
number: Applications to preconditioning and low rank generalized
jacobian updating. <em>COAP</em>, <em>91</em>(1), 235–282. (<a
href="https://doi.org/10.1007/s10589-025-00669-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preconditioning is essential in iterative methods for solving linear systems. It is also the implicit objective in updating approximations of Jacobians in optimization methods, e.g., in quasi-Newton methods. We study a nonclassic matrix condition number, the $$\omega $$ -condition number, $$\omega $$ for short. $$\omega $$ is the ratio of: the arithmetic and geometric means of the singular values, rather than the largest and smallest for the classical $$\kappa $$ -condition number. The simple functions in $$\omega $$ allow one to exploit first order optimality conditions. We use this fact to derive explicit formulae for (i) $$\omega $$ -optimal low rank updating of generalized Jacobians arising in the context of nonsmooth Newton methods; and (ii) $$\omega $$ -optimal preconditioners of special structure for iterative methods for linear systems. In the latter context, we analyze the benefits of $$\omega $$ for (a) improving the clustering of eigenvalues; (b) reducing the number of iterations; and (c) estimating the actual condition of a linear system. Moreover we show strong theoretical connections between the $$\omega $$ -optimal preconditioners and incomplete Cholesky factorizations, and highlight the misleading effects arising from the inverse invariance of $$\kappa $$ . Our results confirm the efficacy of using the $$\omega $$ -condition number compared to the $$\kappa $$ -condition number.},
  archive      = {J_COAP},
  author       = {Jung, Woosuk L. and Torregrosa-Belén, David and Wolkowicz, Henry},
  doi          = {10.1007/s10589-025-00669-w},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {235-282},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The $$\omega $$ -condition number: Applications to preconditioning and low rank generalized jacobian updating},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inertial accelerated stochastic mirror descent for
large-scale generalized tensor CP decomposition. <em>COAP</em>,
<em>91</em>(1), 201–233. (<a
href="https://doi.org/10.1007/s10589-025-00668-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of classic tensor CP decomposition models are designed for squared loss, utilizing Euclidean distance as a local proximal term. However, the Euclidean distance is unsuitable for the generalized loss function applicable to diverse types of real-world data, such as integer and binary data. Consequently, algorithms developed under the squared loss are not easily adaptable to handle these generalized losses, partially due to the absence of the gradient Lipschitz continuity. This paper explores generalized tensor CP decomposition, employing the Bregman distance as the proximal term and introducing an inertial accelerated block randomized stochastic mirror descent algorithm (iTableSMD). Within a broader multi-block variance reduction and inertial acceleration framework, we demonstrate the sublinear convergence rate for the subsequential sequence produced by the iTableSMD algorithm. We further show that iTableSMD requires at most $$\mathcal {O}(\varepsilon ^{-2})$$ iterations in expectation to attain an $$\varepsilon $$ -stationary point and establish the global convergence of the sequence. Numerical experiments on real datasets demonstrate that our proposed algorithm is efficient and achieves better performance than the existing state-of-the-art methods.},
  archive      = {J_COAP},
  author       = {Liu, Zehui and Wang, Qingsong and Cui, Chunfeng and Xia, Yong},
  doi          = {10.1007/s10589-025-00668-x},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {201-233},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inertial accelerated stochastic mirror descent for large-scale generalized tensor CP decomposition},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a family of relaxed gradient descent methods for strictly
convex quadratic minimization. <em>COAP</em>, <em>91</em>(1), 173–200.
(<a href="https://doi.org/10.1007/s10589-025-00670-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the convergence properties of a family of Relaxed $$\ell $$ -Minimal Gradient Descent methods for quadratic optimization; the family includes the omnipresent Steepest Descent method, as well as the Minimal Gradient method. Simple proofs are provided that show, in an appropriately chosen norm, the gradient and the distance of the iterates from optimality converge linearly, for all members of the family. Moreover, the function values decrease linearly, and iteration complexity results are provided. All theoretical results hold when (fixed) relaxation is employed. It is also shown that, given a fixed overhead and storage budget, every Relaxed $$\ell $$ -Minimal Gradient Descent method can be implemented using exactly one matrix vector product. Numerical experiments are presented that illustrate the benefits of relaxation across the family.},
  archive      = {J_COAP},
  author       = {MacDonald, Liam and Murray, Rua and Tappenden, Rachael},
  doi          = {10.1007/s10589-025-00670-3},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {173-200},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On a family of relaxed gradient descent methods for strictly convex quadratic minimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding search directions in quasi-newton methods for
minimizing a quadratic function subject to uncertainty. <em>COAP</em>,
<em>91</em>(1), 145–171. (<a
href="https://doi.org/10.1007/s10589-025-00661-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate quasi-Newton methods for minimizing a strongly convex quadratic function which is subject to errors in the evaluation of the gradients. In particular, we focus on computing search directions for quasi-Newton methods that all give identical behavior in exact arithmetic, generating minimizers of Krylov subspaces of increasing dimensions, thereby having finite termination. The BFGS quasi-Newton method may be seen as an ideal method in exact arithmetic and is empirically known to behave very well on a quadratic problem subject to small errors. We investigate large-error scenarios, in which the expected behavior is not so clear. We consider memoryless methods that are less expensive than the BFGS method, in that they generate low-rank quasi-Newton matrices that differ from the identity by a symmetric matrix of rank two. In addition, a more advanced model for generating the search directions is proposed, based on solving a chance-constrained optimization problem. Our numerical results indicate that for large errors, such a low-rank memoryless quasi-Newton method may perform better than a BFGS method. In addition, the results indicate a potential edge by including the chance-constrained model in the memoryless quasi-Newton method.},
  archive      = {J_COAP},
  author       = {Peng, Shen and Canessa, Gianpiero and Ek, David and Forsgren, Anders},
  doi          = {10.1007/s10589-025-00661-4},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {145-171},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Finding search directions in quasi-newton methods for minimizing a quadratic function subject to uncertainty},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quadratic convex reformulations for a class of complex
quadratic programming problems. <em>COAP</em>, <em>91</em>(1), 125–144.
(<a href="https://doi.org/10.1007/s10589-025-00672-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a class of complex quadratic programming problems characterized by unit-modulus and discrete argument constraints. This problem can be reformulated as a mixed-integer quadratic programming problem, which could be addressed using a commercial solver such as Gurobi. However, the solver’s efficiency is often unsatisfying if the problem formulation is inadequately designed. In this paper, we introduce several quadratic convex reformulations aimed at enhancing the solver’s performance. We extend the classical diagonal perturbation-based reformulation technique to this problem. Additionally, by leveraging the unique structure of the problem, we derive a new quadratic convex reformulation that provides a tighter continuous relaxation compared to the diagonal perturbation-based approach. The numerical tests on random instances and the unimodular code design problem demonstrate the superiority of the newly proposed reformulation.},
  archive      = {J_COAP},
  author       = {Lu, Cheng and Kang, Gaojian and Qu, Guangtai and Deng, Zhibin},
  doi          = {10.1007/s10589-025-00672-1},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {125-144},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Quadratic convex reformulations for a class of complex quadratic programming problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a globally convergent semismooth* newton method in
nonsmooth nonconvex optimization. <em>COAP</em>, <em>91</em>(1), 67–124.
(<a href="https://doi.org/10.1007/s10589-025-00658-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present GSSN, a globalized SCD semismooth $$^{*}$$ Newton method for solving nonsmooth nonconvex optimization problems. The global convergence properties of the method are ensured by the proximal gradient method, whereas locally superlinear convergence is established via the SCD semismooth $$^{*}$$ Newton method under quite weak assumptions. The Newton direction is based on the SC (subspace containing) derivative of the subdifferential mapping and can be computed by the (approximate) solution of an equality-constrained quadratic program. Special attention is given to the efficient numerical implementation of the overall method.},
  archive      = {J_COAP},
  author       = {Gfrerer, Helmut},
  doi          = {10.1007/s10589-025-00658-z},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {67-124},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On a globally convergent semismooth* newton method in nonsmooth nonconvex optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximal gradient method for convex multiobjective
optimization problems without lipschitz continuous gradients.
<em>COAP</em>, <em>91</em>(1), 27–66. (<a
href="https://doi.org/10.1007/s10589-025-00663-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes a proximal gradient method for nondifferentiable convex multiobjective optimization problems, where the components of the objective function are the sum of a proper lower semicontinuous function and a continuously differentiable function. By adopting a typical line search procedure, it is found that without a Lipschitz continuity of the gradients of the smooth part of the objective function, the proposed method is able to generate sequences that converge to weakly Pareto optimal points. The convergence rate of the method is found to be $$\mathcal {O}(1/k)$$ . Further, if the smooth component in the objective function is strongly convex, then the convergence rate is $$\mathcal {O}(r^k)$$ for some $$r\in (0,1)$$ . Moreover, in the absence of a strong convexity assumption, we also consider the accelerated version of the proposed approach based on the Nesterov step strategy. We obtain the improved convergence rate of $$\mathcal {O}(1/k^2)$$ , which is measured by a merit function. Numerical implementation strategies and performance profiles of the proposed methods on the considered problem involving $$\ell _1$$ -norm and indicator function are also provided.},
  archive      = {J_COAP},
  author       = {Zhao, Xiaopeng and Raushan, Ravi and Ghosh, Debdas and Yao, Jen-Chih and Qi, Min},
  doi          = {10.1007/s10589-025-00663-2},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {27-66},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Proximal gradient method for convex multiobjective optimization problems without lipschitz continuous gradients},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A symmetric version of the generalized
chambolle-pock-he-yuan method for saddle point problems. <em>COAP</em>,
<em>91</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10589-025-00671-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primal-dual method for solving convex-concave saddle point problems was initially proposed by Chambolle and Pock, and its convergence was first proved by He and Yuan later. This primal-dual method (“CPHY&quot; for short) reduces to the Arrow–Hurwicz method (as well as the primal-dual hybrid gradient method) when the combination parameter $$\theta =0$$ , and to a special case of the proximal point algorithm when $$\theta =1$$ , both of which have been well-studied. However, for $$\theta \in (0,1)$$ , some theoretical aspects are not yet well-understood, particularly regarding the convergence behavior without imposing strong assumptions. Additionally, although saddle point problems inherently exhibit symmetry between primal and dual variables, the CPHY does not fully exploit this symmetry, as only one variable is updated using an extrapolated step. In this work, we propose a symmetric version of the CPHY by incorporating both symmetry and extrapolation techniques. The resulting algorithm guarantees convergence for $$\theta \in (-1,1)$$ and ensures symmetric updates for both primal and dual variables. Numerical experiments on LASSO, TV image inpainting, and graph cuts demonstrate the algorithm’s improved efficiency.},
  archive      = {J_COAP},
  author       = {Ma, Feng and Li, Si and Zhang, Xiayang},
  doi          = {10.1007/s10589-025-00671-2},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A symmetric version of the generalized chambolle-pock-he-yuan method for saddle point problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
