<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDAR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijdar---10">IJDAR - 10</h2>
<ul>
<li><details>
<summary>
(2025). Deep learning approaches for information extraction from
visually rich documents: Datasets, challenges and methods.
<em>IJDAR</em>, <em>28</em>(1), 121–142. (<a
href="https://doi.org/10.1007/s10032-024-00493-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on Information Extraction from Visually Rich Documents, exploring how deep learning methods are applied in this field. For the purpose of comparing the performance of available resources, including datasets and methods, we first investigate an overview of the existing datasets. Then, we categorize and review published methods, highlighting their strengths and weaknesses in addressing key challenges like text recognition, layout analysis, and information fusion. This survey serves as a valuable resource for researchers and practitioners seeking to advance the field of information extraction (IE) from visually rich documents (VRD) and contribute to its real-world applications.},
  archive      = {J_IJDAR},
  author       = {Gbada, Hamza and Kalti, Karim and Mahjoub, Mohamed Ali},
  doi          = {10.1007/s10032-024-00493-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {121-142},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Deep learning approaches for information extraction from visually rich documents: Datasets, challenges and methods},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based modified-EAST scene text detector:
Insights from a novel multiscript dataset. <em>IJDAR</em>,
<em>28</em>(1), 97–119. (<a
href="https://doi.org/10.1007/s10032-024-00491-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of computer vision has seen significant transformation with the emergence and advancement of deep learning models. Deep learning waves have a significant impact on scene text detection, a vital and active area in computer vision. Numerous scientific, industrial, and academic procedures make use of text analysis. Natural scene text detection is more difficult than document image text detection owing to variations in font, size, style, brightness, etc. The National Institute of Technology Jalandhar-Text Detection dataset (NITJ-TD) is a new dataset that we have put forward in this study for various text analysis tasks including text detection, text segmentation, script identification, text recognition, etc. a deep learning model that seeks to identify the text’s location within the image,which are gathered in an unrestricted setting. The system consists of an NMS to choose the best match and prevent repeated predictions, and a modified EAST to pinpoint the exact ROI in the image. To improve the model’s performance, an enhancement module is added to the fundamental Efficient and Accurate Scene Text detector (EAST). The suggested approach is contrasted in terms of text word detection in the image. Several pre-trained models are used to assign the text word to various intersections over Union (IoU) values. We made use of our NITJ-TD dataset, which is made up of 1500 photos that were gathered from various North Indian sites. Punjabi, English, and Hindi scripts can be seen on the images. We also examined the outcomes of the ICDAR-2013 benchmark dataset. On both the suggested dataset and the benchmarked dataset, our approach performed better.},
  archive      = {J_IJDAR},
  author       = {Mahajan, Shilpa and Rani, Rajneesh and Kamboj, Aman},
  doi          = {10.1007/s10032-024-00491-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {97-119},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Deep learning-based modified-EAST scene text detector: Insights from a novel multiscript dataset},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A 3D scanning based image processing technique for measuring
the sequence of intersecting lines. <em>IJDAR</em>, <em>28</em>(1),
85–96. (<a href="https://doi.org/10.1007/s10032-024-00495-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of the sequence of two intersecting lines is still an ongoing and important issue in questioned document examination. In literature, scanning electron microscopy, optical profilometry, laser profilometry, and video reflectance spectroscopy were used for analyzing the intersecting lines. In this study, a cheap and easy-to-operate 3D scanner was used to study this problem. Intersections of homogenous and heterogeneous strokes were drawn using different brands of 1.0 mm blue, black, and red ballpoint pens on conventional printing paper, i.e., 80 g/m2, and uncoated A4 white paper by two different handwriting examiners. The analysis of the 3D surface of the crossing lines shows that the bottom of the profile of the first line has a considerable trough, while the second line has comparably smaller fluctuations. The 3D scan analysis is not affected by the brands and colors of the pen and is independent of the substrate and the number of sheets lying underneath the paper. The effect of the pen pressure shows that if the pressure of the second line is less than the first one, the sequence determination becomes harder and sometimes impossible. As these cases can be eliminated before the 3D scan, the developed method is sensitive, cheap, and easy to operate.},
  archive      = {J_IJDAR},
  author       = {Asicioglu, Faruk and Gelir, Ali and Yilmaz, Aysegul Sen and De Kinder, Jan and Kadi, Omer F. and Ozdemir, Onur B. and Pekacar, Ilgim and Sasun, Ugur and Ciftci, Saltuk B. and Dayioglu, Nurten},
  doi          = {10.1007/s10032-024-00495-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {85-96},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A 3D scanning based image processing technique for measuring the sequence of intersecting lines},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards fully automated processing and analysis of
construction diagrams: AI-powered symbol detection. <em>IJDAR</em>,
<em>28</em>(1), 71–84. (<a
href="https://doi.org/10.1007/s10032-024-00492-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction drawings are frequently stored in undigitised formats and consequently, their analysis requires substantial manual effort. This is true for many crucial tasks, including material takeoff where the purpose is to obtain a list of the equipment and respective amounts required for a project. Engineering drawing digitisation has recently attracted increased attention, however construction drawings have received considerably less interest compared to other types. To address these issues, this paper presents a novel framework for the automatic processing of construction drawings. Extensive experiments were performed using two state-of-the-art deep learning models for object detection in challenging high-resolution drawings sourced from industry. The results show a significant reduction in the time required for drawing analysis. Promising performance was achieved for symbol detection across various classes, with a mean average precision of 79% for the YOLO-based method and 83% for the Faster R-CNN-based method. This framework enables the digital transformation of construction drawings, improving tasks such as material takeoff and many others.},
  archive      = {J_IJDAR},
  author       = {Jamieson, Laura and Moreno-Garcia, Carlos Francisco and Elyan, Eyad},
  doi          = {10.1007/s10032-024-00492-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {71-84},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Towards fully automated processing and analysis of construction diagrams: AI-powered symbol detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN-based text line segmentation method for challenging
handwritten documents. <em>IJDAR</em>, <em>28</em>(1), 59–69. (<a
href="https://doi.org/10.1007/s10032-024-00488-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text line segmentation (TLS) is an essential step of the end-to-end document analysis systems. The main purpose of this step is to extract the individual text lines of any handwritten documents with high accuracy. Handwritten and historical documents mostly contain touching and overlapping characters, heavy diacritics, footnotes and side notes added over the years. In this work, we present a new TLS method based on generative adversarial networks (GAN). TLS problem is tackled as an image-to-image translation problem and the GAN model was trained to learn the spatial information between the individual text lines and their corresponding masks including the text lines. To evaluate the segmentation performance of the proposed GAN model, two challenging datasets, VML-AHTE and VML-MOC, were used. According to the qualitative and quantitative results, the proposed GAN model achieved the best segmentation accuracy on the VML-MOC dataset and showed competitive performance on the VML-AHTE dataset.},
  archive      = {J_IJDAR},
  author       = {Özşeker, İbrahim and Demir, Ali Alper and Özkaya, Ufuk},
  doi          = {10.1007/s10032-024-00488-5},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {59-69},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {GAN-based text line segmentation method for challenging handwritten documents},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image quality determination of palm leaf heritage documents
using integrated discrete cosine transform features with vision
transformer. <em>IJDAR</em>, <em>28</em>(1), 41–57. (<a
href="https://doi.org/10.1007/s10032-024-00490-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of Palm leaf images into various quality categories is an important step towards the digitization of these heritage documents. Manual inspection and categorization is not only laborious, time-consuming and costly but also subject to inspector’s biases and errors. This study aims to automate the classification of palm leaf document images into three different visual quality categories. A comparative analysis between various structural and statistical features and classifiers against deep neural networks is performed. VGG16, VGG19 and ResNet152v2 architectures along with a custom CNN model are used, while Discrete Cosine Transform (DCT), Grey Level Co-occurrence Matrix (GLCM), Tamura, and Histogram of Gradient (HOG) are chosen from the traditional methods. Based on these extracted features, various classifiers, namely, k-Nearest Neighbors (k-NN), multi-layer perceptron (MLP), Support Vector Machines (SVM), Decision Tree (DT) and Logistic Regression (LR) are trained and evaluated. Accuracy, precision, recall, and F1 scores are used as performance metrics for the evaluation of various algorithms. Results demonstrate that CNN embeddings and DCT features have emerged as superior features. Based on these findings, we integrated DCT with a Vision Transformer (ViT) for the document classification task. The result illustrates that this incorporation of DCT with ViT outperforms all other methods with 96% train F1 score and a test F1 score of 90%.},
  archive      = {J_IJDAR},
  author       = {Sivan, Remya and Pati, Peeta Basa and Kesiman, Made Windu Antara},
  doi          = {10.1007/s10032-024-00490-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {41-57},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Image quality determination of palm leaf heritage documents using integrated discrete cosine transform features with vision transformer},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene text recognition: An indic perspective.
<em>IJDAR</em>, <em>28</em>(1), 31–40. (<a
href="https://doi.org/10.1007/s10032-024-00489-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring Scene Text Recognition (STR) in Indian languages is an important research domain due to its wide applications. This paper proposes a spatial attention-based model (LaSA-Net) that combines visual features and language knowledge for word recognition from scene image word segments. We augment the classical cross-entropy loss with a novel language-attunement loss that enables the model to learn valid and prevalent character sequences in the word. This enhances the model’s ability to perform zero-shot word recognition. Further, to compensate for the lack of rotational invariance in CNN based feature extraction backbone, we propose a training data augmentation strategy involving the creation of glyphs: images of individual characters of different orientations. This improves LaSA-Net’s ability to recognize words in images with curved/vertically aligned text, alleviating the need for computationally expensive preprocessing modules. Our experiments with Tamil, Malayalam, and Telugu scripts on the IIIT-ILST datasets have achieved new benchmark results and outperformed other state-of-the-art STR models.},
  archive      = {J_IJDAR},
  author       = {Vijayan, Vasanthan P. and Chanda, Sukalpa and Doermann, David and Krishnan, Narayanan C.},
  doi          = {10.1007/s10032-024-00489-4},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {31-40},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Scene text recognition: An indic perspective},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic floor plan analysis using a boundary
attention-based deep network. <em>IJDAR</em>, <em>28</em>(1), 19–30. (<a
href="https://doi.org/10.1007/s10032-024-00487-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Floor plan is an important communication tool between architects, construction engineers, and clients for a building project. Estimation of building features from a floor plan image is often a time-consuming task. Automatic analysis of floor plan images can significantly improve work efficiency and accuracy. A few research works have been reported in the literature on automated floor image analysis. However, the scope and performance of the existing techniques are limited. In this paper, a CNN-based technique, referred to as FloorNet, is proposed for the multiclass semantic segmentation of a floor plan. The proposed FloorNet has five modules: Encoder, Room type decoder, Room boundary decoder, Multiscale room boundary attention model and Floor classification. The proposed technique is evaluated using simple brochure type and complex architectural type floor plan images. Experimental results show that the proposed technique provides an improvement of 5–11% mIoU for semantic segmentation (for 9–11 classes) compared to the state-of-the-art techniques.},
  archive      = {J_IJDAR},
  author       = {Xu, Zhongguo and Yang, Cheng and Alheejawi, Salah and Jha, Naresh and Mehadi, Syed and Mandal, Mrinal},
  doi          = {10.1007/s10032-024-00487-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {19-30},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Automatic floor plan analysis using a boundary attention-based deep network},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handwritten stenography recognition and the LION dataset.
<em>IJDAR</em>, <em>28</em>(1), 3–18. (<a
href="https://doi.org/10.1007/s10032-024-00479-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish the first baseline for handwritten stenography recognition, using the novel LION dataset, and investigate the impact of including selected aspects of stenographic theory into the recognition process. We make the LION dataset publicly available with the aim of encouraging future research in handwritten stenography recognition. A state-of-the-art text recognition model is trained to establish a baseline. Stenographic domain knowledge is integrated by transforming the target sequences into representations which approximate diplomatic transcriptions, wherein each symbol in the script is represented by its own character in the transliteration, as opposed to corresponding combinations of characters from the Swedish alphabet. Four such encoding schemes are evaluated and results are further improved by integrating a pre-training scheme, based on synthetic data. The baseline model achieves an average test character error rate (CER) of 29.81% and a word error rate (WER) of 55.14%. Test error rates are reduced significantly (p&lt; 0.01) by combining stenography-specific target sequence encodings with pre-training and fine-tuning, yielding CERs in the range of 24.5–26% and WERs of 44.8–48.2%. An analysis of selected recognition errors illustrates the challenges that the stenographic writing system poses to text recognition. This work establishes the first baseline for handwritten stenography recognition. Our proposed combination of integrating stenography-specific knowledge, in conjunction with pre-training and fine-tuning on synthetic data, yields considerable improvements. Together with our precursor study on the subject, this is the first work to apply modern handwritten text recognition to stenography. The dataset and our code are publicly available via Zenodo.},
  archive      = {J_IJDAR},
  author       = {Heil, Raphaela and Nauwerck, Malin},
  doi          = {10.1007/s10032-024-00479-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {3-18},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Handwritten stenography recognition and the LION dataset},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). International journal on document analysis and recognition
editorial leadership change. <em>IJDAR</em>, <em>28</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s10032-025-00520-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDAR},
  author       = {Lopresti, Daniel and Kise, Koichi and Marinai, Simone},
  doi          = {10.1007/s10032-025-00520-2},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {International journal on document analysis and recognition editorial leadership change},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
