<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ml---14">ML - 14</h2>
<ul>
<li><details>
<summary>
(2025). Minimum discrepancy principle strategy for choosing k in
k-NN regression. <em>ML</em>, <em>114</em>(5), 1–33. (<a
href="https://doi.org/10.1007/s10994-024-06645-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel data-driven strategy to choose the hyperparameter k in the k-NN regression estimator without using any hold-out data. We treat the problem of choosing the hyperparameter as an iterative procedure (over k) and propose using an easily implemented in practice strategy based on the idea of early stopping and the minimum discrepancy principle. This model selection strategy is proven to be minimax-optimal, under the fixed-design assumption on covariates, over some smoothness function classes, for instance, the Lipschitz functions class on a bounded domain. The novel method often improves statistical performance on artificial and real-world data sets in comparison to other model selection strategies, such as the Hold-out method, 5–fold cross-validation, and AIC criterion. The novelty of the strategy comes from reducing the computational time of the model selection procedure while preserving the statistical (minimax) optimality of the resulting estimator. More precisely, given a sample of size n, if one should choose k among $$\left\{ 1, \ldots , n \right\}$$ , and $$\left\{ f^1, \ldots , f^n \right\}$$ are the estimators of the regression function, the minimum discrepancy principle requires calculation of a fraction of the estimators, while this is not the case for the generalized cross-validation, Akaike’s AIC criteria or Lepskii principle.},
  archive      = {J_ML},
  author       = {Averyanov, Yaroslav and Celisse, Alain},
  doi          = {10.1007/s10994-024-06645-5},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Minimum discrepancy principle strategy for choosing k in k-NN regression},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theoretical guarantees for domain adaptation with
hierarchical optimal transport. <em>ML</em>, <em>114</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10994-025-06749-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation arises as an important problem in statistical learning theory, arising when the data-generating processes differ between the training and test samples, respectively called source and target domains. Recent theoretical advances have demonstrated that the success of domain adaptation algorithms heavily relies on their ability to minimize the divergence between the probability distributions of the source and target domains. However, minimizing this divergence cannot be achieved independently of other key ingredients, such as the source risk or the combined error of the ideal joint hypothesis. The trade-off between these terms is often ensured through algorithmic solutions that remain implicit and are not directly reflected by the theoretical guarantees. To get to the bottom of this issue, we propose in this paper a new theoretical framework for domain adaptation through hierarchical optimal transport. This framework provides more explicit generalization bounds and enables us to consider the natural hierarchical organization of samples in both domains into structures, i.e. classes or clusters. Additionally, we provide a new divergence measure between the source and target domains called Hierarchical Wasserstein distance that indicates under mild assumptions, which structures need to be aligned to achieve successful adaptation.},
  archive      = {J_ML},
  author       = {El Hamri, Mourad and Bennani, Younès and Falih, Issam},
  doi          = {10.1007/s10994-025-06749-6},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Theoretical guarantees for domain adaptation with hierarchical optimal transport},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic instance dependent label refinement for noisy
label learning. <em>ML</em>, <em>114</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s10994-024-06668-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label refinement methods are designed to improve the quality of training labels by incorporating model predictions into the original training labels. By adjusting the combination coefficient of the noisy label, the impact of noise is reduced, which in turn makes the training process more robust. However, previous label refinement methods are unable to model instance-dependent noise, which is the most realistic type of noise. To address this limitation, we propose a simple approach, probabilistic instance-dependent label refinement (referred to as $$\pi$$ -LR). Inspired by the fact that humans are more likely to make mistakes when annotating confusing instances, we propose to estimate the probability of whether a sample is confusing, which can be useful for modeling noise generation. Our approach exploits this concept by assigning a confusing probability $$\eta _i$$ to each instance $$\varvec{x}_i$$ from a probabilistic perspective. This provides a clear understanding of how instance-dependent noise affects true labels. Empirical evaluations show that $$\pi$$ -LR improves the robustness of the model in the presence of label noise and outperforms all compared methods on both realistic and synthetic label noise, while maintaining high efficiency in time and space.},
  archive      = {J_ML},
  author       = {He, Hao-Yuan and Liu, Yu and Liu, Ren-Biao and Xie, Zheng and Li, Ming},
  doi          = {10.1007/s10994-024-06668-y},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Probabilistic instance dependent label refinement for noisy label learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gender disentangled representation learning in neural
rankers. <em>ML</em>, <em>114</em>(5), 1–33. (<a
href="https://doi.org/10.1007/s10994-024-06664-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated that while neural ranking methods excel in retrieval effectiveness, they also tend to amplify stereotypical biases, especially those related to gender. Current mitigation strategies often focus on adjusting training methods, like adversarial techniques or data balancing, but typically overlook explicit consideration of gender as an attribute. In this paper, we introduce a systematic approach that treats gender as a distinct component within neural ranker representations. Our neural disentanglement method separates content semantics from gender information, enabling the neural ranker to evaluate document relevance based on content alone, without the interference of gender-related information during retrieval. Our extensive experiments demonstrate that: (1) our disentanglement approach matches the effectiveness of baseline models and offers more consistent performance across queries of different gender affiliations; (2) isolating gender within the representations allows the neural ranker to produce an unbiased list of documents, not favoring any specific gender; and (3) the disentangled gender component effectively and concisely captures gender information independently from the semantic content.},
  archive      = {J_ML},
  author       = {Seyedsalehi, Shirin and Salamat, Sara and Arabzadeh, Negar and Ebrahimi, Sajad and Zihayat, Morteza and Bagheri, Ebrahim},
  doi          = {10.1007/s10994-024-06664-2},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Gender disentangled representation learning in neural rankers},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Conformal load prediction with transductive
graph autoencoders. <em>ML</em>, <em>114</em>(5), 1. (<a
href="https://doi.org/10.1007/s10994-025-06762-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Luo, Rui and Colombo, Nicolo},
  doi          = {10.1007/s10994-025-06762-9},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Conformal load prediction with transductive graph autoencoders},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced route planning with calibrated uncertainty set.
<em>ML</em>, <em>114</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s10994-024-06697-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the application of probabilistic prediction methodologies in route planning within a road network context. Specifically, we introduce the Conformalized Quantile Regression for Graph Autoencoders (CQR-GAE), which leverages the conformal prediction technique to offer a coverage guarantee, thus improving the reliability and robustness of our predictions. By incorporating uncertainty sets derived from CQR-GAE, we substantially improve the decision-making process in route planning under a robust optimization framework. We demonstrate the effectiveness of our approach by applying the CQR-GAE model to a real-world traffic scenario. The results indicate that our model significantly outperforms baseline methods, offering a promising avenue for advancing intelligent transportation systems.},
  archive      = {J_ML},
  author       = {Tang, Lingxuan and Luo, Rui and Zhou, Zhixin and Colombo, Nicolo},
  doi          = {10.1007/s10994-024-06697-7},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Learn.},
  title        = {Enhanced route planning with calibrated uncertainty set},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online dimensionality reduction through stacked
generalization of spectral methods with deep networks. <em>ML</em>,
<em>114</em>(5), 1–40. (<a
href="https://doi.org/10.1007/s10994-024-06715-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing large volumes of high-dimensional data poses significant challenges. Dimensionality reduction aims to reveal the most prominent properties of data by embedding them into a low-dimensional representation. Spectral dimensionality reduction methods using kernel matrices have been proven to yield optimal results. Online versions of those methods are desirable to incrementally project new data without recomputing the whole embedding from the complete dataset. In addition, integrating different spectral methods may have a synergistic effect. This paper presents an online dimensionality reduction method based on deep neural networks that integrates embeddings optimized by statistical approximation of neighborhoods and induced by different spectral methods through stacking ensemble learning. In particular, the proposed method first applies a self-supervised stage in order to train a set of deep encoders based on the embeddings induced by different spectral methods applied to a given input dataset. Those basis encoders are optimized and then integrated through a metamodel constituted by a fully connected network. A supervised and an unsupervised approach have been designed depending on whether the final aim is to enforce topological preservation or cluster induction. The proposed method has been experimentally validated on well-known image datasets and compared to some of the most relevant dimensionality reduction techniques by using widely-used quality measures.},
  archive      = {J_ML},
  author       = {Alvarado-Pérez, Juan Carlos and Garcia, Miguel Angel and Puig, Domènec},
  doi          = {10.1007/s10994-024-06715-8},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {Online dimensionality reduction through stacked generalization of spectral methods with deep networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural RELAGGS. <em>ML</em>, <em>114</em>(5), 1–26. (<a
href="https://doi.org/10.1007/s10994-025-06753-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-relational databases are the basis of most consolidated data collections in science and industry today. Most learning and mining algorithms, however, require data to be represented in a propositional form. While there is a variety of specialized machine learning algorithms that can operate directly on multi-relational data sets, propositionalization algorithms transform multi-relational databases into propositional data sets, thereby allowing the application of traditional machine learning and data mining algorithms without their modification. One prominent propositionalization algorithm is RELAGGS by Krogel and Wrobel, which transforms the data by nested aggregations. We propose a new neural network based algorithm in the spirit of RELAGGS that employs trainable composite aggregate functions instead of the static aggregate functions used in the original approach. In this way, we can jointly train the propositionalization with the prediction model, or, alternatively, use the learned aggegrations as embeddings in other algorithms. We demonstrate the increased predictive performance by comparing N-RELAGGS with RELAGGS and multiple other state-of-the-art algorithms.},
  archive      = {J_ML},
  author       = {Pensel, Lukas and Kramer, Stefan},
  doi          = {10.1007/s10994-025-06753-w},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Neural RELAGGS},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DatRel: A noise-tolerant data relocation approach for
effective synthetic data generation in imbalanced classifiers.
<em>ML</em>, <em>114</em>(5), 1–45. (<a
href="https://doi.org/10.1007/s10994-025-06755-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most machine learning algorithms tend to bias towards the majority class when a dataset exhibits a skewed distribution in the class variable. This is called the class imbalance problem and is frequently encountered in real-life applications. One of the most prevalent methods for addressing class imbalance is data resampling, which generates or removes samples to balance the dataset. A well-known issue with oversampling is noise generation. Noise removal or hybrid resampling is used to deal with noise. However, these methods cause imbalance to re-emerge. In this study, a data relocation approach named DatRel is proposed to address the noise generation problem of oversampling without causing imbalance. The proposed approach utilizes pure and proper class cover catch digraphs (P-CCCD) to determine dominant points and cover areas for minority class. Then, new samples from oversampling are drawn to the dominant points until they are covered. This process ensures that newly generated samples never overlap with a negative sample. Imbalance is not affected since no sample is removed by undersampling. The proposed DatRel approach is applied to commonly used oversampling methods, namely SMOTE, ADASYN, and BLSMOTE. Moreover, the performance of the DatRel approach is compared to noise filtering methods such as Tomeklink, ENN, NEATER, and NearMiss after SMOTE. Several baseline classification algorithms are employed, and comparisons are made using various metrics. Results using 49 imbalanced datasets show that DatRel improves classifier performance in oversampling methods and demonstrates its value in comparison to other noise removal techniques according to AUC, BACC, F1, GMEAN, and MCC.},
  archive      = {J_ML},
  author       = {Sağlam, Fatih},
  doi          = {10.1007/s10994-025-06755-8},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-45},
  shortjournal = {Mach. Learn.},
  title        = {DatRel: A noise-tolerant data relocation approach for effective synthetic data generation in imbalanced classifiers},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal ensemble of multiple patterns’ instances for
continuous prediction of events. <em>ML</em>, <em>114</em>(5), 1–42. (<a
href="https://doi.org/10.1007/s10994-025-06756-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-life data of various domains, such as traffic, meteorology, or healthcare data, events may have varying durations. Moreover, heterogeneous multivariate temporal data may consist of varying samplings, including regular sampling in different frequencies or irregular, as well as events data of different types, having fixed or varying duration. We propose to uniformly represent heterogeneous multivariate temporal data using symbolic time-intervals, from which a model that predicts an occurrence of events early can be learned. We introduce a novel use of time-interval-related patterns (TIRPs), in which patterns that end with an event of interest can be used to continuously estimate the event’s occurrence probability in real-time. Recently, we introduced a model that allows continuous prediction of the completion of a pattern, which is extended in this work, to also predict the expected completion time. This work focuses on predicting the probability and time occurrence of an event based on multiple different instances of patterns that end with the event, for which we propose and evaluate aggregation functions. A rigorous evaluation was conducted on four real-life datasets to assess the effectiveness of the proposed model and the aggregation functions. The proposed model performed better than the baseline models (ResNet, LSTM-FCN, ROCKET, and XGBoost) for all datasets.},
  archive      = {J_ML},
  author       = {Itzhak, Nevo and Jaroszewicz, Szymon and Moskovitch, Robert},
  doi          = {10.1007/s10994-025-06756-7},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-42},
  shortjournal = {Mach. Learn.},
  title        = {Temporal ensemble of multiple patterns’ instances for continuous prediction of events},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive optimization for prediction with missing data.
<em>ML</em>, <em>114</em>(5), 1–37. (<a
href="https://doi.org/10.1007/s10994-025-06757-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2–10% improvement in out-of-sample accuracy.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Delarue, Arthur and Pauphilet, Jean},
  doi          = {10.1007/s10994-025-06757-6},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive optimization for prediction with missing data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An unsupervised adversarial domain adaptation based on
variational auto-encoder. <em>ML</em>, <em>114</em>(5), 1–26. (<a
href="https://doi.org/10.1007/s10994-025-06760-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting a large amount of labeled data in machine learning is always challenging. Often, even with sufficient data, domain differences can cause a shift or bias in data distribution, affecting model performance during testing. Domain adaptation methods, especially adversarial techniques, are effective solutions for these challenges. The goal is to learn a classifier for an unlabeled target dataset using a labeled source dataset, enhancing resistance to domain shifts. However, existing methods sometimes struggle with adapting the joint feature distribution across domains, resulting in negative transfer. To address this, we propose a method that forms class-specific clusters to prevent negative transfer. This method is encapsulated in an unsupervised adversarial domain adaptation framework based on a variational auto-encoder. Our structure is designed to enhance invariant and discriminative feature representation. We process source and target data through a VAE to establish a smooth latent representation. In our method, source and target data are fed into a variational auto-encoder, which produces a smooth latent representation. The feature extractor then plays an adversarial minimax game with the discriminator to learn domain-invariant features, while the feature extractor is shared between the reconstructed source and reconstructed target data. In addition, we proposed a second structure in which the domain discriminator part of the prior structure is eliminated to demonstrate the influence of the variational auto-encoder in domain adaptation. On numerous unsupervised domain adaptation benchmarks, our results indicate that our proposed model outperforms or is comparable to state-of-the-art outcomes.},
  archive      = {J_ML},
  author       = {Hassan Pour Zonoozi, Mahta and Seydi, Vahid and Deypir, Mahmood},
  doi          = {10.1007/s10994-025-06760-x},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {An unsupervised adversarial domain adaptation based on variational auto-encoder},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Nettop: A lightweight-network of
orthogonal-plane features for image recognition. <em>ML</em>,
<em>114</em>(5), 1. (<a
href="https://doi.org/10.1007/s10994-025-06765-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10994-025-06765-6},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: nettop: a lightweight-network of orthogonal-plane features for image recognition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPAT: Cross-patch aggregated transformer for time series
forecasting. <em>ML</em>, <em>114</em>(5), 1–31. (<a
href="https://doi.org/10.1007/s10994-025-06758-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting utilizes historical data to forecast future information over a specific period. It aims to predict forthcoming developmental trends through meticulous statistical analysis and modeling of historical data, addressing real-life challenges like power load prediction, traffic condition prognostication, and extreme weather warnings. Currently, Transformer-based models for time series prediction normally segment the original time series into multiple patches. While this modeling methodology has demonstrated superiority in improving performance, the approach of patch partition based on a fixed length constrains the model’s predictive accuracy when dealing with time series forecasting tasks of varying lengths. To overcome the limitation, this article proposes an innovative Cross-Patch Aggregated Transformer (CPAT), which introduces the Patch Reconstruction module to restructure patches between encoder layers, facilitating cross-patch connections and information interaction. This empowers the model to focus on the correlation among adjacent patches, acquiring effective representations of both global and local features. Consequently, the modeling of time dependency becomes more precise. Extensive experiments conducted on eight publicly available benchmark datasets in real-world scenarios showcase that the proposed CPAT model attains state-of-the-art (SOTA) accuracy overall compared to existing baseline models. Notably, it achieves relative improvement rates of 5.46% and 2.56% for Mean Square Error (MSE) and Mean Absolute Error (MAE), respectively, augmenting the predictive capabilities of Transformer family models in time series tasks.},
  archive      = {J_ML},
  author       = {Liu, Bingyan and Wu, Li and Wang, Xiaoying and Huang, Jianqiang and Zhang, Guojing},
  doi          = {10.1007/s10994-025-06758-5},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {CPAT: Cross-patch aggregated transformer for time series forecasting},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
