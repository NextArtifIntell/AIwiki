<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="air---32">AIR - 32</h2>
<ul>
<li><details>
<summary>
(2025). Learn to optimise for job shop scheduling: A survey with
comparison between genetic programming and reinforcement learning.
<em>AIR</em>, <em>58</em>(6), 1–53. (<a
href="https://doi.org/10.1007/s10462-024-11059-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Job shop scheduling holds significant importance due to its relevance and impact on various industrial and manufacturing processes. It involves dynamically assigning and sequencing jobs to machines in a flexible production environment, where job characteristics, machine availability, and other factors might change over time. Genetic programming and reinforcement learning have emerged as powerful approaches to automatically learn high-quality scheduling heuristics or directly optimise sequences of specific job-machine pairs to generate efficient schedules in manufacturing. Existing surveys on job shop scheduling typically provide overviews from a singular perspective, focusing solely on genetic programming or reinforcement learning, but overlook the hybridisation and comparison of both approaches. This survey aims to bridge this gap by reviewing recent developments in genetic programming and reinforcement learning approaches for job shop scheduling problems, providing a comparison in terms of the learning principles and characteristics for solving different kinds of job shop scheduling problems. In addition, this survey identifies and discusses current issues and challenges in the field of learning to optimise for job shop scheduling. This comprehensive exploration of genetic programming and reinforcement learning in job shop scheduling provides valuable insights into the learning principles for optimising different job shop scheduling problems. It deepens our understanding of recent developments, suggesting potential research directions for future advancements.},
  archive      = {J_AIR},
  author       = {Xu, Meng and Mei, Yi and Zhang, Fangfang and Zhang, Mengjie},
  doi          = {10.1007/s10462-024-11059-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Learn to optimise for job shop scheduling: A survey with comparison between genetic programming and reinforcement learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive gaining-sharing knowledge-based variant algorithm
with historical probability expansion and its application in escape
maneuver decision making. <em>AIR</em>, <em>58</em>(6), 1–43. (<a
href="https://doi.org/10.1007/s10462-024-11096-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To further improve the performance of adaptive gaining-sharing knowledge-based algorithm (AGSK), a novel adaptive gaining sharing knowledge-based algorithm with historical probability expansion (HPE-AGSK) is proposed by modifying the search strategies. Based on AGSK, three improvement strategies are proposed. First, expansion sharing strategy is proposed and added in junior gaining-sharing phase to boost local search ability. Second, historical probability expansion strategy is proposed and added in senior gaining-sharing phase to strengthen global search ability. Last, reverse gaining strategy is proposed and utilized to expand population distribution at the beginning of iterations. The performance of HPE-AGSK is initially evaluated using IEEE CEC 2021 test suite, compared with fifteen state-of-the-art algorithms (AGSK, APGSK, APGSK-IMODE, GLAGSK, EDA2, AAVS-EDA, EBOwithCMAR, LSHADE-SPACMA, HSES, IMODE, MadDE, CJADE, and iLSHADE-RSP). The results demonstrate that HPE-AGSK outperforms both state-of-the-art GSK-based variants and past winners of IEEE CEC competitions. Subsequently, GSK-based variants and other exceptional algorithms in CEC 2021 are selected to further evaluate the performance of HPE-AGSK using IEEE CEC 2018 test suite. The statistical results show that HPE-AGSK has superior exploration ability than the comparison algorithms, and has strong competition with APGSK (state-of-the-art AGSK variant) and IMODE (CEC 2020 Winner) in exploitation ability. Finally, HPE-AGSK is utilized to solve the beyond visual range escape maneuver decision making problem. Its success rate is 100%, and mean maneuver time is 9.10 s, these results show that HPE-AGSK has good BVR escape maneuver decision-making performance. In conclusion, HPE-AGSK is a highly promising AGSK variant that significantly enhances the performance, and is an outstanding development of AGSK. The code of HPE-AGSK can be downloaded from https://github.com/xieleilei0305/HPE-AGSK-CODE.git . (The link will be available for readers after the paper is published).},
  archive      = {J_AIR},
  author       = {Xie, Lei and Wang, Yuan and Tang, Shangqin and Li, Yintong and Zhang, Zhuoran and Huang, Changqiang},
  doi          = {10.1007/s10462-024-11096-4},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Adaptive gaining-sharing knowledge-based variant algorithm with historical probability expansion and its application in escape maneuver decision making},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environmental sound recognition on embedded devices using
deep learning: A review. <em>AIR</em>, <em>58</em>(6), 1–35. (<a
href="https://doi.org/10.1007/s10462-025-11106-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound recognition has a wide range of applications beyond speech and music, including environmental monitoring, sound source classification, mechanical fault diagnosis, audio fingerprinting, and event detection. These applications often require real-time data processing, making them well-suited for embedded systems. However, embedded devices face significant challenges due to limited computational power, memory, and low power consumption. Despite these constraints, achieving high performance in environmental sound recognition typically requires complex algorithms. Deep Learning models have demonstrated high accuracy on existing datasets, making them a popular choice for such tasks. However, these models are resource-intensive, posing challenges for real-time edge applications. This paper presents a comprehensive review of integrating Deep Learning models into embedded systems, examining their state-of-the-art applications, key components, and steps involved. It also explores strategies to optimise performance in resource-constrained environments through a comparison of various implementation approaches such as knowledge distillation, pruning, and quantization, with studies achieving a reduction in complexity of up to 97% compared to the unoptimized model. Overall, we conclude that in spite of the availability of lightweight deep learning models, input features, and compression techniques, their integration into low-resource devices, such as microcontrollers, remains limited. Furthermore, more complex tasks, such as general sound classification, especially with expanded frequency bands and real-time operation have yet to be effectively implemented on these devices. These findings highlight the need for a standardised research framework to evaluate these technologies applied to resource-constrained devices, and for further development to realise the wide range of potential applications.},
  archive      = {J_AIR},
  author       = {Gairí, Pau and Pallejà, Tomàs and Tresanchez, Marcel},
  doi          = {10.1007/s10462-025-11106-z},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Environmental sound recognition on embedded devices using deep learning: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-model integration for dynamic forecasting (MIDF): A
framework for wind speed and direction prediction. <em>AIR</em>,
<em>58</em>(6), 1–37. (<a
href="https://doi.org/10.1007/s10462-025-11140-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of wind speed and direction is critical for the efficient integration of wind power into energy systems, ensuring reliable renewable energy production and grid stability. Traditional methods often struggle with capturing nonlinear interdependencies, quantifying uncertainties, and providing reliable long-term predictions, particularly in complex atmospheric conditions. To address these challenges, this study introduces multi-model Integration for dynamic forecasting (MIDF), an ensemble machine learning framework that combines the strengths of DeepAR and temporal fusion transformer (TFT) models through a two-step meta-learning process. MIDF leverages DeepAR’s probabilistic forecasting capabilities and TFT’s attention mechanisms to enhance accuracy, robustness, and interpretability. Using a custom meteorological dataset spanning January 2010 to May 2023, the model was evaluated against standalone alternatives across multiple metrics, including MSE, RMSE, and R2. MIDF achieved superior performance, with MSE, RMSE, and R2 values of 0.0035, 0.01913, and 0.89 for wind speed, and 0.00052, 0.02507, and 0.86 for wind direction, significantly reducing errors compared to existing methods. These results underscore the potential of ensemble learning in advancing wind forecasting accuracy, enabling more reliable renewable energy management, operational planning, and risk mitigation in meteorological applications.},
  archive      = {J_AIR},
  author       = {Maruthi, Molaka and Kim, Bubryur and Sujeen, Song and An, Jinwoo and Chen, Zengshun},
  doi          = {10.1007/s10462-025-11140-x},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-model integration for dynamic forecasting (MIDF): A framework for wind speed and direction prediction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of small object detection based on deep learning in
aerial images. <em>AIR</em>, <em>58</em>(6), 1–67. (<a
href="https://doi.org/10.1007/s10462-025-11150-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection poses a formidable challenge in the field of computer vision, particularly when it comes to analyzing aerial remote sensing images. Despite the rapid development of deep learning and significant progress in detection techniques in natural scenes, the migration of these algorithms to aerial images has not met expectations. This is primarily due to limitations in imaging acquisition conditions, including small target size, viewpoint specificity, background complexity, as well as scale and orientation diversity. Although the increasing application of deep learning-based algorithms to overcome these problems, few studies have summarized the optimization of different deep learning strategies used for small target detection in aerial images. Therefore, this paper aims to explore the application of deep learning methods for small object detection in aerial images. The primary challenges in small object detection in aerial images will be summarized. Next, a meticulous analysis and categorization of the prevailing deep learning optimization strategies employed to surmount the challenges encountered in aerial image detection is undertaken. Following that, we provide a comprehensive presentation of the object detection datasets utilized in aerial remote sensing images, along with the evaluation metrics employed. Additionally, we furnish experimental data pertaining to the currently proposed detection algorithms. Finally, the advantages and disadvantages of various optimization strategies and potential development trends are discussed. Hopefully, it can provide a reference for researchers in this field.},
  archive      = {J_AIR},
  author       = {Hua, Wei and Chen, Qili},
  doi          = {10.1007/s10462-025-11150-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-67},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of small object detection based on deep learning in aerial images},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BERT applications in natural language processing: A review.
<em>AIR</em>, <em>58</em>(6), 1–49. (<a
href="https://doi.org/10.1007/s10462-025-11162-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BERT (Bidirectional Encoder Representations from Transformers) has revolutionized Natural Language Processing (NLP) by significantly enhancing the capabilities of language models. This review study examines the complex nature of BERT, including its structure, utilization in different NLP tasks, and the further development of its design via modifications. The study thoroughly analyses the methodological aspects, conducting a comprehensive analysis of the planning process, the implemented procedures, and the criteria used to decide which data to include or exclude in the evaluation framework. In addition, the study thoroughly examines the influence of BERT on several NLP tasks, such as Sentence Boundary Detection, Tokenization, Grammatical Error Detection and Correction, Dependency Parsing, Named Entity Recognition, Part of Speech Tagging, Question Answering Systems, Machine Translation, Sentiment analysis, fake review detection and Cross-lingual transfer learning. The review study adds to the current literature by integrating ideas from multiple sources, explicitly emphasizing the problems and prospects in BERT-based models. The objective is to comprehensively comprehend BERT and its implementations, targeting both experienced researchers and novices in the domain of NLP. Consequently, the present study is expected to inspire more research endeavors, promote innovative adaptations of BERT, and deepen comprehension of its extensive capabilities in various NLP applications. The results presented in this research are anticipated to influence the advancement of future language models and add to the ongoing discourse on enhancing technology for understanding natural language.},
  archive      = {J_AIR},
  author       = {Gardazi, Nadia Mushtaq and Daud, Ali and Malik, Muhammad Kamran and Bukhari, Amal and Alsahfi, Tariq and Alshemaimri, Bader},
  doi          = {10.1007/s10462-025-11162-5},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {BERT applications in natural language processing: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GFSNet: Gaussian fourier with sparse attention network for
visual question answering. <em>AIR</em>, <em>58</em>(6), 1–30. (<a
href="https://doi.org/10.1007/s10462-025-11163-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA), a core task in multimodal learning, requires models to effectively integrate visual and natural language information to perform reasoning and semantic understanding in complex scenarios. However, self-attention mechanisms often struggle to capture multi-scale information and key region features within images comprehensively. Moreover, VQA involves multidimensional and deep reasoning about image content, particularly in scenarios involving spatial relationships and frequency-domain features. Existing methods face limitations in modeling multi-scale features and filtering irrelevant information effectively. This paper proposes an innovative Gaussian Fourier with Sparse Attention Network (GFSNet) to address these challenges. GFSNet leverages Fourier transforms to map image attention weights generated by the self-attention mechanism from the spatial domain to the frequency domain, enabling comprehensive modeling of multi-scale frequency information. This enhances the model’s adaptability to complex structures and its capacity for relational modeling. To further improve feature robustness, a Gaussian filter is introduced to suppress high-frequency noise in the frequency domain, preserving critical visual information. Additionally, a sparse attention mechanism dynamically selects optimized frequency-domain features, effectively reducing interference from redundant information while improving interpretability and computational efficiency. Without increasing parameter counts or computational complexity, GFSNet achieves efficient modeling of multi-scale visual information. Experimental results on benchmark VQA datasets (VQA v2, GQA, and CLEVR) demonstrate that GFSNet significantly enhances reasoning capabilities and cross-modal alignment performance, validating its superiority and effectiveness. The code is available at https://github.com/shenxiang-vqa/GFSNet .},
  archive      = {J_AIR},
  author       = {Shen, Xiang and Han, Dezhi and Chang, Chin-Chen and Oad, Ammar and Wu, Huafeng},
  doi          = {10.1007/s10462-025-11163-4},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {GFSNet: Gaussian fourier with sparse attention network for visual question answering},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent games meeting with multi-agent deep
reinforcement learning: A comprehensive review. <em>AIR</em>,
<em>58</em>(6), 1–53. (<a
href="https://doi.org/10.1007/s10462-025-11166-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the great achievement of the AI-driven intelligent games, such as AlphaStar defeating the human experts, and numerous intelligent games have come into the public view. Essentially, deep reinforcement learning (DRL), especially multiple-agent DRL (MADRL) has empowered a variety of artificial intelligence fields, including intelligent games. However, there is lack of systematical review on their correlations. This article provides a holistic picture on smoothly connecting intelligent games with MADRL from two perspectives: theoretical game concepts for MADRL, and MADRL for intelligent games. From the first perspective, information structure and game environmental features for MADRL algorithms are summarized; and from the second viewpoint, the challenges in intelligent games are investigated, and the existing MADRL solutions are correspondingly explored. Furthermore, the state-of-the-art (SOTA) MADRL algorithms for intelligent games are systematically categorized, especially from the perspective of credit assignment. Moreover, a comprehensively review on notorious benchmarks are conducted to facilitate the design and test of MADRL based intelligent games. Besides, a general procedure of MADRL simulations is offered. Finally, the key challenges in integrating intelligent games with MADRL, and potential future research directions are highlighted. This survey hopes to provide a thoughtful insight of developing intelligent games with the assistance of MADRL solutions and algorithms.},
  archive      = {J_AIR},
  author       = {Wang, Yiqin and Wang, Yufeng and Tian, Feng and Ma, Jianhua and Jin, Qun},
  doi          = {10.1007/s10462-025-11166-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Intelligent games meeting with multi-agent deep reinforcement learning: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning frameworks for MRI-based diagnosis of
neurological disorders: A systematic review and meta-analysis.
<em>AIR</em>, <em>58</em>(6), 1–37. (<a
href="https://doi.org/10.1007/s10462-025-11146-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic diagnosis of neurological disorders using Magnetic Resonance Imaging (MRI) is a widely researched problem. MRI is a non-invasive and highly informative imaging modality, which is one of the most widely accepted and used neuroimaging modalities for visualizing the human brain. The advent of tremendous processing capabilities, multi-modal data, and deep-learning techniques has enabled researchers to develop intelligent, sufficiently accurate classification methods. A comprehensive literature review has revealed extensive research on the automatic diagnosis of neurological disorders. However, despite numerous studies, a systematically developed framework is lacking, that relies on a sufficiently robust dataset or ensures reliable accuracy. To date, no consolidated framework has been established to classify multiple diseases and their subtypes effectively based on various types and their planes of orientation in structural and functional MR images. This systematic review provides a detailed and comprehensive analysis of research reported from 2000 to 2023. Systems developed in prior art have been categorized according to their disease diagnosis capabilities. The datasets employed and the tools developed are also summarized to assist researchers to conduct further studies in this crucial domain. The contributions of this research include facilitating the design of a unified framework for multiple neurological disease diagnoses, resulting in the development of a generic assistive tool for hospitals and neurologists to diagnose disorders precisely and swiftly thus potentially saving lives, in addition to increasing the quality of life of patients suffering from neurodegenerative disorders.},
  archive      = {J_AIR},
  author       = {Ali, Syed Saad Azhar and Memon, Khuhed and Yahya, Norashikin and Khan, Shujaat},
  doi          = {10.1007/s10462-025-11146-5},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning frameworks for MRI-based diagnosis of neurological disorders: A systematic review and meta-analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning operations landscape: Platforms and tools.
<em>AIR</em>, <em>58</em>(6), 1–37. (<a
href="https://doi.org/10.1007/s10462-025-11164-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the field of machine learning advances, managing and monitoring intelligent models in production, also known as machine learning operations (MLOps), has become essential. Organizations are increasingly adopting artificial intelligence as a strategic tool, thus increasing the need for reliable, and scalable MLOps platforms. Consequently, every aspect of the machine learning life cycle, from workflow orchestration to performance monitoring, presents both challenges and opportunities that require sophisticated, flexible, and scalable technological solutions. This research addresses this demand by providing a comprehensive assessment framework of MLOps platforms highlighting the key features necessary for a robust MLOps solution. The paper examines 16 MLOps tools widely used, which revolve around capabilities within AI infrastructure management, including but not limited to experiment tracking, model deployment, and model inference. Our three-step evaluation framework starts with a feature analysis of the MLOps platforms, then GitHub stars growth assessment for adoption and prominence, and finally, a weighted scoring method to single out the most influential platforms. From this process, we derive valuable insights into the essential components of effective MLOps systems and provide a decision-making flowchart that simplifies platform selection. This framework provides hands-on guidance for organizations looking to initiate or enhance their MLOps strategies, whether they require an end-end solutions or specialized tools.},
  archive      = {J_AIR},
  author       = {Berberi, Lisana and Kozlov, Valentin and Nguyen, Giang and Sáinz-Pardo Díaz, Judith and Calatrava, Amanda and Moltó, Germán and Tran, Viet and López García, Álvaro},
  doi          = {10.1007/s10462-025-11164-3},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning operations landscape: Platforms and tools},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of hyperspectral image classification based on
graph neural networks. <em>AIR</em>, <em>58</em>(6), 1–56. (<a
href="https://doi.org/10.1007/s10462-025-11169-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images provide rich spectral-spatial information but pose significant classification challenges due to high dimensionality, noise, mixed pixels, and limited labeled samples. Graph Neural Networks (GNNs) have emerged as a promising solution, offering a semi-supervised framework that can capture complex spatial-spectral relationships inherent in non-Euclidean hyperspectral image data. However, existing reviews often concentrate on specific aspects, thus limiting a comprehensive understanding of GNN-based hyperspectral image classification. This review systematically outlines the fundamental concepts of hyperspectral image classification and GNNs, and summarizes leading approaches from both traditional machine learning and deep learning. Then, it categorizes GNN-based methods into four paradigms: graph recurrent neural networks, graph convolutional networks, graph autoencoders, and hybrid graph neural networks, discussing their theoretical underpinnings, architectures, and representative applications. Finally, five key directions are further highlighted: adaptive graph construction, dynamic graph processing, deeper architectures, self-supervised strategies, and robustness enhancement. These insights aim to facilitate continued innovation in GNN-based hyperspectral imaging, guiding researchers toward more efficient and accurate classification frameworks.},
  archive      = {J_AIR},
  author       = {Zhao, Xiaofeng and Ma, Junyi and Wang, Lei and Zhang, Zhili and Ding, Yao and Xiao, Xiongwu},
  doi          = {10.1007/s10462-025-11169-y},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of hyperspectral image classification based on graph neural networks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polyp segmentation in medical imaging: Challenges,
approaches and future directions. <em>AIR</em>, <em>58</em>(6), 1–61.
(<a href="https://doi.org/10.1007/s10462-025-11173-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal cancer has been considered as the third most dangerous disease among the most common cancer types. The early diagnosis of the polyps weakens the spread of colorectal cancer and is significant for more productive treatment. The segmentation of polyps from the colonoscopy images is very critical and significant to identify colorectal cancer. In this comprehensive study, we meticulously scrutinize research papers focused on the automated segmentation of polyps in clinical settings using colonoscopy images proposed in the past five years. Our analysis delves into various dimensions, including input data (datasets and preprocessing methods), model design (encompassing CNNs, transformers, and hybrid approaches), loss functions, and evaluation metrics. By adopting a systematic perspective, we examine how different methodological choices have shaped current trends and identify critical limitations that need to be addressed. To facilitate meaningful comparisons, we provide a detailed summary table of all examined works. Moreover, we offer in-depth future recommendations for polyp segmentation based on the insights gained from this survey study. We believe that our study will serve as a great resource for future researchers in the subject of polyp segmentation offering vital support in the development of novel methodologies.},
  archive      = {J_AIR},
  author       = {Qayoom, Abdul and Xie, Juanying and Ali, Haider},
  doi          = {10.1007/s10462-025-11173-2},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Polyp segmentation in medical imaging: Challenges, approaches and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A context and sequence-based recommendation framework using
GRU networks. <em>AIR</em>, <em>58</em>(6), 1–36. (<a
href="https://doi.org/10.1007/s10462-025-11174-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems play a significant contribution in e-commerce for predicting the more relevant product to the customers based on their interests. The recommendation system refers to the user-item interaction and predicts the next item by considering the similar kind of user interest or item purchased. The context-aware and sequential recommendation is built to predict the interested product based on the current context and sequential behavior pattern interactions. To fulfill the customers’ requirements, this paper proposes a new hybrid personalized recommendation system framework called Target User Context Sequential Prediction Gated Recurrent Unit (TUCSP-GRU) using deep learning methods to recommend suitable products to the users based on their interests and context. The proposed system uses the newly calculated Target User Specific Product Rating (TUS-PR) score, the proposed TUS Gated Recurrent Unit (TUS-GRU) model, and the proposed Top-N item prediction method. Here, (i) the TUS-PR score is used to improve the product rating, (ii) the new TUS-GRU model is used to find the sequence purchase behavior pattern of customers by considering their long-term and short-term interests, and (iii) the proposed Top-N item dynamic prediction method is used to adjust the next interested item list based on the response using the back propagation continuous learning method. The experiment results of the TUCSP-GRU framework show better accuracy in predicting the interested and relevant products or items when compared to existing similar recommendation systems with respect to the standard evaluation metrics.},
  archive      = {J_AIR},
  author       = {Karthik, R. V. and Pandiyaraju, V. and Ganapathy, Sannasi},
  doi          = {10.1007/s10462-025-11174-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A context and sequence-based recommendation framework using GRU networks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of machine learning in early warning system of
geotechnical disaster: A systematic and comprehensive review.
<em>AIR</em>, <em>58</em>(6), 1–45. (<a
href="https://doi.org/10.1007/s10462-025-11175-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancements in monitoring and computational technology have facilitated data accessibility and utilization. Machine learning, as an integral component of the realm of computational technology, is renowned for its universality and efficacy, rendering it pervasive across various domains. Geotechnical disaster early warning systems serve as a crucial safeguard for the preservation of human lives and assets. Machine learning exhibits the capacity to meet the exigencies of prompt and precise disaster prediction, prompting substantial interest in the nexus of these two domains in recent decades. This study accentuates the deployment of machine learning in addressing geotechnical engineering disaster prediction issues through an examination of four types of engineering-specialized research articles spanning the period 2009 to 2024. The study elucidates the evolution and significance of machine learning within the domain of geotechnical engineering disaster prediction, with an emphasis on data analytics and modeling. Addressing the lacunae in existing literature, a user-friendly front-end graphical interface, integrated with machine learning algorithms, is devised to better cater to the requisites of engineering professionals. Furthermore, this research delves into a critical analysis of the prevalent research limitations and puts forth prospective investigational avenues from an applied standpoint.},
  archive      = {J_AIR},
  author       = {Lin, Shan and Liang, Zenglong and Guo, Hongwei and Hu, Quanke and Cao, Xitailang and Zheng, Hong},
  doi          = {10.1007/s10462-025-11175-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of machine learning in early warning system of geotechnical disaster: A systematic and comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification using hyperdimensional computing: A review
with comparative analysis. <em>AIR</em>, <em>58</em>(6), 1–41. (<a
href="https://doi.org/10.1007/s10462-025-11181-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional computing (HD), also known as vector symbolic architectures (VSA), is an emerging and promising paradigm for cognitive computing. At its core, HD/VSA is characterized by its distinctive approach to compositionally representing information using high-dimensional randomized vectors. The recent surge in research within this field gains momentum from its computational efficiency stemming from low-resolution representations and ability to excel in few-shot learning scenarios. Nonetheless, the current literature is missing a comprehensive comparative analysis of various methods since each of them uses a different benchmark to evaluate its performance. This gap obstructs the monitoring of the field’s state-of-the-art advancements and acts as a significant barrier to its overall progress. To address this gap, this review not only offers a conceptual overview of the latest literature but also introduces a comprehensive comparative study of HD/VSA classification methods. The exploration starts with an overview of the strategies proposed to encode information as high-dimensional vectors. These vectors serve as integral components in the construction of classification models. Furthermore, we evaluate diverse classification methods as proposed in the existing literature. This evaluation encompasses techniques such as retraining and regenerative training to augment the model’s performance. To conclude our study, we present a comprehensive empirical study. This study serves as an in-depth analysis, systematically comparing various HD/VSA classification methods using two benchmarks, the first being a set of seven popular datasets used in HD/VSA and the second consisting of 121 datasets being the subset from the UCI Machine Learning repository. To facilitate future research on classification with HD/VSA, we open-sourced the benchmarking and the implementations of the methods we review. Since the considered data are tabular, encodings based on key-value pairs emerge as optimal choices, boasting superior accuracy while maintaining high efficiency. Secondly, iterative adaptive methods demonstrate remarkable efficacy, potentially complemented by a regenerative strategy, depending on the specific problem. Furthermore, we show how HD/VSA is able to generalize while training with a limited number of training instances. Lastly, we demonstrate the robustness of HD/VSA methods by subjecting the model memory to a large number of bit-flips. The results illustrate that the model’s performance remains reasonably stable until the occurrence of 40% of bit flips, where the model’s performance is drastically degraded. Overall, this study performed a thorough performance evaluation on different methods and, on the one hand, a positive trend was observed in terms of improving classification performance but, on the other hand, these developments could often be surpassed by off-the-shelf methods. This calls for better integration with the broader machine learning literature; the developed benchmarking framework provides practical means for doing so.},
  archive      = {J_AIR},
  author       = {Vergés, Pere and Heddes, Mike and Nunes, Igor and Kleyko, Denis and Givargis, Tony and Nicolau, Alexandru},
  doi          = {10.1007/s10462-025-11181-2},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Classification using hyperdimensional computing: A review with comparative analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context in object detection: A systematic literature review.
<em>AIR</em>, <em>58</em>(6), 1–89. (<a
href="https://doi.org/10.1007/s10462-025-11186-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context is an important factor in computer vision as it offers valuable information to clarify and analyze visual data. Utilizing the contextual information inherent in an image or a video can improve the precision and effectiveness of object detectors. For example, where recognizing an isolated object might be challenging, context information can improve comprehension of the scene. This study explores the impact of various context-based approaches to object detection. Initially, we investigate the role of context in object detection and survey it from several perspectives. We then review and discuss the most recent context-based object detection approaches and compare them. Finally, we conclude by addressing research questions and identifying gaps for further studies. More than 265 publications are included in this survey, covering different aspects of context in different categories of object detection, including general object detection, video object detection, small object detection, camouflaged object detection, zero-shot, one-shot, and few-shot object detection. This literature review presents a comprehensive overview of the latest advancements in context-based object detection, providing valuable contributions such as a thorough understanding of contextual information and effective methods for integrating various context types into object detection, thus benefiting researchers.},
  archive      = {J_AIR},
  author       = {Jamali, Mahtab and Davidsson, Paul and Khoshkangini, Reza and Ljungqvist, Martin Georg and Mihailescu, Radu-Casian},
  doi          = {10.1007/s10462-025-11186-x},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-89},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Context in object detection: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiDAR, IMU, and camera fusion for simultaneous localization
and mapping: A systematic review. <em>AIR</em>, <em>58</em>(6), 1–59.
(<a href="https://doi.org/10.1007/s10462-025-11187-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping (SLAM) is a crucial technology for intelligent unnamed systems to estimate their motion and reconstruct unknown environments. However, the SLAM systems with merely one sensor have poor robustness and stability due to the defects in the sensor itself. Recent studies have demonstrated that SLAM systems with multiple sensors, mainly consisting of LiDAR, camera, and IMU, achieve better performance due to the mutual compensation of different sensors. This paper investigates recent progress on multi-sensor fusion SLAM. The review includes a systematic analysis of the advantages and disadvantages of different sensors and the imperative of multi-sensor solutions. It categorizes multi-sensor fusion SLAM systems into four main types by the fused sensors: LiDAR-IMU SLAM, Visual-IMU SLAM, LiDAR-Visual SLAM, and LiDAR-IMU-Visual SLAM, with detailed analysis and discussions of their pipelines and principles. Meanwhile, the paper surveys commonly used datasets and introduces evaluation metrics. Finally, it concludes with a summary of the existing challenges and future opportunities for multi-sensor fusion SLAM.},
  archive      = {J_AIR},
  author       = {Fan, Zheng and Zhang, Lele and Wang, Xueyi and Shen, Yilan and Deng, Fang},
  doi          = {10.1007/s10462-025-11187-w},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {LiDAR, IMU, and camera fusion for simultaneous localization and mapping: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review on financial explainable AI.
<em>AIR</em>, <em>58</em>(6), 1–49. (<a
href="https://doi.org/10.1007/s10462-024-11077-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of artificial intelligence (AI), and deep learning models in particular, has led to their widespread adoption across various industries due to their ability to process huge amounts of data and learn complex patterns. However, due to their lack of explainability, there are significant concerns regarding their use in critical sectors, such as finance and healthcare, where decision-making transparency is of paramount importance. In this paper, we provide a comparative survey of methods that aim to improve the explainability of deep learning models within the context of finance. We categorize the collection of explainable AI methods according to their corresponding characteristics, and we review the concerns and challenges of adopting explainable AI methods, together with future directions we deemed appropriate and important.},
  archive      = {J_AIR},
  author       = {Yeo, Wei Jie and Van Der Heever, Wihan and Mao, Rui and Cambria, Erik and Satapathy, Ranjan and Mengaldo, Gianmarco},
  doi          = {10.1007/s10462-024-11077-7},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review on financial explainable AI},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dna coding theory and algorithms. <em>AIR</em>,
<em>58</em>(6), 1–37. (<a
href="https://doi.org/10.1007/s10462-025-11132-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA computing is an emerging computational model that has garnered significant attention due to its distinctive advantages at the molecular biological level. Since it was introduced by Adelman in 1994, this field has made remarkable progress in solving NP-complete problems, enhancing information security, encrypting images, controlling diseases, and advancing nanotechnology. A key challenge in DNA computing is the design of DNA coding, which aims to minimize nonspecific hybridization and enhance computational reliability. The DNA coding design is a classical combinatorial optimization problem focused on generating high-quality DNA sequences that meet specific constraints, including distance, thermodynamics, secondary structure, and sequence requirements. This paper comprehensively examines the advances in DNA coding design, highlighting mathematical models, counting theory, and commonly used DNA coding methods. These methods include the template method, multi-objective evolutionary methods, and implicit enumeration techniques.},
  archive      = {J_AIR},
  author       = {Xu, Jin and Liu, Wenbin and Zhang, Kai and Zhu, Enqiang},
  doi          = {10.1007/s10462-025-11132-x},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dna coding theory and algorithms},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Current and future roles of artificial intelligence in
retinopathy of prematurity. <em>AIR</em>, <em>58</em>(6), 1–55. (<a
href="https://doi.org/10.1007/s10462-025-11153-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinopathy of prematurity (ROP) is a severe condition affecting premature infants, leading to abnormal retinal blood vessel growth, retinal detachment, and potential blindness. While semi-automated systems have been used in the past to diagnose ROP-related plus disease by quantifying retinal vessel features, traditional machine learning (ML) models face challenges like accuracy and overfitting. Recent advancements in deep learning (DL), especially convolutional neural networks (CNNs), have significantly improved ROP detection and classification. The i-ROP deep learning (i-ROP-DL) system also shows promise in detecting plus disease, offering reliable ROP diagnosis potential. This research comprehensively examines the contemporary progress and challenges associated with using retinal imaging and artificial intelligence (AI) to detect ROP, offering valuable insights that can guide further investigation in this domain. Based on 84 original studies in this field (out of 2025 studies that were comprehensively reviewed), we concluded that traditional methods for ROP diagnosis suffer from subjectivity and manual analysis, leading to inconsistent clinical decisions. AI holds great promise for improving ROP management. This review explores AI’s potential in ROP detection, classification, diagnosis, and prognosis.},
  archive      = {J_AIR},
  author       = {Jafarizadeh, Ali and Maleki, Shadi Farabi and Pouya, Parnia and Sobhi, Navid and Abdollahi, Mirsaeed and Pedrammehr, Siamak and Lim, Chee Peng and Asadi, Houshyar and Alizadehsani, Roohallah and Tan, Ru-San and Islam, Sheikh Mohammed Shariful and Acharya, U. Rajendra},
  doi          = {10.1007/s10462-025-11153-6},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Current and future roles of artificial intelligence in retinopathy of prematurity},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale graph diffusion convolutional network for
multi-view learning. <em>AIR</em>, <em>58</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10462-025-11158-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning has attracted considerable attention owing to its capability to learn more comprehensive representations. Although graph convolutional networks have achieved encouraging results in multi-view research, their limitation to considering only nearest neighbors results in the decrease on the ability to obtain high-order information. Many existing methods acquire high-order correlation by stacking multiple layers onto the model, yet they could lead to the issue of over-smoothing. In this paper, we propose a framework termed multi-scale graph diffusion convolutional network, which aims to gather comprehensive higher-order information without stacking multiple convolutional layers. Specifically, in order to better expand the receptive field of the node and reduce the parameter complexity, the proposed framework utilizes a contractive mapping to transform features from multiple views on decoupled propagation rules. Our framework introduces a multi-scale graph-based diffusion mechanism to adaptively extract the abundant high-order knowledge embedded within multi-scale graphs. Experiments show that the proposed method outperforms other state-of-the-art methods in terms of multi-view semi-supervised classification.},
  archive      = {J_AIR},
  author       = {Wang, Shiping and Li, Jiacheng and Chen, Yuhong and Wu, Zhihao and Huang, Aiping and Zhang, Le},
  doi          = {10.1007/s10462-025-11158-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-scale graph diffusion convolutional network for multi-view learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emrnet: Enhanced micro-expression recognition network with
attention and distance correlation. <em>AIR</em>, <em>58</em>(6), 1–27.
(<a href="https://doi.org/10.1007/s10462-025-11159-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) is inherently challenging due to the difficulty of extracting subtle, localized changes in micro-expressions (MEs). Various optical flow-based methods have been proposed for MER, as optical flow can effectively suppress facial identity information while capturing the movement patterns of MEs. However, these methods, characterized by simple architectures, often fail to extract discriminative features, resulting in suboptimal performance. In this paper, we propose an Enhanced Micro-expression Recognition Network with attention and distance correlation (EMRNet) for MER. EMRNet consists of three key phases: First, we introduce a novel channel-wise region-aware attention mechanism within two identical Inception networks, designed to extract global and local expression features in parallel, based on the optical flow input of the same ME. Second, to enhance ME representations, we propose a regularized dilated loss function incorporating distance correlation, which improves the information entropy transferred between the two branches. Last, emotion categories are predicted by fusing the expression-dilated features in the classification branch. Extensive experiments conducted on the composite database from the MEGC 2019 challenge demonstrate the effectiveness of EMRNet under both leave-one-subject-out (LOSO) cross-validation and the composite database evaluation (CDE) protocol. The results show that our approach successfully generates discriminative features, achieving substantial performance gains. Furthermore, EMRNet outperforms existing single-stream and dual-stream models, delivering superior results in MER.},
  archive      = {J_AIR},
  author       = {Liu, Gaqiong and Huang, Shucheng and Wang, Gang and Li, Mingxing},
  doi          = {10.1007/s10462-025-11159-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Emrnet: Enhanced micro-expression recognition network with attention and distance correlation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bibliometric analysis of artificial intelligence cyberattack
detection models. <em>AIR</em>, <em>58</em>(6), 1–40. (<a
href="https://doi.org/10.1007/s10462-025-11167-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybercriminals have increasingly adopted advanced and cutting-edge methods that expand the scale and speed of their attacks in recent years. This trend coincides with the rising demand for and scarcity of highly skilled cybersecurity specialists, making them both expensive and difficult to find. Recently, researchers have demonstrated the effectiveness of Artificial Intelligence (AI) approaches in combating sophisticated cyberattacks. However, comprehensive bibliometric data illustrating the study of AI approaches in cyberattack detection remain sparse. This study addresses this gap by investigating the current state of AI-based cyberattack detection research. The study analyzed the Scopus database using bibliometric analysis on a pool of over 2,338 articles published between 2014 and 2024, including 1217 journal articles, 828 conference papers, 121 conference reviews, 85 book chapters, 70 reviews, 5 editorials, and 2 books and short surveys. The study explores various AI-based cyberattack detection approaches globally, focusing on machine learning and deep learning algorithms. The bibliometric analysis was conducted using R, an open-source statistical tool, and Biblioshiny. The findings establish that AI, particularly machine learning and deep learning, enhances intrusion detection accuracy and is a growing research trend. Researchers have effectively employed these techniques for malware detection. The USA leads in AI cyberattack research, followed by India, China, Saudi Arabia, and Australia. Despite publishing fewer articles, Canada and Italy received significant citations. Additionally, strong research collaboration exists among the USA, China, Australia, Saudi Arabia, and India. Keyword analysis highlights AI’s effectiveness in identifying patterns and malicious behaviours, enhancing intrusion detection even in complex cyberattacks. Machine learning can detect intrusions based on anomalies caused by malicious or compromised devices, as well as unknown threats, with speed, accuracy, and a low false-positive rate.},
  archive      = {J_AIR},
  author       = {Guembe, Blessing and Misra, Sanjay and Azeta, Ambrose and Lopez-Baldominos, Ines},
  doi          = {10.1007/s10462-025-11167-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Bibliometric analysis of artificial intelligence cyberattack detection models},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MambaYOLACT: You only look at mamba prediction head for
head-neck lymph nodes. <em>AIR</em>, <em>58</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10462-025-11177-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lymph nodes in the head-neck are often infected when malignant tumors metastasize. At present, Magnetic Resonance Imaging (MRI) is widely used in the evaluation of head-neck lymph nodes. However, there are some problems, such as different sizes, low contrast of head-neck lymph nodes. The instance segmentation accuracy of head-neck lymph nodes is decreased, which affects the patients treatment decision and the surgical effect evaluation. To solve these problems, a single stage Mamba YOLACT instance segmentation model is proposed in this paper. The main contributions are as follows: Firstly, a Cross-field and Cross-direction Feature Enhancement module (CCFE) is designed. The module through the channel grouping mechanism, effectively enhances the ability of each group of features to express different spatial semantic information, by mixing attention mechanism to improve the feature extraction ability of lesions with different dimensions. Secondly, a MambaNet-based prediction head module is designed. The module combined the State-Space Model (SSM) and self-attention mechanism to accurately capture global image dependencies, highlight the lesion area. Thirdly, A dataset of MRI images of head-neck lymph nodes is used to verify the model effectiveness. The results show that the values of APdet, APseg, ARdet, ARseg, mAPdet and mAPseg are 69.8%, 70.9%, 55.3%, 56.4%, 39.4% and 41.0%, respectively. The model can achieve accurate segmentation of the lymph nodes, which has positive significance for lymph nodes auxiliary diagnosis.},
  archive      = {J_AIR},
  author       = {Zhou, Tao and Chai, Wenwen and Chang, Defang and Chen, Kaixiong and Zhang, Zhe and Lu, HuiLing},
  doi          = {10.1007/s10462-025-11177-y},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {MambaYOLACT: You only look at mamba prediction head for head-neck lymph nodes},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guidelines for designing visualization tools for group
fairness analysis in binary classification. <em>AIR</em>,
<em>58</em>(6), 1–38. (<a
href="https://doi.org/10.1007/s10462-025-11179-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of machine learning in decision-making has become increasingly pervasive across various fields, from healthcare to finance, enabling systems to learn from data and improve their performance over time. The transformative impact of these new technologies warrants several considerations that demand the development of modern solutions through responsible artificial intelligence—the incorporation of ethical principles into the creation and deployment of AI systems. Fairness is one such principle, ensuring that machine learning algorithms do not produce biased outcomes or discriminate against any group of the population with respect to sensitive attributes, such as race or gender. In this context, visualization techniques can help identify data imbalances and disparities in model performance across different demographic groups. However, there is a lack of guidance towards clear and effective representations that support entry-level users in fairness analysis, particularly when considering that the approaches to fairness visualization can vary significantly. In this regard, the goal of this work is to present a comprehensive analysis of current tools directed at visualizing and examining group fairness in machine learning, with a focus on both data and binary classification model outcomes. These visualization tools are reviewed and discussed, concluding with the proposition of a focused set of visualization guidelines directed towards improving the comprehensibility of fairness visualizations.},
  archive      = {J_AIR},
  author       = {Cruz, António and Salazar, Teresa and Carvalho, Manuel and Maçãs, Catarina and Machado, Penousal and Abreu, Pedro Henriques},
  doi          = {10.1007/s10462-025-11179-w},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Guidelines for designing visualization tools for group fairness analysis in binary classification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer vision based approaches for fish monitoring
systems: A comprehensive study. <em>AIR</em>, <em>58</em>(6), 1–44. (<a
href="https://doi.org/10.1007/s10462-025-11180-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fish monitoring has become increasingly popular due to its growing real-world applications and recent advancements in intelligent technologies such as AI, Computer Vision, and Robotics. The primary objective of this article is to review benchmark datasets used in fish monitoring while introducing a novel framework that categorizes fish monitoring applications into four main domains: Fish Detection and Recognition (FDR), Fish Biomass Estimation (FBE), Fish Behavior Classification (FBC), and Fish Health Analysis (FHA). Additionally, this study proposes dedicated workflows for each domain, marking the first comprehensive effort to establish such a structured approach in this field. The detection and recognition of fish involve identifying fish and fish species. Estimating fish biomass focuses on counting fish and measuring their size and weight. Fish Behavior Classification tracks and analyzes movement and extracts behavioral patterns. Finally, health analysis assesses the general health of the fish. The methodologies and techniques are analyzed separately within each domain, providing a detailed examination of their specific applications and contributions to fish monitoring. These innovations enable fish species classification, fish freshness evaluation, fish counting, and body length measurement for biomass estimation. The study concludes by reviewing the development of key datasets and techniques over time, identifying existing gaps and limitations in current frameworks, and proposing future research directions in fish monitoring applications.},
  archive      = {J_AIR},
  author       = {Al-Abri, Said and Keshvari, Sanaz and Al-Rashdi, Khalfan and Al-Hmouz, Rami and Bourdoucen, Hadj},
  doi          = {10.1007/s10462-025-11180-3},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Computer vision based approaches for fish monitoring systems: A comprehensive study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathologyvlm: A large vision-language model for pathology
image understanding. <em>AIR</em>, <em>58</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10462-025-11190-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The previous advancements in pathology image understanding primarily involved developing models tailored to specific tasks. Recent studies have demonstrated that the large vision-language model can enhance the performance of various downstream tasks in medical image understanding. In this study, we developed a domain-specific large vision-language model (PathologyVLM) for pathology image understanding. Specifically, (1) we first construct a human pathology image-text dataset by cleaning the public medical image-text data for domain-specific alignment; (2) Using the proposed image-text data, we first train a pathology language-image pretraining (PLIP) model as the specialized visual encoder to extract the features of pathology image, and then we developed scale-invariant connector to avoid the information loss caused by image scaling; (3) We adopt two-stage learning to train PathologyVLM, first stage for domain alignment, and second stage for end to end visual question &amp; answering (VQA) task. In experiments, we evaluate our PathologyVLM on both supervised and zero-shot VQA datasets, our model achieved the best overall performance among multimodal models of similar scale. The ablation experiments also confirmed the effectiveness of our design. We posit that our PathologyVLM model and the datasets presented in this work can promote research in field of computational pathology. All codes are available at: https://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA},
  archive      = {J_AIR},
  author       = {Dai, Dawei and Zhang, Yuanhui and Yang, Qianlan and Xu, Long and Shen, Xiaojing and Xia, Shuyin and Wang, Guoyin},
  doi          = {10.1007/s10462-025-11190-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Pathologyvlm: A large vision-language model for pathology image understanding},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sustainable AI-driven wind energy forecasting: Advancing
zero-carbon cities and environmental computation. <em>AIR</em>,
<em>58</em>(6), 1–35. (<a
href="https://doi.org/10.1007/s10462-025-11191-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of wind speed and power is transforming renewable wind farm management, facilitating efficient energy supply for smart and zero-energy cities. This paper introduces a novel low-carbon Sustainable AI-Driven Wind Energy Forecasting System (SAI-WEFS) developed from a promising real-world case study in MENA region. The SAI-WEFS evaluates twelve machine learning algorithms, utilizing both single and ensemble models for forecasting wind speed (WSF) and wind power (WPF) across multiple timeframes (10 min, 30 min, 6 h, 24 h, and 36 h). The system integrates multi-time horizon predictions, where the WSF output is input for the WPF model. The environmental impact of each algorithm is assessed based on CO2 emissions for each computational hour. Predictive accuracy is assessed using mean square error (MSE) and mean absolute percentage error (MAPE). Results indicate that ensemble algorithms consistently outperform single ML models, with tree-based models demonstrating a lower environmental impact, emitting approximately 60 g of CO2 per computational hour compared to deep learning models, which emit up to 500 g per hour. This system enhances the Urban Energy Supply Decarbonization Framework (UESDF) by predicting the Urban Carbon Emission Index (UCEI) to illustrate the Urban Carbon Transition Curve.},
  archive      = {J_AIR},
  author       = {Elmousalami, Haytham and Alnaser, Aljawharah A. and Hui, Felix Kin Peng},
  doi          = {10.1007/s10462-025-11191-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Sustainable AI-driven wind energy forecasting: Advancing zero-carbon cities and environmental computation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy improved snow ablation optimizer: A case
study of optimization of kernel extreme learning machine for flood
prediction. <em>AIR</em>, <em>58</em>(6), 1–47. (<a
href="https://doi.org/10.1007/s10462-025-11192-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kernel Extreme Learning Machine (KELM) has the advantage of automatically extracting data features, learning and processing nonlinear problems from historical data, which can help achieve better prediction results for flood prediction problems with complex and sudden causes. Traditional flood disaster prediction usually only considers one influencing factor without considering the complex factors that affect flood occurrence. This article develops a new method for predicting the probability of flood occurrence based on 20 influencing factors. Firstly, in order to better utilize KELM performance, an improved snow ablation optimization algorithm (MESAO) was proposed for subsequent experiments by introducing a level based selection pressure mechanism, covariance matrix learning strategy, historical position based boundary adjustment strategy, and random centroid reverse learning strategy into snow ablation optimization (SAO). Secondly, MESAO is used to perform hyperparameter optimization on the regularization coefficient C and kernel function parameter S of the KELM model. Finally, the construction of a multi feature input–output model for the application of MESAO-KELM in flood prediction problems was completed. In terms of hyperparameter optimization, the numerical experimental results of this method were superior to the prediction results of 10 other intelligent algorithms and 5 regression prediction models. According to the evaluation index results, the best adaptability of MESAO optimized KELM and higher prediction accuracy and stability compared to other prediction models were demonstrated. This method overcomes the limitations of traditional prediction models based on a single influencing factor and can predict the probability of flood occurrence based on complex and variable factors. It can be said that MESAO-KELM has strong generalization ability. Accurate flood prediction can provide early warning and take measures in advance to protect and reduce the impact of floods on human and social development.},
  archive      = {J_AIR},
  author       = {Cui, Lele and Hu, Gang and Zhu, Yaolin},
  doi          = {10.1007/s10462-025-11192-z},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-strategy improved snow ablation optimizer: A case study of optimization of kernel extreme learning machine for flood prediction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A step gravitational search algorithm for function
optimization and STTM’s synchronous feature selection-parameter
optimization. <em>AIR</em>, <em>58</em>(6), 1–49. (<a
href="https://doi.org/10.1007/s10462-025-11193-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The support tensor train machine (STTM) can make full use of the correlation of tensor data structures, while the parameter training is inefficient and feature redundancy is large. For this, a step gravitational search algorithm (SGSA) is proposed and used for synchronous feature selection and parameter optimization of STTM in this paper. Since the single population structure of the gravitational search algorithm is difficult to balance exploration and exploitation effectively, a new dual population structure is defined by the step function. Subpopulation Pop1 focuses on exploration, and a Kbest-Elite hybrid learning strategy is designed to avoid the rapid decline of exploration ability due to the rapid reduction of the size of Kbest set as well as the gravitational constant G. Subpopulation Pop2 focuses on exploitation, and a position update strategy that integrates Cauchy distribution and Gaussian distribution is designed to make Pop2 always have a certain exploration ability. Finally, use SGSA to solve the synchronous feature selection and parameter optimization problem of STTM (the resulting model is denoted as SGSA-STTM). The algorithm’s optimization performance test results show that SGSA can obtain relatively best results on most test functions compared with other state-of-the-art algorithms. The classification performance test on fMRI datasets shows that SGSA-STTM can remove more than 40% of redundant features on most datasets, which can effectively improve the efficiency of the algorithm, and the classification accuracy for the StarPlus fMRI dataset and the CMU Science 2008 fMRI dataset reached 60 and 70%, respectively.},
  archive      = {J_AIR},
  author       = {Fan, Chaodong and Yang, Laurence T. and Xiao, Leyi},
  doi          = {10.1007/s10462-025-11193-y},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A step gravitational search algorithm for function optimization and STTM’s synchronous feature selection-parameter optimization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review on municipal solid waste
management using machine learning and deep learning. <em>AIR</em>,
<em>58</em>(6), 1–51. (<a
href="https://doi.org/10.1007/s10462-025-11196-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population growth and urbanization have led to a significant increase in solid waste. However, conventional methods of treating and recycling this waste have inherent problems, such as low efficiency, poor precision, high cost, and severe environmental hazards. To address these challenges, Artificial Intelligence (AI) has gained popularity in recent years as a potential solution for municipal solid-waste management (MSWM). A few applications of AI, based on Machine Learning (ML) and Deep Learning (DL) techniques, have been used for MSWM. This study reviews the current landscape in MSWM, highlighting the existing advantages and disadvantages of 69 studies published between 2018 and 2024 using the PRISMA methodology. The applications of ML and DL algorithms demonstrate their ability to enhance decision-making processes, improve resource recovery rates, and promote circular economy principles. Although these technologies offer promising solutions, challenges such as data availability, quality, and interdisciplinary collaboration hinder their effective implementation. The paper suggests future research directions focusing on developing robust datasets, fostering partnerships across sectors, and integrating advanced technologies with traditional waste management strategies. This research aligns with the United Nations’ Sustainable Development Goals (SDG), particularly Goal 11, which aims to make cities inclusive, safe, resilient, and sustainable. In the future, this research can contribute to making cities smarter, greener, and more resilient using ML and DL techniques.},
  archive      = {J_AIR},
  author       = {Dawar, Ishaan and Srivastava, Anisha and Singal, Maanas and Dhyani, Nirjara and Rastogi, Suvi},
  doi          = {10.1007/s10462-025-11196-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic literature review on municipal solid waste management using machine learning and deep learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised hypergraph structure learning. <em>AIR</em>,
<em>58</em>(6), 1–30. (<a
href="https://doi.org/10.1007/s10462-025-11199-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Hypergraph Neural Networks (HGNNs) often assume that hypergraph structures are perfectly constructed, yet real-world hypergraphs are typically corrupted by noise, missing data, or irrelevant information, limiting the effectiveness of hypergraph learning. To address this challenge, we propose SHSL, a novel Self-supervised Hypergraph Structure Learning framework that jointly explores and optimizes hypergraph structures without external labels. SHSL consists of two key components: a self-organizing initialization module that constructs latent hypergraph representations, and a differentiable optimization module that refines hypergraphs through gradient-based learning. These modules collaboratively capture high-order dependencies to enhance hypergraph representations. Furthermore, SHSL introduces a dual learning mechanism to simultaneously guide structure exploration and optimization within a unified framework. Experiments on six public datasets demonstrate that SHSL outperforms state-of-the-art baselines, achieving Accuracy improvements of 1.36% $$-$$ 32.37% and 2.23% $$-$$ 27.54% on hypergraph exploration and optimization tasks, and 1.19% $$-$$ 8.4% on non-hypergraph datasets. Robustness evaluations further validate SHSL’s effectiveness under noisy and incomplete scenarios, highlighting its practical applicability. The implementation of SHSL and all experimental codes are publicly available at: https://github.com/MingyuanLi88888/SHSL.},
  archive      = {J_AIR},
  author       = {Li, Mingyuan and Yang, Yanlin and Meng, Lei and Peng, Lu and Zhao, Haixing and Ye, Zhonglin},
  doi          = {10.1007/s10462-025-11199-6},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Self-supervised hypergraph structure learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
