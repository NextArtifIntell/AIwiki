<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac---20">SAC - 20</h2>
<ul>
<li><details>
<summary>
(2025). Simulation based composite likelihood. <em>SAC</em>,
<em>35</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s11222-025-10584-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference for high-dimensional hidden Markov models is challenging due to the exponential-in-dimension computational cost of calculating the likelihood. To address this issue, we introduce an innovative composite likelihood approach called “Simulation Based Composite Likelihood” (SimBa-CL). With SimBa-CL, we approximate the likelihood by the product of its marginals, which we estimate using Monte Carlo sampling. In a similar vein to approximate Bayesian computation (ABC), SimBa-CL requires multiple simulations from the model, but, in contrast to ABC, it provides a likelihood approximation that guides the optimization of the parameters. Leveraging automatic differentiation libraries, it is simple to calculate gradients and Hessians to not only speed up optimization but also to build approximate confidence sets. We present extensive empirical results which validate our theory and demonstrate its advantage over SMC, and apply SimBa-CL to real-world Aphtovirus data.},
  archive      = {J_SAC},
  author       = {Rimella, Lorenzo and Jewell, Chris and Fearnhead, Paul},
  doi          = {10.1007/s11222-025-10584-z},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Simulation based composite likelihood},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact score and information matrix for panel hidden
semi-markov models. <em>SAC</em>, <em>35</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11222-025-10585-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a general multivariate hidden semi-Markov model for time series and panel data. The model entails multiple response variables arising from exponential families conditionally on covariates and a discrete time-varying latent variable. The latter is modeled through transition probabilities and sojourn time distributions. We derive efficient forward recursions to exactly compute the score and information matrix. In a simulation study, we show the validity of our inferential approach for parameter and standard error estimation. The approach is also illustrated on an original real data example on sales of four arm types from member countries of the North Atlantic Treaty Organization to non-member countries in the period 2002–2022.},
  archive      = {J_SAC},
  author       = {Farcomeni, Alessio},
  doi          = {10.1007/s11222-025-10585-y},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Exact score and information matrix for panel hidden semi-markov models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On approximations of subordinators in <span
class="math display"><em>L</em><sup><em>p</em></sup></span> and the
simulation of tempered stable distributions. <em>SAC</em>,
<em>35</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11222-025-10586-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subordinators are infinitely divisible distributions on the positive half-line. They are often used as mixing distributions in Poisson mixtures. We show that appropriately scaled Poisson mixtures can approximate the mixing subordinator and we derive a rate of convergence in $$L^p$$ for each $$p\in [1,\infty ]$$ . This includes the Kolmogorov and Wasserstein metrics as special cases. As an application, we develop an approach for approximate simulation of the underlying subordinator. In the interest of generality, we present our results in the context of more general mixtures, specifically those that can be represented as differences of randomly stopped Lévy processes. Particular focus is given to the case where the subordinator belongs to the class of tempered stable distributions.},
  archive      = {J_SAC},
  author       = {Grabchak, Michael and Saba, Sina},
  doi          = {10.1007/s11222-025-10586-x},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {On approximations of subordinators in $$L^p$$ and the simulation of tempered stable distributions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperparameter optimization for randomized algorithms: A
case study on random features. <em>SAC</em>, <em>35</em>(3), 1–28. (<a
href="https://doi.org/10.1007/s11222-025-10587-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized algorithms exploit stochasticity to reduce computational complexity. One important example is random feature regression (RFR) that accelerates Gaussian process regression (GPR). RFR approximates an unknown function with a random neural network whose hidden weights and biases are sampled from a probability distribution. Only the final output layer is fit to data. In randomized algorithms like RFR, the hyperparameters that characterize the sampling distribution greatly impact performance, yet are not directly accessible from samples. This makes optimization of hyperparameters via standard (gradient-based) optimization tools inapplicable. Inspired by Bayesian ideas from GPR, this paper introduces a random objective function that is tailored for hyperparameter tuning of vector-valued random features. The objective is minimized with ensemble Kalman inversion (EKI). EKI is a gradient-free particle-based optimizer that is scalable to high-dimensions and robust to randomness in objective functions. A numerical study showcases the new black-box methodology to learn hyperparameter distributions in several problems that are sensitive to the hyperparameter selection: two global sensitivity analyses, integrating a chaotic dynamical system, and solving a Bayesian inverse problem from atmospheric dynamics. The success of the proposed EKI-based algorithm for RFR suggests its potential for automated optimization of hyperparameters arising in other randomized algorithms.},
  archive      = {J_SAC},
  author       = {Dunbar, Oliver R. A. and Nelsen, Nicholas H. and Mutic, Maya},
  doi          = {10.1007/s11222-025-10587-w},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Hyperparameter optimization for randomized algorithms: A case study on random features},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust <span
class="math display"><em>ℓ</em><sub>2, 0</sub></span> -penalized rank
regression for high-dimensional group selection. <em>SAC</em>,
<em>35</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11222-025-10588-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse group selection is the process of selecting a small part of nonoverlapping groups to achieve the good interpretability and prediction on the response, and it has recently seen increasing applications in machine learning, image processing and bio-medical fields. However, developing robust and efficient algorithms for group selection remains a challenging research topic due to the computational complexity and potential outliers in high-dimensional settings. Motivated by the superior performance of rank-based methodology, we design a fast and efficient algorithm based on the $$\ell _{2,0}$$ penalty to achieve the goal of robust group selection for a given size of active groups s. This new algorithm can iteratively detect the active groups and exclude the irrelevant ones. When s is not less than $$s^*$$ (the true size of active groups), we theoretically prove that the proposed algorithm covers the true subset of active groups with high probability and the estimation error of the solution sequence generated by our algorithm decays to the optimal error bound in a few iterations. Moreover, coupled with the group Bayesian information criterion, an adaptive algorithm is further introduced to determine the optimal s. Theoretically, without any prior knowledge of $$s^*$$ , the proposed adaptive algorithm is able to exactly identify the true subset of active groups with probability approaching to one. Finally, extensive simulation examples show that our method outperforms existing competitors, resulting in significant improvements in terms of efficiency and accuracy of group selection and parametric estimation. The Bardet-Biedl syndrome gene expression data set is also analyzed to illustrate the application of our proposed method.},
  archive      = {J_SAC},
  author       = {Lv, Jing and Guo, Chaohui},
  doi          = {10.1007/s11222-025-10588-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Robust $$\ell _{2,0}$$ -penalized rank regression for high-dimensional group selection},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist model averaging under a linear exponential loss.
<em>SAC</em>, <em>35</em>(3), 1–27. (<a
href="https://doi.org/10.1007/s11222-025-10589-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new model averaging approach to consider uncertainty in model specification using an asymmetric loss, linear exponential (LINEX) loss function. We are motivated by the existing model-averaging prediction analysis studies being based on symmetric loss functions, which cannot meet practical situations where different weights are needed for over-prediction and under-prediction. The existing approaches cannot be used for the asymmetric loss. The proposed model averaging estimator established via the LINEX model averaging (LMA) criterion is shown to be optimal in achieving the lowest possible LINEX loss. We demonstrate the superiority of the LMA method and its effectiveness in movie forecasting and bitcoin volatility forecasting applications. Compared to other methods, the LMA estimator effectively reduces asymmetric loss and performs reasonably well even in the case of symmetric loss.},
  archive      = {J_SAC},
  author       = {Li, Xinmin and Liang, Hua and Liu, Huihang and Tong, Tingting and Xie, Tian},
  doi          = {10.1007/s11222-025-10589-8},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Frequentist model averaging under a linear exponential loss},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group inference for high-dimensional mediation models.
<em>SAC</em>, <em>35</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11222-025-10591-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis serves as a foundational statistical approach to comprehending the impact of exposure on an outcome. In this study, we investigate group inference for high-dimensional mediation models, examining mediators within a specified group either jointly or individually, allowing the number of mediators within the group to diverge. For both situations, we construct suitable test statistics and establish their asymptotic distributions. Extensive numerical studies demonstrate the superiority of our proposed methods over recent representative approaches. Our procedure can control the type I error well and exhibit the highest power. We also apply our methods to analyse how Deoxyribonucleic acid (DNA) methylation operates in the regulation of human stress reactivity impacted by childhood trauma. We have pinpointed seven key biological process groups, with the top five significant groups-axon development, neuron projection regeneration, positive regulation of the Mitogen-activated protein kinases (MAPK) cascade, regulation of neuron projection development, and axonogenesis-playing a collective role in nurturing nerve cell growth, development, and signal transmission.},
  archive      = {J_SAC},
  author       = {Yu, Ke and Guo, Xu and Luo, Shan},
  doi          = {10.1007/s11222-025-10591-0},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Group inference for high-dimensional mediation models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational bayesian analysis for joint models of
longitudinal and failure time data with interval censoring.
<em>SAC</em>, <em>35</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s11222-025-10592-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s Disease (AD) progression is marked by a gradual decline in cognitive function, with significant events often occurring within uncertain intervals. To comprehensively understand AD, it is essential to jointly model longitudinal cognitive assessments and interval-censored survival data. However, current methodologies have certain limitations when applied to joint models. Maximum Likelihood Estimation often neglects parameter and model uncertainty, while Bayesian methods permit uncertainty quantification but rely on traditional Markov Chain Monte Carlo algorithms, which suffer from slow convergence and high memory demands. To address these challenges, we propose variational Bayesian methods as a more computationally efficient and scalable alternative. Specifically, we focus on two approaches: the Non-Conjugate Variational Message Passing method and the Non-Conjugate Variational Laplace Approximation method. These techniques effectively approximate complex posterior distributions while minimizing the excessive computational demands typically associated with traditional Bayesian techniques. Additionally, we introduce a variational Bayesian framework for local influence analysis and outlier detection, utilizing sparse priors to enhance the model’s robustness against data anomalies. Through simulation studies and an application to the Alzheimer’s Disease Neuroimaging Initiative dataset, we demonstrate the effectiveness of our variational Bayesian joint modeling approach. Our results underscore the advantages of these methods in terms of computational efficiency and scalability, making them well-suited for analyzing complex longitudinal and interval-censored data in AD research.},
  archive      = {J_SAC},
  author       = {Li, Huiqiong and Luo, Lu and Liu, Wenting and Wang, Min and Tang, Niansheng},
  doi          = {10.1007/s11222-025-10592-z},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Variational bayesian analysis for joint models of longitudinal and failure time data with interval censoring},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multifacet hierarchical sentiment-topic model with
application to multi-brand online review analysis. <em>SAC</em>,
<em>35</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10593-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-brand analysis based on review comments and ratings is a commonly used strategy to compare different brands in marketing. It can help consumers make more informed decisions and help marketers understand their brand’s position in the market. In this work, we propose a multifacet hierarchical sentiment-topic model (MH-STM) to detect brand-associated sentiment polarities towards multiple comparative aspects from online customer reviews. The proposed method is built on a unified generative framework that explains review words with a hierarchical brand-associated topic model and the overall polarity score with a regression model on the empirical topic distribution. Moreover, a novel hierarchical Pólya urn (HPU) scheme is proposed to enhance the topic-word association among topic hierarchy, such that the general topics shared by all brands are separated effectively from the unique topics specific to individual brands. The performance of the proposed method is evaluated on both synthetic data and two real-world review corpora. Experimental studies demonstrate that the proposed method can be effective in detecting reasonable topic hierarchy and deriving accurate brand-associated rankings on multi-aspects.},
  archive      = {J_SAC},
  author       = {Liang, Qiao and Deng, Xinwei},
  doi          = {10.1007/s11222-025-10593-y},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A multifacet hierarchical sentiment-topic model with application to multi-brand online review analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Air-HOLP: Adaptive regularized feature screening for high
dimensional correlated data. <em>SAC</em>, <em>35</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s11222-025-10599-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling high-dimensional datasets presents substantial computational challenges, particularly when the number of features far exceeds the number of observations and when features are highly correlated. A modern approach to mitigate these issues is feature screening. In this work, the High-dimensional Ordinary Least-squares Projection (HOLP) feature screening method is advanced by employing adaptive ridge regularization. The impact of the ridge tuning parameter on the Ridge-HOLP method is examined and Adaptive iterative ridge-HOLP (Air-HOLP) is proposed, a data-adaptive advance to Ridge-HOLP where the ridge-regularization tuning parameter is selected iteratively and optimally for better feature screening performance. The proposed method addresses the challenges of tuning parameter selection in high dimensions by offering a computationally efficient and stable alternative to traditional methods like bootstrapping and cross-validation. Air-HOLP is evaluated using simulated data and a prostate cancer genetic dataset. The empirical results demonstrate that Air-HOLP has improved performance over a large range of simulation settings. We provide R codes implementing the Air-HOLP feature screening method and integrating it into existing feature screening methods that utilize the HOLP formula.},
  archive      = {J_SAC},
  author       = {Joudah, Ibrahim and Muller, Samuel and Zhu, Houying},
  doi          = {10.1007/s11222-025-10599-6},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Air-HOLP: Adaptive regularized feature screening for high dimensional correlated data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metropolis-adjusted interacting particle sampling.
<em>SAC</em>, <em>35</em>(3), 1–31. (<a
href="https://doi.org/10.1007/s11222-025-10595-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, various interacting particle samplers have been developed to sample from complex target distributions, such as those found in Bayesian inverse problems. These samplers are motivated by the mean-field limit perspective and implemented as ensembles of particles that move in the product state space according to coupled stochastic differential equations. The ensemble approximation and numerical time stepping used to simulate these systems can introduce bias and affect the invariance of the particle system with respect to the target distribution. To correct for this, we investigate the use of a Metropolization step, similar to the Metropolis-adjusted Langevin algorithm. We examine Metropolization of either the whole ensemble or smaller subsets of the ensemble, and prove basic convergence of the resulting ensemble Markov chain to the target distribution. Our numerical results demonstrate the benefits of this correction in numerical examples for popular interacting particle samplers such as ALDI, CBS, and stochastic SVGD.},
  archive      = {J_SAC},
  author       = {Sprungk, Björn and Weissmann, Simon and Zech, Jakob},
  doi          = {10.1007/s11222-025-10595-w},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Metropolis-adjusted interacting particle sampling},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monotonic warpings for additive and deep gaussian processes.
<em>SAC</em>, <em>35</em>(3), 1–22. (<a
href="https://doi.org/10.1007/s11222-025-10598-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes (GPs) are canonical as surrogates for computer experiments because they enjoy a degree of analytic tractability. But that breaks when the response surface is constrained, say to be monotonic. Here, we provide a “mono-GP” construction for a single input that is highly efficient even though the calculations are non-analytic. Key ingredients include transformation of a reference process and elliptical slice sampling. We then show how mono-GP may be deployed effectively in two ways. One is additive, extending monotonicity to more inputs; the other is as a prior on injective latent warping variables in a deep Gaussian process for (non-monotonic, multi-input) nonstationary surrogate modeling. We provide illustrative and benchmarking examples throughout, showing that our methods yield improved performance over the state-of-the-art on examples from those two classes of problems.},
  archive      = {J_SAC},
  author       = {Barnett, Steven D. and Beesley, Lauren J. and Booth, Annie S. and Gramacy, Robert B. and Osthus, Dave},
  doi          = {10.1007/s11222-025-10598-7},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Monotonic warpings for additive and deep gaussian processes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification and propagation in
surrogate-based bayesian inference. <em>SAC</em>, <em>35</em>(3), 1–28.
(<a href="https://doi.org/10.1007/s11222-025-10597-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate models are statistical or conceptual approximations for more complex simulation models. In this context, it is crucial to propagate the uncertainty induced by limited simulation budget and surrogate approximation error to predictions, inference, and subsequent decision-relevant quantities. However, quantifying and then propagating the uncertainty of surrogates is usually limited to special analytic cases or is otherwise computationally very expensive. In this paper, we propose a framework enabling a scalable, Bayesian approach to surrogate modeling with thorough uncertainty quantification, propagation, and validation. Specifically, we present three methods for Bayesian inference with surrogate models given measurement data. This is a task where the propagation of surrogate uncertainty is especially relevant, because failing to account for it may lead to biased and/or overconfident estimates of the parameters of interest. We showcase our approach in three detailed case studies for linear and nonlinear real-world modeling scenarios. Uncertainty propagation in surrogate models enables more reliable and safe approximation of expensive simulators and will therefore be useful in various fields of applications.},
  archive      = {J_SAC},
  author       = {Reiser, Philipp and Aguilar, Javier Enrique and Guthke, Anneli and Bürkner, Paul-Christian},
  doi          = {10.1007/s11222-025-10597-8},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Uncertainty quantification and propagation in surrogate-based bayesian inference},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational bayes inference for simultaneous autoregressive
models with missing data. <em>SAC</em>, <em>35</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11222-025-10590-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous autoregressive (SAR) models are often used to analyse spatially correlated data. Markov chain Monte Carlo is one of the most widely used Bayesian methods for estimating the SAR models, but it has significant limitations when it comes to handling missing data in the response variable due to its high computational cost. Variational Bayes (VB) approximation offers an alternative solution to this problem. Two VB-based algorithms employing Gaussian variational approximation with factor covariance structure are presented, joint VB (JVB) and hybrid VB (HVB), suitable for both missing at random and not at random inference. While the JVB method inaccurately estimates the posterior distributions of some SAR parameters and missing values, the standard HVB algorithm struggles to make accurate inferences when dealing with a large number of missing values. Our modified versions of HVB enable accurate inference within a reasonable computational time, thus improving its performance. The performance of the VB methods is evaluated using simulated and real datasets. While we demonstrate the method using SAR models, the approach has broad applicability to various models with missing data.},
  archive      = {J_SAC},
  author       = {Wijayawardhana, Anjana and Gunawan, David and Suesse, Thomas},
  doi          = {10.1007/s11222-025-10590-1},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Variational bayes inference for simultaneous autoregressive models with missing data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and model selection for finite mixtures of
tukey’s g- &amp;-h distributions. <em>SAC</em>, <em>35</em>(3), 1–18.
(<a href="https://doi.org/10.1007/s11222-025-10596-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A finite mixture of distributions is a popular statistical model, which is especially meaningful when the population of interest may include distinct subpopulations. This work is motivated by analysis of protein expression levels quantified using immunofluorescence immunohistochemistry assays of human tissues. The distributions of cellular protein expression levels in a tissue often exhibit multimodality, skewness and heavy tails, but there is a substantial variability between distributions in different tissues from different subjects, while some of these mixture distributions include components consistent with the assumption of a normal distribution. To accommodate such diversity, we propose a mixture of 4-parameter Tukey’s g- &amp;-h distributions for fitting finite mixtures with both Gaussian and non-Gaussian components. Tukey’s g- &amp;-h distribution is a flexible model that allows variable degree of skewness and kurtosis in mixture components, including normal distribution as a particular case. Since the likelihood of the Tukey’s g- &amp;-h mixtures does not have a closed analytical form, we propose a quantile least Mahalanobis distance (QLMD) estimator for parameters of such mixtures. QLMD is an indirect estimator minimizing the Mahalanobis distance between the sample and model-based quantiles, and its asymptotic properties follow from the general theory of indirect estimation. We have developed a stepwise algorithm to select a parsimonious Tukey’s g- &amp;-h mixture model and implemented all proposed methods in the R package QuantileGH available on CRAN. A simulation study was conducted to evaluate performance of the Tukey’s g- &amp;-h mixtures and compare to performance of mixtures of skew-normal or skew-t distributions. The Tukey’s g- &amp;-h mixtures were applied to model cellular expressions of Cyclin D1 protein in breast cancer tissues, and resulting parameter estimates evaluated as predictors of progression-free survival.},
  archive      = {J_SAC},
  author       = {Zhan, Tingting and Yi, Misung and Peck, Amy R. and Rui, Hallgeir and Chervoneva, Inna},
  doi          = {10.1007/s11222-025-10596-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Estimation and model selection for finite mixtures of tukey’s g- &amp;-h distributions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new p-value based multiple testing procedure for
generalized linear models. <em>SAC</em>, <em>35</em>(3), 1–10. (<a
href="https://doi.org/10.1007/s11222-025-10600-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel p-value-based multiple testing approach tailored for generalized linear models. Despite the crucial role of generalized linear models in statistics, existing methodologies face obstacles arising from the heterogeneous variance of response variables and complex dependencies among estimated parameters. Our aim is to address the challenge of controlling the false discovery rate (FDR) amidst arbitrarily dependent test statistics. Through the development of efficient computational algorithms, we present a versatile statistical framework for multiple testing. The proposed framework accommodates a range of tools developed for constructing a new model matrix in regression-type analysis, including random row permutations and Model-X knockoffs. We devise efficient computing techniques to solve the encountered non-trivial quadratic matrix equations, enabling the construction of paired p-values suitable for the two-step multiple testing procedure proposed by Sarkar and Tang (Biometrika 109(4): 1149–1155, 2022). Theoretical analysis affirms the properties of our approach, demonstrating its capability to control the FDR at a given level. Empirical evaluations further substantiate its promising performance across diverse simulation settings.},
  archive      = {J_SAC},
  author       = {Rilling, Joseph and Tang, Cheng Yong},
  doi          = {10.1007/s11222-025-10600-2},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {Stat. Comput.},
  title        = {A new p-value based multiple testing procedure for generalized linear models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian design for sampling anomalous spatio-temporal data.
<em>SAC</em>, <em>35</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10594-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected from arrays of sensors are essential for informed decision-making in various systems. However, the presence of anomalies can compromise the accuracy and reliability of insights drawn from the collected data or information obtained via statistical analysis. This study aims to develop a robust Bayesian optimal experimental design framework with anomaly detection methods for high-quality data collection. We introduce a general framework that involves anomaly generation, detection and error scoring when searching for an optimal design. This method is demonstrated using two comprehensive simulated case studies: the first study uses a spatial dataset, and the second uses a spatio-temporal river network dataset. As a baseline approach, we employed a commonly used prediction-based utility function based on minimising errors. Results illustrate the trade-off between predictive accuracy and anomaly detection performance for our method under various design scenarios. An optimal design robust to anomalies ensures the collection and analysis of more trustworthy data, playing a crucial role in understanding the dynamics of complex systems such as the environment, therefore enabling informed decisions in monitoring, management, and response.},
  archive      = {J_SAC},
  author       = {Buchhorn, Katie and Mengersen, Kerrie and Santos-Fernandez, Edgar and McGree, James},
  doi          = {10.1007/s11222-025-10594-x},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian design for sampling anomalous spatio-temporal data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian analysis of doubly semiparametric mixture cure
models with interval-censored data. <em>SAC</em>, <em>35</em>(3), 1–19.
(<a href="https://doi.org/10.1007/s11222-025-10601-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored data are commonly encountered in medical studies, where the occurrence of a disease can only be observed within specific time intervals or during periodic examinations. In the presence of individuals being cured or never experiencing the disease, a mixture cure model is often assumed for regression analysis accounting for the mixture of cured and uncured individuals in the study population. In this model, the Cox proportional hazards model is typically specified as a latency component for the event time and logistic regression as an incidence component for the probability of uncured. Challenges appear in the analysis when some covariates are time-related. It is unrealistic to assume linear covariate effects on a known transformation of cure probability or the hazard ratio of uncured individuals, as is commonly done. We propose a doubly semiparametric mixture cure model for interval-censored data, providing more flexibility by allowing linear and nonlinear effects of covariates in both the incidence and latency parts. We develop a computationally feasible Bayesian estimation procedure, incorporating a two-stage data augmentation with Poisson latent variables to deal with interval-censored data and splines for modelling the nonlinear terms in the model. We evaluate the finite sample performance of the proposed method via extensive simulations and demonstrate its utility through analysis of data from a hypobaric decompression sickness study.},
  archive      = {J_SAC},
  author       = {Liu, Xiaoyu and Xiang, Liming},
  doi          = {10.1007/s11222-025-10601-1},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian analysis of doubly semiparametric mixture cure models with interval-censored data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse and debiased lasso estimation and statistical
inference for long time series via divide-and-conquer. <em>SAC</em>,
<em>35</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s11222-025-10602-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To tackle long time series with high-dimensional covariates and dependent non-Gaussian errors, we consider the divide-and-conquer strategy and develop a class of sparse and debiased Lasso estimators. To alleviate the serial correlation in long time series data, we sequentially split the long time series into several subseries and apply a generalized penalized least squares (GLS) method for linear regression models in each subseries allowing stationary covariates and AR(q) error processes. To make accurate statistical inference, we further propose a sparse and debiased estimator and investigate its asymptotic properties. By constructing a pseudo-response variable using a squared loss transformation, the proposed GLS method is extended to a unified M-estimation framework including Huber and quantile regression models to reduce computational burden. Extensive simulations validate theoretical properties and demonstrate that our proposed estimators have better performance than some existing methods. The proposed estimators are applied to Beijing Air Quality Data and NIFTY 50 Index Data to illustrate their validity and feasibility.},
  archive      = {J_SAC},
  author       = {Liu, Jin and Ma, Wei and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-025-10602-0},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Sparse and debiased lasso estimation and statistical inference for long time series via divide-and-conquer},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Debiased transfer learning estimation and inference for
multinomial regression. <em>SAC</em>, <em>35</em>(3), 1–30. (<a
href="https://doi.org/10.1007/s11222-025-10607-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning has gained considerable attention for improving the performance of high-dimensional linear and generalized linear models by leveraging source data. However, few studies have explored transfer learning in multinomial regression (MR) for multi-class classification problems. In this paper, we propose a two-step MR transfer learning estimator when the transferable sources are known and establish its error bounds. When the target and source datasets are close, these bounds can be improved over the MR estimator using only target data under mild conditions. To address the bias introduced by the Lasso penalty, we develop a unified debiasing framework based on KKT conditions, establishing the asymptotic normality for the construction of confidence intervals and hypothesis tests. For practical implementation, a transferable source detection algorithm with theoretical guarantees is proposed. Numerical studies and an application to Genotype-Tissue Expression data demonstrate the effectiveness of our proposed methods.},
  archive      = {J_SAC},
  author       = {Yang, Jichen and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-025-10607-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Debiased transfer learning estimation and inference for multinomial regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
