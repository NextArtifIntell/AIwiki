<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cc---22">CC - 22</h2>
<ul>
<li><details>
<summary>
(2025). Deep learning innovations in the detection of lung cancer:
Advances, trends, and open challenges. <em>CC</em>, <em>17</em>(2),
1–46. (<a href="https://doi.org/10.1007/s12559-025-10408-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is the second leading cause of death worldwide, and within this type of disease, lung cancer is the second most diagnosed, but the leading cause of death. Early detection is crucial to increase patient survival rates. One of the primary methods for detecting this disease is through medical imaging, which, due to its features, is well-suited for analysis by deep learning techniques. These techniques have demonstrated exceptional results in similar tasks. Therefore, this paper focusses on analyzing the latest work related to lung cancer detection using deep learning, providing a clear overview of the state of the art and the most common research directions pursued by researchers. We have reviewed DL techniques for lung cancer detection between 2018 and 2023, analyzing the different datasets that have been used in this domain and providing an analysis between the different investigations. In this state-of-the-art review, we describe the main datasets used in this field and the primary deep learning techniques used to detect radiological signs, predominantly convolutional neural networks (CNNs). As the impact of these systems in medicine can pose risks to patients, we also examine the extent to which explainable AI techniques have been applied to enhance the understanding of these systems, a crucial aspect for their real-world application. Finally, we will discuss the trends that the domain is expected to follow in the coming years and the challenges that researchers will need to address.},
  archive      = {J_CC},
  author       = {Liz-López, Helena and de Sojo-Hernández, Áurea Anguera and D’Antonio-Maceiras, Sergio and Díaz-Martínez, Miguel Angel and Camacho, David},
  doi          = {10.1007/s12559-025-10408-2},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-46},
  shortjournal = {Cogn. Comput.},
  title        = {Deep learning innovations in the detection of lung cancer: Advances, trends, and open challenges},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source adversarial domain adaptive fault diagnosis
method based on multi-classifier alignment. <em>CC</em>, <em>17</em>(2),
1–17. (<a href="https://doi.org/10.1007/s12559-025-10414-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning–based fault diagnosis has received intensive attention from researchers. Under various working conditions, high-precision cross-domain fault diagnosis remains a problem due to distribution differences between different source domains and between source and target domains. Therefore, reducing the distribution difference between source domain and target domain data is crucial for improving the model’s ability to learn domain-invariant features and fault-representative features. To address this challenge, this paper proposes a multi-source adversarial domain adaptation approach for fault diagnosis, referred to as MSD-MCA, which is based on the alignment of multiple classifiers. The method constructs a sub-network for each source domain and utilizes domain adversarial training to extract domain-invariant features. It then generates a fault feature set for each source domain by leveraging the domain-invariant features corresponding to various fault types. To align the target domain with the source domains, the Wasserstein distance is calculated between the target features and each fault feature set. Minimizing the entropy of the distribution distance vector facilitates the learning of fault-representative features. Additionally, an association matrix is employed to enhance the stability of the decision boundaries during the training process. This approach improves the model’s capacity to generalize across multiple domains while effectively capturing fault-related information. To validate the efficacy of the proposed MSD-MCA method, a comparative analysis was conducted against several state-of-the-art diagnostic approaches. The evaluation was performed on bearing fault data from Case Western Reserve University, as well as two real-world industrial datasets. The results indicate that MSD-MCA shows improved accuracy and enhanced generalization capabilities across both datasets. Consequently, MSD-MCA can better learn the domain-invariant features and fault-representative features and improve the accuracy of fault diagnosis.},
  archive      = {J_CC},
  author       = {Zheng, Zhiwei and He, Yu and Ma, Tianyu and Xiang, Qingsong},
  doi          = {10.1007/s12559-025-10414-4},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Multi-source adversarial domain adaptive fault diagnosis method based on multi-classifier alignment},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional time series modeling for pneumoconiosis
progression risk prediction with missing data. <em>CC</em>,
<em>17</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s12559-025-10417-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pneumoconiosis is a serious occupational disease with high morbidity and disability rates. However, the evolution of pneumoconiosis is complex and changeable, and the course of most cases is incomplete, resulting in a lack of continuity in a large number of data samples, which makes it challenging for radiologists to accurately assess the development of the disease. We propose a conditional self-attention TimesNet as the backbone network for time series analysis tasks, aiming to improve prognosis prediction based on disease progression information in pneumoconiosis image data at different time periods. In our approach, we train the model using chest X-ray images of the same patient at different time points, incorporating a hierarchical attention structure and self-attention blocks to fully consider the contextual correlation information of consecutive time-point images. Additionally, diverse clinical features of patients are utilized as conditional inputs in disease progression prediction. The goal is to better learn the progression status of the disease and reflect the disease trajectory representation of missing time series data, enhancing the model’s predictive capabilities. Simultaneously, an adversarial diffusion generation model is designed to fill in missing values in the time series data. The missing data generated by the model effectively improves radiologists’ judgment of pneumoconiosis progression. We trained our model using missing time series images to predict clinical outcomes. Experimental validation on two medical datasets shows that the AUC, sensitivity, specificity, and DSC achieved 90.33%, 87.89%, 85.01%, and 88.54%, respectively. These results highlight the competitive performance of our method across multiple evaluation metrics. Our model can capture the correlation between short-term/long-term/missing time series lesion features and time in pneumoconiosis images. This approach holds significant implications for predicting clinical outcomes and progression risk in pneumoconiosis, providing valuable guidance for the assessment and prognosis of pneumoconiosis.},
  archive      = {J_CC},
  author       = {Ren, Xueting and Zhao, Zijuan and Jia, Liye and Zhao, Juanjuan and Jia, Baoping and Qiang, Yan and Zhao, Huilan and Yue, Huajie},
  doi          = {10.1007/s12559-025-10417-1},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Conditional time series modeling for pneumoconiosis progression risk prediction with missing data},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSAC-UFormer: Groupwise self-attention convolutional
transformer-based UNet for medical image segmentation. <em>CC</em>,
<em>17</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s12559-025-10425-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional transformers struggle to effectively capture local contextual information. Conversely, CNNs face challenges in modeling long-range dependencies. To address these limitations, this paper introduces GSAC-UFormer, an innovative Groupwise Self-Attention Convolutional Transformer-based UNet for medical image segmentation. The design of GSAC-UFormer focuses on efficiently integrating both local and global information, balancing the strengths of different processing techniques. At the core of GSAC-UFormer is the GSAC-Former block. This module combines groupwise convolution with a CNN-adaptive self-attention mechanism, enabling parallel integration of local and global contexts. This architecture allows the model to effectively capture intricate dependencies across various data dimensions while processing local features with high efficiency. The Guided Contextual Feature Attention (GCFA) mechanism further enhances feature selection. It emphasizes the most relevant contextual information, refining spatial and channel-wise relationships in the extracted features. This targeted approach mitigates noise and improves model accuracy. Additionally, the Multi-Depth Partitioned Depthwise Convolution Transformer (MDPDC-Former) serves as a bottleneck module. It optimizes feature mapping and enhances network learning efficiency by dynamically adjusting the receptive field. This enables the model to capture multi-scale semantic information more effectively. Experimental results highlight the superior performance of GSAC-UFormer compared to state-of-the-art methods. It achieves Dice coefficients of 91.6%, 94.61%, and 82.24% on the MICCAI 2017 (red lesion), PH2, and CVC-ClinicalDB datasets, respectively. These results underscore its effectiveness in advancing medical image segmentation.},
  archive      = {J_CC},
  author       = {Garbaz, Anass and Oukdach, Yassine and Charfi, Said and El Ansari, Mohamed and Koutti, Lahcen and Salihoun, Mouna},
  doi          = {10.1007/s12559-025-10425-1},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {GSAC-UFormer: Groupwise self-attention convolutional transformer-based UNet for medical image segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved alternative queuing method of interval-set
dissimilarity measures and possibility degrees for multi-expert
multi-criteria decision-making. <em>CC</em>, <em>17</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s12559-025-10426-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-expert multi-criteria decision-making (MEMCDM) based on interval-set information is novel and valuable, and it already adopts an effective strategy of alternative queuing method (AQM), called AQM-IS. AQM-IS mainly relies on dissimilarity measures and possibility degrees of interval sets, and the two types of uncertainty measures have absolute-quantitative limitations on rough information extraction to imply improvement space. In this work, improved dissimilarity measures and possibility degrees of interval sets are constructed from a better perspective of relative quantization related to systematic structuring and statistical fusion, so improved AQM (called IAQM-IS) is established to advance MEMCDM by using the interval-set information transformation. As bases, relative dissimilarity measures are proposed to modify absolute dissimilarity measures for both interval-set pairs and families on closeness and deviation; thus, relevant internal relationships, mutual sizes, axiomatic properties, and illustrative examples are acquired. Aiming at interval-set information, improved AQM (i.e., IAQM-IS) is investigated for MEMCDM. Concretely, absolute dissimilarity measures are chosen to determine criterion weights based on judgement matrix and maximum deviation, and improved possibility degrees of interval sets are proposed by systematic likelihood characterizations and arithmetic mean combination; using the weight arithmetic mean of improved dissimilarity measures and possibility degrees, a more powerful index for sorting alternatives is generated to formulate IAQM-IS. For algorithmic evaluation, two assessment indices of decision rankings (called separability and goodness) are designed; accordingly, the two algorithms of MEMCDM — AQM-IS and IAQM-IS — are demonstrated and compared via both an applied examples of e-commerce platforms and six simulated experiments of public datasets, and thus the effectiveness and superiority of IAQM-IS are verified. In summary by the double-quantization technique, the improved dissimilarity measures and possibility degrees deepen uncertainty measures of interval-set information tables, and corresponding IAQM-IS has better decision performance than current AQM-IS in specific application scenarios of social cognition.},
  archive      = {J_CC},
  author       = {Xie, Xin and Zhang, Xianyong and Lv, Zhiying and Chen, Jiang},
  doi          = {10.1007/s12559-025-10426-0},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {Improved alternative queuing method of interval-set dissimilarity measures and possibility degrees for multi-expert multi-criteria decision-making},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLKT: Optimizing cognitive load management in knowledge
tracing. <em>CC</em>, <em>17</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s12559-025-10427-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of online adaptive learning, Knowledge Tracing (KT) has become an indispensable component of online education systems. KT assesses the knowledge level of each learner by tracing their learning activities. Managing cognitive load is crucial in the learners’ cognitive process; too low a load may lead to a lack of concentration, while excessively high cognitive load can impede information processing. In pursuit of an ideal learning model, this paper proposes the Cognitive Load-based Knowledge Tracing (CLKT) model. This model employs a Heterogeneous Cognitive Graph Convolutional Network (HCGCN) to extract learners’ knowledge representations and establish connections between learning tasks or instructional resources and learners, providing the model with interpretable learning path recommendations. By introducing the Attention Concentration (AC) mechanism, the model dynamically processes information and efficiently integrates it into learners’ knowledge structures to maintain an appropriate cognitive load level, thus maximizing effective learning. Experiments conducted on the ASSISTMENTS dataset, which contains real-world student interaction data from an online tutoring system, focus on studying the impact of different cognitive loads on the learning process. The experimental results delve into the effects of cognitive load on learner performance, ensuring that learners can engage in learning with appropriate pace and difficulty, thereby enhancing their learning outcomes.},
  archive      = {J_CC},
  author       = {Wu, Qianxi and Ji, Weidong and Zhou, Guohui},
  doi          = {10.1007/s12559-025-10427-z},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {CLKT: Optimizing cognitive load management in knowledge tracing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging graph convolutional networks for semi-supervised
learning in multi-view non-graph data. <em>CC</em>, <em>17</em>(2),
1–15. (<a href="https://doi.org/10.1007/s12559-025-10428-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning with a graph-based approach has gained prominence in machine learning, particularly in scenarios where labeling data involves substantial costs. Graph convolution networks (GCNs) have found widespread application in semi-supervised learning, predominantly on graph-structured data such as citation and social networks. However, a noticeable gap exists in the application of these methods to non-graph multi-view data, such as collections of images. In an effort to address this gap, we introduce two innovative deep semi-supervised multi-view classification models specifically tailored for non-graph data. Both models share a common architecture, leveraging GCNs and integrating a label smoothing constraint. The primary distinction lies in the construction of the consensus similarity graph. The first model directly reconstructs the consensus graph from different views using a specialized objective function designed for flexible graph-based semi-supervised classification. In contrast, the second model independently reconstructs individual graphs and subsequently adaptively merges them into a unified consensus graph. Our experiments encompass various multiple-view image datasets. The results consistently demonstrate the superior performance of our proposed approach compared to traditional fusion methods with GCNs. In this research, we present two approaches for tackling semi-supervised classification challenges involving multiple views. One method is named Semi-supervised Classification with a Unified Graph (SCUG), and the other is referred to as Semi-supervised Classification with a Fused Graph (SC-Fused). Both methods share a common semi-supervised classification process, utilizing the GCN framework and incorporating label smoothing. However, the key distinction lies in the construction of the similarity graph. Unlike traditional ad hoc graph construction approaches, our proposed methods, SCUG and SC-Fused, estimate the unified graph or individual graphs, respectively, alongside the labels. This results in more optimized graphs that benefit from data smoothing and the semi-supervised context.},
  archive      = {J_CC},
  author       = {Dornaika, F. and Bi, J. and Charafeddine, J.},
  doi          = {10.1007/s12559-025-10428-y},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Leveraging graph convolutional networks for semi-supervised learning in multi-view non-graph data},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A holistic comparative study of large language models as
emotional support dialogue systems. <em>CC</em>, <em>17</em>(2), 1–21.
(<a href="https://doi.org/10.1007/s12559-025-10429-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional support conversation aims to convey understanding, sympathy, care, and support through conversation, to help others cope with emotional distress, pressure, or challenges. In this study, we conduct a holistic comparative study to investigate how well the most recent large language models (LLMs), which have recently proved to have empathy, and act as emotional supporters. To this end, we make use of the emotional support conversation (ESC) framework and assess multiple maintain LLMs accordingly. We then have an in-depth comparison between these LLM-based emotional supporters to humans in terms of the use of emotional support strategies as well as the use of language. Surprisingly, we find that there is still a huge gap until these LLMs become effective emotional supporters. This is because, on the one hand, they have strong preference biases on using a limited set of strategies, making them always show empathy but rarely take real actions (such as providing suggestions), which is key in ESC. On the other hand, they often over-generate responses, making what they utter a departure from those of human experts.},
  archive      = {J_CC},
  author       = {Bai, Xin and Chen, Guanyi and He, Tingting and Zhou, Chenlian and Guo, Cong},
  doi          = {10.1007/s12559-025-10429-x},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {A holistic comparative study of large language models as emotional support dialogue systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-enabled multi-layer subword joint learning for
chinese word embedding. <em>CC</em>, <em>17</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s12559-025-10431-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Chinese word embeddings have attracted significant attention in the field of natural language processing (NLP). The complex structures and diverse influences of Chinese characters present distinct challenges for semantic representation. As a result, Chinese word embeddings are primarily investigated in conjunction with characters and their subcomponents. Previous research has demonstrated that word vectors frequently fail to capture the subtle semantics embedded within the complex structure of Chinese characters. Furthermore, they often neglect the varying contributions of subword information to semantics at different levels. To tackle these challenges, we present a weight-based word vector model that takes into account the internal structure of Chinese words at various levels. The model further categorizes the internal structure of Chinese words into six layers of subword information: words, characters, components, pinyin, strokes, and structures. The semantics of Chinese words can be derived by integrating the subword information from various layers. Moreover, the model considers the varying contributions of each subword layer to the semantics of Chinese words. It utilizes an attention mechanism to determine the weights between and within the subword layers, facilitating the comprehensive extraction of word semantics. The word-level subwords act as the attention mechanism query for subwords in other layers to learn semantic bias. Experimental results show that the proposed word vector model achieves enhancements in various evaluation metrics, such as word similarity, word analogy, text categorization, and case studies.},
  archive      = {J_CC},
  author       = {Xue, Pengpeng and Xiong, Jing and Tan, Liang and Liu, Zhongzhu and Liu, Kanglong},
  doi          = {10.1007/s12559-025-10431-3},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Attention-enabled multi-layer subword joint learning for chinese word embedding},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling visual attention based on gestalt theory.
<em>CC</em>, <em>17</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s12559-025-10410-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gestalt theory laid the foundation for modern cognitive learning theory and emphasizes that the whole is greater than the sum of its parts, where similarity and proximity are two important principles. However, exploiting Gestalt theory to detect multiple salient objects remains challenging. In this paper, we propose a very simple yet efficient saliency model based on Gestalt theory, namely, the color similarity and spatial proximity (CSSP) model. It utilizes content-based image retrieval (CBIR) techniques to detect salient objects. The methodology has three important highlights: (1) a novel weighted distance is proposed to calculate spatial proximity. It can control spatial proximity within a certain range and detect salient objects robustly. (2) Two novel and efficient saliency scoring calculation methods are proposed under the framework of CBIR techniques, where color similarity and spatial proximity are used for image matching and the ordering of retrieved images. This enables the robust identification of multiple salient objects. (3) A very simple yet efficient integration method is proposed to combine saliency maps. Using this integration method, impurities around salient objects are greatly reduced, and their interiors are highlighted robustly. Experiments with several well-known benchmark datasets validate the performance of the CSSP model. The CSSP method resulted in fewer grey patches inside salient objects, and it is superior to many existing state-of-the-art methods. The detected salient regions were brighter, improving the effectiveness of multiple salient objects detection. In addition, the CSSP method can detect salient objects robustly even when they touch the image boundaries. It has demonstrated that modeling visual attention based on Gestalt theory is a novel, viable approach.},
  archive      = {J_CC},
  author       = {Liu, Guang-Hai and Yang, Jing-Yu},
  doi          = {10.1007/s12559-025-10410-8},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Modeling visual attention based on gestalt theory},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way decision approach based on utility and dynamic
localization transformational procedures within a circular q-rung
orthopair fuzzy set for ranking and grading large language models.
<em>CC</em>, <em>17</em>(2), 1–26. (<a
href="https://doi.org/10.1007/s12559-025-10432-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have made significant advancements in natural language processing (NLP), impacting both academia and industry. Evaluating LLMs is crucial, as these models are developed for multiple NLP tasks. However, no single LLM has successfully fulfilled all tasks simultaneously, creating a research gap. This, in turn, leads to the identification of the most and least effective LLMs in real-world problems, presenting a multi-criteria decision-making (MCDM) challenge due to the diversity of evaluation tasks, task prioritization, data variability, and issues related to ranking and grading with binary data. While the three-way decision (3WD) approach based on MCDM methods can address this, it often leaves uncertainty as an open issue, highlighting a theoretical gap. To address this, the contribution of this study is the development of a new 3WD approach based on utility and dynamic localization transformational procedures within a circular q-rung orthopair fuzzy set (C-q-ROFS) for ranking and grading LLMs. The methodology includes (1) reformulating the fuzzy weighted zero inconsistency-based interrelationship process (FWZICbIP) using C-q-ROFS (C-q-ROFS–FWZICbIP method) to prioritize tasks and address weighting uncertainty; (2) formulating a decision matrix by intersecting LLMs with NLP tasks while applying utility and dynamic localization procedures to handle binary input issues; and (3) reformulating the conditional probabilities by opinion scores (CPOS) method within the C-q-ROFS context (C-q-ROFS–CPOS method) to determine decision thresholds for each LLM. This involves incorporating Bayesian decision theory under C-q-ROFS to establish decision thresholds for all LLMs, thereby enhancing the certainty and effectiveness of the grading process. Based on this, the 3WD approach is developed to offer a robust mechanism for ranking and grading LLMs. Forty LLMs were ranked and graded across 11 NLP tasks, with the findings showing that LLM14 demonstrated high efficacy, ranking in the positive region for nine σ values, but falling into the boundary region at σ = 0.05. Sensitivity and comparison analyses were conducted to evaluate the robustness and stability of the methodology.},
  archive      = {J_CC},
  author       = {Qahtan, Sarah and Mourad, Nahia and Alsattar, H. A. and Zaidan, A. A. and Zaidan, B. B. and Pamucar, Dragan and Simic, Vladimir and Ding, Weiping and Yatim, Khaironi},
  doi          = {10.1007/s12559-025-10432-2},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Cogn. Comput.},
  title        = {Three-way decision approach based on utility and dynamic localization transformational procedures within a circular q-rung orthopair fuzzy set for ranking and grading large language models},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A communication-efficient distributed frank-wolfe online
algorithm with an event-triggered mechanism. <em>CC</em>,
<em>17</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s12559-025-10438-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed learning is an effective method for solving large-scale cognitively inspired online machine learning problems. However, frequent communications between nodes lead to expensive communication burden. Meanwhile, projection operations because of the constraints in decision variables cause a lot of computational cost. In order to address the above problems, this paper presents a communication-efficient distributed Frank-Wolfe online optimization method, which integrates the event-triggered mechanism into the distributed projection-free online optimization algorithm. Furthermore, we provide a rigorous theoretical analysis for the regret of the proposed algorithm. Finally, we verify the performance of the proposed method through a variety of numerical experiments. The theoretical results show that the regret reaches a sublinear growth of iterations for convex objective functions. The proposed algorithm outperforms the baseline methods on two datasets. Our research indicates that, in addition to reducing computational overhead, the event-triggered scheme has the potential to enhance the communication efficiency of distributed network system.},
  archive      = {J_CC},
  author       = {Gao, Huimin and Liu, Muhua and Ji, Zhihang and Zheng, Ruijuan and Wu, Qingtao},
  doi          = {10.1007/s12559-025-10438-w},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {A communication-efficient distributed frank-wolfe online algorithm with an event-triggered mechanism},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online signature watermarking in the transform domain.
<em>CC</em>, <em>17</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s12559-025-10436-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing reliance on digital signatures for secure authentication and verification necessitates advanced watermarking techniques to protect signature integrity. Transform-domain methods, including the discrete cosine transform (DCT) and the discrete wavelet transform (DWT), are proposed for their potential to balance robustness, imperceptibility, and recognition accuracy in online signature biometrics. This study explores multi-bit watermarking approaches applied to online handwritten signatures using the MCYT signature database. We investigate the effects of embedding multiple bits per sample with adjustable watermark strength ( $$\alpha $$ ) in the DCT and DWT domains. The trade-offs between signal distortion, watermark extraction accuracy, and biometric recognition rates are systematically evaluated. Experimental results reveal that while increasing $$\alpha $$ enhances watermark robustness, it also leads to perceptible distortions in signature samples. We identify the minimum $$\alpha $$ thresholds required for error-free watermark extraction and analyze their impact on identification and verification performance. The proposed multi-bit embedding strategy in the DCT domain demonstrates a viable compromise between robustness and imperceptibility, maintaining acceptable biometric recognition rates. Transform-domain watermarking techniques provide a promising solution for secure and robust online signature biometrics. This study highlights the feasibility of incorporating multi-bit watermarking schemes with adjustable strength into online signature systems, enhancing security while preserving recognition accuracy.},
  archive      = {J_CC},
  author       = {Faundez-Zanuy, Marcos},
  doi          = {10.1007/s12559-025-10436-y},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Online signature watermarking in the transform domain},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rain streak removal using improved generative adversarial
network with loss function optimization. <em>CC</em>, <em>17</em>(2),
1–15. (<a href="https://doi.org/10.1007/s12559-025-10435-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain is a typical meteorological phenomenon that can significantly impair the functionality of outdoor computer vision systems, including autonomous navigation and surveillance. Depending on how far away the streaks are from the camera, they may appear differently in the images. One input image serves as the foundation for the majority of current rain removal techniques. However, estimating a trustworthy depth map for rain removal is challenging on a single image. To overcome these challenges, this research introduces a novel approach for rain streak removal utilizing generative adversarial networks (GANs). Leveraging the discriminative power of GANs, the proposed technique effectively distinguishes between rain streaks and clean image content, resulting in the generation of realistic, rain-free images. The workflow involves initial image pre-processing using a cross-guided bilateral filter for detail layer extraction. The rain streak removal is then executed through an improved de-rain GAN (DR_GAN), where the generator module is replaced with a dense bidirectional network with self-attention (Attn_DBNet). This integration incorporates DenseNet-121, bidirectional gated recurrent unit (BiGRU), and self-attention mechanisms, enhancing the overall performance of the rain streak removal process. The research further introduces chaotic logistic gazelle optimization (CL-G) for optimizing the loss function, addressing local optimal trapping issues through the incorporation of chaotic logistic mapping. With notable gains in the metrics, comparative analysis shows that the proposed method is superior to the state-of-the-art approaches. These successes demonstrate the usefulness and superiority of the proposed GAN-based rain streak removal method over dual CNN, QSAM-Net, and MGPDNet approaches, with significant percentage advantages over these networks.},
  archive      = {J_CC},
  author       = {R, Prabha and R, Suma and Babu D, Suresh and Saila, S},
  doi          = {10.1007/s12559-025-10435-z},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Rain streak removal using improved generative adversarial network with loss function optimization},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). T-norms and t-conorms of symmetrical linear orthopair fuzzy
sets and their cognitive applications in multiple-criteria
decision-making. <em>CC</em>, <em>17</em>(2), 1–29. (<a
href="https://doi.org/10.1007/s12559-025-10439-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthopair fuzzy sets (OFSs) generally include q-rung orthopair fuzzy sets (q-ROFSs) and symmetrical linear orthopair fuzzy sets (SLOFSs), and the latter two models have a common element: intuitionistic fuzzy sets (IFSs). T-norms, t-conorms, and multiple-criteria decision-making (MCDM) are applied to q-ROFSs, but they have not been applied to SLOFSs. These valuable parts of SLOFSs are investigated by extending and simulating relevant results in IFSs, and their operational connections to addition and scalar multiplication are addressed in detail. For SLOFSs, axiomatic definitions, general properties, and concrete constructions for t-norms and t-conorms are first given. Then, special types of t-norms and t-conorms are used to motivate the addition and scalar multiplication operations, and related properties of the operations are obtained. Finally, addition and scalar multiplication are linearly combined with aggregation, and a relevant technique for order preference by similarity to an ideal solution (TOPSIS) method is designed for decision cognition. In this way, a new MCDM method based on SLOFSs is established, and its high reliability is validated by comparing the corresponding method based on q-ROFSs in two practical examples. This study advances work on SLOFSs and linearly extends IFS results, thus enriching OFSs, especially for cognitive computations and applications.},
  archive      = {J_CC},
  author       = {Gao, Shan and Zhang, Xianyong and Mo, Zhiwen},
  doi          = {10.1007/s12559-025-10439-9},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Cogn. Comput.},
  title        = {T-norms and T-conorms of symmetrical linear orthopair fuzzy sets and their cognitive applications in multiple-criteria decision-making},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: A novel interpretable graph convolutional
neural network for multimodal brain tumor segmentation. <em>CC</em>,
<em>17</em>(2), 1. (<a
href="https://doi.org/10.1007/s12559-025-10440-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CC},
  author       = {Choudhry, Imran Arshad and Iqbal, Saeed and Alhussein, Musaed and Aurangzeb, Khursheed and Qureshi, Adnan N. and Hussain, Amir},
  doi          = {10.1007/s12559-025-10440-2},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1},
  shortjournal = {Cogn. Comput.},
  title        = {Correction to: A novel interpretable graph convolutional neural network for multimodal brain tumor segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-criteria group decision-making using complex p,
q-quasirung orthopair fuzzy sets: Application in the selection of
renewable energy projects for investments. <em>CC</em>, <em>17</em>(2),
1–23. (<a href="https://doi.org/10.1007/s12559-025-10424-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the hesitation inherent in decision-making models is a significant hurdle for experts. The p, q-quasirung fuzzy set (p, q-QOFS) is a new development in fuzzy set theory that plays a vital role in addressing these challenges. This article integrates the notions of p, q-QOFSs and complex fuzzy sets (CFSs) to introduce complex p, q-quasirung orthopair fuzzy sets (Cp,qQOFSs) and outline their basic principles. The Cp,qQOFSs expend the range of membership and non-membership degree of p, q-QOFS from real numbers to encompass complex unit disc values. In the first phase, some basic operational laws and their properties are defined. Future, we will use these operational laws to define some aggregation operators such as complex p, q-quasirung orthopair fuzzy weighted averaging and complex p, q-quasirung orthopair fuzzy weighted geometric operators to aggregated complex p, q-quasirung orthopair fuzzy information. Based on these aggregation operators, a new multi-attribute group decision-making (MCGDM) approach is constructed to handle real-life complex decision-making problems. Moreover, the weights of the criteria are calculated using the entropy method. An illustrative numerical example showcasing the proposed MCGDM method has been provided, focusing on renewable energy investments with seven alternatives and five criteria. Additionally, we conduct the sensitivity analysis to demonstrate the impact of parameters p and q over aggregated results. Lastly, the proposed approach is compared with existing methods to highlight its superiority and flexibility.},
  archive      = {J_CC},
  author       = {Rahim, Muhammad and Bajri, Sanaa Ahmed and Alqahtani, Haifa and Alhabeeb, Somayah Abdualziz and Khalifa, Hamiden Abd El-Wahed},
  doi          = {10.1007/s12559-025-10424-2},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Multi-criteria group decision-making using complex p, q-quasirung orthopair fuzzy sets: Application in the selection of renewable energy projects for investments},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medivision: Empowering colorectal cancer diagnosis and tumor
localization through supervised learning classifications and grad-CAM
visualization of medical colonoscopy images. <em>CC</em>,
<em>17</em>(2), 1–39. (<a
href="https://doi.org/10.1007/s12559-025-10433-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medivision is a modern diagnostic system that has been developed to improve colorectal cancer detection.  By applying deep learning acquisitions, including convolutional neural networks, Gray-Level Co-occurrence Matrix feature extraction, and visualization with Grad-CAM. After developing the Medivision system, several colonoscopy image datasets were used for evaluating a number of state-of-the-art deep CNN architectures, such as ResNet50, VGG16, VGG19, and DenseNet201—namely, CVC Clinic DB, Kvasir2, and Hyper Kvasir, however, through exhaustive model comparisons with integrated CNNs DEV-22 and RV-22. By the incorporation of GLCM feature extraction, textual analysis has become enhanced in the capturing of some critical features within an image that enhances model sensitivity in the detection of subtle variation within colorectal polyps. Grad-CAM visualizations enhance interpretability by enabling clinicians to attribute diagnostically important regions and insight into the model’s process of decision-making, incorporated with cloud storage.  This work confirmed the stability and reliability of VGG16 in different conditions of image shooting. Among these, the best performances for all datasets were obtained by the model VGG16, confirming its consistency and robustness in varying conditions of image acquisition. On the CVC Clinic DB, VGG16 assured a training accuracy of 99.22% and a testing accuracy of 96.12%. The model, when trained on the Kvasir2 dataset, achieved an accuracy of 96.56% during training and 94.25% when testing, while on the Hyper Kvasir dataset, the model supported training accuracy of 95.19% and a testing accuracy of 98.87%. Apart from that, VGG16 also showed a good localization capability for the average IoU of 0.78 on CVC Clinic DB, 0.79 on Kvasir2, and 0.77 on Hyper Kvasir, indicating its precision in identifying polyp regions. It also tested the performance of integrated CNN models DEV-22 and RV-22 in complex multi-dataset scenarios. DEV-22 gave the best-performing integrated model against test accuracies of 97.86% against CVC Clinic DB, 89.37% against Kvasir2, and 76.08% against Hyper Kvasir, while RV-22 resulted in relatively poor performance across these datasets.This indicates that DEV-22 might be much better suited for colorectal cancer detection tasks whenever multiple datasets are concerned. The high testing accuracy and precise localization capabilities of VGG16 and DEV-22 show their robustness within the Medivision system for delivering appropriate clinically relevant returns on colorectal cancer screening across all three datasets.  Medivision enables real-time analysis in an accessible and efficient way for healthcare providers. This paper presents the clinical relevance of this system in enhancing diagnostic accuracy and interpretation for colorectal cancer screening workflows. It represents a state-of-the-art integration of high-performance deep CNN models, GLCM, and Grad-CAM for accurate, interpretable, and actionable outcomes in the diagnosis of colorectal cancer, by representing one great bound in AI-mediated medical diagnosis.},
  archive      = {J_CC},
  author       = {Raju, Akella S. Narasimha and Venkatesh, K and Gatla, Ranjith Kumar and Hussain, Shaik Jakeer and Polamuri, Subba Rao},
  doi          = {10.1007/s12559-025-10433-1},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-39},
  shortjournal = {Cogn. Comput.},
  title        = {Medivision: Empowering colorectal cancer diagnosis and tumor localization through supervised learning classifications and grad-CAM visualization of medical colonoscopy images},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-valued neutrosophic distance measure-based
MEREC-RANCOM-WISP for solving sustainable energy storage technology
problem. <em>CC</em>, <em>17</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s12559-025-10437-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy storage technology (EST) is crucial in mitigating the environmental impact of energy storage and reducing carbon footprints. It is a vital component of renewable energy sources and decarbonization of world energy structures. The selection of a suitable EST depends on multiple aspects of sustainability; thus, the decision-making methods are a more proper way to systematically deal with this problem. Uncertainty is commonly occurred in the selection of a suitable EST. As a generalization of a fuzzy set, a single-valued neutrosophic set (SVNS) has been demonstrated as a useful framework for handling indeterminate, inconsistent, and uncertain data of realistic decision-making situations. Considering the idea of SVNS, this paper develops a hybrid multi-criteria group decision-making (MCGDM) approach to assess and prioritize ESTs over different qualitative and quantitative criteria. The proposed “simple weighted sum product (WISP)” method combines the Hellinger distance measure-based decision experts’ weighting tool and integrated criteria weight-determining model to deal the single-valued neutrosophic information (SVNI)-based MCGDM problems with completely unknown weights of decision experts (DEs) as well as defined criteria. To evade the drawbacks of extent distance measures, we introduce a novel single-valued neutrosophic Hellinger distance measure to compute the degree of discrimination on SVNSs. Some illustrative examples are taken to exemplify the efficiency of developed Hellinger distances over existing ones. Further, the developed Hellinger distance is utilized to derive the weight of DEs. In addition, the criteria weight-determining procedure is given by integrating objective weight through a “method based on the removal effects of criteria (MEREC)” and subjective weight through “ranking comparison (RANCOM)” under SVNI. Based on these models, a combined WISP approach is developed to rank the alternatives on SVNI. The proposed ranking framework is employed in an empirical study of the EST selection problem, which shows its practicality and feasibility. In this study, the evaluation criteria are categorized into technical, environmental, social, economic, and performance dimensions with the DEs’ opinions. Comparative and sensitivity analyses are made to approve the validity and stability of the developed ranking approach. The present study offers valuable insights for choosing multi-criteria EST under an indeterminate, inconsistent, and uncertain environment, which also expands the application scopes of the combined MEREC-SWARA-WISP method.},
  archive      = {J_CC},
  author       = {Mishra, Arunodaya Raj and Pamucar, Dragan and Rani, Pratibha and Hezam, Ibrahim M.},
  doi          = {10.1007/s12559-025-10437-x},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {Single-valued neutrosophic distance measure-based MEREC-RANCOM-WISP for solving sustainable energy storage technology problem},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curriculum-guided self-supervised representation learning of
dynamic heterogeneous networks. <em>CC</em>, <em>17</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s12559-025-10441-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since most real-world network data include nodes and edges that evolve gradually, an embedding model for dynamic heterogeneous networks is crucial for network analysis. Transformer models have remarkable success in natural language processing but are rarely applied to learning representations of dynamic heterogeneous networks. In this study, we propose a new transformer model (DHG-BERT) that (i) constructs a dataset based on a network curriculum and (ii) includes pre/post-learning through self-supervised learning. Our proposed model learns complex relationships by leveraging an easier understanding of relationships through data reconstruction. Additionally, we use self-supervised learning to learn network structural features and temporal changes in structure and then fine-tune the proposed model by focusing on specific meta-paths by considering domain characteristics or target tasks. We evaluated the quality of the vector representation produced by the proposed transducer model using real bibliographic networks. Our model achieved an average accuracy of 0.94 in predicting research collaboration between researchers, outperforming existing models by a minimum of 0.13 and a maximum of 0.35. As a result, we confirmed that DHG-BERT is an effective transformer model tailored to dynamic heterogeneous network embeddings. Our study highlights the model’s ability to understand complex network relationships and appropriately capture the structural nuances and temporal changes inherent in networks. This study provides future research directions for applying the transformer model to real-world network data and a new approach to analyzing dynamic heterogeneous networks using transformers.},
  archive      = {J_CC},
  author       = {Jung, Namgyu and Camacho, David and Choi, Chang and Lee, O.-Joun},
  doi          = {10.1007/s12559-025-10441-1},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Curriculum-guided self-supervised representation learning of dynamic heterogeneous networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). <span class="math display"><em>A</em><sup>2</sup></span> DM:
Enhancing EEG artifact removal by fusing artifact representation into
the time-frequency domain. <em>CC</em>, <em>17</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s12559-025-10442-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electroencephalogram (EEG) provides essential data for analyzing brain activities. However, artifacts such as electrooculography (EOG) and electromyography (EMG) often interleave with the EEG signals, significantly affecting the quality of EEG signal analysis. The heterogeneous distribution of these artifacts in the time-frequency domain makes it challenging to remove multiple artifacts using a unified model. In this paper, we propose an artifact-aware EEG denoising model, referred to as $$A^2$$ DM, to effectively remove various types of artifacts in a unified manner. We first obtain an artifact representation that indicates the type of artifact from a pre-trained artifact classification model. This artifact representation is then used as prior knowledge, which is fused into the denoising model in the time-frequency domain. This enables the model to become aware of the artifact type and precisely remove artifacts based on their type. Due to the heterogeneous distributions of artifacts in the frequency domain, we introduce a frequency enhancement module that can identify specific types of artifacts based on their representation and remove them using a hard attention mechanism. Additionally, we design a time-domain compensation module to enhance the denoising capability of $$A^2$$ DM by compensating for potential losses of global information. Comprehensive experiments demonstrate that $$A^2$$ DM significantly outperforms the novel CNN in denoising EEG signals, showing a notable 12% improvement in correlation coefficient (CC) metrics. This work demonstrates that artifact representation can be used in artifact removal models to effectively remove multiple types of artifacts.},
  archive      = {J_CC},
  author       = {Li, Haoran and Feng, Fan and Kang, Jiarong and Zhang, Jin and Gong, Xiaoli and Lu, Tingjuan and Li, Shuang and Sun, Zhe and Solé-Casals, Jordi},
  doi          = {10.1007/s12559-025-10442-0},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {$$A^{2}$$ DM: Enhancing EEG artifact removal by fusing artifact representation into the time-frequency domain},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of hybrid and ensemble machine learning
approaches in predicting football player transfer values. <em>CC</em>,
<em>17</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s12559-025-10443-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In football economics, a player’s transfer market value extends beyond performance metrics, with popularity playing a crucial role in clubs’ decisions. Reputation indexes, reflecting a player’s standing in the industry, are derived from various sources. Traditional metrics include goals, assists, and defensive prowess, while social media activity (likes on Facebook and Instagram), press citations, and Wikipedia page views add a new dimension. This study utilized Fédération Internationale de Football Association 19 data and a real-world statistical dataset, encompassing 54 features for 491 players across various leagues. After adding valuable data and removing ineffective features and outliers, two filtering-based feature selection methods identified the 20 most critical features for predicting market value. The study applied Extreme Gradient Boosting and Adaptive Boosting regression models, along with their hybrid forms optimized by metaheuristic algorithms. The Extreme Gradient Boosting optimized with the Ali Baba and Forty Thieves algorithm model showed the best performance, with a 99% match to actual values and a misestimation of around €1.9 million. Ensemble models, averaging predictions from all hybrid models, provided reliable market value estimates. These insights help managers make informed decisions to improve team performance and secure financial benefits for the club.},
  archive      = {J_CC},
  author       = {Zhang, Wenjing and Cao, Dan},
  doi          = {10.1007/s12559-025-10443-z},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Comparative analysis of hybrid and ensemble machine learning approaches in predicting football player transfer values},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
