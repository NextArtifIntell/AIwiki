<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JMUI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jmui---7">JMUI - 7</h2>
<ul>
<li><details>
<summary>
(2025). Impact of communication modalities on social presence and
regulation processes in a collaborative game. <em>JMUI</em>,
<em>19</em>(1), 101–118. (<a
href="https://doi.org/10.1007/s12193-024-00450-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, leveraging communication technologies to foster collaborative learning is of utmost importance. This study explores the impact of different communication modalities, such as text, audio and video, on social presence and regulation processes within a computer-supported collaborative learning (CSCL) environment. Using learning analytics, we examine the influences of these modalities on collaboration and derive recommendations for their optimized use in the design of future CSCL environments. Our findings reveal a significant impact of communication modalities on the sense of social presence and regulation of collaborative activities. Audio communication results in enhanced co-presence, psychobehavioral accessibility, and better regulation processes compared to video and text modalities, indicating that audio is the most suitable modality in collaborative virtual environments for decision-making tasks. Conversely, video communication still facilitated strategic planning and enhanced self-regulation. Chat communication showed the lowest sense of social presence, yet improvements over time suggest that participants adapt to this modality, enhancing their collaborative efficiency.},
  archive      = {J_JMUI},
  author       = {Basille, Anthony and Lavoué, Élise and Serna, Audrey},
  doi          = {10.1007/s12193-024-00450-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {101-118},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Impact of communication modalities on social presence and regulation processes in a collaborative game},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vibration feedback reduces perceived difficulty of
virtualized fine motor task. <em>JMUI</em>, <em>19</em>(1), 93–99. (<a
href="https://doi.org/10.1007/s12193-024-00449-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has been increasingly used in the development or rehabilitation of sensorimotor skills as it provides a safe, personalized, repeatable, realistic, and interactive environment. However, the use of VR technology to simulate fine motor interactions is still rather limited. This study evaluated the performance and user experience of a virtualized fine motor task and the potential impact of vibration feedback to complement the VR simulation. The Nine Hole Peg test (NHPT), which is widely used in health care to assess hand motor functions, was considered. 100 healthy subjects were recruited to compare the performance of the conventional, VR-based, and VR-based with vibration feedback (VR+vibration) implementation of the NHPT. Results demonstrated a significant increase in the task execution time (about 50% increase) in VR-based and VR+vibration conditions as compared to the conventional condition (Kruskal Wallis test, Bonferroni correction, p &lt; 0.0001). Participants reported a significant decrease in perceived difficulty of the VR+vibration condition as compared to the VR-based condition (Wilcoxon signed-rank test, p &lt; 0.05). Another interesting finding was the gender effect - female participants spent significantly more time completing the task in VR as compared to their male counterparts. These results indicate that vibration feedback enhances the usability of virtualized fine motor tasks.},
  archive      = {J_JMUI},
  author       = {Park, Wanjoo and Jamil, Muhammad Hassan and Eid, Mohamad},
  doi          = {10.1007/s12193-024-00449-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {93-99},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Vibration feedback reduces perceived difficulty of virtualized fine motor task},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pointing gestures accelerate collaborative problem-solving
on tangible user interfaces. <em>JMUI</em>, <em>19</em>(1), 75–92. (<a
href="https://doi.org/10.1007/s12193-024-00448-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaboration is included in the Learning and Innovation skills of the 21st Century. Collaborative problem-solving represents the interaction of two dimensions i) complex problem-solving and ii) collaboration. Technology-based assessment of collaborative problem-solving should focus on both dimensions. We ran user studies with 66 participants at three secondary schools in Luxembourg and Belgium to observe the gestural user behaviour of triads while solving a collaborative problem on a tangible user interface (TUI). Social interactions and embodiment by using gestures are important collaboration channels. Our main objective is the relation between the usage of gestures with collaboration and complex problem-solving performance. Our apparatus to test collaborative problem-solving is a tangible tabletop and a micro-world about power plants. We analysed the videos manually and found correlations between gestures, complex problem-solving performance, and user experience. The results showed that pointing gestures and adaptors significantly correlate with response time of problem-solving. Our results on user experience showed that the use of a TUI was regarded as a novel and straightforward solution that many people could learn to use very quickly. We suggest gesture performance to be considered as one indicator of collaboration.},
  archive      = {J_JMUI},
  author       = {Anastasiou, Dimitra and Maquil, Valérie},
  doi          = {10.1007/s12193-024-00448-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {75-92},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Pointing gestures accelerate collaborative problem-solving on tangible user interfaces},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented conversations: AR face filters for facilitating
comfortable in-person interactions. <em>JMUI</em>, <em>19</em>(1),
57–74. (<a href="https://doi.org/10.1007/s12193-024-00446-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals with social anxiety often experience heightened anxiety during face-to-face conversations due to their fear of negative judgments when perceiving neutral facial expressions from others. This research aims to alleviate this anxiety by introducing a novel approach that involves overlaying visual effects onto the conversation partner via an augmented reality head-mounted display. We developed an AR application for HoloLens 2, allowing users to overlay either an anime-style avatar or a smiling face photo during in-person interactions. We conducted a user study where participants engaged in dyadic conversations using our AR application. 29 participants compared three conditions: control, anime-style avatar, and smiling face photo. The findings reveal two significant outcomes: (1) overlaying an anime-style avatar onto the conversation partner enhances conversational comfort, and (2) individuals with pronounced social interaction anxiety and intense fear of negative evaluation benefit from our AR-based system. This research presents possibilities for practical solutions that could improve the well-being of individuals with social anxiety during in-person conversations.},
  archive      = {J_JMUI},
  author       = {Yoneyama, Juri and Fujimoto, Yuichiro and Okazaki, Kosuke and Sawabe, Taishi and Kanbara, Masayuki and Kato, Hirokazu},
  doi          = {10.1007/s12193-024-00446-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {57-74},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Augmented conversations: AR face filters for facilitating comfortable in-person interactions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effects of haptic, visual and olfactory augmentations on
food consumed while wearing an extended reality headset. <em>JMUI</em>,
<em>19</em>(1), 37–55. (<a
href="https://doi.org/10.1007/s12193-024-00447-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current food production system is unsustainable, necessitating a shift towards plant-based diets. Nutritious options fulfill basic needs but may not satisfy hedonic ones. Our novel approach is to promote healthier eating habits without compromising on the pleasantness of eating by using extended reality technologies and multimodal interaction. We present a multisensory augmentation system integrating augmentations in olfaction, touch, and vision. We studied the experience of eating plant-based balls and meatballs. In an experiment with 40 participants, haptic and visual augmentations were found to have significant effects: augmented meatballs and plant-based balls were perceived as bigger and heavier compared to non-augmented versions. However, olfactory augmentation did not produce a similar effect: participants did not notice a stronger aroma with augmented balls compared to non-augmented balls, and the augmented plant-based version had a less appealing scent than its non-augmented counterpart. Moreover, the findings of the study indicate that our multisensory augmentation system had no significant effect on taste perception.},
  archive      = {J_JMUI},
  author       = {Karhu, Natalia and Rantala, Jussi and Farooq, Ahmed and Sand, Antti and Pennanen, Kyösti and Lappi, Jenni and Nayak, Mohit and Sozer, Nesli and Raisamo, Roope},
  doi          = {10.1007/s12193-024-00447-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {37-55},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The effects of haptic, visual and olfactory augmentations on food consumed while wearing an extended reality headset},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and testing of (a)MICO: A multimodal feedback system
to facilitate the interaction between cobot and human operator.
<em>JMUI</em>, <em>19</em>(1), 21–36. (<a
href="https://doi.org/10.1007/s12193-024-00444-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work describes the design, development and testing of a multimodal feedback system, named (A)MICO, with visual and acoustic feedback designed to facilitate the interaction of workers with collaborative robots (cobots) in production lines. The feedback is designed to make the human operator more aware of the cobot’s ongoing and future activities, and therefore gain more control over the situation. The ultimate goal is to obtain a new intuitive mode for transferring information through the combination of lights and sounds, not only to facilitate the flow of communication from the cobot to the operator, but also to make the interaction more accessible to neurodivergent groups, such as people with autism spectrum disorders. The design process focused on the evaluation of the human–robot interaction to select the situations where additional information is needed, and which is the best way to transfer messages as intuitively as possible. Potential end-users were actively involved during all stages of the design and development process. Five volunteers with high functioning autism participated in a preliminary co-design to identify the issues related to the interaction with the cobot and the logic of the multimodal signals. Then, to assess the system’s adaptability to several needs and the level of usability in providing information, validation tests were carried out involving a wider group of participants with ASD. The results suggest that the adoption of a multimodal communication strategy can be useful for making the workplace accessible and improving the well-being of all workers.},
  archive      = {J_JMUI},
  author       = {Dei, Carla and Meregalli Falerni, Matteo and Cilsal, Turgut and Redaelli, Davide Felice and Lavit Nicora, Matteo and Chiappini, Mattia and Storm, Fabio Alexander and Malosio, Matteo},
  doi          = {10.1007/s12193-024-00444-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {21-36},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Design and testing of (A)MICO: A multimodal feedback system to facilitate the interaction between cobot and human operator},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of comparative evaluation techniques for signing
agents: A study with deaf adults. <em>JMUI</em>, <em>19</em>(1), 1–19.
(<a href="https://doi.org/10.1007/s12193-024-00442-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign languages are considered fully-fledged and complete natural languages that are utilized by individuals who are deaf or hard of hearing as a means of communication within the visual-gestural modality. The utilization of virtual avatars as virtual assistants has witnessed a notable surge over the course of the previous fifteen years. Research on sign language recognition has already shown significant potential in achieving reliable and efficient automatic sign language recognition. Nevertheless, the development of physiologically believable (naturally looking) sign language synthesis and generation techniques is currently in its nascent stages. Moreover, traditional models often are rule-based, rely on manually programmed commands, and require the expertise of proficient interpreters, whereas data-driven approaches have the potential to offer more advanced solutions. In addition to the advancement of sign language systems, scholarly investigations indicate a notable lack in the signing systems evaluation by individuals who utilize sign language (deaf signers and interpreters). In this study, we introduce a sign language interpreting avatar based on data-driven techniques. Additionally, we conduct a subjective evaluation of the avatar’s performance. This paper presents the findings of a study conducted with deaf signers, which aimed to compare three different signing agents to a highly skilled sign language human interpreter. The study utilized well-known metrics that are considered to provide valuable insights into participants’ perceptions of signing agents, also their respective advantages and limitations.},
  archive      = {J_JMUI},
  author       = {Imashev, Alfarabi and Oralbayeva, Nurziya and Baizhanova, Gulmira and Sandygulova, Anara},
  doi          = {10.1007/s12193-024-00442-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Assessment of comparative evaluation techniques for signing agents: A study with deaf adults},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
