<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DMKD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dmkd---8">DMKD - 8</h2>
<ul>
<li><details>
<summary>
(2025). Multilayer horizontal visibility graphs for multivariate
time series analysis. <em>DMKD</em>, <em>39</em>(3), 1–42. (<a
href="https://doi.org/10.1007/s10618-025-01089-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series analysis is a vital but challenging task, with multidisciplinary applicability, tackling the characterization of multiple interconnected variables over time and their dependencies. Traditional methodologies often adapt univariate approaches or rely on assumptions specific to certain domains or problems, presenting limitations. A recent promising alternative is to map multivariate time series into high-level network structures such as multiplex networks, with past work relying on connecting successive time series components with interconnections between contemporary timestamps. In this work, we first define a novel cross-horizontal visibility mapping between lagged timestamps of different time series and then introduce the concept of multilayer horizontal visibility graphs. This allows describing cross-dimension dependencies via inter-layer edges, leveraging the entire structure of multilayer networks. To this end, a novel parameter-free topological measure is proposed and common measures are extended for the multilayer setting. Our approach is general and applicable to any kind of multivariate time series data. We provide an extensive experimental evaluation with both synthetic and real-world datasets. We first explore the proposed methodology and the data properties highlighted by each measure, showing that inter-layer edges based on cross-horizontal visibility preserve more information than previous mappings, while also complementing the information captured by commonly used intra-layer edges. We then illustrate the applicability and validity of our approach in multivariate time series mining tasks, showcasing its potential for enhanced data analysis and insights.},
  archive      = {J_DMKD},
  author       = {Freitas Silva, Vanessa and Silva, Maria Eduarda and Ribeiro, Pedro and Silva, Fernando},
  doi          = {10.1007/s10618-025-01089-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-42},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multilayer horizontal visibility graphs for multivariate time series analysis},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forming coordinated teams that balance task coverage and
expert workload. <em>DMKD</em>, <em>39</em>(3), 1–37. (<a
href="https://doi.org/10.1007/s10618-025-01090-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a new formulation of the team-formation problem, where the goal is to form teams to work on a given set of tasks requiring different skills. Deviating from the classic problem setting where one is asking to cover all skills of each given task, we aim to cover as many skills as possible while also trying to minimize the maximum workload among the experts. We do this by combining penalization terms for the coverage and load constraints into one objective. We call the corresponding assignment problem Balanced-Coverage, and show that it is $$\textbf{NP}$$ -hard. We also consider a variant of this problem, where the experts are organized into a graph, which encodes how well they work together. Utilizing such a coordination graph, we aim to find teams to assign to tasks such that each team’s radius does not exceed a given threshold. We refer to this problem as Network-Balanced-Coverage. We develop a generic template algorithm for approximating both problems in polynomial time, and we show that our template algorithm for Balanced-Coverage has provable guarantees. We describe a set of computational speedups that we can apply to our algorithms and make them scale for reasonably large datasets. From the practical point of view, we demonstrate how to efficiently tune the two parts of the objective and tailor their importance to a particular application. Our experiments with a variety of real-world datasets demonstrate the utility of our problem formulation as well as the efficiency of our algorithms in practice.},
  archive      = {J_DMKD},
  author       = {Vombatkere, Karan and Gionis, Aristides and Terzi, Evimaria},
  doi          = {10.1007/s10618-025-01090-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-37},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Forming coordinated teams that balance task coverage and expert workload},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient outlier detection in numerical and categorical
data. <em>DMKD</em>, <em>39</em>(3), 1–46. (<a
href="https://doi.org/10.1007/s10618-024-01084-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to spot outliers in a large, unlabeled dataset with both numerical and categorical attributes? How to do it in a fast and scalable way? Outlier detection has many applications; it is covered therefore by an extensive literature. The distance-based detectors are the most popular ones. However, they still have two major drawbacks: (a) the intensive neighborhood search that takes hours or even days to complete in large data, and; (b) the inability to process categorical attributes. This paper tackles both problems by presenting HySortOD: a new, fast and scalable detector for numerical and categorical data. Our main focus is the analysis of datasets with many instances, and a low-to-moderate number of attributes. We studied dozens of real, benchmark datasets with up to one million instances; HySortOD outperformed nine competitors from the state of the art in runtime, being up to six orders of magnitude faster in large data, while maintaining high accuracy. Finally, we also performed an extensive experimental evaluation that confirms the ability of our method to obtain high-quality results from both real and synthetic datasets with categorical attributes.},
  archive      = {J_DMKD},
  author       = {Cabral, Eugênio F. and Sánchez Vinces, Braulio V. and Silva, Guilherme D. F. and Sander, Jörg and Cordeiro, Robson L. F.},
  doi          = {10.1007/s10618-024-01084-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-46},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Efficient outlier detection in numerical and categorical data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommending the right academic programs: An interest mining
approach using BERTopic. <em>DMKD</em>, <em>39</em>(3), 1–40. (<a
href="https://doi.org/10.1007/s10618-024-01087-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective students face the challenging task of selecting a university program that will shape their academic and professional careers. For decision-makers and support services, it is often time-consuming and extremely difficult to match personal interests with suitable programs due to the vast and complex catalogue information available. This paper presents the first information system that provides students with efficient recommendations based on both program content and personal preferences. BERTopic, a powerful topic modeling algorithm, is used that leverages text embedding techniques to generate topic representations. It enables us to mine interest topics from all course descriptions, representing the full body of knowledge taught at the institution. Underpinned by the student’s individual choice of topics, a shortlist of the most relevant programs is computed through statistical backtracking in the knowledge map, a novel characterization of the program-course relationship. This approach can be applied to a wide range of educational settings, including professional and vocational training. A case study at a post-secondary school with 80 programs and over 5,000 courses shows that the system provides immediate and effective decision support. The presented interest topics are meaningful, leading to positive effects such as serendipity, personalization, and fairness, as revealed by a qualitative study involving 65 students. Over 98% of users indicated that the recommendations aligned with their interests, and about 94% stated they would use the tool in the future. Quantitative analysis shows the system can be configured to ensure fairness, achieving 98% program coverage while maintaining a personalization score of 0.77. These findings suggest that universities could expand student support services by implementing this real-time, user-centered, data-driven system to improve the program selection process.},
  archive      = {J_DMKD},
  author       = {Hill, Alessandro and Goo, Kalen and Agarwal, Puneet},
  doi          = {10.1007/s10618-024-01087-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-40},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Recommending the right academic programs: An interest mining approach using BERTopic},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-neighbor social recommendation with attentional graph
convolutional network. <em>DMKD</em>, <em>39</em>(3), 1–22. (<a
href="https://doi.org/10.1007/s10618-025-01094-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social network is usually utilized as auxiliary data to alleviate data sparsity and cold start problems of recommender systems. However, the social network obeys the power-law distribution and the social relations are indiscriminately utilized, resulting in providing insufficient and inaccurate information. Early efforts ignore these drawbacks and fail to exploit abundant information fully. In this paper, we propose a novel multi-neighbor social recommendation framework MNGCN based on graph convolutional network. The representations of user and item could be learned by iteratively integrating their multiple types of neighbor information. In addition, we apply a node-level attention mechanism to aggregate the same type of neighbors and a category-level attention mechanism to incorporate different categories of neighbors. A sampler is utilized to accurately select the social neighbors of users with regard to different items. Besides, the interactions and ratings are captured simultaneously in the user-item interactive network. Extensive experiments on two classical datasets illustrate that MNGCN achieves the best performance, and the ablation study demonstrates the necessity and the effectiveness of each component.},
  archive      = {J_DMKD},
  author       = {Zhang, Min and Liao, Xiao and Wang, Xinlei and Wang, Xiaojuan and Jin, Lei},
  doi          = {10.1007/s10618-025-01094-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multi-neighbor social recommendation with attentional graph convolutional network},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextualization of soccer analysis with tactical
periodization and machine learning. <em>DMKD</em>, <em>39</em>(3), 1–34.
(<a href="https://doi.org/10.1007/s10618-025-01092-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has become common practice in topflight leagues to track position data of soccer players and the ball. Analyzing sports performance based on this high-resolution data is a non-trivial task due to the great complexity and simultaneous lack of structure of the game. Sports practitioners tackle this problem through tactical periodization, i.e., mapping the course of the game onto different states, so-called match phases. However, creating manual tactical periodizations is a time-consuming task prone to subjective biases. Automatic approaches are thus preferred, but validated and open match phase models are currently lacking. The present study addresses this issue by (1) formalizing a domain-specific, qualitative match phase annotation scheme from related work, (2) creating and validating a multi-annotator set of annotations, (3) training several supervised machine learning architectures to fully automate the task of annotation, and (4) demonstrating the usefulness by conducting a contextualized detection of playing formations with the best model, referred to as FeatGRU. Steps (2) through (4) were performed on a set of real-world soccer data and the best-performing model is made available. FeatGRU is of value to the soccer community as it provides a fully automatic, frame-by-frame match phase annotation that matches domain experts’ opinions with 80% accuracy while being modular extendable for future work. Moreover, we found a strong relation between semantic complexity of matchphases, expert agreements, and classification performance, highlighting the importance of valid label generation. Thus, our approach presents an interesting benchmark to domains where automatic approaches are required while ambiguity between human expert opinions exists.},
  archive      = {J_DMKD},
  author       = {Biermann, Henrik and Memmert, Daniel and Petersen, Niklas and Raabe, Dominik},
  doi          = {10.1007/s10618-025-01092-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-34},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Contextualization of soccer analysis with tactical periodization and machine learning},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ada-context: Adaptive context-aware grid-based approach for
curation of data streams. <em>DMKD</em>, <em>39</em>(3), 1–30. (<a
href="https://doi.org/10.1007/s10618-025-01095-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing and real-time applications have changed how data is collected and processed. However, data quality is crucial for its usefulness. In this paper, we introduce Ada-Context, an approach that uses external contextual information to improve data quality assessment. It involves offline and online analysis components and uses a grid structure to map streaming data to cells, enhancing performance of quality control in data streams. Results show that contextual data especially external context improves data cleansing accuracy and the grid design boosts quality control effectiveness for data streams.},
  archive      = {J_DMKD},
  author       = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
  doi          = {10.1007/s10618-025-01095-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Ada-context: Adaptive context-aware grid-based approach for curation of data streams},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting anomalies using rotated isolation forest.
<em>DMKD</em>, <em>39</em>(3), 1–31. (<a
href="https://doi.org/10.1007/s10618-025-01096-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Isolation Forest (iForest), proposed by Liu et al. at TKDE 2012, has become a prominent tool for unsupervised anomaly detection. However, recent research by Hariri, Kind, and Brunner, published in TKDE 2021, has revealed issues with iForest. They identified the presence of axis-aligned ghost clusters that can be misidentified as normal clusters, leading to biased anomaly scores and inaccurate predictions. In response, they developed the Extended Isolation Forest (EIF), which effectively solves these issues by eliminating the ghost clusters introduced by iForest. This enhancement results in improved consistency of anomaly scores and superior performance. We reveal a previously overlooked problem in the EIF, showing that it is vulnerable to ghost inter-clusters between normal clusters of data points. In this paper, we introduce the Rotated Isolation Forest (RIF) algorithm which effectively addresses both the axis-aligned ghost clusters observed in iForest and the ghost inter-clusters seen in EIF. RIF accomplishes this by randomly rotating the dataset (using random rotation matrices and QR decomposition) before feeding it into the iForest construction, thereby increasing dataset variation and eliminating ghost clusters. Our experiments conclusively demonstrate that the RIF algorithm outperforms iForest and EIF, as evidenced by the results obtained from both synthetic datasets and real-world datasets.},
  archive      = {J_DMKD},
  author       = {Monemizadeh, Vahideh and Kiani, Kourosh},
  doi          = {10.1007/s10618-025-01096-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detecting anomalies using rotated isolation forest},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
