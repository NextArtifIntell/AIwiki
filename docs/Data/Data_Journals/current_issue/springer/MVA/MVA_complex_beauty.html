<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva---9">MVA - 9</h2>
<ul>
<li><details>
<summary>
(2025). Enforced clustering for zero-to-one-shot texture anomaly
detection. <em>MVA</em>, <em>36</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s00138-025-01670-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on anomaly detection (AD) for industrial products typically address the problem in an unsupervised manner, requiring only normal data for training. This approach alleviates the need for anomalous data but still requires a set of normal samples and often involves demanding computations. More recent methods aim to solve this problem in zero-, one-, or few-shot settings but suffer from performance drops or rely on additional contexts, such as language guidance and text encoding, which add overhead. This paper focuses on homogeneous textures and demonstrates how the problem can be addressed without any training samples or additional training (zero-shot), only requiring one normal sample (one-shot) for hyperparameter selection, which is an additional challenge in unsupervised settings. This is achieved by enforcing K-means clustering with $${K}=2$$ on each of the testing samples independently, distinguishing it from the typical use of clustering methods in outlier detection, which are applied to a set of samples. The confidence score of each locality belonging to the smaller cluster, considered the potential anomalous cluster for evaluation, forms the anomaly map used in anomaly localization, and the maximum values in this map are used in AD. Competitive performance is achieved through the careful selection of the distance metric, feature layers, and clustering method. Experiments show that this zero-to-one-shot method, which facilitates deployment by reducing data dependency, maintains performance comparable to, or even higher than, conventional many-shot methods, all with relatively high speed.},
  archive      = {J_MVA},
  author       = {Amirian Varnousefaderani, Bahar and Rakhmonov, Akhrorjon Akhmadjon Ugli and Kim, Jae-Soo and Kim, Jeonghong},
  doi          = {10.1007/s00138-025-01670-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enforced clustering for zero-to-one-shot texture anomaly detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring filter placement in convolutional layer topologies
based on ResNet for image classification. <em>MVA</em>, <em>36</em>(3),
1–11. (<a href="https://doi.org/10.1007/s00138-025-01674-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate the impact that altering the convolutional layer topology has upon the performance of computer vision tasks using a variety of widely used benchmark image datasets. Despite the widespread convention in convolutional neural networks, of incrementally doubling the filter count at each layer, there is little evidence substantiating the superiority of this method over other possible topologies. Our research reveals that a contrarian strategy—reducing the filters by half—can achieve performance on par with, if not superior to, this usual approach. We have extended our investigation to include a variety of novel topological structures. These empirical results challenge the prevailing assumption, that the sequential doubling of number of filters in the network configuration will always yield the best results with all datasets. Our findings advocate for a more nuanced approach to neural network design, incorporating a flexible approach to filter topologies into workflows. This could potentially have a significant impact upon the architectural standards in deep learning for visual recognition tasks.},
  archive      = {J_MVA},
  author       = {Liu, Haixia and Brailsford, Tim and Bull, Larry},
  doi          = {10.1007/s00138-025-01674-z},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Exploring filter placement in convolutional layer topologies based on ResNet for image classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on SLAM and machine learning
approaches for indoor autonomous navigation of mobile robots.
<em>MVA</em>, <em>36</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01673-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex and dynamic nature of indoor environments presents significant challenges for mobile robots autonomous navigation. Traditional navigation methods, reliant on handcrafted features and algorithms, often struggle to adapt to these challenges. Recently, machine learning techniques have emerged as a promising approach for indoor autonomous navigation, offering the ability to learn from data to extract features and develop robust navigation strategies. This survey presents recent strategies for indoor autonomous navigation of mobile robots, providing a comprehensive overview of traditional methods for indoor autonomous mobile robots simultaneous localization and mapping mapping (SLAM), path planning and obstacle avoidance, and machine learning approaches, including deep learning and reinforcement learning. Furthermore, the paper discusses the specific challenges of indoor autonomous navigation for mobile robots and examines the advantages, challenges, and limitations of applying machine learning techniques in this context. Since performance evaluation is crucial for proving the efficiency of each novel developed algorithm and method, the most important performance evaluation metrics are described and mathematically presented with formulas. A systematic review on recent advances in indoor autonomous mobile robot navigation is further supported by presenting relevant patents on the topic of the paper and the field. Additionally, the survey identifies promising future research directions in machine learning-based indoor autonomous navigation. Last but not least, this survey aims to serve as a valuable resource for researchers and engineers interested in developing advanced and machine learning-based indoor autonomous navigation systems for mobile robots.},
  archive      = {J_MVA},
  author       = {Damjanović, Davor and Biočić, Petar and Prakljačić, Stjepan and Činčurak, Dorian and Balen, Josip},
  doi          = {10.1007/s00138-025-01673-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A comprehensive survey on SLAM and machine learning approaches for indoor autonomous navigation of mobile robots},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive fusion attention for enhanced classification and
interpretability in medical imaging. <em>MVA</em>, <em>36</em>(3), 1–23.
(<a href="https://doi.org/10.1007/s00138-025-01665-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate medical image classification is crucial for effective clinical decision support, improving patient outcomes and reducing healthcare costs. However, developing expert Computer-Aided Diagnosis systems for accurate medical image classification remains a challenging task. Recent advancements in attention mechanisms have revolutionized deep learning-based approaches, leading to improved performance even in applications with limited labeled data. Despite these advances, challenges such as overfitting and poor generalization persist. This work introduces a novel deep learning-based model that incorporates Adaptive Fusion Attention to enhance medical image analysis. The proposed attention module employs a hierarchical fusion of spatial and temporal attention mechanisms, complemented by adaptive refinement. This enables the model to focus on the most discriminative features in medical images, improving its ability to detect abnormalities. Additionally, GRAD-CAM visualizations demonstrate that the model effectively highlights pathological regions while minimizing attention on non-relevant areas. The model is evaluated on three benchmark datasets-APTOS-2019, Figshare, and SARS-CoV-2-demonstrating its effectiveness in Diabetic Retinopathy grading, Brain Tumor Classification, and COVID-19 detection. Experimental results show substantial improvements, achieving 84.56% accuracy for retinopathy grading, 99.60% accuracy for tumor classification, and 99.35% accuracy for COVID-19 detection. These results, along with superior performance across other metrics such as ROC-AUC, and F1-scores, demonstrate the effectiveness of the proposed model with Adaptive Fusion Attention over existing approaches.},
  archive      = {J_MVA},
  author       = {Shaik, Nagur Shareef and Veeranjaneulu, N. and Bodapati, Jyostna Devi},
  doi          = {10.1007/s00138-025-01665-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adaptive fusion attention for enhanced classification and interpretability in medical imaging},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cigarette defect detection algorithm based on attention
mechanism and multi-gradient feature fusion. <em>MVA</em>,
<em>36</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s00138-025-01681-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defect detection remains a persistent and challenging task. Aiming at the detection of surface defects in cigarettes, we propose an enhanced YOLOX-S model. Firstly, an improved attention mechanism named MS-GCT (Multi-Spectral Gaussian Context Transformer) is introduced into the model’s backbone to enhance the model’s ability of capturing the global context information within images and improve its comprehension of semantic feature information; secondly, we propose the DMG (Dynamic convolution and MS-GCT) module, and combined with the C2f (CSPLayer with 2 convolutions) module to construct the C2f-DMG module,which is introduced into the model to enhance feature interaction and feature extraction ability, to strengthen long-distance dependency ability of global features; finally, we replace the loss function with SIoU to enhance model performance and accelerate model convergence. To validate the effectiveness of our model, we conduct experiments on both the self-made cigarette dataset and the public dataset. The experimental results indicate that the improved model not only ensures the lightweight of the model, but also boosts the model’s mAP by 2.02, while achieving a detection speed of 73.17 frames−1. Furthermore, the proposed algorithm fulfills the real-time detection requirements for cigarette appearance defects.},
  archive      = {J_MVA},
  author       = {Shi, Weiya and Zhang, Shiqiang and Zhang, Shaowen},
  doi          = {10.1007/s00138-025-01681-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cigarette defect detection algorithm based on attention mechanism and multi-gradient feature fusion},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D multi-object tracking based on parallel multimodal data
association. <em>MVA</em>, <em>36</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-025-01675-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel 3D multi-object tracker based on tracking-by-detection (TBD) framework. The system leverages parallel feature extraction and data association methods to process 2D appearance features and 3D spatial information, respectively, to achieve accurate tracking of targets in autonomous vehicles and intelligent transportation systems. By combining the Siamese network-based image feature extractor to extract image appearance features and the kinematic model established using Kalman filtering, our tracker effectively utilizes the appearance and spatial information of the object, thus improving tracker accuracy and reliability. Experimental results demonstrate the competitiveness of our tracker on the KITTI tracking benchmark. Compared to previous methods, A parallel feature extraction algorithm is proposed that can independently extract the spatial and appearance information of the object. A novel data association algorithm is designed that makes full use of the spatial information of the object from the point cloud and the appearance information from the image. This work provides substantial technical underpinnings for the advancement of autonomous driving and intelligent transportation technology. https://github.com/TanShiyu2022/PMTrack .},
  archive      = {J_MVA},
  author       = {Tan, Shiyu and Li, Xu and Xu, QiMin and Zhu, Jianxiao},
  doi          = {10.1007/s00138-025-01675-y},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3D multi-object tracking based on parallel multimodal data association},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature distribution statistics as a loss objective for
robust white balance correction. <em>MVA</em>, <em>36</em>(3), 1–20. (<a
href="https://doi.org/10.1007/s00138-025-01680-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White balance (WB) correction is critical for accurate color reproduction in digital images, especially under complex, multi-illuminant lighting conditions. Traditional methods, such as the Gray-World assumption, rely on global statistics and struggle in real-world, non-uniform lighting scenarios. Modern deep learning approaches, including convolutional and attention-based architectures, have significantly advanced WB correction but often fail to explicitly account for higher-order feature distribution statistics, which may limit their robustness in challenging environments. This study introduces a novel framework that leverages Exact Feature Distribution Matching (EFDM) as a loss objective to align feature distributions across multiple moments, including mean, variance, skewness, and kurtosis. By modeling lighting as a style factor, the method explicitly addresses distributional shifts caused by complex illumination, offering a robust solution for WB correction. The framework integrates EFDM with a Vision Transformer architecture, enabling precise handling of global and local lighting variations. Extensive experiments on the large-scale multi-illuminant (LSMI) dataset demonstrate the superiority of the proposed approach over state-of-the-art methods and commonly used loss functions when applied to the same architecture. Qualitative and quantitative evaluations highlight its effectiveness in achieving perceptually accurate WB correction, particularly in multi-illuminant environments. By bridging statistical modeling with modern deep learning, this work establishes the critical role of feature distribution alignment in advancing WB correction and sets a new benchmark for robustness and generalization in complex lighting scenarios.},
  archive      = {J_MVA},
  author       = {Kınlı, Furkan and Kıraç, Furkan},
  doi          = {10.1007/s00138-025-01680-1},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Feature distribution statistics as a loss objective for robust white balance correction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViCap-AD: Video caption-based weakly supervised video
anomaly detection. <em>MVA</em>, <em>36</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s00138-025-01676-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection becomes increasingly critical amidst rising crime rates and concerns for public safety. Traditional unsupervised video anomaly detection methods primarily focused on normal data, limiting their ability to achieve optimal performance due to their inability to effectively utilize abnormal data. Weakly supervised video anomaly detection methods addressed some of these limitations but still struggled to leverage anomalous video labels effectively, often being susceptible to noise in classification scores. In this paper, we propose a novel approach named Video Caption Anomaly Detector (ViCap-AD), which leverages video captions alongside a combination of BERT and the multiple instance learning (MIL) framework for anomaly detection. ViCap-AD integrates video captions generated using CLIP4Clip with video features within the MIL framework augmented by BERT. In our experimental evaluations on the UCF-Crime and XD-Violence datasets, ViCap-AD achieves state-of-the-art performance, achieving AUC scores of 87.20% and 85.02%, respectively. These results underscore the robustness and effectiveness of our approach across different datasets, demonstrating its powerful performance and stability. This paper contributes a significant advancement in anomaly detection methodologies, highlighting the potential of ViCap-AD to enhance anomaly detection accuracy and reliability in real-world applications.},
  archive      = {J_MVA},
  author       = {Lim, Junwoo and Lee, Juyeob and Kim, Hyunji and Park, Eunil},
  doi          = {10.1007/s00138-025-01676-x},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ViCap-AD: Video caption-based weakly supervised video anomaly detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep representation learning for license plate recognition
in low quality video images. <em>MVA</em>, <em>36</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s00138-025-01678-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {License plate recognition is an important technology in many application scenarios such as traffic monitoring and vehicle management. Due to variations in viewpoint, illumination, motion-blur, and degradation during the imaging process, it is still a challenging problem to detect and recognize license plates in low quality video images. In this paper, we focus on efficient deep representation learning for license plate recognition, detection and tracking. For license plate recognition, we mainly investigate the configuration of different network structures. We design a novel backbone network structure called SACNN, which combines convolutional neural network (CNN) and self-attention mechanism to learn non-linear representations for the structural patterns of characters in low quality video images. The proposed license plate recognition model employs the SACNN backbone network, a Long Short-Term Memory (LSTM) encoder and a Transformer decoder. For license plate detection, a Transformer encoder–decoder based method is adopted. To tackle the variations in license plate appearances and perspectives, an image rectification method is incorporated by using a spatial transformer network. For license plate tracking, a multi-object tracking method is incorporated by using Kalman filtering and temporal matching to associate detected license plates in video frames. Experiments are mainly carried out on the public large-scale video-based license plate dataset (LSV-LP) to validate the proposed methods.},
  archive      = {J_MVA},
  author       = {Zhao, Kemeng and Peng, Liangrui and Ding, Ning and Yao, Gang and Tang, Pei and Wang, Shengjin},
  doi          = {10.1007/s00138-025-01678-9},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep representation learning for license plate recognition in low quality video images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
