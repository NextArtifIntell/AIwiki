<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir---3">IJMIR - 3</h2>
<ul>
<li><details>
<summary>
(2025). Cross-modal alignment with synthetic caption for text-based
person search. <em>IJMIR</em>, <em>14</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s13735-025-00356-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search aims to retrieve target person from a large gallery based on natural language description. Existing methods take it as one-to-one embedding or many-to-many embedding matching problem. The former approach relies on the assumption of the existence of strong alignment between text and images, while the latter inevitably leads to issues of intra-class variation. Rather than being confined to these two approaches, we propose a new strategy that achieves cross-modal alignment with synthetic caption for joint image-text-caption optimization, named CASC. The core of this strategy lies in generating fine-grained captions that are informative for multimodal alignment. To realize this, we introduce two novel components: Granularity Awareness Sensor (GAS) and Conditional Contrastive Learning (CCL). GAS selects relative features through an innovative adaptive masking strategy, endowing the model with an enhanced perception of discriminative features. CCL aligns different modalities through further constraints on the synthetic captions by comparing the similarity of hard negative samples, protecting the disruption from noisy contents. With the incorporation of extra caption supervision, the model has access to learn more comprehensive feature representation, which in turn boosts the retrieval performance during inference. Experiments demonstrate that CASC outperforms existing state-of-the-art methods by 1.20%, 2.35% and 2.29% in terms of Rank@1 on CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively.},
  archive      = {J_IJMIR},
  author       = {Zhao, Weichen and Lu, Yuxing and Liu, Zhiyuan and Yang, Yuan and Jiao, Ge},
  doi          = {10.1007/s13735-025-00356-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Cross-modal alignment with synthetic caption for text-based person search},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMFNet: Geometric multi-scale pixel-level contrastive
learning for video salient object detection. <em>IJMIR</em>,
<em>14</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s13735-025-00361-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For video salient object detection (VSOD) tasks, the geometric variations of object foregrounds and backgrounds across multiple scales pose significant challenges for deep learning models in extracting and integrating semantic features from video streams. Current deep learning approaches, such as recurrent neural networks and transformers, struggle to capture both short- and long-term temporal dependencies at a global level due to their fixed kernel structures. Additionally, these methods are computationally intensive, limiting their practical application. To address these challenges and achieve a balance between accuracy and computational efficiency, a novel lightweight Deformable Multi-scale Fusion Network is proposed, which extracts both attention-based multi-scale features and geometric features together to generate the efficient saliency map. Further, the Geometric Multi-Scale Pixel-level Contrastive Learning (GMPCL) approach, which enhances the geometric representation of features is proposed using GMPCL loss and separates the geometric representations of foreground and background features of objects at the pixel level. The performance evaluation is done on six benchmark datasets and compared with twenty-two state-of-the-art (SOTA) models. The main highlight of this work is that it performs well on most challenging datasets DAVSOD-Difficult as compared to SOTA models and has 6.2 million network parameters, 5.6 G FLOPS, and 90 FPS inference speed.},
  archive      = {J_IJMIR},
  author       = {Singh, Hemraj and Verma, Mridula and Cheruku, Ramalingaswamy},
  doi          = {10.1007/s13735-025-00361-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {DMFNet: Geometric multi-scale pixel-level contrastive learning for video salient object detection},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concept-based and embedding-based models in lifelog
retrieval: An empirical comparison of performance. <em>IJMIR</em>,
<em>14</em>(2), 1–9. (<a
href="https://doi.org/10.1007/s13735-025-00359-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many lifelog retrieval systems have been introduced that apply various approaches to their search engines. The traditional method was to match concepts, which are visual objects detected in images and semantic queries. This concept-based approach has been applied in many retrieval systems, achieving the top performance in lifelog search challenges. Many novel embedding-based cross-modality retrieval models, such as CLIP, BLIP, or HADA, have been developed recently and obtained state-of-the-art (SOTA) results in the image-text retrieval task. These models have recently been applied in several lifelog search challenges. However, there is no comprehensive comparison between them since many benchmarking evaluations contain bias factors such as different user interfaces of participated lifelog retrieval systems. In this paper, we conducted non-biased experiments in both automatic (non-interactive) and interactive configurations to evaluate the performance of many SOTA retrieval models, including the traditional concept-based approach, in the lifelog retrieval task. Furthermore, we retrained the models in a lifelog Q&amp;A dataset to assess whether retraining on a small lifelog dataset could improve the performance. The result showed that embedding-based search engines outperformed the concept-based approach by a large margin in both settings. The finding opens the opportunity to apply the embedding-based models as a new generation of lifelog retrieval models instead of the conventional concept-based approach. The source code and detailed result are available online https://github.com/m2man/Comparing-models-in-Lifelog-Retrieval-Task .},
  archive      = {J_IJMIR},
  author       = {Nguyen, Manh-Duy and Nguyen, Binh T. and Gurrin, Cathal},
  doi          = {10.1007/s13735-025-00359-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Concept-based and embedding-based models in lifelog retrieval: An empirical comparison of performance},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
