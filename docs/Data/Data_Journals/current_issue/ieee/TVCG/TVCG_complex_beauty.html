<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---21">TVCG - 21</h2>
<ul>
<li><details>
<summary>
(2025). Errata to “depth perception in optical see-through augmented
reality: Investigating the impact of texture density, luminance
contrast, and color contrast.” <em>TVCG</em>, <em>31</em>(4), 2257. (<a
href="https://doi.org/10.1109/TVCG.2025.3531019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In This paper, the information regarding the corresponding authors is missing. The corresponding authors of the paper should be Shining Ma and Weitao Song.},
  archive      = {J_TVCG},
  author       = {Chaochao Liu and Shining Ma and Yue Liu and Yongtian Wang and Weitao Song},
  doi          = {10.1109/TVCG.2025.3531019},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2257},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Errata to “Depth perception in optical see-through augmented reality: Investigating the impact of texture density, luminance contrast, and color contrast”},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IVESA – visual analysis of time-stamped event sequences.
<em>TVCG</em>, <em>31</em>(4), 2235–2256. (<a
href="https://doi.org/10.1109/TVCG.2024.3382760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-stamped event sequences (TSEQs) are time-oriented data without value information, shifting the focus of users to the exploration of temporal event occurrences. TSEQs exist in application domains, such as sleeping behavior, earthquake aftershocks, and stock market crashes. Domain experts face four challenges, for which they could use interactive and visual data analysis methods. First, TSEQs can be large with respect to both the number of sequences and events, often leading to millions of events. Second, domain experts need validated metrics and features to identify interesting patterns. Third, after identifying interesting patterns, domain experts contextualize the patterns to foster sensemaking. Finally, domain experts seek to reduce data complexity by data simplification and machine learning support. We present IVESA, a visual analytics approach for TSEQs. It supports the analysis of TSEQs at the granularities of sequences and events, supported with metrics and feature analysis tools. IVESA has multiple linked views that support overview, sort+filter, comparison, details-on-demand, and metadata relation-seeking tasks, as well as data simplification through feature analysis, interactive clustering, filtering, and motif detection and simplification. We evaluated IVESA with three case studies and a user study with six domain experts working with six different datasets and applications. Results demonstrate the usability and generalizability of IVESA across applications and cases that had up to 1,000,000 events.},
  archive      = {J_TVCG},
  author       = {Jürgen Bernard and Clara-Maria Barth and Eduard Cuba and Andrea Meier and Yasara Peiris and Ben Shneiderman},
  doi          = {10.1109/TVCG.2024.3382760},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2235-2256},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IVESA – visual analysis of time-stamped event sequences},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards quantum ray tracing. <em>TVCG</em>, <em>31</em>(4),
2223–2234. (<a href="https://doi.org/10.1109/TVCG.2024.3386103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering on conventional computers is capable of generating realistic imagery, but the computational complexity of these light transport algorithms is a limiting factor of image synthesis. Quantum computers have the potential to significantly improve rendering performance through reducing the underlying complexity of the algorithms behind light transport. This article investigates hybrid quantum-classical algorithms for ray tracing, a core component of most rendering techniques. Through a practical implementation of quantum ray tracing in a 3D environment, we show quantum approaches provide a quadratic improvement in query complexity compared to the equivalent classical approach. Based on domain specific knowledge, we then propose algorithms to significantly reduce the computation required for quantum ray tracing through exploiting image space coherence and a principled termination criteria for quantum searching. We show results obtained using a simulator for both Whitted style ray tracing, and for accelerating ray tracing operations when performing classical Monte Carlo integration for area lights and indirect illumination.},
  archive      = {J_TVCG},
  author       = {Luís Paulo Santos and Thomas Bashford-Rogers and João Barbosa and Paul Navrátil},
  doi          = {10.1109/TVCG.2024.3386103},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2223-2234},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards quantum ray tracing},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualization for diagnostic review of copy number variants
in complex DNA sequencing data. <em>TVCG</em>, <em>31</em>(4),
2211–2222. (<a href="https://doi.org/10.1109/TVCG.2024.3385118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genomics is at the core of precision medicine, and there are high expectations on genomics-enabled improvement of patient outcomes in the years to come. Around the world, initiatives to increase the use of DNA sequencing in clinical routine are being deployed, such as the use of broad panels in the standard care for oncology patients. Such a development comes at the cost of increased demands on throughput in genomic data analysis. In this paper, we use the task of copy number variant (CNV) analysis as a context for exploring visualization concepts for clinical genomics. CNV calls are generated algorithmically, but time-consuming manual intervention is needed to separate relevant findings from irrelevant ones in the resulting large call candidate lists. We present a visualization environment, named Copycat, to support this review task in a clinical scenario. Key components are a scatter-glyph plot replacing the traditional list visualization, and a glyph representation designed for at-a-glance relevance assessments. Moreover, we present results from a formative evaluation of the prototype by domain specialists, from which we elicit insights to guide both prototype improvements and visualization for clinical genomics in general.},
  archive      = {J_TVCG},
  author       = {Emilia Ståhlbom and Jesper Molin and Claes Lundström and Anders Ynnerman},
  doi          = {10.1109/TVCG.2024.3385118},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2211-2222},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization for diagnostic review of copy number variants in complex DNA sequencing data},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bimodal visualization of industrial x-ray and neutron
computed tomography data. <em>TVCG</em>, <em>31</em>(4), 2196–2210. (<a
href="https://doi.org/10.1109/TVCG.2024.3382607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced manufacturing creates increasingly complex objects with material compositions that are often difficult to characterize by a single modality. Our collaborating domain scientists are going beyond traditional methods by employing both X-ray and neutron computed tomography to obtain complementary representations expected to better resolve material boundaries. However, the use of two modalities creates its own challenges for visualization, requiring either complex adjustments of bimodal transfer functions or the need for multiple views. Together with experts in nondestructive evaluation, we designed a novel interactive bimodal visualization approach to create a combined view of the co-registered X-ray and neutron acquisitions of industrial objects. Using an automatic topological segmentation of the bivariate histogram of X-ray and neutron values as a starting point, the system provides a simple yet effective interface to easily create, explore, and adjust a bimodal visualization. We propose a widget with simple brushing interactions that enables the user to quickly correct the segmented histogram results. Our semiautomated system enables domain experts to intuitively explore large bimodal datasets without the need for either advanced segmentation algorithms or knowledge of visualization techniques. We demonstrate our approach using synthetic examples, industrial phantom objects created to stress bimodal scanning techniques, and real-world objects, and we discuss expert feedback.},
  archive      = {J_TVCG},
  author       = {Xuan Huang and Haichao Miao and Hyojin Kim and Andrew Townsend and Kyle Champley and Joseph Tringe and Valerio Pascucci and Peer-Timo Bremer},
  doi          = {10.1109/TVCG.2024.3382607},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2196-2210},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Bimodal visualization of industrial X-ray and neutron computed tomography data},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CloudMix: Dual mixup consistency for unpaired point cloud
completion. <em>TVCG</em>, <em>31</em>(4), 2182–2195. (<a
href="https://doi.org/10.1109/TVCG.2024.3383434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the unsatisfactory performance of supervised methods on unpaired real-world scans, point cloud completion via cross-domain adaptation has recently drawn growing attention. Nevertheless, previous approaches only focus on alleviating the distribution shift through domain alignment, resulting in massive information loss of real-world domain data. To tackle this issue, we propose a dual mixup-induced consistency regularization to integrate both source and target domain to improve robustness and generalization capability. Specifically, we mix up virtual and real-world shapes in the input and latent feature space respectively, and then regularize the completion network by forcing two kinds of mixed completion predictions to be consistent. To further adapt to each instance within the real-world domain, we design a novel density-aware refiner to utilize local context information to preserve the fine-grained details and remove noise or outliers for coarse completion. Extensive experiments on real-world scans and our synthetic unpaired datasets demonstrate the superiority of our method over existing state-of-the-art approaches.},
  archive      = {J_TVCG},
  author       = {Fengqi Liu and Jingyu Gong and Qianyu Zhou and Xuequan Lu and Ran Yi and Yuan Xie and Lizhuang Ma},
  doi          = {10.1109/TVCG.2024.3383434},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2182-2195},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CloudMix: Dual mixup consistency for unpaired point cloud completion},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chart2Vec: A universal embedding of context-aware
visualizations. <em>TVCG</em>, <em>31</em>(4), 2167–2181. (<a
href="https://doi.org/10.1109/TVCG.2024.3383089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advances in AI-enabled techniques have accelerated the creation and automation of visualizations in the past decade. However, presenting visualizations in a descriptive and generative format remains a challenge. Moreover, current visualization embedding methods focus on standalone visualizations, neglecting the importance of contextual information for multi-view visualizations. To address this issue, we propose a new representation model, Chart2Vec, to learn a universal embedding of visualizations with context-aware information. Chart2Vec aims to support a wide range of downstream visualization tasks such as recommendation and storytelling. Our model considers both structural and semantic information of visualizations in declarative specifications. To enhance the context-aware capability, Chart2Vec employs multi-task learning on both supervised and unsupervised tasks concerning the cooccurrence of visualizations. We evaluate our method through an ablation study, a user study, and a quantitative comparison. The results verified the consistency of our embedding method with human cognition and showed its advantages over existing methods.},
  archive      = {J_TVCG},
  author       = {Qing Chen and Ying Chen and Ruishi Zou and Wei Shuai and Yi Guo and Jiazhe Wang and Nan Cao},
  doi          = {10.1109/TVCG.2024.3383089},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2167-2181},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Chart2Vec: A universal embedding of context-aware visualizations},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of background, foreground, and manipulated object
rendering on egocentric depth perception in virtual and augmented indoor
environments. <em>TVCG</em>, <em>31</em>(4), 2155–2166. (<a
href="https://doi.org/10.1109/TVCG.2024.3382616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigated how the similarity of the rendering parameters of background and foreground objects affected egocentric depth perception in indoor virtual and augmented environments. We refer to the similarity of the rendering parameters as visual ‘congruence’. Study participants manipulated the depth of a sphere to match the depth of a designated target peg. In the first experiment, the sphere and peg were both virtual, while in the second experiment, the sphere is virtual and the peg is real. In both experiments, depth perception accuracy was found to depend on the levels of realism and congruence between the sphere, pegs, and background. In Experiment 1, realistic backgrounds lead to overestimation of depth, but resulted in underestimation when the background was virtual, and when depth cues were applied to the sphere and target peg. In Experiment 2, background and target pegs were real but matched with the virtual sphere; in comparison to Experiment 1, realistically rendered targets prompted an underestimation and more accuracy with the manipulated object. These findings suggest that congruence can affect distance estimation and the underestimation effect in the AR environment resulted from increased graphical fidelity of the foreground target and background.},
  archive      = {J_TVCG},
  author       = {Matthew McQuaigue and Kalpathi Subramanian and Paula Goolkasian and Zachary Wartell},
  doi          = {10.1109/TVCG.2024.3382616},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2155-2166},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impact of background, foreground, and manipulated object rendering on egocentric depth perception in virtual and augmented indoor environments},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing complex shaped clothing from a single image
with feature stable unsigned distance fields. <em>TVCG</em>,
<em>31</em>(4), 2142–2154. (<a
href="https://doi.org/10.1109/TVCG.2024.3381937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-view clothing reconstruction usually relies on topologically fixed clothing templates to reduce the problem complexity, but this strategy also makes the reconstructed clothing shape contours simple and lack diversity. In this article, we propose a novel clothing reconstruction method to generate complex shape contours and open clothing mesh from a single image. At the heart of our work is an implicit unsigned distance field condition on clothing-oriented and pose-stable spatial shape features to represent the clothing from the image. This feature can provide spatially aligned clothing shape priors to improve the pose robustness. It is based on a type-generic clothing template derived from the mainstream clothing generative model to avoid tedious template design and switching. To output open clothing mesh results from noisy clothing unsigned distance fields, we develop a two-stage clothing mesh extraction method. It takes the point clouds as an intermediate representation and produces smooth, plausible and editable clothing mesh results. To provide effective supervision, we construct a pose-rich and shape-complete clothing scan dataset by enhancing clothing pose diversity and complementing missing clothing geometry caused by occlusion. Extensive experiments demonstrate that our method achieves state-of-the-art levels. More importantly, we provide a simple but effective, and low-cost way to reconstruct complex shape contours clothing from a single image.},
  archive      = {J_TVCG},
  author       = {Xinqi Liu and Jituo Li and Guodong Lu},
  doi          = {10.1109/TVCG.2024.3381937},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2142-2154},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reconstructing complex shaped clothing from a single image with feature stable unsigned distance fields},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VSFormer: Mining correlations in flexible view set for
multi-view 3D shape understanding. <em>TVCG</em>, <em>31</em>(4),
2127–2141. (<a href="https://doi.org/10.1109/TVCG.2024.3381152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View-based methods have demonstrated promising performance in 3D shape understanding. However, they tend to make strong assumptions about the relations between views or learn the multi-view correlations indirectly, which limits the flexibility of exploring inter-view correlations and the effectiveness of target tasks. To overcome the above problems, this article investigates flexible organization and explicit correlation learning for multiple views. In particular, we propose to incorporate different views of a 3D shape into a permutation-invariant set, referred to as View Set, which removes rigid relation assumptions and facilitates adequate information exchange and fusion among views. Based on that, we devise a nimble Transformer model, named VSFormer, to explicitly capture pairwise and higher-order correlations of all elements in the set. Meanwhile, we theoretically reveal a natural correspondence between the Cartesian product of a view set and the correlation matrix in the attention mechanism, which supports our model design. Comprehensive experiments suggest that VSFormer has better flexibility, efficient inference efficiency and superior performance. Notably, VSFormer reaches state-of-the-art results on various 3 d recognition datasets, including ModelNet40, ScanObjectNN and RGBD. It also establishes new records on the SHREC’17 retrieval benchmark.},
  archive      = {J_TVCG},
  author       = {Hongyu Sun and Yongcai Wang and Peng Wang and Haoran Deng and Xudong Cai and Deying Li},
  doi          = {10.1109/TVCG.2024.3381152},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2127-2141},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VSFormer: Mining correlations in flexible view set for multi-view 3D shape understanding},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). De-cluttering scatterplots with integral images.
<em>TVCG</em>, <em>31</em>(4), 2114–2126. (<a
href="https://doi.org/10.1109/TVCG.2024.3381453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatterplots provide a visual representation of bivariate data (or 2D embeddings of multivariate data) that allows for effective analyses of data dependencies, clusters, trends, and outliers. Unfortunately, classical scatterplots suffer from scalability issues, since growing data sizes eventually lead to overplotting and visual clutter on a screen with a fixed resolution, which hinders the data analysis process. We propose an algorithm that compensates for irregular sample distributions by a smooth transformation of the scatterplot&#39;s visual domain. Our algorithm evaluates the scatterplot&#39;s density distribution to compute a regularization mapping based on integral images of the rasterized density function. The mapping preserves the samples’ neighborhood relations. Few regularization iterations suffice to achieve a nearly uniform sample distribution that efficiently uses the available screen space. We further propose approaches to visually convey the transformation that was applied to the scatterplot and compare them in a user study. We present a novel parallel algorithm for fast GPU-based integral-image computation, which allows for integrating our de-cluttering approach into interactive visual data analysis systems.},
  archive      = {J_TVCG},
  author       = {Hennes Rave and Vladimir Molchanov and Lars Linsen},
  doi          = {10.1109/TVCG.2024.3381453},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2114-2126},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {De-cluttering scatterplots with integral images},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient GPU computation of large protein solvent-excluded
surface. <em>TVCG</em>, <em>31</em>(4), 2101–2113. (<a
href="https://doi.org/10.1109/TVCG.2024.3380100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Solvent-Excluded Surface (SES) is an essential representation of molecules which is massively used in molecular modeling and drug discovery since it represents the interacting surface between molecules. Based on its properties, it supports the visualization of both large scale shapes and details of molecules. While several methods targeted its computation, the ability to process large molecular structures to address the introduction of big complex analysis while leveraging the massively parallel architecture of GPUs has remained a challenge. This is mostly caused by the need for consequent memory allocation or by the complexity of the parallelization of its processing. In this paper, we leverage the last theoretical advances made for the depiction of the SES to provide fast analytical computation with low impact on memory. We show that our method is able to compute the complete surface while handling large molecular complexes with competitive computation time costs compared to previous works.},
  archive      = {J_TVCG},
  author       = {Cyprien Plateau–Holleville and Maxime Maria and Stéphane Mérillou and Matthieu Montes},
  doi          = {10.1109/TVCG.2024.3380100},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2101-2113},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient GPU computation of large protein solvent-excluded surface},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cartoon animation outpainting with region-guided motion
inference. <em>TVCG</em>, <em>31</em>(4), 2086–2100. (<a
href="https://doi.org/10.1109/TVCG.2024.3379125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cartoon animation video is a popular visual entertainment form worldwide, however many classic animations were produced in a 4:3 aspect ratio that is incompatible with modern widescreen displays. Existing methods like cropping lead to information loss while retargeting causes distortion. Animation companies still rely on manual labor to renovate classic cartoon animations, which is tedious and labor-intensive, but can yield higher-quality videos. Conventional extrapolation or inpainting methods tailored for natural videos struggle with cartoon animations due to the lack of textures in anime, which affects the motion estimation of the objects. In this article, we propose a novel framework designed to automatically outpaint 4:3 anime to 16:9 via region-guided motion inference. Our core concept is to identify the motion correspondences between frames within a sequence in order to reconstruct missing pixels. Initially, we estimate optical flow guided by region information to address challenges posed by exaggerated movements and solid-color regions in cartoon animations. Subsequently, frames are stitched to produce a pre-filled guide frame, offering structural clues for the extension of optical flow maps. Finally, a voting and fusion scheme utilizes learned fusion weights to blend the aligned neighboring reference frames, resulting in the final outpainting frame. Extensive experiments confirm the superiority of our approach over existing methods.},
  archive      = {J_TVCG},
  author       = {Huisi Wu and Hao Meng and Chengze Li and Xueting Liu and Zhenkun Wen and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2024.3379125},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2086-2100},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cartoon animation outpainting with region-guided motion inference},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bio-inspired model for bee simulations. <em>TVCG</em>,
<em>31</em>(4), 2073–2085. (<a
href="https://doi.org/10.1109/TVCG.2024.3379080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As eusocial creatures, bees display unique macro collective behavior and local body dynamics that hold potential applications in various fields, such as computer animation, robotics, and social behavior. Unlike birds and fish, bees fly in a low-aligned zigzag pattern. Additionally, bees rely on visual signals for foraging and predator avoidance, exhibiting distinctive local body oscillations, such as body lifting, thrusting, and swaying. These inherent features pose significant challenges to realistic bee simulations in practical animation applications. In this article, we present a bio-inspired model for bee simulations capable of replicating both macro collective behavior and local body dynamics of bees. Our approach utilizes a visually-driven system to simulate a bee&#39;s local body dynamics, incorporating obstacle perception and body rolling control for effective collision avoidance. Moreover, we develop an oscillation rule that captures the dynamics of the bee&#39;s local bodies, drawing on insights from biological research. Our model extends beyond simulating individual bees’ dynamics; it can also represent bee swarms by integrating a fluid-based field with the bees’ innate noise and zigzag motions. To fine-tune our model, we utilize pre-collected honeybee flight data. Through extensive simulations and comparative experiments, we demonstrate that our model can efficiently generate realistic low-aligned and inherently noisy bee swarms.},
  archive      = {J_TVCG},
  author       = {Qiang Chen and Wenxiu Guo and Yuming Fang and Yang Tong and Tingsong Lu and Xiaogang Jin and Zhigang Deng},
  doi          = {10.1109/TVCG.2024.3379080},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2073-2085},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A bio-inspired model for bee simulations},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A primal-dual box-constrained QP pressure poisson solver
with topology-aware geometry-inspired aggregation AMG. <em>TVCG</em>,
<em>31</em>(4), 2058–2072. (<a
href="https://doi.org/10.1109/TVCG.2024.3378725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new barrier-based box-constrained convex QP solver based on a primal-dual interior point method to efficiently solve large-scale pressure Poisson problems with non-negative pressure constraints, which commonly arise in liquid animation. The performance of prior active-set-based approaches is limited by the need to repeatedly update the active set. Our solver eliminates this issue by entirely avoiding the use of an active set, which in turn makes the inner problems of our Newton iteration process fully unconstrained. For efficiency, exploiting the solution uniqueness of convex QPs and the fact that the pressure constraints are simple box constraints, we aggressively update solution candidates without performing any step selection procedure (such as line search) and instead directly clamp candidates back to the bounds wherever constraint violations occur. Additionally, to accelerate the inner linear solves, we present a topology-aware geometry-inspired aggregation algebraic multigrid preconditioner and describe in detail several key performance optimizations that we incorporate. We demonstrate the efficacy of our solver in various practical scenarios and show that it often surpasses various alternatives in terms of speed and memory usage.},
  archive      = {J_TVCG},
  author       = {Tetsuya Takahashi and Christopher Batty},
  doi          = {10.1109/TVCG.2024.3378725},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2058-2072},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A primal-dual box-constrained QP pressure poisson solver with topology-aware geometry-inspired aggregation AMG},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UPST-NeRF: Universal photorealistic style transfer of neural
radiance fields for 3D scene. <em>TVCG</em>, <em>31</em>(4), 2045–2057.
(<a href="https://doi.org/10.1109/TVCG.2024.3378692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photorealistic stylization of 3D scenes aims to generate photorealistic images from arbitrary novel views according to a given style image, while ensuring consistency when rendering video from different viewpoints. Some existing stylization methods using neural radiance fields can effectively predict stylized scenes by combining the features of the style image with multi-view images to train 3D scenes. However, these methods generate novel view images that contain undesirable artifacts. In addition, they cannot achieve universal photorealistic stylization for a 3D scene. Therefore, a stylization image needs to retrain a 3D scene representation network based on a neural radiation field. We propose a novel photorealistic 3D scene stylization transfer framework to address these issues. It can realize photorealistic 3D scene style transfer with a 2D style image for novel view video rendering. We first pre-trained a 2D photorealistic style transfer network, which can satisfy the photorealistic style transfer between any content image and style image. Then, we use voxel features to optimize a 3D scene and obtain the geometric representation of the scene. Finally, we jointly optimize a hypernetwork to realize the photorealistic style transfer of arbitrary style images. In the transfer stage, we use a pre-trained 2D photorealistic network to constrain the photorealistic style of different views and different style images in the 3D scene. The experimental results show that our method not only realizes the 3D photorealistic style transfer of arbitrary style images, but also outperforms the existing methods in terms of visual quality and consistency.},
  archive      = {J_TVCG},
  author       = {Yaosen Chen and Qi Yuan and Zhiqiang Li and Yuegen Liu and Wei Wang and Chaoping Xie and Xuming Wen and Qien Yu},
  doi          = {10.1109/TVCG.2024.3378692},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2045-2057},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {UPST-NeRF: Universal photorealistic style transfer of neural radiance fields for 3D scene},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ResGEM: Multi-scale graph embedding network for residual
mesh denoising. <em>TVCG</em>, <em>31</em>(4), 2028–2044. (<a
href="https://doi.org/10.1109/TVCG.2024.3378309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh denoising is a crucial technology that aims to recover a high-fidelity 3D mesh from a noise-corrupted one. Deep learning methods, particularly graph convolutional networks (GCNs) based mesh denoisers, have demonstrated their effectiveness in removing various complex real-world noises while preserving authentic geometry. However, it is still a quite challenging work to faithfully regress uncontaminated normals and vertices on meshes with irregular topology. In this article, we propose a novel pipeline that incorporates two parallel normal-aware and vertex-aware branches to achieve a balance between smoothness and geometric details while maintaining the flexibility of surface topology. We introduce ResGEM, a new GCN, with multi-scale embedding modules and residual decoding structures to facilitate normal regression and vertex modification for mesh denoising. To effectively extract multi-scale surface features while avoiding the loss of topological information caused by graph pooling or coarsening operations, we encode the noisy normal and vertex graphs using four edge-conditioned embedding modules (EEMs) at different scales. This allows us to obtain favorable feature representations with multiple receptive field sizes. Formulating the denoising problem into a residual learning problem, the decoder incorporates residual blocks to accurately predict true normals and vertex offsets from the embedded feature space. Moreover, we propose novel regularization terms in the loss function that enhance the smoothing and generalization ability of our network by imposing constraints on normal fidelity and consistency. Comprehensive experiments have been conducted to demonstrate the superiority of our method over the state-of-the-art on both synthetic and real-scanned datasets.},
  archive      = {J_TVCG},
  author       = {Ziqi Zhou and Mengke Yuan and Mingyang Zhao and Jianwei Guo and Dong-Ming Yan},
  doi          = {10.1109/TVCG.2024.3378309},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2028-2044},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ResGEM: Multi-scale graph embedding network for residual mesh denoising},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explicit topology optimization of voronoi foams.
<em>TVCG</em>, <em>31</em>(4), 2012–2027. (<a
href="https://doi.org/10.1109/TVCG.2024.3375012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topology optimization can maximally leverage the high DOFs and mechanical potentiality of porous foams but faces challenges in adapting to free-form outer shapes, maintaining full connectivity between adjacent foam cells, and achieving high simulation accuracy. Utilizing the concept of Voronoi tessellation may help overcome the challenges owing to its distinguished properties on highly flexible topology, natural edge connectivity, and easy shape conforming. However, a variational optimization of the so-called Voronoi foams has not yet been fully explored. In addressing the issue, a concept of explicit topology optimization of open-cell Voronoi foams is proposed that can efficiently and reliably guide the foam&#39;s topology and geometry variations under critical physical and geometric requirements. Taking the site (or seed) positions and beam radii as the DOFs, we explore the differentiability of the open-cell Voronoi foams w.r.t. its seed locations, and propose a highly efficient local finite difference method to estimate the derivatives. During the gradient-based optimization, the foam topology can change freely, and some seeds may even be pushed out of shape, which greatly alleviates the challenges of prescribing a fixed underlying grid. The foam&#39;s mechanical property is also computed with a much-improved efficiency by an order of magnitude, in comparison with benchmark FEM, via a new material-aware numerical coarsening method on its highly heterogeneous density field counterpart. We show the improved performance of our Voronoi foam in comparison with classical topology optimization approaches and demonstrate its advantages in various settings.},
  archive      = {J_TVCG},
  author       = {Ming Li and Jingqiao Hu and Wei Chen and Weipeng Kong and Jin Huang},
  doi          = {10.1109/TVCG.2024.3375012},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {2012-2027},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explicit topology optimization of voronoi foams},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Region-aware color smudging. <em>TVCG</em>, <em>31</em>(4),
1999–2011. (<a href="https://doi.org/10.1109/TVCG.2024.3374210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color smudge operations from digital painting software enable users to create natural shading effects in high-fidelity paintings by interactively mixing colors. To precisely control results in traditional painting software, users tend to organize flat-filled color regions in multiple layers and smudge them to generate different color gradients. However, the requirement to carefully deal with regions makes the smudging process time-consuming and laborious, especially for non-professional users. This motivates us to investigate how to infer user-desired smudging effects when users smudge over regions in a single layer. To investigate improving color smudge performance, we first conduct a formative study. Following the findings of this study, we design SmartSmudge, a novel smudge tool that offers users dynamical smudge brushes and real-time region selection for easily generating natural and efficient shading effects. We demonstrate the efficiency and effectiveness of the proposed tool via a user study and quantitative analysis.},
  archive      = {J_TVCG},
  author       = {Ying Jiang and Pengfei Xu and Congyi Zhang and Hongbo Fu and Henry Lau and Wenping Wang},
  doi          = {10.1109/TVCG.2024.3374210},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1999-2011},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Region-aware color smudging},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to restore compressed point cloud attribute: A
fully data-driven approach and a rules-unrolling-based optimization.
<em>TVCG</em>, <em>31</em>(4), 1985–1998. (<a
href="https://doi.org/10.1109/TVCG.2024.3375861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of holographic media drives the standardization of Geometry-based Point Cloud Compression (G-PCC) to sustain networked service provisioning. However, G-PCC inevitably introduces visually annoying artifacts, degrading the quality of experience (QoE). This work focuses on restoring G-PCC compressed point cloud attributes, e.g., RGB colors, to which fully data-driven and rules-unrolling-based post-processing filters are studied. At first, as compressed attributes exhibit nested blockiness, we develop a learning-based sample adaptive offset (NeuralSAO), which leverages a neural model using multiscale feature aggregation and embedding to characterize local correlations for quantization error compensation. Later, given statistically Gaussian distributed quantization noise, we suggest the utilization of a bilateral filter with Gaussian kernels to weigh neighbors by jointly considering their geometric and photometric contributions for restoration. Since local signals often present varying distributions, we propose estimating the smoothing parameters of the bilateral filter using an ultra-lightweight neural model. Such a bilateral filter with learnable parameters is called NeuralBF. The proposed NeuralSAO demonstrates the state-of-art restoration quality improvement, e.g., $&amp;gt; $20% BD-BR (Bjøntegaard delta rate) reduction over G-PCC on solid points clouds. However, NeuralSAO is computationally intensive and may suffer from poor generalization. On the other hand, although NeuralBF only achieves half of the gains of NeuralSAO, it is lightweight and exhibits impressive generalization across various samples. This comparative study between the data-driven large-scale NeuralSAO and the rules-unrolling-based small-scale NeuralBF helps to understand the capacity (i.e., performance, complexity, generalization) of underlying filters in terms of the quality restoration for compressed point cloud attribute.},
  archive      = {J_TVCG},
  author       = {Junteng Zhang and Junzhe Zhang and Dandan Ding and Zhan Ma},
  doi          = {10.1109/TVCG.2024.3375861},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1985-1998},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning to restore compressed point cloud attribute: A fully data-driven approach and a rules-unrolling-based optimization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). F-RDW: Redirected walking with forecasting future position.
<em>TVCG</em>, <em>31</em>(4), 1970–1984. (<a
href="https://doi.org/10.1109/TVCG.2024.3376080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to serve better VR experiences to users, existing predictive methods of Redirected Walking (RDW) exploit future information to reduce the number of reset occurrences. However, such methods often impose a precondition during deployment, either in the virtual environment&#39;s layout or the user&#39;s walking direction, which constrains its universal applications. To tackle this challenge, we propose a mechanism F-RDW that is twofold: (1) forecasts the future information of a user in the virtual space without any assumptions by using the conventional method, and (2) fuse this information while maneuvering existing RDW methods. The backbone of the first step is an LSTM-based model that ingests the user&#39;s spatial and eye-tracking data to predict the user&#39;s future position in the virtual space, and the following step feeds those predicted values into existing RDW methods (such as MPCRed, S2C, TAPF, and ARC) while respecting their internal mechanism in applicable ways. The results of our simulation test and user study demonstrate the significance of future information when using RDW in small physical spaces or complex environments. We prove that the proposed mechanism significantly reduces the number of resets and increases the traveled distance between resets, hence augmenting the redirection performance of all RDW methods explored in this work.},
  archive      = {J_TVCG},
  author       = {Sang-Bin Jeon and Jaeho Jung and Jinhyung Park and In-Kwon Lee},
  doi          = {10.1109/TVCG.2024.3376080},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {4},
  number       = {4},
  pages        = {1970-1984},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {F-RDW: Redirected walking with forecasting future position},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
