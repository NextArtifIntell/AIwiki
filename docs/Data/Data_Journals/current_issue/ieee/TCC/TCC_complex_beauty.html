<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcc---30">TCC - 30</h2>
<ul>
<li><details>
<summary>
(2025). A cost-aware operator migration approach for distributed
stream processing system. <em>TCC</em>, <em>13</em>(1), 441–454. (<a
href="https://doi.org/10.1109/TCC.2025.3538512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing is integral to edge computing due to its low-latency attributes. Nevertheless, variability in user group sizes and disparate computing capabilities of edge devices necessitate frequent operator migrations within the stream. Moreover, intricate dependencies among stream operators often obscure the detection of potential bottleneck operators until an identified bottleneck is migrated in the stream. To address this, we propose a Cost-Aware Operator Migration (CAOM) scheme. The CAOM scheme incorporates a bottleneck operator detection mechanism that directly identifies all bottleneck operators based on task running metrics. This approach avoids multiple consecutive operator migrations in complex tasks, reducing the number of task interruptions caused by operator migration. Moreover, CAOM takes into account the temporal variance in operator migration costs. By factoring in the fluctuating data generation rate from data sources at different time intervals, CAOM selects the optimal start time for operator migration to minimize the amount of accumulated data during task interruptions. Finally, we implemented CAOM on Apache Flink and evaluated its performance using the WordCount and Nexmark applications. Our experiments show that CAOM effectively reduces the number of necessary operator migrations in tasks with complex topologies and decreases the latency overhead associated with operator migration compared to state-of-the-art schemes.},
  archive      = {J_TCC},
  author       = {Jiawei Tan and Zhuo Tang and Wentong Cai and Wen Jun Tan and Xiong Xiao and Jiapeng Zhang and Yi Gao and Kenli Li},
  doi          = {10.1109/TCC.2025.3538512},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {441-454},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A cost-aware operator migration approach for distributed stream processing system},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developments on the “machine learning as a service for high
energy physics” framework and related cloud native solution.
<em>TCC</em>, <em>13</em>(1), 429–440. (<a
href="https://doi.org/10.1109/TCC.2025.3535793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) techniques have been successfully used in many areas of High Energy Physics (HEP) and will play a significant role in the success of upcoming High-Luminosity Large Hadron Collider (HL-LHC) program at CERN. An unprecedented amount of data at the exascale will be collected by LHC experiments in the next decade, and this effort will require novel approaches to train and use ML models. The work presented in this paper is focused on the developments of a ML as a Service (MLaaS) solution for HEP, aiming to provide a cloud service that allows HEP users to run ML pipelines via HTTPs calls. These pipelines are executed by using MLaaS4HEP framework, which allows reading data, processing data, and training ML models directly using ROOT files of arbitrary size from local or distributed data sources. In particular, new features implemented on the framework will be presented as well as updates on the architecture of an existing prototype of the MLaaS4HEP cloud service will be provided. This solution includes two OAuth2 proxy servers as authentication/authorization layer, a MLaaS4HEP server, an XRootD proxy server for enabling access to remote ROOT data, and the TensorFlow as a Service (TFaaS) service in charge of the inference phase.},
  archive      = {J_TCC},
  author       = {Luca Giommi and Daniele Spiga and Mattia Paladino and Valentin Kuznetsov and Daniele Bonacorsi},
  doi          = {10.1109/TCC.2025.3535793},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {429-440},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Developments on the “Machine learning as a service for high energy physics” framework and related cloud native solution},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint computation offloading and resource allocation in
mobile-edge cloud computing: A two-layer game approach. <em>TCC</em>,
<em>13</em>(1), 411–428. (<a
href="https://doi.org/10.1109/TCC.2025.3538090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile-Edge Cloud Computing (MECC) plays a crucial role in balancing low-latency services at the edge with the computational capabilities of cloud data centers (DCs). However, many existing studies focus on single-provider settings or limit their analysis to interactions between mobile devices (MDs) and edge servers (ESs), often overlooking the competition that occurs among ESs from different providers. This article introduces an innovative two-layer game framework that captures independent self-interested competition among MDs and ESs, providing a more accurate reflection of multi-vendor environments. Additionally, the framework explores the influence of cloud-edge collaboration on ES competition, offering new insights into these dynamics. The proposed model extends previous research by developing algorithms that optimize task offloading and resource allocation strategies for both MDs and ESs, ensuring the convergence to Nash equilibrium in both layers. Simulation results demonstrate the potential of the framework to improve resource efficiency and system responsiveness in multi-provider MECC environments.},
  archive      = {J_TCC},
  author       = {Zhenli He and Ying Guo and Xiaolong Zhai and Mingxiong Zhao and Wei Zhou and Keqin Li},
  doi          = {10.1109/TCC.2025.3538090},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {411-428},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint computation offloading and resource allocation in mobile-edge cloud computing: A two-layer game approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Verifiable encrypted image retrieval with reversible data
hiding in cloud environment. <em>TCC</em>, <em>13</em>(1), 397–410. (<a
href="https://doi.org/10.1109/TCC.2025.3535937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing numbers of users outsourcing images to cloud servers, privacy-preserving content-based image retrieval (CBIR) is widely studied. However, existing privacy-preserving CBIR schemes have limitations in terms of low search accuracy and efficiency due to the use of unreasonable index structures or retrieval methods. Meanwhile, existing result verification schemes do not consider the privacy of verification information. To address these problems, we propose a new secure verification encrypted image retrieval scheme. Specifically, we design an additional homomorphic bitmap index structure by using a pre-trained CNN model with modified fully connected layers to extract image feature vectors and organize them into a bitmap. It makes the extracted features more representative and robust compared to manually designed features, and only performs vector addition during the search process, improving search efficiency and accuracy. Moreover, we design a reversible data hiding (RDH) technique with color images, which embeds the verification information into the least significant bits of the encrypted image pixels to improve the security of the verification information. Finally, we analyze the security of our scheme against chosen-plaintext attacks (CPA) in the security analysis and demonstrate the effectiveness of our scheme on two real-world datasets (i.e., COCO and Flickr-25 k) through experiments.},
  archive      = {J_TCC},
  author       = {Mingyue Li and Yuting Zhu and Ruizhong Du and Chunfu Jia},
  doi          = {10.1109/TCC.2025.3535937},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {397-410},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Verifiable encrypted image retrieval with reversible data hiding in cloud environment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PiCoP: Service mesh for sharing microservices in multiple
environments using protocol-independent context propagation.
<em>TCC</em>, <em>13</em>(1), 383–396. (<a
href="https://doi.org/10.1109/TCC.2025.3531954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous integration and continuous delivery require many production-like environments in a cluster for testing, staging, debugging, and previewing. In applications built on microservice architecture, sharing common microservices in multiple environments is an effective way to reduce resource consumption. Previous methods extend application layer protocols like HTTP and gRPC to propagate contexts including environment identifiers and to route requests. However, microservices also use other protocols such as MySQL, Redis, Memcached, and AMQP, and extending each protocol requires lots of effort to implement the extensions. This paper proposes PiCoP, a framework to share microservices in multiple environments by propagating contexts and routing requests independently of application layer protocols. PiCoP provides a protocol that propagates contexts by appending them to the front of each TCP byte stream and constructs a service mesh that uses the protocol to route requests. We design the protocol to make it easy to instrument into a system. We demonstrate that PiCoP can reduce resource usage and that it applies to a real-world application, enabling the sharing of microservices in multiple environments using any application layer protocol.},
  archive      = {J_TCC},
  author       = {Hiroya Onoe and Daisuke Kotani and Yasuo Okabe},
  doi          = {10.1109/TCC.2025.3531954},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {383-396},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PiCoP: Service mesh for sharing microservices in multiple environments using protocol-independent context propagation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint adaptive aggregation and resource allocation for
hierarchical federated learning systems based on edge-cloud
collaboration. <em>TCC</em>, <em>13</em>(1), 369–382. (<a
href="https://doi.org/10.1109/TCC.2025.3530681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical federated learning shows excellent potential for communication-computation trade-offs and reliable data privacy protection by introducing edge-cloud collaboration. Considering non-independent and identically distributed data distribution among devices and edges, this article aims to minimize the final loss function under time and energy budget constraints by optimizing the aggregation frequency and resource allocation jointly. Although there is no closed-form expression relating the final loss function to optimization variables, we divide the hierarchical federated learning process into multiple cloud intervals and analyze the convergence bound for each cloud interval. Then, we transform the initial problem into one that can be adaptively optimized in each cloud interval. We propose an adaptive hierarchical federated learning process, termed as AHFLP, where we determine edge and cloud aggregation frequency for each cloud interval based on estimated parameters, and then the CPU frequency of devices and wireless channel bandwidth allocation can be optimized in each edge. Simulations are conducted under different models, datasets and data distributions, and the results demonstrate the superiority of our proposed AHFLP compared with existing schemes.},
  archive      = {J_TCC},
  author       = {Yi Su and Wenhao Fan and Qingcheng Meng and Penghui Chen and Yuan&#39;an Liu},
  doi          = {10.1109/TCC.2025.3530681},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {369-382},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint adaptive aggregation and resource allocation for hierarchical federated learning systems based on edge-cloud collaboration},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid serverless platform for smart deployment of service
function chains. <em>TCC</em>, <em>13</em>(1), 351–368. (<a
href="https://doi.org/10.1109/TCC.2025.3528573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Data Centres deal with dynamic changes all the time. Networks in particular, need to adapt their configurations to changing workloads. Given these expectations, Network Function Virtualization (NFV) using Software Defined Networks (SDNs) has realized the aspect of programmability in networks. NFVs allow network services to be programmed as software entities that can be deployed on commodity clusters in the Cloud. Being software, they inherently carry the ability to be customized to specific tenants’ requirements and thus support multi-tenant variations with ease. However, the ability to exploit scaling in alignment with changing demands with minimal loss of service, and improving resource usage efficiency still remains a challenge. Several recent works in literature have proposed platforms to realize Virtual Network functions (VNFs) on the Cloud using service offerings such as Infrastructure as a Service (IaaS) and serverless computing. These approaches are limited by deployment difficulties (configuration and sizing), adaptability to performance requirements (elastic scaling), and changing workload dynamics (scaling and customization). In the current work, we propose a Hybrid Serverless Platform (HSP) to address these identified lacunae. The HSP is implemented using a combination of persistent IaaS, and FaaS components. The IaaS components handle the steady state load, whereas the FaaS components activate during the dynamic change associated with scaling to minimize service loss. The HSP controller takes provisioning decisions based on Quality of Service (QoS) rules and flow statistics using an auto recommender, alleviating users of sizing decisions for function deployment. HSP controller design exploits data locality in SFC realization, reducing data-transfer times between VNFs. It also enables the usage of application characteristics to offer higher control over SFC deployment. A proof-of-concept realization of HSP is presented in the paper and is evaluated for a representative Service Function Chain (SFC) for a dynamic workload, which shows minimal loss in flowlet service, up to 35% resource savings as compared to a pure IaaS deployment and up to 55% lower end-to-end times as compared to a baseline FaaS implementation.},
  archive      = {J_TCC},
  author       = {Sheshadri K R and J. Lakshmi},
  doi          = {10.1109/TCC.2025.3528573},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {351-368},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Hybrid serverless platform for smart deployment of service function chains},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-aware offloading of containerized tasks in cloud
native V2X networks. <em>TCC</em>, <em>13</em>(1), 336–350. (<a
href="https://doi.org/10.1109/TCC.2025.3529245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud-native environments, executing vehicle-to-everything (V2X) tasks in edge nodes close to users significantly reduces service end-to-end latency. Containerization further reduces resource and time consumption, and, subsequently, application latency. Since edge nodes are typically resource and energy-constrained, optimizing offloading decisions and managing edge energy consumption is crucial. However, the offloading of containerized tasks has not been thoroughly explored from a practical implementation perspective. This paper proposes an optimization framework for energy-aware offloading of V2X tasks implemented as Kubernetes pods. A weighted utility function is derived based on cumulative pod response time, and an edge-to-cloud offloading decision algorithm (ECODA) is proposed. The system&#39;s energy cost model is derived, and a closed-loop repeated reward-based mechanism for CPU adjustment is presented. An energy-aware (EA)-ECODA is proposed to solve the offloading optimization problem while adjusting CPU usage according to energy considerations. Simulations show that ECODA and EA-ECODA outperform first-in, first-served (FIFS) and EA-FIFS in terms of utility, average pod response time, and resource usage, with low computational complexity. Additionally, a real testbed evaluation of a vulnerable road user application demonstrates that ECODA outperforms Kubernetes vertical scaling in terms of service-level delay. Moreover, EA-ECODA significantly improves energy usage utility.},
  archive      = {J_TCC},
  author       = {Estela Carmona-Cejudo and Francesco Iadanza},
  doi          = {10.1109/TCC.2025.3529245},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {336-350},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Energy-aware offloading of containerized tasks in cloud native V2X networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CARL: Cost-optimized online container placement on VMs using
adversarial reinforcement learning. <em>TCC</em>, <em>13</em>(1),
321–335. (<a href="https://doi.org/10.1109/TCC.2025.3528446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containerization has become popular for the deployment of applications on public clouds. Large enterprises may host 100 s of applications on 1000 s containers that are placed onto Virtual Machines (VMs). Such placement decisions happen continuously as applications are updated by DevOps pipelines that deploy the containers. Managing the placement of container resource requests onto the available capacities of VMs needs to be cost-efficient. This is well-studied, and usually modelled as a multi-dimensional Vector Bin-packing Problem (VBP). Many heuristics, and recently machine learning approaches, have been developed to solve this NP-hard problem for real-time decisions. We propose CARL, a novel approach to solve VBP through Adversarial Reinforcement Learning (RL) for cost minimization. It mimics the placement behavior of an offline semi-optimal VBP solver (teacher), while automatically learning a reward function for reducing the VM costs which out-performs the teacher. It requires limited historical container workload traces to train, and is resilient to changes in the workload distribution during inferencing. We extensively evaluate CARL on workloads derived from realistic traces from Google and Alibaba for the placement of 5 k–10 k container requests onto 2 k–8 k VMs, and compare it with classic heuristics and state-of-the-art RL methods. (1) CARL is fast, e.g., making placement decisions at $\approx 1900$ requests/sec onto 8,900 candidate VMs. (2) It is efficient, achieving $\approx 16\%$ lower VM costs than classic and contemporary RL methods. (3) It is robust to changes in the workload, offering competitive results even when the resource needs or inter-arrival time of the container requests skew from the training workload.},
  archive      = {J_TCC},
  author       = {Prathamesh Saraf Vinayak and Saswat Subhajyoti Mallick and Lakshmi Jagarlamudi and Anirban Chakraborty and Yogesh Simmhan},
  doi          = {10.1109/TCC.2025.3528446},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {321-335},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CARL: Cost-optimized online container placement on VMs using adversarial reinforcement learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ByteTuning: Watermark tuning for RoCEv2. <em>TCC</em>,
<em>13</em>(1), 303–320. (<a
href="https://doi.org/10.1109/TCC.2025.3525496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDMA over Converged Ethernet v2 (RoCEv2) is one of the most popular high-speed datacenter networking solutions. Watermark is the general term for various trigger and release thresholds of RoCEv2 flow control protocols, and its reasonable configuration is an important factor affecting RoCEv2 performance. In this paper, we propose ByteTuning, a centralized watermark tuning system for RoCEv2. First, three real cases of network performance degradation caused by non-optimal or improper watermark configuration are reported, and the network performance results of different watermark configurations in three typical scenarios are traversed, indicating the necessity of watermark tuning. Then, based on the RDMA Fluid model, the influence of watermark on the RoCEv2 performance is modeled and evaluated. Next, the design of the ByteTuning is introduced, which includes three mechanisms. They are 1) using simulated annealing algorithm to make the real-time watermark converge to the near-optimal configuration, 2) using network telemetry to optimize the feedback overhead, 3) compressing the search space to improve the tuning efficiency. Finally, We validate the performance of ByteTuning in multiple real datacenter networking environments, and the results show that ByteTuning outperforms existing solutions.},
  archive      = {J_TCC},
  author       = {Lizhuang Tan and Zhuo Jiang and Kefei Liu and Haoran Wei and Pengfei Huo and Huiling Shi and Wei Zhang and Wei Su},
  doi          = {10.1109/TCC.2025.3525496},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {303-320},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {ByteTuning: Watermark tuning for RoCEv2},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-edge collaborative service architecture with
large-tiny models based on deep reinforcement learning. <em>TCC</em>,
<em>13</em>(1), 288–302. (<a
href="https://doi.org/10.1109/TCC.2024.3525076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offshore drilling platforms (ODPs) are critical infrastructure for exploring and developing marine oil and gas resources. As these platforms’ capabilities expand, deploying intelligent surveillance services to ensure safe production has become increasingly important. However, the unique geographical locations and harsh environmental conditions of ODPs pose significant challenges for processing large volumes of video data, complicating the implementation of efficient surveillance systems. This study proposes a Cloud-Edge Large-Tiny Model Collaborative (CELTC) architecture grounded in deep reinforcement learning to optimize the processing and decision-making of surveillance data in offshore drilling platform scenarios. CELTC architecture leverages edge-cloud computing, deploying complex, high-precision large models on cloud servers and lightweight tiny models on edge devices. This dual deployment strategy capitalizes on tiny models’ rapid response and large cloud models’ high-precision capabilities. Additionally, the architecture integrates a deep reinforcement learning algorithm designed to optimize the scheduling and offloading of computational tasks between large and tiny models in the cloud-edge environment. The efficacy of the proposed architecture is validated using real-world surveillance data from ODPs through simulations and comparative experiments.},
  archive      = {J_TCC},
  author       = {Xiaofeng Ji and Faming Gong and Nuanlai Wang and Junjie Xu and Xing Yan},
  doi          = {10.1109/TCC.2024.3525076},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {288-302},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-edge collaborative service architecture with large-tiny models based on deep reinforcement learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient online computing offloading for budget-
constrained cloud-edge collaborative video streaming systems.
<em>TCC</em>, <em>13</em>(1), 273–287. (<a
href="https://doi.org/10.1109/TCC.2024.3524310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-Edge Collaborative Architecture (CEA) is a prominent framework that provides low-latency and energy-efficient solutions for video stream processing. In Cloud-Edge Collaborative Video Streaming Systems (CEAVS), efficient online offloading strategies for video tasks are crucial for enhancing user experience. However, most existing works overlook budget constraints, which limits their applicability in real-world scenarios constrained by finite resources. Moreover, they fail to adequately address the heterogeneity of video task redundancies, leading to suboptimal utilization of CEAVS&#39;s limited resources. To bridge these gaps, we propose an Efficient Online Computing framework for CEAVS (EOCA) that jointly optimizes accuracy, energy consumption, and latency performance through adaptive online offloading and redundancy compression, without requiring future task information. Technically, we formulate computing offloading and adaptive compression under budget constraints as a stochastic optimization problem that maximizes system satisfaction, defined as a weighted combination of accuracy, latency, and energy performance. We employ Lyapunov optimization to decouple the long-term budget constraint. We prove that the decoupled problem is a generalized ordinal potential game and propose algorithms based on generalized Benders decomposition (GBD) and the best response to obtain Nash equilibrium strategies for computing offloading and task compression. Finally, we analyze EOCA&#39;s performance bound, convergence rate, and worst-case performance guarantees. Evaluations demonstrate that EOCA effectively improves satisfaction while effectively balancing satisfaction and computational overhead.},
  archive      = {J_TCC},
  author       = {Shijing Yuan and Yuxin Liu and Song Guo and Jie Li and Hongyang Chen and Chentao Wu and Yang Yang},
  doi          = {10.1109/TCC.2024.3524310},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {273-287},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient online computing offloading for budget- constrained cloud-edge collaborative video streaming systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private and truthful reverse auction with
dynamic resource provisioning for VNFI procurement in NFV markets.
<em>TCC</em>, <em>13</em>(1), 259–272. (<a
href="https://doi.org/10.1109/TCC.2024.3522963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of network function virtualization (NFV), many users resort to network service provisioning through virtual network function instances (VNFIs) run on the standard physical server in clouds. Following this trend, NFV markets are emerging, which allow a user to procure VNFIs from cloud service providers (CSPs). In such procurement process, it is a significant challenge to ensure differential privacy and truthfulness while explicitly considering dynamic resource provisioning, location sensitiveness and budget of each VNFI. As such, we design a differentially private and truthful reverse auction with dynamic resource provisioning (PTRA-DRP) to resolve the VNFI procurement (VNFIP) problem. To allow dynamic resource provisioning, PTRA-DRP enables CSPs to submit a set of bids and accept as many as possible, and decides the provisioning VNFIs based on the auction outcomes. To be specific, we first devise a greedy heuristic approach to select the set of the winning bids in a differentially privacy-preserving manner. Next, we design a pricing strategy to compute the charges of CSPs, aiming to guarantee truthfulness. Strict theoretical analysis proves that PTRA-DRP can ensure differential privacy, truthfulness, individual rationality, computational efficiency and approximate social cost minimization. Extensive simulations also demonstrate the effectiveness and efficiency of PTRA-DRP.},
  archive      = {J_TCC},
  author       = {Xueyi Wang and Xingwei Wang and Zhitong Wang and Rongfei Zeng and Ruiyun Yu and Qiang He and Min Huang},
  doi          = {10.1109/TCC.2024.3522963},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {259-272},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Differentially private and truthful reverse auction with dynamic resource provisioning for VNFI procurement in NFV markets},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SROdcn: Scalable and reconfigurable optical DCN architecture
for high-performance computing. <em>TCC</em>, <em>13</em>(1), 245–258.
(<a href="https://doi.org/10.1109/TCC.2024.3523433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Center Network (DCN) flexibility is critical for providing adaptive and dynamic bandwidth while optimizing network resources to manage variable traffic patterns generated by heterogeneous applications. To provide flexible bandwidth, this work proposes a machine learning approach with a new Scalable and Reconfigurable Optical DCN (SROdcn) architecture that maintains dynamic and non-uniform network traffic according to the scale of the high-performance optical interconnected DCN. Our main device is the Fiber Optical Switch (FOS), which offers competitive wavelength resolution. We propose a new top-of-rack (ToR) switch that utilizes Wavelength Selective Switches (WSS) to investigate Software-Defined Networking (SDN) with machine learning-enabled flow prediction for reconfigurable optical Data Center Networks (DCNs). Our architecture provides highly scalable and flexible bandwidth allocation. Results from Mininet experimental simulations demonstrate that under the management of an SDN controller, machine learning traffic flow prediction and graph connectivity allow each optical bandwidth to be automatically reconfigured according to variable traffic patterns. The average server-to-server packet delay performance of the reconfigurable SROdcn improves by 42.33% compared to inflexible interconnects. Furthermore, the network performance of flexible SROdcn servers shows up to a 49.67% latency improvement over the Passive Optical Data Center Architecture (PODCA), a 16.87% latency improvement over the optical OPSquare DCN, and up to a 71.13% latency improvement over the fat-tree network. Additionally, our optimized Unsupervised Machine Learning (ML-UnS) method for SROdcn outperforms Supervised Machine Learning (ML-S) and Deep Learning (DL).},
  archive      = {J_TCC},
  author       = {Kassahun Geresu and Huaxi Gu and Xiaoshan Yu and Meaad Fadhel and Hui Tian and Wenting Wei},
  doi          = {10.1109/TCC.2024.3523433},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {245-258},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SROdcn: Scalable and reconfigurable optical DCN architecture for high-performance computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the availability and security of attestation
scheme for multiparty-involved DLaaS: A circular approach. <em>TCC</em>,
<em>13</em>(1), 227–244. (<a
href="https://doi.org/10.1109/TCC.2024.3522993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a remote attestation approach based on multiple verifiers named CARE. CARE aims to enhance the practicality and efficiency of remote attestation while addressing trust issues within environments involving multiple stakeholders. Specifically, CARE adopts the concept of swarm verification, and employs a circular collaboration model with multiple verifiers to collect and validate evidence, thereby resolving trust issues and enhancing verification efficiency. Moreover, CARE introduces a meticulously designed filtering mechanism to address the issue of false positives in verification outcomes non-invasively. CARE utilizes a multiway tree structure to construct the baseline value library, which enhances the flexibility and fine-grained management capability of the system. Security analysis indicates that CARE can effectively resist collusion attacks. Further, detailed simulation experiments have validated its capability to convincingly attest to the trustworthiness of the dynamically constructed environment. Notably, CARE is also suitable for the remote attestation of large-scale virtual machines, achieving an efficiency 9 times greater than the classical practice approach. To the best of our knowledge, CARE is the first practical solution to address inaccuracies in remote attestation results caused by the activation of Integrity Measurement Architecture (IMA) at the application layer.},
  archive      = {J_TCC},
  author       = {Miaomiao Yang and Guosheng Huang and Honghai Chen and Yongyi Liao and Qixu Wang and Xingshu Chen},
  doi          = {10.1109/TCC.2024.3522993},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {227-244},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enhancing the availability and security of attestation scheme for multiparty-involved DLaaS: A circular approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StreamSys: A lightweight executable delivery system for edge
computing. <em>TCC</em>, <em>13</em>(1), 213–226. (<a
href="https://doi.org/10.1109/TCC.2024.3521978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing brings several challenges when it comes to data movement. First, moving large data from edge devices to the server is likely to waste bandwidth. Second, complex data patterns (e.g., traffic cameras) on devices require flexible handling. An ideal approach is to move code to data instead. However, since only a small portion of code is required, moving the executable as well as their libraries to the devices can be an overkill. While loading code on demand from remote such as NFS can be a stopgap, but on the other hand leads to low efficiency for irregular access patterns. This article presents StreamSys, a lightweight executable delivery system that loads code on demand by redirecting the local disk IO to the server through optimized network IO. We employ a Markov-based prefetch mechanism on the server side. It learns the access pattern of code and predicts the block sequence for the client to reduce the network round trip. Meanwhile, server-side StreamSys asynchronously prereads the block sequence from the disk to conceal disk IO latency beforehand. Evaluation shows that the latency of StreamSys is up to 71.4% lower than the native Linux file system based on SD card and up to 62% lower than NFS in wired environments.},
  archive      = {J_TCC},
  author       = {Jun Lu and Zhenya Ma and Yinggang Gao and Sheng Yue and Ju Ren and Yaoxue Zhang},
  doi          = {10.1109/TCC.2024.3521978},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {213-226},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {StreamSys: A lightweight executable delivery system for edge computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding serverless inference in mobile-edge networks:
A benchmark approach. <em>TCC</em>, <em>13</em>(1), 198–212. (<a
href="https://doi.org/10.1109/TCC.2024.3521657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the emerging serverless paradigm has the potential to become a dominant way of deploying cloud-service tasks across millions of mobile and IoT devices, the overhead characteristics of executing these tasks on such a volume of mobile devices remain largely unclear. To address this issue, this paper conducts a deep analysis based on the OpenFaaS platform—a popular open-source serverless platform for mobile edge environments—to investigate the overhead of performing deep learning inference tasks on mobile devices. To thoroughly evaluate the inference overhead, we develop a performance benchmark, named ESBench, whereby a set of comprehensive experiments are conducted with respect to a bunch of simulated mobile devices associated with an edge cluster. Our investigation reveals that the performance of deep learning inference tasks is significantly influenced by the model size and resource contention in mobile devices, leading to up to $3\times$ degradation in performance. Moreover, we observe that the network environment can negatively impact the performance of mobile inference, increasing the CPU overhead under poor network conditions. Based on our findings, we further propose some recommendations for designing efficient serverless platforms and resource management strategies as well as for deploying serverless computing in the mobile edge environment.},
  archive      = {J_TCC},
  author       = {Junhong Chen and Yanying Lin and Shijie Peng and Shuaipeng Wu and Kenneth Kent and Hao Dai and Kejiang Ye and Yang Wang},
  doi          = {10.1109/TCC.2024.3521657},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {198-212},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Understanding serverless inference in mobile-edge networks: A benchmark approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing sustainability in data centers: Evaluation of
hybrid air/liquid cooling schemes for IT payload using sea water.
<em>TCC</em>, <em>13</em>(1), 184–197. (<a
href="https://doi.org/10.1109/TCC.2024.3521666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth in cloud computing, Big Data, AI and high-performance computing (HPC) necessitate the deployment of additional data centers (DC’s) with high energy demands. The unprecedented increase in the Thermal Design Power (TDP) of the computing chips will require innovative cooling techniques. Furthermore, DC’s are increasingly limited in their ability to add powerful GPU servers by power capacity constraints. As cooling energy use accounts for up to 40% of DC energy consumption, creative cooling solutions are urgently needed to allow deployment of additional servers, enhance sustainability and increase energy efficiency of DC’s. The information in this study is provided from Start Campus’ Sines facility supported by Alfa Laval for the heat exchanger and CO2 emission calculations. The study evaluates the performance and sustainability impact of various data center cooling strategies including an air-only deployment and a subsequent hybrid air/water cooling solution all utilizing sea water as the cooling source. We evaluate scenarios from 3 MW to 15+1 MW of IT load in 3 MW increments which correspond to the size of heat exchangers used in the Start Campus’ modular system design. This study also evaluates the CO2 emissions compared to a conventional chiller system for all the presented scenarios. Results indicate that the effective use of the sea water cooled system combined with liquid cooled systems improve the efficiency of the DC, plays a role in decreasing the CO2 emissions and supports in achieving sustainability goals.},
  archive      = {J_TCC},
  author       = {Imran Latif and Muhammad Mubashar Ashraf and Umaima Haider and Gemma Reeves and Alexandrina Untaroiu and Fábio Coelho and Denis Browne},
  doi          = {10.1109/TCC.2024.3521666},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {184-197},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Advancing sustainability in data centers: Evaluation of hybrid Air/Liquid cooling schemes for IT payload using sea water},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI applications resource allocation in computing continuum:
A stackelberg game approach. <em>TCC</em>, <em>13</em>(1), 166–183. (<a
href="https://doi.org/10.1109/TCC.2024.3521213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth, development, and commercialization of artificial intelligence-based technologies such as self-driving cars, augmented-reality viewers, chatbots, and virtual assistants are driving the need for increased computing power. Most of these applications rely on Deep Neural Networks (DNNs), which demand substantial computing capacity to meet user demands. However, this capacity cannot be fully provided by users’ local devices due to their limited processing power, nor by cloud data centers due to high transmission latency from long distances. Edge cloud computing addresses this issue by processing user requests through 5G, which reduces transmission latency from local devices to computing resources and allows the offloading of some computations to cloud back-ends. This paper introduces a model for a Mobile Edge Cloud system designed for an application based on a DNN. The interaction among multiple mobile users and the edge platform is formulated as a one-leader multi-follower Stackelberg game, resulting in a challenging non-convex mixed integer nonlinear programming (MINLP) problem. To tackle this, we propose a heuristic approach based on Karush-Kuhn-Tucker conditions, which solves the MINLP problem significantly faster than the commercial state-of-the-art solvers (up to 50,000 times). Furthermore, we present an algorithm to estimate optimal platform profit when sensitive user parameters are unknown. Comparing this with the full-knowledge scenario, we observe a profit loss of approximately 1%. Lastly, we analyze the advantages for an edge provider to engage in a Stackelberg game rather than setting a fixed price for its users, showing potential profit increases ranging from 16% to 66%.},
  archive      = {J_TCC},
  author       = {Roberto Sala and Hamta Sedghani and Mauro Passacantando and Giacomo Verticale and Danilo Ardagna},
  doi          = {10.1109/TCC.2024.3521213},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {166-183},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {AI applications resource allocation in computing continuum: A stackelberg game approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage learning approach for semantic-aware task
scheduling in container-based clouds. <em>TCC</em>, <em>13</em>(1),
148–165. (<a href="https://doi.org/10.1109/TCC.2024.3520101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container-based task scheduling is critical for ensuring a reliable, flexible and cost-effective cloud computing mode. However, in different business cloud systems, state-of-the-art scheduling models are not as effective as those in the simulated world due to the sparsity issues associated with sample sizes and features. Herein, we propose a novel containerized task scheduling framework (SA2CTS) based on reinforcement learning (RL) that incorporates cross-modal contrastive learning (CL) loss. This framework optimizes the scheduler&#39;s understanding of the container-based cloud state in RL by adding a pretraining stage, promoting accurate scheduling action inference. Specifically, we design a two-stage learning pipeline. The initial stage involves pretraining the model on a large collection of aligned image-text pairs to extract fine-grained scheduling affinity features, and the high-level semantic representations of scheduling tasks are learned in the multimodal space. In the second stage, we fine-tune the pretrained model with multisource cluster feedback, i.e., build a mapping from state representations to scheduling actions through the RL paradigm, achieving task-oriented and semantic-aware scheduling. The experimental results obtained on three large-scale production cluster datasets substantiate that the proposed SA2CTS method can provide average convergence efficiency and resource utilization improvements of 17.57% and 10.42%, respectively, over the state-of-the-art RL scheduling methods.},
  archive      = {J_TCC},
  author       = {Lilu Zhu and Kai Huang and Yanfeng Hu and Yang Wang},
  doi          = {10.1109/TCC.2024.3520101},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {148-165},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Two-stage learning approach for semantic-aware task scheduling in container-based clouds},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SST-LOF: Container anomaly detection method based on
singular spectrum transformation and local outlier factor. <em>TCC</em>,
<em>13</em>(1), 130–147. (<a
href="https://doi.org/10.1109/TCC.2024.3514297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of container cloud platforms has experienced rapid growth. However, because containers are operating-system-level virtualization, their isolation is far less than that of virtual machines, posing considerable challenges for multi-tenant container cloud platforms. To address the issues associated with current container anomaly detection algorithms, such as the difficulty in mining periodic features and the high rate of false positives due to noisy data, we propose an anomaly detection method named SST-LOF, based on singular spectrum transformation and the local outlier factor. Our method enhances the traditional Singular Spectrum Transformation (SST) algorithm to meet the needs of streaming unsupervised detection. Furthermore, our method improves the calculation mode of the anomaly score of the Local Outlier Factor algorithm (LOF) and reduces false positives of noisy data with dynamic sliding windows. Additionally, we have designed and implemented a container cloud anomaly detection system that can perform real-time, unsupervised, streaming anomaly detection on containers quickly and accurately. The experimental results demonstrate the effectiveness and efficiency of our method in detecting anomalies in containers in both simulated and real cloud environments.},
  archive      = {J_TCC},
  author       = {Shilei Bu and Minpeng Jin and Jie Wang and Yulai Xie and Liangkang Zhang},
  doi          = {10.1109/TCC.2024.3514297},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {130-147},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SST-LOF: Container anomaly detection method based on singular spectrum transformation and local outlier factor},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IBNR-RD: Intra-block neighborhood relationship-based
resemblance detection for high-performance multi-node
post-deduplication. <em>TCC</em>, <em>13</em>(1), 118–129. (<a
href="https://doi.org/10.1109/TCC.2024.3514784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-deduplication in traditional cloud environments primarily focuses on single-node, where delta compression is performed on the same deduplication node located on server side. However, with data explosion, the multi-node post-deduplication, also called global deduplication, has become a hot issue in research communities, which aims to simultaneously execute delta compression on data distributed across all nodes. Simply setting up single-node deduplication systems on multi-node environments would significantly affect storage utilization and incur secondary overhead from file migration. Nevertheless, existing global deduplication solutions suffer from lower data compression ratios and high computational overhead due to their resemblance detection&#39;s inherent limitations and overly coarse granularities. Similar blocks typically have high correlations between sub-blocks; inspired by this observation, we propose IBNR (Intra-Block Neighborhood Relationship-Based Resemblance Detection for High-Performance Multi-Node Post-Deduplication), which introduces a novel resemblance detection based on relationships between sub-blocks and determines the ownership of blocks in entry stage to achieve efficient global deduplication. Furthermore, the by-products of IBNR have shown powerful scalability by replacing internal resemblance detection scheme with existing solutions on practical workloads. Experimental results indicate that IBNR outperforms state-of-the-art solutions, achieving an average 1.99× data reduction ratio and varying degrees of improvement across other key metrics.},
  archive      = {J_TCC},
  author       = {Dewen Zeng and Wenlong Tian and Tingting He and Ruixuan Li and Xuming Ye and Zhiyong Xu},
  doi          = {10.1109/TCC.2024.3514784},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {118-129},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {IBNR-RD: Intra-block neighborhood relationship-based resemblance detection for high-performance multi-node post-deduplication},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient dynamic resource management for spatial
multitasking GPUs. <em>TCC</em>, <em>13</em>(1), 99–117. (<a
href="https://doi.org/10.1109/TCC.2024.3511548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of microservice architecture enables complex cloud applications to be realized via a set of individually isolated components, increasing their flexibility and performance. As these applications require massive computing resources, graphics processing units (GPUs) are being widely used as high-speed parallel computing devices to meet the stringent demands. Although current GPUs allow application components to be executed concurrently via spatial multitasking, they face several challenges. The first challenge is allocating the computing resources to components dynamically to maximize efficiency. The second challenge is avoiding performance degradation caused by the data transfer overhead between the components. To address these challenges, we propose an efficient GPU resource management technique that dynamically allocates GPU resources to application components. The proposed method allocates resources based on component workloads and uses online performance monitoring to guarantee the application&#39;s performance. We also propose a GPU memory manager to reduce the data transfer overhead between components via shared memory. Our evaluation results indicate that the proposed dynamic resource allocation method improves application throughput by up to 134.12% compared to the state-of-the-art spatial multitasking techniques. We also show that using a shared memory results in 6x throughput improvement compared to the baseline User Datagram Protocol (UDP)-based technique.},
  archive      = {J_TCC},
  author       = {Hoda Sedighi and Daniel Gehberger and Amin Ebrahimzadeh and Fetahi Wuhib and Roch H. Glitho},
  doi          = {10.1109/TCC.2024.3511548},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {99-117},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient dynamic resource management for spatial multitasking GPUs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optical self-adjusting data center networks in the scalable
matching model. <em>TCC</em>, <em>13</em>(1), 87–98. (<a
href="https://doi.org/10.1109/TCC.2024.3510916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-Adjusting Networks (SAN) optimize their physical topology toward the demand in an online manner. Their application in data center networks is motivated by emerging hardware technologies, such as 3D MEMS Optical Circuit Switches (OCS). The Matching Model (MM) has been introduced to study the hybrid architecture of such networks. It abstracts from the electrical switches and focuses on the added (reconfigurable) optical ones. MM defines any SAN topology as a union of matchings over a set of top-of-rack (ToR) nodes, and assumes that rearranging the edges of a single matching comes at a fixed cost. In this work, we propose and study the Scalable Matching Model (SMM), a generalization of the MM, and present OpticNet, a framework that maps a set of ToRs to a set of OCSs to form a SAN topology. We prove that OpticNet uses the minimum number of switches to realize any bounded-degree topology and allows existing SAN algorithms to run on top of it, while preserving amortized performance guarantees. Our experimental results based on real workloads show that OpticNet is a flexible and efficient framework for the implementation and evaluation of SAN algorithms in reconfigurable data center environments.},
  archive      = {J_TCC},
  author       = {Caio Alves Caldeira and Otávio Augusto de Oliveira Souza and Olga Goussevskaia and Stefan Schmid},
  doi          = {10.1109/TCC.2024.3510916},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {87-98},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optical self-adjusting data center networks in the scalable matching model},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient delegatable order-revealing encryption scheme
for multi-user range queries. <em>TCC</em>, <em>13</em>(1), 75–86. (<a
href="https://doi.org/10.1109/TCC.2024.3506614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To balance data confidentiality and availability, order-revealing encryption (ORE) has emerged as a pivotal primitive facilitating range queries on encrypted data. However, challenges arise in diverse user domains where data is encrypted with different keys, giving rise to the development of delegatable order-revealing encryption (DORE) schemes. Regrettably, existing DORE schemes are susceptible to authorization token forgery attacks and rely on computationally intensive bilinear pairings. This work proposes a novel solution to address these challenges. We first introduce a delegatable equality-revealing encryption scheme, enabling the comparison of ciphertexts encrypted by distinct secret keys through authorization tokens. Building upon this, we present a delegatable order-revealing encryption that leverages bitwise encryption. DORE supports efficient multi-user ciphertext comparison while robustly resisting authorization token forgery attacks. Significantly, our approach distinguishes itself by minimizing bilinear pairings. Experimental results highlight the efficacy of DORE, showcasing a notable speedup of $2.8\times$ in encryption performance and $1.33\times$ in comparison performance compared to previous DORE schemes, respectively.},
  archive      = {J_TCC},
  author       = {Jingru Xu and Cong Peng and Rui Li and Jintao Fu and Min Luo},
  doi          = {10.1109/TCC.2024.3506614},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {75-86},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An efficient delegatable order-revealing encryption scheme for multi-user range queries},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A run-time framework for ensuring zero-trust state of
client’s machines in cloud environment. <em>TCC</em>, <em>13</em>(1),
61–74. (<a href="https://doi.org/10.1109/TCC.2024.3503358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the unprecedented demand for cloud computing, ensuring trust in the underlying environment is challenging. Applications executing in the cloud are prone to attacks of different types including malware, network and data manipulation. These attacks may remain undetected for a significant length of time thus causing a lack of trust. Untrusted cloud services can also lead to business losses in many cases and therefore need urgent attention. In this paper, we present Trusted Public Cloud (TPC), a generic framework ensuring the Zero-trust security of client machine. It tracks the system state, alerting the user of unexpected changes in the machine’s state, thus increasing the run-time detection of security vulnerabilities. We validated TPC on Microsoft Azure with Local, Software Trusted Platform Module (SWTPM) and Software Guard Extension (SGX)-enabled SWTPM security providers. We also evaluated the scalability of TPC on Amazon Web Services (AWS) with a varying number of client machines executing in a concurrent environment. The execution results show the effectiveness of TPC as it takes a maximum of 35.6 seconds to recognise the system state when there are 128 client machines attached.},
  archive      = {J_TCC},
  author       = {Devki Nandan Jha and Graham Lenton and James Asker and David Blundell and Martin Higgins and David C. H. Wallom},
  doi          = {10.1109/TCC.2024.3503358},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {61-74},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A run-time framework for ensuring zero-trust state of client’s machines in cloud environment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyperPart: A hypergraph-based abstraction for deduplicated
storage systems. <em>TCC</em>, <em>13</em>(1), 46–60. (<a
href="https://doi.org/10.1109/TCC.2024.3502464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, deduplication techniques are utilized to minimize the space overhead by deleting redundant data blocks across large-scale servers in data centers. However, such a process exacerbates the fragmentation of data blocks, causing more cross-server file retrievals with plummeting retrieval throughput. Some attempts prefer better file retrieval performance by confining all blocks of a file to one single server, resulting in non-trivial space consumption for more replicated blocks across servers. An ideal network storage system, in effect, should take both the deduplication and retrieval performance into account by implementing reasonable assignment of the detected unique blocks. Such a fine-grained assignment requires an accurate and comprehensive abstraction of the files, blocks, and the file-block affiliation relationships. To achieve this, we innovatively design the weighted hypergraph to profile the multivariate data correlations. With this delicate abstraction in place, we propose HyperPart, which elegantly transforms this complex block allocation problem into a hypergraph partition problem. For more general scenarios with dynamic file updates, we further propose a two-phase incremental hypergraph repartition scheme, which mitigates the performance degradation with minimal migration volume. We implement a prototype system of HyperPart, and the experiment results validate that it saves around 50% of the storage space and improves the retrieval throughput by approximately 30% of state-of-the-art methods under the balance constraints.},
  archive      = {J_TCC},
  author       = {Geyao Cheng and Junxu Xia and Lailong Luo and Haibo Mi and Deke Guo and Richard T. B. Ma},
  doi          = {10.1109/TCC.2024.3502464},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {46-60},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HyperPart: A hypergraph-based abstraction for deduplicated storage systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method to compare scaling algorithms for cloud-based
services. <em>TCC</em>, <em>13</em>(1), 34–45. (<a
href="https://doi.org/10.1109/TCC.2024.3500139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many services are offered via the cloud, i.e., they rely on interacting software components that can run on a set of connected Commercial Off-The-Shelf (COTS) servers sitting in data centers. As the demand for any particular service evolves over time, the computational resources associated with the service must be scaled accordingly while keeping the Key Performance Indicators (KPIs) associated with the service under control. Consequently, scaling always involves a delicate trade-off between using the cloud resources and complying with the KPIs. In this paper, we show that a (workload-dependent) Pareto front embodies this trade-off’s limits. We identify this Pareto front for various workloads and assess the ability of several scaling algorithms to approach that Pareto front.},
  archive      = {J_TCC},
  author       = {Danny De Vleeschauwer and Chia-Yu Chang and Paola Soto and Yorick De Bock and Miguel Camelo and Koen De Schepper},
  doi          = {10.1109/TCC.2024.3500139},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {34-45},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A method to compare scaling algorithms for cloud-based services},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-granularity federated learning by graph-partitioning.
<em>TCC</em>, <em>13</em>(1), 18–33. (<a
href="https://doi.org/10.1109/TCC.2024.3494765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge computing, energy-limited distributed edge clients present challenges such as heterogeneity, high energy consumption, and security risks. Traditional blockchain-based federated learning (BFL) struggles to address all three of these challenges simultaneously. This article proposes a Graph-Partitioning Multi-Granularity Federated Learning method on a consortium blockchain, namely GP-MGFL. To reduce the overall communication overhead, we adopt a balanced graph partitioning algorithm while introducing observer and consensus nodes. This method groups clients to minimize high-cost communications and focuses on the guidance effect within each group, thereby ensuring effective guidance with reduced overhead. To fully leverage heterogeneity, we introduce a cross-granularity guidance mechanism. This mechanism involves fine-granularity models guiding coarse-granularity models to enhance the accuracy of the latter models. We also introduce a credit model to adjust the contribution of models to the global model dynamically and to dynamically select leaders responsible for model aggregation. Finally, we implement a prototype system on real physical hardware and compare it with several baselines. Experimental results show that the accuracy of the GP-MGFL algorithm is 5.6% higher than that of ordinary BFL algorithms. In addition, compared to other grouping methods, such as greedy grouping, the accuracy of the proposed method improves by about 1.5%. In scenarios with malicious clients, the maximum accuracy improvement reaches 11.1%. We also analyze and summarize the impact of grouping and the number of clients on the model, as well as the impact of this method on the inherent security of the blockchain itself.},
  archive      = {J_TCC},
  author       = {Ziming Dai and Yunfeng Zhao and Chao Qiu and Xiaofei Wang and Haipeng Yao and Dusit Niyato},
  doi          = {10.1109/TCC.2024.3494765},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {18-33},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-granularity federated learning by graph-partitioning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing renewable energy utilization in cloud data
centers through dynamic overbooking: An MDP-based approach.
<em>TCC</em>, <em>13</em>(1), 1–17. (<a
href="https://doi.org/10.1109/TCC.2024.3487954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shift towards renewable energy sources for powering data centers is increasingly important in the era of cloud computing. However, integrating renewable energy sources into cloud data centers presents a challenge due to their variable and intermittent nature. The unpredictable workload demands in cloud data centers further complicate this problem. In response to this pressing challenge, we propose a novel approach in this paper: adapting the workload to match the renewable energy supply. Our solution involves dynamic overbooking of resources, providing energy flexibility to data center operators. We propose a framework that stochastically models both workload and energy source information, leveraging Markov Decision Processes (MDP) to determine the optimal overbooking degree based on the workload flexibility of data center clients. We validate the proposed algorithm in realistic settings through extensive simulations. Results demonstrate the superiority of our proposed method over existing approaches, achieving better matching with the renewable energy supply by 55.6%, 34.65%, and 40.7% for workload traces from Nectar Cloud, Google, and Wikipedia, respectively.},
  archive      = {J_TCC},
  author       = {Tuhin Chakraborty and Carlo Kopp and Adel N. Toosi},
  doi          = {10.1109/TCC.2024.3487954},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimizing renewable energy utilization in cloud data centers through dynamic overbooking: An MDP-based approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
