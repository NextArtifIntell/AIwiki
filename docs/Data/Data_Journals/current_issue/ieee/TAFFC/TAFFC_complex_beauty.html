<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc---31">TAFFC - 31</h2>
<ul>
<li><details>
<summary>
(2025). AVES: An audio-visual emotion stream dataset for temporal
emotion detection. <em>IEEE Transactions on Affective Computing</em>,
<em>16</em>(1), 438–450. (<a
href="https://doi.org/10.1109/TAFFC.2024.3440924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Human emotions vary over time, which can be vividly described as a stream of emotions. Observing the emotion stream in daily life provides valuable insights into an individual&#39;s mental state. However, existing research in emotion understanding has mainly focused on classification tasks, assigning an emotion category to a well-trimmed segment or each frame within a continuous signal. In contrast, the task of temporal emotion detection, which involves locating the boundaries of emotion segments and recognizing their categories in untrimmed signals, has not been fully explored. To advance research in this area, this paper introduces an in-the-wild Audio-Visual Emotion Stream (AVES) dataset, which is reliably annotated with the time boundaries and emotion category for each emotion segment in the videos. Thus, AVES can serve as a solid benchmark for temporal emotion detection tasks. Moreover, considering the flexible boundaries and varying durations of emotion segments, we propose a Boundary Combination Network (BoCoNet) for temporal emotion detection, which leverages short-term temporal context information to first predict the boundaries of emotion segments and then locate the entire emotion segments. Extensive experiments conducted on various representative unimodal and multimodal representations demonstrate that BoCoNet achieves state-of-the-art results. The AVES dataset will be released to the research community. We expect that this paper can advance the research on emotion stream and temporal emotion detection.},
  archive  = {J},
  author   = {Yan Li and Wei Gan and Ke Lu and Dongmei Jiang and Ramesh Jain},
  doi      = {10.1109/TAFFC.2024.3440924},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {438-450},
  title    = {AVES: An audio-visual emotion stream dataset for temporal emotion detection},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distant handshakes: Conveying social intentions through
multi-modal soft haptic gloves. <em>IEEE Transactions on Affective
Computing</em>, <em>16</em>(1), 423–437. (<a
href="https://doi.org/10.1109/TAFFC.2024.3438761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Due to the lack of physical touch, social distancing caused by COVID-19 makes it difficult to convey social intentions through handshakes. To mitigate this situation, one promising solution is to simulate distinguishable handshake patterns through haptic feedback devices during distant social communication. This paper reports a distant handshake scheme with multi-modal soft haptic gloves. To address the challenge of duplicating rich haptic stimuli of real handshakes in a compact glove, we extracted four haptic features including grip location, grip strength, skin temperature, and shaking frequency from abundant components of the haptic perception system to mimic handshake behaviors. To guide the interference-free spatial layout of multiple actuators in a limited hand-sized space, we measured the handshake contact area and grip strength of different handshake patterns, which together with the thermosensitivity of the human hand determine the grip location, grip strength, and salient thermal stimulating location. We developed a multi-modal soft haptic glove by rendering four features through pneumatic pressure, thermal, and vibrotactile stimuli, respectively. A user study was conducted to validate the performance of our glove, which set two states for each of the four features, showing over 90% accuracy in distinguishing 16 handshake patterns. Furthermore, a user study on the identification of social intentions yields the finding that distant handshakes with our haptic gloves can convey positive, neutral, and negative social intentions. These results inform the potential of distant handshakes with haptic gloves to convey social intentions in remote interactions including business, politics, and daily life in severe and post-pandemic situations, as well as in future metaverse-based society.},
  archive  = {J},
  author   = {Qianqian Tong and Wenxuan Wei and Yuan Guo and Tianhao Jin and Ziqi Wang and Hongxing Zhang and Yuru Zhang and Dangxiao Wang},
  doi      = {10.1109/TAFFC.2024.3438761},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {423-437},
  title    = {Distant handshakes: Conveying social intentions through multi-modal soft haptic gloves},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SVFAP: Self-supervised video facial affect perceiver.
<em>IEEE Transactions on Affective Computing</em>, <em>16</em>(1),
405–422. (<a href="https://doi.org/10.1109/TAFFC.2024.3436913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Video-based facial affect analysis has recently attracted increasing attention owing to its critical role in human-computer interaction. Previous studies mainly focus on developing various deep learning architectures and training them in a fully supervised manner. Although significant progress has been achieved by these supervised methods, the longstanding lack of large-scale high-quality labeled data severely hinders their further improvements. Motivated by the recent success of self-supervised learning in computer vision, this paper introduces a self-supervised approach, termed Self-supervised Video Facial Affect Perceiver (SVFAP), to address the dilemma faced by supervised methods. Specifically, SVFAP leverages masked facial video autoencoding to perform self-supervised pre-training on massive unlabeled facial videos. Considering that large spatiotemporal redundancy exists in facial videos, we propose a novel temporal pyramid and spatial bottleneck Transformer as the encoder of SVFAP, which not only largely reduces computational costs but also achieves excellent performance. To verify the effectiveness of our method, we conduct experiments on nine datasets spanning three downstream tasks, including dynamic facial expression recognition, dimensional emotion recognition, and personality recognition. Comprehensive results demonstrate that SVFAP can learn powerful affect-related representations via large-scale self-supervised pre-training and it significantly outperforms previous state-of-the-art methods on all datasets.},
  archive  = {J},
  author   = {Licai Sun and Zheng Lian and Kexin Wang and Yu He and Mingyu Xu and Haiyang Sun and Bin Liu and Jianhua Tao},
  doi      = {10.1109/TAFFC.2024.3436913},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {405-422},
  title    = {SVFAP: Self-supervised video facial affect perceiver},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The affective audio dataset (AAD) for non-musical,
non-vocalized, audio emotion research. <em>IEEE Transactions on
Affective Computing</em>, <em>16</em>(1), 394–404. (<a
href="https://doi.org/10.1109/TAFFC.2024.3437153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Affective Audio Dataset (AAD) is a new and novel dataset of non-musical, non-anthropomorphic sounds intended for use in affective research. Sounds are annotated for their affective qualities by sets of human participants. The dataset was created in response to a lack of suitable datasets within the domain of audio emotion recognition. A total of 780 sounds are selected from the BBC Sounds Library. Participants are recruited online and asked to rate a subset of sounds based on how they make them feel. Each sound is rated for arousal and valence. It was found that while evenly distributed, there was bias towards the low-valence, high-arousal quadrant, and displayed a greater range of ratings in comparison to others. The AAD is compared with existing datasets to check its consistency and validity, with differences in data collection methods and intended use-cases highlighted. Using a subset of the data, the online ratings were validated against an in-person data collection experiment with findings strongly correlating. The AAD is used to train a basic affect-prediction model and results are discussed. Uses of this dataset include, human-emotion research, cultural studies, other affect-based research, and industry use such as audio post-production, gaming, and user-interface design.},
  archive  = {J},
  author   = {Harrison Ridley and Stuart Cunningham and John Darby and John Henry and Richard Stocker},
  doi      = {10.1109/TAFFC.2024.3437153},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {394-404},
  title    = {The affective audio dataset (AAD) for non-musical, non-vocalized, audio emotion research},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling gold standard moment-to-moment ratings of
perception of stress from audio recordings. <em>IEEE Transactions on
Affective Computing</em>, <em>16</em>(1), 376–393. (<a
href="https://doi.org/10.1109/TAFFC.2024.3435502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Enabling continuous and unobtrusive monitoring of stress is essential for delivering personalized stress interventions at opportune moments. To achieve automatic stress detection on a time-continuous basis, reliable moment-to-moment ratings of stress are required. However, the current research lacks a large-scale multimodal dataset that provides time-continuous ratings of perceived stress. Existing datasets mainly consist of single-valued self-reported ratings obtained after the stress-inducing task or rely on audio-visual recordings to capture moment-to-moment ratings from multiple annotators. The collection of time-continuous ratings of stress based solely on audio recordings has not been extensively explored. In this paper, we introduce an updated version of the publicly available VerBIO dataset that contains moment-to-moment ratings of perceived stress from multiple annotators. These annotators rated their perception of stress by listening to participants who had conducted a public speaking task. Time-continuous ratings of stress are obtained from four annotators using 22 hours of audio recordings from 339 public speaking sessions performed by 53 individuals. These time-continuous ratings of stress perception were obtained from the annotators solely based on speech, without incorporating the visual modality as an expressive marker. We examine the reliability of the annotation scheme employed in this study and investigate the factors contributing to the observed variation in perceived stress among annotators. Next, we introduce an annotation fusion technique based on expectation-maximization to obtain a reliable gold standard rating by aggregating the ratings from multiple annotators. Results indicate that the proposed annotation fusion technique yields aggregated ratings that can be estimated more reliably using acoustic features compared to the ratings yielded from conventional annotation fusion techniques. The newly generated annotations are publicly available within the proposed updated version of the existing VerBIO dataset, facilitating research in the field of continuous stress detection.},
  archive  = {J},
  author   = {Ehsanul Haque Nirjhar and Theodora Chaspari},
  doi      = {10.1109/TAFFC.2024.3435502},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {376-393},
  title    = {Modeling gold standard moment-to-moment ratings of perception of stress from audio recordings},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The expression of happiness in social media of individuals
reporting depression. <em>IEEE Transactions on Affective Computing</em>,
<em>16</em>(1), 360–375. (<a
href="https://doi.org/10.1109/TAFFC.2024.3434482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression has long been studied in the NLP field, with most works focusing on individuals’ negative emotions. People with depression experience happiness, but this has not been extensively studied. Previous works have shown that sentiment or emotion classification approaches are unsuitable for extracting happy moments because they may not be expressed only in positive words. In this work, we conduct a large-scale study of happy moments from social media texts of individuals mentioning a depression diagnosis. We develop an extensive deep learning-based framework to extract happy moments from text, and annotate them with semantic topics, gender labels, and agency and sociality measures. We analyze over 400,000 happy moments and show significant differences in topics, agency, and sociality of users in the depression and control groups, varying by gender. We found that male and female users in the depression group expressed more sociality in their happy moments than control users. Furthermore, male users’ agency was not impaired in depression, while female users in the depression group expressed fewer happy moments with agency than the control group. Our research can inform psychology interventions, which can foster feelings of longer-lasting happiness and represent a promising path of collaboration between computational linguistics and psychology.},
  archive  = {J},
  author   = {Ana-Maria Bucur and Berta Chulvi and Adrian Cosma and Paolo Rosso},
  doi      = {10.1109/TAFFC.2024.3434482},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {360-375},
  title    = {The expression of happiness in social media of individuals reporting depression},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mechanoreceptive aβ primary afferents discriminate
naturalistic social touch inputs at a functionally relevant time scale.
<em>IEEE Transactions on Affective Computing</em>, <em>16</em>(1),
346–359. (<a href="https://doi.org/10.1109/TAFFC.2024.3435060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Interpersonal touch is an important channel of social emotional interaction. How these physical skin-to-skin touch expressions are processed in the peripheral nervous system is not well understood. From microneurography recordings in humans, we evaluated the capacity of six subtypes of cutaneous mechanoreceptive afferents to differentiate human-delivered social touch expressions. Leveraging statistical and classification analyses, we found that single units of multiple mechanoreceptive Aβ subtypes, especially slowly adapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can reliably differentiate social touch expressions at accuracies similar to human recognition. We then identified the most informative firing patterns of SA-II and HFA afferents, which indicate that average durations of 3-4 s of firing provide sufficient discriminative information. Those two subtypes also exhibit robust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch expressions due to their specific firing properties. Greater shifts in spike-timing, however, can change a firing pattern&#39;s envelope to resemble that of another expression and drastically compromise an afferent&#39;s discrimination capacity. Altogether, the findings indicate that SA-II and HFA afferents differentiate the skin contact of social touch at time scales relevant for such interactions, which are 1-2 orders of magnitude longer than those for non-social touch.},
  archive  = {J},
  author   = {Shan Xu and Steven C. Hauser and Saad S. Nagi and James A. Jablonski and Merat Rezaei and Ewa Jarocka and Andrew G. Marshall and Håkan Olausson and Sarah McIntyre and Gregory J. Gerling},
  doi      = {10.1109/TAFFC.2024.3435060},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {346-359},
  title    = {Mechanoreceptive aβ primary afferents discriminate naturalistic social touch inputs at a functionally relevant time scale},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multivariate dynamics of emotions through
time-varying self-assessed arousal and valence ratings. <em>IEEE
Transactions on Affective Computing</em>, <em>16</em>(1), 333–345. (<a
href="https://doi.org/10.1109/TAFFC.2024.3434456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions arise from a complex interplay of various factors, including conscious experience, physiological processes, and contextual elements. Although emotions are inherently dynamic processes, this aspect is oftentimes neglected in experimental protocols. In this study, we employed dynamical systems theory to investigate the time-varying self-assessed emotion ratings. We used the continuous ratings of the publicly available CASE dataset, in which thirty individuals rated their level of arousal and valence while watching videos designed to evoke four different emotions. First, we analyzed the univariate dynamics by reconstructing the phase space from the arousal and valence series separately, and quantified their regularity and spatial complexity by using three metrics: Fuzzy, Sample, and Distribution Entropy. Then, we combined the arousal and valence series and proposed a novel index, the Multichannel Distribution Entropy (MDistEn), to estimate the complexity of the bivariate phase space. By coupling the two dimensions, we found that MDistEn resulted as an effective marker of fear, showing patterns statistically different from all of the other stimuli (p-value $\leq$ 0.001). These findings support the investigation of the time-varying dynamics of annotated emotion ratings as a promising pathway to discriminate the onset of fear-related pathological states.},
  archive  = {J},
  author   = {Andrea Gargano and Mimma Nardelli and Enzo Pasquale Scilingo},
  doi      = {10.1109/TAFFC.2024.3434456},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {333-345},
  title    = {Exploring multivariate dynamics of emotions through time-varying self-assessed arousal and valence ratings},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grop: Graph orthogonal purification network for EEG emotion
recognition. <em>IEEE Transactions on Affective Computing</em>,
<em>16</em>(1), 319–332. (<a
href="https://doi.org/10.1109/TAFFC.2024.3433613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The existence of emotion-irrelevant representations and individual variability impedes the extraction of robust emotional representations, limiting the adaptability of EEG emotion recognition. Massive studies focus on the mining of emotion-aware information, overlooking emotion-agnostic information, which is insufficient for the extraction of emotion-relevant features against redundancy and variation. In this paper, Graph Orthogonal Purification Network (Grop) is proposed to enhance individual adaptability through improvements in the orthogonality and transferability between emotion-relevant and emotion-irrelevant features. Specifically, the proposed Grop utilized a graph representation extraction module to capture both emotion-relevant and emotion-irrelevant features by the dual graph. The representation orthogonal purification module is developed to eliminate redundant information through feature projection and feature purification. Moreover, the dual emotional space alignment module is imposed to align distribution discrepancies in different emotion feature spaces. To assess the effectiveness of the proposed Grop, various experiments are conducted on two public EEG emotion datasets, i.e., SEED and SEED-IV. The results achieve state-of-the-art performance, demonstrating the capability of the Grop to capture robust emotion features and alleviate the intra- and inter-subject discrepancies.},
  archive  = {J},
  author   = {Mengqi Wu and C. L. Philip Chen and Bianna Chen and Tong Zhang},
  doi      = {10.1109/TAFFC.2024.3433613},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {319-332},
  title    = {Grop: Graph orthogonal purification network for EEG emotion recognition},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Versatile audio-visual learning for emotion recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>16</em>(1),
306–318. (<a href="https://doi.org/10.1109/TAFFC.2024.3433386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most current audio-visual emotion recognition models lack the flexibility needed for deployment in practical applications. We envision a multimodal system that works even when only one modality is available and can be implemented interchangeably for either predicting emotional attributes or recognizing categorical emotions. Achieving such flexibility in a multimodal emotion recognition system is difficult due to the inherent challenges in accurately interpreting and integrating varied data sources. It is also a challenge to robustly handle missing or partial information while allowing direct switch between regression or classification tasks. This study proposes a versatile audio-visual learning (VAVL) framework for handling unimodal and multimodal systems for emotion regression or emotion classification tasks. We implement an audio-visual framework that can be trained even when audio and visual paired data is not available for part of the training set (i.e., audio only or only video is present). We achieve this effective representation learning with audio-visual shared layers, residual connections over shared layers, and a unimodal reconstruction task. Our experimental results reveal that our architecture significantly outperforms strong baselines on the CREMA-D, MSP-IMPROV, and CMU-MOSEI corpora. Notably, VAVL attains a new state-of-the-art performance in the emotional attribute prediction task on the MSP-IMPROV corpus.},
  archive  = {J},
  author   = {Lucas Goncalves and Seong-Gyun Leem and Wei-Cheng Lin and Berrak Sisman and Carlos Busso},
  doi      = {10.1109/TAFFC.2024.3433386},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {306-318},
  title    = {Versatile audio-visual learning for emotion recognition},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised dual-stream self-attentive adversarial graph
contrastive learning for cross-subject EEG-based emotion recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>16</em>(1),
290–305. (<a href="https://doi.org/10.1109/TAFFC.2024.3433470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentive fusion module is developed for feature fusion, sample selection, and emotion recognition, which highlights EEG features more relevant to emotions and data samples in the labeled source domain that are closer to the target domain. Extensive experiments are conducted on four benchmark databases (SEED, SEED-IV, SEED-V, and FACED) using a semi-supervised cross-subject leave-one-subject-out cross-validation evaluation protocol. The results show that the proposed model outperforms existing methods under different incomplete label conditions with an average improvement of 2.17%, which demonstrates its effectiveness in addressing the label scarcity problem in cross-subject EEG-based emotion recognition.},
  archive  = {J},
  author   = {Weishan Ye and Zhiguo Zhang and Fei Teng and Min Zhang and Jianhong Wang and Dong Ni and Fali Li and Peng Xu and Zhen Liang},
  doi      = {10.1109/TAFFC.2024.3433470},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {290-305},
  title    = {Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject EEG-based emotion recognition},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vicarious evaluation of a decision model for human-agent
social touch interactions. <em>IEEE Transactions on Affective
Computing</em>, <em>16</em>(1), 277–289. (<a
href="https://doi.org/10.1109/TAFFC.2024.3433009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we present a decision model aimed at determining when a social touch by a socially interactive agent (SIA) would be considered socially ‘correct’ : both coherent (meaningful given the current situation) and acceptable to the human interlocutor. Those decisions are computed by taking the context of interaction and the estimated level of rapport between the human and the agent into account. We then present a study based on a vicarious protocol where participants evaluated recorded interactions between an avatar (a human-controlled 3D character) and our agent. Results indicate that the estimations made by our decision model regarding the state of the situation mirror those made by human observers. Our initial hypothesis that the level of rapport would positively influence the acceptability of a touch receives mixed support. Social touch occurrences did not feel as coherent and acceptable as hoped yet. Avenues of improvement for human-agent interactions including social touch are discussed.},
  archive  = {J},
  author   = {Fabien Boucaud and Catherine Pelachaud and Indira Thouvenin},
  doi      = {10.1109/TAFFC.2024.3433009},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {277-289},
  title    = {Vicarious evaluation of a decision model for human-agent social touch interactions},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain sentiment analysis via disentangled
representation and prototypical learning. <em>IEEE Transactions on
Affective Computing</em>, <em>16</em>(1), 264–276. (<a
href="https://doi.org/10.1109/TAFFC.2024.3431946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cross-domain sentiment analysis (CDSA) aims to predict the sentiment polarities of reviews in the target domain using a sentiment classifier learned from the source labeled domain. Most existing studies are dominant with adversarial learning methods and focus on learning domain-invariant sentiment representations in both the source and target domains. However, since sentiment-specific features are not explicitly decoupled, the model may confuse domain features with sentiment features, thus affecting its generalization ability on target domains. Unlike previous studies, in this paper, we tackle the CDSA task from the view of disentangled representation learning, which explicitly learns the disentangled representations of review, focusing in particular on sentiment and domain semantics. Specifically, we disentangle sentiment-specific and domain-specific features from the text representation of the review by two different linear transformations. Then, we introduce a straightforward disentangled loss to disallow the sentiment-specific feature to capture domain information. Moreover, we leverage target unlabeled data to improve the quality of the learned sentiment-specific features via prototypical learning. It indirectly encourages the sentiment-specific features of target samples having potentially different classes more discriminative. Extensive experiments on widely used CDSA datasets show that our method surpasses competitive baselines and achieves new state-of-the-art results, demonstrating its effectiveness and superiority.},
  archive  = {J},
  author   = {Qianlong Wang and Zhiyuan Wen and Keyang Ding and Bin Liang and Ruifeng Xu},
  doi      = {10.1109/TAFFC.2024.3431946},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {264-276},
  title    = {Cross-domain sentiment analysis via disentangled representation and prototypical learning},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity and balance: Multimodal sentiment analysis using
multimodal-prefixed and cross-modal attention. <em>IEEE Transactions on
Affective Computing</em>, <em>16</em>(1), 250–263. (<a
href="https://doi.org/10.1109/TAFFC.2024.3430045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal Sentiment Analysis (MSA) is the technology of intelligently recognizing and assessing human sentiments using various data forms such as text, image, and audio. Despite current mainstream methods have made significant progress, MSA still faces the following issues: 1) most current methods train models based on pre-extracted features, lacking a sufficient understanding of sentiment diversity in multimodal data and may even lead to the loss of critical information in the raw data; and 2) textual modality, which possesses high-level semantic features, should typically dominate the fusion process, yet current methods fail to fully leverage this characteristic to balance modality information. To address the aforementioned issues, we propose a novel Multimodal Sentiment Analysis framework using Multimodal-Prefixed and Cross-Modal Attention (DB-MPCA). For the first issue, DB-MPCA employs multimodal raw data for pre-training, which not only allows for in-depth exploration of multimodal information but also significantly enhances the model’s learning capabilities and generalization, while reducing the substantial costs associated with manual annotation. Regarding the second issue, DB-MPCA introduces two prefix encoders designed to convert acoustic and visual features into prefix tokens. These tokens are then embedded into a pre-trained language model, where they are encoded together with textual tokens. Through this approach, DB-MPCA effectively learns cross-modal attention while maintaining the dominance of the textual modality, thereby optimizing the fusion of modalities. Comprehensive experiments conducted on the widely utilized dataset (CMU-MOSI) demonstrate the effectiveness of our model, highlighting its superiority over baseline models.},
  archive  = {J},
  author   = {Meng Li and Zhenfang Zhu and Kefeng Li and Hongli Pei},
  doi      = {10.1109/TAFFC.2024.3430045},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {250-263},
  title    = {Diversity and balance: Multimodal sentiment analysis using multimodal-prefixed and cross-modal attention},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does gamified breath-biofeedback promote adherence,
relaxation, and skill transfer in the wild? <em>IEEE Transactions on
Affective Computing</em>, <em>16</em>(1), 237–249. (<a
href="https://doi.org/10.1109/TAFFC.2024.3428390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper investigates whether gamification of deep breathing (DB) exercises promotes relaxation, skill transfer, and adherence to treatment in ambulatory settings. We designed a game-biofeedback (GBF) intervention where users perform DB exercises while playing a video game, and the game adapts according to the user&#39;s breathing rate using negative reinforcement instrumental conditioning. As a control, we developed an interactive paced-breathing treatment (PACE) where users follow a visual signal with their breathing and touch. In a user study, 30 participants were randomly assigned to GBF or PACE, and were allowed to practice at their leisure over the course of three days. Results show that the GBF group practiced their treatments significantly more often, achieved better skill transfer at post-test, and obtained a higher increase in self-reported positivity and relaxation during treatment. Our findings suggest that the use of negative reinforcement coupled with a fun casual game can be used as an alternative tool to promote relaxation and improve adherence to stress management interventions.},
  archive  = {J},
  author   = {Dennis R. da Cunha Silva and Ricardo Gutierrez-Osuna},
  doi      = {10.1109/TAFFC.2024.3428390},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {237-249},
  title    = {Does gamified breath-biofeedback promote adherence, relaxation, and skill transfer in the wild?},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating visual context into language models for situated
social conversation starters. <em>IEEE Transactions on Affective
Computing</em>, <em>16</em>(1), 223–236. (<a
href="https://doi.org/10.1109/TAFFC.2024.3428704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Embodied conversational agents that interact socially with people in the physical world require multi-modal capabilities, such as appropriately responding to visual features of users. While existing vision-and-language models can generate language based on visual input, this language is not situated in a social interaction in the physical world. We present a novel task called Visual Conversation Starters, where an agent generates a conversation-starting question referring to features visible in an image of the user. We collect a dataset of 4000 images of people with 12000 crowdsourced conversation starters, compare various model architectures: fine-tuning smaller seq2seq or image-to-text models versus zero-shot prompting of GPT-3.5, using image captions versus end-to-end image input, training on human data versus synthetic questions generated by GPT-3.5. Models were used to generate friendly conversation starters which were evaluated on criteria including language fluency, visual grounding, interestingness, politeness. Results show that GPT-3.5 generates more interesting, polite questions than smaller models that are fine-tuned on crowdsourced data, but vision-to-language models are better at referencing visual features, they can mimick GPT-3.5&#39;s performance. This demonstrates the feasibility of deep visiolinguistic models for situated social agents, forming an important first stage in creating situated multimodal social interaction.},
  archive  = {J},
  author   = {Ruben Janssens and Pieter Wolfert and Thomas Demeester and Tony Belpaeme},
  doi      = {10.1109/TAFFC.2024.3428704},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {223-236},
  title    = {Integrating visual context into language models for situated social conversation starters},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level contrastive learning: Hierarchical alleviation
of heterogeneity in multimodal sentiment analysis. <em>IEEE Transactions
on Affective Computing</em>, <em>16</em>(1), 207–222. (<a
href="https://doi.org/10.1109/TAFFC.2024.3423671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, multimodal fusion efforts have achieved remarkable success in Multimodal Sentiment Analysis (MSA). However, most of the existing methods are based on model-level fusion, and the challenge of heterogeneity between modalities is not well resolved. Heterogeneity lies in the different feature distributions and distinct representation spaces among different modalities. To mitigate this problem, we propose that fusion is a progressive process, and we introduce a novel multi-level contrastive learning and multi-layer convolution fusion (MCL-MCF) method for MSA. Due to the relationships among multimodal data, the fusion process that involves single-modal to single-modal, single-modal to bimodal or trimodal, and higher-level fused modality semantic consistency is divided into three levels. The first-level contrast learning alleviates heterogeneity between unimodal modalities at the early level of multimodal feature fusion. The second-level contrast learning mitigates heterogeneity between unimodal and fused modalities. At the third level, we introduce a tensor convolution fusion (TCF) module that extracts high-level semantic features from the fused modalities and mitigates heterogeneity at the higher feature level through contrastive learning. To simulate fusion as a progressive process, MCF is proposed to fuse shallow and deep features to model complex relationships among modalities. Experiments on three public datasets show our approach&#39;s state-of-the-art performance.},
  archive  = {J},
  author   = {Cunhang Fan and Kang Zhu and Jianhua Tao and Guofeng Yi and Jun Xue and Zhao Lv},
  doi      = {10.1109/TAFFC.2024.3423671},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {207-222},
  title    = {Multi-level contrastive learning: Hierarchical alleviation of heterogeneity in multimodal sentiment analysis},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eye action units as combinations of discrete eye behaviors
for wearable mental state analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>16</em>(1), 191–206. (<a
href="https://doi.org/10.1109/TAFFC.2024.3422448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mental state induced by different task contexts and load levels is of interest for human health and wellness, and eye activity extracted from infrared eye images is well-suited to estimate it. As a useful tool for emotion recognition, facial action units (FAUs) extracted from facial images are well-established, however these are insufficiently detailed for the eye. In this paper, we extract discrete eye behaviors from eyelid, iris and pupil boundaries and propose eye action units (EAUs) based on the eye appearance (behavior), providing a detailed and interpretable representation that shares the advantages of FAUs and describes the wide range of eye states and shapes. Eight volunteers annotated 11 EAUs for 120 eye images, represented by a series of discrete eye behaviors. Analysis shows that the EAUs can be viably characterized by fundamental eye behaviors with moderate to substantial agreement. When evaluating discrete eye behaviors and EAUs for recognition of four mental states and two load levels, the former achieved significantly higher accuracy than conventional features of pupil size change and blink rate, especially using behavior duration features, and EAUs outperformed combinations of discrete eye behaviors in general, implying their utility as an action unit.},
  archive  = {J},
  author   = {Siyuan Chen and Julien Epps},
  doi      = {10.1109/TAFFC.2024.3422448},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {191-206},
  title    = {Eye action units as combinations of discrete eye behaviors for wearable mental state analysis},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting when and what to explain from multimodal eye
tracking and task signals. <em>IEEE Transactions on Affective
Computing</em>, <em>16</em>(1), 179–190. (<a
href="https://doi.org/10.1109/TAFFC.2024.3419696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While interest in the field of explainable agents increases, it is still an open problem to incorporate a proactive explanation component into a real-time human–agent collaboration. Thus, when collaborating with a human, we want to enable an agent to identify critical moments requiring timely explanations. We differentiate between situations requiring explanations about the agent&#39;s decision-making and assistive explanations supporting the user. In order to detect these situations, we analyze eye tracking signals of participants engaging in a collaborative virtual cooking scenario. First, we show how users’ gaze patterns differ between moments of user confusion, the agent making errors, and the user successfully collaborating with the agent. Second, we evaluate different state-of-the-art models on the task of predicting whether the user is confused or the agent makes errors using gaze- and task-related data. An ensemble of MiniRocket classifiers performs best, especially when updating its predictions with high frequency based on input samples capturing time windows of 3 to 5 seconds. We find that gaze is a significant predictor of when and what to explain. Gaze features are crucial to our classifier&#39;s accuracy, with task-related features benefiting the classifier to a smaller extent.},
  archive  = {J},
  author   = {Lennart Wachowiak and Peter Tisnikar and Gerard Canal and Andrew Coles and Matteo Leonetti and Oya Celiktutan},
  doi      = {10.1109/TAFFC.2024.3419696},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {179-190},
  title    = {Predicting when and what to explain from multimodal eye tracking and task signals},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage temporal modelling framework for video-based
depression recognition using graph representation. <em>IEEE Transactions
on Affective Computing</em>, <em>16</em>(1), 161–178. (<a
href="https://doi.org/10.1109/TAFFC.2024.3415770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Video-based automatic depression analysis provides a fast, objective and repeatable self-assessment solution, which has been widely developed in recent years. While depression cues may be reflected by human facial behaviours of various temporal scales, most existing approaches either focused on modelling depression from short-term or video-level facial behaviours. In this sense, we propose a two-stage framework that models depression severity from multi-scale short-term and video-level facial behaviours. The short-term depressive behaviour modelling stage first deep learns depression-related facial behavioural features from multiple short temporal scales, where a Depression Feature Enhancement (DFE) module is proposed to enhance the depression-related cues for all temporal scales and remove non-depression related noise. Two novel graph encoding strategies are proposed in the video-level depressive behavior modeling stage, i.e., Sequential Graph Representation (SEG) and Spectral Graph Representation (SPG), to re-encode all short-term features of the target video into a video-level graph representation, summarizing depression-related multi-scale video-level temporal information. As a result, the produced graph representations predict depression severity using both short-term and long-term facial behaviour patterns. The experimental results on AVEC 2013, AVEC 2014 and AVEC 2019 datasets show that the proposed DFE module constantly enhanced the depression severity estimation performance for various CNN models while the SPG is superior than other video-level modelling methods. More importantly, the result achieved for the proposed two-stage framework shows its promising and solid performance compared to widely-used one-stage modelling approaches.},
  archive  = {J},
  author   = {Jiaqi Xu and Hatice Gunes and Keerthy Kusumam and Michel Valstar and Siyang Song},
  doi      = {10.1109/TAFFC.2024.3415770},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {161-178},
  title    = {Two-stage temporal modelling framework for video-based depression recognition using graph representation},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image encoding and fusion of multi-modal data enhance
depression diagnosis in parkinson’s disease patients. <em>IEEE
Transactions on Affective Computing</em>, <em>16</em>(1), 145–160. (<a
href="https://doi.org/10.1109/TAFFC.2024.3418415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The diagnosis of depression in individuals with Parkinson&#39;s Disease (PD) through the utilization of multimodal fusion techniques represents a significant domain. The primary challenge involves the creation of a robust fusion framework to address the heterogeneity among different modalities effectively. However, previous studies primarily focused on interactions between heterogeneous data, neglecting the structural similarities among isomorphic data, resulting in a substantial loss of feature information when merging heterogeneous data. In this study, we introduced a multi-modal data image encoding and fusion approach for diagnosing depression in PD patients. Additionally, we proposed a multi-modal dataset encompassing motion, facial expression, and audio data. First, we designed an RGB and sparse coding method to encode the multi-modal data, achieving the isomorphic transformation of multi-modal information and extracting feature information from lower-dimensional spaces. Furthermore, we introduced a Spatial-Temporal Network (STN) to fuse the three types of encoded images. We incorporated the Relation Global Attention (RGA) to enhance feature extraction and leverage all encoded image location feature nodes for balanced decision attention. Finally, recognizing the limitations of traditional machine learning algorithms in handling multi-tasks in medical diagnosis, we established a multi-task weighted loss function to achieve depression identification and severity prediction through Multi-Task learning (MTL).},
  archive  = {J},
  author   = {Jian Li and Yuliang Zhao and Huawei Zhang and Wayne Jason Li and Changzeng Fu and Chao Lian and Peng Shan},
  doi      = {10.1109/TAFFC.2024.3418415},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {145-160},
  title    = {Image encoding and fusion of multi-modal data enhance depression diagnosis in parkinson&#39;s disease patients},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal sentimental privileged information embedding for
improving facial expression recognition. <em>IEEE Transactions on
Affective Computing</em>, <em>16</em>(1), 133–144. (<a
href="https://doi.org/10.1109/TAFFC.2024.3415625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expression recognition (FER) has always been one of the key task in affective computing. Over the years, researchers have worked to improve the performance of FER by designing models with more powerful feature extraction, embedding attention mechanism, and reconstructing missing information, etc. Different from the paradigms above, we attempt to improve FER performance by using multimodal sentiment data, such as audio and text, as privileged information (PI) for facial images. To this end, a multimodal privileged information embedded facial expression recognition network (MPI-FER) is proposed in this paper. During the training phase, this model achieves the PI embedding of multimodal data for FER by developing cross-modality translation between multimodal sentiment data. During the test phase, input images alone are sufficient for the model inference to accomplish the FER task input. The MPI-FER is a large-scale, heterogeneous deep neural network. To achieve effective training of this model with limited training samples, we design a multi-stage training strategy of module-wise pre-training followed by end-to-end fine-tuning. In addition, a strategy of filling the multimodal sentiment quaternion is proposed for implementing our method on a facial expression database consisting only of face images. We conducted extensive experiments to evaluate the proposed method on two databases of multimodal sentiment analysis (CH-SIMS and CMU-MOSI) and two databases of FER in the wild (RAF-DB and AffectNet). The results show that embedding multimodal sentiment data as privileged information into the FER task based on face images can significantly improve the accuracy of FER. Furthermore, by only using image in the test phase, the proposed method can achieve better results of multimodal sentiment analysis than those methods achieved by using multimodal sentimental data fusion.},
  archive  = {J},
  author   = {Ning Sun and Changwei You and Wenming Zheng and Jixin Liu and Lei Chai and Haian Sun},
  doi      = {10.1109/TAFFC.2024.3415625},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {133-144},
  title    = {Multimodal sentimental privileged information embedding for improving facial expression recognition},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MOCAS: A multimodal dataset for objective cognitive workload
assessment on simultaneous tasks. <em>IEEE Transactions on Affective
Computing</em>, <em>16</em>(1), 116–132. (<a
href="https://doi.org/10.1109/TAFFC.2024.3414330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents MOCAS, a multimodal dataset dedicated for human cognitive workload (CWL) assessment. In contrast to existing datasets based on virtual game stimuli, the data in MOCAS was collected from realistic closed-circuit television (CCTV) monitoring tasks, increasing its applicability for real-world scenarios. To build MOCAS, two off-the-shelf wearable sensors and one webcam were utilized to collect physiological signals and behavioral features from 21 human subjects. After each task, participants reported their CWL by completing the NASA-Task Load Index (NASA-TLX) and Instantaneous Self-Assessment (ISA). Personal background (e.g., personality and prior experience) was surveyed using demographic and Big Five Factor personality questionnaires, and two domains of subjective emotion information (i.e., arousal and valence) were obtained from the Self-Assessment Manikin (SAM), which could serve as potential indicators for improving CWL recognition performance. Technical validation was conducted to demonstrate that target CWL levels were elicited during simultaneous CCTV monitoring tasks; its results support the high quality of the collected multimodal signals.},
  archive  = {J},
  author   = {Wonse Jo and Ruiqi Wang and Go-Eum Cha and Su Sun and Revanth Krishna Senthilkumaran and Daniel Foti and Byung-Cheol Min},
  doi      = {10.1109/TAFFC.2024.3414330},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {116-132},
  title    = {MOCAS: A multimodal dataset for objective cognitive workload assessment on simultaneous tasks},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new look at breathing for affective studies. <em>IEEE
Transactions on Affective Computing</em>, <em>16</em>(1), 98–115. (<a
href="https://doi.org/10.1109/TAFFC.2024.3413053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In affective computing, breathing has seen lighter use than the heart and EDA channels. Several reasons have contributed to this, including difficulties in disambiguating affective from speech effects and perceived lack of generalizability. Here we report a framework that addresses these issues. The cornerstone of the framework is a comprehensive set of physiologically informed features, comprised of three groups: breathing depth, respiratory time quotient (RTQ), and breathing speed features. The breathing depth features capture either mental arousal or fear effects. The RTQ features capture speech production. The breathing speed features capture arousal effects due to emotional influences. The said framework appears to have broad applicability. In the naturalistic Office Tasks 2019 dataset with speaking sessions, the said features used either in regression or random forest models led to robust classification of arousal ($\overline{\text{AUC}}$ in [0.75, 0.96]) stemming from three different conditions: a) mental-emotional stressor effected through a time-pressured knowledge task; b) pure mental stressor effected through a long knowledge task; c) mental-social stressor effected through a public speech task. In the stylized CASE dataset with silent sessions, the same features and algorithms led to solid classification of arousal ($\overline{\text{AUC}}$ in [0.71, 0.85]) stemming from scary vs. non-scary movie clips.},
  archive  = {J},
  author   = {Nanfei Sun and Ioannis Pavlidis},
  doi      = {10.1109/TAFFC.2024.3413053},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {98-115},
  title    = {A new look at breathing for affective studies},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying children with autism spectrum disorder via
transformer-based representation learning from dynamic facial cues.
<em>IEEE Transactions on Affective Computing</em>, <em>16</em>(1),
83–97. (<a href="https://doi.org/10.1109/TAFFC.2024.3412032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognizing autism spectrum disorder (ASD) has faced great challenges due to insufficient professional clinicians and complex procedures. Automated data-driven ASD recognition models can reduce the subjectivity and physician dependency of traditional evaluation methods. Facial data, which can encode important perceptual and social behaviors, have emerged in ASD research to explore novel biomarkers for screening, diagnosing, and treating ASD. However, existing research mainly focuses on extracting low-level hand-crafted facial features for analysis and classification. Determining how to learn discriminative deep representations from dynamic facial data for computational model construction remains an unresolved challenge. In this study, we propose an ASD recognition model based on facial videos to fill the lack of temporal correlation learning of facial features. First, we utilize a vision transformer to extract frame-based global facial features. Then, we use a Longformer to establish the correlation of facial features over time. In the experiment, we recruited 146 subjects between 2 and 8 years of age to record their facial videos under a computer-based eye-tracking experiment and 76 subjects to conduct a smartphone-based experiment. Quantitative comparisons have shown the effectiveness and reliability of the proposed model. Furthermore, we have confirmed the correlation between facial and eye-tracking modalities in visual attention.},
  archive  = {J},
  author   = {Chen Xia and Hexu Chen and Junwei Han and Dingwen Zhang and Kuan Li},
  doi      = {10.1109/TAFFC.2024.3412032},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {83-97},
  title    = {Identifying children with autism spectrum disorder via transformer-based representation learning from dynamic facial cues},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controllable multi-speaker emotional speech synthesis with
an emotion representation of high generalization capability. <em>IEEE
Transactions on Affective Computing</em>, <em>16</em>(1), 68–82. (<a
href="https://doi.org/10.1109/TAFFC.2024.3412152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The aim of multi-speaker emotional speech synthesis is to generate speech for a designated speaker in a desired emotional state. The task is challenging due to the presence of speech variations, such as noise, content, and timbre, which can obstruct emotion extraction and transfer. This paper proposes a new approach to performing multi-speaker emotional speech synthesis. The proposed method, which is based on a seq2seq synthesizer, integrates emotion embedding as a conditioned variable to convey exact emotional information from reference audio to the synthesized speech. To boost emotion representation capability, we utilize a three-dimensional acoustic feature as input. And an emotion generalization module with adaptive instance normalization (AdaIN) is proposed to obtain emotion embedding with high generalization ability, which also results in improved controllability. The derived emotion embedding from the generalization module can be readily conditioned by affine parameters, allowing for control both the emotion category and the emotion intensity of synthesized speech. Various emotional speech synthesis experimental results of the propposed method demonstrate its state-of-the-art performance in multi-speaker emotional speech synthesis, coupled with its advantage of high emotion controllability.},
  archive  = {J},
  author   = {Junjie Zheng and Jian Zhou and Wenming Zheng and Liang Tao and Hon Keung Kwan},
  doi      = {10.1109/TAFFC.2024.3412152},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {68-82},
  title    = {Controllable multi-speaker emotional speech synthesis with an emotion representation of high generalization capability},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative data-driven study of intensity-based
categorical emotion representations for MER. <em>IEEE Transactions on
Affective Computing</em>, <em>16</em>(1), 56–67. (<a
href="https://doi.org/10.1109/TAFFC.2024.3411651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data-driven analysis and modeling of music-perceived emotions have widespread applications in MIR, with representations of perceived musical emotions forming a crucial component. Though some emotion representations are popular in the literature, their relative merits and demerits in terms of expressiveness and broad applicability have been sparsely studied. The application-specific emotion representations used in multiple studies lead to incomparability of algorithms and performance metrics and non-reusability of representation-specific emotion data across studies. In this work, we study an intensity ratings-based, categorical emotion representation called Emotion-Word Intensity-Value (EWIV) representation, with emotion classes adapted from the aesthetic concept of Nava Rasa. We also introduce EmoRaga - a novel clip-set annotated with perceived emotions and emotion motifs for emotion analysis of Hindustani classical music. We explore the applicability of EWIV towards diverse MIR applications, e.g., dominant and secondary emotion identification, and temporal emotion pattern study. Last, we report a data-driven comparison of EWIV, categorical and dimensional representations, using statistical out-of-sample goodness of fit tests to measure and compare their representativeness over both benchmark datasets and collected emotion data. We conclude that EWIV is applicable to a range of MIR tasks, with higher representative and generalization potential compared to popular representations in certain cases.},
  archive  = {J},
  author   = {Sanga Chaki and Sourangshu Bhattacharya and Junmoni Borgohain and Priyadarshi Patnaik and Raju Mullick and Gouri Karambelkar},
  doi      = {10.1109/TAFFC.2024.3411651},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {56-67},
  title    = {A comparative data-driven study of intensity-based categorical emotion representations for MER},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minority views matter: Evaluating speech emotion classifiers
with human subjective annotations by an all-inclusive aggregation rule.
<em>IEEE Transactions on Affective Computing</em>, <em>16</em>(1),
41–55. (<a href="https://doi.org/10.1109/TAFFC.2024.3411290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {When selecting test data for subjective tasks, most studies define ground truth labels using aggregation methods such as the majority or plurality rules. These methods discard data points without consensus, making the test set easier than practical tasks where a prediction is needed for each sample. However, the discarded data points often express ambiguous cues that elicit coexisting traits perceived by annotators. This paper addresses the importance of considering all the annotations and samples in the data, highlighting that only showing the model&#39;s performance on an incomplete test set selected by using the majority or plurality rules can lead to bias in the models’ performances. We focus on speech-emotion recognition (SER) tasks. We observe that traditional aggregation rules have a data loss ratio ranging from 5.63% to 89.17%. From this observation, we propose a flexible method named the all-inclusive aggregation rule to evaluate SER systems on the complete test data. We contrast traditional single-label formulations with a multi-label formulation to consider the coexistence of emotions. We show that training an SER model with the data selected by the all-inclusive aggregation rule shows consistently higher macro-F1 scores when tested in the entire test set, including ambiguous samples without agreement.},
  archive  = {J},
  author   = {Huang-Cheng Chou and Lucas Goncalves and Seong-Gyun Leem and Ali N. Salman and Chi-Chun Lee and Carlos Busso},
  doi      = {10.1109/TAFFC.2024.3411290},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {41-55},
  title    = {Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an all-inclusive aggregation rule},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion recognition in the real world: Passively collecting
and estimating emotions from natural speech data of individuals with
bipolar disorder. <em>IEEE Transactions on Affective Computing</em>,
<em>16</em>(1), 28–40. (<a
href="https://doi.org/10.1109/TAFFC.2024.3407683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotions provide critical information regarding a person&#39;s health and well-being. Therefore, the ability to track emotion and patterns in emotion over time could provide new opportunities in measuring health longitudinally. This is of particular importance for individuals with bipolar disorder (BD), where emotion dysregulation is a hallmark symptom of increasing mood severity. However, measuring emotions typically requires self-assessment, a willful action outside of one&#39;s daily routine. In this paper, we describe a novel approach for collecting real-world natural speech data from daily life and measuring emotions from these data. The approach combines a novel data collection pipeline and validated robust emotion recognition models. We describe a deployment of this pipeline that included parallel clinical and self-report measures of mood and self-reported measures of emotion. Finally, we present approaches to estimate clinical and self-reported mood measures using a combination of passive and self-reported emotion measures. The results demonstrate that both passive and self-reported measures of emotion contribute to our ability to accurately estimate mood symptom severity for individuals with BD.},
  archive  = {J},
  author   = {Emily Mower Provost and Sarah H Sperry and James Tavernor and Steve Anderau and Anastasia Yocum and Melvin G McInnis},
  doi      = {10.1109/TAFFC.2024.3407683},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {28-40},
  title    = {Emotion recognition in the real world: Passively collecting and estimating emotions from natural speech data of individuals with bipolar disorder},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FEAD: Introduction to the fNIRS-EEG affective database -
video stimuli. <em>IEEE Transactions on Affective Computing</em>,
<em>16</em>(1), 15–27. (<a
href="https://doi.org/10.1109/TAFFC.2024.3407380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents FEAD, a fNIRS-EEG Affective Database that can be used for training emotion recognition models. The electrical activity and brain hemodynamic responses of 37 participants were recorded, as well as the categorical and dimensional emotion ratings they gave to 24 affective audio-visual stimuli. The relationship between the neurophysiological signals with the subjective ratings was investigated, with a significant correlation found in the prefrontal cortex region. A binary classification of affective states was performed using a subject-dependent approach, taking into account the fusion of both modalities, functional Near-Infrared Spectroscopy and Electroencephalography, and each single modality separately. In addition, we explored the temporal dynamics of the recorded data in shorter trials and found that the fusion of features from both modalities yielded significantly better results than using a single modality. This database will be made publicly available with the aim to encourage researchers to develop more advanced algorithms for affective computing and emotion recognition.},
  archive  = {J},
  author   = {Alireza Farrokhi Nia and Vanessa Tang and Valery Malyshau and Amit Barde and Gonzalo Daniel Maso Talou and Mark Billinghurst},
  doi      = {10.1109/TAFFC.2024.3407380},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {15-27},
  title    = {FEAD: Introduction to the fNIRS-EEG affective database - video stimuli},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic causal disentanglement model for dialogue emotion
detection. <em>IEEE Transactions on Affective Computing</em>,
<em>16</em>(1), 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2024.3406710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion detection is a critical technology extensively employed in diverse fields. While the incorporation of commonsense knowledge has proven beneficial for existing emotion detection methods, dialogue-based emotion detection encounters numerous difficulties and challenges due to human agency and the variability of dialogue content. In dialogues, human emotions tend to accumulate in bursts. However, they are often implicitly expressed. This implies that many genuine emotions remain concealed within a plethora of unrelated words and dialogues. In this article, we propose a Dynamic Causal Disentanglement Model founded on the separation of hidden variables, which effectively decomposes the content of dialogues and investigates the temporal accumulation of emotions, thereby enabling more precise emotion recognition. First, we introduce a novel Causal Directed Acyclic Graph (DAG) to establish the correlation between hidden emotional information and other observed elements. Subsequently, our approach utilizes pre-extracted personal states and utterance topics as guiding factors for the distribution of hidden variables, aiming to separate irrelevant ones. Specifically, we propose a Dynamic Causal Disentanglement Model to infer the propagation of utterances and hidden variables, enabling the accumulation of emotion-related information throughout the conversation. To guide this disentanglement process, we leverage the GPT4.0 and LSTM networks to extract utterance topics and personal states as observed information. Finally, we test our approach on popular datasets in dialogue emotion detection and relevant experimental results verified the model&#39;s superiority.},
  archive  = {J},
  author   = {Yuting Su and Yichen Wei and Weizhi Nie and Sicheng Zhao and Anan Liu},
  doi      = {10.1109/TAFFC.2024.3406710},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1-3},
  number   = {1},
  pages    = {1-14},
  title    = {Dynamic causal disentanglement model for dialogue emotion detection},
  volume   = {16},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
