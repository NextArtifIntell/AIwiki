<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TNNLS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tnnls---150">TNNLS - 150</h2>
<ul>
<li><details>
<summary>
(2025). Learning rates of deep nets for geometrically strongly
mixing sequence. <em>TNNLS</em>, <em>36</em>(3), 5826–5832. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great success of deep learning poses an urgent challenge to establish the theoretical basis for its working mechanism. Recently, research on the convergence of deep neural networks (DNNs) has made great progress. However, the existing studies are based on the assumption that the samples are independent, which is too strong to be applied to many real-world scenarios. In this brief, we establish a fast learning rate for the empirical risk minimization (ERM) on DNN regression with dependent samples, and the dependence is expressed in terms of geometrically strongly mixing sequence. To the best of our knowledge, this is the first convergence result of DNN methods based on mixing sequences. This result is a natural generalization of the independent sample case.},
  archive      = {J_TNNLS},
  author       = {Yueyang Men and Liang Li and Ziqing Hu and Yongli Xu},
  doi          = {10.1109/TNNLS.2024.3371025},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5826-5832},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning rates of deep nets for geometrically strongly mixing sequence},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel generator with auxiliary branch for improving GAN
performance. <em>TNNLS</em>, <em>36</em>(3), 5818–5825. (<a
href="https://doi.org/10.1109/TNNLS.2024.3361087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generator in the generative adversarial network (GAN) learns image generation in a coarse-to-fine manner in which earlier layers learn the overall structure of the image and the latter ones refine the details. To propagate the coarse information well, recent works usually build their generators by stacking up multiple residual blocks. Although the residual block can produce a high-quality image as well as be trained stably, it often impedes the information flow in the network. To alleviate this problem, this brief introduces a novel generator architecture that produces the image by combining features obtained through two different branches: the main and auxiliary branches. The goal of the main branch is to produce the image by passing through the multiple residual blocks, whereas the auxiliary branch is to convey the coarse information in the earlier layer to the later one. To combine the features in the main and auxiliary branches successfully, we also propose a gated feature fusion module (GFFM) that controls the information flow in those branches. To prove the superiority of the proposed method, this brief provides extensive experiments using various standard datasets including CIFAR-10, CIFAR-100, LSUN, CelebA-HQ, AFHQ, and tiny-ImageNet. Furthermore, we conducted various ablation studies to demonstrate the generalization ability of the proposed method. Quantitative evaluations prove that the proposed method exhibits impressive GAN performance in terms of Inception score (IS) and Frechet inception distance (FID). For instance, the proposed method boosts the FID and IS scores on the tiny-ImageNet dataset from 35.13 to 25.00 and 20.23 to 25.57, respectively.},
  archive      = {J_TNNLS},
  author       = {Seung Park and Yong-Goo Shin},
  doi          = {10.1109/TNNLS.2024.3361087},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5818-5825},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel generator with auxiliary branch for improving GAN performance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward efficient convolutional neural networks with
structured ternary patterns. <em>TNNLS</em>, <em>36</em>(3), 5810–5817.
(<a href="https://doi.org/10.1109/TNNLS.2024.3380827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-efficiency deep learning (DL) models are necessary not only to facilitate their use in devices with limited resources but also to improve resources required for training. Convolutional neural networks (ConvNets) typically exert severe demands on local device resources and this conventionally limits their adoption within mobile and embedded platforms. This brief presents work toward utilizing static convolutional filters generated from the space of local binary patterns (LBPs) and Haar features to design efficient ConvNet architectures. These are referred to as Structured Ternary Patterns (STePs) and can be generated during network initialization in a systematic way instead of having learnable weight parameters thus reducing the total weight updates. The ternary values require significantly less storage and with the appropriate low-level implementation, can also lead to inference improvements. The proposed approach is validated using four image classification datasets, demonstrating that common network backbones can be made more efficient and provide competitive results. It is also demonstrated that it is possible to generate completely custom STeP-based networks that provide good trade-offs for on-device applications such as unmanned aerial vehicle (UAV)-based aerial vehicle detection. The experimental results show that the proposed method maintains high detection accuracy while reducing the trainable parameters by 40%–80%. This work motivates further research toward good priors for nonlearnable weights that can make DL architectures more efficient without having to alter the network during or after training.},
  archive      = {J_TNNLS},
  author       = {Christos Kyrkou},
  doi          = {10.1109/TNNLS.2024.3380827},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5810-5817},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward efficient convolutional neural networks with structured ternary patterns},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A revised formation of trace ratio LDA for small sample size
problem. <em>TNNLS</em>, <em>36</em>(3), 5803–5809. (<a
href="https://doi.org/10.1109/TNNLS.2024.3362512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is a classic tool for supervised dimensionality reduction. Because the projected samples can be classified effectively, LDA has been successfully applied in many applications. Among the variants of LDA, trace ratio LDA (TR-LDA) is a classic form due to its explicit meaning. Unfortunately, when the sample size is much smaller than the data dimension, the algorithm for solving TR-LDA does not converge. The so-called small sample size (SSS) problem severely limits the application of TR-LDA. To solve this problem, we propose a revised formation of TR-LDA, which can be applied to datasets with different sizes in a unified form. Then, we present an optimization algorithm to solve the proposed method, explain why it can avoid the SSS problem, and analyze the convergence and computational complexity of the optimization algorithm. Next, based on the introduced theorems, we quantitatively elaborate on when the SSS problem will occur in TR-LDA. Finally, the experimental results on real-world datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Zhengxin Li and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2024.3362512},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5803-5809},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A revised formation of trace ratio LDA for small sample size problem},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed dynamic task allocation for moving target
tracking of networked mobile robots using k-WTA network. <em>TNNLS</em>,
<em>36</em>(3), 5795–5802. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tasks allocation plays a pivotal role in cooperative robotics. This study proposes a novel fully distributed task allocation method for target tracking, by which mobile robots only need to share state information with communication neighbors. The proposed method adopts a distributed k winners-take-all (k-WTA) network to select the k mobile robots closest to the moving target to perform the target tracking task. In addition, an innovative robot control law is designed, incorporating speed feedback and nonlinear activation functions to achieve finite-time error convergence. Unlike previous approaches, our distributed task allocation method yields finite-time error convergence, does not rely on consensus filters, and eliminates the need for a central computing unit to get the k-WTA result during the control process. We demonstrate the effectiveness of the proposed method through theoretical analysis and simulations. Compared to traditional methods, our method leads to smaller total moving distances and speed norms, which underscores the significance of our method in enhancing the efficiency and performance of mobile robots in dynamic task allocation.},
  archive      = {J_TNNLS},
  author       = {Kexin Liu and Yinyan Zhang},
  doi          = {10.1109/TNNLS.2024.3377433},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5795-5802},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed dynamic task allocation for moving target tracking of networked mobile robots using k-WTA network},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Position-sensing graph neural networks: Proactively learning
nodes relative positions. <em>TNNLS</em>, <em>36</em>(3), 5787–5794. (<a
href="https://doi.org/10.1109/TNNLS.2024.3374464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing graph neural networks (GNNs) learn node embeddings using the framework of message passing and aggregation. Such GNNs are incapable of learning relative positions between graph nodes within a graph. To empower GNNs with the awareness of node positions, some nodes are set as anchors. Then, using the distances from a node to the anchors, GNNs can infer relative positions between nodes. However, position-aware GNNs (P-GNNs) arbitrarily select anchors, leading to compromising position awareness and feature extraction. To eliminate this compromise, we demonstrate that selecting evenly distributed and asymmetric anchors is essential. On the other hand, we show that choosing anchors that can aggregate embeddings of all the nodes within a graph is NP-complete. Therefore, devising efficient optimal algorithms in a deterministic approach is practically not feasible. To ensure position awareness and bypass NP-completeness, we propose position-sensing GNNs (PSGNNs), learning how to choose anchors in a backpropagatable fashion. Experiments verify the effectiveness of PSGNNs against state-of-the-art GNNs, substantially improving performance on various synthetic and real-world graph datasets while enjoying stable scalability. Specifically, PSGNNs on average boost area under the curve (AUC) more than 14% for pairwise node classification and 18% for link prediction over the existing state-of-the-art position-aware methods. Our source code is publicly available at: https://github.com/ZhenyueQin/PSGNN.},
  archive      = {J_TNNLS},
  author       = {Yiqun Zhang and Zhenyue Qin and Saeed Anwar and Dongwoo Kim and Yang Liu and Pan Ji and Tom Gedeon},
  doi          = {10.1109/TNNLS.2024.3374464},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5787-5794},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Position-sensing graph neural networks: Proactively learning nodes relative positions},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-step multi-view clustering with diverse representation.
<em>TNNLS</em>, <em>36</em>(3), 5774–5786. (<a
href="https://doi.org/10.1109/TNNLS.2024.3378194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-View clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-View clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, limiting the model’s expressiveness. Moreover, a range of methods suffers from a two-step process, i.e., multimodal learning and the subsequent k-means, inevitably causing a suboptimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation (OMVCDR) method, which incorporates multi-view learning and k-means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervised manner. Then, we directly use the information matrices under diverse dimensions to obtain consensus discrete clustering labels. The unified work of representation learning and clustering boosts the quality of the final results. Furthermore, we develop an efficient optimization algorithm with proven convergence to solve the resultant problem. Comprehensive experiments on various datasets demonstrate the promising clustering performance of our proposed method. The code is publicly available at https://github.com/wanxinhang/OMVCDR.},
  archive      = {J_TNNLS},
  author       = {Xinhang Wan and Jiyuan Liu and Xinbiao Gan and Xinwang Liu and Siwei Wang and Yi Wen and Tianjiao Wan and En Zhu},
  doi          = {10.1109/TNNLS.2024.3378194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5774-5786},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {One-step multi-view clustering with diverse representation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaptiveClick: Click-aware transformer with adaptive focal
loss for interactive image segmentation. <em>TNNLS</em>, <em>36</em>(3),
5759–5773. (<a
href="https://doi.org/10.1109/TNNLS.2024.3378295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive image segmentation (IIS) has emerged as a promising technique for decreasing annotation time. Substantial progress has been made in pre- and post-processing for IIS, but the critical issue of interaction ambiguity, notably hindering segmentation quality, has been under-researched. To address this, we introduce ADAPTIVE CLICK — a click-aware transformer incorporating an adaptive focal loss (AFL) that tackles annotation inconsistencies with tools for mask- and pixel-level ambiguity resolution. To the best of our knowledge, AdaptiveClick is the first transformer-based, mask-adaptive segmentation framework for IIS. The key ingredient of our method is the click-aware mask-adaptive transformer decoder (CAMD), which enhances the interaction between click and image features. Additionally, AdaptiveClick enables pixel-adaptive differentiation of hard and easy samples in the decision space, independent of their varying distributions. This is primarily achieved by optimizing a generalized AFL with a theoretical guarantee, where two adaptive coefficients control the ratio of gradient values for hard and easy pixels. Our analysis reveals that the commonly used Focal and BCE losses can be considered special cases of the proposed AFL. With a plain ViT backbone, extensive experimental results on nine datasets demonstrate the superiority of AdaptiveClick compared to state-of-the-art methods. The source code is publicly available at https://github.com/lab206/AdaptiveClick.},
  archive      = {J_TNNLS},
  author       = {Jiacheng Lin and Jiajun Chen and Kailun Yang and Alina Roitberg and Siyu Li and Zhiyong Li and Shutao Li},
  doi          = {10.1109/TNNLS.2024.3378295},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5759-5773},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AdaptiveClick: Click-aware transformer with adaptive focal loss for interactive image segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal network embedding enhanced with long-range dynamics
and self-supervised learning. <em>TNNLS</em>, <em>36</em>(3), 5747–5758.
(<a href="https://doi.org/10.1109/TNNLS.2024.3384348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal network embedding (TNE) has promoted the research of knowledge discovery and reasoning on networks. It aims to embed vertices of temporal networks into a low-dimensional vector space while preserving network structures and temporal properties. However, most existing methods have limitations in capturing dynamics over long distances, which makes it difficult to explore multihop topological associations among vertices. To tackle this challenge, we propose LongTNE, which learns the long-range dynamics of vertices to endow TNE with the ability to capture high-order proximity (HP) of networks. In LongTNE, we employ graph self-supervised learning (Graph SSL) to optimize the establishment probability of deep links in each network snapshot. We also present an accumulated forward update (AFU) module to fathom global temporal evolution among multiple network snapshots. The empirical results on six temporal networks demonstrate that, in addition to achieving state-of-the-art performance on network mining tasks, LongTNE can be handily extended to existing TNE methods.},
  archive      = {J_TNNLS},
  author       = {Zhizheng Wang and Yuanyuan Sun and Zhihao Yang and Liang Yang and Hongfei Lin},
  doi          = {10.1109/TNNLS.2024.3384348},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5747-5758},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal network embedding enhanced with long-range dynamics and self-supervised learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive neural consensus observer networks design for a
class of semilinear parabolic PDE systems. <em>TNNLS</em>,
<em>36</em>(3), 5734–5746. (<a
href="https://doi.org/10.1109/TNNLS.2024.3383030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns the investigation on the consensus problem for the joint state-uncertainty estimation of a class of parabolic partial differential equation (PDE) systems with parametric and nonparametric uncertainties. We propose a two-layer network consisting of informed and uninformed boundary observers where novel adaptation laws are developed for the identification of uncertainties. Particularly, all observer agents in the network transmit their information with each other across the entire network. The proposed adaptation laws include a penalty term of the mismatch between the parameter estimates generated by the other observer agents. Moreover, for the nonparametric uncertainties, radial basis function (RBF) neural networks are employed for the universal approximation of unknown nonlinear functions. Given the persistently exciting condition, it is shown that the proposed network of adaptive observers can achieve exponential joint state-uncertainty estimation in the presence of parametric uncertainties and ultimate bounded estimation in the presence of nonparametric uncertainties based on the Lyapunov stability theory. The effects of the proposed consensus method are demonstrated through a typical reaction–diffusion system example, which implies convincing numerical findings.},
  archive      = {J_TNNLS},
  author       = {Mingxing Cai and Yuan Yuan and Biao Luo and Fanbiao Li and Xiaodong Xu and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TNNLS.2024.3383030},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5734-5746},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural consensus observer networks design for a class of semilinear parabolic PDE systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSTS: Exploring class-specific and task-shared embedding
representation for few-shot learning. <em>TNNLS</em>, <em>36</em>(3),
5721–5733. (<a
href="https://doi.org/10.1109/TNNLS.2024.3380833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) is a challenging yet promising technique that aims to discriminate objects based on a few labeled examples. Learning a high-quality feature representation is key with few-shot data, and many existing models attempt to extract general information from the sample or task levels. However, the common sample-level means of feature representation limits the models generalizability to different tasks, while task-level representation may lose class characteristics due to excessive information aggregation. In this article, we synchronize the class-specific and task-shared information from the class and task levels to obtain a better representation. Structure-based contrastive learning is introduced to obtain class-specific representations by increasing the interclass distance. A hierarchical class structure is constructed by clustering semantically similar classes using the idea of granular computing. When guided by a class structure, it is more difficult to distinguish samples in different classes that have similar characteristics than those with large interclass differences. To this end, structure-guided contrastive learning is introduced to study class-specific information. A hierarchical graph neural network is established to transfer task-shared information from coarse to fine. It hierarchically infers the target sample based on all samples in the task and yields a more general representation for FSL classification. Experiments on four benchmark datasets demonstrate the advantages of our model over several state-of-the-art models.},
  archive      = {J_TNNLS},
  author       = {Hong Zhao and Yuling Su and Zhiping Wu and Weiping Ding},
  doi          = {10.1109/TNNLS.2024.3380833},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5721-5733},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CSTS: Exploring class-specific and task-shared embedding representation for few-shot learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum spatial graph convolutional neural network model
on quantum circuits. <em>TNNLS</em>, <em>36</em>(3), 5706–5720. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a quantum spatial graph convolutional neural network (QSGCN) model that is implementable on quantum circuits, providing a novel avenue to processing non-Euclidean type data based on the state-of-the-art parameterized quantum circuit (PQC) computing platforms. Four basic blocks are constructed to formulate the whole QSGCN model, including the quantum encoding, the quantum graph convolutional layer, the quantum graph pooling layer, and the network optimization. In particular, the trainability of the QSGCN model is analyzed through discussions on the barren plateau phenomenon. Simulation results from various types of graph data are presented to demonstrate the learning, generalization, and robustness capabilities of the proposed quantum neural network (QNN) model.},
  archive      = {J_TNNLS},
  author       = {Jin Zheng and Qing Gao and Maciej Ogorzałek and Jinhu Lü and Yue Deng},
  doi          = {10.1109/TNNLS.2024.3382174},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5706-5720},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A quantum spatial graph convolutional neural network model on quantum circuits},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCL: A hierarchical contrastive learning framework for
zero-shot relation extraction. <em>TNNLS</em>, <em>36</em>(3),
5694–5705. (<a
href="https://doi.org/10.1109/TNNLS.2024.3379527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot relation extraction (ZSRE) is shown to become more significant in the current information extraction system, which aims at predicting relation classes that lack annotations or have just never appeared during training. Previous works focus on projecting sentences with their corresponding relation descriptions to an intermediate semantic space and searching the nearest semantic for predicting unseen classes. Though these methods can achieve sound performance, they only obtain inferior semantic information via a trivial distance metric and neglect the interaction in the instance representations. We are thus motivated to tackle these issues and propose a hierarchical contrastive learning (HCL) framework for ZSRE including projection-level and instance-level modules. Specifically, the projection-level component replaces the distance score function by contrastive loss to connect the input sentence with the relation semantic space. And the instance-level component integrates the external knowledge from sentence entities to establish new contrastive pairs for efficiently learning representations from mutual information. The experimental results on three well-known datasets demonstrate that our model surpasses the existing SOTA by at most 18.97% improvement on the F1 score when unseen classes are 15. Moreover, our model can achieve more competitive performance alone with the increasing number of unseen classes.},
  archive      = {J_TNNLS},
  author       = {Tianwei Yan and Shan Zhao and Minghao Hu and Mengzhu Wang and Xiang Zhang and Zhigang Luo and Meng Wang},
  doi          = {10.1109/TNNLS.2024.3379527},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5694-5705},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HCL: A hierarchical contrastive learning framework for zero-shot relation extraction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient online stream clustering based on fast peeling of
boundary micro-cluster. <em>TNNLS</em>, <em>36</em>(3), 5680–5693. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing number of applications generate streaming data, making data stream mining a popular research topic. Classification-based streaming algorithms require pre-training on labeled data. Manually labeling a large number of samples in the data stream is impractical and cost-prohibitive. Stream clustering algorithms rely on unsupervised learning. They have been widely studied for their ability to effectively analyze high-speed data streams without prior knowledge. Stream clustering plays a key role in data stream mining. Currently, most data stream clustering algorithms adopt the online–offline framework. In the online stage, micro-clusters are maintained, and in the offline stage, they are clustered using an algorithm similar to density-based spatial clustering of applications with noise (DBSCAN). When data streams have clusters with varying densities and ambiguous boundaries, traditional data stream clustering algorithms may be less effective. To overcome the above limitations, this article proposes a fully online stream clustering algorithm called fast boundary peeling stream clustering (FBPStream). First, FBPStream defines a decay-based kernel density estimation (KDE). It can discover clusters with varying densities and identify the evolving trend of streams well. Then, FBPStream implements an efficient boundary micro-cluster peeling technique to identify the potential core micro-clusters. Finally, FBPStream employs a parallel clustering strategy to effectively cluster core and boundary micro-clusters. The proposed algorithm is compared with ten popular algorithms on 15 data streams. Experimental results show that FBPStream is competitive with the other ten popular algorithms.},
  archive      = {J_TNNLS},
  author       = {Jiarui Sun and Mingjing Du and Chen Sun and Yongquan Dong},
  doi          = {10.1109/TNNLS.2024.3382033},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5680-5693},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient online stream clustering based on fast peeling of boundary micro-cluster},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex-valued convolutional gated recurrent neural network
for ultrasound beamforming. <em>TNNLS</em>, <em>36</em>(3), 5668–5679.
(<a href="https://doi.org/10.1109/TNNLS.2024.3384314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound detection is a potent tool for the clinical diagnosis of various diseases due to its real-time, convenient, and noninvasive qualities. Yet, existing ultrasound beamforming and related methods face a big challenge to improve both the quality and speed of imaging for the required clinical applications. The most notable characteristic of ultrasound signal data is its spatial and temporal features. Because most signals are complex-valued, directly processing them by using real-valued networks leads to phase distortion and inaccurate output. In this study, for the first time, we propose a complex-valued convolutional gated recurrent (CCGR) neural network to handle ultrasound analytic signals with the aforementioned properties. The complex-valued network operations proposed in this study improve the beamforming accuracy of complex-valued ultrasound signals over traditional real-valued methods. Further, the proposed deep integration of convolution and recurrent neural networks makes a great contribution to extracting rich and informative ultrasound signal features. Our experimental results reveal its outstanding imaging quality over existing state-of-the-art methods. More significantly, its ultrafast processing speed of only 0.07 s per image promises considerable clinical application potential. The code is available at https://github.com/zhangzm0128/CCGR.},
  archive      = {J_TNNLS},
  author       = {Zhiming Zhang and Zhenyu Lei and MengChu Zhou and Hideyuki Hasegawa and Shangce Gao},
  doi          = {10.1109/TNNLS.2024.3384314},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5668-5679},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Complex-valued convolutional gated recurrent neural network for ultrasound beamforming},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MultiFair: Model fairness with multiple sensitive
attributes. <em>TNNLS</em>, <em>36</em>(3), 5654–5667. (<a
href="https://doi.org/10.1109/TNNLS.2024.3384181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While existing fairness interventions show promise in mitigating biased predictions, most studies concentrate on single-attribute protections. Although a few methods consider multiple attributes, they either require additional constraints or prediction heads, incurring high computational overhead or jeopardizing the stability of the training process. More critically, they consider per-attribute protection approaches, raising concerns about fairness gerrymandering where certain attribute combinations remain unfair. This work aims to construct a neutral domain containing fused information across all subgroups and attributes. It delivers fair predictions as the fused input contains neutralized information for all considered attributes. Specifically, we adopt mixup operations to generate samples with fused information. However, our experiments reveal that directly adopting the operations leads to degraded prediction results. The excessive mixup operations result in unrecognizable training data. To this end, we design three distinct mixup schemes that balance information fusion across attributes while retaining distinct visual features critical for training valid models. Extensive experiments with multiple datasets and up to eight sensitive attributes demonstrate that the proposed MultiFair method can deliver fairness protections for multiple attributes while maintaining valid prediction results.},
  archive      = {J_TNNLS},
  author       = {Huan Tian and Bo Liu and Tianqing Zhu and Wanlei Zhou and Philip S. Yu},
  doi          = {10.1109/TNNLS.2024.3384181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5654-5667},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MultiFair: Model fairness with multiple sensitive attributes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting on-policy actor–critic with shallow updates in
critic. <em>TNNLS</em>, <em>36</em>(3), 5644–5653. (<a
href="https://doi.org/10.1109/TNNLS.2024.3378913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) benefits from the representation power of deep neural networks (NNs), to approximate the value function and policy in the learning process. Batch reinforcement learning (BRL) benefits from stable training and data efficiency with fixed representation and enjoys solid theoretical analysis. This work proposes least-squares deep policy gradient (LSDPG), a hybrid approach that combines least-squares reinforcement learning (RL) with online DRL to achieve the best of both worlds. LSDPG leverages a shared network to share useful features between policy (actor) and value function (critic). LSDPG learns policy, value function, and representation separately. First, LSDPG views deep NNs of the critic as a linear combination of representation weighted by the weights of the last layer and performs policy evaluation with regularized least-squares temporal difference (LSTD) methods. Second, arbitrary policy gradient algorithms can be applied to improve the policy. Third, an auxiliary task is used to periodically distill the features from the critic into the representation. Unlike most DRL methods, where the critic algorithms are often used in a nonstationary situation, i.e., the policy to be evaluated is changing, the critic in LSDPG is working on a stationary case in each iteration of the critic update. We prove that, under some conditions, the critic converges to the regularized TD fixpoint of current policy, and the actor converges to the local optimal policy. The experimental results on challenging Procgen benchmark illustrate the improvement of sample efficiency of LSDPG over proximal policy optimization and phasic policy gradient (PPG).},
  archive      = {J_TNNLS},
  author       = {Luntong Li and Yuanheng Zhu},
  doi          = {10.1109/TNNLS.2024.3378913},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5644-5653},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Boosting on-policy Actor–Critic with shallow updates in critic},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LRNAS: Differentiable searching for adversarially robust
lightweight neural architecture. <em>TNNLS</em>, <em>36</em>(3),
5629–5643. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adversarial robustness is critical to deep neural networks (DNNs) in deployment. However, the improvement of adversarial robustness often requires compromising with the network size. Existing approaches to addressing this problem mainly focus on the combination of model compression and adversarial training. However, their performance heavily relies on neural architectures, which are typically manual designs with extensive expertise. In this article, we propose a lightweight and robust neural architecture search (LRNAS) method to automatically search for adversarially robust lightweight neural architectures. Specifically, we propose a novel search strategy to quantify contributions of the components in the search space, based on which the beneficial components can be determined. In addition, we further propose an architecture selection method based on a greedy strategy, which can keep the model size while deriving sufficient beneficial components. Owing to these designs in LRNAS, the lightness, the natural accuracy, and the adversarial robustness can be collectively guaranteed to the searched architectures. We conduct extensive experiments on various benchmark datasets against the state of the arts. The experimental results demonstrate that the proposed LRNAS method is superior at finding lightweight neural architectures that are both accurate and adversarially robust under popular adversarial attacks. Moreover, ablation studies are also performed, which reveals the validity of the individual components designed in LRNAS and the component effects in positively deciding the overall performance.},
  archive      = {J_TNNLS},
  author       = {Yuqi Feng and Zeqiong Lv and Hongyang Chen and Shangce Gao and Fengping An and Yanan Sun},
  doi          = {10.1109/TNNLS.2024.3382724},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5629-5643},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LRNAS: Differentiable searching for adversarially robust lightweight neural architecture},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient hybrid federated learning for
e-health with horizontal and vertical data partitioning. <em>TNNLS</em>,
<em>36</em>(3), 5614–5628. (<a
href="https://doi.org/10.1109/TNNLS.2024.3383748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic healthcare (e-health) allows smart devices and medical institutions to collaboratively collect patients’ data, which is trained by artificial intelligence (AI) technologies to help doctors make diagnosis. By allowing multiple devices to train models collaboratively, federated learning is a promising solution to address the communication and privacy issues in e-health. However, applying federated learning in e-health faces many challenges. First, medical data are both horizontally and vertically partitioned. Since single horizontal federated learning (HFL) or vertical federated learning (VFL) techniques cannot deal with both types of data partitioning, directly applying them may consume excessive communication cost due to transmitting a part of raw data when requiring high modeling accuracy. Second, a naive combination of HFL and VFL has limitations including low training efficiency, unsound convergence analysis, and lack of parameter tuning strategies. In this article, we provide a thorough study on an effective integration of HFL and VFL, to achieve communication efficiency and overcome the above limitations when data are both horizontally and vertically partitioned. Specifically, we propose a hybrid federated learning framework with one intermediate result exchange and two aggregation phases. Based on this framework, we develop a hybrid stochastic gradient descent (HSGD) algorithm to train models. Then, we theoretically analyze the convergence upper bound of the proposed algorithm. Using the convergence results, we design adaptive strategies to adjust the training parameters and shrink the size of transmitted data. The experimental results validate that the proposed HSGD algorithm can achieve the desired accuracy while reducing communication cost, and they also verify the effectiveness of the adaptive strategies.},
  archive      = {J_TNNLS},
  author       = {Chong Yu and Shuaiqi Shen and Shiqiang Wang and Kuan Zhang and Hai Zhao},
  doi          = {10.1109/TNNLS.2024.3383748},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5614-5628},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Communication-efficient hybrid federated learning for E-health with horizontal and vertical data partitioning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing narrative commonsense reasoning with multilevel
causal knowledge. <em>TNNLS</em>, <em>36</em>(3), 5601–5613. (<a
href="https://doi.org/10.1109/TNNLS.2024.3380851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Narratives is an account of the unfolding of events, along with explanations of how and why these processes and events came to be. To understand narratives, causality has been proven to be especially useful. Causality manifests itself primarily at both the event and sentence levels, offering essential insights into understanding narratives. However, previous works utilize either sentence-level or event-level causalities. In this article, we devise a two-stage approach to fully exploit both levels of causal relationships. In the first stage, by devising posttraining tasks, we inject sentence-level causalities into pretrained language models (PLMs). The causal-enhanced PLMs, which carry sentence-level causalities, can be transferred to downstream tasks. In the second stage, we utilize event causalities to further refine narrative commonsense reasoning. But, the event sparsity problem brings about the difficulty to learn event representations and capture useful causal semantics. To alleviate this problem, we break down events into multiple word components, enabling the retrieval of word–word relations between these components. And it is possible to alleviate the event sparsity problem since word–word relations capture the interplays between event components. Based on the event-level causalities and the word-level relations, we construct the hierarchical knowledge graph (KG) as knowledge ground. A KG-based reasoning process is then employed for narrative commonsense reasoning. Experimental results affirm the effectiveness of our framework.},
  archive      = {J_TNNLS},
  author       = {Feiteng Mu and Wenjie Li},
  doi          = {10.1109/TNNLS.2024.3380851},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5601-5613},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhancing narrative commonsense reasoning with multilevel causal knowledge},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Universal approximation abilities of a modular
differentiable neural network. <em>TNNLS</em>, <em>36</em>(3),
5586–5600. (<a
href="https://doi.org/10.1109/TNNLS.2024.3378697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximation ability is one of the most important topics in the field of neural networks (NNs). Feedforward NNs, activated by rectified linear units and some of their specific smoothed versions, provide universal approximators to convex as well as continuous functions. However, most of these networks are investigated empirically, or their characteristics are analyzed based on specific operation rules. Moreover, an adequate level of interpretability of the networks is missing as well. In this work, we propose a class of new network architecture, built with reusable neural modules (functional blocks), to supply differentiable and interpretable approximators for convex and continuous target functions. Specifically, first, we introduce a concrete model construction mechanism with particular blocks based on differentiable programming and the composition essence of the max operator, extending the scope of existing activation functions. Moreover, explicit block diagrams are provided for a clear understanding of the external architecture and the internal processing mechanism. Subsequently, the approximation behavior of the proposed network to convex functions and continuous functions is rigorously proved as well, by virtue of mathematical induction. Finally, plenty of numerical experiments are conducted on a wide variety of problems, which exhibit the effectiveness and the superiority of the proposed model over some existing ones.},
  archive      = {J_TNNLS},
  author       = {Jian Wang and Shujun Wu and Huaqing Zhang and Bin Yuan and Caili Dai and Nikhil R. Pal},
  doi          = {10.1109/TNNLS.2024.3378697},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5586-5600},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Universal approximation abilities of a modular differentiable neural network},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal learning output tracking control: A model-free
policy optimization method with convergence analysis. <em>TNNLS</em>,
<em>36</em>(3), 5574–5585. (<a
href="https://doi.org/10.1109/TNNLS.2024.3379207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal learning output tracking control (OLOTC) in a model-free manner has received increasing attention in both the intelligent control and the reinforcement learning (RL) communities. Although the model-free tracking control has been achieved via off-policy learning and Q-learning, another popular RL idea of direct policy learning, with its easy-to-implement feature, is still rarely considered. To fill this gap, this article aims to develop a novel model-free policy optimization (PO) algorithm to achieve the OLOTC for unknown linear discrete-time (DT) systems. The iterative control policy is parameterized to directly improve the discounted value function of the augmented system via the gradient-based method. To implement this algorithm in a model-free manner, a model-free two-point policy gradient (PG) algorithm is designed to approximate the gradient of discounted value function by virtue of the sampled states and the reference trajectories. The global convergence of model-free PO algorithm to the optimal value function is demonstrated with the sufficient quantity of samples and proper conditions. Finally, numerical simulation results are provided to validate the effectiveness of the present method.},
  archive      = {J_TNNLS},
  author       = {Mingduo Lin and Bo Zhao and Derong Liu},
  doi          = {10.1109/TNNLS.2024.3379207},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5574-5585},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal learning output tracking control: A model-free policy optimization method with convergence analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-world light field image super-resolution via
degradation modulation. <em>TNNLS</em>, <em>36</em>(3), 5559–5573. (<a
href="https://doi.org/10.1109/TNNLS.2024.3378420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the great advances of deep neural networks (DNNs) in light field (LF) image super-resolution (SR). However, existing DNN-based LF image SR methods are developed on a single fixed degradation (e.g., bicubic downsampling), and thus cannot be applied to super-resolve real LF images with diverse degradation. In this article, we propose a simple yet effective method for real-world LF image SR. In our method, a practical LF degradation model is developed to formulate the degradation process of real LF images. Then, a convolutional neural network is designed to incorporate the degradation prior into the SR process. By training on LF images using our formulated degradation, our network can learn to modulate different degradation while incorporating both spatial and angular information in LF images. Extensive experiments on both synthetically degraded and real-world LF images demonstrate the effectiveness of our method. Compared with existing state-of-the-art single and LF image SR methods, our method achieves superior SR performance under a wide range of degradation, and generalizes better to real LF images. Codes and models are available at https://yingqianwang.github.io/LF-DMnet/.},
  archive      = {J_TNNLS},
  author       = {Yingqian Wang and Zhengyu Liang and Longguang Wang and Jungang Yang and Wei An and Yulan Guo},
  doi          = {10.1109/TNNLS.2024.3378420},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5559-5573},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Real-world light field image super-resolution via degradation modulation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multichannel orthogonal transform-based perceptron layers
for efficient ResNets. <em>TNNLS</em>, <em>36</em>(3), 5547–5558. (<a
href="https://doi.org/10.1109/TNNLS.2024.3384316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a set of transform-based neural network layers as an alternative to the $3\,\, {}\times {}3$ Conv2D layers in convolutional neural networks (CNNs). The proposed layers can be implemented based on orthogonal transforms, such as the discrete cosine transform (DCT), Hadamard transform (HT), and biorthogonal block wavelet transform (BWT). Furthermore, by taking advantage of the convolution theorems, convolutional filtering operations are performed in the transform domain using elementwise multiplications. Trainable soft-thresholding layers, that remove noise in the transform domain, bring nonlinearity to the transform domain layers. Compared with the Conv2D layer, which is spatial-agnostic and channel-specific, the proposed layers are location-specific and channel-specific. Moreover, these proposed layers reduce the number of parameters and multiplications significantly while improving the accuracy results of regular ResNets on the ImageNet-1K classification task. Furthermore, they can be inserted with a batch normalization (BN) layer before the global average pooling layer in the conventional ResNets as an additional layer to improve classification accuracy.},
  archive      = {J_TNNLS},
  author       = {Hongyi Pan and Emadeldeen Hamdan and Xin Zhu and Salih Atici and Ahmet Enis Cetin},
  doi          = {10.1109/TNNLS.2024.3384316},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5547-5558},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multichannel orthogonal transform-based perceptron layers for efficient ResNets},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph exploration for effective multiagent q-learning.
<em>TNNLS</em>, <em>36</em>(3), 5535–5546. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an exploration technique for multiagent reinforcement learning (MARL) with graph-based communication among agents. We assume that the individual rewards received by the agents are independent of the actions by the other agents, while their policies are coupled. In the proposed framework, neighboring agents collaborate to estimate the uncertainty about the state–action space in order to execute more efficient explorative behavior. Different from existing works, the proposed algorithm does not require counting mechanisms and can be applied to continuous-state environments without requiring complex conversion techniques. Moreover, the proposed scheme allows agents to communicate in a fully decentralized manner with minimal information exchange. And for continuous-state scenarios, each agent needs to exchange only a single parameter vector. The performance of the algorithm is verified with theoretical results for discrete-state scenarios and with experiments for the continuous ones.},
  archive      = {J_TNNLS},
  author       = {Ainur Zhaikhan and Ali H. Sayed},
  doi          = {10.1109/TNNLS.2024.3382480},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5535-5546},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph exploration for effective multiagent Q-learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMFAN: Cross-modal feature alignment network for few-shot
single-view 3D reconstruction. <em>TNNLS</em>, <em>36</em>(3),
5522–5534. (<a
href="https://doi.org/10.1109/TNNLS.2024.3383039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot single-view 3D reconstruction learns to reconstruct the novel category objects based on a query image and a few support shapes. However, since the query image and the support shapes are of different modalities, there is an inherent feature misalignment problem damaging the reconstruction. Previous works in the literature do not consider this problem. To this end, we propose the cross-modal feature alignment network (CMFAN) with two novel techniques. One is a strategy for model pretraining, namely, cross-modal contrastive learning (CMCL), here the 2D images and 3D shapes of the same objects compose the positives, and those from different objects form the negatives. With CMCL, the model learns to embed the 2D and 3D modalities of the same object into a tight area in the feature space and push away those from different objects, thus effectively aligning the global cross-modal features. The other is cross-modal feature fusion (CMFF), which further aligns and fuses the local features. Specifically, it first re-represents the local features with the cross-attention operation, making the local features share more information. Then, CMFF generates a descriptor for the support features and attaches it to each local feature vector of the query image with dense concatenation. Moreover, CMFF can be applied to multilevel local features and brings further advantages. We conduct extensive experiments to evaluate the effectiveness of our designs, and CMFAN sets new state-of-the-art performance in all of the 1-/10-/25-shot tasks of ShapeNet and ModelNet datasets.},
  archive      = {J_TNNLS},
  author       = {Lvlong Lai and Jian Chen and Zehong Zhang and Guosheng Lin and Qingyao Wu},
  doi          = {10.1109/TNNLS.2024.3383039},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5522-5534},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CMFAN: Cross-modal feature alignment network for few-shot single-view 3D reconstruction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric matching for cross-modal retrieval.
<em>TNNLS</em>, <em>36</em>(3), 5509–5521. (<a
href="https://doi.org/10.1109/TNNLS.2024.3381347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its significant progress, cross-modal retrieval still suffers from one-to-many matching cases, where the multiplicity of semantic instances in another modality could be acquired by a given query. However, existing approaches usually map heterogeneous data into the learned space as deterministic point vectors. In spite of their remarkable performance in matching the most similar instance, such deterministic point embedding suffers from the insufficient representation of rich semantics in one-to-many correspondence. To address the limitations, we intuitively extend a deterministic point into a closed geometry and develop geometric representation learning methods for cross-modal retrieval. Thus, a set of points inside such a geometry could be semantically related to many candidates, and we could effectively capture the semantic uncertainty. We then introduce two types of geometric matching for one-to-many correspondence, i.e., point-to-rectangle matching (dubbed P2RM) and rectangle-to-rectangle matching (termed R2RM). The former treats all retrieved candidates as rectangles with zero volume (equivalent to points) and the query as a box, while the latter encodes all heterogeneous data into rectangles. Therefore, we could evaluate semantic similarity among heterogeneous data by the Euclidean distance from a point to a rectangle or the volume of intersection between two rectangles. Additionally, both strategies could be easily employed for off-the-self approaches and further improve the retrieval performance of baselines. Under various evaluation metrics, extensive experiments and ablation studies on several commonly used datasets, two for image-text matching and two for video-text retrieval, demonstrate our effectiveness and superiority.},
  archive      = {J_TNNLS},
  author       = {Zheng Wang and Zhenwei Gao and Yang Yang and Guoqing Wang and Chengbo Jiao and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2024.3381347},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5509-5521},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Geometric matching for cross-modal retrieval},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangling modality and posture factors: Memory-attention
and orthogonal decomposition for visible-infrared person
re-identification. <em>TNNLS</em>, <em>36</em>(3), 5494–5508. (<a
href="https://doi.org/10.1109/TNNLS.2024.3384023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Striving to match the person identities between visible (VIS) and near-infrared (NIR) images, VIS-NIR reidentification (Re-ID) has attracted increasing attention due to its wide applications in low-light scenes. However, owing to the modality and pose discrepancies exhibited in heterogeneous images, the extracted representations inevitably comprise various modality and posture factors, impacting the matching of cross-modality person identity. To solve the problem, we propose a disentangling modality and posture factors (DMPFs) model to disentangle modality and posture factors by fusing the information of features memory and pedestrian skeleton. Specifically, the DMPF comprises three modules: three-stream features extraction network (TFENet), modality factor disentanglement (MFD), and posture factor disentanglement (PFD). First, aiming to provide memory and skeleton information for modality and posture factors disentanglement, the TFENet is designed as a three-stream network to extract VIS-NIR image features and skeleton features. Second, to eliminate modality discrepancy across different batches, we maintain memory queues of previous batch features through the momentum updating mechanism and propose MFD to integrate features in the whole training set by memory-attention layers. These layers explore intramodality and intermodality relationships between features from the current batch and memory queues under the optimization of the optimal transport (OT) method, which encourages the heterogeneous features with the same identity to present higher similarity. Third, to decouple the posture factors from representations, we introduce the PFD module to learn posture-unrelated features with the assistance of the skeleton features. Besides, we perform subspace orthogonal decomposition on both image and skeleton features to separate the posture-related and identity-related information. The posture-related features are adopted to disentangle the posture factors from representations by a designed posture-features consistency (PfC) loss, while the identity-related features are concatenated to obtain more discriminative identity representations. The effectiveness of DMPF is validated through comprehensive experiments on two VIS-NIR pedestrian Re-ID datasets.},
  archive      = {J_TNNLS},
  author       = {Zefeng Lu and Ronghao Lin and Haifeng Hu},
  doi          = {10.1109/TNNLS.2024.3384023},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5494-5508},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disentangling modality and posture factors: Memory-attention and orthogonal decomposition for visible-infrared person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizable and discriminative representations for
adversarially robust few-shot learning. <em>TNNLS</em>, <em>36</em>(3),
5480–5493. (<a
href="https://doi.org/10.1109/TNNLS.2024.3379172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot image classification (FSIC) is beneficial for a variety of real-world scenarios, aiming to construct a recognition system with limited training data. In this article, we extend the original FSIC task by incorporating defense against malicious adversarial examples. This can be an arduous challenge because numerous deep learning-based approaches remain susceptible to adversarial examples, even when trained with ample amounts of data. Previous studies on this problem have predominantly concentrated on the meta-learning framework, which involves sampling numerous few-shot tasks during the training stage. In contrast, we propose a straightforward but effective baseline via learning robust and discriminative representations without tedious meta-task sampling, which can further be generalized to unforeseen adversarial FSIC tasks. Specifically, we introduce an adversarial-aware (AA) mechanism that exploits feature-level distinctions between the legitimate and the adversarial domains to provide supplementary supervision. Moreover, we design a novel adversarial reweighting training strategy to ameliorate the imbalance among adversarial examples. To further enhance the adversarial robustness without compromising discriminative features, we propose the cyclic feature purifier during the postprocessing projection, which can reduce the interference of unforeseen adversarial examples. Furthermore, our method can obtain robust feature embeddings that maintain superior transferability, even when facing cross-domain adversarial examples. Extensive experiments and systematic analyses demonstrate that our method achieves state-of-the-art robustness as well as natural performance among adversarially robust FSIC algorithms on three standard benchmarks by a substantial margin.},
  archive      = {J_TNNLS},
  author       = {Junhao Dong and Yuan Wang and Xiaohua Xie and Jianhuang Lai and Yew-Soon Ong},
  doi          = {10.1109/TNNLS.2024.3379172},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5480-5493},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalizable and discriminative representations for adversarially robust few-shot learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CycleTrans: Learning neutral yet discriminative features via
cycle construction for visible- infrared person re-identification.
<em>TNNLS</em>, <em>36</em>(3), 5469–5479. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is the task of matching the same individuals across the visible and infrared modalities. Its main challenge lies in the modality gap caused by the cameras operating on different spectra. Existing VI-ReID methods mainly focus on learning general features across modalities, often at the expense of feature discriminability. To address this issue, we present a novel cycle-construction-based network for neutral yet discriminative feature learning, termed CycleTrans. Specifically, CycleTrans uses a lightweight knowledge capturing module (KCM) to capture rich semantics from the modality-relevant feature maps according to pseudo anchors. Afterward, a discrepancy modeling module (DMM) is deployed to transform these features into neutral ones according to the modality-irrelevant prototypes. To ensure feature discriminability, another two KCMs are further deployed for feature cycle constructions. With cycle construction, our method can learn effective neutral features for visible and infrared images while preserving their salient semantics. Extensive experiments on SYSU-MM01 and RegDB datasets validate the merits of CycleTrans against a flurry of state-of-the-art (SOTA) methods, $+1.88\%$ on rank-1 in SYSU-MM01 and $+1.1\%$ on rank-1 in RegDB. Our code is available at https://github.com/DoubtedSteam/CycleTrans.},
  archive      = {J_TNNLS},
  author       = {Qiong Wu and Jiaer Xia and Pingyang Dai and Yiyi Zhou and Yongjian Wu and Rongrong Ji},
  doi          = {10.1109/TNNLS.2024.3382937},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5469-5479},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CycleTrans: Learning neutral yet discriminative features via cycle construction for visible- infrared person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AttMOT: Improving multiple-object tracking by introducing
auxiliary pedestrian attributes. <em>TNNLS</em>, <em>36</em>(3),
5454–5468. (<a
href="https://doi.org/10.1109/TNNLS.2024.3384446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiobject tracking (MOT) is a fundamental problem in computer vision with numerous applications, such as intelligent surveillance and automated driving. Despite the significant progress made in MOT, pedestrian attributes, such as gender, hairstyle, body shape, and clothing features, which contain rich and high-level information, have been less explored. To address this gap, we propose a simple, effective, and generic method to predict pedestrian attributes to support general reidentification (Re-ID) embedding. We first introduce attribute multi-object tracking (AttMOT), a large, highly enriched synthetic dataset for pedestrian tracking, containing over 80k frames and six million pedestrian identity switches (IDs) with different times, weather conditions, and scenarios. To the best of authors’ knowledge, AttMOT is the first MOT dataset with semantic attributes. Subsequently, we explore different approaches to fuse Re-ID embedding and pedestrian attributes, including attention mechanisms, which we hope will stimulate the development of attribute-assisted MOT. The proposed method attribute-assisted method (AAM) demonstrates its effectiveness and generality on several representative pedestrian MOT benchmarks, including MOT17 and MOT20, through experiments on the AttMOT dataset. When applied to the state-of-the-art trackers, AAM achieves consistent improvements in multi-object tracking accuracy (MOTA), higher order tracking accuracy (HOTA), association accuracy (AssA), IDs, and IDF1 scores. For instance, on MOT17, the proposed method yields a +1.1 MOTA, +1.7 HOTA, and +1.8 IDF1 improvement when used with FairMOT. To further encourage related research, we release the data and code at https://github.com/HengLan/AttMOT.},
  archive      = {J_TNNLS},
  author       = {Yunhao Li and Zhen Xiao and Lin Yang and Dan Meng and Xin Zhou and Heng Fan and Libo Zhang},
  doi          = {10.1109/TNNLS.2024.3384446},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5454-5468},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AttMOT: Improving multiple-object tracking by introducing auxiliary pedestrian attributes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-step multiview clustering via adaptive graph learning
and spectral rotation. <em>TNNLS</em>, <em>36</em>(3), 5442–5453. (<a
href="https://doi.org/10.1109/TNNLS.2024.3381223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In graph based multiview clustering methods, the ultimate partition result is usually achieved by spectral embedding of the consistent graph using some traditional clustering methods, such as $k$ -means. However, optimal performance will be reduced by this multistep procedure since it cannot unify graph learning with partition generation closely. In this article, we propose a one-step multiview clustering method through adaptive graph learning and spectral rotation (AGLSR). For every view, AGLSR adaptively learns affinity graphs to capture similar relationships of samples. Then, a spectral embedding is designed to take advantage of the potential feature space shared by different views. In addition, AGLSR utilizes a spectral rotation strategy to obtain the discrete clustering labels from the learned spectral embeddings directly. An effective updating algorithm with proven convergence is derived to optimize the optimization problem. Sufficient experiments on benchmark datasets have clearly demonstrated the effectiveness of the proposed method in six metrics. The code of AGLSR is uploaded at https://github.com/tangchuan2000/AGLSR.},
  archive      = {J_TNNLS},
  author       = {Chuan Tang and Minhui Wang and Kun Sun},
  doi          = {10.1109/TNNLS.2024.3381223},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5442-5453},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {One-step multiview clustering via adaptive graph learning and spectral rotation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local–global geometric information and view complementarity
introduced multiview metric learning. <em>TNNLS</em>, <em>36</em>(3),
5428–5441. (<a
href="https://doi.org/10.1109/TNNLS.2024.3380020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometry studies the spatial structure and location information of objects, providing a priori knowledge and intuitive explanation for classification methods. Considering samples from a geometric perspective offers a novel approach to understanding their information. In this article, we propose a method called local–global geometric information and view complementarity introduced multiview metric learning (GIVCMML). Our method effectively exploits the geometric information of multiview samples. The learned metric space retains the geometric relations of samples and makes them more separable. First, we propose the global geometrical constraint in the maximum margin criterion framework. By maximizing the distance between class centers in the metric space, we ensure that samples from different classes are well separated. Second, to maintain the manifold structure of the original space, we build an adjacency matrix that contains the sample label information. This helps explore the local geometric information of sample pairs. Finally, to better mine the complementary information of multiview samples, GIVCMML maximizes the correlation between each view in the metric space. This enables each view to adaptively learn from the others and explore the complementary information between views. We extensively evaluate the effectiveness of our method on real-world datasets. The experimental results demonstrate that GIVCMML achieves competitive performance compared with multiview metric learning (MvML) methods.},
  archive      = {J_TNNLS},
  author       = {Xinlei Xu and Zhe Wang and Shuangyan Ren and Saisai Niu and Dongdong Li},
  doi          = {10.1109/TNNLS.2024.3380020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5428-5441},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local–Global geometric information and view complementarity introduced multiview metric learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive critic learning-based optimal bipartite consensus
for multiagent systems with prescribed performance. <em>TNNLS</em>,
<em>36</em>(3), 5417–5427. (<a
href="https://doi.org/10.1109/TNNLS.2024.3379503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a distributed bipartite optimal consensus scheme while ensuring user-predefined performance is essential in practical applications. Existing approaches to this problem typically require a complex controller structure due to adopting an identifier-actor–critic framework and prescribed performance cannot be guaranteed. In this work, an adaptive critic learning (ACL)-based optimal bipartite consensus scheme is developed to bridge the gap. A newly designed error scaling function, which defines the user-predefined settling time and steady accuracy without relying on the initial conditions, is then integrated into a cost function. The backstepping framework combines the ACL and integral reinforcement learning (IRL) algorithm to develop the adaptive optimal bipartite consensus scheme, which contributes a critic-only controller structure by removing the identifier and actor networks in the existing methods. The adaptive law of the critic network is derived by the gradient descent algorithm and experience replay to minimize the IRL-based residual error. It is shown that a compute-saving learning mechanism can achieve the optimal consensus, and the error variables of the closed-loop system are uniformly ultimately bounded (UUB). Besides, in any bounded initial condition, the evolution of bipartite consensus is limited to a user-prescribed boundary under bounded initial conditions. The illustrative simulation results validate the efficacy of the approach.},
  archive      = {J_TNNLS},
  author       = {Lei Yan and Junhe Liu and Guanyu Lai and C. L. Philip Chen and Zongze Wu and Zhi Liu},
  doi          = {10.1109/TNNLS.2024.3379503},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5417-5427},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive critic learning-based optimal bipartite consensus for multiagent systems with prescribed performance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrast-assisted domain-specificity-removal network for
semi-supervised generalization fault diagnosis. <em>TNNLS</em>,
<em>36</em>(3), 5403–5416. (<a
href="https://doi.org/10.1109/TNNLS.2024.3383467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unknown domain shift caused by the unavailability of target domain during training phase degrades the performance of intelligent fault diagnosis models in practical applications. Domain generalization (DG)-based methods have recently emerged to alleviate the influence of domain shift and improve the generalization ability of models toward invisible working conditions. However, most existing studies are conducted on multiple fully labeled source domains. Meanwhile, domain-specific information related to the variations of working conditions is often neglected during model training. Therefore, in order to realize reliable generalization fault diagnosis based on partially labeled source domains, this article proposes a contrast-assisted domain-specificity-removal network (CDSRN) to extract transferable features from domain-specificity-removal perspective. Concretely, a domain-specific feature removal branch is designed to disentangle domain-invariant features and domain-specific features, thus excavating generalized information only in domain-invariance dimension. Simultaneously, proxy-contrastive representation enhancement module is embedded to facilitate the fault class-discriminative and domain-discriminative feature learning, thereby assisting the model in further improvement of generalization capability. Experimental studies confirm the effectiveness and competitiveness of the proposed CDSRN in semi-supervised generalization fault diagnosis.},
  archive      = {J_TNNLS},
  author       = {Qiuyu Song and Xingxing Jiang and Jie Liu and Juanjuan Shi and Zhongkui Zhu},
  doi          = {10.1109/TNNLS.2024.3383467},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5403-5416},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Contrast-assisted domain-specificity-removal network for semi-supervised generalization fault diagnosis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network compression based on tensor ring
decomposition. <em>TNNLS</em>, <em>36</em>(3), 5388–5402. (<a
href="https://doi.org/10.1109/TNNLS.2024.3383392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have made great breakthroughs and seen applications in many domains. However, the incomparable accuracy of DNNs is achieved with the cost of considerable memory consumption and high computational complexity, which restricts their deployment on conventional desktops and portable devices. To address this issue, low-rank factorization, which decomposes the neural network parameters into smaller sized matrices or tensors, has emerged as a promising technique for network compression. In this article, we propose leveraging the emerging tensor ring (TR) factorization to compress the neural network. We investigate the impact of both parameter tensor reshaping and TR decomposition (TRD) on the total number of compressed parameters. To achieve the maximal parameter compression, we propose an algorithm based on prime factorization that simultaneously identifies the optimal tensor reshaping and TRD. In addition, we discover that different execution orders of the core tensors result in varying computational complexities. To identify the optimal execution order, we construct a novel tree structure. Based on this structure, we propose a top-to-bottom splitting algorithm to schedule the execution of core tensors, thereby minimizing computational complexity. We have performed extensive experiments using three kinds of neural networks with three different datasets. The experimental results demonstrate that, compared with the three state-of-the-art algorithms for low-rank factorization, our algorithm can achieve better performance with much lower memory consumption and lower computational complexity.},
  archive      = {J_TNNLS},
  author       = {Kun Xie and Can Liu and Xin Wang and Xiaocan Li and Gaogang Xie and Jigang Wen and Kenli Li},
  doi          = {10.1109/TNNLS.2024.3383392},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5388-5402},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network compression based on tensor ring decomposition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pontryagin’s minimum principle-guided RL for minimum-time
exploration of spatiotemporal fields. <em>TNNLS</em>, <em>36</em>(3),
5375–5387. (<a
href="https://doi.org/10.1109/TNNLS.2024.3379654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the trajectory planning problem of an autonomous vehicle for exploring a spatiotemporal field subject to a constraint on cumulative information. Since the resulting problem depends on the signal strength distribution of the field, which is unknown in practice, we advocate the use of a model-free reinforcement learning (RL) method to find the solution. Given the vehicle’s dynamical model, a critical (and open) question is how to judiciously merge the model-based optimality conditions into the model-free RL framework for improved efficiency and generalization, for which this work provides some positive results. Specifically, we discretize the continuous action space by leveraging analytic optimality conditions for the minimum-time optimization problem via Pontryagin’s minimum principle (PMP). This allows us to develop a novel discrete PMP-based RL trajectory planning algorithm, which learns a planning policy faster than those based on a continuous action space. Simulation results: 1) validate the effectiveness of the PMP-based RL algorithm and 2) demonstrate its advantages, in terms of both learning efficiency and the vehicle’s exploration time, over two baseline methods for continuous control inputs.},
  archive      = {J_TNNLS},
  author       = {Zhuo Li and Jian Sun and Antonio G. Marques and Gang Wang and Keyou You},
  doi          = {10.1109/TNNLS.2024.3379654},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5375-5387},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pontryagin’s minimum principle-guided RL for minimum-time exploration of spatiotemporal fields},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NGDE: A niching-based gradient-directed evolution algorithm
for nonconvex optimization. <em>TNNLS</em>, <em>36</em>(3), 5363–5374.
(<a href="https://doi.org/10.1109/TNNLS.2024.3378805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonconvex optimization issues are prevalent in machine learning and data science. While gradient-based optimization algorithms can rapidly converge and are dimension-independent, they may, unfortunately, fall into local optimal solutions or saddle points. In contrast, evolutionary algorithms (EAs) gradually adapt the population of solutions to explore global optimal solutions. However, this approach requires substantial computational resources to perform numerous fitness function evaluations, which poses challenges for high-dimensional optimization in particular. This study introduces a novel nonconvex optimization algorithm, the niching-based gradient-directed evolution (NGDE) algorithm, designed specifically for high-dimensional nonconvex optimization. The NGDE algorithm generates potential solutions and divides them into multiple niches to explore distinct areas within the feasible region. Subsequently, each individual creates candidate offspring using the gradient-directed mutation operator we designed. The convergence properties of the NGDE algorithm are investigated in two scenarios: accessing the full gradient and approximating the gradient with mini-batch samples. The experimental studies demonstrate the superior performance of the NGDE algorithm in minimizing multimodal optimization functions. Additionally, when applied to train the neural networks of LeNet-5, NGDE shows significantly improved classification accuracy, especially in smaller training sizes.},
  archive      = {J_TNNLS},
  author       = {Qi Yu and Xijun Liang and Mengzhen Li and Ling Jian},
  doi          = {10.1109/TNNLS.2024.3378805},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5363-5374},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {NGDE: A niching-based gradient-directed evolution algorithm for nonconvex optimization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical machine learning for power flow analysis
considering the influence of weather factors on photovoltaic power
generation. <em>TNNLS</em>, <em>36</em>(3), 5348–5362. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally accepted that the impact of weather variation is gradually increasing in modern distribution networks with the integration of high-proportion photovoltaic (PV) power generation and weather-sensitive loads. This article analyzes power flow using a novel stochastic weather generator (SWG) based on statistical machine learning (SML). The proposed SML model, which incorporates generative adversarial networks (GANs), probability theory, and information theory, enables the generation and evaluation of simulated hourly weather data throughout the year. The GAN model captures various weather variation characteristics, including weather uncertainties, diurnal variations, and seasonal patterns. Compared to shallow learning models, the proposed deep learning model exhibits significant advantages in stochastic weather simulation. The simulated data generated by the proposed model closely resemble real data in terms of time-series regularity, integrity, and stochasticity. The SWG is applied to model PV power generation and weather-sensitive loads. Then, we actively conduct a power flow analysis (PFA) on a real distribution network in Guangdong, China, using simulated data for an entire year. The results provide evidence that the GAN-based SWG surpasses the shallow machine learning approach in terms of accuracy. The proposed model ensures accurate analysis of weather-related power flow and provides valuable insights for the analysis, planning, and design of distribution networks.},
  archive      = {J_TNNLS},
  author       = {Xueqian Fu and Chunyu Zhang and Yan Xu and Youmin Zhang and Hongbin Sun},
  doi          = {10.1109/TNNLS.2024.3382763},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5348-5362},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Statistical machine learning for power flow analysis considering the influence of weather factors on photovoltaic power generation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dimensionality reduction method for the output regulation of
boolean control networks. <em>TNNLS</em>, <em>36</em>(3), 5334–5347. (<a
href="https://doi.org/10.1109/TNNLS.2024.3380247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a dimensionality reduction approach to study the output regulation problem (ORP) of Boolean control networks (BCNs), which has much lower computational complexity than previous results. First, an auxiliary system which is much smaller in scale than the augmented system in previous approach is constructed. By analyzing the set stabilization of the auxiliary system as well as the original BCN, a necessary and sufficient condition to detect the solvability of the ORP is presented. Second, a method to design the state feedback controls for the ORP is proposed. Finally, two biological examples are given to demonstrate the effectiveness and advantage of the obtained new results.},
  archive      = {J_TNNLS},
  author       = {Shihua Fu and Jun-e Feng and Yuan Zhao and Jianjun Wang and Jinfeng Pan},
  doi          = {10.1109/TNNLS.2024.3380247},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5334-5347},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dimensionality reduction method for the output regulation of boolean control networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEs-inspired accelerated unfolded linearized ADMM networks
for inverse problems. <em>TNNLS</em>, <em>36</em>(3), 5319–5333. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many research works have shown that the traditional alternating direction multiplier methods (ADMMs) can be better understood by continuous-time differential equations (DEs). On the other hand, many unfolded algorithms directly inherit the traditional iterations to build deep networks. Although they achieve superior practical performance and a faster convergence rate than traditional counterparts, there is a lack of clear insight into unfolded network structures. Thus, we attempt to explore the unfolded linearized ADMM (LADMM) from the perspective of DEs, and design more efficient unfolded networks. First, by proposing an unfolded Euler LADMM scheme and inspired by the trapezoid discretization, we design a new more accurate Trapezoid LADMM scheme. For the convenience of implementation, we provide its explicit version via a prediction–correction strategy. Then, to expand the representation space of unfolded networks, we design an accelerated variant of our Euler LADMM scheme, which can be interpreted as second-order DEs with stronger representation capabilities. To fully explore this representation space, we designed an accelerated Trapezoid LADMM scheme. To the best of our knowledge, this is the first work to explore a comprehensive connection with theoretical guarantees between unfolded ADMMs and first- (second-) order DEs. Finally, we instantiate our schemes as (A-)ELADMM and (A-)TLADMM with the proximal operators, and (A-)ELADMM-Net and (A-)TLADMM-Net with convolutional neural networks (CNNs). Extensive inverse problem experiments show that our Trapezoid LADMM schemes perform better than well-known methods.},
  archive      = {J_TNNLS},
  author       = {Weixin An and Yuanyuan Liu and Fanhua Shang and Hongying Liu and Licheng Jiao},
  doi          = {10.1109/TNNLS.2024.3382030},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5319-5333},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DEs-inspired accelerated unfolded linearized ADMM networks for inverse problems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision-making with speculative opponent models.
<em>TNNLS</em>, <em>36</em>(3), 5304–5318. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opponent modeling has proven effective in enhancing the decision-making of the controlled agent by constructing models of opponent agents. However, existing methods often rely on access to the observations and actions of opponents, a requirement that is infeasible when such information is either unobservable or challenging to obtain. To address this issue, we introduce distributional opponent-aided multiagent actor–critic (DOMAC), the first speculative opponent modeling algorithm that relies solely on local information (i.e., the controlled agent’s observations, actions, and rewards). Specifically, the actor maintains a speculated belief about the opponents using the tailored speculative opponent models that predict the opponents’ actions using only local information. Moreover, DOMAC features distributional critic models that estimate the return distribution of the actor’s policy, yielding a more fine-grained assessment of the actor’s quality. This thus more effectively guides the training of the speculative opponent models that the actor depends upon. Furthermore, we formally derive a policy gradient theorem with the proposed opponent models. Extensive experiments under eight different challenging multiagent benchmark tasks within the MPE, Pommerman, and starcraft multiagent challenge (SMAC) demonstrate that our DOMAC successfully models opponents’ behaviors and delivers superior performance against state-of-the-art (SOTA) methods with a faster convergence speed.},
  archive      = {J_TNNLS},
  author       = {Jing Sun and Shuo Chen and Cong Zhang and Yining Ma and Jie Zhang},
  doi          = {10.1109/TNNLS.2024.3382985},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5304-5318},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decision-making with speculative opponent models},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilabel feature selection via shared latent sublabel
structure and simultaneous orthogonal basis clustering. <em>TNNLS</em>,
<em>36</em>(3), 5288–5303. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel feature selection solves the dimension distress of high-dimensional multilabel data by selecting the optimal subset of features. Noisy and incomplete labels of raw multilabel data hinder the acquisition of label-guided information. In existing approaches, mapping the label space to a low-dimensional latent space by semantic decomposition to mitigate label noise is considered an effective strategy. However, the decomposed latent label space contains redundant label information, which misleads the capture of potential label relevance. To eliminate the effect of redundant information on the extraction of latent label correlations, a novel method named SLOFS via shared latent sublabel structure and simultaneous orthogonal basis clustering for multilabel feature selection is proposed. First, a latent orthogonal base structure shared (LOBSS) term is engineered to guide the construction of a redundancy-free latent sublabel space via the separated latent clustering center structure. The LOBSS term simultaneously retains latent sublabel information and latent clustering center structure. Moreover, the structure and relevance information of nonredundant latent sublabels are fully explored. The introduction of graph regularization ensures structural consistency in the data space and latent sublabels, thus helping the feature selection process. SLOFS employs a dynamic sublabel graph to obtain a high-quality sublabel space and uses regularization to constrain label correlations on dynamic sublabel projections. Finally, an effective convergence provable optimization scheme is proposed to solve the SLOFS method. The experimental studies on the 18 datasets demonstrate that the presented method performs consistently better than previous feature selection methods.},
  archive      = {J_TNNLS},
  author       = {Ronghua Shang and Jingyu Zhong and Weitong Zhang and Songhua Xu and Yangyang Li},
  doi          = {10.1109/TNNLS.2024.3382911},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5288-5303},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilabel feature selection via shared latent sublabel structure and simultaneous orthogonal basis clustering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-order neighbors aware representation learning for
knowledge graph completion. <em>TNNLS</em>, <em>36</em>(3), 5273–5287.
(<a href="https://doi.org/10.1109/TNNLS.2024.3383873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a building block of knowledge acquisition, knowledge graph completion (KGC) aims at inferring missing facts in knowledge graphs (KGs) automatically. Previous studies mainly focus on graph convolutional network (GCN)-based KG embedding (KGE) to determine the representations of entities and relations, accordingly predicting missing triplets. However, most existing KGE methods suffer from limitations in predicting tail entities that are far away or even unreachable in KGs. This limitation can be attributed to the related high-order information being largely ignored. In this work, we focus on learning the information from the related high-order neighbors in KGs to improve the performance of prediction. Specifically, we first introduce a set of new nodes called pedalnodes to augment the KGs for facilitating message passing between related high-order entities, effectively injecting the information of high-order neighbors into entity representation. Additionally, we propose strength-guided graph neural networks to aggregate neighboring entity representations. To address the issue of transmitting irrelevant higher order information to entities through pedal nodes, which can potentially hurt entity representation, we further propose to dynamically integrate the aggregated representation of each node with its corresponding self-representation. Extensive experiments have been conducted on three benchmark datasets and the results demonstrate the superiority of our method compared to strong baseline models.},
  archive      = {J_TNNLS},
  author       = {Hong Yin and Jiang Zhong and Rongzhen Li and Jiaxing Shang and Chen Wang and Xue Li},
  doi          = {10.1109/TNNLS.2024.3383873},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5273-5287},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {High-order neighbors aware representation learning for knowledge graph completion},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To combat multiclass imbalanced problems by aggregating
evolutionary hierarchical classifiers. <em>TNNLS</em>, <em>36</em>(3),
5258–5272. (<a
href="https://doi.org/10.1109/TNNLS.2024.3383672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets are often imbalanced, posing frequent challenges to canonical machine learning algorithms that assume a balanced class distribution. Moreover, the imbalance problem becomes more complicated when the dataset is multiclass. Although many approaches have been presented for imbalanced learning (IL), research on the multiclass imbalanced problem is relatively limited and deficient. To alleviate these issues, we propose a forest of evolutionary hierarchical classifiers (FEHC) method for multiclass IL (MCIL). FEHC can be seen as a classifier fusion framework with a forest structure, and it aggregates several evolutionary hierarchical multiclassifiers (EHMCs) to reduce generalization error. Specifically, a multichromosome genetic algorithm (MCGA) is designed to simultaneously select (sub)optimal features, classifiers, and hierarchical structures when generating these EHMCs. The MCGA adopts a dynamic weighting module to learn difficult classes and promote the diversity of FEHC. We also present the “stratified underbagging” (SUB) strategy to address class imbalance and the “soft tree traversal” (STT) strategy to make FEHC converge faster and better. We thoroughly evaluate the proposed algorithm using 14 multiclass imbalanced datasets with various properties. Compared with popular and state-of-the-art approaches, FEHC obtains better performance under different evaluation metrics. Codes have been made publicly available on GitHub.1},
  archive      = {J_TNNLS},
  author       = {Zhihan Ning and Zhixing Jiang and David Zhang},
  doi          = {10.1109/TNNLS.2024.3383672},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5258-5272},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {To combat multiclass imbalanced problems by aggregating evolutionary hierarchical classifiers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New RNN algorithms for different time-variant matrix
inequalities solving under discrete-time framework. <em>TNNLS</em>,
<em>36</em>(3), 5244–5257. (<a
href="https://doi.org/10.1109/TNNLS.2024.3382199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A series of discrete time-variant matrix inequalities is generally regarded as one of the challenging problems in science and engineering fields. As a discrete time-variant problem, the existing solving schemes generally need the theoretical support under the continuous-time framework, and there is no independent solving scheme under the discrete-time framework. The theoretical deficiency of solving scheme greatly limits the theoretical research and practical application of discrete time-variant matrix inequalities. In this article, new discrete-time recurrent neural network (RNN) algorithms are proposed, analyzed, and investigated for solving different time-variant matrix inequalities under the discrete-time framework, including discrete time-variant matrix vector inequality (discrete time-variant MVI), discrete time-variant generalized matrix inequality (discrete time-variant GMI), discrete time-variant generalized-Sylvester matrix inequality (discrete time-variant GSMI), and discrete time-variant complicated-Sylvester matrix inequality (discrete time-variant CSMI), and all solving processes are based on the direct discretization thought. Specifically, first of all, four discrete time-variant matrix inequalities are presented as the target problems of these researches. Second, for solving such problems, we propose corresponding discrete-time recurrent neural network (RNN) (DT-RNN) algorithms (termed DT-RNN-MVI algorithm, DT-RNN-GMI algorithm, DT-RNN-GSMI algorithm, and DT-RNN-CSMI algorithm), which are different from the traditional DT-RNN design thought because second-order Taylor expansion is applied to derive the DT-RNN algorithms. This creative process avoids the intervention of continuous-time framework. Then, theoretical analyses are presented, which show the convergence and precision of the DT-RNN algorithms. Abundant numerical experiments are further carried out, which further confirm the excellent properties of the DT-RNN algorithms.},
  archive      = {J_TNNLS},
  author       = {Yang Shi and Chenling Ding and Shuai Li and Bin Li and Xiaobing Sun},
  doi          = {10.1109/TNNLS.2024.3382199},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5244-5257},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New RNN algorithms for different time-variant matrix inequalities solving under discrete-time framework},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining semantic correlations between mispredictions and
corrections for interactive semantic segmentation. <em>TNNLS</em>,
<em>36</em>(3), 5230–5243. (<a
href="https://doi.org/10.1109/TNNLS.2024.3379585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive semantic segmentation pursues high-quality segmentation results at the cost of a small number of user clicks. It is attracting more and more research attention for its convenience in labeling semantic pixel-level data. Existing interactive segmentation methods often pursue higher interaction efficiency by mining the latent information of user clicks or exploring efficient interaction manners. However, these works neglect to explicitly exploit the semantic correlations between user corrections and model mispredictions, thus suffering from two flaws. First, similar prediction errors frequently occur in actual use, causing users to repeatedly correct them. Second, the interaction difficulty of different semantic classes varies across images, but existing models use monotonic parameters for all images which lack semantic pertinence. Therefore, in this article, we explore the semantic correlations existing in corrections and mispredictions by proposing a simple yet effective online learning solution to the above problems, named correction-misprediction correlation mining (CM2). Specifically, we leverage the correction-misprediction similarities to design a confusion memory module (CMM) for automatic correction when similar prediction errors reappear. Furthermore, we measure the semantic interaction difficulty by counting the correction-misprediction pairs and design a challenge adaptive convolutional layer (CACL), which can adaptively switch different parameters according to interaction difficulties to better segment the challenging classes. Our method requires no extra training besides the online learning process and can effectively improve interaction efficiency. Our proposed CM2 achieves state-of-the-art results on three public semantic segmentation benchmarks.},
  archive      = {J_TNNLS},
  author       = {Yutong Gao and Congyan Lang and Fayao Liu and Chuan-Sheng Foo and Yuanzhouhan Cao and Lijuan Sun and Yunchao Wei},
  doi          = {10.1109/TNNLS.2024.3379585},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5230-5243},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mining semantic correlations between mispredictions and corrections for interactive semantic segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trainable delays in time delay neural networks for learning
delayed dynamics. <em>TNNLS</em>, <em>36</em>(3), 5219–5229. (<a
href="https://doi.org/10.1109/TNNLS.2024.3379020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the connection between time delay systems and time delay neural networks (TDNNs) is presented from a continuous-time perspective. TDNNs are utilized to learn the nonlinear dynamics of time delay systems from trajectory data. The concept of TDNN with trainable delay (TrTDNN) is established, and training algorithms are constructed for learning the time delays and the nonlinearities simultaneously. The proposed techniques are tested on learning the dynamics of autonomous systems from simulation data and on learning the delayed longitudinal dynamics of a connected automated vehicle (CAV) from real experimental data.},
  archive      = {J_TNNLS},
  author       = {Xunbi A. Ji and Gábor Orosz},
  doi          = {10.1109/TNNLS.2024.3379020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5219-5229},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Trainable delays in time delay neural networks for learning delayed dynamics},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-wise sampling convolutions for arbitrary-oriented
object detection in aerial images. <em>TNNLS</em>, <em>36</em>(3),
5204–5218. (<a
href="https://doi.org/10.1109/TNNLS.2024.3367331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary-oriented object detection (AOOD) has been widely applied to locate and classify objects with diverse orientations in remote sensing images. However, the inconsistent features for the localization and classification tasks in AOOD models may lead to ambiguity and low-quality object predictions, which constrains the detection performance. In this article, an AOOD method called task-wise sampling convolutions (TS-Conv) is proposed. TS-Conv adaptively samples task-wise features from respective sensitive regions and maps these features together in alignment to guide a dynamic label assignment for better predictions. Specifically, sampling positions of the localization convolution in TS-Conv are supervised by the oriented bounding box (OBB) prediction associated with spatial coordinates, while sampling positions and convolutional kernel of the classification convolution are designed to be adaptively adjusted according to different orientations for improving the orientation robustness of features. Furthermore, a dynamic task-consistent-aware label assignment (DTLA) strategy is developed to select optimal candidate positions and assign labels dynamically according to ranked task-aware scores obtained from TS-Conv. Extensive experiments on several public datasets covering multiple scenes, multimodal images, and multiple categories of objects demonstrate the effectiveness, scalability, and superior performance of the proposed TS-Conv.},
  archive      = {J_TNNLS},
  author       = {Zhanchao Huang and Wei Li and Xiang-Gen Xia and Hao Wang and Ran Tao},
  doi          = {10.1109/TNNLS.2024.3367331},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5204-5218},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Task-wise sampling convolutions for arbitrary-oriented object detection in aerial images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative upper-level policy imitation learning with
pareto-improvement for energy-efficient advanced machining systems.
<em>TNNLS</em>, <em>36</em>(3), 5190–5203. (<a
href="https://doi.org/10.1109/TNNLS.2024.3372641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential intelligence behind advanced machining systems (AMSs) offers positive contributions toward process improvement. Imitation learning (IL) offers an appealing approach to accessing this intelligence by observing demonstrations from skilled technologists. However, existing IL algorithms that implement single policy strategies have yet to consider realistic scenarios for complex AMS tasks, where the available demonstrations may have come from various experts. Moreover, most IL assumes that the expert’s policy is optimal, preventing the learning from fulfilling the previously ignored green missions. This article introduces a novel three-phase policy search algorithm based on IL, enabling the learning of heterogeneous expert policies while balancing energy preferences. The first phase equips the agent with machining basics through upper-level policy learning, generating an imitation policy distribution with various decision-making principles. The second phase enhances energy conservation capabilities by employing Pareto-improvement learning and fine-tuning the agent’s policies to a Pareto-policy manifold. The third phase produces outcomes and amplifies the efficacy of human feedback by utilizing ensemble policies. The experimental results indicate that the proposed method outperforms meta-heuristics, exhibiting superior solution quality and faster computation times compared to four diverse baseline methods, each with diverse samples.},
  archive      = {J_TNNLS},
  author       = {Qinge Xiao and Ben Niu and Ying Tan and Zhile Yang and Xingzheng Chen},
  doi          = {10.1109/TNNLS.2024.3372641},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5190-5203},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generative upper-level policy imitation learning with pareto-improvement for energy-efficient advanced machining systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Use of parallel explanatory models to enhance transparency
of neural network configurations for cell degradation detection.
<em>TNNLS</em>, <em>36</em>(3), 5177–5189. (<a
href="https://doi.org/10.1109/TNNLS.2024.3373101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a previous paper, we have shown that a recurrent neural network (RNN) can be used to detect cellular network radio signal degradations accurately. We unexpectedly found, though, that accuracy gains diminished as we added layers to the RNN. To investigate this, in this article, we build a parallel model to illuminate and understand the internal operation of neural networks (NNs), such as the RNN, which store their internal state to process sequential inputs. This model is widely applicable in that it can be used with any input domain where the inputs can be represented by a Gaussian mixture. By looking at RNN processing from a probability density function (pdf) perspective, we are able to show how each layer of the RNN transforms the input distributions to increase detection accuracy. At the same time we also discover a side effect acting to limit the improvement in accuracy. To demonstrate the fidelity of the model, we validate it against each stage of RNN processing and output predictions. As a result, we have been able to explain the reasons for RNN performance limits with useful insights for future designs for RNNs and similar types of NN.},
  archive      = {J_TNNLS},
  author       = {David Mulvey and Chuan Heng Foh and Muhammad Ali Imran and Rahim Tafazolli},
  doi          = {10.1109/TNNLS.2024.3373101},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5177-5189},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Use of parallel explanatory models to enhance transparency of neural network configurations for cell degradation detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MNTZNN for solving hybrid double-deck dynamic nonlinear
equation system applied to robot manipulator control. <em>TNNLS</em>,
<em>36</em>(3), 5166–5176. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with conventional dynamic nonlinear equation systems, a hybrid double-deck dynamic nonlinear equation system (H3DNES) not only has multiple layers describing more different tasks in practice, but also has a hybrid nonlinear structure of solution and its derivative describing their nonlinear constraints. Its characteristics lead to the ability to describe more complicated problems involving multiple constraints, and strong nonlinear and dynamic features, such as robot manipulator tracking control. Besides, noises are inevitable in practice and thus strong robustness of models solving H3DNES is also necessary. In this work, a multilayered noise-tolerant zeroing neural network (MNTZNN) model is proposed for solving H3DNES. MNTZNN model has strong robustness and it solves H3DNES successfully even when noises exist in both the two layers of H3DNES. In order to develop the MNTZNN model, a new zeroing neural network (ZNN) design formula is proposed. It not only enables equations with respect to solutions to become equations with respect to the second-order derivatives of solutions but also makes the corresponding model have strong robustness. The robustness of the MNTZNN model is proved when parameters in the model satisfy a loose constraint and the error bounds are programmable via setting appropriate parameter values. Finally, the MNTZNN model is applied to the tracking control of the six-link planar robot manipulator and PUMA560 robot manipulator with hybrid nonlinear constraints of joint angle and velocity.},
  archive      = {J_TNNLS},
  author       = {Xinhui Zhu and Jianwei Fan and Shuang Pan and Yanling Li and Jian Li and Mingliang Xu},
  doi          = {10.1109/TNNLS.2024.3371543},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5166-5176},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MNTZNN for solving hybrid double-deck dynamic nonlinear equation system applied to robot manipulator control},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive synaptic scaling in spiking networks for continual
learning and enhanced robustness. <em>TNNLS</em>, <em>36</em>(3),
5151–5165. (<a
href="https://doi.org/10.1109/TNNLS.2024.3373599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synaptic plasticity plays a critical role in the expression power of brain neural networks. Among diverse plasticity rules, synaptic scaling presents indispensable effects on homeostasis maintenance and synaptic strength regulation. In the current modeling of brain-inspired spiking neural networks (SNN), backpropagation through time is widely adopted because it can achieve high performance using a small number of time steps. Nevertheless, the synaptic scaling mechanism has not yet been well touched. In this work, we propose an experience-dependent adaptive synaptic scaling mechanism (AS-SNN) for spiking neural networks. The learning process has two stages: First, in the forward path, adaptive short-term potentiation or depression is triggered for each synapse according to afferent stimuli intensity accumulated by presynaptic historical neural activities. Second, in the backward path, long-term consolidation is executed through gradient signals regulated by the corresponding scaling factor. This mechanism shapes the pattern selectivity of synapses and the information transfer they mediate. We theoretically prove that the proposed adaptive synaptic scaling function follows a contraction map and finally converges to an expected fixed point, in accordance with state-of-the-art results in three tasks on perturbation resistance, continual learning, and graph learning. Specifically, for the perturbation resistance and continual learning tasks, our approach improves the accuracy on the N-MNIST benchmark over the baseline by 44% and 25%, respectively. An expected firing rate callback and sparse coding can be observed in graph learning. Extensive experiments on ablation study and cost evaluation evidence the effectiveness and efficiency of our nonparametric adaptive scaling method, which demonstrates the great potential of SNN in continual learning and robust learning.},
  archive      = {J_TNNLS},
  author       = {Mingkun Xu and Faqiang Liu and Yifan Hu and Hongyi Li and Yuanyuan Wei and Shuai Zhong and Jing Pei and Lei Deng},
  doi          = {10.1109/TNNLS.2024.3373599},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5151-5165},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive synaptic scaling in spiking networks for continual learning and enhanced robustness},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral super-resolution via deep low-rank tensor
representation. <em>TNNLS</em>, <em>36</em>(3), 5140–5150. (<a
href="https://doi.org/10.1109/TNNLS.2024.3359852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral super-resolution has attracted the attention of more researchers for obtaining hyperspectral images (HSIs) in a simpler and cheaper way. Although many convolutional neural network (CNN)-based approaches have yielded impressive results, most of them ignore the low-rank prior of HSIs resulting in huge computational and storage costs. In addition, the ability of CNN-based methods to capture the correlation of global information is limited by the receptive field. To surmount the problem, we design a novel low-rank tensor reconstruction network (LTRN) for spectral super-resolution. Specifically, we treat the features of HSIs as 3-D tensors with low-rank properties due to their spectral similarity and spatial sparsity. Then, we combine canonical-polyadic (CP) decomposition with neural networks to design an adaptive low-rank prior learning (ALPL) module that enables feature learning in a 1-D space. In this module, there are two core modules: the adaptive vector learning (AVL) module and the multidimensionwise multihead self-attention (MMSA) module. The AVL module is designed to compress an HSI into a 1-D space by using a vector to represent its information. The MMSA module is introduced to improve the ability to capture the long-range dependencies in the row, column, and spectral dimensions, respectively. Finally, our LTRN, mainly cascaded by several ALPL modules and feedforward networks (FFNs), achieves high-quality spectral super-resolution with fewer parameters. To test the effect of our method, we conduct experiments on two datasets: the CAVE dataset and the Harvard dataset. Experimental results show that our LTRN not only is as effective as state-of-the-art methods but also has fewer parameters. The code is available at https://github.com/renweidian/LTRN.},
  archive      = {J_TNNLS},
  author       = {Renwei Dian and Yuanye Liu and Shutao Li},
  doi          = {10.1109/TNNLS.2024.3359852},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5140-5150},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spectral super-resolution via deep low-rank tensor representation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CADM+: Confusion-based learning framework with drift
detection and adaptation for real-time safety assessment.
<em>TNNLS</em>, <em>36</em>(3), 5126–5139. (<a
href="https://doi.org/10.1109/TNNLS.2024.3369315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time safety assessment (RTSA) of dynamic systems holds substantial implications across diverse fields, including industrial and electronic applications. However, the complexity and rapid flow nature of data streams, coupled with the expensive label cost and pose significant challenges. To address these issues, a novel confusion-based learning framework, termed confusion-and-detection method plus (CADM+), is proposed in this article. When drift occurs, the model is updated with uncertain samples, which may cause confusion between existing and new concepts, resulting in performance differences. The cosine similarity is used to measure the degree of such conceptual confusion in the model. Furthermore, the change of standard deviation within a fixed-size cosine similarity window is introduced as an indicator for drift detection. Theoretical demonstrations show the asymptotic increase of cosine similarity. In addition, the approximate independence of the change in standard deviation with the number of trained samples is indicated. Finally, the extreme value theory (EVT) is applied to determine the threshold of judging drifts. Several experiments are conducted to verify its effectiveness. Experimental results prove that the proposed framework is more suitable for RTSA tasks compared with state-of-the-art algorithms. The source code is available at https://github.com/THUFDD/CADM-plus.},
  archive      = {J_TNNLS},
  author       = {Songqiao Hu and Zeyi Liu and Minyue Li and Xiao He},
  doi          = {10.1109/TNNLS.2024.3369315},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5126-5139},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CADM+: Confusion-based learning framework with drift detection and adaptation for real-time safety assessment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TCJA-SNN: Temporal-channel joint attention for spiking
neural networks. <em>TNNLS</em>, <em>36</em>(3), 5112–5125. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are attracting widespread interest due to their biological plausibility, energy efficiency, and powerful spatiotemporal information representation ability. Given the critical role of attention mechanisms in enhancing neural network performance, the integration of SNNs and attention mechanisms exhibits tremendous potential to deliver energy-efficient and high-performance computing paradigms. In this article, we present a novel temporal-channel joint attention mechanism for SNNs, referred to as TCJA-SNN. The proposed TCJA-SNN framework can effectively assess the significance of spike sequence from both spatial and temporal dimensions. More specifically, our essential technical contribution lies on: 1) we employ the squeeze operation to compress the spike stream into an average matrix. Then, we leverage two local attention mechanisms based on efficient 1-D convolutions to facilitate comprehensive feature extraction at the temporal and channel levels independently and 2) we introduce the cross-convolutional fusion (CCF) layer as a novel approach to model the interdependencies between the temporal and channel scopes. This layer effectively breaks the independence of these two dimensions and enables the interaction between features. Experimental results demonstrate that the proposed TCJA-SNN outperforms the state-of-the-art (SOTA) on all standard static and neuromorphic datasets, including Fashion-MNIST, CIFAR10, CIFAR100, CIFAR10-DVS, N-Caltech 101, and DVS128 Gesture. Furthermore, we effectively apply the TCJA-SNN framework to image generation tasks by leveraging a variation autoencoder. To the best of our knowledge, this study is the first instance where the SNN-attention mechanism has been employed for high-level classification and low-level generation tasks. Our implementation codes are available at https://github.com/ridgerchu/TCJA.},
  archive      = {J_TNNLS},
  author       = {Rui-Jie Zhu and Malu Zhang and Qihang Zhao and Haoyu Deng and Yule Duan and Liang-Jian Deng},
  doi          = {10.1109/TNNLS.2024.3377717},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5112-5125},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TCJA-SNN: Temporal-channel joint attention for spiking neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving text-based person retrieval by excavating
all-round information beyond color. <em>TNNLS</em>, <em>36</em>(3),
5097–5111. (<a
href="https://doi.org/10.1109/TNNLS.2024.3368217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person retrieval is the process of searching a massive visual resource library for images of a particular pedestrian, based on a textual query. Existing approaches often suffer from a problem of color (CLR) over-reliance, which can result in a suboptimal person retrieval performance by distracting the model from other important visual cues such as texture and structure information. To handle this problem, we propose a novel framework to Excavate All-round Information Beyond Color for the task of text-based person retrieval, which is therefore termed EAIBC. The EAIBC architecture includes four branches, namely an RGB branch, a grayscale (GRS) branch, a high-frequency (HFQ) branch, and a CLR branch. Furthermore, we introduce a mutual learning (ML) mechanism to facilitate communication and learning among the branches, enabling them to take full advantage of all-round information in an effective and balanced manner. We evaluate the proposed method on three benchmark datasets, including CUHK-PEDES, ICFG-PEDES, and RSTPReid. The experimental results demonstrate that EAIBC significantly outperforms existing methods and achieves state-of-the-art (SOTA) performance in supervised, weakly supervised, and cross-domain settings.},
  archive      = {J_TNNLS},
  author       = {Aichun Zhu and Zijie Wang and Jingyi Xue and Xili Wan and Jing Jin and Tian Wang and Hichem Snoussi},
  doi          = {10.1109/TNNLS.2024.3368217},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5097-5111},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving text-based person retrieval by excavating all-round information beyond color},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auxiliary tasks enhanced dual-affinity learning for weakly
supervised semantic segmentation. <em>TNNLS</em>, <em>36</em>(3),
5082–5096. (<a
href="https://doi.org/10.1109/TNNLS.2024.3373566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing weakly supervised semantic segmentation (WSSS) methods rely on class activation mapping (CAM) to extract coarse class-specific localization maps using image-level labels. Prior works have commonly used an off-line heuristic thresholding process that combines the CAM maps with off-the-shelf saliency maps produced by a general pretrained saliency model to produce more accurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly supervised auxiliary learning framework to explore the rich information from these saliency maps and the significant intertask correlation between saliency detection and semantic segmentation. In the proposed AuxSegNet+, saliency detection and multilabel image classification are used as auxiliary tasks to improve the primary task of semantic segmentation with only image-level ground-truth labels. We also propose a cross-task affinity learning mechanism to learn pixel-level affinities from the saliency and segmentation feature maps. In particular, we propose a cross-task dual-affinity learning module to learn both pairwise and unary affinities, which are used to enhance the task-specific features and predictions by aggregating both query-dependent and query-independent global context for both saliency detection and semantic segmentation. The learned cross-task pairwise affinity can also be used to refine and propagate CAM maps to provide better pseudo labels for both tasks. Iterative improvement of segmentation performance is enabled by cross-task affinity learning and pseudo-label updating. Extensive experiments demonstrate the effectiveness of the proposed approach with new state-of-the-art WSSS results on the challenging PASCAL VOC and MS COCO benchmarks.},
  archive      = {J_TNNLS},
  author       = {Lian Xu and Mohammed Bennamoun and Farid Boussaid and Wanli Ouyang and Ferdous Sohel and Dan Xu},
  doi          = {10.1109/TNNLS.2024.3373566},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5082-5096},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Auxiliary tasks enhanced dual-affinity learning for weakly supervised semantic segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An accelerated approach on adaptive gradient neural network
for solving time-dependent linear equations: A state-triggered
perspective. <em>TNNLS</em>, <em>36</em>(3), 5070–5081. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the acceleration performance, a hybrid state-triggered discretization (HSTD) is proposed for the adaptive gradient neural network (AGNN) for solving time-dependent linear equations (TDLEs). Unlike the existing approaches that use an activation function or a time-varying coefficient for acceleration, the proposed HSTD is uniquely designed from a control theory perspective. It comprises two essential components: adaptive sampling interval state-triggered discretization (ASISTD) and adaptive coefficient state-triggered discretization (ACSTD). The former addresses the gap in acceleration methods related to the variable sampling period, while the latter considers the underlying evolutionary dynamics of the Lyapunov function to determine coefficients greedily. Finally, compared with commonly used discretization methods, the acceleration performance and computational advantages of the proposed HSTD are substantiated by the numerical simulations and applications to robotics.},
  archive      = {J_TNNLS},
  author       = {Haoen Huang and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2024.3371008},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5070-5081},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An accelerated approach on adaptive gradient neural network for solving time-dependent linear equations: A state-triggered perspective},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent attention network with position perception for visual
question answering. <em>TNNLS</em>, <em>36</em>(3), 5059–5069. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For exploring the complex relative position relationships among multiobject with multiple position prepositions in the question, we propose a novel latent attention (LA) network for visual question answering (VQA), in which LA with position perception is extracted by a novel LA generation module (LAGM) and encoded along with absolute and relative position relations by our proposed position-aware module (PAM). The LAGM reconstructs original attention into LA by capturing the tendency of visual attention shifting according to the position prepositions in the question. The LA accurately captures the complex relative position features of multiple objects and helps the model locate the attention to the correct object or region. The PAM adopts latent state and relative position relations to enhance the capability of comprehending the multiobject correlations. In addition, we also propose a novel gated counting module (GCM) to strengthen the sensitivity of quantitative knowledge for effectively improving the performance of counting questions. Extensive experiments demonstrate that our proposed method achieves excellent performance on VQA and outperforms state-of-the-art methods on the widely used datasets VQA v2 and VQA v1.},
  archive      = {J_TNNLS},
  author       = {Jing Zhang and Xiaoqiang Liu and Zhe Wang},
  doi          = {10.1109/TNNLS.2024.3377636},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5059-5069},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Latent attention network with position perception for visual question answering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). T-HyperGNNs: Hypergraph neural networks via tensor
representations. <em>TNNLS</em>, <em>36</em>(3), 5044–5058. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph neural networks (HyperGNNs) are a family of deep neural networks designed to perform inference on hypergraphs. HyperGNNs follow either a spectral or a spatial approach, in which a convolution or message-passing operation is conducted based on a hypergraph algebraic descriptor. While many HyperGNNs have been proposed and achieved state-of-the-art performance on broad applications, there have been limited attempts at exploring high-dimensional hypergraph descriptors (tensors) and joint node interactions carried by hyperedges. In this article, we depart from hypergraph matrix representations and present a new tensor-HyperGNN (T-HyperGNN) framework with cross-node interactions (CNIs). The T-HyperGNN framework consists of T-spectral convolution, T-spatial convolution, and T-message-passing HyperGNNs (T-MPHN). The T-spectral convolution HyperGNN is defined under the t-product algebra that closely connects to the spectral space. To improve computational efficiency for large hypergraphs, we localize the T-spectral convolution approach to formulate the T-spatial convolution and further devise a novel tensor-message-passing algorithm for practical implementation by studying a compressed adjacency tensor representation. Compared to the state-of-the-art approaches, our T-HyperGNNs preserve intrinsic high-order network structures without any hypergraph reduction and model the joint effects of nodes through a CNI layer. These advantages of our T-HyperGNNs are demonstrated in a wide range of real-world hypergraph datasets. The implementation code is available at https://github.com/wangfuli/T-HyperGNNs.git.},
  archive      = {J_TNNLS},
  author       = {Fuli Wang and Karelia Pena-Pena and Wei Qian and Gonzalo R. Arce},
  doi          = {10.1109/TNNLS.2024.3371382},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5044-5058},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {T-HyperGNNs: Hypergraph neural networks via tensor representations},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural preassigned performance control for state-constrained
nonlinear systems subject to disturbances. <em>TNNLS</em>,
<em>36</em>(3), 5032–5043. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the finite-time neural predefined performance control (PPC) issue for state-constrained nonlinear systems (NSs) with exogenous disturbances. By integrating the predefined-time performance function (PTPF) and the conventional barrier Lyapunov function (BLF), a new set of time-varying BLFs is designed to constrain the error variables. This establishes conditions for satisfying full-state constraints while ensuring that the tracking error meets the predefined performance indicators (PPIs) within a predefined time. Additionally, the incorporation of the nonlinear disturbance observer technique (NDOT) in the control design significantly enhances the ability of the system to reject disturbances and improves overall robustness. Leveraging recursive design based on dynamic surface control (DSC), a finite-time neural adaptive PPC strategy is devised to ensure that the closed-loop system is semi-globally practically finite-time stable (SPFS) and achieves the desired PPIs. Finally, the simulation results of two practical examples validate the efficacy and viability of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Wei Liu and Jianhang Zhao and Huanyu Zhao and Qian Ma and Shengyuan Xu and Ju H. Park},
  doi          = {10.1109/TNNLS.2024.3377462},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5032-5043},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural preassigned performance control for state-constrained nonlinear systems subject to disturbances},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised dual transformer learning for 3-d textured
surface segmentation. <em>TNNLS</em>, <em>36</em>(3), 5020–5031. (<a
href="https://doi.org/10.1109/TNNLS.2024.3365515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of the 3-D texture is indispensable for various tasks, such as retrieval, segmentation, classification, and inspection of sculptures, knit fabrics, and biological tissues. A 3-D texture represents a locally repeated surface variation (SV) that is independent of the overall shape of the surface and can be determined using the local neighborhood and its characteristics. Existing methods mostly employ computer vision techniques that analyze a 3-D mesh globally, derive features, and then utilize them for classification or retrieval tasks. While several traditional and learning-based methods have been proposed in the literature, only a few have addressed 3-D texture analysis, and none have considered unsupervised schemes so far. This article proposes an original framework for the unsupervised segmentation of 3-D texture on the mesh manifold. The problem is approached as a binary surface segmentation task, where the mesh surface is partitioned into textured and nontextured regions without prior annotation. The proposed method comprises a mutual transformer-based system consisting of a label generator (LG) and a label cleaner (LC). Both models take geometric image representations of the surface mesh facets and label them as texture or nontexture using an iterative mutual learning scheme. Extensive experiments on three publicly available datasets with diverse texture patterns demonstrate that the proposed framework outperforms standard and state-of-the-art unsupervised techniques and performs reasonably well compared to supervised methods.},
  archive      = {J_TNNLS},
  author       = {Iyyakutti Iyappan Ganapathi and Fayaz Ali Dharejo and Sajid Javed and Syed Sadaf Ali and Naoufel Werghi},
  doi          = {10.1109/TNNLS.2024.3365515},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5020-5031},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised dual transformer learning for 3-D textured surface segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous domain adaptation with generalized similarity
and dissimilarity regularization. <em>TNNLS</em>, <em>36</em>(3),
5006–5019. (<a
href="https://doi.org/10.1109/TNNLS.2024.3372004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation (HDA) aims to address the transfer learning problems where the source domain and target domain are represented by heterogeneous features. The existing HDA methods based on matrix factorization have been proven to learn transferable features effectively. However, these methods only preserve the original neighbor structure of samples in each domain and do not use the label information to explore the similarity and separability between samples. This would not eliminate the cross-domain bias of samples and may mix cross-domain samples of different classes in the common subspace, misleading the discriminative feature learning of target samples. To tackle the aforementioned problems, we propose a novel matrix factorization-based HDA method called HDA with generalized similarity and dissimilarity regularization (HGSDR). Specifically, we propose a similarity regularizer by establishing the cross-domain Laplacian graph with label information to explore the similarity between cross-domain samples from the identical class. And we propose a dissimilarity regularizer based on the inner product strategy to expand the separability of cross-domain labeled samples from different classes. For unlabeled target samples, we keep their neighbor relationship to preserve the similarity and separability between them in the original space. Hence, the generalized similarity and dissimilarity regularization is built by integrating the above regularizers to facilitate cross-domain samples to form discriminative class distributions. HGSDR can more efficiently match the distributions of the two domains both from the global and sample viewpoints, thereby learning discriminative features for target samples. Extensive experiments on the benchmark datasets demonstrate the superiority of the proposed method against several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yuqing Chen and Zhencai Shen and Daoliang Li and Ping Zhong and Yingyi Chen},
  doi          = {10.1109/TNNLS.2024.3372004},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {5006-5019},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous domain adaptation with generalized similarity and dissimilarity regularization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantized magnetic domain wall synapse for efficient deep
neural networks. <em>TNNLS</em>, <em>36</em>(3), 4996–5005. (<a
href="https://doi.org/10.1109/TNNLS.2024.3369969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantization of synaptic weights using emerging nonvolatile memory (NVM) devices has emerged as a promising solution to implement computationally efficient neural networks on resource constrained hardware. However, the practical implementation of such synaptic weights is hampered by the imperfect memory characteristics, specifically the availability of limited number of quantized states and the presence of large intrinsic device variation and stochasticity involved in writing the synaptic states. This article presents ON-chip training and inference of a neural network using quantized magnetic domain wall (DW)-based synaptic array and CMOS peripheral circuits. A rigorous model of the magnetic DW device considering stochasticity and process variations has been utilized for the synapse. To achieve stable quantized weights, DW pinning has been achieved by means of physical constrictions. Finally, VGG8 architecture for CIFAR-10 image classification has been simulated by using the extracted synaptic device characteristics. The performance in terms of accuracy, energy, latency, and area consumption has been evaluated while considering the process variations and nonidealities in the DW device as well as the peripheral circuits. The proposed quantized neural network (QNN) architecture achieves efficient ON-chip learning with 92.4% and 90.4% training and inference accuracy, respectively. In comparison to pure CMOS-based design, it demonstrates an overall improvement in area, energy, and latency by $13.8\times $ , $9.6\times $ , and $3.5\times $ , respectively.},
  archive      = {J_TNNLS},
  author       = {Seema Dhull and Walid Al Misba and Arshid Nisar and Jayasimha Atulasimha and Brajesh Kumar Kaushik},
  doi          = {10.1109/TNNLS.2024.3369969},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4996-5005},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quantized magnetic domain wall synapse for efficient deep neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smooth-guided implicit data augmentation for domain
generalization. <em>TNNLS</em>, <em>36</em>(3), 4984–4995. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training process of a domain generalization (DG) model involves utilizing one or more interrelated source domains to attain optimal performance on an unseen target domain. Existing DG methods often use auxiliary networks or require high computational costs to improve the model’s generalization ability by incorporating a diverse set of source domains. In contrast, this work proposes a method called Smooth-Guided Implicit Data Augmentation (SGIDA) that operates in the feature space to capture the diversity of source domains. To amplify the model’s generalization capacity, a distance metric learning (DML) loss function is incorporated. Additionally, rather than depending on deep features, the suggested approach employs logits produced from cross entropy (CE) losses with infinite augmentations. A theoretical analysis shows that logits are effective in estimating distances defined on original features, and the proposed approach is thoroughly analyzed to provide a better understanding of why logits are beneficial for DG. Moreover, to increase the diversity of the source domain, a sampling-based method called smooth is introduced to obtain semantic directions from interclass relations. The effectiveness of the proposed approach is demonstrated through extensive experiments on widely used DG, object detection, and remote sensing datasets, where it achieves significant improvements over existing state-of-the-art methods across various backbone networks.},
  archive      = {J_TNNLS},
  author       = {Mengzhu Wang and Junze Liu and Ge Luo and Shanshan Wang and Wei Wang and Long Lan and Ye Wang and Feiping Nie},
  doi          = {10.1109/TNNLS.2024.3377439},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4984-4995},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Smooth-guided implicit data augmentation for domain generalization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot relation extraction with automatically generated
prompts. <em>TNNLS</em>, <em>36</em>(3), 4971–4983. (<a
href="https://doi.org/10.1109/TNNLS.2024.3365858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction (RE) tends to struggle when the supervised training data is few and difficult to be collected. In this article, we elicit relational and factual knowledge from large pretrained language models (PLMs) for few-shot RE (FSRE) with prompting techniques. Concretely, we automatically generate a diverse set of natural language templates and modulate PLM’s behavior through these prompts for FSRE. To mitigate the template bias which leads to unstableness of few-shot learning, we propose a simple yet effective template regularization network (TRN) to prevent deep networks from over-fitting uncertain templates and thus stabilize the FSRE models. TRN alleviates the template bias with three mechanisms: 1) an attention mechanism over mini-batch to weight each template; 2) a ranking regularization mechanism to regularize the attention weights and constrain the importance of uncertain templates; and 3) a template calibration module with two calibrating techniques to modify the uncertain templates in the lowest-ranked group. Experimental results on two benchmark datasets (i.e., FewRel and NYT) show that our model has robust superiority over strong competitors. For reproducibility, we will release our code and data upon the publication of this article.},
  archive      = {J_TNNLS},
  author       = {Xiaoyan Zhao and Min Yang and Qiang Qu and Ruifeng Xu},
  doi          = {10.1109/TNNLS.2024.3365858},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4971-4983},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Few-shot relation extraction with automatically generated prompts},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coexistence of cyclic sequential pattern recognition and
associative memory in neural networks by attractor mechanisms.
<em>TNNLS</em>, <em>36</em>(3), 4959–4970. (<a
href="https://doi.org/10.1109/TNNLS.2024.3368092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are developed to model the behavior of the brain. One crucial question in this field pertains to when and how a neural network can memorize a given set of patterns. There are two mechanisms to store information: associative memory and sequential pattern recognition. In the case of associative memory, the neural network operates with dynamical attractors that are point attractors, each corresponding to one of the patterns to be stored within the network. In contrast, sequential pattern recognition involves the network memorizing a set of patterns and subsequently retrieving them in a specific order over time. From a dynamical perspective, this corresponds to the presence of a continuous attractor or a cyclic attractor composed of the sequence of patterns stored within the network in a given order. Evidence suggests that the brain is capable of simultaneously performing both associative memory and sequential pattern recognition. Therefore, these types of attractors coexist within the neural network, signifying that some patterns are stored as point attractors, while others are stored as continuous or cyclic attractors. This article investigates the coexistence of cyclic attractors and continuous or point attractors in certain nonlinear neural networks, enabling the simultaneous emergence of various memory mechanisms. By selectively grouping neurons, conditions are established for the existence of cyclic attractors, continuous attractors, and point attractors, respectively. Furthermore, each attractor is explicitly represented, and a competitive dynamic emerges among these coexisting attractors, primarily regulated by adjustments to external inputs.},
  archive      = {J_TNNLS},
  author       = {Jingyang Huo and Jiali Yu and Min Wang and Zhang Yi and Jinsong Leng and Yong Liao},
  doi          = {10.1109/TNNLS.2024.3368092},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4959-4970},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coexistence of cyclic sequential pattern recognition and associative memory in neural networks by attractor mechanisms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast multiview anchor-graph clustering. <em>TNNLS</em>,
<em>36</em>(3), 4947–4958. (<a
href="https://doi.org/10.1109/TNNLS.2024.3359690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its high computational complexity, graph-based methods have limited applicability in large-scale multiview clustering tasks. To address this issue, many accelerated algorithms, especially anchor graph-based methods and indicator learning-based methods, have been developed and made a great success. Nevertheless, since the restrictions of the optimization strategy, these accelerated methods still need to approximate the discrete graph-cutting problem to a continuous spectral embedding problem and utilize different discretization strategies to obtain discrete sample categories. To avoid the loss of effectiveness and efficiency caused by the approximation and discretization, we establish a discrete fast multiview anchor graph clustering (FMAGC) model that first constructs an anchor graph of each view and then generates a discrete cluster indicator matrix by solving the discrete multiview graph-cutting problem directly. Since the gradient descent-based method makes it hard to solve this discrete model, we propose a fast coordinate descent-based optimization strategy with linear complexity to solve it without approximating it as a continuous one. Extensive experiments on widely used normal and large-scale multiview datasets show that FMAGC can improve clustering effectiveness and efficiency compared to other state-of-the-art baselines.},
  archive      = {J_TNNLS},
  author       = {Ben Yang and Xuetao Zhang and Jinghan Wu and Feiping Nie and Zhiping Lin and Fei Wang and Badong Chen},
  doi          = {10.1109/TNNLS.2024.3359690},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4947-4958},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast multiview anchor-graph clustering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiking neural network for ultralow-latency and
high-accurate object detection. <em>TNNLS</em>, <em>36</em>(3),
4934–4946. (<a
href="https://doi.org/10.1109/TNNLS.2024.3372613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) have attracted significant attention for their energy-efficient and brain-inspired event-driven properties. Recent advancements, notably Spiking-YOLO, have enabled SNNs to undertake advanced object detection tasks. Nevertheless, these methods often suffer from increased latency and diminished detection accuracy, rendering them less suitable for latency-sensitive mobile platforms. Additionally, the conversion of artificial neural networks (ANNs) to SNNs frequently compromises the integrity of the ANNs’ structure, resulting in poor feature representation and heightened conversion errors. To address the issues of high latency and low detection accuracy, we introduce two solutions: timestep compression and spike-time-dependent integrated (STDI) coding. Timestep compression effectively reduces the number of timesteps required in the ANN-to-SNN conversion by condensing information. The STDI coding employs a time-varying threshold to augment information capacity. Furthermore, we have developed an SNN-based spatial pyramid pooling (SPP) structure, optimized to preserve the network’s structural efficacy during conversion. Utilizing these approaches, we present the ultralow latency and highly accurate object detection model, SUHD. SUHD exhibits exceptional performance on challenging datasets like PASCAL VOC and MS COCO, achieving a remarkable reduction of approximately 750 times in timesteps and a 30% enhancement in mean average precision (mAP) compared to Spiking-YOLO on MS COCO. To the best of our knowledge, SUHD is currently the deepest spike-based object detection model, achieving ultralow timesteps for lossless conversion.},
  archive      = {J_TNNLS},
  author       = {Jinye Qu and Zeyu Gao and Tielin Zhang and Yanfeng Lu and Huajin Tang and Hong Qiao},
  doi          = {10.1109/TNNLS.2024.3372613},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4934-4946},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spiking neural network for ultralow-latency and high-accurate object detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Set-membership state estimation for multirate nonlinear
complex networks under FlexRay protocols: A neural-network-based
approach. <em>TNNLS</em>, <em>36</em>(3), 4922–4933. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the set-membership state estimation problem is investigated for a class of nonlinear complex networks under the FlexRay protocols (FRPs). In order to address practical engineering requirements, the multirate sampling is taken into account which allows for different sampling periods of the system state and the measurement. On the other hand, the FRP is deployed in the communication network from sensors to estimators in order to alleviate the communication burden. The underlying nonlinearity studied in this article is of a general nature, and an approach based on neural networks is employed to handle the nonlinearity. By utilizing the convex optimization technique, sufficient conditions are established in order to restrain the estimation errors within certain ellipsoidal constraints. Then, the estimator gains and the tuning scalars of the neural network are derived by solving several optimization problems. Finally, a practical simulation is conducted to verify the validity of the developed set-membership estimation scheme.},
  archive      = {J_TNNLS},
  author       = {Yuxuan Shen and Zidong Wang and Hongli Dong and Hongjian Liu and Yun Chen},
  doi          = {10.1109/TNNLS.2024.3377537},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4922-4933},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Set-membership state estimation for multirate nonlinear complex networks under FlexRay protocols: A neural-network-based approach},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DER-GCN: Dialog and event relation-aware graph convolutional
neural network for multimodal dialog emotion recognition.
<em>TNNLS</em>, <em>36</em>(3), 4908–4921. (<a
href="https://doi.org/10.1109/TNNLS.2024.3367940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of deep learning (DL), the task of multimodal dialog emotion recognition (MDER) has recently received extensive research attention, which is also an essential branch of DL. The MDER aims to identify the emotional information contained in different modalities, e.g., text, video, and audio, and in different dialog scenes. However, the existing research has focused on modeling contextual semantic information and dialog relations between speakers while ignoring the impact of event relations on emotion. To tackle the above issues, we propose a novel dialog and event relation-aware graph convolutional neural network (DER-GCN) for multimodal emotion recognition method. It models dialog relations between speakers and captures latent event relations information. Specifically, we construct a weighted multirelationship graph to simultaneously capture the dependencies between speakers and event relations in a dialog. Moreover, we also introduce a self-supervised masked graph autoencoder (SMGAE) to improve the fusion representation ability of features and structures. Next, we design a new multiple information Transformer (MIT) to capture the correlation between different relations, which can provide a better fuse of the multivariate information between relations. Finally, we propose a loss optimization strategy based on contrastive learning to enhance the representation learning ability of minority class features. We conduct extensive experiments on the benchmark datasets, Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Multimodal EmotionLines Dataset (MELD), which verify the effectiveness of the DER-GCN model. The results demonstrate that our model significantly improves both the average accuracy and the $F1$ value of emotion recognition. Our code is publicly available at https://github.com/yuntaoshou/DER-GCN.},
  archive      = {J_TNNLS},
  author       = {Wei Ai and Yuntao Shou and Tao Meng and Keqin Li},
  doi          = {10.1109/TNNLS.2024.3367940},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4908-4921},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DER-GCN: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MUSIC: Accelerated convergence for distributed optimization
with inexact and exact methods. <em>TNNLS</em>, <em>36</em>(3),
4893–4907. (<a
href="https://doi.org/10.1109/TNNLS.2024.3376421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-type distributed optimization methods have blossomed into one of the most important tools for solving a minimization learning task over a networked agent system. However, only one gradient update per iteration makes it difficult to achieve a substantive acceleration of convergence. In this article, we propose an accelerated framework named multiupdates single-combination (MUSIC) allowing each agent to perform multiple local updates and a single combination in each iteration. More importantly, we equip inexact and exact distributed optimization methods into this framework, thereby developing two new algorithms that exhibit accelerated linear convergence and high communication efficiency. Our rigorous convergence analysis reveals the sources of steady-state errors arising from inexact policies and offers effective solutions. Numerical results based on synthetic and real datasets demonstrate both our theoretical motivations and analysis, as well as performance advantages.},
  archive      = {J_TNNLS},
  author       = {Mou Wu and Haibin Liao and Zhengtao Ding and Yonggang Xiao},
  doi          = {10.1109/TNNLS.2024.3376421},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4893-4907},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MUSIC: Accelerated convergence for distributed optimization with inexact and exact methods},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-free multiview k-means clustering with coordinate
descent method. <em>TNNLS</em>, <em>36</em>(3), 4879–4892. (<a
href="https://doi.org/10.1109/TNNLS.2024.3373532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, more and more real-world datasets have been composed of heterogeneous but related features from diverse views. Multiview clustering provides a promising attempt at a solution for partitioning such data according to heterogeneous information. However, most existing methods suffer from hyper-parameter tuning trouble and high computational cost. Besides, there is still an opportunity for improvement in clustering performance. To this end, a novel multiview framework, called parameter-free multiview $k$ -means clustering with coordinate descent method (PFMVKM), is presented to address the above problems. Specifically, PFMVKM is completely parameter-free and learns the weights via a self-weighted scheme, which can avoid the intractable process of hyper-parameters tuning. Moreover, our model is capable of directly calculating the cluster indicator matrix, with no need to learn the cluster centroid matrix and the indicator matrix simultaneously as previous multiview methods have to do. What’s more, we propose an efficient optimization algorithm utilizing the idea of coordinate descent, which can not only reduce the computational complexity but also improve the clustering performance. Extensive experiments on various types of real datasets illustrate that the proposed method outperforms existing state-of-the-art competitors and conforms well with the actual situation.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Han Liu and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2024.3373532},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4879-4892},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter-free multiview K-means clustering with coordinate descent method},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On model of recurrent neural network on a time scale:
Exponential convergence and stability research. <em>TNNLS</em>,
<em>36</em>(3), 4864–4878. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of the results on modeling recurrent neural networks (RNNs) are obtained using delayed differential equations, which imply continuous time representation. On the other hand, these models must be discrete in time, given their practical implementation in computer systems, requiring their versatile utilization across arbitrary time scales. Hence, the goal of this research is to model and investigate the architecture design of a delayed RNN using delayed differential equations on a time scale. Internal memory can be utilized to describe the calculation of the future states using discrete and distributed delays, which is a representation of the deep learning architecture for artificial RNNs. We focus on qualitative behavior and stability study of the system. Special attention is paid to taking into account the effect of the time-scale parameters on neural network dynamics. Here, we delve into the exploration of exponential stability in RNN models on a time scale that incorporates multiple discrete and distributed delays. Two approaches for constructing exponential estimates, including the Hilger and the usual exponential functions, are considered and compared. The Lyapunov–Krasovskii (L–K) functional method is employed to study stability on a time scale in both cases. The established stability criteria, resulting in an exponential-like estimate, utilizes a tuple of positive definite matrices, decay rate, and graininess of the time scale. The models of RNNs for the two-neuron network with four discrete and distributed delays, as well as the ring lattice delayed network of seven identical neurons, are numerically investigated. The results indicate how the time scale (graininess) and model characteristics (weights) influence the qualitative behavior, leading to a transition from stable focus to quasiperiodic limit cycles.},
  archive      = {J_TNNLS},
  author       = {Vasyl Martsenyuk and Marcin Bernas and Aleksandra Klos-Witkowska},
  doi          = {10.1109/TNNLS.2024.3377446},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4864-4878},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On model of recurrent neural network on a time scale: Exponential convergence and stability research},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-MolNet: A cross-domain benchmark for few examples drug
discovery. <em>TNNLS</em>, <em>36</em>(3), 4849–4863. (<a
href="https://doi.org/10.1109/TNNLS.2024.3359657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the pharmacological activity, toxicity, and pharmacokinetic properties of molecules is a central task in drug discovery. Existing machine learning methods are transferred from one resource rich molecular property to another data scarce property in the same scaffold dataset. However, existing models may produce fragile and highly uncertain predictions for new scaffold molecules. And these models were tested on different benchmarks, which seriously affected the quality of their evaluation results. In this article, we introduce Meta-MolNet, a collection of data benchmark and algorithms, which is a standard benchmark platform for measuring model generalization and uncertainty quantification capabilities. Meta-MolNet manages a wide range of molecular datasets with high ratio of molecules/scaffolds, which often leads to more difficult data shift and generalization problems. Furthermore, we propose a graph attention network based on cross-domain meta-learning, Meta-GAT, which uses bilevel optimization to learn meta-knowledge from the scaffold family molecular dataset in the source domain. Meta-GAT benefits from meta-knowledge that reduces the requirement of sample complexity to enable reliable predictions of new scaffold molecules in the target domain through internal iteration of a few examples. We evaluate existing methods as baselines for the community, and the Meta-MolNet benchmark demonstrates the effectiveness of measuring the proposed algorithm in domain generalization and uncertainty quantification. Extensive experiments demonstrate that the Meta-GAT model has state-of-the-art domain generalization performance and robustly estimates uncertainty under few examples constraints. By publishing AI-ready data, evaluation frameworks, and baseline results, we hope to see the Meta-MolNet suite become a comprehensive resource for the AI-assisted drug discovery community. Meta-MolNet is freely accessible at https://github.com/lol88/Meta-MolNet.},
  archive      = {J_TNNLS},
  author       = {Qiujie Lv and Guanxing Chen and Ziduo Yang and Weihe Zhong and Calvin Yu-Chian Chen},
  doi          = {10.1109/TNNLS.2024.3359657},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4849-4863},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta-MolNet: A cross-domain benchmark for few examples drug discovery},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MEOL: A maximum-entropy framework for options learning.
<em>TNNLS</em>, <em>36</em>(3), 4834–4848. (<a
href="https://doi.org/10.1109/TNNLS.2024.3376538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Options, the temporally extended courses of actions that can be taken at varying time scale, have provided a concrete, key framework for learning levels of temporal abstraction in hierarchical tasks. While methods of learning options end-to-end is well researched, how to explore good options and actions simultaneously is still challenging. We address this issue by maximizing reward augmented with entropies of both option and action selection policy in options learning. To this end, we reveal our novel optimization objective by reformulating options learning from perspective of probabilistic inference and propose a soft options iteration method to guarantee convergence to the optimum. In implementation, we propose an off-policy algorithm called the maximum-entropy options critic (MEOC) and evaluate it on series of continuous control benchmarks. Comparative results demonstrate that our method outperforms baselines in efficiency and final result on most benchmarks, and the performance exhibits superiority and robustness especially on complex tasks. Ablated studies further explain that entropy maximization on hierarchical exploration promotes learning performance through efficient options specialization and multimodality in action level.},
  archive      = {J_TNNLS},
  author       = {Pin Zhang and Wenhan Dong and Ming Cai and Shengde Jia and Zi-Peng Wang},
  doi          = {10.1109/TNNLS.2024.3376538},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4834-4848},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MEOL: A maximum-entropy framework for options learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MH6D: Multi-hypothesis consistency learning for
category-level 6-d object pose estimation. <em>TNNLS</em>,
<em>36</em>(3), 4820–4833. (<a
href="https://doi.org/10.1109/TNNLS.2024.3360712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Six-degree-of-freedom (6DoF) object pose estimation is a crucial task for virtual reality and accurate robotic manipulation. Category-level 6DoF pose estimation has recently become popular as it improves generalization to a complete category of objects. However, current methods focus on data-driven differential learning, which makes them highly dependent on the quality of the real-world labeled data and limits their ability to generalize to unseen objects. To address this problem, we propose multi-hypothesis (MH) consistency learning (MH6D) for category-level 6-D object pose estimation without using real-world training data. MH6D uses a parallel consistency learning structure, alleviating the uncertainty problem of single-shot feature extraction and promoting self-adaptation of domain to reduce the synthetic-to-real domain gap. Specifically, three randomly sampled pose transformations are first performed in parallel on the input point cloud. An attention-guided category-level 6-D pose estimation network with channel attention (CA) and global feature cross-attention (GFCA) modules is then proposed to estimate the three hypothesized 6-D object poses by extracting and fusing the global and local features effectively. Finally, we propose a novel loss function that considers both the process and the final result information allowing MH6D to perform robust consistency learning. We conduct experiments under two different training data settings (i.e., only synthetic data and synthetic and real-world data) to verify the generalization ability of MH6D. Extensive experiments on benchmark datasets demonstrate that MH6D achieves state-of-the-art (SOTA) performance, outperforming most data-driven methods even without using any real-world data. The code is available at https://github.com/CNJianLiu/MH6D.},
  archive      = {J_TNNLS},
  author       = {Jian Liu and Wei Sun and Chongpei Liu and Hui Yang and Xing Zhang and Ajmal Mian},
  doi          = {10.1109/TNNLS.2024.3360712},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4820-4833},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MH6D: Multi-hypothesis consistency learning for category-level 6-D object pose estimation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-scaler: A meta-learning framework for the selection of
scaling techniques. <em>TNNLS</em>, <em>36</em>(3), 4805–4819. (<a
href="https://doi.org/10.1109/TNNLS.2024.3366615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataset scaling, a.k.a. normalization, is an essential preprocessing step in a machine learning (ML) pipeline. It aims to adjust the scale of attributes in a way that they all vary within the same range. This transformation is known to improve the performance of classification models. Still, there are several scaling techniques (STs) to choose from, and no ST is guaranteed to be the best for a dataset regardless of the classifier chosen. It is thus a problem- and classifier-dependent decision. Furthermore, there can be a huge difference in performance when selecting the wrong technique; hence, it should not be neglected. That said, the trial-and-error process of finding the most suitable technique for a particular dataset can be unfeasible. As an alternative, we propose the Meta-scaler, which uses meta-learning (MtL) to build meta-models to automatically select the best ST for a given dataset and classification algorithm. The meta-models learn to represent the relationship between meta-features extracted from the datasets and the performance of specific classification algorithms on these datasets when scaled with different techniques. Our experiments using 12 base classifiers, 300 datasets, and five STs demonstrate the feasibility and effectiveness of the approach. When using the ST selected by the Meta-scaler for each dataset, 10 of 12 base models tested achieved statistically significantly better classification performance than any fixed choice of a single ST. The Meta-scaler also outperforms state-of-the-art MtL approaches for ST selection. The source code, data, and results from the experiments in this article are available at a GitHub repository (https://github.com/amorimlb/meta_scaler).},
  archive      = {J_TNNLS},
  author       = {Lucas B. V. de Amorim and George D. C. Cavalcanti and Rafael M. O. Cruz},
  doi          = {10.1109/TNNLS.2024.3366615},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4805-4819},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta-scaler: A meta-learning framework for the selection of scaling techniques},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite-time boundedness of impulsive delayed
reaction–diffusion stochastic neural networks. <em>TNNLS</em>,
<em>36</em>(3), 4794–4804. (<a
href="https://doi.org/10.1109/TNNLS.2024.3360711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the impulsive delayed reaction–diffusion stochastic neural networks (IDRDSNNs) with hybrid impulses, the finite-time boundedness (FTB) and finite-time contractive boundedness (FTCB) are investigated in this article. First, a novel delay integral inequality is presented. By integrating this inequality with the comparison principle, some sufficient conditions that ensure the FTB and FTCB of IDRDSNNs are obtained. This study demonstrates that the FTB of neural networks with hybrid impulses can be maintained, even in the presence of impulsive perturbations. And for a system that is not FTB due to impulsive perturbations, achieving FTB is possible through the implementation of appropriate impulsive control and optimization of the average impulsive intervals. In addition, to validate the practicality of our results, three illustrative examples are provided. In the end, these theoretical findings are successfully applied to image encryption.},
  archive      = {J_TNNLS},
  author       = {Qi Yao and Tengda Wei and Ping Lin and Linshan Wang},
  doi          = {10.1109/TNNLS.2024.3360711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4794-4804},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time boundedness of impulsive delayed Reaction–Diffusion stochastic neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for solving vehicle routing
problems with backhauls. <em>TNNLS</em>, <em>36</em>(3), 4779–4793. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem with backhauls (VRPBs) is a challenging problem commonly studied in computer science and operations research. Featured by linehaul (or delivery) and backhaul (or pickup) customers, the VRPB has broad applications in real-world logistics. In this article, we propose a neural heuristic based on deep reinforcement learning (DRL) to solve the traditional and improved VRPB variants, with an encoder–decoder structured policy network trained to sequentially construct the routes for vehicles. Specifically, we first describe the VRPB based on a graph and cast the solution construction as a Markov decision process (MDP). Then, to identify the relationship among the nodes (i.e., linehaul and backhaul customers, and the depot), we design a two-stage attention-based encoder, including a self-attention and a heterogeneous attention for each stage, which could yield more informative representations of the nodes so as to deliver high-quality solutions. The evaluation on the two VRPB variants reveals that, our neural heuristic performs favorably against both the conventional and neural heuristic baselines on randomly generated instances and benchmark instances. Moreover, the trained policy network exhibits a desirable capability of generalization to various problem sizes and distributions.},
  archive      = {J_TNNLS},
  author       = {Conghui Wang and Zhiguang Cao and Yaoxin Wu and Long Teng and Guohua Wu},
  doi          = {10.1109/TNNLS.2024.3371781},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4779-4793},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning for solving vehicle routing problems with backhauls},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning ordinal–hierarchical constraints for deep learning
classifiers. <em>TNNLS</em>, <em>36</em>(3), 4765–4778. (<a
href="https://doi.org/10.1109/TNNLS.2024.3360641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world classification problems may disclose different hierarchical levels where the categories are displayed in an ordinal structure. However, no specific deep learning (DL) models simultaneously learn hierarchical and ordinal constraints while improving generalization performance. To fill this gap, we propose the introduction of two novel ordinal–hierarchical DL methodologies, namely, the hierarchical cumulative link model (HCLM) and hierarchical–ordinal binary decomposition (HOBD), which are able to model the ordinal structure within different hierarchical levels of the labels. In particular, we decompose the hierarchical–ordinal problem into local and global graph paths that may encode an ordinal constraint for each hierarchical level. Thus, we frame this problem as simultaneously minimizing global and local losses. Furthermore, the ordinal constraints are set by two approaches [ordinal binary decomposition (OBD) and cumulative link model (CLM)] within each global and local function. The effectiveness of the proposed approach is measured on four real-use case datasets concerning industrial, biomedical, computer vision, and financial domains. The extracted results demonstrate a statistically significant improvement to state-of-the-art nominal, ordinal, and hierarchical approaches.},
  archive      = {J_TNNLS},
  author       = {Riccardo Rosati and Luca Romeo and Víctor Manuel Vargas and Pedro Antonio Gutiérrez and Emanuele Frontoni and César Hervás-Martínez},
  doi          = {10.1109/TNNLS.2024.3360641},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4765-4778},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning Ordinal–Hierarchical constraints for deep learning classifiers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generative shape compositional framework to synthesize
populations of virtual chimeras. <em>TNNLS</em>, <em>36</em>(3),
4750–4764. (<a
href="https://doi.org/10.1109/TNNLS.2024.3374121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating virtual organ populations that capture sufficient variability while remaining plausible is essential to conduct in silico trials (ISTs) of medical devices. However, not all anatomical shapes of interest are always available for each individual in a population. The imaging examinations and modalities used can vary between subjects depending on their individualized clinical pathways. Different imaging modalities may have various fields of view and are sensitive to signals from other tissues/organs, or both. Hence, missing/partially overlapping anatomical information is often available across individuals. We introduce a generative shape model for multipart anatomical structures, learnable from sets of unpaired datasets, i.e., where each substructure in the shape assembly comes from datasets with missing or partially overlapping substructures from disjoint subjects of the same population. The proposed generative model can synthesize complete multipart shape assemblies coined virtual chimeras (VCs). We applied this framework to build VCs from databases of whole-heart shape assemblies that each contribute samples for heart substructures. Specifically, we propose a graph neural network-based generative shape compositional framework, which comprises two components, a part-aware generative shape model that captures the variability in shape observed for each structure of interest in the training population and a spatial composition network that assembles/composes the structures synthesized by the former into multipart shape assemblies (i.e., VCs). We also propose a novel self-supervised learning scheme that enables the spatial composition network to be trained with partially overlapping data and weak labels. We trained and validated our approach using shapes of cardiac structures derived from cardiac magnetic resonance (MR) images in the UK Biobank (UKBB). When trained with complete and partially overlapping data, our approach significantly outperforms a principal component analysis (PCA)-based shape model (trained with complete data) in terms of generalizability and specificity. This demonstrates the superiority of the proposed method, as the synthesized cardiac virtual populations are more plausible and capture a greater degree of shape variability than those generated by the PCA-based shape model.},
  archive      = {J_TNNLS},
  author       = {Haoran Dou and Seppo Virtanen and Nishant Ravikumar and Alejandro F. Frangi},
  doi          = {10.1109/TNNLS.2024.3374121},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4750-4764},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A generative shape compositional framework to synthesize populations of virtual chimeras},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrinsic consistency preservation with adaptively reliable
samples for source-free domain adaptation. <em>TNNLS</em>,
<em>36</em>(3), 4738–4749. (<a
href="https://doi.org/10.1109/TNNLS.2024.3362948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to alleviate the domain shift by transferring knowledge learned from a labeled source dataset to an unlabeled target domain. Although UDA has seen promising progress recently, it requires access to data from both domains, making it problematic in source data-absent scenarios. In this article, we investigate a practical task source-free domain adaptation (SFDA) that alleviates the limitations of the widely studied UDA in simultaneously acquiring source and target data. In addition, we further study the imbalanced SFDA (ISFDA) problem, which addresses the intra-domain class imbalance and inter-domain label shift in SFDA. We observe two key issues in SFDA that: 1) target data form clusters in the representation space regardless of whether the target data points are aligned with the source classifier and 2) target samples with higher classification confidence are more reliable and have less variation in their classification confidence during adaptation. Motivated by these observations, we propose a unified method, named intrinsic consistency preservation with adaptively reliable samples (ICPR), to jointly cope with SFDA and ISFDA. Specifically, ICPR first encourages the intrinsic consistency in the predictions of neighbors for unlabeled samples with weak augmentation (standard flip-and-shift), regardless of their reliability. ICPR then generates strongly augmented views specifically for adaptively selected reliable samples and is trained to fix the intrinsic consistency between weakly and strongly augmented views of the same image concerning predictions of neighbors and their own. Additionally, we propose to use a prototype-like classifier to avoid the classification confusion caused by severe intra-domain class imbalance and inter-domain label shift. We demonstrate the effectiveness and general applicability of ICPR on six benchmarks of both SFDA and ISFDA tasks. The reproducible code of our proposed ICPR method is available at https://github.com/CFM-MSG/Code_ICPR.},
  archive      = {J_TNNLS},
  author       = {Jialin Tian and Abdulmotaleb El Saddik and Xing Xu and Dongshuai Li and Zuo Cao and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2024.3362948},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4738-4749},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intrinsic consistency preservation with adaptively reliable samples for source-free domain adaptation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced unfolding induced tensor nuclear norms for
high-order tensor completion. <em>TNNLS</em>, <em>36</em>(3), 4724–4737.
(<a href="https://doi.org/10.1109/TNNLS.2024.3373384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed tensor tubal rank has been witnessed to obtain extraordinary success in real-world tensor data completion. However, existing works usually fix the transform orientation along the third mode and may fail to turn multidimensional low-tubal-rank structure into account. To alleviate these bottlenecks, we introduce two unfolding induced tensor nuclear norms (TNNs) for the tensor completion (TC) problem, which naturally extends tensor tubal rank to high-order data. Specifically, we show how multidimensional low-tubal-rank structure can be captured by utilizing a novel balanced unfolding strategy, upon which two TNNs, namely, overlapped TNN (OTNN) and latent TNN (LTNN), are developed. We also show the immediate relationship between the tubal rank of unfolding tensor and the existing tensor network (TN) rank, e.g., CANDECOMP/PARAFAC (CP) rank, Tucker rank, and tensor ring (TR) rank, to demonstrate its efficiency and practicality. Two efficient TC models are then proposed with theoretical guarantees by analyzing a unified nonasymptotic upper bound. To solve optimization problems, we develop two alternating direction methods of multipliers (ADMM) based algorithms. The proposed models have been demonstrated to exhibit superior performance based on experimental findings involving synthetic and real-world tensors, including facial images, light field images, and video sequences.},
  archive      = {J_TNNLS},
  author       = {Yuning Qiu and Guoxu Zhou and Andong Wang and Qibin Zhao and Shengli Xie},
  doi          = {10.1109/TNNLS.2024.3373384},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4724-4737},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Balanced unfolding induced tensor nuclear norms for high-order tensor completion},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-triggered approximate optimal neuro-control for
nonlinear systems through adaptive dynamic programming. <em>TNNLS</em>,
<em>36</em>(3), 4713–4723. (<a
href="https://doi.org/10.1109/TNNLS.2024.3362800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel self-triggered approximate optimal neuro-control scheme is presented for nonlinear systems by utilizing adaptive dynamic programming (ADP). According to the Bellman principle of optimality, the cost function of the general nonlinear system is approximated by building a critic neural network with a nested updating weight vector. Thus, the Hamilton–Jacobi–Bellman equation is solved to indirectly obtain the approximate optimal neuro-control input. In order to reduce the computation, the communication bandwidth, and the energy consumption, an appropriate self-triggering condition is designed as an alternative way to predict the updating time instants of the approximate optimal neuro-control policy. On the basis of Lyapunov’s direct method, the stability of the closed-loop nonlinear system is analyzed and guaranteed to be uniformly ultimately bounded. Simulation results of two practical systems illustrate the present ADP-based self-triggered approximate optimal neuro-control scheme to be reasonable and effective.},
  archive      = {J_TNNLS},
  author       = {Bo Zhao and Shunchao Zhang and Derong Liu},
  doi          = {10.1109/TNNLS.2024.3362800},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4713-4723},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-triggered approximate optimal neuro-control for nonlinear systems through adaptive dynamic programming},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A predefined-time adaptive zeroing neural network for
solving time-varying linear equations and its application to UR5 robot.
<em>TNNLS</em>, <em>36</em>(3), 4703–4712. (<a
href="https://doi.org/10.1109/TNNLS.2024.3373040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying linear equations (TVLEs) play a fundamental role in the engineering field and are of great practical value. Existing methods for the TVLE still have issues with long computation time and insufficient noise resistance. Zeroing neural network (ZNN) with parallel distribution and interference tolerance traits can mitigate these deficiencies and thus are good candidates for the TVLE. Therefore, a new predefined-time adaptive ZNN (PTAZNN) model is proposed for addressing the TVLE in this article. Unlike previous ZNN models with time-varying parameters, the PTAZNN model adopts a novel error-based adaptive parameter, which makes the convergence process more rapid and avoids unnecessary waste of computational resources caused by large parameters. Moreover, the stability, convergence, and robustness of the PTAZNN model are rigorously analyzed. Two numerical examples reflect that the PTAZNN model possesses shorter convergence time and better robustness compared with several variable-parameter ZNN models. In addition, the PTAZNN model is applied to solve the inverse kinematic solution of UR5 robot on the simulation platform CoppeliaSim, and the results further indicate the feasibility of this model intuitively.},
  archive      = {J_TNNLS},
  author       = {Wensheng Tang and Hang Cai and Lin Xiao and Yongjun He and Linju Li and Qiuyue Zuo and Jichun Li},
  doi          = {10.1109/TNNLS.2024.3373040},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4703-4712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A predefined-time adaptive zeroing neural network for solving time-varying linear equations and its application to UR5 robot},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding adversarial robustness from feature maps of
convolutional layers. <em>TNNLS</em>, <em>36</em>(3), 4690–4702. (<a
href="https://doi.org/10.1109/TNNLS.2024.3360463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adversarial robustness of a neural network mainly relies on two factors: model capacity and antiperturbation ability. In this article, we study the antiperturbation ability of the network from the feature maps of convolutional layers. Our theoretical analysis discovers that larger convolutional feature maps before average pooling can contribute to better resistance to perturbations, but the conclusion is not true for max pooling. It brings new inspiration to the design of robust neural networks and urges us to apply these findings to improve existing architectures. The proposed modifications are very simple and only require upsampling the inputs or slightly modifying the stride configurations of downsampling operators. We verify our approaches on several benchmark neural network architectures, including AlexNet, VGG, RestNet18, and PreActResNet18. Nontrivial improvements in terms of both natural accuracy and adversarial robustness can be achieved under various attack and defense mechanisms. The code is available at https://github.com/MTandHJ/rcm.},
  archive      = {J_TNNLS},
  author       = {Cong Xu and Wei Zhang and Jun Wang and Min Yang},
  doi          = {10.1109/TNNLS.2024.3360463},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4690-4702},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding adversarial robustness from feature maps of convolutional layers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional neural heuristic for multiobjective vehicle
routing problems. <em>TNNLS</em>, <em>36</em>(3), 4677–4689. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing neural heuristics for multiobjective vehicle routing problems (MOVRPs) are primarily conditioned on instance context, which failed to appropriately exploit preference and problem size, thus holding back the performance. To thoroughly unleash the potential, we propose a novel conditional neural heuristic (CNH) that fully leverages the instance context, preference, and size with an encoder–decoder structured policy network. Particularly, in our CNH, we design a dual-attention-based encoder to relate preferences and instance contexts, so as to better capture their joint effect on approximating the exact Pareto front (PF). We also design a size-aware decoder based on the sinusoidal encoding to explicitly incorporate the problem size into the embedding, so that a single trained model could better solve instances of various scales. Besides, we customize the REINFORCE algorithm to train the neural heuristic by leveraging stochastic preferences (SPs), which further enhances the training performance. Extensive experimental results on random and benchmark instances reveal that our CNH could achieve favorable approximation to the whole PF with higher hypervolume (HV) and lower optimality gap (Gap) than those of the existing neural and conventional heuristics. More importantly, a single trained model of our CNH can outperform other neural heuristics that are exclusively trained on each size. In addition, the effectiveness of the key designs is also verified through ablation studies.},
  archive      = {J_TNNLS},
  author       = {Mingfeng Fan and Yaoxin Wu and Zhiguang Cao and Wen Song and Guillaume Sartoretti and Huan Liu and Guohua Wu},
  doi          = {10.1109/TNNLS.2024.3371706},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4677-4689},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conditional neural heuristic for multiobjective vehicle routing problems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Locating target regions for image retrieval in an
unsupervised manner. <em>TNNLS</em>, <em>36</em>(3), 4664–4676. (<a
href="https://doi.org/10.1109/TNNLS.2024.3363163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image retrieval performance can be improved by training a convolutional neural network (CNN) model with annotated data to facilitate accurate localization of target regions. However, obtaining sufficiently annotated data is expensive and impractical in real settings. It is challenging to achieve accurate localization of target regions in an unsupervised manner. To address this problem, we propose a new unsupervised image retrieval method named unsupervised target region localization (UTRL) descriptors. It can precisely locate target regions without supervisory information or learning. Our method contains three highlights: 1) we propose a novel zero-label transfer learning method to address the problem of co-localization in target regions. This enhances the potential localization ability of pretrained CNN models through a zero-label data-driven approach; 2) we propose a multiscale attention accumulation method to accurately extract distinguishable target features. It distinguishes the importance of features by using local Gaussian weights; and 3) we propose a simple yet effective method to reduce vector dimensionality, named twice-PCA-whitening (TPW), which reduces the performance degradation caused by feature compression. Notably, TPW is a robust and general method that can be widely applied to image retrieval tasks to improve retrieval performance. This work also facilitates the development of image retrieval based on short vector features. Extensive experiments on six popular benchmark datasets demonstrate that our method achieves about 7% greater mean average precision (mAP) compared to existing state-of-the-art unsupervised methods.},
  archive      = {J_TNNLS},
  author       = {Bo-Jian Zhang and Guang-Hai Liu and Zuo-Yong Li and Shu-Xiang Song},
  doi          = {10.1109/TNNLS.2024.3363163},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4664-4676},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Locating target regions for image retrieval in an unsupervised manner},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image deblurring by exploring in-depth properties of
transformer. <em>TNNLS</em>, <em>36</em>(3), 4652–4663. (<a
href="https://doi.org/10.1109/TNNLS.2024.3359810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deblurring continues to achieve impressive performance with the development of generative models. Nonetheless, there still remains a displeasing problem if one wants to improve perceptual quality and quantitative scores of recovered image at the same time. In this study, drawing inspiration from the research of transformer properties, we introduce the pretrained transformers to address this problem. In particular, we leverage deep features extracted from a pretrained vision transformer (ViT) to encourage recovered images to be sharp without sacrificing the performance measured by the quantitative metrics. The pretrained transformer can capture the global topological relations (i.e., self-similarity) of image, and we observe that the captured topological relationships about the sharp image will change when blur occurs. By comparing the transformer features between recovered image and target one, the pretrained transformer provides high-resolution blur-sensitive semantic information, which is critical in measuring the sharpness of the deblurred image. On the basis of the advantages, we present two types of novel perceptual losses to guide image deblurring. One regards the features as vectors and computes the discrepancy between representations extracted from recovered image and target one in Euclidean space. The other type considers the features extracted from an image as a distribution and compares the distribution discrepancy between recovered image and target one. We demonstrate the effectiveness of transformer properties in improving the perceptual quality while not sacrificing the quantitative scores peak signal-to-noise ratio (PSNR) over the most competitive models, such as Uformer, Restormer, and NAFNet, on defocus deblurring and motion deblurring tasks. The code is available at https://github. com/erfect2020/TransformerPerceptualLoss.},
  archive      = {J_TNNLS},
  author       = {Pengwei Liang and Junjun Jiang and Xianming Liu and Jiayi Ma},
  doi          = {10.1109/TNNLS.2024.3359810},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4652-4663},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image deblurring by exploring in-depth properties of transformer},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical reinforcement learning for UAV-PE game with
alternative delay update method. <em>TNNLS</em>, <em>36</em>(3),
4639–4651. (<a
href="https://doi.org/10.1109/TNNLS.2024.3362969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel hierarchical reinforcement learning (HRL) algorithm for unmanned aerial vehicle pursuit-evasion (UAV-PE) game systems with an alternative delay update (ADU) method. In the proposed algorithm, the approximate solutions of the UAV-PE game problem are derived from a hierarchical learning process, which relies on a zero-sum game process of kinematics and a corresponding optimal process of dynamics. In this case, deep neural networks (NNs) are used to approximate the policy and value functions of UAV-PE game systems in kinematics and dynamics level. Furthermore, the ADU method is adopted to improve the training efficiency of deep NN by fixing one player of the UAV-PE game systems to form a stable environment. The goal of this article is to develop an HRL algorithm with an ADU method for obtaining approximate Nash equilibrium (NE) solutions of the considered UAV-PE game systems which are subjected to the coupling of kinematics and dynamics. Subsequently, sufficient conditions are provided for analyzing the convergence and optimality of the proposed HRL algorithm. Moreover, the inequalities of overload are obtained to guarantee that the state of dynamics tracks with the control input of kinematics in UAV-PE game systems. Finally, simulation examples are provided to demonstrate the feasibility and usefulness of the proposed HRL algorithm and ADU method.},
  archive      = {J_TNNLS},
  author       = {Xiao Ma and Yuan Yuan and Lei Guo},
  doi          = {10.1109/TNNLS.2024.3362969},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4639-4651},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical reinforcement learning for UAV-PE game with alternative delay update method},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSSTU: Generative spatial self-attention transformer unit
for enhanced video prediction. <em>TNNLS</em>, <em>36</em>(3),
4625–4638. (<a
href="https://doi.org/10.1109/TNNLS.2024.3359716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future frame prediction is a challenging task in computer vision with practical applications in areas such as video generation, autonomous driving, and robotics. Traditional recurrent neural networks have limited effectiveness in capturing long-range dependencies between frames, and combining convolutional neural networks (CNNs) with recurrent networks has limitations in modeling complex dependencies. Generative adversarial networks have shown promising results, but they are computationally expensive and suffer from instability during training. In this article, we propose a novel approach for future frame prediction that combines the encoding capabilities of 3-D CNNs with the sequence modeling capabilities of Transformers. We also propose a spatial self-attention mechanism and a novel neighborhood pixel intensity loss to preserve structural information and local intensity, respectively. Our approach outperforms existing methods in terms of structural similarity (SSIM), peak signal-to-noise ratio (PSNR), and learned perceptual image patch similarity (LPIPS) scores on five public datasets. More precisely, our model exhibited an average improvement of 4.64%, 18.5%, and 42% concerning SSIM, PSNR, and LPIPS for the second most proficient method correspondingly, across all datasets. The results demonstrate the effectiveness of our proposed method in generating high-quality predictions of future frames.},
  archive      = {J_TNNLS},
  author       = {Binit Singh and Divij Singh and Rohan Kaushal and Agrya Halder and Pratik Chattopadhyay},
  doi          = {10.1109/TNNLS.2024.3359716},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4625-4638},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GSSTU: Generative spatial self-attention transformer unit for enhanced video prediction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable moment propagation and analysis of variational
distributions for practical bayesian deep learning. <em>TNNLS</em>,
<em>36</em>(3), 4614–4624. (<a
href="https://doi.org/10.1109/TNNLS.2024.3367363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian deep learning is one of the key frameworks employed in handling predictive uncertainty. Variational inference (VI), an extensively used inference method, derives the predictive distributions by Monte Carlo (MC) sampling. The drawback of MC sampling is its extremely high computational cost compared to that of ordinary deep learning. In contrast, the moment propagation (MP)-based approach propagates the output moments of each layer to derive predictive distributions instead of MC sampling. Because of this computational property, it is expected to realize faster inference than MC-based approaches. However, the applicability of the MP-based method in deep models has not been explored sufficiently, even though some studies have demonstrated the effectiveness of MP only in small toy models. One of the reasons is that it is difficult to train deep models by MP because of the large variance in activations. To realize MP in deep models, some normalization layers are required but have not yet been studied. In addition, it is still difficult to design well-calibrated MP-based models, because the effectiveness of MP-based methods under various variational distributions has also not been investigated. In this study, we propose a fast and reliable MP-based Bayesian deep-learning method. First, to train deep-learning models using MP, we introduce a batch normalization layer extended to random variables to prevent increases in the variance of activations. Second, to identify the appropriate variational distribution in MP, we investigate the treatment of moments of several variational distributions and evaluate their uncertainty quality of predictions. Experiments with regression tasks demonstrate that the MP-based method provides qualitatively and quantitatively equivalent predictive performance to MC-based methods regardless of variational distributions. In the classification tasks, we show that we can train MP-based deep models by extended batch normalization. We also show that the MP-based approach realizes 2.0–2.8 times faster inference than the MC-based approach while maintaining the predictive performance. The results of this study can help realize a fast and well-calibrated uncertainty estimation method that can be deployed in a wider range of reliability-aware applications.},
  archive      = {J_TNNLS},
  author       = {Yuki Hirayama and Shinya Takamaeda-Yamazaki},
  doi          = {10.1109/TNNLS.2024.3367363},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4614-4624},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scalable moment propagation and analysis of variational distributions for practical bayesian deep learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formulating and representing multiagent systems with
hypergraphs. <em>TNNLS</em>, <em>36</em>(3), 4599–4613. (<a
href="https://doi.org/10.1109/TNNLS.2024.3368111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-learning methods, especially graph neural networks (GNNs), have shown remarkable effectiveness in handling non-Euclidean data and have achieved great success in various scenarios. Existing GNNs are primarily based on message-passing schemes, that is, aggregating information from neighboring nodes. However, the diversity and complexity of complex systems from real-world circumstances are not sufficiently taken into account. In these cases, the individual should be treated as an agent, with the ability to perceive their surroundings and interact with other individuals, rather than just be viewed as nodes in existing graph approaches. Additionally, the pairwise interactions used in existing methods also lack the expressiveness for the higher-order complex relations among multiple agents, thus limiting the performance in various tasks. In this work, we propose a Multiagent Hypergraph Force-learning method dubbed MHGForce. First, we formalize the multiagent system (MAS) and illustrate its connection to graph learning. Then, we propose a generalized multiagent hypergraph-learning framework. In this framework, we integrate message-passing and force-based interactions to devise a pluggable method. The method empowers graph approaches to excel in downstream tasks while effectively maintaining structural information in the representations. Experimental results on the Cora, Citeseer, Cora-CA, Zoo, and NTU2012 datasets in node classification demonstrate the effectiveness and generality of our proposed method. We also discuss the characteristics of the MHGForce and explore its role through parametric analysis and visualization. Finally, we give a discussion, conclude our work, and propose future directions.},
  archive      = {J_TNNLS},
  author       = {Shuo Yu and Huafei Huang and Yanming Shen and Pengfei Wang and Qiang Zhang and Ke Sun and Honglong Chen},
  doi          = {10.1109/TNNLS.2024.3368111},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4599-4613},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Formulating and representing multiagent systems with hypergraphs},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing conditional independence between latent variables by
independence residuals. <em>TNNLS</em>, <em>36</em>(3), 4586–4598. (<a
href="https://doi.org/10.1109/TNNLS.2024.3368561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional independence (CI) testing is an important problem, especially in causal discovery. Most testing methods assume that all variables are fully observable and then test the CI among the observed data. Such an assumption is often untenable beyond applications dealing with, e.g., psychological analysis about the mental health status and medical diagnosing (researchers need to consider the existence of latent variables in these scenarios); and typically adopted latent CI test schemes mainly suffer from robust or efficient issues. Accordingly, this article investigates the problem of testing CI between latent variables. To this end, we offer an auxiliary regression-based CI (AReCI) test by taking the measured variable as the surrogate variable of the latent variables to conduct the regression over the latent variables under the linear causal models, in which each latent variable has some certain measured variables. Specifically, given a pair of latent variables $L_{X}$ and $L_{Y}$ , and a corresponding latent variable set $\mathcal {L}_{O}$ , $L_{X} \mathrel {\perp \mspace {-10mu}\perp } L_{Y} | \mathcal {L}_{O}$ holds if and only if $A_{\{L_{X}\}}-\omega _{1}^{\intercal} A^{\prime }_{\{\mathcal {L}_{O}\}}$ and $A_{\{L_{Y}\}}-\omega _{2}^{\intercal} A^{\prime \prime }_{\{\mathcal {L}_{O}\}}$ are statistically independent, where $A^{\prime }$ and $A^{\prime \prime }$ are the two disjoint subset of the measured variable for the corresponding latent variables, $A^{\prime }_{\{\mathcal {L}_{O}\}} \cap A^{\prime \prime }_{\{\mathcal {L}_{O}\}} =\emptyset $ , and $\omega _{1}$ is a parameter vector characterized from the cross covariance between $A_{\{L_{X}\}}$ and $A^{\prime }_{\{\mathcal {L}_{O}\}}$ , and $\omega _{2}$ is a parameter vector characterized from the cross covariance between $A_{\{L_{Y}\}}$ and $A^{\prime \prime }_{\{\mathcal {L}_{O}\}}$ . We theoretically show that the AReCI test is capable of addressing both Gaussian and non-Gaussian data. In addition, we find that the well-known partial correlation test can be seen as a special case of the AReCI test. Finally, we devise a causal discovery method by using the AReCI test as the CI test. The experimental results on synthetic and real-world data illustrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Zhengming Chen and Jie Qiao and Feng Xie and Ruichu Cai and Zhifeng Hao and Keli Zhang},
  doi          = {10.1109/TNNLS.2024.3368561},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4586-4598},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Testing conditional independence between latent variables by independence residuals},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Playing the lottery with concave regularizers for sparse
trainable neural networks. <em>TNNLS</em>, <em>36</em>(3), 4575–4585.
(<a href="https://doi.org/10.1109/TNNLS.2024.3373609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of sparse neural networks, i.e., of networks with a reduced number of parameters, has been attracting increasing research attention in the last few years. The use of sparse models may significantly reduce the computational and storage footprint in the inference phase. In this context, the lottery ticket hypothesis (LTH) constitutes a breakthrough result, that addresses not only the performance of the inference phase, but also of the training phase. It states that it is possible to extract effective sparse subnetworks, called winning tickets, that can be trained in isolation. The development of effective methods to play the lottery, i.e., to find winning tickets, is still an open problem. In this article, we propose a novel class of methods to play the lottery. The key point is the use of concave regularization to promote the sparsity of a relaxed binary mask, which represents the network topology. We theoretically analyze the effectiveness of the proposed method in the convex framework. Then, we propose extended numerical tests on various datasets and architectures, that show that the proposed method can improve the performance of state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Giulia Fracastoro and Sophie M. Fosson and Andrea Migliorati and Giuseppe C. Calafiore},
  doi          = {10.1109/TNNLS.2024.3373609},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4575-4585},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Playing the lottery with concave regularizers for sparse trainable neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble prototype network for weakly supervised temporal
action localization. <em>TNNLS</em>, <em>36</em>(3), 4560–4574. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal action localization (TAL) aims to localize the action instances in untrimmed videos using only video-level action labels. Without snippet-level labels, this task should be hard to distinguish all snippets with accurate action/background categories. The main difficulties are the large variations brought by the unconstraint background snippets and multiple subactions in action snippets. The existing prototype model focuses on describing snippets by covering them with clusters (defined as prototypes). In this work, we argue that the clustered prototype covering snippets with simple variations still suffers from the misclassification of the snippets with large variations. We propose an ensemble prototype network (EPNet), which ensembles prototypes learned with consensus-aware clustering. The network stacks a consensus prototype learning (CPL) module and an ensemble snippet weight learning (ESWL) module as one stage and extends one stage to multiple stages in an ensemble learning way. The CPL module learns the consensus matrix by estimating the similarity of clustering labels between two successive clustering generations. The consensus matrix optimizes the clustering to learn consensus prototypes, which can predict the snippets with consensus labels. The ESWL module estimates the weights of the misclassified snippets using the snippet-level loss. The weights update the posterior probabilities of the snippets in the clustering to learn prototypes in the next stage. We use multiple stages to learn multiple prototypes, which can cover the snippets with large variations for accurate snippet classification. Extensive experiments show that our method achieves the state-of-the-art weakly supervised TAL methods on two benchmark datasets, that is, THUMOS’14, ActivityNet v1.2, and ActivityNet v1.3 datasets.},
  archive      = {J_TNNLS},
  author       = {Kewei Wu and Wenjie Luo and Zhao Xie and Dan Guo and Zhao Zhang and Richang Hong},
  doi          = {10.1109/TNNLS.2024.3377468},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4560-4574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ensemble prototype network for weakly supervised temporal action localization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal dual-embedding networks for malware open-set
recognition. <em>TNNLS</em>, <em>36</em>(3), 4545–4559. (<a
href="https://doi.org/10.1109/TNNLS.2024.3373809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware open-set recognition (MOSR) is an emerging research domain that aims at jointly classifying malware samples from known families and detecting the ones from novel unknown families, respectively. Existing works mostly rely on a well-trained classifier considering the predicted probabilities of each known family with a threshold-based detection to achieve the MOSR. However, our observation reveals that the feature distributions of malware samples are extremely similar to each other even between known and unknown families. Thus, the obtained classifier may produce overly high probabilities of testing unknown samples toward known families and degrade the model performance. In this article, we propose the multi\modal dual-embedding networks, dubbed MDENet, to take advantage of comprehensive malware features from different modalities to enhance the diversity of malware feature space, which is more representative and discriminative for down-stream recognition. Concretely, we first generate a malware image for each observed sample based on their numeric features using our proposed numeric encoder with a re- designed multiscale CNN structure, which can better explore their statistical and spatial correlations. Besides, we propose to organize tokenized malware features into a sentence for each sample considering its behaviors and dynamics, and utilize language models as the textual encoder to transform it into a representable and computable textual vector. Such parallel multimodal encoders can fuse the above two components to enhance the feature diversity. Last, to further guarantee the open-set recognition (OSR), we dually embed the fused multimodal representation into one primary space and an associated sub-space, i.e., discriminative and exclusive spaces, with contrastive sampling and $\rho $ -bounded enclosing sphere regularizations, which resort to classification and detection, respectively. Moreover, we also enrich our previously proposed large-scaled malware dataset MAL-100 with multimodal characteristics and contribute an improved version dubbed MAL-100+. Experimental results on the widely used malware dataset Mailing and the proposed MAL-100+ demonstrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Jingcai Guo and Han Wang and Yuanyuan Xu and Wenchao Xu and Yufeng Zhan and Yuxia Sun and Song Guo},
  doi          = {10.1109/TNNLS.2024.3373809},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4545-4559},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multimodal dual-embedding networks for malware open-set recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy state-driven cross-time spatial dependence learning
for multivariate time-series anomaly detection. <em>TNNLS</em>,
<em>36</em>(3), 4532–4544. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-time spatial dependence (i.e., the interaction between different variables at different time points) is indispensable for detecting anomalies in multivariate time series, as certain anomalies may have time delays in their propagation from one variable to another. However, accurately capturing cross-time spatial dependence remains a challenge. Specifically, real-world time series usually exhibits complex and incomprehensible evolutions that may be compounded by multiple temporal states (i.e., temporal patterns, such as rising, fluctuating, and peak). These temporal states mix and overlap with each other and exhibit dynamic and heterogeneous evolution laws in different time series, making the cross-time spatial dependence extremely intricate and mutable. Therefore, a cross-time spatial graph network with fuzzy embedding is proposed to disentangle latent and mixing temporal states and exploit it to meticulously learn cross-time spatial dependence. First, considering that temporal states are diversiform and their mixing modes are unknown, we introduce a fuzzy state set to uniformly characterize potential temporal states and adaptively generate corresponding membership degrees to depict how these states mix. Further, we propose a cross-time spatial graph, quantifying similarities among fuzzy states and sensing their dynamic evolutions, to flexibly learn mutable cross-time spatial dependence. Finally, we design state diversity and temporal proximity constraints to ensure the differences among fuzzy states and the evolution continuity of fuzzy states. Experiments on real-world datasets show that the proposed model outperforms the state-of-the-art models.},
  archive      = {J_TNNLS},
  author       = {Kun Zhu and Pengyu Song and Chunhui Zhao},
  doi          = {10.1109/TNNLS.2024.3371109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4532-4544},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fuzzy state-driven cross-time spatial dependence learning for multivariate time-series anomaly detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging the cross-modality semantic gap in visual question
answering. <em>TNNLS</em>, <em>36</em>(3), 4519–4531. (<a
href="https://doi.org/10.1109/TNNLS.2024.3370925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of visual question answering (VQA) is to adequately comprehend a question and identify relevant contents in an image that can provide an answer. Existing approaches in VQA often combine visual and question features directly to create a unified cross-modality representation for answer inference. However, this kind of approach fails to bridge the semantic gap between visual and text modalities, resulting in a lack of alignment in cross-modality semantics and the inability to match key visual content accurately. In this article, we propose a model called the caption bridge-based cross-modality alignment and contrastive learning model (CBAC) to address the issue. The CBAC model aims to reduce the semantic gap between different modalities. It consists of a caption-based cross-modality alignment module and a visual-caption (V-C) contrastive learning module. By utilizing an auxiliary caption that shares the same modality as the question and has closer semantic associations with the visual, we are able to effectively reduce the semantic gap by separately matching the caption with both the question and the visual to generate pre-alignment features for each, which are then used in the subsequent fusion process. We also leverage the fact that V-C pairs exhibit stronger semantic connections compared to question-visual (Q-V) pairs to employ a contrastive learning mechanism on visual and caption pairs to further enhance the semantic alignment capabilities of single-modality encoders. Extensive experiments conducted on three benchmark datasets demonstrate that the proposed model outperforms previous state-of-the-art VQA models. Additionally, ablation experiments confirm the effectiveness of each module in our model. Furthermore, we conduct a qualitative analysis by visualizing the attention matrices to assess the reasoning reliability of the proposed model.},
  archive      = {J_TNNLS},
  author       = {Boyue Wang and Yujian Ma and Xiaoyan Li and Junbin Gao and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TNNLS.2024.3370925},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4519-4531},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bridging the cross-modality semantic gap in visual question answering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPNet: Dual-path network for real-time object detection with
lightweight attention. <em>TNNLS</em>, <em>36</em>(3), 4504–4518. (<a
href="https://doi.org/10.1109/TNNLS.2024.3376563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advances in compressing high-accuracy convolutional neural networks (CNNs) have witnessed remarkable progress in real-time object detection. To accelerate detection speed, lightweight detectors always have few convolution layers using a single-path backbone. Single-path architecture, however, involves continuous pooling and downsampling operations, always resulting in coarse and inaccurate feature maps that are disadvantageous to locate objects. On the other hand, due to limited network capacity, recent lightweight networks are often weak in representing large-scale visual data. To address these problems, we present a dual-path network, named DPNet, with a lightweight attention scheme for real-time object detection. The dual-path architecture enables us to extract in parallel high-level semantic features and low-level object details. Although DPNet has a nearly duplicated shape with respect to single-path detectors, the computational costs and model size are not significantly increased. To enhance representation capability, a lightweight self-correlation module (LSCM) is designed to capture global interactions, with only a few computational overheads and network parameters. In the neck, LSCM is extended into a lightweight cross correlation module (LCCM), capturing mutual dependencies among neighboring scale features. We have conducted exhaustive experiments on MS COCO, Pascal VOC 2007, and ImageNet datasets. The experimental results demonstrate that DPNet achieves a state-of-the-art trade off between detection accuracy and implementation efficiency. More specifically, DPNet achieves 31.3% AP on MS COCO test-dev, 82.7% mAP on Pascal VOC 2007 test set, and 41.6% mAP on ImageNet validation set, together with nearly 2.5M model size, 1.04 GFLOPs, and 164 and 196 frames/s (FPS) FPS for $320 \; \times \; 320$ input images of three datasets.},
  archive      = {J_TNNLS},
  author       = {Quan Zhou and Huimin Shi and Weikang Xiang and Bin Kang and Longin Jan Latecki},
  doi          = {10.1109/TNNLS.2024.3376563},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4504-4518},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DPNet: Dual-path network for real-time object detection with lightweight attention},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot face stylization via GAN prior distillation.
<em>TNNLS</em>, <em>36</em>(3), 4492–4503. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face stylization has made notable progress in recent years. However, when training on limited data, the performance of existing approaches significantly declines. Although some studies have attempted to tackle this problem, they either failed to achieve the few-shot setting (less than 10) or can only get suboptimal results. In this article, we propose GAN Prior Distillation (GPD) to enable effective few-shot face stylization. GPD contains two models: a teacher network with GAN Prior and a student network that fulfills end-to-end translation. Specifically, we adapt the teacher network trained on large-scale data in the source domain to the target domain using a handful of samples, where it can learn the target domain’s knowledge. Then, we can achieve few-shot augmentation by generating source domain and target domain images simultaneously with the same latent codes. We propose an anchor-based knowledge distillation module that can fully use the difference between the training and the augmented data to distill the knowledge of the teacher network into the student network. The trained student network achieves excellent generalization performance with the absorption of additional knowledge. Qualitative and quantitative experiments demonstrate that our method achieves superior results than state-of-the-art approaches in a few-shot setting.},
  archive      = {J_TNNLS},
  author       = {Ruoyu Zhao and Mingrui Zhu and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2024.3377609},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4492-4503},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Few-shot face stylization via GAN prior distillation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Off-policy prediction learning: An empirical study of online
algorithms. <em>TNNLS</em>, <em>36</em>(3), 4477–4491. (<a
href="https://doi.org/10.1109/TNNLS.2024.3373749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-policy prediction—learning the value function for one policy from data generated while following another policy—is one of the most challenging problems in reinforcement learning. This article makes two main contributions: 1) it empirically studies 11 off-policy prediction learning algorithms with emphasis on their sensitivity to parameters, learning speed, and asymptotic error and 2) based on the empirical results, it proposes two step-size adaptation methods called Step-size Ratchet and Soft Step-size Ratchet that help the algorithm with the lowest error from the experimental study learn faster. Many off-policy prediction learning algorithms have been proposed in the past decade, but it remains unclear which algorithms learn faster than others. In this article, we empirically compare 11 off-policy prediction learning algorithms with linear function approximation on three small tasks: the Collision task, the Rooms task, and the High Variance Rooms task. The Collision task is a small off-policy problem analogous to that of an autonomous car trying to predict whether it will collide with an obstacle. The Rooms and High Variance Rooms tasks are designed such that learning fast in them is challenging. In the Rooms task, the product of importance sampling ratios can be as large as $2^{14}$ . To control the high variance caused by the product of the importance sampling ratios, step size should be set small, which, in turn, slows down learning. The High Variance Rooms task is more extreme in that the product of the ratios can become as large as $2^{14} \times 25$ . The algorithms considered are Off-policy TD( $\lambda $ ), five Gradient-TD algorithms, two Emphatic-TD algorithms, Vtrace, and variants of Tree Backup and ABQ that are applicable to the prediction setting. We found that the algorithms’ performance is highly affected by the variance induced by the importance sampling ratios. Tree Backup( $\lambda $ ), Vtrace( $\lambda $ ), and ABTD( $\zeta $ ) are not affected by the high variance as much as other algorithms, but they restrict the effective bootstrapping parameter in a way that is too limiting for tasks where high variance is not present. We observed that Emphatic TD( $\lambda $ ) tends to have lower asymptotic error than other algorithms but might learn more slowly in some cases. Based on the empirical results, we propose two step-size adaptation algorithms, which we collectively refer to as the Ratchet algorithms, with the same underlying idea: keep the step-size parameter as large as possible and ratchet it down only when necessary to avoid overshoot. We show that the Ratchet algorithms are effective by comparing them with other popular step-size adaptation algorithms, such as the Adam optimizer.},
  archive      = {J_TNNLS},
  author       = {Sina Ghiassian and Banafsheh Rafiee and Richard S. Sutton},
  doi          = {10.1109/TNNLS.2024.3373749},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4477-4491},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Off-policy prediction learning: An empirical study of online algorithms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label informed contrastive pretraining for node importance
estimation on knowledge graphs. <em>TNNLS</em>, <em>36</em>(3),
4462–4476. (<a
href="https://doi.org/10.1109/TNNLS.2024.3363695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node importance estimation (NIE) is the task of inferring the importance scores of the nodes in a graph. Due to the availability of richer data and knowledge, recent research interests of NIE have been dedicated to knowledge graphs (KGs) for predicting future or missing node importance scores. Existing state-of-the-art NIE methods train the model by available labels, and they consider every interested node equally before training. However, the nodes with higher importance often require or receive more attention in real-world scenarios, e.g., people may care more about the movies or webpages with higher importance. To this end, we introduce Label Informed ContrAstive Pretraining (LICAP) to the NIE problem for being better aware of the nodes with high importance scores. Specifically, LICAP is a novel type of contrastive learning (CL) framework that aims to fully utilize continuous labels to generate contrastive samples for pretraining embeddings. Considering the NIE problem, LICAP adopts a novel sampling strategy called top nodes preferred hierarchical sampling to first group all interested nodes into a top bin and a nontop bin based on node importance scores, and then divide the nodes within the top bin into several finer bins also based on the scores. The contrastive samples are generated from those bins and are then used to pretrain node embeddings of KGs via a newly proposed predicate-aware graph attention networks (PreGATs), so as to better separate the top nodes from nontop nodes, and distinguish the top nodes within the top bin by keeping the relative order among finer bins. Extensive experiments demonstrate that the LICAP pretrained embeddings can further boost the performance of existing NIE methods and achieve new state-of-the-art performance regarding both regression and ranking metrics. The source code for reproducibility is available at https://github.com/zhangtia16/LICAP.},
  archive      = {J_TNNLS},
  author       = {Tianyu Zhang and Chengbin Hou and Rui Jiang and Xuegong Zhang and Chenghu Zhou and Ke Tang and Hairong Lv},
  doi          = {10.1109/TNNLS.2024.3363695},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4462-4476},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Label informed contrastive pretraining for node importance estimation on knowledge graphs},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A continuous volatility forecasting model based on neural
differential equations and scale-similarity. <em>TNNLS</em>,
<em>36</em>(3), 4448–4461. (<a
href="https://doi.org/10.1109/TNNLS.2024.3376530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volatility forecasting is a problem in finance that attracts the attention of both academia and industry. While existing approaches typically utilize a discrete-time latent process that governs the volatility to forecast its future level, volatility is considered to evolve continuously, which makes discrete-time modeling inevitably lose some critical information about the evolution of volatility. In this article, a novel neural-network-based model, Continuous Volatility Forecasting Model, CVFM is proposed to tackle this problem. First, CVFM introduces a continuous-time latent process, whose evolution is modeled with neural differential equations (NDEs), to govern volatility, which effectively captures the continuous evolutionary behavior of volatility in a data-driven way. Second, a scale-similarity-based mechanism is designed to calibrate the evolution equation of the latent process with real-world observations in the absence of high-frequency data. CVFM is tested on six real-world stock index datasets. The main experimental results show that CVFM can significantly outperform existing models in terms of both forecasting accuracy and high-volatility recognition.},
  archive      = {J_TNNLS},
  author       = {Bowen Pang and Liyi Huang and Qingsong Li and Wei Wei},
  doi          = {10.1109/TNNLS.2024.3376530},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4448-4461},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A continuous volatility forecasting model based on neural differential equations and scale-similarity},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual balanced class-incremental learning with im-softmax and
angular rectification. <em>TNNLS</em>, <em>36</em>(3), 4437–4447. (<a
href="https://doi.org/10.1109/TNNLS.2024.3368341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the superior performances, exemplar-based methods with knowledge distillation (KD) are widely applied in class incremental learning (CIL). However, it suffers from two drawbacks: 1) data imbalance between the old/learned and new classes causes the bias of the new classifier toward the head/new classes and 2) deep neural networks (DNNs) suffer from distribution drift when learning sequence tasks, which results in narrowed feature space and deficient representation of old tasks. For the first problem, we analyze the insufficiency of softmax loss when dealing with the problem of data imbalance in theory and then propose the imbalance softmax (im-softmax) loss to relieve the imbalanced data learning, where we re-scale the output logits to underfit the head/new classes. For another problem, we calibrate the feature space by incremental-adaptive angular margin (IAAM) loss. The new classes form a complete distribution in feature space yet the old are squeezed. To recover the old feature space, we first compute the included angle of normalized features and normalized anchor prototypes, and use the angle distribution to represent the class distribution, then we replenish the old distribution with the deviation from the new. Each anchor prototype is predefined as a learnable vector for a designated class. The proposed im-softmax reduces the bias in the linear classification layer. IAAM rectifies the representation learning, reduces the intra-class distance, and enlarges the inter-class margin. Finally, we seamlessly combine the im-softmax and IAAM in an end-to-end training framework, called the dual balanced class incremental learning (DBL), for further improvements. Experiments demonstrate the proposed method achieves state-of-the-art (SOTA) performances on several benchmarks, such as CIFAR10, CIFAR100, Tiny-ImageNet, and ImageNet-100.},
  archive      = {J_TNNLS},
  author       = {Ruicong Zhi and Yicheng Meng and Junyi Hou and Jun Wan},
  doi          = {10.1109/TNNLS.2024.3368341},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4437-4447},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual balanced class-incremental learning with im-softmax and angular rectification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical distributed data-driven adaptive learning
control for nonaffine nonlinear MASs. <em>TNNLS</em>, <em>36</em>(3),
4428–4436. (<a
href="https://doi.org/10.1109/TNNLS.2024.3362864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article designs a new hierarchical distributed data-driven adaptive learning control algorithm to accomplish the leader-following tracking control objective for nonaffine nonlinear multiagent systems (MASs). The proposed hierarchical control structure is composed of a distributed observer and a decentralized data-driven adaptive learning controller. Considering that some followers cannot directly receive information from the leader, a distributed observer is designed to estimate the information of the leader. Based on this, a decentralized data-driven adaptive learning controller is further devised to enable the follower to track the estimated information of the leader, where the model parameter learning algorithm is developed to capture the dynamic characteristics of the original system. One advantage of the developed hierarchical control learning algorithm is that neither the leader’s system model nor the follower’s system model is needed. The other one is the elimination of the noncausal problem without the additional assumption. Simulation results exemplify the merits of the theoretical results by comparisons.},
  archive      = {J_TNNLS},
  author       = {Yong-Sheng Ma and Wei-Wei Che},
  doi          = {10.1109/TNNLS.2024.3362864},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4428-4436},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hierarchical distributed data-driven adaptive learning control for nonaffine nonlinear MASs},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Orthogonal subspace representation for generative
adversarial networks. <em>TNNLS</em>, <em>36</em>(3), 4413–4427. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentanglement learning aims to separate explanatory factors of variation so that different attributes of the data can be well characterized and isolated, which promotes efficient inference for downstream tasks. Mainstream disentanglement approaches based on generative adversarial networks (GANs) learn interpretable data representation. However, most typical GAN-based works lack the discussion of the latent subspace, causing insufficient consideration of the variation of independent factors. Although some recent research analyzes the latent space on pretrained GANs for image editing, they do not emphasize learning representation directly from the subspace perspective. Appropriate subspace properties could facilitate corresponding feature representation learning to satisfy the independent variation requirements of the obtained explanatory factors, which is crucial for better disentanglement. In this work, we propose a unified framework for ensuring disentanglement, which fully investigates latent subspace learning (SL) in GAN. The novel GAN-based architecture explores orthogonal subspace representation (OSR) on vanilla GAN, named OSRGAN. To guide a subspace with strong correlation, less redundancy, and robust distinguishability, our OSR includes three stages, self-latent-aware, orthogonal subspace-aware, and structure representation-aware, respectively. First, the self-latent-aware stage promotes the latent subspace strongly correlated with the data space to discover interpretable factors, but with poor independence of variation. Second, the following orthogonal subspace-aware stage adaptively learns some 1-D linear subspace spanned by a set of orthogonal bases in the latent space. There is less redundancy between them, expressing the corresponding independence. Third, the structure representation-aware stage aligns the projection on the orthogonal subspace and the latent variables. Accordingly, feature representation in each linear subspace can be distinguishable, enhancing the independent expression of interpretable factors. In addition, we design an alternating optimization step, achieving a tradeoff training of OSRGAN on different properties. Despite it strictly constrains orthogonality, the loss weight coefficient of distinguishability induced by orthogonality could be adjusted and balanced with correlation constraint. To elucidate, this tradeoff training prevents our OSRGAN from overemphasizing any property and damaging the expressiveness of the feature representation. It takes into account both interpretable factors and their independent variation characteristics. Meanwhile, alternating optimization could keep the cost and efficiency of forward inference unchanged and will not burden the computational complexity. In theory, we clarify the significance of OSR, which brings better independence of factors, along with interpretability as correlation could converge to a high range faster. Moreover, through the convergence behavior analysis, including the objective functions under different constraints and the evaluation curve with iterations, our model demonstrates enhanced stability and definitely converges toward a higher peak for disentanglement. To depict the performance in downstream tasks, we compared the state-of-the-art GAN-based and even VAE-based approaches on different datasets. Our OSRGAN achieves higher disentanglement scores on FactorVAE, SAP, MIG, and VP metrics. All the experimental results illustrate that our novel GAN-based framework has considerable advantages on disentanglement.},
  archive      = {J_TNNLS},
  author       = {Hongxiang Jiang and Xiaoyan Luo and Jihao Yin and Huazhu Fu and Fuxiang Wang},
  doi          = {10.1109/TNNLS.2024.3377436},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4413-4427},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Orthogonal subspace representation for generative adversarial networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointWavelet: Learning in spectral domain for 3-d point
cloud analysis. <em>TNNLS</em>, <em>36</em>(3), 4400–4412. (<a
href="https://doi.org/10.1109/TNNLS.2024.3363244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent success of deep learning in 2-D visual recognition, deep-learning-based 3-D point cloud analysis has received increasing attention from the community, especially due to the rapid development of autonomous driving technologies. However, most existing methods directly learn point features in the spatial domain, leaving the local structures in the spectral domain poorly investigated. In this article, we introduce a new method, PointWavelet, to explore local graphs in the spectral domain via a learnable graph wavelet transform. Specifically, we first introduce the graph wavelet transform to form multiscale spectral graph convolution to learn effective local structural representations. To avoid the time-consuming spectral decomposition, we then devise a learnable graph wavelet transform, which significantly accelerates the overall training process. Extensive experiments on four popular point cloud datasets, ModelNet40, ScanObjectNN, ShapeNet-Part, and S3DIS, demonstrate the effectiveness of the proposed method on point cloud classification and segmentation.},
  archive      = {J_TNNLS},
  author       = {Cheng Wen and Jianzhi Long and Baosheng Yu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2024.3363244},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4400-4412},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PointWavelet: Learning in spectral domain for 3-D point cloud analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMESH: A structure-preserving diffusion model for 3-d mesh
denoising. <em>TNNLS</em>, <em>36</em>(3), 4385–4399. (<a
href="https://doi.org/10.1109/TNNLS.2024.3367327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising diffusion models have shown a powerful capacity for generating high-quality image samples by progressively removing noise. Inspired by this, we present a diffusion-based mesh denoiser that progressively removes noise from mesh. In general, the iterative algorithm of diffusion models attempts to manipulate the overall structure and fine details of target meshes simultaneously. For this reason, it is difficult to apply the diffusion process to a mesh denoising task that removes artifacts while maintaining a structure. To address this, we formulate a structure-preserving diffusion process. Instead of diffusing the mesh vertices to be distributed as zero-centered isotopic Gaussian distribution, we diffuse each vertex into a specific noise distribution, in which the entire structure can be preserved. In addition, we propose a topology-agnostic mesh diffusion model by projecting the vertex into multiple 2-D viewpoints to efficiently learn the diffusion using a deep network. This enables the proposed method to learn the diffusion of arbitrary meshes that have an irregular topology. Finally, the denoised mesh can be obtained via refinement based on 2-D projections obtained from reverse diffusion. Through extensive experiments, we demonstrate that our method outperforms the state-of-the-art mesh denoising methods in both quantitative and qualitative evaluations.},
  archive      = {J_TNNLS},
  author       = {Seongmin Lee and Suwoong Heo and Sanghoon Lee},
  doi          = {10.1109/TNNLS.2024.3367327},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4385-4399},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DMESH: A structure-preserving diffusion model for 3-D mesh denoising},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HOT-GAN: Hilbert optimal transport for generative
adversarial network. <em>TNNLS</em>, <em>36</em>(3), 4371–4384. (<a
href="https://doi.org/10.1109/TNNLS.2024.3370617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network (GAN) has achieved remarkable success in generating high-quality synthetic data by learning the underlying distributions of target data. Recent efforts have been devoted to utilizing optimal transport (OT) to tackle the gradient vanishing and instability issues in GAN. They use the Wasserstein distance as a metric to measure the discrepancy between the generator distribution and the real data distribution. However, most optimal transport GANs define loss functions in Euclidean space, which limits their capability in handling high-order statistics that are of much interest in a variety of practical applications. In this article, we propose a computational framework to alleviate this issue from both theoretical and practical perspectives. Particularly, we generalize the optimal transport-based GAN from Euclidean space to the reproducing kernel Hilbert space (RKHS) and propose Hilbert Optimal Transport GAN (HOT-GAN). First, we design HOT-GAN with a Hilbert embedding that allows the discriminator to tackle more informative and high-order statistics in RKHS. Second, we prove that HOT-GAN has a closed-form kernel reformulation in RKHS that can achieve a tractable objective under the GAN framework. Third, HOT-GAN’s objective enjoys the theoretical guarantee of differentiability with respect to generator parameters, which is beneficial to learn powerful generators via adversarial kernel learning. Extensive experiments are conducted, showing that our proposed HOT-GAN consistently outperforms the representative GAN works.},
  archive      = {J_TNNLS},
  author       = {Qian Li and Zhichao Wang and Haiyang Xia and Gang Li and Yanan Cao and Lina Yao and Guandong Xu},
  doi          = {10.1109/TNNLS.2024.3370617},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4371-4384},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {HOT-GAN: Hilbert optimal transport for generative adversarial network},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced network compression through tensor decompositions
and pruning. <em>TNNLS</em>, <em>36</em>(3), 4358–4370. (<a
href="https://doi.org/10.1109/TNNLS.2024.3370294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network compression techniques that combine tensor decompositions and pruning have shown promise in leveraging the advantages of both strategies. In this work, we propose enhanced Network cOmpRession through TensOr decompositions and pruNing (NORTON), a novel method for network compression. NORTON introduces the concept of filter decomposition, enabling a more detailed decomposition of the network while preserving the weight’s multidimensional properties. Our method incorporates a novel structured pruning approach, effectively integrating the decomposed model. Through extensive experiments on various architectures, benchmark datasets, and representative vision tasks, we demonstrate the usefulness of our method. NORTON achieves superior results compared to state-of-the-art (SOTA) techniques in terms of complexity and accuracy. Our code is also available for research purposes.},
  archive      = {J_TNNLS},
  author       = {Van Tien Pham and Yassine Zniyed and Thanh Phuong Nguyen},
  doi          = {10.1109/TNNLS.2024.3370294},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4358-4370},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhanced network compression through tensor decompositions and pruning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A gradient-guided evolutionary neural architecture search.
<em>TNNLS</em>, <em>36</em>(3), 4345–4357. (<a
href="https://doi.org/10.1109/TNNLS.2024.3371432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) is a popular method that can automatically design deep neural network structures. However, designing a neural network using NAS is computationally expensive. This article proposes a gradient-guided evolutionary NAS (GENAS) to design convolutional neural networks (CNNs) for image classification. GENAS is a hybrid algorithm that combines evolutionary global and local search operators to evolve a population of subnets sampled from a supernet. Each candidate architecture is encoded as a table describing which operations are associated with the edges between nodes signifying feature maps. Besides, evolutionary optimization uses novel crossover and mutation operators to manipulate the subnets using the proposed tabular encoding. Every $n$ generations, the candidate architectures undergo a local search inspired by differentiable NAS. GENAS is designed to overcome the limitations of both evolutionary and gradient descent NAS. This algorithmic structure enables the performance assessment of the candidate architecture without retraining, thus limiting the NAS calculation time. Furthermore, subnet individuals are decoupled during evaluation to prevent strong coupling of operations in the supernet. The experimental results indicate that the searched structures achieve test errors of 2.45%, 16.86%, and 23.9% on CIFAR-10/100/ImageNet datasets and it costs only 0.26 GPU days on a graphic card. GENAS can effectively expedite the training and evaluation processes and obtain high-performance network structures.},
  archive      = {J_TNNLS},
  author       = {Yu Xue and Xiaolong Han and Ferrante Neri and Jiafeng Qin and Danilo Pelusi},
  doi          = {10.1109/TNNLS.2024.3371432},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4345-4357},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A gradient-guided evolutionary neural architecture search},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Closed-form gaussian spread estimation for small and large
support vector classification. <em>TNNLS</em>, <em>36</em>(3),
4336–4344. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The support vector machine (SVM) with Gaussian kernel often achieves state-of-the-art performance in classification problems, but requires the tuning of the kernel spread. Most optimization methods for spread tuning require training, being slow and not suited for large-scale datasets. We formulate an analytic expression to calculate, directly from data without iterative search, the spread minimizing the difference between Gaussian and ideal kernel matrices. The proposed direct gamma tuning (DGT) equals the performance of and is one to two orders of magnitude faster than the state-of-the art approaches on 30 small datasets. Combined with random sampling of training patterns, it also runs on large classification problems. Our method is very efficient in experiments with 20 large datasets up to 31 million of patterns, it is faster and performs significantly better than linear SVM, and it is also faster than iterative minimization. Code is available upon paper acceptance from this link: https://persoal.citius.usc.es/manuel.fernandez.delgado/papers/dgt/index.html and from CodeOcean: https://codeocean.com/capsule/4271163/tree/v1.},
  archive      = {J_TNNLS},
  author       = {Diego Isla-Cernadas and Manuel Fernández-Delgado and Eva Cernadas and Manisha S. Sirsat and Haitham Maarouf and Senén Barro},
  doi          = {10.1109/TNNLS.2024.3377370},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4336-4344},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Closed-form gaussian spread estimation for small and large support vector classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collective neural dynamics for sparse motion planning of
redundant manipulators without hessian matrix inversion. <em>TNNLS</em>,
<em>36</em>(3), 4326–4335. (<a
href="https://doi.org/10.1109/TNNLS.2024.3363241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redundant manipulators have been widely used in various industries whose applications not only improve production efficiency and reduce manual labor but also promote innovation in robotics and artificial intelligence. Kinematic control plays a fundamental and crucial role in robot control. Over the past few decades, numerous motion control schemes have been proposed and applied to trajectory tracking tasks. However, most of these schemes do not consider the introduction of sparsity into the motion control of redundant manipulators, resulting in excessive joint movements, which not only consume extra energy but also increase the risk of unexpected collisions in complex environments. To solve this problem, we transform the issue of increasing the sparsity into a nonconvex optimization problem. Furthermore, a collective neural dynamics for sparse motion planning (CNDSMP) scheme for motion planning of redundant manipulators is proposed. By incorporating sparsity into the control scheme, the excessive joint movements are minimized, leading to improved efficiency and reduced collision risks. Through simulations, comparisons, and physical experiments, the effectiveness and superiority of the proposed scheme are demonstrated.},
  archive      = {J_TNNLS},
  author       = {Long Jin and Jinchuan Zhao and Liangming Chen and Shuai Li},
  doi          = {10.1109/TNNLS.2024.3363241},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4326-4335},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Collective neural dynamics for sparse motion planning of redundant manipulators without hessian matrix inversion},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Focus affinity perception and super-resolution embedding for
multifocus image fusion. <em>TNNLS</em>, <em>36</em>(3), 4311–4325. (<a
href="https://doi.org/10.1109/TNNLS.2024.3367782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the fact that there is a remarkable achievement on multifocus image fusion, most of the existing methods only generate a low-resolution image if the given source images suffer from low resolution. Obviously, a naive strategy is to independently conduct image fusion and image super-resolution. However, this two-step approach would inevitably introduce and enlarge artifacts in the final result if the result from the first step meets artifacts. To address this problem, in this article, we propose a novel method to simultaneously achieve image fusion and super-resolution in one framework, avoiding step-by-step processing of fusion and super-resolution. Since a small receptive field can discriminate the focusing characteristics of pixels in detailed regions, while a large receptive field is more robust to pixels in smooth regions, a subnetwork is first proposed to compute the affinity of features under different types of receptive fields, efficiently increasing the discriminability of focused pixels. Simultaneously, in order to prevent from distortion, a gradient embedding-based super-resolution subnetwork is also proposed, in which the features from the shallow layer, the deep layer, and the gradient map are jointly taken into account, allowing us to get an upsampled image with high resolution. Compared with the existing methods, which implemented fusion and super-resolution independently, our proposed method directly achieves these two tasks in a parallel way, avoiding artifacts caused by the inferior output of image fusion or super-resolution. Experiments conducted on the real-world dataset substantiate the superiority of our proposed method compared with state of the arts.},
  archive      = {J_TNNLS},
  author       = {Huafeng Li and Ming Yuan and Jinxing Li and Yu Liu and Guangming Lu and Yong Xu and Zhengtao Yu and David Zhang},
  doi          = {10.1109/TNNLS.2024.3367782},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4311-4325},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Focus affinity perception and super-resolution embedding for multifocus image fusion},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive intermediate class-wise distribution alignment: A
universal domain adaptation and generalization method for machine fault
diagnosis. <em>TNNLS</em>, <em>36</em>(3), 4296–4310. (<a
href="https://doi.org/10.1109/TNNLS.2024.3376449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many transfer learning methods have been proposed to implement fault transfer diagnosis, and their loss functions are usually composed of task-related losses, distribution distance losses, and correlation regularization losses. The intrinsic parameters and trade-off parameters between losses, however, need to be tuned according to the specific diagnosis tasks; thus, the generalization abilities of these methods in multiple tasks are limited. Besides, the alignment goal of most domain adaptation (DA) mechanisms dynamically changes during the training process, which will result in loss oscillation, slow convergence and poor robustness. To overcome the above-mentioned issues, a novel and simple transfer learning diagnosis method named adaptive intermediate class-wise distribution alignment (AICDA) model is proposed, and it is established via the proposed AICDA mechanism, dynamic intermediate alignment (DIA) adaptive layer and AdaSoftmax loss. The AICDA mechanism develops an adaptive intermediate distribution as the alignment goal of multiple source domains and target domains, and it can simultaneously align the global and class-wise distributions of these domains. The DIA layer is designed to adaptively achieve domain confusion without the distribution distance loss and the correlation regularization loss. Meanwhile, to ensure the classification performance of the AICDA mechanism, AdaSoftmax loss is proposed for boosting the separability of Softmax loss. Finally, in order to evaluate the effectiveness and universality of the AICDA diagnosis model to the most degree, various multisource mixed fault transfer diagnosis tasks of wind turbine planetary gearboxes, including DA and domain generalization (DG), are implemented, and the experimental results indicate that our proposed AICDA model has a higher diagnosis accuracy and a stronger generalization ability than other state-of-the-art transfer learning methods.},
  archive      = {J_TNNLS},
  author       = {Quan Qian and Jun Luo and Yi Qin},
  doi          = {10.1109/TNNLS.2024.3376449},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4296-4310},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive intermediate class-wise distribution alignment: A universal domain adaptation and generalization method for machine fault diagnosis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated graph neural networks: Overview, techniques, and
challenges. <em>TNNLS</em>, <em>36</em>(3), 4279–4295. (<a
href="https://doi.org/10.1109/TNNLS.2024.3360429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have attracted extensive research attention in recent years due to their capability to progress with graph data and have been widely used in practical applications. As societies become increasingly concerned with the need for data privacy protection, GNNs face the need to adapt to this new normal. Besides, as clients in federated learning (FL) may have relationships, more powerful tools are required to utilize such implicit information to boost performance. This has led to the rapid development of the emerging research field of federated GNNs (FedGNNs). This promising interdisciplinary field is highly challenging for interested researchers to grasp. The lack of an insightful survey on this topic further exacerbates the entry difficulty. In this article, we bridge this gap by offering a comprehensive survey of this emerging field. We propose a 2-D taxonomy of the FedGNN literature: 1) the main taxonomy provides a clear perspective on the integration of GNNs and FL by analyzing how GNNs enhance FL training as well as how FL assists GNN training and 2) the auxiliary taxonomy provides a view on how FedGNNs deal with heterogeneity across FL clients. Through discussions of key ideas, challenges, and limitations of existing works, we envision future research directions that can help build more robust, explainable, efficient, fair, inductive, and comprehensive FedGNNs.},
  archive      = {J_TNNLS},
  author       = {Rui Liu and Pengwei Xing and Zichao Deng and Anran Li and Cuntai Guan and Han Yu},
  doi          = {10.1109/TNNLS.2024.3360429},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4279-4295},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Federated graph neural networks: Overview, techniques, and challenges},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning in human activity recognition: A
survey and outlook. <em>TNNLS</em>, <em>36</em>(3), 4267–4278. (<a
href="https://doi.org/10.1109/TNNLS.2024.3360990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) is a popular research field in computer vision that has already been widely studied. However, it is still an active research field since it plays an important role in many current and emerging real-world intelligent systems, like visual surveillance and human–computer interaction. Deep reinforcement learning (DRL) has recently been used to address the activity recognition problem with various purposes, such as finding attention in video data or obtaining the best network structure. DRL-based HAR has only been around for a short time, and it is a challenging, novel field of study. Therefore, to facilitate further research in this area, we have constructed a comprehensive survey on activity recognition methods that incorporate DRL. Throughout the article, we classify these methods according to their shared objectives and delve into how they are ingeniously framed within the DRL framework. As we navigate through the survey, we conclude by shedding light on the prominent challenges and lingering questions that await the attention of future researchers, paving the way for further advancements and breakthroughs in this exciting domain.},
  archive      = {J_TNNLS},
  author       = {Bahareh Nikpour and Dimitrios Sinodinos and Narges Armanfard},
  doi          = {10.1109/TNNLS.2024.3360990},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4267-4278},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning in human activity recognition: A survey and outlook},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic neural network structure: A review for its theories
and applications. <em>TNNLS</em>, <em>36</em>(3), 4246–4266. (<a
href="https://doi.org/10.1109/TNNLS.2024.3377194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic neural network (DNN), in contrast to the static counterpart, offers numerous advantages, such as improved accuracy, efficiency, and interpretability. These benefits stem from the network’s flexible structures and parameters, making it highly attractive and applicable across various domains. As the broad learning system (BLS) continues to evolve, DNNs have expanded beyond deep learning (DL), orienting a more comprehensive range of domains. Therefore, this comprehensive review article focuses on two prominent areas where DNN structures have rapidly developed: 1) DL and 2) broad learning. This article provides an in-depth exploration of the techniques related to dynamic construction and inference. Furthermore, it discusses the applications of DNNs in diverse domains while also addressing open issues and highlighting promising research directions. By offering a comprehensive understanding of DNNs, this article serves as a valuable resource for researchers, guiding them toward future investigations.},
  archive      = {J_TNNLS},
  author       = {Jifeng Guo and C. L. Philip Chen and Zhulin Liu and Xixin Yang},
  doi          = {10.1109/TNNLS.2024.3377194},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4246-4266},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic neural network structure: A review for its theories and applications},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic correlation transfer for heterogeneous domain
adaptation. <em>TNNLS</em>, <em>36</em>(3), 4233–4245. (<a
href="https://doi.org/10.1109/TNNLS.2022.3199619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation (HDA) is expected to achieve effective knowledge transfer from a label-rich source domain to a heterogeneous target domain with scarce labeled data. Most prior HDA methods strive to align the cross-domain feature distributions by learning domain invariant representations without considering the intrinsic semantic correlations among categories, which inevitably results in the suboptimal adaptation performance across domains. Therefore, to address this issue, we propose a novel semantic correlation transfer (SCT) method for HDA, which not only matches the marginal and conditional distributions between domains to mitigate the large domain discrepancy, but also transfers the category correlation knowledge underlying the source domain to target by maximizing the pairwise class similarity across source and target. Technically, the domainwise and classwise centroids (prototypes) are first computed and aligned according to the feature embeddings. Then, based on the derived classwise prototypes, we leverage the cosine similarity of each two classes in both domains to transfer the supervised source semantic correlation knowledge among different categories to target effectively. As a result, the feature transferability and category discriminability can be simultaneously improved during the adaptation process. Comprehensive experiments and ablation studies on standard HDA tasks, such as text-to-image, image-to-image, and text-to-text, have demonstrated the superiority of our proposed SCT against several state-of-the-art HDA methods.},
  archive      = {J_TNNLS},
  author       = {Ying Zhao and Shuang Li and Rui Zhang and Chi Harold Liu and Weipeng Cao and Xizhao Wang and Song Tian},
  doi          = {10.1109/TNNLS.2022.3199619},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4233-4245},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semantic correlation transfer for heterogeneous domain adaptation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel and feature transfer fusion for unsupervised
cross-dataset person reidentification. <em>TNNLS</em>, <em>36</em>(3),
4220–4232. (<a
href="https://doi.org/10.1109/TNNLS.2021.3128269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised cross-dataset person reidentification (Re-ID) has attracted more and more attention, which aims to transfer knowledge of a labeled source domain to an unlabeled target domain. There are two common frameworks: one is pixel-alignment of transferring low-level knowledge, and the other is feature-alignment of transferring high-level knowledge. In this article, we propose a novel recurrent autoencoder (RAE) framework to unify these two kinds of methods and inherit their merits. Specifically, the proposed RAE includes three modules, i.e., a feature-transfer (FT) module, a pixel-transfer (PT) module, and a fusion module. The FT module utilizes an encoder to map source and target images to a shared feature space. In the space, not only features are identity-discriminative but also the gap between source and target features is reduced. The PT module takes a decoder to reconstruct original images with its features. Here, we hope that the images reconstructed from target features are in the source style. Thus, the low-level knowledge can be propagated to the target domain. After transferring both high-and low-level knowledge with the two proposed modules above, we design another bilinear pooling layer to fuse both kinds of knowledge. Extensive experiments on Market-1501, DukeMTMC-ReID, and MSMT17 datasets show that our method significantly outperforms either pixel-alignment or feature-alignment Re-ID methods and achieves new state-of-the-art results.},
  archive      = {J_TNNLS},
  author       = {Yang Yang and Guan’an Wang and Prayag Tiwari and Hari Mohan Pandey and Zhen Lei},
  doi          = {10.1109/TNNLS.2021.3128269},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4220-4232},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pixel and feature transfer fusion for unsupervised cross-dataset person reidentification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global–local multiple granularity learning for
cross-modality visible–infrared person reidentification. <em>TNNLS</em>,
<em>36</em>(3), 4209–4219. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality visible–infrared person reidentification (VI-ReID), which aims to retrieve pedestrian images captured by both visible and infrared cameras, is a challenging but essential task for smart surveillance systems. The huge barrier between visible and infrared images has led to the large cross-modality discrepancy and intraclass variations. Most existing VI-ReID methods tend to learn discriminative modality-sharable features based on either global or part-based representations, lacking effective optimization objectives. In this article, we propose a novel global–local multichannel (GLMC) network for VI-ReID, which can learn multigranularity representations based on both global and local features. The coarse- and fine-grained information can complement each other to form a more discriminative feature descriptor. Besides, we also propose a novel center loss function that aims to simultaneously improve the intraclass cross-modality similarity and enlarge the interclass discrepancy to explicitly handle the cross-modality discrepancy issue and avoid the model fluctuating problem. Experimental results on two public datasets have demonstrated the superiority of the proposed method compared with state-of-the-art approaches in terms of effectiveness.},
  archive      = {J_TNNLS},
  author       = {Liyan Zhang and Guodong Du and Fan Liu and Huawei Tu and Xiangbo Shu},
  doi          = {10.1109/TNNLS.2021.3085978},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4209-4219},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global–Local multiple granularity learning for cross-modality Visible–Infrared person reidentification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VAG: A uniform model for cross-modal visual-audio mutual
generation. <em>TNNLS</em>, <em>36</em>(3), 4196–4208. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering both audio and visual modalities is helpful for understanding a video. In the face of harsh environmental interference or signal packet loss, automatically compensating for audio and vision is a challenging task. We propose a dynamic cross-modal visual-audio mutual generation model (VAMG), which includes audio to visual conversion, visual to audio conversion, audio self-generation, and visual self-generation. VAMG jointly optimizes modal reconstruction and adversarial constraints, effectively solving the problems of structural alignment and signal compensation in incomplete videos. We conducted an instrument-oriented and pose-oriented cross-modal audio-visual mutual generation experiment on the sub-University of Rochester Musical Performance dataset to verify the effectiveness of the model.},
  archive      = {J_TNNLS},
  author       = {Wangli Hao and He Guan and Zhaoxiang Zhang},
  doi          = {10.1109/TNNLS.2022.3161314},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4196-4208},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {VAG: A uniform model for cross-modal visual-audio mutual generation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Region-object relation-aware dense captioning via
transformer. <em>TNNLS</em>, <em>36</em>(3), 4184–4195. (<a
href="https://doi.org/10.1109/TNNLS.2022.3152990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense captioning provides detailed captions of complex visual scenes. While a number of successes have been achieved in recent years, there are still two broad limitations: 1) most existing methods adopt an encoder–decoder framework, where the contextual information is sequentially encoded using long short-term memory (LSTM). However, the forget gate mechanism of LSTM makes it vulnerable when dealing with a long sequence and 2) the vast majority of prior arts consider regions of interests (RoIs) equally important, thus failing to focus on more informative regions. The consequence is that the generated captions cannot highlight important contents of the image, which does not seem natural. To overcome these limitations, in this article, we propose a novel end-to-end transformer-based dense image captioning architecture, termed the transformer-based dense captioner (TDC). TDC learns the mapping between images and their dense captions via a transformer, prioritizing more informative regions. To this end, we present a novel unit, named region-object correlation score unit (ROCSU), to measure the importance of each region, where the relationships between detected objects and the region, alongside the confidence scores of detected objects within the region, are taken into account. Extensive experimental results and ablation studies on the standard dense-captioning datasets demonstrate the superiority of the proposed method to the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zhuang Shao and Jungong Han and Demetris Marnerides and Kurt Debattista},
  doi          = {10.1109/TNNLS.2022.3152990},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4184-4195},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Region-object relation-aware dense captioning via transformer},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep selective fusion of visible and near-infrared images
using unsupervised u-net. <em>TNNLS</em>, <em>36</em>(3), 4172–4183. (<a
href="https://doi.org/10.1109/TNNLS.2022.3142780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low light conditions, visible (VIS) images are of a low dynamic range (low contrast) with severe noise and color, while near-infrared (NIR) images contain clear textures without noise and color. Multispectral fusion of VIS and NIR images produces color images of high quality, rich textures, and little noise by taking both advantages of VIS and NIR images. In this article, we propose the deep selective fusion of VIS and NIR images using unsupervised U-Net. Existing image fusion methods are afflicted with the low contrast in VIS images and flash-like effect in NIR images. Thus, we adopt unsupervised U-Net to achieve deep selective fusion of multiple scale features. Due to the absence of the ground truth, we use unsupervised learning by formulating an energy function as a loss function. To deal with insufficient training data, we perform data augmentation by rotating images and adjusting their intensity. We synthesize training data by degrading clean VIS images and masking clean NIR images using a circle. First, we utilize pretrained visual geometry group (VGG) to extract features from VIS images. Second, we build an encoding network to obtain edge information from NIR images. Finally, we combine all features and feed them into a decoding network for fusion. Experimental results demonstrate that the proposed fusion network produces visually pleasing results with fine details, little noise, and natural color and it is superior to state-of-the-art methods in terms of visual quality and quantitative measurements.},
  archive      = {J_TNNLS},
  author       = {Qihui Han and Cheolkon Jung},
  doi          = {10.1109/TNNLS.2022.3142780},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4172-4183},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep selective fusion of visible and near-infrared images using unsupervised U-net},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilateral cross-modality graph matching attention for
feature fusion in visual question answering. <em>TNNLS</em>,
<em>36</em>(3), 4160–4171. (<a
href="https://doi.org/10.1109/TNNLS.2021.3135655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Answering semantically complicated questions according to an image is challenging in a visual question answering (VQA) task. Although the image can be well represented by deep learning, the question is always simply embedded and cannot well indicate its meaning. Besides, the visual and textual features have a gap for different modalities, it is difficult to align and utilize the cross-modality information. In this article, we focus on these two problems and propose a graph matching attention (GMA) network. First, it not only builds graph for the image but also constructs graph for the question in terms of both syntactic and embedding information. Next, we explore the intramodality relationships by a dual-stage graph encoder and then present a bilateral cross-modality GMA to infer the relationships between the image and the question. The updated cross-modality features are then sent into the answer prediction module for final answer prediction. Experiments demonstrate that our network achieves the state-of-the-art performance on the GQA dataset and the VQA 2.0 dataset. The ablation studies verify the effectiveness of each module in our GMA network.},
  archive      = {J_TNNLS},
  author       = {Jianjian Cao and Xiameng Qin and Sanyuan Zhao and Jianbing Shen},
  doi          = {10.1109/TNNLS.2021.3135655},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4160-4171},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bilateral cross-modality graph matching attention for feature fusion in visual question answering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly aligned feature fusion for multimodal object
detection. <em>TNNLS</em>, <em>36</em>(3), 4145–4159. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve accurate and robust object detection in the real-world scenario, various forms of images are incorporated, such as color, thermal, and depth. However, multimodal data often suffer from the position shift problem, i.e., the image pair is not strictly aligned, making one object has different positions in different modalities. For the deep learning method, this problem makes it difficult to fuse multimodal features and puzzles the convolutional neural network (CNN) training. In this article, we propose a general multimodal detector named aligned region CNN (AR-CNN) to tackle the position shift problem. First, a region feature (RF) alignment module with adjacent similarity constraint is designed to consistently predict the position shift between two modalities and adaptively align the cross-modal RFs. Second, we propose a novel region of interest (RoI) jitter strategy to improve the robustness to unexpected shift patterns. Third, we present a new multimodal feature fusion method that selects the more reliable feature and suppresses the less useful one via feature reweighting. In addition, by locating bounding boxes in both modalities and building their relationships, we provide novel multimodal labeling named KAIST-Paired. Extensive experiments on 2-D and 3-D object detection, RGB-T, and RGB-D datasets demonstrate the effectiveness and robustness of our method.},
  archive      = {J_TNNLS},
  author       = {Lu Zhang and Zhiyong Liu and Xiangyu Zhu and Zhan Song and Xu Yang and Zhen Lei and Hong Qiao},
  doi          = {10.1109/TNNLS.2021.3105143},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4145-4159},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weakly aligned feature fusion for multimodal object detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IRFR-net: Interactive recursive feature-reshaping network
for detecting salient objects in RGB-d images. <em>TNNLS</em>,
<em>36</em>(3), 4132–4144. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using attention mechanisms in saliency detection networks enables effective feature extraction, and using linear methods can promote proper feature fusion, as verified in numerous existing models. Current networks usually combine depth maps with red–green–blue (RGB) images for salient object detection (SOD). However, fully leveraging depth information complementary to RGB information by accurately highlighting salient objects deserves further study. We combine a gated attention mechanism and a linear fusion method to construct a dual-stream interactive recursive feature-reshaping network (IRFR-Net). The streams for RGB and depth data communicate through a backbone encoder to thoroughly extract complementary information. First, we design a context extraction module (CEM) to obtain low-level depth foreground information. Subsequently, the gated attention fusion module (GAFM) is applied to the RGB depth (RGB-D) information to obtain advantageous structural and spatial fusion features. Then, adjacent depth information is globally integrated to obtain complementary context features. We also introduce a weighted atrous spatial pyramid pooling (WASPP) module to extract the multiscale local information of depth features. Finally, global and local features are fused in a bottom-up scheme to effectively highlight salient objects. Comprehensive experiments on eight representative datasets demonstrate that the proposed IRFR-Net outperforms 11 state-of-the-art (SOTA) RGB-D approaches in various evaluation indicators.},
  archive      = {J_TNNLS},
  author       = {Wujie Zhou and Qinling Guo and Jingsheng Lei and Lu Yu and Jenq-Neng Hwang},
  doi          = {10.1109/TNNLS.2021.3105484},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4132-4144},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IRFR-net: Interactive recursive feature-reshaping network for detecting salient objects in RGB-D images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Duality-gated mutual condition network for RGBT tracking.
<em>TNNLS</em>, <em>36</em>(3), 4118–4131. (<a
href="https://doi.org/10.1109/TNNLS.2022.3157594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-quality modalities contain not only a lot of noisy information but also some discriminative features in RGB-Thermal (RGBT) tracking. However, the potentials of low-quality modalities are not well explored in existing RGBT tracking algorithms. In this work, we propose a novel duality-gated mutual condition network to fully exploit the discriminative information of all modalities while suppressing the effects of data noise. In specific, we design a mutual condition module, which takes the discriminative information of a modality as the condition to guide feature learning of target appearance in another modality. Such a module can effectively enhance target representations of all modalities even in the presence of low-quality modalities. To improve the quality of conditions and further reduce data noise, we propose a duality-gated mechanism and integrate it into the mutual condition module. To deal with the tracking failure caused by sudden camera motion, which often occurs in RGBT tracking, we design a resampling strategy based on optical flow. It does not increase much computational cost since we perform optical flow calculation only when the model prediction is unreliable and then execute resampling when the sudden camera motion is detected. Extensive experiments on four RGBT tracking benchmark datasets show that our method performs favorably against the state-of-the-art tracking algorithms.},
  archive      = {J_TNNLS},
  author       = {Andong Lu and Cun Qian and Chenglong Li and Jin Tang and Liang Wang},
  doi          = {10.1109/TNNLS.2022.3157594},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4118-4131},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Duality-gated mutual condition network for RGBT tracking},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning selective sensor fusion for state estimation.
<em>TNNLS</em>, <em>36</em>(3), 4103–4117. (<a
href="https://doi.org/10.1109/TNNLS.2022.3176677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous vehicles and mobile robotic systems are typically equipped with multiple sensors to provide redundancy. By integrating the observations from different sensors, these mobile agents are able to perceive the environment and estimate system states, e.g., locations and orientations. Although deep learning (DL) approaches for multimodal odometry estimation and localization have gained traction, they rarely focus on the issue of robust sensor fusion—a necessary consideration to deal with noisy or incomplete sensor observations in the real world. Moreover, current deep odometry models suffer from a lack of interpretability. To this extent, we propose SelectFusion, an end-to-end selective sensor fusion module that can be applied to useful pairs of sensor modalities, such as monocular images and inertial measurements, depth images, and light detection and ranging (LIDAR) point clouds. Our model is a uniform framework that is not restricted to specific modality or task. During prediction, the network is able to assess the reliability of the latent features from different sensor modalities and to estimate trajectory at both scale and global pose. In particular, we propose two fusion modules—a deterministic soft fusion and a stochastic hard fusion—and offer a comprehensive study of the new strategies compared with trivial direct fusion. We extensively evaluate all fusion strategies both on public datasets and on progressively degraded datasets that present synthetic occlusions, noisy and missing data, and time misalignment between sensors, and we investigate the effectiveness of the different fusion strategies in attending the most reliable features, which in itself provides insights into the operation of the various models.},
  archive      = {J_TNNLS},
  author       = {Changhao Chen and Stefano Rosa and Chris Xiaoxuan Lu and Bing Wang and Niki Trigoni and Andrew Markham},
  doi          = {10.1109/TNNLS.2022.3176677},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4103-4117},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning selective sensor fusion for state estimation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CACNN: Capsule attention convolutional neural networks for
3D object recognition. <em>TNNLS</em>, <em>36</em>(3), 4091–4102. (<a
href="https://doi.org/10.1109/TNNLS.2023.3326606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, view-based approaches, which recognize a 3D object through its projected 2-D images, have been extensively studied and have achieved considerable success in 3D object recognition. Nevertheless, most of them use a pooling operation to aggregate viewwise features, which usually leads to the visual information loss. To tackle this problem, we propose a novel layer called capsule attention layer (CAL) by using attention mechanism to fuse the features expressed by capsules. In detail, instead of dynamic routing algorithm, we use an attention module to transmit information from the lower level capsules to higher level capsules, which obviously improves the speed of capsule networks. In particular, the view pooling layer of multiview convolutional neural network (MVCNN) becomes a special case of our CAL when the trainable weights are chosen on some certain values. Furthermore, based on CAL, we propose a capsule attention convolutional neural network (CACNN) for 3D object recognition. Extensive experimental results on three benchmark datasets demonstrate the efficiency of our CACNN and show that it outperforms many state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Kai Sun and Jiangshe Zhang and Shuang Xu and Zixiang Zhao and Chunxia Zhang and Junmin Liu and Junying Hu},
  doi          = {10.1109/TNNLS.2023.3326606},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4091-4102},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CACNN: Capsule attention convolutional neural networks for 3D object recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking 3-d LiDAR point cloud segmentation.
<em>TNNLS</em>, <em>36</em>(3), 4079–4090. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many point-based semantic segmentation methods have been designed for indoor scenarios, but they struggle if they are applied to point clouds that are captured by a light detection and ranging (LiDAR) sensor in an outdoor environment. In order to make these methods more efficient and robust such that they can handle LiDAR data, we introduce the general concept of reformulating 3-D point-based operations such that they can operate in the projection space. While we show by means of three point-based methods that the reformulated versions are between 300 and 400 times faster and achieve higher accuracy, we furthermore demonstrate that the concept of reformulating 3-D point-based operations allows to design new architectures that unify the benefits of point-based and image-based methods. As an example, we introduce a network that integrates reformulated 3-D point-based operations into a 2-D encoder-decoder architecture that fuses the information from different 2-D scales. We evaluate the approach on four challenging datasets for semantic LiDAR point cloud segmentation and show that leveraging reformulated 3-D point-based operations with 2-D image-based operations achieves very good results for all four datasets.},
  archive      = {J_TNNLS},
  author       = {Shijie Li and Yun Liu and Juergen Gall},
  doi          = {10.1109/TNNLS.2021.3132836},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4079-4090},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethinking 3-D LiDAR point cloud segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polarity loss: Improving visual-semantic alignment for
zero-shot detection. <em>TNNLS</em>, <em>36</em>(3), 4066–4078. (<a
href="https://doi.org/10.1109/TNNLS.2022.3184821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional object detection models require large amounts of training data. In comparison, humans can recognize previously unseen objects by merely knowing their semantic description. To mimic similar behavior, zero-shot object detection (ZSD) aims to recognize and localize “unseen” object instances by using only their semantic information. The model is first trained to learn the relationships between visual and semantic domains for seen objects, later transferring the acquired knowledge to totally unseen objects. This setting gives rise to the need for correct alignment between visual and semantic concepts so that the unseen objects can be identified using only their semantic attributes. In this article, we propose a novel loss function called “polarity loss” that promotes correct visual-semantic alignment for an improved ZSD. On the one hand, it refines the noisy semantic embeddings via metric learning on a “semantic vocabulary” of related concepts to establish a better synergy between visual and semantic domains. On the other hand, it explicitly maximizes the gap between positive and negative predictions to achieve better discrimination between seen, unseen, and background objects. Our approach is inspired by embodiment theories in cognitive science that claim human semantic understanding to be grounded in past experiences (seen objects), related linguistic concepts (word vocabulary), and visual perception (seen/unseen object images). We conduct extensive evaluations on the Microsoft Common Objects in Context (MS-COCO) and Pascal Visual Object Classes (VOC) datasets, showing significant improvements over state of the art. Our code and evaluation protocols available at: https://github.com/salman-h-khan/PL-ZSD_Release.},
  archive      = {J_TNNLS},
  author       = {Shafin Rahman and Salman Khan and Nick Barnes},
  doi          = {10.1109/TNNLS.2022.3184821},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4066-4078},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Polarity loss: Improving visual-semantic alignment for zero-shot detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative mixup networks for zero-shot learning.
<em>TNNLS</em>, <em>36</em>(3), 4054–4065. (<a
href="https://doi.org/10.1109/TNNLS.2022.3142181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning casts light on lacking unseen class data by transferring knowledge from seen classes via a joint semantic space. However, the distributions of samples from seen and unseen classes are usually imbalanced. Many zero-shot learning methods fail to obtain satisfactory results in the generalized zero-shot learning task, where seen and unseen classes are all used for the test. Also, irregular structures of some classes may result in inappropriate mapping from visual features space to semantic attribute space. A novel generative mixup networks with semantic graph alignment is proposed in this article to mitigate such problems. To be specific, our model first attempts to synthesize samples conditioned with class-level semantic information as the prototype to recover the class-based feature distribution from the given semantic description. Second, the proposed model explores a mixup mechanism to augment training samples and improve the generalization ability of the model. Third, triplet gradient matching loss is developed to guarantee the class invariance to be more continuous in the latent space, and it can help the discriminator distinguish the real and fake samples. Finally, a similarity graph is constructed from semantic attributes to capture the intrinsic correlations and guides the feature generation process. Extensive experiments conducted on several zero-shot learning benchmarks from different tasks prove that the proposed model can achieve superior performance over the state-of-the-art generalized zero-shot learning.},
  archive      = {J_TNNLS},
  author       = {Bingrong Xu and Zhigang Zeng and Cheng Lian and Zhengming Ding},
  doi          = {10.1109/TNNLS.2022.3142181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4054-4065},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generative mixup networks for zero-shot learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time semantic segmentation via spatial-detail guided
context propagation. <em>TNNLS</em>, <em>36</em>(3), 4042–4053. (<a
href="https://doi.org/10.1109/TNNLS.2022.3154443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, vision-based computing tasks play an important role in various real-world applications. However, many vision computing tasks, e.g., semantic segmentation, are usually computationally expensive, posing a challenge to the computing systems that are resource-constrained but require fast response speed. Therefore, it is valuable to develop accurate and real-time vision processing models that only require limited computational resources. To this end, we propose the spatial-detail guided context propagation network (SGCPNet) for achieving real-time semantic segmentation. In SGCPNet, we propose the strategy of spatial-detail guided context propagation. It uses the spatial details of shallow layers to guide the propagation of the low-resolution global contexts, in which the lost spatial information can be effectively reconstructed. In this way, the need for maintaining high-resolution features along the network is freed, therefore largely improving the model efficiency. On the other hand, due to the effective reconstruction of spatial details, the segmentation accuracy can be still preserved. In the experiments, we validate the effectiveness and efficiency of the proposed SGCPNet model. On the Cityscapes dataset, for example, our SGCPNet achieves 69.5% mIoU segmentation accuracy, while its speed reaches 178.5 FPS on 768 $\times $ 1536 images on a GeForce GTX 1080 Ti GPU card. In addition, SGCPNet is very lightweight and only contains 0.61 M parameters. The code will be released at https://github.com/zhouyuan888888/SGCPNet.},
  archive      = {J_TNNLS},
  author       = {Shijie Hao and Yuan Zhou and Yanrong Guo and Richang Hong and Jun Cheng and Meng Wang},
  doi          = {10.1109/TNNLS.2022.3154443},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4042-4053},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Real-time semantic segmentation via spatial-detail guided context propagation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAB net: A semantic attention boosting framework for
semantic segmentation. <em>TNNLS</em>, <em>36</em>(3), 4029–4041. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has achieved great progress by effectively fusing features of contextual information. In this article, we propose an end-to-end semantic attention boosting (SAB) framework to adaptively fuse the contextual information iteratively across layers with semantic regularization. Specifically, we first propose a pixelwise semantic attention (SAP) block, with a semantic metric representing the pixelwise category relationship, to aggregate the nonlocal contextual information. In addition, we improve the computation complexity of SAP block from $O(n^{2})$ to $O(n)$ for images with size $n$ . Second, we present a categorywise semantic attention (SAC) block to adaptively balance the nonlocal contextual dependencies and the local consistency with a categorywise weight, overcoming the contextual information confusion caused by the feature imbalance within intra-category. Furthermore, we propose the SAB module to refine the segmentation with SAC and SAP blocks. By applying the SAB module iteratively across layers, our model shrinks the semantic gap and enhances the structure reasoning by fully utilizing the coarse segmentation information. Extensive quantitative evaluations demonstrate that our method significantly improves the segmentation results and achieves superior performance on the PASCAL VOC 2012, Cityscapes, PASCAL Context, and ADE20K datasets.},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Ding and Chaomin Shen and Tieyong Zeng and Yaxin Peng},
  doi          = {10.1109/TNNLS.2022.3144003},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4029-4041},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SAB net: A semantic attention boosting framework for semantic segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Position fusing and refining for clear salient object
detection. <em>TNNLS</em>, <em>36</em>(3), 4019–4028. (<a
href="https://doi.org/10.1109/TNNLS.2022.3213557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilevel feature fusion plays a pivotal role in salient object detection (SOD). High-level features present rich semantic information but lack object position information, whereas low-level features contain object position information but are mixed with noises such as backgrounds. Appropriately addressing the gap between low- and high-level features is important in SOD. We first propose a global position embedding attention (GPEA) module to minimize the discrepancy between multilevel features in this article. We extract the position information by utilizing the semantic information at high-level features to resist noises at low-level features. Object refine attention (ORA) module is introduced to refine features used to predict saliency maps further without any additional supervision and heighten discriminative regions near the salient object, such as boundaries. Moreover, we find that the saliency maps generated by the previous methods contain some blurry regions, and we design a pixel value (PV) loss to help the model generate saliency maps with improved clarity. Experimental results on five commonly used SOD datasets demonstrated that the proposed method is effective and outperforms the state-of-the-art approaches on multiple metrics.},
  archive      = {J_TNNLS},
  author       = {Xing Zhao and Haoran Liang and Ronghua Liang},
  doi          = {10.1109/TNNLS.2022.3213557},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4019-4028},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Position fusing and refining for clear salient object detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical self-distilled feature learning for
fine-grained visual categorization. <em>TNNLS</em>, <em>36</em>(3),
4005–4018. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual categorization (FGVC) relies on hierarchical features extracted by deep convolutional neural networks (CNNs) to recognize closely alike objects. Particularly, shallow layer features containing rich spatial details are vital for specifying subtle differences between objects but are usually inadequately optimized due to gradient vanishing during backpropagation. In this article, hierarchical self-distillation (HSD) is introduced to generate well-optimized CNNs features for accurate fine-grained categorization. HSD inherits from the widely applied deep supervision and implements multiple intermediate losses for reinforced gradients. Besides that, we observe that the hard (one-hot) labels adopted for intermediate supervision hurt the performance of FGVC by enforcing overstrict supervision. As a solution, HSD seeks self-distillation where soft predictions generated by deeper layers of the network are hierarchically exploited to supervise shallow parts. Moreover, self-information entropy loss (SIELoss) is designed in HSD to adaptively soften intermediate predictions and facilitate better convergence. In addition, the gradient detached fusion (GDF) module is incorporated to produce an ensemble result with multiscale features via effective feature fusion. Extensive experiments on four challenging fine-grained datasets show that, with neglectable parameter increase, the proposed HSD framework and the GDF module both bring significant performance gains over different backbones, which also achieves state-of-the-art classification performance.},
  archive      = {J_TNNLS},
  author       = {Yutao Hu and Xiaolong Jiang and Xuhui Liu and Xiaoyan Luo and Yao Hu and Xianbin Cao and Baochang Zhang and Jun Zhang},
  doi          = {10.1109/TNNLS.2021.3124135},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {4005-4018},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical self-distilled feature learning for fine-grained visual categorization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAReID: Grouped and attentive high-order representation
learning for person re-identification. <em>TNNLS</em>, <em>36</em>(3),
3990–4004. (<a
href="https://doi.org/10.1109/TNNLS.2022.3209537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As person parts are frequently misaligned between detected human boxes, an image representation that can handle this part misalignment is required. In this work, we propose an effective grouped attentive re-identification (GAReID) framework to learn part-aligned and background robust representations for person re-identification (ReID). Specifically, the GAReID framework consists of grouped high-order pooling (GHOP) and attentive high-order pooling (AHOP) layers, which generate high-order image and foreground features, respectively. In addition, a novel grouped Kronecker product (GKP) is proposed to use both channel group and shuffle strategies for high-order feature compression, while promoting the representational capabilities of compressed high-order features. We show that our method derives from an interpretable motivation and elegantly reduces part misalignments without using landmark detection or feature partition. This article theoretically and experimentally demonstrates the superiority of the GAReID framework, achieving state-of-the-art performance on various person ReID datasets.},
  archive      = {J_TNNLS},
  author       = {Pingyu Wang and Fei Su and Zhicheng Zhao and Yanyun Zhao and Nikolaos V. Boulgouris},
  doi          = {10.1109/TNNLS.2022.3209537},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3990-4004},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GAReID: Grouped and attentive high-order representation learning for person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Go deep or broad? Exploit hybrid network architecture for
weakly supervised object classification and localization.
<em>TNNLS</em>, <em>36</em>(3), 3976–3989. (<a
href="https://doi.org/10.1109/TNNLS.2022.3225180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object classification and localization are learned object classes and locations using only image-level labels, as opposed to bounding box annotations. Conventional deep convolutional neural network (CNN)-based methods activate the most discriminate part of an object in feature maps and then attempt to expand feature activation to the whole object, which leads to deteriorating the classification performance. In addition, those methods only use the most semantic information in the last feature map, while ignoring the role of shallow features. So, it remains a challenge to enhance classification and localization performance with a single frame. In this article, we propose a novel hybrid network, namely deep and broad hybrid network (DB-HybridNet), which combines deep CNNs with a broad learning network to learn discriminative and complementary features from different layers, and then integrates multilevel features (i.e., high-level semantic features and low-level edge features) in a global feature augmentation module. Importantly, we exploit different combinations of deep features and broad learning layers in DB-HybridNet and design an iterative training algorithm based on gradient descent to ensure the hybrid network work in an end-to-end framework. Through extensive experiments on caltech-UCSD birds (CUB)-200 and imagenet large scale visual recognition challenge (ILSVRC) 2016 datasets, we achieve state-of-the-art classification and localization performance.},
  archive      = {J_TNNLS},
  author       = {Shan Gao and Guangqian Guo and Hanqiao Huang and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2022.3225180},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3976-3989},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Go deep or broad? exploit hybrid network architecture for weakly supervised object classification and localization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DONet: Dual-octave network for fast MR image reconstruction.
<em>TNNLS</em>, <em>36</em>(3), 3965–3975. (<a
href="https://doi.org/10.1109/TNNLS.2021.3090303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance (MR) image acquisition is an inherently prolonged process, whose acceleration has long been the subject of research. This is commonly achieved by obtaining multiple undersampled images, simultaneously, through parallel imaging. In this article, we propose the dual-octave network (DONet), which is capable of learning multiscale spatial-frequency features from both the real and imaginary components of MR data, for parallel fast MR image reconstruction. More specifically, our DONet consists of a series of dual-octave convolutions (Dual-OctConvs), which are connected in a dense manner for better reuse of features. In each Dual-OctConv, the input feature maps and convolutional kernels are first split into two components (i.e., real and imaginary) and then divided into four groups according to their spatial frequencies. Then, our Dual-OctConv conducts intragroup information updating and intergroup information exchange to aggregate the contextual information across different groups. Our framework provides three appealing benefits: 1) it encourages information interaction and fusion between the real and imaginary components at various spatial frequencies to achieve richer representational capacity; 2) the dense connections between the real and imaginary groups in each Dual-OctConv make the propagation of features more efficient by feature reuse; and 3) DONet enlarges the receptive field by learning multiple spatial-frequency features of both the real and imaginary components. Extensive experiments on two popular datasets (i.e., clinical knee and fastMRI), under different undersampling patterns and acceleration factors, demonstrate the superiority of our model in accelerated parallel MR image reconstruction.},
  archive      = {J_TNNLS},
  author       = {Chun-Mei Feng and Zhanyuan Yang and Huazhu Fu and Yong Xu and Jian Yang and Ling Shao},
  doi          = {10.1109/TNNLS.2021.3090303},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3965-3975},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DONet: Dual-octave network for fast MR image reconstruction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based real image restoration. <em>TNNLS</em>,
<em>36</em>(3), 3954–3964. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks perform better on images containing spatially invariant degradations, also known as synthetic degradations; however, their performance is limited on real-degraded photographs and requires multiple-stage network modeling. To advance the practicability of restoration algorithms, this article proposes a novel single-stage blind real image restoration network ( $\text{R}^{2}$ Net) by employing a modular architecture. We use a residual on the residual structure to ease low-frequency information flow and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality for four restoration tasks, i.e., denoising, super-resolution, raindrop removal, and JPEG compression on 11 real degraded datasets against more than 30 state-of-the-art algorithms, demonstrates the superiority of our $\text{R}^{2}$ Net. We also present the comparison on three synthetically generated degraded datasets for denoising to showcase our method’s capability on synthetics denoising. The codes, trained models, and results are available on https://github.com/saeed-anwar/R2Net.},
  archive      = {J_TNNLS},
  author       = {Saeed Anwar and Nick Barnes and Lars Petersson},
  doi          = {10.1109/TNNLS.2021.3131739},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3954-3964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-based real image restoration},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality-driven regularization for deep learning networks and
its application to industrial soft sensors. <em>TNNLS</em>,
<em>36</em>(3), 3943–3953. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of data collection in industrial processes has led to a renewed emphasis on the development of data-driven soft sensors. A key step in building an accurate, reliable soft sensor is feature representation. Deep networks have shown great ability to learn hierarchical data features using unsupervised pretraining and supervised fine-tuning. For typical deep networks like stacked auto-encoder (SAE), the pretraining stage is unsupervised, in which some important information related to quality variables may be discarded. In this article, a new quality-driven regularization (QR) is proposed for deep networks to learn quality-related features from industrial process data. Specifically, a QR-based SAE (QR-SAE) is developed, which changes the loss function to control the weights of the different input variables. By choosing an appropriate inductive bias for the weight matrix, the model provides quality-relevant information for predictive modeling. Finally, the proposed QR-SAE is used to predict the quality of a real industrial hydrocracking process. Comparative experiments show that QR-SAE can extract quality-related features and achieve accurate prediction performance.},
  archive      = {J_TNNLS},
  author       = {Chen Ou and Hongqiu Zhu and Yuri A. W. Shardt and Lingjian Ye and Xiaofeng Yuan and Yalin Wang and Chunhua Yang},
  doi          = {10.1109/TNNLS.2022.3144162},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3943-3953},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quality-driven regularization for deep learning networks and its application to industrial soft sensors},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adding before pruning: Sparse filter fusion for deep
convolutional neural networks via auxiliary attention. <em>TNNLS</em>,
<em>36</em>(3), 3930–3942. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filter pruning is a significant feature selection technique to shrink the existing feature fusion schemes (especially on convolution calculation and model size), which helps to develop more efficient feature fusion models while maintaining state-of-the-art performance. In addition, it reduces the storage and computation requirements of deep neural networks (DNNs) and accelerates the inference process dramatically. Existing methods mainly rely on manual constraints such as normalization to select the filters. A typical pipeline comprises two stages: first pruning the original neural network and then fine-tuning the pruned model. However, choosing a manual criterion can be somehow tricky and stochastic. Moreover, directly regularizing and modifying filters in the pipeline suffer from being sensitive to the choice of hyperparameters, thus making the pruning procedure less robust. To address these challenges, we propose to handle the filter pruning issue through one stage: using an attention-based architecture that adaptively fuses the filter selection with filter learning in a unified network. Specifically, we present a pruning method named adding before pruning (ABP) to make the model focus on the filters of higher significance by training instead of man-made criteria such as norm, rank, etc. First, we add an auxiliary attention layer into the original model and set the significance scores in this layer to be binary. Furthermore, to propagate the gradients in the auxiliary attention layer, we design a specific gradient estimator and prove its effectiveness for convergence in the graph flow through mathematical derivation. In the end, to relieve the dependence on the complicated prior knowledge for designing the thresholding criterion, we simultaneously prune and train the filters to automatically eliminate network redundancy with recoverability. Extensive experimental results on the two typical image classification benchmarks, CIFAR-10 and ILSVRC-2012, illustrate that the proposed approach performs favorably against previous state-of-the-art filter pruning algorithms.},
  archive      = {J_TNNLS},
  author       = {Guanzhong Tian and Yiran Sun and Yuang Liu and Xianfang Zeng and Mengmeng Wang and Yong Liu and Jiangning Zhang and Jun Chen},
  doi          = {10.1109/TNNLS.2021.3106917},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3930-3942},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adding before pruning: Sparse filter fusion for deep convolutional neural networks via auxiliary attention},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modulated convolutional networks. <em>TNNLS</em>,
<em>36</em>(3), 3916–3929. (<a
href="https://doi.org/10.1109/TNNLS.2021.3060830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the deep convolutional neural network (DCNN) has achieved overwhelming success in various vision tasks, its heavy computational and storage overhead hinders the practical use of resource-constrained devices. Recently, compressing DCNN models has attracted increasing attention, where binarization-based schemes have generated great research popularity due to their high compression rate. In this article, we propose modulated convolutional networks (MCNs) to obtain binarized DCNNs with high performance. We lead a new architecture in MCNs to efficiently fuse the multiple features and achieve a similar performance as the full-precision model. The calculation of MCNs is theoretically reformulated as a discrete optimization problem to build binarized DCNNs, for the first time, which jointly consider the filter loss, center loss, and softmax loss in a unified framework. Our MCNs are generic and can decompose full-precision filters in DCNNs, e.g., conventional DCNNs, VGG, AlexNet, ResNets, or Wide-ResNets, into a compact set of binarized filters which are optimized based on a projection function and a new updated rule during the backpropagation. Moreover, we propose modulation filters (M-Filters) to recover filters from binarized ones, which lead to a specific architecture to calculate the network model. Our proposed MCNs substantially reduce the storage cost of convolutional filters by a factor of 32 with a comparable performance to the full-precision counterparts, achieving much better performance than other state-of-the-art binarized models.},
  archive      = {J_TNNLS},
  author       = {Baochang Zhang and Runqi Wang and Xiaodi Wang and Jungong Han and Rongrong Ji},
  doi          = {10.1109/TNNLS.2021.3060830},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3916-3929},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modulated convolutional networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ResDNet: Efficient dense multi-scale representations with
residual learning for high-level vision tasks. <em>TNNLS</em>,
<em>36</em>(3), 3904–3915. (<a
href="https://doi.org/10.1109/TNNLS.2022.3169779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep feature fusion plays a significant role in the strong learning ability of convolutional neural networks (CNNs) for computer vision tasks. Recently, works continually demonstrate the advantages of efficient aggregation strategy and some of them refer to multiscale representations. In this article, we describe a novel network architecture for high-level computer vision tasks where densely connected feature fusion provides multiscale representations for the residual network. We term our method the ResDNet which is a simple and efficient backbone made up of sequential ResDNet modules containing the variants of dense blocks named sliding dense blocks (SDBs). Compared with DenseNet, ResDNet enhances the feature fusion and reduces the redundancy by shallower densely connected architectures. Experimental results on three classification benchmarks including CIFAR-10, CIFAR-100, and ImageNet demonstrate the effectiveness of ResDNet. ResDNet always outperforms DenseNet using much less computation on CIFAR-100. On ImageNet, ResDNet-B-129 achieves 1.94% and 0.89% top-1 accuracy improvement over ResNet-50 and DenseNet-201 with similar complexity. Besides, ResDNet with more than 1000 layers achieves remarkable accuracy on CIFAR compared with other state-of-the-art results. Based on MMdetection implementation of RetinaNet, ResDNet-B-129 improves mAP from 36.3 to 39.5 compared with ResNet-50 on COCO dataset.},
  archive      = {J_TNNLS},
  author       = {Yuanduo Hong and Huihui Pan and Yisong Jia and Weichao Sun and Huijun Gao},
  doi          = {10.1109/TNNLS.2022.3169779},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3904-3915},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {ResDNet: Efficient dense multi-scale representations with residual learning for high-level vision tasks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision fusion networks for image classification.
<em>TNNLS</em>, <em>36</em>(3), 3890–3903. (<a
href="https://doi.org/10.1109/TNNLS.2022.3196129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks, in which each layer receives features from the previous layer(s) and then aggregates/abstracts higher level features from them, are widely adopted for image classification. To avoid information loss during feature aggregation/abstraction and fully utilize lower layer features, we propose a novel decision fusion module (DFM) for making an intermediate decision based on the features in the current layer and then fuse its results with the original features before passing them to the next layers. This decision is devised to determine an auxiliary category corresponding to the category at a higher hierarchical level, which can, thus, serve as category-coherent guidance for later layers. Therefore, by stacking a collection of DFMs into a classification network, the generated decision fusion network is explicitly formulated to progressively aggregate/abstract more discriminative features guided by these decisions and then refine the decisions based on the newly generated features in a layer-by-layer manner. Comprehensive results on four benchmarks validate that the proposed DFM can bring significant improvements for various common classification networks at a minimal additional computational cost and are superior to the state-of-the-art decision fusion-based methods. In addition, we demonstrate the generalization ability of the DFM to object detection and semantic segmentation.},
  archive      = {J_TNNLS},
  author       = {Keke Tang and Yuexin Ma and Dingruibo Miao and Peng Song and Zhaoquan Gu and Zhihong Tian and Wenping Wang},
  doi          = {10.1109/TNNLS.2022.3196129},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3890-3903},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decision fusion networks for image classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial special issue on effective feature fusion in
deep neural networks. <em>TNNLS</em>, <em>36</em>(3), 3886–3889. (<a
href="https://doi.org/10.1109/TNNLS.2025.3525946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TNNLS},
  author       = {Yanwei Pang and Fahad Shahbaz Khan and Xin Lu and Fabio Cuzzolin},
  doi          = {10.1109/TNNLS.2025.3525946},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {3},
  number       = {3},
  pages        = {3886-3889},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on effective feature fusion in deep neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
