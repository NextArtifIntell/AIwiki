<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms---20">THMS - 20</h2>
<ul>
<li><details>
<summary>
(2025). Time-based protocol for continuous action iterated dilemma
in information lossy networks. <em>THMS</em>, <em>55</em>(2), 315–321.
(<a href="https://doi.org/10.1109/THMS.2025.3532598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel prescribed time-based method for analyzing the convergence of evolutionary game dynamics in an information lossy network. Traditional game theory limits players to two choices, i.e., either cooperation or defection. However, player behavior in real-world scenarios is often multidimensional and complex; therefore, this work employs a continuous action iterated dilemma that allows players to choose a wider range of strategies. Moreover, traditional convergence analysis often relies on Jacobian matrices, which entail complex derivations. In contrast, the proposed strategy employs a time generator-based protocol that achieves agreement between all the players at a prescribed time, explicitly set by the user through a time parameter within the protocol. A comprehensive Lyapunov analysis affirms the prescribed time convergence even when the network is exposed to information loss during data transfer. Numerical simulations illustrate that the proposed scheme leads to a faster agreement at the preassigned time and with a better resilience performance compared to existing methods.},
  archive      = {J_THMS},
  author       = {Syed Muhammad Amrr and Mohamed Zaery and S. M. Suhail Hussain and Mohammad A. Abido},
  doi          = {10.1109/THMS.2025.3532598},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {315-321},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Time-based protocol for continuous action iterated dilemma in information lossy networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A single-camera method for estimating lift asymmetry angles
using deep learning computer vision algorithms. <em>THMS</em>,
<em>55</em>(2), 309–314. (<a
href="https://doi.org/10.1109/THMS.2025.3539187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computer vision (CV) method to automatically measure the revised NIOSH lifting equation asymmetry angle (A) from a single camera is described and tested. A laboratory study involving ten participants performing various lifts was used to estimate A in comparison to ground truth joint coordinates obtained using 3-D motion capture (MoCap). To address challenges, such as obstructed views and limitations in camera placement in real-world scenarios, the CV method utilized video-derived coordinates from a selected set of landmarks. A 2-D pose estimator (HR-Net) detected landmark coordinates in each video frame, and a 3-D algorithm (VideoPose3D) estimated the depth of each 2-D landmark by analyzing its trajectories. The mean absolute precision error for the CV method, compared to MoCap measurements using the same subset of landmarks for estimating A, was 6.25° (SD = 10.19°, N = 360). The mean absolute accuracy error of the CV method, compared against conventional MoCap landmark markers was 9.45° (SD = 14.01°, N = 360).},
  archive      = {J_THMS},
  author       = {Zhengyang Lou and Zitong Zhan and Huan Xu and Yin Li and Yu Hen Hu and Ming-Lun Lu and Dwight M. Werren and Robert G. Radwin},
  doi          = {10.1109/THMS.2025.3539187},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {309-314},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A single-camera method for estimating lift asymmetry angles using deep learning computer vision algorithms},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time series signal analysis with information granulation
based on permutation entropy: An application to electroencephalography
signals. <em>THMS</em>, <em>55</em>(2), 300–308. (<a
href="https://doi.org/10.1109/THMS.2025.3538098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we reported a novel granulation method composed of complexity information based on permutation entropy (PeEn). This method aims to recognize the electroencephalography (EEG) patterns using this proposed granulation method. First, we define the complexity information for granular computing by a technique with fast calculation, i.e., PeEn. Then, the information granule can be constructed based on the time domain information, which completes complexity information. Together with the support vector machine algorithm, the proposed granulation method outperformed the existing classification methods in accuracy. It is utilized by classifying three motor imaginary EEG signals. Two of them are binary-class datasets, i.e., one dataset includes two-hand actions, and another includes hand and foot actions. The third dataset is multiclass, including two hands and two feet actions. In addition, the proposed granulation method overcomes the difficulties in cross-individual cases when classifying the EEG signals with a higher accuracy than the existing methods. Meanwhile, this classification procedure makes it interpretable and has a high performance.},
  archive      = {J_THMS},
  author       = {Youpeng Yang and Sanghyuk Lee and Haolan Zhang and Witold Pedrycz},
  doi          = {10.1109/THMS.2025.3538098},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {300-308},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Time series signal analysis with information granulation based on permutation entropy: An application to electroencephalography signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A miner mental state evaluation scheme with decision level
fusion based on multidomain EEG information. <em>THMS</em>,
<em>55</em>(2), 289–299. (<a
href="https://doi.org/10.1109/THMS.2025.3538162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been proven that electroencephalography (EEG) is an effective method for evaluating an individual&#39;s mental state. However, when it comes to the evaluation of miners&#39; mental state, there are still some issues with missing EEG dataset and unsatisfactory evaluation accuracy. Therefore, this article proposes a miner mental state evaluation scheme with decision-level fusion based on multidomain EEG information. First, in the comprehensive lab for coal-related programs of Xi&#39;an University of Science and Technology, the coal mine environment is simulated, and a realistic EEG dataset is constructed. Second, the multidomain features are extracted to represent abundant information in time, frequency, time-frequency, and space domain. These features with low dimension are classified adopting support vector machine (SVM), k-nearest neighbor (kNN), and back propagation (BP) network to obtain the optimal evaluation submodel (four domains corresponding to four submodels). Finally, based on the state probabilities provided by the optimal evaluation submodel, we adopt stack fusion and an improved Yager rule to fuse four submodels in order to find the most suitable fusion algorithm. The experimental results demonstrate that the average accuracy can reach 93.19% on the self-built dataset when utilizing the improved Yager rule with weight, and it realizes a better evaluation accuracy.},
  archive      = {J_THMS},
  author       = {Hongguang Pan and Shiyu Tong and Haoqian Song and Xin Chu},
  doi          = {10.1109/THMS.2025.3538162},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {289-299},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A miner mental state evaluation scheme with decision level fusion based on multidomain EEG information},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual interfaces to mitigate eye problems in a virtual
environment via triggering eye blinking and movement. <em>THMS</em>,
<em>55</em>(2), 278–288. (<a
href="https://doi.org/10.1109/THMS.2025.3542452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase of virtual reality (VR) applications in daily life, protecting the comfort and health of VR users has become increasingly important. The immersive nature of VR often results in decreased eye blinking and movement, putting users at risk of developing conditions such as dry eye syndrome and eye strain. In this article, we propose visual interfaces to induce temporary eye blinks or movements by drawing users&#39; attention temporarily in order to mitigate the negative effects of VR on eye health. Our proposed interfaces can induce eye blinking and movement, which are known to mitigate eye problems in VR. The experimental results confirmed that our interfaces increase the frequency of eye blinking and movement in VR users.},
  archive      = {J_THMS},
  author       = {Jongwook Jeong and Myeongseok Kwak and HyeongYeop Kang},
  doi          = {10.1109/THMS.2025.3542452},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {278-288},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Visual interfaces to mitigate eye problems in a virtual environment via triggering eye blinking and movement},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficacy assessments of virtual reality systems for
immersive consumer testing—two case studies with tortilla chip
evaluations. <em>THMS</em>, <em>55</em>(2), 266–277. (<a
href="https://doi.org/10.1109/THMS.2024.3524916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sensory science, the use of immersive technologies has gained popularity for their ability to restore relevant contextual factors during consumer testing and overcome the low ecological validity of controlled laboratory environments. Despite this, there is scant literature evaluating the effectiveness of immersive technologies in facilitating virtual product evaluation experiences; this is especially true with virtual reality (VR) headsets and the unique technical challenges associated with this technology. To fill this gap, we assessed virtual presence, system usability, engagement, and ease of task completion, in subjects using two iterations of a VR application (controllers or hand tracking) designed to address the major limitations of current systems. Results revealed that both systems exceeded the benchmark usability score of 68. System 1 (controllers) performed better for interactions with the virtual tablet interface to answer questions, whereas interactions with the food objects were easier using System 2 (hand tracking). Participants also experienced a high sense of virtual presence using both systems. When measured in System 2, a high level of subject engagement during the immersive product evaluations was observed. These studies indicate that collecting both quantitative and qualitative feedback on VR systems can provide useful insights and directions for application optimization to ensure valid investigation of context effects in future research.},
  archive      = {J_THMS},
  author       = {Kym K. W. Man and Jeremy A. Patterson and Christopher T. Simons},
  doi          = {10.1109/THMS.2024.3524916},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {266-277},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Efficacy assessments of virtual reality systems for immersive consumer Testing—Two case studies with tortilla chip evaluations},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time myoelectric-based neural-drive decoding for
concurrent and continuous control of robotic finger forces.
<em>THMS</em>, <em>55</em>(2), 256–265. (<a
href="https://doi.org/10.1109/THMS.2025.3532209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural or muscular injuries, such as due to amputation, spinal cord injury, and stroke, can affect hand functions, profoundly impacting independent living. This has motivated the advancement of cutting-edge assistive robotic hands. However, unintuitive myoelectric control of these devices remains challenging, which limits the clinical translation of these devices. Accordingly, we developed a robust motor-intent decoding approach to continuously predict the intended fingertip forces of single and multiple fingers in real time. We used population motor neuron discharge activities (i.e., neural drive from brain to spinal cord) decoded from a high-density surface electromyogram (HD-sEMG) signals as the control signals instead of the conventional global sEMG features. To enable real-time neural-drive prediction, we employed a convolutional neural network model to establish the mapping from global HD-sEMG features to finger-specific neural-drive signals, which were then employed for continuous and real-time control of three prosthetic fingers (index, middle, and ring). As a result, the neural-drive-based approach can decode the motor intent of single-finger and multifinger forces with significantly lower force estimation errors than that obtained using the global HD-sEMG-amplitude approach. Besides, the force prediction accuracy was consistent over time and demonstrated strong robustness to signal interference. Our network-based decoder can also achieve better finger isolation with minimal forces predicted in unintended fingers. Our work demonstrates that the accurate and robust finger force control could be achieved through this new decoding approach. The outcomes offer an efficient intent prediction approach that allows users to have intuitive control of prosthetic fingertip forces in a dexterous way.},
  archive      = {J_THMS},
  author       = {Long Meng and Luis Vargas and Derek G. Kamper and Xiaogang Hu},
  doi          = {10.1109/THMS.2025.3532209},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {256-265},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Real-time myoelectric-based neural-drive decoding for concurrent and continuous control of robotic finger forces},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human comfort index estimation in industrial human–robot
collaboration task. <em>THMS</em>, <em>55</em>(2), 246–255. (<a
href="https://doi.org/10.1109/THMS.2025.3530530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective human–robot collaboration (HRC) requires robots to understand and adapt to humans&#39; psychological states. This research presents a novel approach to quantitatively measure human comfort levels during HRC through the development of two metrics: a comfortability index (CI) and an uncomfortability index (UnCI). We conducted HRC experiments where participants performed assembly tasks while the robot&#39;s behavior was systematically varied. Participants&#39; subjective responses (including surprise, anxiety, boredom, calmness, and comfortability ratings) were collected alongside physiological signals, including electrocardiogram, galvanic skin response, and pupillometry data. We propose two novel approaches for estimating CI/UnCI: an adaptation of the emotion circumplex model that maps comfort levels to the arousal–valence space, and a kernel density estimation model trained on physiological data. Time-domain features were extracted from the physiological signals and used to train machine learning models for real-time comfort levels estimation. Our results demonstrate that the proposed approaches can effectively estimate human comfort levels from physiological signals alone, with the circumplex model showing particular promise in detecting high discomfort states. This work enables real-time measurement of human comfort during HRC, providing a foundation for developing more adaptive and human-aware collaborative robots.},
  archive      = {J_THMS},
  author       = {Celal Savur and Jamison Heard and Ferat Sahin},
  doi          = {10.1109/THMS.2025.3530530},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {246-255},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human comfort index estimation in industrial Human–Robot collaboration task},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiOpen: A robust wi-fi-based open-set gesture recognition
framework. <em>THMS</em>, <em>55</em>(2), 234–245. (<a
href="https://doi.org/10.1109/THMS.2025.3532910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition. However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training. This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known class during testing. To address this issue, we propose WiOpen, a robust Wi-Fi-based open-set gesture recognition (OSGR) framework. Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data. Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise. To tackle these challenges, WiOpen adopts a twofold approach to eliminate uncertainty and define precise decision boundaries. Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the channel state information (CSI) ratio. Next, it designs the OSGR network based on an uncertainty quantification method. Throughout the learning process, this network effectively mitigates uncertainty stemming from domains. Ultimately, the network leverages relationships among samples&#39; neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR. Comprehensive experiments on publicly accessible datasets confirm WiOpen&#39;s effectiveness.},
  archive      = {J_THMS},
  author       = {Xiang Zhang and Jinyang Huang and Huan Yan and Yuanhao Feng and Peng Zhao and Guohang Zhuang and Zhi Liu and Bin Liu},
  doi          = {10.1109/THMS.2025.3532910},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {234-245},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {WiOpen: A robust wi-fi-based open-set gesture recognition framework},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global-local image perceptual score (GLIPS): Evaluating
photorealistic quality of AI-generated images. <em>THMS</em>,
<em>55</em>(2), 223–233. (<a
href="https://doi.org/10.1109/THMS.2025.3527397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the global-local image perceptual score (GLIPS), an image metric designed to assess the photorealistic image quality of AI-generated images with a high degree of alignment to human visual perception. Traditional metrics such as Fréchet inception distance (FID) and kernel inception distance scores do not align closely with human evaluations. The proposed metric incorporates advanced transformer-based attention mechanisms to assess local similarity and maximum mean discrepancy to evaluate global distributional similarity. To evaluate the performance of GLIPS, we conducted a human study on photorealistic image quality. Comprehensive tests across various generative models demonstrate that GLIPS consistently outperforms existing metrics like FID, structural similarity index measure, and multiscale structural similarity index measure in terms of correlation with human scores. In addition, we introduce the interpolative binning scale, a refined scaling method that enhances the interpretability of metric scores by aligning them more closely with human evaluative standards. The proposed metric and scaling approach not only provide more reliable assessments of AI-generated images but also suggest pathways for future enhancements in image generation technologies.},
  archive      = {J_THMS},
  author       = {Memoona Aziz and Umair Rehman and Muhammad Umair Danish and Katarina Grolinger},
  doi          = {10.1109/THMS.2025.3527397},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {223-233},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Global-local image perceptual score (GLIPS): Evaluating photorealistic quality of AI-generated images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chatbot dialog design for improved human performance in
domain knowledge discovery. <em>THMS</em>, <em>55</em>(2), 207–222. (<a
href="https://doi.org/10.1109/THMS.2024.3514742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of machine learning (ML) has led to the widespread adoption of developing task-oriented dialog systems for scientific applications (e.g., science gateways) where voluminous information sources are retrieved and curated for domain users. Yet, there still exists a challenge in designing chatbot dialog systems that achieve widespread diffusion among scientific communities. In this article, we propose a novel Vidura advisor design framework (VADF) to develop dialog system designs for information retrieval (IR) and question-answering (QA) tasks, while enabling the quantification of system utility based on human performance in diverse application environments. We adopt a socio-technical approach in our framework for designing dialog systems by utilizing domain expert feedback, which features a sparse retriever for enabling accurate responses in QA settings using linear interpolation smoothing. We apply our VADF for an exemplar science gateway, viz. KnowCOVID-19, to conduct experiments that demonstrate the utility of dialog systems based on IR and QA performance, application utility, and perceived adoption. Experimental results show our VADF approach significantly improves IR performance against retriever baselines (up to 5% increase) and QA performance against large language models (LLMs) such as ChatGPT (up to 43% increase) on scientific literature datasets. In addition, through a usability survey, we observe that measuring application utility and human performance when applying VADF to KnowCOVID-19 translates to an increase in perceived community adoption.},
  archive      = {J_THMS},
  author       = {Roland Oruche and Xiyao Cheng and Zian Zeng and Audrey Vazzana and MD Ashraful Goni and Bruce Wang Shibo and Sai Keerthana Goruganthu and Kerk Kee and Prasad Calyam},
  doi          = {10.1109/THMS.2024.3514742},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {207-222},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Chatbot dialog design for improved human performance in domain knowledge discovery},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy adaptive controller of a wearable assistive upper limb
exoskeleton using a disturbance observer. <em>THMS</em>, <em>55</em>(2),
197–206. (<a href="https://doi.org/10.1109/THMS.2025.3529759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motivation behind the development of a wearable assistive upper limb exoskeleton robot was to provide comprehensive multijoint therapy by assisting physiotherapists in enhancing the recovery of hemiplegic patients. However, the controlling of an upper limb exoskeleton for rehabilitation is a challenging task because of its nonlinear characteristics. This article presents a novel fuzzy adaptive controller that utilizes a high-dimensional integral-type Lyapunov function for a wearable assistive upper limb exoskeleton. A disturbance observer had been used to tackle uncertainties in the exoskeleton&#39;s dynamic model, thereby enhancing the tracking performance of the joints. The aim of this control scheme was to overcome unknown parameters in the dynamic model. The performance of the adaptive controller was validated through human interactive experiments and periodically repeated reference trajectory tests. The results demonstrated that the proposed fuzzy adaptive controller, with the inclusion of a disturbance observer, could effectively compensate for uncertain disturbances and could achieve efficient tracking of the reference trajectory. The statistical analysis revealed that the fuzzy adaptive controller performed 45%, 44%, and 31% less in average error compared to adaptive conventional controllers. The findings ascertained the potential of the proposed controller in improving the recovery of motor functions of hemiplegic patients.},
  archive      = {J_THMS},
  author       = {Mohammad Soleimani Amiri and Rizauddin Ramli},
  doi          = {10.1109/THMS.2025.3529759},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {197-206},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fuzzy adaptive controller of a wearable assistive upper limb exoskeleton using a disturbance observer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing a passive shoulder exoskeleton in a production
plant: A longitudinal observation of its effects on workers.
<em>THMS</em>, <em>55</em>(2), 185–196. (<a
href="https://doi.org/10.1109/THMS.2025.3536199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occupational exoskeletons have the potential to prevent work-related musculoskeletal disorders. Their widespread adoption should be promoted by investigating their long-term innocuity, sustained effectiveness, and practicability. This article presents a six-months longitudinal study exploring effects of an arm support exoskeleton (ASE) on six male workers, examining potential side effects, ASE&#39;s effectiveness, and its integration into daily work practices. Monthly clinical visits were scheduled to monitor workers’ health. Effectiveness, usability and acceptance metrics were collected at the beginning of the study and after six months. No side effects were found in clinical metrics during the study. Significant reductions, consistent overtime, were observed in shoulder muscle activity (up to 30%) and in effort perception-related metrics (up to 2.4 out of 10 points). Usage time settled around 10% of the monthly work-shift and gradually decreased possibly due to external factors (e.g., social, motivational, and seasonal factors) beyond researchers&#39; control. Results encourage the continuation of similar investigations to strengthen these findings and promote the use of occupational exoskeletons.},
  archive      = {J_THMS},
  author       = {Andrea Parri and Ilaria Pacifico and Eleonora Guanziroli and Federica Aprigliano and Silverio Taglione and Francesco Giovacchini and Francesco Saverio Violante and Franco Molteni and Nicola Vitiello and Simona Crea},
  doi          = {10.1109/THMS.2025.3536199},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {185-196},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Introducing a passive shoulder exoskeleton in a production plant: A longitudinal observation of its effects on workers},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlations between biomechanical variables and subjective
measures of satisfaction while using a passive upper-limb exoskeleton
for overhead tasks in the field. <em>THMS</em>, <em>55</em>(2), 176–184.
(<a href="https://doi.org/10.1109/THMS.2025.3532358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel evaluation approach on wearing passive upper-limb exoskeletons for overhead tasks in real-world automotive manufacturing lines. We determined that wearing exoskeletons reduced the biomechanical efforts of workers measured by joint kinematics and electromyography as well as the estimated shoulder joint reaction forces and torques derived from simulation. These quantitatively measured variables were statistically associated with subjective measures collected through satisfaction questionnaires. We specifically found that participants increased the shoulder flexion and abduction angles as well as the shoulder range of motion while wearing exoskeletons. Participants also reduced muscle activities, joint torques for shoulder flexion, and reaction forces exerted on the shoulder joints while wearing exoskeletons. Interestingly, our analysis also found that the increased shoulder movement while wearing the device was negatively associated with the satisfaction level. This indicates that although the assistance provided by the device allows users to perform a wider range of arm lifting movements, the deviation from their original movement with the device may lead to decreases in satisfaction levels. This integrative approach using biomechanics and ergonomics suggests that we can potentially predict the subjective scale of satisfaction based on biomechanical variables and preliminarily evaluate the usability and comfort while wearing exoskeletons in real-world settings.},
  archive      = {J_THMS},
  author       = {Sungwoo Park and Moon Ki Jung and Kyujung Kim and HyunSeop Lim and JuYoung Yoon and Dong Jin Hyun},
  doi          = {10.1109/THMS.2025.3532358},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {176-184},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Correlations between biomechanical variables and subjective measures of satisfaction while using a passive upper-limb exoskeleton for overhead tasks in the field},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive virtual fixture based on learning trajectory
distribution for comanipulation tasks. <em>THMS</em>, <em>55</em>(2),
165–175. (<a href="https://doi.org/10.1109/THMS.2025.3540123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual fixture is a powerful tool to improve safety and efficiency for co-manipulation tasks. However, traditional virtual fixtures with constant stiffness are inadequate for scenarios where robots need to leave the constraints to perform tasks. To address this, we propose an adaptive virtual fixture based on the motion refinement tube, which dynamically adjusts the guiding force according to the distribution of trajectories. To prevent tube deformation in the Cartesian space due to the neglect of off-diagonal elements of covariance matrices, the refinement tube radii and nonlinear stiffness terms are computed in local coordinate systems based on the decomposed covariance matrix. An energy-tank-based passivity controller is designed to ensure system stability when employing the virtual fixture with state-dependent stiffness terms. In the validation tests with 18 participants, the proposed method showed improvements in task efficiency (18.69% increase) and collision avoidance (97.87% reduction) for a typical pick-and-place task with scattered materials. It also provided better subjective experiences of the users than traditional virtual fixtures. Meanwhile, compared with the method that neglects off-diagonal elements of the covariance matrix, the proposed method exhibited a 4.28% efficiency improvement and a 40.42% decrease in collision occurrences.},
  archive      = {J_THMS},
  author       = {Shaqi Luo and Min Cheng and Ruqi Ding},
  doi          = {10.1109/THMS.2025.3540123},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {165-175},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Adaptive virtual fixture based on learning trajectory distribution for comanipulation tasks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Receding-horizon reinforcement learning for time-delayed
human–machine shared control of intelligent vehicles. <em>THMS</em>,
<em>55</em>(2), 155–164. (<a
href="https://doi.org/10.1109/THMS.2024.3496899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–machine shared control has recently been regarded as a promising paradigm to improve safety and performance in complex driving scenarios. One crucial task in shared control is dynamically optimizing the driving weights between the driver and the intelligent vehicle to adapt to dynamic driving scenarios. However, designing an optimal human–machine shared controller with guaranteed performance and stability is challenging due to nonnegligible time delays caused by communication protocols and uncertainties in driver behavior. This article proposes a novel receding-horizon reinforcement learning approach for time-delayed human–machine shared control of intelligent vehicles. First, we build a multikernel-based data-driven model of vehicle dynamics and driving behavior, considering time delays and uncertainties of drivers&#39; actions. Second, a model-based receding horizon actor–critic learning algorithm is presented to learn an explicit policy for time-delayed human–machine shared control online. Unlike classic reinforcement learning, policy learning of the proposed approach is performed according to a receding-horizon strategy to enhance learning efficiency and adaptability. In theory, the closed-loop stability under time delays is analyzed. Hardware-in-the-loop experiments on the time-delayed human–machine shared control of intelligent vehicles have been conducted in variable curvature road scenarios. The results demonstrate that our approach has significant improvements in driving performance and driver workload compared with pure manual driving and previous shared control methods.},
  archive      = {J_THMS},
  author       = {Xinxin Yao and Jiahang Liu and Xinglong Zhang and Xin Xu},
  doi          = {10.1109/THMS.2024.3496899},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {155-164},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Receding-horizon reinforcement learning for time-delayed Human–Machine shared control of intelligent vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep radiomics for autism diagnosis and age prediction.
<em>THMS</em>, <em>55</em>(2), 144–154. (<a
href="https://doi.org/10.1109/THMS.2025.3526957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiomics combined with deep learning is an emerging field within biomedical engineering that aims to extract important characteristics from medical images to develop a predictive model that can support clinical decision-making. This method could be used in the realm of brain disorders, particularly autism spectrum disorder (ASD), to facilitate prompt identification. We propose a novel radiomic features [deep radiomic features (DTF)], involving the use of principal component analysis to encode convolutional neural network (CNN) features, thereby capturing distinctive features related to brain regions in subjects with ASD subjects and their age. Using these features in random forest (RF) models, we explore two scenarios, such as site-specific radiomic analysis and feature extraction from unaffected brain regions to alleviate site-related variations. Our experiments involved comparing the proposed method with standard radiomics (SR) and 2-D/3-D CNNs for the classification of ASD versus healthy control (HC) individuals and different age groups (below median and above median). When using the RF model with DTF, the analysis at individual sites revealed an area under the receiver operating characteristic (ROC) curve (AUC) range of 79%–85% for features, such as the left lateral-ventricle, cerebellum-white-matter, and pallidum, as well as the right choroid-plexus and vessel. In the context of fivefold cross validation with the RF model, the combined features (DTF from 3-D CNN, ResNet50, DarketNet53, and NasNet_large with SR) achieved the highest AUC value of 76.67%. Furthermore, our method also showed notable AUC values for predicting age in subjects with ASD (80.91%) and HC (75.64%). The results indicate that DTFs consistently exhibit predictive value in classifying ASD from HC subjects and in predicting age.},
  archive      = {J_THMS},
  author       = {Ahmad Chaddad},
  doi          = {10.1109/THMS.2025.3526957},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {144-154},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Deep radiomics for autism diagnosis and age prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning mutual excitation for hand-to-hand and
human-to-human interaction recognition. <em>THMS</em>, <em>55</em>(2),
134–143. (<a href="https://doi.org/10.1109/THMS.2024.3522974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human–robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairwise entities. Compared with graph convolution, our proposed me-GC gradually learns mutual information in each layer and each stage of graph convolution operations. Extensive experiments on a challenging hand-to-hand interaction dataset, i.e., the Assembely101 dataset, and two large-scale human-to-human interaction datasets, i.e., NTU60-Interaction and NTU120-Interaction consistently verify the superiority of our proposed method, which outperforms the state-of-the-art GCN-based and Transformer-based methods.},
  archive      = {J_THMS},
  author       = {Mengyuan Liu and Chen Chen and Songtao Wu and Fanyang Meng and Hong Liu},
  doi          = {10.1109/THMS.2024.3522974},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {134-143},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Learning mutual excitation for hand-to-hand and human-to-human interaction recognition},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiobjective discrete harmony search optimizer for
disassembly line balancing problems considering human factors.
<em>THMS</em>, <em>55</em>(2), 124–133. (<a
href="https://doi.org/10.1109/THMS.2025.3528629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ecological environment and natural resource issues are becoming more and more prominent, which promotes the recycling of waste products for green economy. Disassembly plays a key role in the remanufacturing and reuse of waste products. However, with the rapid development of production automation, designers tend to ignore the fact that manual operation is more flexible. It is of great importance to consider human factors in a disassembly process. This work considers two human disassembly postures, namely standing and sitting. The multiobjective disassembly line balancing problem considering human posture changes is studied. A mathematical model with the objective functions of maximizing profit, minimizing the number of posture changes at a workstation, and minimizing the difference of maximum posture changes between any two workstations is established. The model is solved through a newly proposed Pareto-based discrete harmony search algorithm. Three neighborhood structures are designed to enlarge the search space for better solutions. Furthermore, an elite reserve strategy is used to improve the global optimization ability of the proposed algorithm. Finally, the proposed model and algorithm are applied to cases of different scales of complexities, and the effectiveness of the proposed model and algorithm is verified in comparison with four competitive algorithms.},
  archive      = {J_THMS},
  author       = {Tingting Wei and Xiwang Guo and Mengchu Zhou and Jiacun Wang and Shixin Liu and Shujin Qin and Ying Tang},
  doi          = {10.1109/THMS.2025.3528629},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {124-133},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A multiobjective discrete harmony search optimizer for disassembly line balancing problems considering human factors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervision of multiple remote tower centers: Evaluating a
new air traffic control interface based on mental workload and eye
tracking. <em>THMS</em>, <em>55</em>(2), 114–123. (<a
href="https://doi.org/10.1109/THMS.2025.3527136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote air traffic control offers inexpensive and efficient service to multiple airports. Recent research shows that one remote air traffic control officer can safely control up to three low-traffic airports simultaneously. In a multiple remote tower center, airports can be flexibly allocated across air traffic control officers based on prospective traffic loads. The main task of the supervisor in such a center is balancing the workload of each air traffic control officer by allocating airports accordingly. This study analyzes the supervisor&#39;s visual attention during interaction with a planning tool for their daily tasks. Five use cases were identified as the main tasks of the supervisor representing a mixture of planned and unplanned events. A mixed methods within-subjects design was used to assess the workload and eye-movement patterns associated with each of these use cases. In total, 15 professional air traffic control officers participated in the study. Workload and eye movement were analyzed independently in relation to the use cases but also in combination with each other. Across all use cases, a small correlation between subjective workload ratings and fixation duration was found, supporting previous findings of fixation duration being associated with information processing. Transitions between areas of interest on the supervisor planning tool provided valuable insights into the layout design of future supervisor planning tools.},
  archive      = {J_THMS},
  author       = {Leo Julius Materne and Maik Friedrich},
  doi          = {10.1109/THMS.2025.3527136},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {114-123},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Supervision of multiple remote tower centers: Evaluating a new air traffic control interface based on mental workload and eye tracking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
