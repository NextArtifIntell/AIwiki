<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip---20">TIP - 20</h2>
<ul>
<li><details>
<summary>
(2025). SEGSID: A semantic-guided framework for sonar image
despeckling. <em>TIP</em>, <em>34</em>, 652–666. (<a
href="https://doi.org/10.1109/TIP.2024.3512378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonar imagery is substantially degraded by speckle noise, making the task of despeckling crucial for improving image quality. Self-supervised despeckling methods, represented by blind-spot networks (BSNs), have shown promise in this regard. However, these methods consistently face significant challenges due to the spatial correlation of speckle noise and the inherent information loss within BSNs. In this paper, we introduce SEGSID, a BSN-based, semantic-guided sonar despeckling framework designed to address these challenges. Specifically, the SEGSID framework primarily comprises a Receptive Field Augmentation (RFA) module and a Global Semantic Enhancement (GSE) module. To address the noise spatial correlation, the RFA module is crafted to strategically extract valuable local information while avoiding the exploitation of noise-correlated pixels. Concurrently, the GSE module extracts the global semantic information from entire images and injects it into the extracted local features. This enhances BSNs’ ability to harness more comprehensive image information and compensates for their inherent information loss. Furthermore, to bolster efficiency, we employ knowledge distillation techniques to transfer the expertise from the trained SEGSID into a more streamlined network suitable for broader practical applications. Extensive experiments on three distinct sonar datasets demonstrate that SEGSID outperforms both traditional despeckling methods and state-of-the-art self-supervised despeckling techniques. The implementation is publicly accessible at https://github.com/deng-ai-lab/SEGSID.},
  archive      = {J_TIP},
  author       = {Shaohua Liu and Junzhe Lu and Hongkun Dou and Jiajun Li and Yue Deng},
  doi          = {10.1109/TIP.2024.3512378},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {652-666},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SEGSID: A semantic-guided framework for sonar image despeckling},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linearly transformed color guide for low-bitrate
diffusion-based image compression. <em>TIP</em>, <em>34</em>, 468–482.
(<a href="https://doi.org/10.1109/TIP.2024.3521301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the challenge of controlling the global color aspect of images generated by a diffusion model without training or fine-tuning. We rewrite the guidance equations to ensure that the outputs are closer to a known color map, without compromising the quality of the generation. Our method results in new guidance equations. In the context of color guidance, we show that the scaling of the guidance should not decrease but rather increase throughout the diffusion process. In a second contribution, our guidance is applied in a compression framework, where we combine both semantic and general color information of the image to decode at very low cost. We show that our method is effective in improving the fidelity and realism of compressed images at extremely low bit rates ( $10^{-2}$ bpp), performing better on these criteria when compared to other classical or more semantically oriented approaches. The implementation of our method is available on gitlab at https://gitlab.inria.fr/tbordin/color-guidance.},
  archive      = {J_TIP},
  author       = {Tom Bordin and Thomas Maugey},
  doi          = {10.1109/TIP.2024.3521301},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {468-482},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Linearly transformed color guide for low-bitrate diffusion-based image compression},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VDMUFusion: A versatile diffusion model-based unsupervised
framework for image fusion. <em>TIP</em>, <em>34</em>, 441–454. (<a
href="https://doi.org/10.1109/TIP.2024.3512365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion facilitates the integration of information from various source images of the same scene into a composite image, thereby benefiting perception, analysis, and understanding. Recently, diffusion models have demonstrated impressive generative capabilities in the field of computer vision, suggesting significant potential for application in image fusion. The forward process in the diffusion models requires the gradual addition of noise to the original data. However, typical unsupervised image fusion tasks (e.g., infrared-visible, medical, and multi-exposure image fusion) lack ground truth images (corresponding to the original data in diffusion models), thereby preventing the direct application of the diffusion models. To address this problem, we propose a versatile diffusion model-based unsupervised framework for image fusion, termed as VDMUFusion. In the proposed method, we integrate the fusion problem into the diffusion sampling process by formulating image fusion as a weighted average process and establishing appropriate assumptions about the noise in the diffusion model. To simplify the training process, we propose a multi-task learning framework that replaces the original noise prediction network, allowing for simultaneous prediction of noise and fusion weights. Meanwhile, our method employs joint training across various fusion tasks, which significantly improves noise prediction accuracy and yields higher quality fused images compared to training on a single task. Extensive experimental results demonstrate that the proposed method delivers very competitive performance across various image fusion tasks. The code is available at https://github.com/yuliu316316/VDMUFusion.},
  archive      = {J_TIP},
  author       = {Yu Shi and Yu Liu and Juan Cheng and Z. Jane Wang and Xun Chen},
  doi          = {10.1109/TIP.2024.3512365},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {441-454},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VDMUFusion: A versatile diffusion model-based unsupervised framework for image fusion},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Passive non-line-of-sight imaging with light transport
modulation. <em>TIP</em>, <em>34</em>, 410–424. (<a
href="https://doi.org/10.1109/TIP.2024.3518097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in recent years, due to its ability to image objects that are out of sight. The light transport condition plays an important role in this task since changing the conditions will lead to different imaging models. Existing learning-based NLOS methods usually train independent models for different light transport conditions, which is computationally inefficient and impairs the practicality of the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging method that effectively handles multiple light transport conditions with a single network. We achieve this by inferring a latent light transport representation from the projection image and using this representation to modulate the network that reconstructs the hidden image from the projection image. We train a light transport encoder together with a vector quantizer to obtain the light transport representation. To further regulate this representation, we jointly learn both the reconstruction network and the reprojection network during training. A set of light transport modulation blocks is used to modulate the two jointly trained networks in a multi-scale way. Extensive experiments on a large-scale passive NLOS dataset demonstrate the superiority of the proposed method. The code is available at https://github.com/JerryOctopus/NLOS-LTM.},
  archive      = {J_TIP},
  author       = {Jiarui Zhang and Ruixu Geng and Xiaolong Du and Yan Chen and Houqiang Li and Yang Hu},
  doi          = {10.1109/TIP.2024.3518097},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {410-424},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Passive non-line-of-sight imaging with light transport modulation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Residual quotient learning for zero-reference low-light
image enhancement. <em>TIP</em>, <em>34</em>, 365–378. (<a
href="https://doi.org/10.1109/TIP.2024.3519997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, neural networks have become the dominant approach to low-light image enhancement (LLIE), with at least one-third of them adopting a Retinex-related architecture. However, through in-depth analysis, we contend that this most widely accepted LLIE structure is suboptimal, particularly when addressing the non-uniform illumination commonly observed in natural images. In this paper, we present a novel variant learning framework, termed residual quotient learning, to substantially alleviate this issue. Instead of following the existing Retinex-related decomposition-enhancement-reconstruction process, our basic idea is to explicitly reformulate the light enhancement task as adaptively predicting the latent quotient with reference to the original low-light input using a residual learning fashion. By leveraging the proposed residual quotient learning, we develop a lightweight yet effective network called ResQ-Net. This network features enhanced non-uniform illumination modeling capabilities, making it more suitable for real-world LLIE tasks. Moreover, due to its well-designed structure and reference-free loss function, ResQ-Net is flexible in training as it allows for zero-reference optimization, which further enhances the generalization and adaptability of our entire framework. Extensive experiments on various benchmark datasets demonstrate the merits and effectiveness of the proposed residual quotient learning, and our trained ResQ-Net outperforms state-of-the-art methods both qualitatively and quantitatively. Furthermore, a practical application in dark face detection is explored, and the preliminary results confirm the potential and feasibility of our method in real-world scenarios.},
  archive      = {J_TIP},
  author       = {Chao Xie and Linfeng Fei and Huanjie Tao and Yaocong Hu and Wei Zhou and Jiun Tian Hoe and Weipeng Hu and Yap-Peng Tan},
  doi          = {10.1109/TIP.2024.3519997},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {365-378},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Residual quotient learning for zero-reference low-light image enhancement},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling dual-exposure quad-bayer patterns for joint
denoising and deblurring. <em>TIP</em>, <em>34</em>, 350–364. (<a
href="https://doi.org/10.1109/TIP.2024.3515873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image degradation caused by noise and blur remains a persistent challenge in imaging systems, stemming from limitations in both hardware and methodology. Single-image solutions face an inherent tradeoff between noise reduction and motion blur. While short exposures can capture clear motion, they suffer from noise amplification. Long exposures reduce noise but introduce blur. Learning-based single-image enhancers tend to be over-smooth due to the limited information. Multi-image solutions using burst mode avoid this tradeoff by capturing more spatial-temporal information but often struggle with misalignment from camera/scene motion. To address these limitations, we propose a physical-model-based image restoration approach leveraging a novel dual-exposure Quad-Bayer pattern sensor. By capturing pairs of short and long exposures at the same starting point but with varying durations, this method integrates complementary noise-blur information within a single image. We further introduce a Quad-Bayer synthesis method (B2QB) to simulate sensor data from Bayer patterns to facilitate training. Based on this dual-exposure sensor model, we design a hierarchical convolutional neural network called QRNet to recover high-quality RGB images. The network incorporates input enhancement blocks and multi-level feature extraction to improve restoration quality. Experiments demonstrate superior performance over state-of-the-art deblurring and denoising methods on both synthetic and real-world datasets. The code, model, and datasets are publicly available at https://github.com/zhaoyuzhi/QRNet.},
  archive      = {J_TIP},
  author       = {Yuzhi Zhao and Lai-Man Po and Xin Ye and Yongzhe Xu and Qiong Yan},
  doi          = {10.1109/TIP.2024.3515873},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {350-364},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Modeling dual-exposure quad-bayer patterns for joint denoising and deblurring},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MISC: Ultra-low bitrate image semantic compression driven by
large multimodal model. <em>TIP</em>, <em>34</em>, 335–349. (<a
href="https://doi.org/10.1109/TIP.2024.3515874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, all existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. During recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content. It can achieve optimal consistency and perception results while saving 50% bitrate, which has strong potential applications in the next generation of storage and communication. The code will be released on https://github.com/lcysyzxdxc/MISC.},
  archive      = {J_TIP},
  author       = {Chunyi Li and Guo Lu and Donghui Feng and Haoning Wu and Zicheng Zhang and Xiaohong Liu and Guangtao Zhai and Weisi Lin and Wenjun Zhang},
  doi          = {10.1109/TIP.2024.3515874},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {335-349},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MISC: Ultra-low bitrate image semantic compression driven by large multimodal model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HomuGAN: A 3D-aware GAN with the method of cylindrical
spatial-constrained sampling. <em>TIP</em>, <em>34</em>, 320–334. (<a
href="https://doi.org/10.1109/TIP.2024.3520423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllable 3D-aware scene synthesis seeks to disentangle the various latent codes in the implicit space enabling the generation network to create highly realistic images with 3D consistency. Recent approaches often integrate Neural Radiance Fields with the upsampling method of StyleGAN2, employing Convolutions with style modulation to transform spatial coordinates into frequency domain representations. Our analysis indicates that this approach can give rise to a bubble phenomenon in StyleNeRF. We argue that the style modulation introduces extraneous information into the implicit space, disrupting 3D implicit modeling and degrading image quality. We introduce HomuGAN, incorporating two key improvements. First, we disentangle the style modulation applied to implicit modeling from that utilized for super-resolution, thus alleviating the bubble phenomenon. Second, we introduce Cylindrical Spatial-Constrained Sampling and Parabolic Sampling. The latter sampling method, as an alternative method to the former, specifically contributes to the performance of foreground modeling of vehicles. We evaluate HomuGAN on publicly available datasets, comparing its performance to existing methods. Empirical results demonstrate that our model achieves the best performance, exhibiting relatively outstanding disentanglement capability. Moreover, HomuGAN addresses the training instability problem observed in StyleNeRF and reduces the bubble phenomenon.},
  archive      = {J_TIP},
  author       = {Haochen Yu and Weixi Gong and Jiansheng Chen and Huimin Ma},
  doi          = {10.1109/TIP.2024.3520423},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {320-334},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HomuGAN: A 3D-aware GAN with the method of cylindrical spatial-constrained sampling},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking masked representation learning for 3D point cloud
understanding. <em>TIP</em>, <em>34</em>, 247–262. (<a
href="https://doi.org/10.1109/TIP.2024.3520008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised point cloud representation learning aims to acquire robust and general feature representations from unlabeled data. Recently, masked point modeling-based methods have shown significant performance improvements for point cloud understanding, yet these methods rely on overlapping grouping strategies (k-nearest neighbor algorithm) resulting in early leakage of structural information of mask groups, and overlook the semantic modeling of object components resulting in parts with the same semantics having obvious feature differences due to position differences. In this work, we rethink grouping strategies and pretext tasks that are more suitable for self-supervised point cloud representation learning and propose a novel hierarchical masked representation learning method, including an optimal transport-based hierarchical grouping strategy, a prototype-based part modeling module, and a hierarchical attention encoder. The proposed method enjoys several merits. First, the proposed grouping strategy partitions the point cloud into non-overlapping groups, eliminating the early leakage of structural information in the masked groups. Second, the proposed prototype-based part modeling module dynamically models different object components, ensuring feature consistency on parts with the same semantics. Extensive experiments on four downstream tasks demonstrate that our method surpasses state-of-the-art 3D representation learning methods. Furthermore, Comprehensive ablation studies and visualizations demonstrate the effectiveness of the proposed modules.},
  archive      = {J_TIP},
  author       = {Chuxin Wang and Yixin Zha and Jianfeng He and Wenfei Yang and Tianzhu Zhang},
  doi          = {10.1109/TIP.2024.3520008},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {247-262},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking masked representation learning for 3D point cloud understanding},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HYRE: Hybrid regressor for 3D human pose and shape
estimation. <em>TIP</em>, <em>34</em>, 235–246. (<a
href="https://doi.org/10.1109/TIP.2024.3515872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression-based 3D human pose and shape estimation often fall into one of two different paradigms. Parametric approaches, which regress the parameters of a human body model, tend to produce physically plausible but image-mesh misalignment results. In contrast, non-parametric approaches directly regress human mesh vertices, resulting in pixel-aligned but unreasonable predictions. In this paper, we consider these two paradigms together for a better overall estimation. To this end, we propose a novel HYbrid REgressor (HYRE) that greatly benefits from the joint learning of both paradigms. The core of our HYRE is a hybrid intermediary across paradigms that provides complementary clues to each paradigm at the shared feature level and fuses their results at the part-based decision level, thereby bridging the gap between the two. We demonstrate the effectiveness of the proposed method through both quantitative and qualitative experimental analyses, resulting in improvements for each approach and ultimately leading to better hybrid results. Our experiments show that HYRE outperforms previous methods on challenging 3D human pose and shape benchmarks.},
  archive      = {J_TIP},
  author       = {Wenhao Li and Mengyuan Liu and Hong Liu and Bin Ren and Xia Li and Yingxuan You and Nicu Sebe},
  doi          = {10.1109/TIP.2024.3515872},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {235-246},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HYRE: Hybrid regressor for 3D human pose and shape estimation},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularization by denoising: Bayesian model and
langevin-within-split gibbs sampling. <em>TIP</em>, <em>34</em>,
221–234. (<a href="https://doi.org/10.1109/TIP.2024.3520012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. These contributions advance Bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework.},
  archive      = {J_TIP},
  author       = {Elhadji C. Faye and Mame Diarra Fall and Nicolas Dobigeon},
  doi          = {10.1109/TIP.2024.3520012},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {221-234},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Regularization by denoising: Bayesian model and langevin-within-split gibbs sampling},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Normalizing batch normalization for long-tailed recognition.
<em>TIP</em>, <em>34</em>, 209–220. (<a
href="https://doi.org/10.1109/TIP.2024.3518099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, the number of training samples across classes usually subjects to a long-tailed distribution. The conventionally trained network may achieve unexpected inferior performance on the rare class compared to the frequent class. Most previous works attempt to rectify the network bias from the data-level or from the classifier-level. Differently, in this paper, we identify that the bias towards the frequent class may be encoded into features, i.e., the rare-specific features which play a key role in discriminating the rare class are much weaker than the frequent-specific features. Based on such an observation, we introduce a simple yet effective approach, normalizing the parameters of Batch Normalization (BN) layer to explicitly rectify the feature bias. To achieve this end, we represent the Weight/Bias parameters of a BN layer as a vector, normalize it into a unit one and multiply the unit vector by a scalar learnable parameter. Through decoupling the direction and magnitude of parameters in BN layer to learn, the Weight/Bias exhibits a more balanced distribution and thus the strength of features becomes more even. Extensive experiments on various long-tailed recognition benchmarks (i.e., CIFAR-10/100-LT, ImageNet-LT and iNaturalist 2018) show that our method outperforms previous state-of-the-arts remarkably.},
  archive      = {J_TIP},
  author       = {Yuxiang Bao and Guoliang Kang and Linlin Yang and Xiaoyue Duan and Bo Zhao and Baochang Zhang},
  doi          = {10.1109/TIP.2024.3518099},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {209-220},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Normalizing batch normalization for long-tailed recognition},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-world low-dose CT image denoising by patch similarity
purification. <em>TIP</em>, <em>34</em>, 196–208. (<a
href="https://doi.org/10.1109/TIP.2024.3515878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing the radiation dose in CT scanning is important to alleviate the damage to the human health in clinical scenes. A promising way is to replace the normal-dose CT (NDCT) imaging by low-dose CT (LDCT) imaging with lower tube voltage and tube current. This often brings severe noise to the LDCT images, which adversely affects the diagnosis accuracy. Most of existing LDCT image denoising networks are trained either with synthetic LDCT images or real-world LDCT and NDCT image pairs with huge spatial misalignment. However, the synthetic noise is very different from the complex noise in real-world LDCT images, while the huge spatial misalignment brings inaccurate predictions of tissue structures in the denoised LDCT images. To well utilize real-world LDCT and NDCT image pairs for LDCT image denoising, in this paper, we introduce a new Patch Similarity Purification (PSP) strategy to construct high-quality training dataset for network training. Specifically, our PSP strategy first perform binarization for each pair of image patches cropped from the corresponding LDCT and NDCT image pairs. For each pair of binary masks, it then computes their similarity ratio by common mask calculation, and the patch pair can be selected as a training sample if their mask similarity ratio is higher than a threshold. By using our PSP strategy, each training set of our Rabbit and Patient datasets contain hundreds of thousands of real-world LDCT and NDCT image patch pairs with negligible misalignment. Extensive experiments demonstrate the usefulness of our PSP strategy on purifying the training data and the effectiveness of training LDCT image denoising networks on our datasets. The code and dataset are provided at https://github.com/TuTusong/PSP.},
  archive      = {J_TIP},
  author       = {Zeya Song and Liqi Xue and Jun Xu and Baoping Zhang and Chao Jin and Jian Yang and Changliang Zou},
  doi          = {10.1109/TIP.2024.3515878},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {196-208},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real-world low-dose CT image denoising by patch similarity purification},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HEOI: Human attention prediction in natural daily life with
fine-grained human-environment-object interaction model. <em>TIP</em>,
<em>34</em>, 170–182. (<a
href="https://doi.org/10.1109/TIP.2024.3512380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper handles the problem of human attention prediction in natural daily life from the third-person view. Due to the significance of this topic in various applications, researchers in the computer vision community have proposed many excellent models in the past few decades, and many models have begun to focus on natural daily life scenarios in recent years. However, existing mainstream models usually ignore a basic fact that human attention is a typical interdisciplinary concept. Specifically, the mainstream definition is direction-level or pixel-level, while many interdisciplinary studies argue the object-level definition. Additionally, the mainstream model structure converges to the dual-pathway architecture or its variants, while the majority of interdisciplinary studies claim attention is involved in the human-environment interaction procedure. Grounded on solid theories and studies in interdisciplinary fields including computer vision, cognition, neuroscience, psychology, and philosophy, this paper proposes a fine-grained Human-Environment-Object Interaction (HEOI) model, which for the first time integrates multi-granularity human cues to predict human attention. Our model is explainable and lightweight, and validated to be effective by a wide range of comparison, ablation, and visualization experiments on two public datasets.},
  archive      = {J_TIP},
  author       = {Zhixiong Nan and Leiyu Jia and Bin Xiao},
  doi          = {10.1109/TIP.2024.3512380},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {170-182},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HEOI: Human attention prediction in natural daily life with fine-grained human-environment-object interaction model},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning feature matching via matchable keypoint-assisted
graph neural network. <em>TIP</em>, <em>34</em>, 154–169. (<a
href="https://doi.org/10.1109/TIP.2024.3512352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately matching local features between a pair of images corresponding to the same 3D scene is a challenging computer vision task. Previous studies typically utilize attention-based graph neural networks (GNNs) with fully-connected graphs over keypoints within/across images for visual and geometric information reasoning. However, in the background of local feature matching, a significant number of keypoints are non-repeatable due to factors like occlusion and failure of the detector, and thus irrelevant for message passing. The connectivity with non-repeatable keypoints not only introduces redundancy, resulting in limited efficiency (quadratic computational complexity w.r.t. the keypoint number), but also interferes with the representation aggregation process, leading to limited accuracy. Aiming at the best of both worlds on accuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN architecture which bypasses non-repeatable keypoints and leverages matchable ones to guide compact and meaningful message passing. More specifically, our Bilateral Context-Aware Sampling (BCAS) Module first dynamically samples two small sets of well-distributed keypoints with high matchability scores from the image pair. Then, our Matchable Keypoint-Assisted Context Aggregation (MKACA) Module regards sampled informative keypoints as message bottlenecks and thus constrains each keypoint only to retrieve favorable contextual information from intra- and inter-matchable keypoints, evading the interference of irrelevant and redundant connectivity with non-repeatable ones. Furthermore, considering the potential noise in initial keypoints and sampled matchable ones, the MKACA module adopts a matchability-guided attentional aggregation operation for purer data-dependent context propagation. By these means, MaKeGNN outperforms the state-of-the-arts on multiple highly challenging benchmarks, while significantly reducing computational and memory complexity compared to typical attentional GNNs.},
  archive      = {J_TIP},
  author       = {Zizhuo Li and Jiayi Ma},
  doi          = {10.1109/TIP.2024.3512352},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {154-169},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning feature matching via matchable keypoint-assisted graph neural network},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subjective and objective analysis of indian social media
video quality. <em>TIP</em>, <em>34</em>, 140–153. (<a
href="https://doi.org/10.1109/TIP.2024.3512376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conducted a large-scale subjective study of the perceptual quality of User-Generated Mobile Video Content on a set of mobile-originated videos obtained from ShareChat, a social media platform widely used across India. The content viewed by volunteer human subjects under controlled laboratory conditions has the benefit of culturally diversifying the existing corpus of User-Generated Content (UGC) video quality datasets. There is a great need for large and diverse UGC-VQA datasets, given the explosive global growth of the visual internet and social media platforms. This is particularly true in regard to videos obtained by smartphones, especially in rapidly emerging economies like India. ShareChat provides a safe and cultural community oriented space for users to generate and share content in their preferred Indian languages and dialects. Our subjective quality study, which is based on this data, supplies much needed cultural, visual, and language diversification to the overall shareable corpus of video quality data. We expect that this new data resource will also allow for the development of systems that can predict the perceived visual quality of Indian social media videos, and in this context, control scaling and compression protocols for streaming, provide better user recommendations, and guide content analysis and processing. We demonstrate the value of the new data resource by conducting a study of leading No-Reference Video Quality Assessment (NR-VQA) models on it, including a simple new model, called MoEVA, which deploys a mixture of experts to predict video quality. Both the new LIVE-ShareChat Database and sample source code for MoEVA are being made freely available to the research community at https://github.com/sandeep-sm/LIVE-SC.},
  archive      = {J_TIP},
  author       = {Sandeep Mishra and Mukul Jha and Alan C. Bovik},
  doi          = {10.1109/TIP.2024.3512376},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {140-153},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Subjective and objective analysis of indian social media video quality},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-adaptive feature extraction method for aerial-view
geo-localization. <em>TIP</em>, <em>34</em>, 126–139. (<a
href="https://doi.org/10.1109/TIP.2024.3513157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-view geo-localization aims to match the same geographic location from different view images, e.g., drone-view images and geo-referenced satellite-view images. Due to UAV cameras’ different shooting angles and heights, the scale of the same captured target building in the drone-view images varies greatly. Meanwhile, there is a difference in size and floor area for different geographic locations in the real world, such as towers and stadiums, which also leads to scale variants of geographic targets in the images. However, existing methods mainly focus on extracting the fine-grained information of the geographic targets or the contextual information of the surrounding area, which overlook the robust feature for scale changes and the importance of feature alignment. In this study, we argue that the key underpinning of this task is to train a network to mine a discriminative representation against scale variants. To this end, we design an effective and novel end-to-end network called Self-Adaptive Feature Extraction Network (Safe-Net) to extract powerful scale-invariant features in a self-adaptive manner. Safe-Net includes a global representation-guided feature alignment module and a saliency-guided feature partition module. The former applies an affine transformation guided by the global feature for adaptive feature alignment. Without extra region annotations, the latter computes saliency distribution for different regions of the image and adopts the saliency information to guide a self-adaptive feature partition on the feature map to learn a visual representation against scale variants. Experiments on two prevailing large-scale aerial-view geo-localization benchmarks, i.e., University-1652 and SUES-200, show that the proposed method achieves state-of-the-art results. In addition, our proposed Safe-Net has a significant scale adaptive capability and can extract robust feature representations for those query images with small target buildings. The source code of this study is available at: https://github.com/AggMan96/Safe-Net .},
  archive      = {J_TIP},
  author       = {Jinliang Lin and Zhiming Luo and Dazhen Lin and Shaozi Li and Zhun Zhong},
  doi          = {10.1109/TIP.2024.3513157},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {126-139},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A self-adaptive feature extraction method for aerial-view geo-localization},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning lossless compression for high bit-depth volumetric
medical image. <em>TIP</em>, <em>34</em>, 113–125. (<a
href="https://doi.org/10.1109/TIP.2024.3513156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in learning-based methods have markedly enhanced the capabilities of image compression. However, these methods struggle with high bit-depth volumetric medical images, facing issues such as degraded performance, increased memory demand, and reduced processing speed. To address these challenges, this paper presents the Bit-Division based Lossless Volumetric Image Compression (BD-LVIC) framework, which is tailored for high bit-depth medical volume compression. The BD-LVIC framework skillfully divides the high bit-depth volume into two lower bit-depth segments: the Most Significant Bit-Volume (MSBV) and the Least Significant Bit-Volume (LSBV). The MSBV concentrates on the most significant bits of the volumetric medical image, capturing vital structural details in a compact manner. This reduction in complexity greatly improves compression efficiency using traditional codecs. Conversely, the LSBV deals with the least significant bits, which encapsulate intricate texture details. To compress this detailed information effectively, we introduce an effective learning-based compression model equipped with a Transformer-Based Feature Alignment Module, which exploits both intra-slice and inter-slice redundancies to accurately align features. Subsequently, a Parallel Autoregressive Coding Module merges these features to precisely estimate the probability distribution of the least significant bit-planes. Our extensive testing demonstrates that the BD-LVIC framework not only sets new performance benchmarks across various datasets but also maintains a competitive coding speed, highlighting its significant potential and practical utility in the realm of volumetric medical image compression.},
  archive      = {J_TIP},
  author       = {Kai Wang and Yuanchao Bai and Daxin Li and Deming Zhai and Junjun Jiang and Xianming Liu},
  doi          = {10.1109/TIP.2024.3513156},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {113-125},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning lossless compression for high bit-depth volumetric medical image},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrossEI: Boosting motion-oriented object tracking with an
event camera. <em>TIP</em>, <em>34</em>, 73–84. (<a
href="https://doi.org/10.1109/TIP.2024.3505672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the differential sensitivity and high time resolution, event cameras can record detailed motion clues, which form a complementary advantage with frame-based cameras to enhance the object tracking, especially in challenging dynamic scenes. However, how to better match heterogeneous event-image data and exploit rich complementary cues from them still remains an open issue. In this paper, we align event-image modalities by proposing a motion adaptive event sampling method, and we revisit the cross-complementarities of event-image data to design a bidirectional-enhanced fusion framework. Specifically, this sampling strategy can adapt to different dynamic scenes and integrate aligned event-image pairs. Besides, we design an image-guided motion estimation unit for extracting explicit instance-level motions, aiming at refining the uncertain event clues to distinguish primary objects and background. Then, a semantic modulation module is devised to utilize the enhanced object motion to modify the image features. Coupled with these two modules, this framework learns both the high motion sensitivity of events and the full texture of images to achieve more accurate and robust tracking. The proposed method is easily embedded in existing tracking pipelines, and trained end-to-end. We evaluate it on four large benchmarks, i.e. FE108, VisEvent, FE240hz and CoeSot. Extensive experiments demonstrate our method achieves state-of-the-art performance, and large improvements are pointed as contributions by our sampling strategy and fusion concept.},
  archive      = {J_TIP},
  author       = {Zhiwen Chen and Jinjian Wu and Weisheng Dong and Leida Li and Guangming Shi},
  doi          = {10.1109/TIP.2024.3505672},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {73-84},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CrossEI: Boosting motion-oriented object tracking with an event camera},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Portrait shadow removal using context-aware illumination
restoration network. <em>TIP</em>, <em>34</em>, 1–15. (<a
href="https://doi.org/10.1109/TIP.2024.3497802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portrait shadow removal is a challenging task due to the complex surface of the face. Although existing work in this field makes substantial progress, these methods tend to overlook information in the background areas. However, this background information not only contains some important illumination cues but also plays a pivotal role in achieving lighting harmony between the face and the background after shadow elimination. In this paper, we propose a Context-aware Illumination Restoration Network (CIRNet) for portrait shadow removal. Our CIRNet consists of three stages. First, the Coarse Shadow Removal Network (CSRNet) mitigates the illumination discrepancies between shadow and non-shadow areas. Next, the Area-aware Shadow Restoration Network (ASRNet) predicts the illumination characteristics of shadowed areas by utilizing background context and non-shadow portrait context as references. Lastly, we introduce a Global Fusion Network to adaptively merge contextual information from different areas and generate the final shadow removal result. This approach leverages the illumination information from the background region while ensuring a more consistent overall illumination in the generated images. Our approach can also be extended to high-resolution portrait shadow removal and portrait specular highlight removal. Besides, we construct the first real facial shadow dataset for portrait shadow removal, consisting of 6200 pairs of facial images. Qualitative and quantitative comparisons demonstrate the advantages of our proposed dataset as well as our method.},
  archive      = {J_TIP},
  author       = {Jiangjian Yu and Ling Zhang and Qing Zhang and Qifei Zhang and Daiguo Zhou and Chao Liang and Chunxia Xiao},
  doi          = {10.1109/TIP.2024.3497802},
  journal      = {IEEE Transactions on Image Processing},
  month        = {12},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Portrait shadow removal using context-aware illumination restoration network},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
