<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmc---64">TMC - 64</h2>
<ul>
<li><details>
<summary>
(2025). Resource allocation for metaverse experience optimization: A
multi-objective multi-agent evolutionary reinforcement learning
approach. <em>TMC</em>, <em>24</em>(4), 3473–3488. (<a
href="https://doi.org/10.1109/TMC.2024.3509680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Metaverse, real-time, concurrent services such as virtual classrooms and immersive gaming require local graphic rendering to maintain low latency. However, the limited processing power and battery capacity of user devices make it challenging to balance Quality of Experience (QoE) and terminal energy consumption. In this paper, we investigate a multi-objective optimization problem (MOP) regarding power control and rendering capacity allocation by formulating it as a multi-objective optimization problem. This problem aims to minimize energy consumption while maximizing Meta-Immersion (MI), a metric that integrates objective network performance with subjective user perception. To solve this problem, we propose a Multi-Objective Multi-Agent Evolutionary Reinforcement Learning with User-Object-Attention (M2ERL-UOA) algorithm. The algorithm employs a prediction-driven evolutionary learning mechanism for multi-agents, coupled with optimized rendering capacity decisions for virtual objects. The algorithm can yield a superior Pareto front that attains the Nash equilibrium. Simulation results demonstrate that the proposed algorithm can generate Pareto fronts, effectively adapts to dynamic user preferences, and significantly reduces decision-making time compared to several benchmarks.},
  archive      = {J_TMC},
  author       = {Lei Feng and Xiaoyi Jiang and Yao Sun and Dusit Niyato and Yu Zhou and Shiyi Gu and Zhixiang Yang and Yang Yang and Fanqin Zhou},
  doi          = {10.1109/TMC.2024.3509680},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3473-3488},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource allocation for metaverse experience optimization: A multi-objective multi-agent evolutionary reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do as the romans do: Location imitation-based edge task
offloading for privacy protection. <em>TMC</em>, <em>24</em>(4),
3456–3472. (<a href="https://doi.org/10.1109/TMC.2024.3509418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge computing, a user prefers offloading his/her task to nearby edge servers to maximize the offloading utility. However, this inevitably exposes the user&#39;s location privacy information when suffering from the side-channel attacks based on offloading decision behaviors and Received Signal Strength Indicators (RSSI). Existing works only consider the scenario with one untrusted edge server or defend only against one of the attacks. In this paper, we first study the edge task offloading problem with comprehensive privacy protection against these side-channel attacks from multiple edge servers. To address this problem while ensuring satisfactory offloading utility, we develop a Location Imitation-based Edge Task Offloading approach LITO. Specifically, we first determine a suitable perturbation region centered at the user&#39;s real location for a balance between offloading utility and privacy protection, and then propose a modified Laplace mechanism to generate a fake location meeting geo-indistinguishability within the region. Subsequently, to mislead the side-channel attacks to the fake location, we design an approximate algorithm and a transmit power control strategy to imitate the offloading decisions and RSSIs at the fake location, respectively. Theoretical analysis and experimental evaluations demonstrate the performance of LITO in improving privacy protection and guaranteeing offloading utility.},
  archive      = {J_TMC},
  author       = {Jiahao Zhu and Lu Zhao and Jian Zhou and Hui Cai and Fu Xiao},
  doi          = {10.1109/TMC.2024.3509418},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3456-3472},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Do as the romans do: Location imitation-based edge task offloading for privacy protection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing fault-tolerant time-aware flow scheduling in
TSN-5G networks. <em>TMC</em>, <em>24</em>(4), 3441–3455. (<a
href="https://doi.org/10.1109/TMC.2024.3510604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of time-sensitive networking (TSN) and fifth-generation (5G) offers a promising solution for real-time and reliable data transmission in the Industrial Internet of Things (IIoT). However, current research focuses on traffic scheduling in TSN-5G networks to support low latency. New challenges arise when TSN-5G networks leverage time-aware shaper (TAS) and frame replication and elimination for reliability (FRER) to achieve low latency and high reliability. Simply combining TAS and FRER (SCTF) requires scheduling all time-triggered (TT) flows and their replica flows, which substantially increases the computational complexity of gate control lists (GCLs) and severely weakens scheduling capabilities. Moreover, the packet elimination function (PEF) in FRER may induce packet misordering. In this paper, we propose an efficient and fault-tolerant time-aware shaper (EF-TAS) mechanism for TSN-5G networks. EF-TAS only allocates timeslots for TT flows, while replica TT (RT) flows are delivered using a best-effort strategy. Due to the potential violation of deadlines in RT flows, we design an adaptive cyclic GCL window (ACGW)-based hybrid scheduling (AHS) algorithm to schedule TT and RT flows differentially. The AHS algorithm utilizes network calculus to ensure the timely arrival of RT flows without affecting the deterministic transmission of TT flows. In particular, we provide upper bounds on the amount of reordering to quantify the disorder caused by PEF and analyze the impact of introducing the packet ordering function (POF) on EF-TAS performance. The evaluation results show that EF-TAS not only meets the reliability and deadline requirements but also significantly reduces the total number of GCL entries and the computation time of GCLs compared to state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Guizhen Li and Shuo Wang and Yudong Huang and Tao Huang and Yuanhao Cui and Zehui Xiong},
  doi          = {10.1109/TMC.2024.3510604},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3441-3455},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing fault-tolerant time-aware flow scheduling in TSN-5G networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile tile-based 360<span
class="math inline"><sup>∘</sup></span>&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;∘&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;
video multicast with cybersickness alleviation. <em>TMC</em>,
<em>24</em>(4), 3423–3440. (<a
href="https://doi.org/10.1109/TMC.2024.3514852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) imaging is 360°, which requires a large bandwidth for video transmission. To address this challenge, tile-based streaming has been proposed to deliver only the focused part of the video instead of the entire one. However, the impact of cybersickness, akin to motion sickness, on tile selection in VR has not been explored. In this paper, we investigate Multi-user Tile Streaming with Cybersickness Control (MTSCC) in an adaptive 360$^\circ$ video streaming system with multicast and cybersickness alleviation. We propose a novel $m^{2}$-competitive online algorithm that utilizes Individual Sickness Indicator (ISI) and Bitrate Restriction Indicator (BRI) to evaluate user cybersickness tendency and network bandwidth efficiency. Moreover, we introduce the Video Loss Indicator (VLI) and Quality Variance Indicator (QVI) to assess video quality loss and quality difference between tiles. We also propose a multi-armed bandit (MAB) algorithm with confidence bound-based reward (video quality) and cost (cybersickness) estimation. The algorithm learns the weighting factor of each user&#39;s cost to slow down cybersickness accumulation for users with high cybersickness tendencies. We prove that the algorithm converges to an optimal solution over time. According to simulation with real network settings, our proposed algorithms outperform baselines in terms of video quality and cybersickness accumulation.},
  archive      = {J_TMC},
  author       = {Chiao-Wen Lin and De-Nian Yang and Wanjiun Liao},
  doi          = {10.1109/TMC.2024.3514852},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3423-3440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobile tile-based 360$^\circ$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow/&gt;&lt;mml:mo&gt;∘&lt;/mml:mo&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt; video multicast with cybersickness alleviation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing RFID technology for virtual boundary detection.
<em>TMC</em>, <em>24</em>(4), 3407–3422. (<a
href="https://doi.org/10.1109/TMC.2024.3514895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A boundary is a physical or virtual line that marks the edge or limit of a specific region, which has been widely used in many applications, such as autonomous driving, virtual wall, and robotic lawn mowers. However, none of existing work can well balance the deployability and the scalability of a boundary. In this paper, we propose a brand new RFID-based virtual boundary scheme together with its detection algorithm called RF-Boundary, which has the competitive advantages of being battery-free and easy-to-maintain. We develop two technologies of phase gradient and dual-antenna AoA to address the key challenges posed by RF-boundary, in terms of lack of calibration information and multi-edge interference. Besides, we consider the presence of multipath in the real world applications, model the effect on signals in the dynamic scenarios, and demonstrate the robustness of our phase gradient-based scheme under multipath. We implement a prototype of RF-Boundary with commercial RFID systems and a mobile robot. Extensive experiments verify the feasibility as well as the good performance of RF-Boundary, with a mean detection error of only 8.6 cm.},
  archive      = {J_TMC},
  author       = {Xiaoyu Li and Jia Liu and Zihao Lin and Xuan Liu and Yanyan Wang and Shigeng Zhang and Baoliu Ye},
  doi          = {10.1109/TMC.2024.3514895},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3407-3422},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Advancing RFID technology for virtual boundary detection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASMAFL: Adaptive staleness-aware momentum asynchronous
federated learning in edge computing. <em>TMC</em>, <em>24</em>(4),
3390–3406. (<a href="https://doi.org/10.1109/TMC.2024.3510135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with synchronous federated learning (FL), asynchronous FL (AFL) has attracted more and more attention in edge computing (EC) fields because of its strong adaptability to heterogeneous application scenarios. However, the non-independent and identically distributed (Non-IID) data across devices and the staleness-aware estimation of unreliable wireless connections and limited edge resources make it much more difficult to achieve better AFL-related applications. To handle this problem, we propose an Adaptive Staleness-aware Momentum Accelerated AFL (ASMAFL) algorithm to reduce the resources consumption of heterogeneous wireless communication EC (WCEC) scenarios, as well as decrease the negative impact of Non-IID data for model training. Specifically, we first introduce the staleness-aware parameter and a unified momentum gradient descent (GD) framework to reformulate AFL. Then, we establish global convergence properties of AFL, derive an upper bound on AFL convergence rate, and find that the bound is related to the staleness-aware parameter and Non-IIDness. Next, we formulate the bound into a minimization problem of resource consumption under given model accuracy, and the corresponding staleness-aware parameter of devices will be recomputed after each asynchronous aggregation to eliminate the differences of local models’ contribution to global model aggregation. Finally, extensive experiments are carried out to validate the superiority of ASMAFL in model accuracy, convergence rate, resources consumption, Non-IID issue, etc.},
  archive      = {J_TMC},
  author       = {Dewen Qiao and Songtao Guo and Jun Zhao and Junqing Le and Pengzhan Zhou and Mingyan Li and Xuetao Chen},
  doi          = {10.1109/TMC.2024.3510135},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3390-3406},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ASMAFL: Adaptive staleness-aware momentum asynchronous federated learning in edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of models and strategies for computation
offloading in the internet of vehicles: Efficiency and trust.
<em>TMC</em>, <em>24</em>(4), 3372–3389. (<a
href="https://doi.org/10.1109/TMC.2024.3509542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Vehicles (IoV), vehicles will generate massive data and computation demands, necessitating computation offloading at the edge. However, existing research faces challenges in efficiency and trust. In this paper, we explore the IoV computation offloading from both user and edge facility provider perspectives, working to optimize the quality of experience (QoE), load balancing, and success rate based on challenges to efficiency and trust. First, two vehicle interconnection models are constructed to extend the linkable range of intra-road and inter-road vehicles while considering the maximum link time constraint. Then, a dynamic planning method is proposed, combining the reputation and feedback mechanisms, which can schedule edge resources online based on the cumulative computation latency of each service side, reliability value, and historical behavior. These two phases further improve the efficiency of edge services. Subsequently, blockchain is combined to optimize the trust problem of edge collaboration, and an edge-limited Byzantine fault tolerance local consensus mechanism is proposed to optimize consensus efficiency and ensure the reliability of edge services. Finally, this paper conducts dynamic experiments on real-world datasets, verifying the effectiveness of the proposed algorithm and models in multiple vehicle density datasets and experimental scenarios.},
  archive      = {J_TMC},
  author       = {Qinghang Gao and Jianmao Xiao and Zhiyong Feng and Jingyu Li and Yang Yu and Hongqi Chen and Qiaoyun Yin},
  doi          = {10.1109/TMC.2024.3509542},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3372-3389},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimization of models and strategies for computation offloading in the internet of vehicles: Efficiency and trust},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient hybrid transmission for cell-free systems via NOMA
and multiuser diversity. <em>TMC</em>, <em>24</em>(4), 3359–3371. (<a
href="https://doi.org/10.1109/TMC.2024.3514165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free technology is considered a pivotal advancement for next-generation mobile communications, which can effectively enhance the quality of service for user equipments (UEs) located at the cell edge. For cell-free systems, in this paper, we propose a hybrid downlink transmission method that combines non-orthogonal multiple access (NOMA) and multiuser diversity (MUD). To evaluate the communication performance of the system, we derive closed-form expressions for both instantaneous and average sum rates of UEs using the NOMA and MUD transmission methods. Furthermore, we comprehensively investigate the spectrum efficiency of the NOMA and MUD transmission methods to provide a basis for selecting the hybrid transmission strategy. On the basis of the proposed hybrid transmission strategy, we can derive an optimal hybrid transmission strategy for the scenarios with two access points (APs) and two UEs. Particularly, we extend the aforementioned strategy to the scenarios with multiple UEs, and formulate an optimization problem to maximize the system spectrum efficiency subject to the transmission strategy and power allocation. Furthermore, we propose a low-complexity user selection strategy and power allocation algorithm to solve the problem. Numerical results demonstrate that the hybrid transmission method and power allocation strategy can achieve higher system spectrum efficiency. Our results reveal the influence of key parameters on the downlink spectrum efficiency, analytically and numerically.},
  archive      = {J_TMC},
  author       = {Lin Bai and Jinpeng Xu and Jiaxing Wang and Rui Han and Jinho Choi},
  doi          = {10.1109/TMC.2024.3514165},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3359-3371},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient hybrid transmission for cell-free systems via NOMA and multiuser diversity},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge assisted low-latency cooperative BEV perception with
progressive state estimation. <em>TMC</em>, <em>24</em>(4), 3346–3358.
(<a href="https://doi.org/10.1109/TMC.2024.3509716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern intelligent vehicles (IVs) are equipped with a variety of sensors and communication modules, empowering Advanced Driver Assistance Systems (ADAS) and enabling inter-vehicle connectivity. This paper focuses on multi-vehicle cooperative perception, with a primary objective of achieving low latency. The task involves nearby cooperative vehicles sending their camera data to an edge server, which then merges the local views to create a global traffic view. While multi-camera perception has been actively researched, existing solutions often rely on deep learning models, resulting in excessive processing latency. In contrast, we propose leveraging the state estimation technique from the robotics field for this task. We explicitly model and solve for the system state, addressing additional challenges brought by object mobility and vision obstruction. Furthermore, we introduce a progressive state estimation pipeline to further accelerate system state notifications, supported by a motion prediction method that optimizes position accuracy and perception smoothness. Experimental results demonstrate the superiority of our approach over the deep learning method, with 12.0 × to 27.4 × reductions in server processing delay, while maintaining mean absolute errors below 1 m.},
  archive      = {J_TMC},
  author       = {Yuhan Lin and Haoran Xu and Zhimeng Yin and Guang Tan},
  doi          = {10.1109/TMC.2024.3509716},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3346-3358},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge assisted low-latency cooperative BEV perception with progressive state estimation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint encoding and enhancement for low-light video analytics
in mobile edge networks. <em>TMC</em>, <em>24</em>(4), 3330–3345. (<a
href="https://doi.org/10.1109/TMC.2024.3514214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present our design and analysis of a Joint Encoding and Enhancement (JEE) system for low-light video analytics in mobile edge networks. First, it is observed that, relying solely on a single pipeline for encoding and enhancement of mobile videos proves insufficient, because of the fluctuations in end-edge bandwidth and computing resources. Therefore, two distinct pipelines are introduced in the JEE system, namely, the encode-decode-enhance pipeline and the enhance-encode-decode pipeline. We then characterize the relationship of accuracy, transmission overhead, and computing overhead of these two pipelines through extensive experiments. Considering the significant demands of transmission and computing for low-light videos, we formulate an optimization problem to strike a balance between accuracy and delay, where the available end-edge bandwidth and computing resources are unknown in advance. To solve this mixed-integer nonlinear programming problem, we propose an algorithm based on online gradient descent, enabling adaptive pipeline selection and joint encoding and enhancement configuration. Theoretical analysis indicates that the proposed algorithm achieves sub-linear dynamic regret, highlighting its capability to the accuracy improvement and delay reduction in online environments. Experimental comparison against baselines demonstrates that, JEE can achieve up to a 27.32% increase in accuracy and a 26.18% reduction in delay.},
  archive      = {J_TMC},
  author       = {Yuanyi He and Peng Yang and Tian Qin and Jiawei Hou and Ning Zhang},
  doi          = {10.1109/TMC.2024.3514214},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3330-3345},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint encoding and enhancement for low-light video analytics in mobile edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offloading game for mobile edge computing with random access
in IoT. <em>TMC</em>, <em>24</em>(4), 3316–3329. (<a
href="https://doi.org/10.1109/TMC.2024.3514204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Things (IoT), numerous devices and sensors are deployed to collect data sets. Although some IoT devices can process data locally, most devices may have limited power and computational capability. Since mobile edge computing (MEC) is a new paradigm to provide strong computing capability at the edge of networks close to users, these devices can offload their tasks to MEC servers. Therefore, designing an efficient computation offloading strategy to decide whether the tasks to be offloaded to MEC servers becomes crucial. In this paper, we study the computation offloading for IoT devices based on a non-cooperative game with one-shot random access, where users’ offloading decisions can be made independently to realize distributed offloading. In particular, we discuss the offloading game with and without sharing information among devices and find the Nash equilibrium (NE). Besides, we analyze the effective bandwidth as a performance metric from a device perspective, which considering the Quality of Service (QoS) of network layer while analyzing users’ offloading strategies. Simulation results show the effectiveness of proposed strategies and the impact of offloading tasks to users’ strategies in time-varying channel based on effective bandwidth.},
  archive      = {J_TMC},
  author       = {Rui Han and Yue Yu and Qingzhe Zeng and Jiaxing Wang and Lin Bai and Jinho Choi and Wei Zhang},
  doi          = {10.1109/TMC.2024.3514204},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3316-3329},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Offloading game for mobile edge computing with random access in IoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedMTPP: Federated multivariate temporal point processes for
distributed event sequence forecasting. <em>TMC</em>, <em>24</em>(4),
3302–3315. (<a href="https://doi.org/10.1109/TMC.2024.3509915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile network technology and wearable mobile devices, user-scenario interactions generate a large amount of user behavioral data in the form of multivariate event sequences. Due to data isolation, these multi-scenario events need to be jointly trained to achieve better prediction results. However, traditional federated learning methods face significant challenges when handling distributed event sequences. And the effectiveness of existing modeling approaches for event sequences in federated contexts has not been thoroughly explored. To this end, we propose Federated Multivariate Temporal Point Processes (FedMTPP), which enables learning from distributed event sequences within a novel federated learning framework and leverages efficient event modeling technology, MTPP, to forecast future events. Specifically, FedMTPP restores the temporal structure of the original event sequence by rearranging event embeddings and redesigns the autoregressive-based hidden representation computation in traditional MTPP, making it more suitable for federated prediction tasks. Additionally, FedMTPP incorporates advanced encryption techniques to effectively safeguard user privacy and security. Experimental results on both synthetic and real datasets demonstrate that FedMTPP substantially improves the performance of local models and achieves results comparable to state-of-the-art centralized MTPP methods.},
  archive      = {J_TMC},
  author       = {Houxin Gong and Haishuai Wang and Peng Zhang and Sheng Zhou and Hongyang Chen and Jiajun Bu},
  doi          = {10.1109/TMC.2024.3509915},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3302-3315},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedMTPP: Federated multivariate temporal point processes for distributed event sequence forecasting},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel insights from a cross-layer analysis of TCP and UDP
traffic over full-duplex WLANs. <em>TMC</em>, <em>24</em>(4), 3288–3301.
(<a href="https://doi.org/10.1109/TMC.2024.3510099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Full-duplex (FD) communication is a promising new technology that enables simultaneous transmission and reception in wireless local area networks (WLANs). The benefits of FD on the medium access control (MAC) layer throughput in IEEE 802.11 WLANs are well-documented. However, cross-layer interactions between the FD MAC protocol and transport layer protocols such as Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) are less explored. We consider a WLAN with uplink and downlink TCP flows as well as UDP flows between stations (STAs) and a server via an FD access point (AP). We study an STA-initiated FD MAC protocol in which the AP can transmit on the downlink while receiving on the uplink. Using a novel FD-specific STA saturation approximation, Markov renewal theory, and fixed-point analysis, we derive novel expressions for the uplink and downlink TCP and UDP saturation throughputs. Our analysis shows that the AP is no longer a bottleneck and may be unsaturated unlike in conventional half-duplex (HD) WLANs. Despite greater contention and cross-link interference between STAs, FD achieves a higher TCP throughput than HD. FD causes a significant degradation in the UDP throughput. In the unsaturated regime, FD achieves a lower average downlink TCP packet delay than HD.},
  archive      = {J_TMC},
  author       = {Vinay U. Pai and Neelesh B. Mehta and Chandramani Singh},
  doi          = {10.1109/TMC.2024.3510099},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3288-3301},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Novel insights from a cross-layer analysis of TCP and UDP traffic over full-duplex WLANs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoS-driven contextual MAB for MPQUIC supporting video
streaming in mobile networks. <em>TMC</em>, <em>24</em>(4), 3274–3287.
(<a href="https://doi.org/10.1109/TMC.2024.3507051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video streaming performance may degrade substantially in a mobile environment due to fast-changing wireless links. On the other hand, to provide ubiquitous services, heterogeneous static and mobile access and backbone networks will be integrated in the sixth-generation (6G) systems, so mobile users can take advantage of multiple access options for better services. Multi-path transport-layer protocols like Multi-Path QUIC (MPQUIC) show promise in utilizing multiple access links to address the impact of mobility. However, the optimal link selection that aims to provide statistical QoS guarantee for video streaming in a mobile environment with both user mobility and network mobility remains an open issue. In this paper, based on a lightweight Multi-Armed Bandit (MAB) technique, we develop a QoS-driven Contextual MAB (QC-MAB) framework for MPQUIC, which makes an intelligent access network selection and adaptively enables FEC coding to trade off delay, reliability and goodput. Extensive simulation results with ns-3 show that the proposed QC-MAB framework can outperform the state-of-the-art solutions. It achieves up to ten times lower video interruption ratio and three times higher goodput in highly dynamic mobile environments.},
  archive      = {J_TMC},
  author       = {Wenjun Yang and Lin Cai and Shengjie Shu and Amir Sepahi and Zhiming Huang and Jianping Pan},
  doi          = {10.1109/TMC.2024.3507051},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3274-3287},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoS-driven contextual MAB for MPQUIC supporting video streaming in mobile networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeLLM: Fast on-device LLM inference with speculative
decoding. <em>TMC</em>, <em>24</em>(4), 3256–3273. (<a
href="https://doi.org/10.1109/TMC.2024.3513457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device&#39;s memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3× faster than existing engines.},
  archive      = {J_TMC},
  author       = {Daliang Xu and Wangsong Yin and Hao Zhang and Xin Jin and Ying Zhang and Shiyun Wei and Mengwei Xu and Xuanzhe Liu},
  doi          = {10.1109/TMC.2024.3513457},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3256-3273},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EdgeLLM: Fast on-device LLM inference with speculative decoding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-enabled multiple sensitive task-offloading
mechanism for MEC applications. <em>TMC</em>, <em>24</em>(4), 3241–3255.
(<a href="https://doi.org/10.1109/TMC.2024.3507153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As mobile devices proliferate and mobile applications diversify, Mobile Edge Computing (MEC) has become widely adopted to efficiently allocate computing resources at the network edge and alleviate network congestion. In the MEC initial phase, the absence of vital information presents challenges in devising task-offloading policies, and identifying malicious devices responsible for providing inaccurate feedback is complex. To fill in such gaps, we introduce a consortium blockchain-enabled Committee Voting based Task Offloading Model (CVTOM) to collaboratively formulate resource allocation policies and establish deterrence against malicious servers producing erroneous results intentionally. Different voting principle mechanisms of each committee member are first designed in a Blockchain-enabled system which helps to represent the system&#39;s resource status. Additionally, we propose a Multi-armed Bandits related Thompson Sampling based Adaptive Preference Optimization (TSAPO) algorithm for task-offloading policy, enhancing the timely identification of potent edge servers to improve computing resource utilization which first considers dynamic edge server space and parallel computing scenarios. The solid proof process greatly contributes to the theoretical analysis of the TSAPO. The simulation experiments demonstrate the delay and budget can be reduced by around 25% and 10% respectively, showcasing the superior performance of our approach.},
  archive      = {J_TMC},
  author       = {Yang Xu and Hangfan Li and Cheng Zhang and Zhiqing Tang and Xiaoxiong Zhong and Ju Ren and Hongbo Jiang and Yaoxue Zhang},
  doi          = {10.1109/TMC.2024.3507153},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3241-3255},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-enabled multiple sensitive task-offloading mechanism for MEC applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PandORA: Automated design and comprehensive evaluation of
deep reinforcement learning agents for open RAN. <em>TMC</em>,
<em>24</em>(4), 3223–3240. (<a
href="https://doi.org/10.1109/TMC.2024.3505781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The highly heterogeneous ecosystem of Next Generation (NextG) wireless communication systems calls for novel networking paradigms where functionalities and operations can be dynamically and optimally reconfigured in real time to adapt to changing traffic conditions and satisfy stringent and diverse Quality of Service (QoS) demands. Open Radio Access Network (RAN) technologies, and specifically those being standardized by the O-RAN Alliance, make it possible to integrate network intelligence into the once monolithic RAN via intelligent applications, namely, xApps and rApps. These applications enable flexible control of the network resources and functionalities, network management, and orchestration through data-driven intelligent control loops. Recent work has showed how Deep Reinforcement Learning (DRL) is effective in dynamically controlling O-RAN systems. However, how to design these solutions in a way that manages heterogeneous optimization goals and prevents unfair resource allocation is still an open challenge, with the logic within DRL agents often considered as a opaque system. In this paper, we introduce PandORA, a framework to automatically design and train DRL agents for Open RAN applications, package them as xApps and evaluate them in the Colosseum wireless network emulator. We benchmark 23 xApps that embed DRL agents trained using different architectures, reward design, action spaces, and decision-making timescales, and with the ability to hierarchically control different network parameters. We test these agents on the Colosseum testbed under diverse traffic and channel conditions, in static and mobile setups. Our experimental results indicate how suitable fine-tuning of the RAN control timers, as well as proper selection of reward designs and DRL architectures can boost network performance according to the network conditions and demand. Notably, finer decision-making granularities can improve Massive Machine-Type Communications (mMTC)’s performance by $\sim\! 56\%$ and even increase Enhanced Mobile Broadband (eMBB) Throughput by $\sim\! 99\%$.},
  archive      = {J_TMC},
  author       = {Maria Tsampazi and Salvatore D&#39;Oro and Michele Polese and Leonardo Bonati and Gwenael Poitau and Michael Healy and Mohammad Alavirad and Tommaso Melodia},
  doi          = {10.1109/TMC.2024.3505781},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3223-3240},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PandORA: Automated design and comprehensive evaluation of deep reinforcement learning agents for open RAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to collaborate: Towards maximizing the generalization
performance in cross-silo federated learning. <em>TMC</em>,
<em>24</em>(4), 3211–3222. (<a
href="https://doi.org/10.1109/TMC.2024.3509852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has attracted vivid attention as a privacy-preserving distributed learning framework. In this work, we focus on cross-silo FL, where clients become the model owners after training and are only concerned about the model&#39;s generalization performance on their local data. Due to the data heterogeneity issue, asking all the clients to join a single FL training process may result in model performance degradation. To investigate the effectiveness of collaboration, we first derive a generalization bound for each client when collaborating with others or when training independently. We show that the generalization performance of a client can be improved by collaborating with other clients that have more training data and similar data distributions. Our analysis allows us to formulate a client utility maximization problem by partitioning clients into multiple collaborating groups. A hierarchical clustering-based collaborative training (HCCT) scheme is then proposed, which does not need to fix in advance the number of groups. We further analyze the convergence of HCCT for general non-convex loss functions which unveils the effect of data similarity among clients. Extensive simulations show that HCCT achieves better generalization performance than baseline schemes, whereas it degenerates to independent training and conventional FL in specific scenarios.},
  archive      = {J_TMC},
  author       = {Yuchang Sun and Marios Kountouris and Jun Zhang},
  doi          = {10.1109/TMC.2024.3509852},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3211-3222},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {How to collaborate: Towards maximizing the generalization performance in cross-silo federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint trajectory planning and task offloading for MIMO
AAV-aided mobile edge computing. <em>TMC</em>, <em>24</em>(4),
3196–3210. (<a href="https://doi.org/10.1109/TMC.2024.3510272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is conducive to reducing service response time and improving service quality by pushing cloud functions to a network&#39;s edges. Most existing works in edge computing focus on utility maximization of task offloading on static edges with a single antenna. Besides, trajectory planning of mobile edges, e.g., autonomous aerial vehicles (AAVs) is also rarely discussed. In this paper, we are the first to jointly discuss the deadline-ware task offloading and AAV trajectory planning problem in a multi-input multi-output (MIMO) AAV-aided mobile edge computing system. Due to discrete variables and highly coupling nonconvex constraints, we equivalently convert the original problem into a more solvable form by introducing auxiliary variables. Next, a penalty dual decomposition-based algorithm is developed to achieve a global optimal solution to the problem. Besides, we proposed a profit-based fireworks algorithm in a relatively lower time to reduce the execution time for large-scale networks. Extensive evaluation results reveal that our proposed optimal algorithms could significantly outperform static offloading algorithms and other algorithms by 25% on average.},
  archive      = {J_TMC},
  author       = {Xuewen Dong and Shuangrui Zhao and Ximeng Liu and Zijie Di and Yuzhen Zhang and Yulong Shen},
  doi          = {10.1109/TMC.2024.3510272},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3196-3210},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint trajectory planning and task offloading for MIMO AAV-aided mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wireless eavesdropping on wired audio with radio-frequency
retroreflector attack. <em>TMC</em>, <em>24</em>(4), 3178–3195. (<a
href="https://doi.org/10.1109/TMC.2024.3505268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated the feasibility of eavesdropping on audio via radio frequency signals or videos, which capture physical surface vibrations from surrounding objects. However, these methods are inadequate for intercepting internally transmitted audio through wired media. In this work, we introduce radio-frequency retroreflector attack (RFRA) and bridge this gap by proposing an RFRA-based eavesdropping system, RF-Parrot${}^{\mathbf {2}}$, capable of wirelessly capturing audio signals transmitted through earphone wires. Our system entails embedding a tiny field-effect transistor within the wire to establish a battery-free retroreflector, whose reflective efficiency is correlated with the amplitude of the audio signal. To preserve the details of audio signals, we designed a unique retroreflector using a depletion-mode MOSFET (D-MOSFET). This MOSFET can be triggered by any voltage level present in the audio signals, thus guaranteeing no information loss during activation. However, the D-MOSFET introduces a nonlinear convolution operation on the original audio, resulting in distorted audio eavesdropping. Thus, we devised an engineering solution which utilized a novel convolutional neural network in conjunction with an efficient Parallel WaveGAN vocoder to reconstruct the original audio. Our comprehensive experiments demonstrate a strong similarity between the reconstructed audio and the original, achieving an impressive 95% accuracy in speech command recognition.},
  archive      = {J_TMC},
  author       = {Genglin Wang and Zheng Shi and Yanni Yang and Zhenlin An and Guoming Zhang and Pengfei Hu and Xiuzhen Cheng and Jiannong Cao},
  doi          = {10.1109/TMC.2024.3505268},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3178-3195},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Wireless eavesdropping on wired audio with radio-frequency retroreflector attack},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HearLoc: Locating unknown sound sources in 3D with a
small-sized microphone array. <em>TMC</em>, <em>24</em>(4), 3163–3177.
(<a href="https://doi.org/10.1109/TMC.2024.3507035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor Sound Source Localization (ISSL) is under growing focus with the rapid development of smart IOT intelligence. The predominant approaches typically involve constructing large microphone (Mic) array systems or extracting multiple angles of arrival (AOAs). However, the performance of these solutions is often constrained by the physical size of the array. Besides, there has been limited focus on 3D localization with a single small-sized Mic array. In this paper, we propose HearLoc, an ISSL system that can directly locate 3D sources with a ten-$cm$ Mic array. We demonstrate that the localization ability and dimensional capability can be significantly enhanced by incorporating the time differences of arrival (TDOAs) between the line-of-sight (LOS) and ECHO signals from nearby reflective surfaces. Our approach involves a localization method that selectively sums the correlation powers at useful TDOAs induced by each location. We also design a data processing pipeline with interpolation, normalization and pruning techniques to improve system accuracy and efficiency. To further enhance scalability, we design an iterative algorithm for the ISSL problem with multiple sources and an array location calibration scheme. Experiments demonstrate that the HearLoc can effectively locate sound sources, exhibiting $2\times$/$3.7\times$ improvements in accuracy for 2D and 3D localization, respectively, and a $4\times$ increase in efficiency compared to the existing AOA-based ISSL solutions.},
  archive      = {J_TMC},
  author       = {Zhaohui Li and Yongmin Zhang and Lin Cai and Yaoxue Zhang},
  doi          = {10.1109/TMC.2024.3507035},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3163-3177},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HearLoc: Locating unknown sound sources in 3D with a small-sized microphone array},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CL-shield: A continuous learning system for protecting user
privacy. <em>TMC</em>, <em>24</em>(4), 3148–3162. (<a
href="https://doi.org/10.1109/TMC.2024.3504721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The video analytics system utilizes deep learning models (DNN) to perform inference on the videos captured by cameras. Continuous learning algorithms are used to address the data drift problem in video analytics systems. However, uploading images from deployment environments and processing on the cloud carry the risk of privacy leakage. In this paper, we have designed a system called CL-Shield to protect user’s privacy. First, we review the causes of privacy leakage in a continuous learning system and propose the objective of full privacy protection. Second, we design an online training mechanism based on a scene library to avoid direct uploading of user’s frames to the cloud server. Lastly, we design a fast training set search algorithm based on a novel Ebv-List, which effectively improves the speed of model updates. We collect various real-world scenario data to build our scene library and validate our system on a dataset of over 10 hours. The experiments demonstrate that our privacy-aware continuous learning system achieves an F1-score of over 92% compared to the conventional systems without protecting privacy and has long-term stability in analytic F1-score.},
  archive      = {J_TMC},
  author       = {Tianyu Li and Hanling Wang and Qing Li and Yong Jiang and Zhenhui Yuan},
  doi          = {10.1109/TMC.2024.3504721},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3148-3162},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CL-shield: A continuous learning system for protecting user privacy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TJCCT: A two-timescale approach for UAV-assisted mobile edge
computing. <em>TMC</em>, <em>24</em>(4), 3130–3147. (<a
href="https://doi.org/10.1109/TMC.2024.3505155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services in close proximity to mobile devices (MDs). However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply discrepancy between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different time-scale dynamics of the network. To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility. Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility. Since the problem is a non-convex and NP-hard mixed integer nonlinear programming (MINLP), we propose a two-timescale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach for solving the problem. In the short timescale, we propose a price-incentive model for on-demand computing resource allocation and a matching mechanism-based method for computation offloading. In the long timescale, we propose a convex optimization-based method for UAV trajectory control. Besides, we theoretically prove the stability and polynomial complexity of TJCCT. Extensive simulation results demonstrate that the proposed TJCCT is able to achieve superior performances in terms of the system utility, average processing rate, average completion delay, average completion ratio, and average cost, while meeting the energy constraints despite the trade-off of the increased energy consumption.},
  archive      = {J_TMC},
  author       = {Zemin Sun and Geng Sun and Qingqing Wu and Long He and Shuang Liang and Hongyang Pan and Dusit Niyato and Chau Yuen and Victor C. M. Leung},
  doi          = {10.1109/TMC.2024.3505155},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3130-3147},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TJCCT: A two-timescale approach for UAV-assisted mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSTAR-FL: Stochastic client selection for tree all-reduce
federated learning. <em>TMC</em>, <em>24</em>(4), 3110–3129. (<a
href="https://doi.org/10.1109/TMC.2024.3507381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is widely applied in privacy-sensitive domains, such as healthcare, finance, and education, due to its privacy-preserving properties. However, implementing FL in dynamic wireless networks poses substantial communication challenges. Central to these challenges is the need for efficient communication strategies that can adapt to fluctuating network conditions and the growing number of participating devices, which can lead to unacceptable communication delays. In this article, we propose Stochastic Client Selection for Tree All-Reduce Federated Learning (CSTAR-FL), a novel approach that combines a probabilistic User Device (UD) selection strategy with a tree-based communication architecture to enhance communication efficiency in FL within densely populated wireless networks. By optimizing UD selection for effective model aggregation and employing an efficient data transmission structure, CSTAR-FL significantly reduces communication time and improves FL efficiency. Additionally, our approach ensures high global model accuracy under scenarios where data distribution is heterogeneous from User Device (UD)s. Extensive simulations in dynamic wireless network scenarios demonstrate that CSTAR-FL outperforms existing state-of-the-art methods, reducing model convergence time by up to 40% without losing the global model accuracy. This makes CSTAR-FL a robust solution for efficient and scalable FL deployments in high-density environments.},
  archive      = {J_TMC},
  author       = {Zimu Xu and Antonio Di Maio and Eric Samikwa and Torsten Braun},
  doi          = {10.1109/TMC.2024.3507381},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3110-3129},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSTAR-FL: Stochastic client selection for tree all-reduce federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User association and channel allocation in 5G mobile
asymmetric multi-band heterogeneous networks. <em>TMC</em>,
<em>24</em>(4), 3092–3109. (<a
href="https://doi.org/10.1109/TMC.2024.3503632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of mobile terminals, the continuous upgrading of services, 4G LTE networks are showing signs of weakness. To enhance the capacity of wireless networks, millimeter waves are introduced to drive the evolution of networks towards multi-band 5G heterogeneous networks. The distinct propagation characteristics of mmWaves, microwaves, as well as the vastly different hardware configurations of heterogeneous base stations, make traditional access strategies no longer effective. Therefore, to narrowing the gap between theory, practice, we investigate the access strategy in multi-band 5G heterogeneous networks, taking into account the characteristics of mobile users, asynchronous switching between uplink, downlink of pico base stations, asymmetric service requirements, user communication continuity. We formulate the problem as integer nonlinear programming, prove its intractability. Thereby, we decouple it into three subproblems: user association, switch point selection, subchannel allocation, design an algorithm based on optimal matching, spectral clustering to solve it efficiently. The simulation results show that the proposed algorithm outperforms the comparison methods in terms of overall data rate, effective data rate, number of satisfied users.},
  archive      = {J_TMC},
  author       = {Miao Dai and Gang Sun and Hongfang Yu and Sheng Wang and Dusit Niyato},
  doi          = {10.1109/TMC.2024.3503632},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3092-3109},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {User association and channel allocation in 5G mobile asymmetric multi-band heterogeneous networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparsified random partial model update for personalized
federated learning. <em>TMC</em>, <em>24</em>(4), 3076–3091. (<a
href="https://doi.org/10.1109/TMC.2024.3507286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) stands as a privacy-preserving machine learning paradigm that enables collaborative training of a global model across multiple clients. However, the practical implementation of FL models often confronts challenges arising from data heterogeneity and limited communication resources. To address the aforementioned issues simultaneously, we develop a Sparsified Random Partial Update framework for personalized Federated Learning (SRP-pFed), which builds upon the foundation of dynamic partial model updates. Specifically, we decouple the local model into personal and shared parts to achieve personalization. For each client, the ratio of its personal part associated with the local model, referred to as the update rate, is regularly renewed over the training procedure via a random walk process endowed with reinforced memory. In each global iteration, clients are clustered into different groups where the ones in the same group share a common update rate. Benefiting from such design, SRP-pFed realizes model personalization while substantially reducing communication costs in the uplink transmissions. We conduct extensive experiments on various training tasks with diverse heterogeneous data settings. The results demonstrate that the SRP-pFed consistently outperforms the state-of-the-art methods in test accuracy and communication efficiency.},
  archive      = {J_TMC},
  author       = {Xinyi Hu and Zihan Chen and Chenyuan Feng and Geyong Min and Tony Q. S. Quek and Howard H. Yang},
  doi          = {10.1109/TMC.2024.3507286},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3076-3091},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sparsified random partial model update for personalized federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentive mechanism design for cross-device federated
learning: A reinforcement auction approach. <em>TMC</em>,
<em>24</em>(4), 3059–3075. (<a
href="https://doi.org/10.1109/TMC.2024.3508260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the operational context of a cross-device federated learning (FL), the efficient allocation of resources, such as transmission powers, channels, and computation resources, significantly impacts overall performance. Existing research in cross-device FL has predominantly concentrated on either resource allocation to enhance training accuracy or incentivizing participation, while ignoring their integrated designs for further improving the performance in cross-device FL. Different from existing work, in this paper, we jointly integrate the power allocation, channel assignment, user selection, and allocation of computation frequency into the design of incentive mechanism, where each mobile user plays a dual role as both a buyer and a seller. Because of complex resource allocation, truthfulness guarantee in a dual role scenario, and unavailable prior information, the considered mechanism design problem is challenging. To tackle such combinatorial problem, we propose a Reinforcement Auction Mechanism (RAM), comprising two layers. The upper layer features a Hybrid Action Reinforcement Learning scheme to learn the outcomes of user selection and payments. In the lower layer, each selected mobile user optimizes its resources to maximize its utility. Theoretical analyses affirm that our proposed RAM ensures individual rationality and truthfulness. Extensive simulations have been conducted to validate the effectiveness of the proposed RAM.},
  archive      = {J_TMC},
  author       = {Gang Li and Jun Cai and Jianfeng Lu and Hongming Chen},
  doi          = {10.1109/TMC.2024.3508260},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3059-3075},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incentive mechanism design for cross-device federated learning: A reinforcement auction approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective aerial collaborative secure communication
optimization via generative diffusion model-enabled deep reinforcement
learning. <em>TMC</em>, <em>24</em>(4), 3041–3058. (<a
href="https://doi.org/10.1109/TMC.2024.3502685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to flexibility and low-cost, unmanned aerial vehicles (UAVs) are increasingly crucial for enhancing coverage and functionality of wireless networks. However, incorporating UAVs into next-generation wireless communication systems poses significant challenges, particularly in sustaining high-rate and long-range secure communications against eavesdropping attacks. In this work, we consider a UAV swarm-enabled secure surveillance network system, where a UAV swarm forms a virtual antenna array to transmit sensitive surveillance data to a remote base station (RBS) via collaborative beamforming (CB) so as to resist mobile eavesdroppers. Specifically, we formulate an aerial secure communication and energy efficiency multi-objective optimization problem (ASCEE-MOP) to maximize the secrecy rate of the system and to minimize the flight energy consumption of the UAV swarm. To address the non-convex, NP-hard and dynamic ASCEE-MOP, we propose a generative diffusion model-enabled twin delayed deep deterministic policy gradient (GDMTD3) method. Specifically, GDMTD3 leverages an innovative application of diffusion models to determine optimal excitation current weights and position decisions of UAVs. The diffusion models can better capture the complex dynamics and the trade-off of the ASCEE-MOP, thereby yielding promising solutions. Simulation results highlight the superior performance of the proposed approach compared with traditional deployment strategies and some other deep reinforcement learning (DRL) benchmarks. Moreover, performance analysis under various parameter settings of GDMTD3 and different numbers of UAVs verifies the robustness of the proposed approach.},
  archive      = {J_TMC},
  author       = {Chuang Zhang and Geng Sun and Jiahui Li and Qingqing Wu and Jiacheng Wang and Dusit Niyato and Yuanwei Liu},
  doi          = {10.1109/TMC.2024.3502685},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3041-3058},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-objective aerial collaborative secure communication optimization via generative diffusion model-enabled deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coverage-aware high-quality sensing data collection method
in mobile crowd sensing. <em>TMC</em>, <em>24</em>(4), 3025–3040. (<a
href="https://doi.org/10.1109/TMC.2024.3502158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we leverage unmanned aerial vehicles (UAVs) to enhance mobile crowd sensing (MCS) by addressing two critical challenges: uncontrollable data quality and inevitable unsensed points of interest (PoIs). We introduce a UAV-assisted method to deal with these challenges. To ensure the accuracy of sensing data contributed by human participants, the proposed truth discovery method utilizes UAV-collected sensing data as few-shot samples to train the truth discovery model, which is then employed to calibrate sensing data solely collected by human participants. Additionally, to meet the sensing coverage requirement, we present a method that predicts data values for unsensed PoIs by utilizing their historical sensing data and the sensed neighboring PoIs information. The method employs a graph neural network to capture spatio-temporal relationships of the sensing data, facilitating accurate estimation of unsensed PoIs. Through extensive simulations, our approaches demonstrate superior performance compared to existing methods, showcasing the potential of UAV-assisted MCS for overcoming challenges and enhancing data collection efficiency in various domains.},
  archive      = {J_TMC},
  author       = {Ye Wang and Hui Gao and Edith C. H. Ngai and Kun Niu and Tan Yang and Bo Zhang and Wendong Wang},
  doi          = {10.1109/TMC.2024.3502158},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3025-3040},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A coverage-aware high-quality sensing data collection method in mobile crowd sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed learn-to-optimize: Limited communications
optimization over networks via deep unfolded distributed ADMM.
<em>TMC</em>, <em>24</em>(4), 3012–3024. (<a
href="https://doi.org/10.1109/TMC.2024.3502574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed optimization is a fundamental framework for collaborative inference over networks. The operation is modeled as the joint minimization of a shared objective which typically depends on local observations. Distributed optimization algorithms, such as the distributed alternating direction method of multipliers (D-ADMM), iteratively combine local computations and message exchanges. A main challenge associated with distributed optimization, and particularly with D-ADMM, is that it requires a large number of communications to reach consensus. In this work we propose unfolded D-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM to operate reliably with a predefined and small number of messages exchanged by each agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while leveraging data to tune the hyperparameters of each iteration. These hyperparameters can either be agent-specific, aiming at achieving the best performance within a fixed number of iterations over a given network, or shared among the agents, allowing to learn to distributedly optimize over different networks. We specialize unfolded D-ADMM for two representative settings: a distributed sparse recovery setup, and a distributed machine learning learning scenario. Our numerical results demonstrate that the proposed approach dramatically reduces the number of communications utilized by D-ADMM, without compromising on its performance.},
  archive      = {J_TMC},
  author       = {Yoav Noah and Nir Shlezinger},
  doi          = {10.1109/TMC.2024.3502574},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3012-3024},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed learn-to-optimize: Limited communications optimization over networks via deep unfolded distributed ADMM},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart shield: Prevent aerial eavesdropping via cooperative
intelligent jamming based on multi-agent reinforcement learning.
<em>TMC</em>, <em>24</em>(4), 2995–3011. (<a
href="https://doi.org/10.1109/TMC.2024.3505206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spotlight on autonomous aerial vehicles (AAVs) is to enhance wireless communications while ignoring the potential risk of AAVs acting as adversaries. Due to their mobility and flexibility, AAV eavesdroppers pose an immeasurable threat to legitimate wireless transmissions. However, the existing fixed jamming scheme without cooperation cannot counter the flexible and dynamic AAV eavesdropping. In this article, a cooperative intelligent jamming scheme is proposed, authorizing ground jammers (GJs) to interfere with AAV eavesdroppers, generating specific jamming shields between AAV eavesdroppers and legitimate users. Toward this end, we formulate a secrecy capacity maximization problem and model the problem as a decentralized partially observable Markov decision process (Dec-POMDP). To address the challenge of the huge state space and action space with network dynamics, we leverage a deep reinforcement learning (DRL) algorithm with a dueling network and double-Q learning (i.e., dueling double deep Q-network) to train policy networks. Then, we propose a multi-agent mixing network framework (QMIX)-based collaborative jamming algorithm to enable GJs to independently make decisions without sharing local information. Additionally, we perform extensive simulations to validate the superiority of our proposed scheme and present useful insights into practical implementation by elucidating the relationship between the deployment settings of GJs and the instantaneous secrecy capacity.},
  archive      = {J_TMC},
  author       = {Qubeijian Wang and Shiyue Tang and Wen Sun and Yin Zhang and Geng Sun and Hong-Ning Dai and Mohsen Guizani},
  doi          = {10.1109/TMC.2024.3505206},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2995-3011},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Smart shield: Prevent aerial eavesdropping via cooperative intelligent jamming based on multi-agent reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility-aware dependent task offloading in edge computing:
A digital twin-assisted reinforcement learning approach. <em>TMC</em>,
<em>24</em>(4), 2979–2994. (<a
href="https://doi.org/10.1109/TMC.2024.3506221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute tasks from end devices. Task offloading is a fundamental problem in CEC that decides when and where tasks are executed upon the arrival of tasks. However, the mobility of users often results in unstable connections, leading to network failures and resource underutilization. Existing works have not adequately addressed joint mobility-aware dependent task offloading and network flow scheduling, resulting in network congestion and suboptimal performance. To address this, we formulate an online joint mobility-aware dependent task offloading and bandwidth allocation problem, to improve the quality of service by reducing task completion time and energy consumption. We introduce a Mobility-aware Digital Twin-assisted Deep Reinforcement Learning (MDT-DRL) algorithm. Our digital twin model equips the reinforcement learning process by providing future states of mobile users, enabling efficient offloading plans for adapting to the mobile CEC system. Experimental results on real-world and synthetic datasets show that MDT-DRL surpasses state-of-the-art baselines on average task completion time and energy consumption.},
  archive      = {J_TMC},
  author       = {Xiangchun Chen and Jiannong Cao and Yuvraj Sahni and Mingjin Zhang and Zhixuan Liang and Lei Yang},
  doi          = {10.1109/TMC.2024.3506221},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2979-2994},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility-aware dependent task offloading in edge computing: A digital twin-assisted reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating update conflict in non-IID federated learning via
orthogonal class gradients. <em>TMC</em>, <em>24</em>(4), 2967–2978. (<a
href="https://doi.org/10.1109/TMC.2024.3503682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly popular federated learning still faces the practical challenge of non-independent and identically distributed data. Most efforts to address this issue focus on limiting local updates or enhancing model aggregation. However, these methods either restrict the learning capacity of local models or overlook the negative knowledge transfer caused by local objective divergences. In contrast, we observe that the global update can be re-expressed as a weighted sum of the gradients of samples from different classes. Therefore, we hypothesize that the competition among local updates may arise from the conflict between the gradients of samples belonging to different classes. Inspired by this insight, we introduce the novel perspective of orthogonal class gradients, aimed at eliminating interference between updates from different classes without the aforementioned drawbacks. To this end, this paper presents FedOCF, which implements orthogonal class gradient constraints by encouraging orthogonality among features of different classes. Specifically, FedOCF maintains a generator to learn features that are orthogonal for different classes and utilizes it to regularize features learned in local learning. Theoretically, we also demonstrate that FedOCF can improve generalization performance through feature conditional distribution alignment during local learning. Extensive experiments validate the excellent performance of FedOCF in various heterogeneous scenarios.},
  archive      = {J_TMC},
  author       = {Siyang Guo and Yaming Guo and Hui Zhang and Junbo Wang},
  doi          = {10.1109/TMC.2024.3503682},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2967-2978},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mitigating update conflict in non-IID federated learning via orthogonal class gradients},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TapWristband: A wearable keypad system based on wrist
vibration sensing. <em>TMC</em>, <em>24</em>(4), 2949–2966. (<a
href="https://doi.org/10.1109/TMC.2024.3503417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained human motion detection has become increasingly important with the growing popularity of human computer interaction (HCI). However, traditional gesture-based HCI systems often require the design of new operation modes rather than conforming to user habits, thus increasing system learning costs. In this paper, we present TapWristband, a novel wearable sensor-based vibration sensing system that detects finger tapping by measuring wrist vibrations. We first perform real-world experiments to collect measurements for modeling the effects of the tapping motion on wearable wristband sensors including piezoelectric transducer (PZT) and inertial measurement unit (IMU). We find that a damped vibration model can be used to represent the relaxing phase of a vibration response due to tapping motion. Thus, we propose a mutual cross-correlation-based event segmentation algorithm to extract the vibration signal during the relaxing phase. After that, we develop feature extraction and classification algorithms to recognize the tapping patterns of five fingers across twelve key locations of a keypad system. Finally, we performed extensive experiments with thirteen participants to evaluate our system. Experimental results show that our low-cost vibration sensing system can achieve an average accuracy of over 93% with a tapping speed of over 100 taps per minute in real-world tapping scenarios.},
  archive      = {J_TMC},
  author       = {Jialiang Yan and Siyao Cheng and Yang Zhao and Jie Liu},
  doi          = {10.1109/TMC.2024.3503417},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2949-2966},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TapWristband: A wearable keypad system based on wrist vibration sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high-reliability small-area task offloading mechanism with
trust evaluation and fuzzy logic in power IoTs. <em>TMC</em>,
<em>24</em>(4), 2935–2948. (<a
href="https://doi.org/10.1109/TMC.2024.3502167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem that high-priority tasks can not be processed timely and reliably due to the disorder of multi-task and dynamicity in Power Internet of Things(PIoTs), a high-reliability small-area task offloading mechanism with trust evaluation and fuzzy logic(HRSATF) is proposed. First, considering task priority, preemptive priority queue is introduced to ensure high-priority tasks processed preferentially, and minimum resource allocation coefficients(MRACs) of tasks are solved to ensure the effectiveness of offloading. Second, the trust model between smart device(SD) and edge server(ES) is established, and ESs are divided into three priorities based on trust value and computing power by fast non-dominated sorting. Thirdly, fuzzy logic is applied to select target ES when the priorities of task and ES do not match or the ES is offline, and MRAC is used to schedule tasks between SD and ES. Finally, NSGA2 is modified (MNSGA2) to verify the effectiveness of HRSATF in terms of success rate, time, power consumption and load balancing, where success rate is increased by $102.3\%$, and time and power consumption are decreased by $90.7\%$, $89.3\%$ at most, respectively.},
  archive      = {J_TMC},
  author       = {Suhong Wang and Tuanfa Qin and Tingting Chen and Wenhao Guo and Yongle Hu and Hongmin Sun},
  doi          = {10.1109/TMC.2024.3502167},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2935-2948},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A high-reliability small-area task offloading mechanism with trust evaluation and fuzzy logic in power IoTs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). G3R: Generating rich and fine-grained mmWave radar data from
2D videos for generalized gesture recognition. <em>TMC</em>,
<em>24</em>(4), 2917–2934. (<a
href="https://doi.org/10.1109/TMC.2024.3502668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter wave radar is gaining traction recently as a promising modality for enabling pervasive and privacy-preserving gesture recognition. However, the lack of rich and fine-grained radar datasets hinders progress in developing generalized deep learning models for gesture recognition across various user postures (e.g., standing, sitting), positions, and scenes. To remedy this, we resort to designing a software pipeline that exploits wealthy 2D videos to generate realistic radar data, but it needs to address the challenge of simulating diversified and fine-grained reflection properties of user gestures. To this end, we design G3R with three key components: i) a gesture reflection point generator expands the arm&#39;s skeleton points to form human reflection points; ii) a signal simulation model simulates the multipath reflection and attenuation of radar signals to output the human intensity map; iii) an encoder-decoder model combines a sampling module and a fitting module to address the differences in number and distribution of points between generated and real-world radar data for generating realistic radar data. We implement and evaluate G3R using 2D videos from public data sources and self-collected real-world radar data, demonstrating its superiority over other state-of-the-art approaches for gesture recognition.},
  archive      = {J_TMC},
  author       = {Kaikai Deng and Dong Zhao and Wenxin Zheng and Yue Ling and Kangwen Yin and Huadong Ma},
  doi          = {10.1109/TMC.2024.3502668},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2917-2934},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {G3R: Generating rich and fine-grained mmWave radar data from 2D videos for generalized gesture recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualizing the smart environment in AR: An approach based
on visual geometry matching. <em>TMC</em>, <em>24</em>(4), 2900–2916.
(<a href="https://doi.org/10.1109/TMC.2024.3504960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Insight, an AR system for visualizing the IoT-enabled smart environment without relying on the unique appearances, barcodes, world coordinates, or wireless signals of IoT infrastructures. The system analyzes the camera video and motion data taken by mobile AR equipment to extract the self and cross visual geometries describing the poses and geographic distribution of nearby IoT devices. To recognize IoT devices using the extracted geometries, Insight operates in two phases. At deployment time, it learns pairwise mappings from the visual geometries to the corresponding device identities. After that, it leverages the geometries scanned at run time to look for a partial assignment to the recorded geometries, allowing it to automatically recognize the IoT devices in AR view. As such, our system turns the IoT device recognition task into a geometry matching problem, which is further formalized as to perform Subset, Incomplete, and Duplicated Point Cloud Registration (SID-PCR) in this work. We design a deep neural network paying specific edge- and spectral-wise graph attention to solve SID-PCR, and implement a prototype that adaptively requests visual geometry scan and registration operations for accurate recognition. The performance of Insight is validated using both synthetic data and a real-world testbed.},
  archive      = {J_TMC},
  author       = {Ming Xia and Min Huang and Qiuqi Pan and Yunhan Wang and Xiaoyan Wang and Kaikai Chi},
  doi          = {10.1109/TMC.2024.3504960},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2900-2916},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Visualizing the smart environment in AR: An approach based on visual geometry matching},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To optimize human-in-the-loop learning in repeated routing
games. <em>TMC</em>, <em>24</em>(4), 2889–2899. (<a
href="https://doi.org/10.1109/TMC.2024.3502076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today navigation applications (e.g., Waze and Google Maps) enable human users to learn and share the latest traffic observations, yet such information sharing simply aids selfish users to predict and choose the shortest paths to jam each other. Prior routing game studies focus on myopic users in oversimplified one-shot scenarios to regulate selfish routing via information hiding or pricing mechanisms. For practical human-in-the-loop learning (HILL) in repeated routing games, we face non-myopic users of differential past observations and need new mechanisms (preferably non-monetary) to persuade users to adhere to the optimal path recommendations. We model the repeated routing game in a typical parallel transportation network, which generally contains one deterministic path and $N$ stochastic paths. We first prove that no matter under the information sharing mechanism in use or the latest routing literature’s hiding mechanism, the resultant price of anarchy (PoA) for measuring the efficiency loss from social optimum can approach infinity, telling arbitrarily poor exploration-exploitation tradeoff over time. Then we propose a novel user-differential probabilistic recommendation (UPR) mechanism to differentiate and randomize path recommendations for users with differential learning histories. We prove that our UPR mechanism ensures interim individual rationality for all users and significantly reduces $\text{PoA}=\infty$ to close-to-optimal $\text{PoA}=1+\frac{1}{4N+3}$, which cannot be further reduced by any other non-monetary mechanism. In addition to theoretical analysis, we conduct extensive experiments using real-world datasets to generalize our routing graphs and validate the close-to-optimal performance of UPR mechanism.},
  archive      = {J_TMC},
  author       = {Hongbo Li and Lingjie Duan},
  doi          = {10.1109/TMC.2024.3502076},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2889-2899},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {To optimize human-in-the-loop learning in repeated routing games},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiSDA: Subdomain adaptation human activity recognition
method using wi-fi signals. <em>TMC</em>, <em>24</em>(4), 2876–2888. (<a
href="https://doi.org/10.1109/TMC.2024.3501299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition based on Wi-Fi signals has become one part of integrated sensing and communications, which has promising application prospects. Detecting activities across different domains is an important and challenging problem. To reduce model complexity and improve recognition accuracy, we propose a novel approach to realize activity recognition across domains, named WiSDA. The proposed WiSDA contains two parts: data augmentation and a deep learning model. The recursive plots method is employed as the data augmentation to transform Wi-Fi channel state information into images, which can take advantage of the image recognition ability of the latter deep learning model. The proposed learning model utilizes weighted cosine similarity to align feature distributions among sub-domains activated by a deep network layer across different domains, thereby a domain-independent feature representation is generated. Based on this representation, WiSDA can make the recognition decision independent of domains, then the cross-domain recognition accuracy is increased. The numerical results illustrate that WiSDA achieves higher recognition accuracy and has lower complexity. The cross-domain recognition accuracy ranges from 89% to 93% with offline pre-training. Enhancing the pre-trained WiSDA with limited samples boosts cross-domain recognition accuracy to 97%.},
  archive      = {J_TMC},
  author       = {Wanguo Jiao and Changsheng Zhang and Wei Du and Shuai Ma},
  doi          = {10.1109/TMC.2024.3501299},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2876-2888},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WiSDA: Subdomain adaptation human activity recognition method using wi-fi signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative cloud-edge approach for robust edge workload
forecasting. <em>TMC</em>, <em>24</em>(4), 2861–2875. (<a
href="https://doi.org/10.1109/TMC.2024.3502683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of edge computing in the post-COVID19 pandemic period, precise workload forecasting is considered the basis for making full use of the edge-limited resources, and both edge service providers (ESPs) and edge service consumers (ESCs) can benefit significantly from it. Existing paradigms of workload forecasting (i.e., edge-only or cloud-only) are improper, due to failing to consider the inter-site correlations and might suffer from significant data transmission delays. With the increasing adoption of edge platforms by web services, it is critical to balance both accuracy and efficiency in workload forecasting. In this paper, we propose XELASTIC, which offers three key improvements over the conference version. First, we redesigned the aggregation and disaggregation layers using GCNs to capture more complex relationships among workload series. Second, we introduced a supervised contrastive loss to enhance robustness against outliers, particularly for handling missing or abnormal data in real-world scenarios. Finally, we expanded the evaluation with additional baselines and larger datasets. Extensive experiments on realistic edge workload datasets collected from China’s largest edge service provider (Alibaba ENS) show that XELASTIC outperforms state-of-the-art methods, decreases time consumption, and reduces communication costs.},
  archive      = {J_TMC},
  author       = {Yanan Li and Penghong Zhao and Xiao Ma and Haitao Yuan and Zhe Fu and Mengwei Xu and Shangguang Wang},
  doi          = {10.1109/TMC.2024.3502683},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2861-2875},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A collaborative cloud-edge approach for robust edge workload forecasting},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preference-aware vehicle repositioning recommendation for
MoD systems: A coulomb force directed perspective. <em>TMC</em>,
<em>24</em>(4), 2847–2860. (<a
href="https://doi.org/10.1109/TMC.2024.3502235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle repositioning is widely used in Mobility on-Demand (MoD) systems to address supply-demand imbalances and improve order completion rates. Existing methods typically offer repositioning recommendations focused on enhancing vehicle coordination toward supply-demand re-balance. However, these methods often overlook the possibility that drivers may not follow these recommendations due to their personal preferences, leading to recommendation-decision inconsistency and further disrupting the supply-demand balance. To address this issue, we propose a preference-aware vehicle repositioning recommendation strategy for MoD systems, named FREE, which is based on a Coulomb Force directed approach. The core idea is to strike a balance between vehicle coordination and consistency between recommendations and driver decisions. First, we introduce a Coulomb force-based representation (CFR) to model coordination among vehicles. In this model, the interactions between vehicles and orders are represented as forces that drive the repositioning of vehicles. Next, we develop a driver preference learning model that accurately captures drivers’ preferences using triplet and consistency loss. We then integrate these preferences with the CFR into a multi-agent deep reinforcement learning (MADRL) based repositioning algorithm to generate optimal recommendations. Finally, we validate the effectiveness of FREE through simulations using real-world data, demonstrating its superiority over existing benchmarks.},
  archive      = {J_TMC},
  author       = {Xiaobo Zhou and Shuxin Ge and Tie Qiu and Xingwei Wang},
  doi          = {10.1109/TMC.2024.3502235},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2847-2860},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Preference-aware vehicle repositioning recommendation for MoD systems: A coulomb force directed perspective},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards effective transportation mode-aware trajectory
recovery: Heterogeneity, personalization and efficiency. <em>TMC</em>,
<em>24</em>(4), 2832–2846. (<a
href="https://doi.org/10.1109/TMC.2024.3501280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the transportation-aware trajectory recovery problem, which is distinct from the conventional vehicle-based trajectory recovery, facing three major challenges: heterogeneity, personalization and efficiency. For the heterogeneity, the velocity of the mobile object is intrinsically correlated with the specific transportation mode, containing inherent heterogeneity. For the personalization, the trajectory data is complicated by substantial variations in users, which are different in personalized behaviors. For the efficiency, previous works mostly employ sequence-to-sequence framework which limits their efficiency due to the auto-regressive inference pattern. To address these challenges, we design a novel efficient and effective multi-modal deep model, coined as PTrajRec, for transportation-aware trajectory recovery. Specifically, we initially embed location, behavior, and transportation mode modalities in distinct channels, which not only reflect spatio-temporal information encapsulated in location sequences but also introduce the heterogeneity and personalization characteristics associated with mode and behavior sequences. For further modeling these modalities, we employ the auto-correlation mechanism to learn periodic dependencies on the temporal dimension and the graph attention mechanism to learn road network dependencies on the spatial dimension. At last, we propose a dual-view constraint mechanism to assist the efficient trajectory recovery framework and design three auxiliary tasks to address the inherent heterogeneity and efficiency design. Extensive experimental results on two real-world datasets demonstrate the superiority of our proposed method compared to state-of-the-art baselines with reduced computation cost and excellent performance.},
  archive      = {J_TMC},
  author       = {Chenxing Wang and Fang Zhao and Haiyong Luo and Yuchen Fang and Haichao Zhang and Haoyu Xiong},
  doi          = {10.1109/TMC.2024.3501280},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2832-2846},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards effective transportation mode-aware trajectory recovery: Heterogeneity, personalization and efficiency},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HT-FL: Hybrid training federated learning for heterogeneous
edge-based IoT networks. <em>TMC</em>, <em>24</em>(4), 2817–2831. (<a
href="https://doi.org/10.1109/TMC.2024.3502686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous rolling-out of edge computing, Federated Learning (FL) has become a promising solution for intelligent Internet-of-things (IoT). In addition to resource constraints, deploying FL schemes in IoT networks is greatly challenged by heterogeneity in multiple dimensions. While heterogeneity in data distribution and computation capability has been extensively studied, the impact of distinct, even hybrid training paradigms on FL performances remains largely unknown. To answer this open question in the IoT context, we propose a Hybrid-Training Federated Learning (HT-FL) algorithm for the power-constrained IoT networks, incorporating both sequential and parallel training that naturally adapts to various sub-network topologies, while greatly reducing the energy consumption during the training stage. We demonstrate through analysis that the convergence of HT-FL is theoretically guaranteed, achieving $O (\frac{1}{\sqrt{K}})$ for carefully chosen learning rates. Experiments on multiple datasets show that, the proposed HT-FL outperforms existing FL schemes on multiple training tasks under various data distribution settings, while reducing an average of 20% energy consumption. In a more practical sense, a self-adaptive parameter-tuning strategy is also designed for HT-FL deployment, which can be easily extended to other multi-layer FL schemes in complex application scenarios.},
  archive      = {J_TMC},
  author       = {Yixun Gu and Jie Wang and Shengjie Zhao},
  doi          = {10.1109/TMC.2024.3502686},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2817-2831},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HT-FL: Hybrid training federated learning for heterogeneous edge-based IoT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LBDT: A lightweight blockchain-based data trading scheme in
internet of vehicles using proof-of-reputation. <em>TMC</em>,
<em>24</em>(4), 2800–2816. (<a
href="https://doi.org/10.1109/TMC.2024.3497934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of data in the Internet of Vehicles (IoV) has created opportunities to improve traffic safety and efficiency through data trading. However, establishing trust among highly mobile and resource-constrained vehicles poses significant challenges for effective data trading in IoV. To address this issue, we propose a lightweight blockchain-based data trading scheme (LBDT), which ensures secure and efficient data trading in IoV. We introduce a proof-of-reputation (PoR) consensus mechanism to establish trustworthiness for data trading. Specifically, we use a progressive reputation mechainism to support the PoR consensus. LBDT utilizes a parallel-chain structure for the PoR consensus to minimize communication and storage costs while reducing transaction confirmation latency. Additionally, we adopt a double auction mechanism as an incentivizing strategy to encourage vehicle participation in data trading. We evaluate the performance of LBDT through extensive experiments. The experimental results demonstrate that LBDT is highly effective and secure, achieving a transaction latency of approximately 4 seconds. Moreover, LBDT successfully mitigates communication and storage overheads by over 90%, thus establishing its superiority over state-of-the-art solutions under comparable conditions.},
  archive      = {J_TMC},
  author       = {Weilin Chen and Wei Yang and Mingjun Xiao and Lide Xue and Shaowei Wang},
  doi          = {10.1109/TMC.2024.3497934},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2800-2816},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LBDT: A lightweight blockchain-based data trading scheme in internet of vehicles using proof-of-reputation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-sense: Exploiting cooperative dark pixels in radio
sensing for non-stationary target. <em>TMC</em>, <em>24</em>(4),
2783–2799. (<a href="https://doi.org/10.1109/TMC.2024.3498048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio sensing has emerged as a promising solution for monitoring vital signs in a contactless manner. However, most of the existing designs focus on stationary target and struggle with body motion interference. While some efforts have been made to address this issue, the lack of a physical explanation for the motion elimination principle makes them work as a blind signal separation way and thus leaves the body motion elimination problem still as an open challenge. In this paper, we reveal for the first time the existence of “dark pixels”–specific points on the same rigid body parts that share the same body movement but exhibit varying physiological motions, with these variations still preserving the physiological rhythm. By exploiting the inherent relationship between the dark pixels, we propose a cooperative sensing framework, Co-Sense, that can achieve robust radio sensing for non-stationary targets in an explainable way. Through extensive experiments, Co-Sense demonstrates its superiority over existing methods, achieving effective motion cancellation and breath sensing with a median absolute respiratory rate (RR) error of 0.36 respiration per minute (RPM) and breath wave correlation of 0.61 under non-stationary scenarios. The results indicate the great potential of Co-Sense in enhancing the accuracy of vital sign sensing with radio signals, especially in real-world environments where targets are rarely stationary.},
  archive      = {J_TMC},
  author       = {Jinbo Chen and Dongheng Zhang and Ganlin Zhang and Haoyu Wang and Qibin Sun and Yan Chen},
  doi          = {10.1109/TMC.2024.3498048},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2783-2799},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Co-sense: Exploiting cooperative dark pixels in radio sensing for non-stationary target},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Similarity caching in dynamic cooperative edge networks: An
adversarial bandit approach. <em>TMC</em>, <em>24</em>(4), 2769–2782.
(<a href="https://doi.org/10.1109/TMC.2024.3500132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike traditional edge caching paradigms, similarity edge caching enables the retrieval of similar content from local caches to fulfill user requests, reducing reliance on remote data centers and improving system performance. Although several pioneering works have contributed to similarity edge caching, most focus on single-edge nodes and/or static environment settings, which are impractical for real-world applications. To address this gap, we investigate the similarity caching problem in dynamic cooperative edge networks, where a set of edge nodes cooperatively serve requests generated from arbitrary distributions with similar content over fluctuating transmission links. This presents a significant challenge, as it requires balancing content similarity with delivery latency over the transmission network and learning the environment in real-time to optimize caching policies. We frame this problem within an adversarial Multi-Armed Bandit framework to accommodate the continuously changing operational environment. To solve this, we propose an online learning-based approach named MABSCP, which dynamically updates caching policies based on real-time feedback to minimize the service cost of edge caching networks. To enhance implementation efficiency, we devise both an offline compact strategy construction method and an online Gibbs sampling method. Finally, trace-driven simulation results demonstrate that our proposed approach outperforms several existing methods in terms of system performance.},
  archive      = {J_TMC},
  author       = {Liang Wang and Yaru Wang and Zhiwen Yu and Fei Xiong and Lianbo Ma and Huan Zhou and Bin Guo},
  doi          = {10.1109/TMC.2024.3500132},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2769-2782},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Similarity caching in dynamic cooperative edge networks: An adversarial bandit approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-aware gradient compression for FL in
communication-constrained mobile computing. <em>TMC</em>,
<em>24</em>(4), 2755–2768. (<a
href="https://doi.org/10.1109/TMC.2024.3504284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) in mobile environments faces significant communication bottlenecks. Gradient compression has proven as an effective solution to this issue, offering substantial benefits in environments with limited bandwidth and metered data. Yet, it encounters severe performance drops in non-IID environments due to a one-size-fits-all compression approach, which does not account for the varying data volumes across workers. Assigning varying compression ratios to workers with distinct data distributions and volumes is therefore a promising solution. This work derives the convergence rate of distributed SGD with non-uniform compression, which reveals the intricate relationship between model convergence and the compression ratios applied to individual workers. Accordingly, we frame the relative compression ratio assignment as an $n$-variable chi-squared nonlinear optimization problem, constrained by a limited communication budget. We propose DAGC-R, which assigns conservative compression to workers handling larger data volumes. Recognizing the computational limitations of mobile devices, we propose the DAGC-A, which is computationally less demanding and enhances the robustness of compression in non-IID scenarios. Our experiments confirm that the DAGC-R and DAGC-A can speed up the training speed by up to 25.43% and 16.65% compared to the uniform compression respectively, when dealing with highly imbalanced data volume distribution and restricted communication.},
  archive      = {J_TMC},
  author       = {Rongwei Lu and Yutong Jiang and Yinan Mao and Chen Tang and Bin Chen and Laizhong Cui and Zhi Wang},
  doi          = {10.1109/TMC.2024.3504284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2755-2768},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Data-aware gradient compression for FL in communication-constrained mobile computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Device selection and resource allocation with
semi-supervised method for federated edge learning. <em>TMC</em>,
<em>24</em>(4), 2740–2754. (<a
href="https://doi.org/10.1109/TMC.2024.3504271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of distributed learning and workflow orchestration, Federated Edge Learning has emerged as a solution, enabling multiple edge devices to collaboratively train a large model without the need for sharing raw data. Beyond considering bandwidth and computational resource limitations in the Internet of Things (IoT) environment, it is crucial to address the issue of IoT devices often collecting data that lacks timely annotations, which can lead to latency and label deficiency issues. In most Federated Edge Learning mechanisms, clients’ weights are selected for offloading to the server. In this paper, we propose a solution for dynamic edge selection and wireless network allocation under semi-supervised and privacy protection settings, termed Semi-supervised Scheduling and Allocation Optimization for Federated Edge Learning (SSAFL). SSAFL is designed to adapt to various scenarios, including channel state variations, device heterogeneity, resource incentives, deadline control, label deficiencies, and Non-IID data distributions. This adaptability is achieved through the utilization of an Incentive Optimization framework that encompasses bandwidth allocation and device scheduling policies. Within SSAFL, we introduce the concept of a weighted bipartite graph network to tackle the Incentive Optimization problem and achieve a balance in large-scale optimization of device selection. Additionally, to address the label deficiency issue, we devise a Dynamic Timer for deadline control for each client. Comprehensive and confidential results demonstrate that our proposed approach significantly outperforms other Federated Edge Learning baselines.},
  archive      = {J_TMC},
  author       = {Ruihan Hu and Haochen Yuan and Daimin Tan and Zhongjie Wang},
  doi          = {10.1109/TMC.2024.3504271},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2740-2754},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Device selection and resource allocation with semi-supervised method for federated edge learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive task assignment in spatial crowdsourcing: A
human-in-the-loop approach. <em>TMC</em>, <em>24</em>(4), 2726–2739. (<a
href="https://doi.org/10.1109/TMC.2024.3501734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, adaptive task assignment has been explored in spatial crowdsourcing. The challenge lies in how to adaptively partition the task stream to achieve the best utility for task assignment. A number of existing works have attempted to solve this challenge and achieve better performance by utilizing learning-based methods. Specifically, they mainly employ reinforcement learning to divide the task stream into a series of suitable batches and then perform task assignment in a batch fashion. Drawing inspiration from the effectiveness of human-machine collaborative decision-making, we aim to investigate human-in-the-loop methods to further enhance the performance of adaptive task assignment. In this paper, we propose a novel framework called Human-in-the-Loop Adaptive Partition (HLAP), which consists of two primary modules: Reinforcement Learning Partition Decision (RL-PD) and Human Supervision and Guidance (HSG). In the RL-PD module, we develop an RL agent, referred to as the decision-maker, by integrating the dual attention network into the Deep Q-Network (DQN) algorithm to capture cross-dimensional contextual information and long-range dependencies for a better understanding of the environment. In the HSG module, we design a human-in-the-loop mechanism to optimize the performance of the decision-maker, focusing on addressing two key issues: when and how humans interact with the decision-maker. Furthermore, to alleviate the heavy workload on humans, we construct a supervisor based on RL to oversee the decision-maker&#39;s partition process and adaptively determine when human intervention is necessary. We conduct extensive experiments on two real-world datasets, and the results demonstrate the efficiency and effectiveness of the HLAP framework.},
  archive      = {J_TMC},
  author       = {Qingshun Wu and Yafei Li and Jinxing Yan and Mei Zhang and Jianliang Xu and Mingliang Xu},
  doi          = {10.1109/TMC.2024.3501734},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2726-2739},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive task assignment in spatial crowdsourcing: A human-in-the-loop approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-stream adaptive bitrate streaming for short-video
services. <em>TMC</em>, <em>24</em>(4), 2708–2725. (<a
href="https://doi.org/10.1109/TMC.2024.3497954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-video services have seen explosive growth in recent years. Streaming over mobile networks is inherently challenging due to the latter&#39;s bandwidth fluctuations, motivating researchers to develop many sophisticated adaptive bitrate (ABR) algorithms to compensate. While ABR, together with prefetching, has been proposed for playlist streaming, its application to non-playlist streaming has received little attention. This work fills this gap by first exploring the efficacy of directly applying ABR to non-playlist streaming. Observing their limitations motivates the development of a new class of inter-stream bitrate adaptation (ISA) algorithms. Unlike ABR, ISA adapts bitrate on a per-video basis, which is not only simpler to implement and deploy but can even outperform ABR algorithms by up to 66.71% across a wide range of networks. Moreover, ISA and ABR are complementary such that they can be combined into Integrated Bitrate Adaptation (IBA) algorithms to raise performance gains further by up to 77.03%. In addition, this work develops a novel adaptive rebuffering duration (ARD) algorithm specifically designed for frame-based playback common in short-video services to further improve their performance under challenging network conditions. Together, ISA and ARD offer a new set of tools with progressive complexity-performance tradeoffs for enhancing the performance of short-video services.},
  archive      = {J_TMC},
  author       = {Yuming Zhang and Shengtong Zhu and Yan Liu and Lingfeng Guo and Ji Li and Jack Y. B. Lee},
  doi          = {10.1109/TMC.2024.3497954},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2708-2725},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Inter-stream adaptive bitrate streaming for short-video services},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeTA: Neuron-grained scaling of foundation models in
edge-side retraining. <em>TMC</em>, <em>24</em>(4), 2690–2707. (<a
href="https://doi.org/10.1109/TMC.2024.3504859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models (FMs) such as large language models are becoming the backbone technology for artificial intelligence systems. It is particularly challenging to deploy multiple FMs on edge devices, which not only have limited computational resources, but also encounter unseen input data from evolving domains or learning tasks. When new data arrives, existing prior art of FM mainly focuses on retraining compressed models of predetermined network architectures, limiting the feasibility of edge devices to efficiently achieve high accuracy for FMs. In this paper, we propose EdgeTA, a neuron-grained FM scaling system to maximize the overall accuracy of FMs promptly in response to their data dynamics. EdgeTA&#39;s key design features in scaling are (i) proxy mechanism, which adaptively transforms a FM into a compact architecture retaining the most important neurons to the input data, and (ii) neuron-grained scheduler, which jointly optimizes model sizes and resource allocation for all FMs on edge devices. Under tight retraining window and limited device resources, the design of EdgeTA can achieve most of the original FM&#39;s accuracy with much smaller retraining costs. We implement EdgeTA on FMs of natural language processing, computer vision and multimodal applications. Comparison results against state-of-the-art techniques show that our approach improves accuracy by 21.88% and reduces memory footprint and energy consumptions by 27.14% and 65.65%, while further achieving 15.96% overall accuracy improvement via neuron-grained scheduling.},
  archive      = {J_TMC},
  author       = {Qinglong Zhang and Rui Han and Chi Harold Liu and Guoren Wang and Song Guo and Lydia Y. Chen},
  doi          = {10.1109/TMC.2024.3504859},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2690-2707},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EdgeTA: Neuron-grained scaling of foundation models in edge-side retraining},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed computation offloading for energy provision
minimization in WP-MEC networks with multiple HAPs. <em>TMC</em>,
<em>24</em>(4), 2673–2689. (<a
href="https://doi.org/10.1109/TMC.2024.3502004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a wireless powered mobile edge computing (WP-MEC) network with multiple hybrid access points (HAPs) in a dynamic environment, where wireless devices (WDs) harvest energy from radio frequency (RF) signals of HAPs, and then compute their computation data locally (i.e., local computing mode) or offload it to the chosen HAPs (i.e., edge computing mode). In order to pursue a green computing design, we formulate an optimization problem that minimizes the long-term energy provision of the WP-MEC network subject to the energy, computing delay and computation data demand constraints. The transmit power of HAPs, the duration of the wireless power transfer (WPT) phase, the offloading decisions of WDs, the time allocation for offloading and the CPU frequency for local computing are jointly optimized adapting to the time-varying generated computation data and wireless channels of WDs. To efficiently address the formulated non-convex mixed integer programming (MIP) problem in a distributed manner, we propose a Two-stage Multi-Agent deep reinforcement learning-based Distributed computation Offloading (TMADO) framework, which consists of a high-level agent and multiple low-level agents. The high-level agent residing in all HAPs optimizes the transmit power of HAPs and the duration of the WPT phase, while each low-level agent residing in each WD optimizes its offloading decision, time allocation for offloading and CPU frequency for local computing. Simulation results show the superiority of the proposed TMADO framework in terms of the energy provision minimization.},
  archive      = {J_TMC},
  author       = {Xiaoying Liu and Anping Chen and Kechen Zheng and Kaikai Chi and Bin Yang and Tarik Taleb},
  doi          = {10.1109/TMC.2024.3502004},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2673-2689},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed computation offloading for energy provision minimization in WP-MEC networks with multiple HAPs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation of a light-weight channel vector
classifier based on support vector machine for real-time 5G beam index
detection. <em>TMC</em>, <em>24</em>(4), 2660–2672. (<a
href="https://doi.org/10.1109/TMC.2024.3494757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) is recently considered a key technology for bringing outstanding performance to wireless communications. Conventional research has highlighted the potential of Support Vector Machines (SVMs), which train their model based on optimization theory, to enhance the performance of wireless communications. However, there are practical issues that makes SVM difficult to apply to a wireless communication system. SVM generally entails a heavy training process with high computational complexity, and the model requires a significant amount of time for training. Also, the entire dataset needs to be trained at once, requiring a substantial amount of memory for data storage. To enable SVM in wireless communications, we propose Real-Time Channel Vector Classifier (RTCVC), which employs a light-weight SVM model capable of training and processing incoming data in real-time. A novel input data pre-processing technique is implemented to reduce the computational overhead associated with calculating non-linear functions. The rearranged formulation of the original problem also allows each SVM sub-model to be trained distributively over time based on incremental parameters. For performance evaluation, we implement the RTCVC inter-operating with 5G beam index detection, whose detection probability has been theoretically proven to be significantly enhanced by SVM. The software modules of the RTCVC are based on LibSVM, a well-known open-source library for implementing SVM sub-models. The experimental results confirm that RTCVC significantly reduces training time while maintaining suitable performance for 5G beam index detection.},
  archive      = {J_TMC},
  author       = {Juyeop Kim and Soomin Kwon and Jiyoon Han and Taegyeom Lee and Ohyun Jo},
  doi          = {10.1109/TMC.2024.3494757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2660-2672},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Design and implementation of a light-weight channel vector classifier based on support vector machine for real-time 5G beam index detection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency-energy efficient task offloading in the satellite
network-assisted edge computing via deep reinforcement learning.
<em>TMC</em>, <em>24</em>(4), 2644–2659. (<a
href="https://doi.org/10.1109/TMC.2024.3502643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand for global computing coverage continues to surge, satellite edge computing emerges as a pivotal technology for the next generation of networks. Unlike ground-based edge computing, Low Earth Orbit (LEO) satellites face distinctive challenges, including high-speed mobility and resource limitations, etc. Therefore, effectively utilizing LEO satellites for global coverage services is crucial but challenging due to their dynamic coverage areas and diverse task requirements. To address these challenges, we introduce a novel dual-cloud edge collaborative task offloading architecture in the satellite network-assisted edge computing environment, namely, Satellite-Ground Task Offloading (SGTO). The architecture employs a Geostationary Earth Orbit (GEO) satellite and a ground cloud computing center as satellite cloud and ground cloud, respectively, and LEO satellites as edge nodes. We formally define the task offloading problem in the SGTO with the aim of minimizing the average latency and average energy consumption. We then propose an adaptive approach named SGTO-A from the perspective of satellites to adaptively solve the problem leveraging deep reinforcement learning. Specifically, we transform the task offloading problem into a Markov decision process and adopt the generalized proximal policy optimization (GePPO) algorithm to solve the problem. Finally, experimental results demonstrate that SGTO architecture and SGTO-A outperform the representative approaches in terms of average latency, average energy consumption and running time.},
  archive      = {J_TMC},
  author       = {Jian Zhou and Juewen Liang and Lu Zhao and Shaohua Wan and Hui Cai and Fu Xiao},
  doi          = {10.1109/TMC.2024.3502643},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2644-2659},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Latency-energy efficient task offloading in the satellite network-assisted edge computing via deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint order dispatching and vehicle repositioning for
dynamic ridesharing. <em>TMC</em>, <em>24</em>(4), 2628–2643. (<a
href="https://doi.org/10.1109/TMC.2024.3493974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic ridesharing has gained significant attention in recent years. However, existing ridesharing studies often focus on optimizing order dispatching and vehicle repositioning separately, leading to short-sighted decisions and underutilization of the ridesharing potential. In this paper, we propose a novel joint optimization framework called $\mathtt {JODR}$. By coordinating order dispatching and vehicle repositioning, $\mathtt {JODR}$ enhances ridesharing efficiency while ensuring high-quality service. The core idea of $\mathtt {JODR}$ is to dispatch ride orders with high demand in specific mobility directions to vehicles with sufficient available capacity, effectively balancing future supply and demand in those directions. To achieve this, we introduce a novel mobility value function that can predict the long-term mobility value of matching an order with its travel direction. By considering orders’ directional mobility values, service quality assessments, and available vehicle capacities, $\mathtt {JODR}$ formulates the order dispatching as a minimum-cost maximum-flow problem to derive the optimal order-vehicle assignments. Furthermore, the value function helps the intelligent repositioning of idle vehicles. Extensive experiments conducted on a large real-world dataset demonstrate the superiority of $\mathtt {JODR}$ over state-of-the-art methods across various performance metrics. These experimental results validate the effectiveness of $\mathtt {JODR}$ in improving the ridesharing efficiency and experience.},
  archive      = {J_TMC},
  author       = {Zhidan Liu and Guofeng Ouyang and Bolin Zhang and Bo Du and Chao Chen and Kaishun Wu},
  doi          = {10.1109/TMC.2024.3493974},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2628-2643},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint order dispatching and vehicle repositioning for dynamic ridesharing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the potential of self-supervised RF learning with
group shuffle. <em>TMC</em>, <em>24</em>(4), 2612–2627. (<a
href="https://doi.org/10.1109/TMC.2024.3497972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) is a powerful approach that learns general semantic representations from large-scale unlabeled data to make downstream tasks solve easier, offering significant potential in enhancing downstream performance and alleviating the appetite for large-scale annotated data. However, existing SSL techniques, predominantly designed for natural images, may be prone to shortcuts when applied to RF signals. This study presents surprising empirical findings showing that SSL can indeed learn meaningful RF representations by employing simple group shuffle (GS) and asymmetry augmentation techniques. The GS augmentation is inspired by blind calibration tasks in Time-Interleaved Analog-to-Digital Converters (TIADC). By treating the original RF signal as a composite output from sub-ADCs, GS augmentation enriches RF signals while preserving their global semantics. We also provide a theoretical validation of the GS augmentation’s singular value consistency. Notably, we observe that the shortcut is essentially a domain gap between the pre-trained and the downstream task models. This issue can be mitigated by an asymmetry augmentation technique, which maximizes the similarity between an original RF signal and its augmented version, rather than between two augmentations of the same RF signal. By integrating group shuffle and asymmetry augmentation (GSAA) into an existing contrastive learning framework, we develop an effective contrastive learning approach for RF signals. Our evaluations, spanning seven downstream RF sensing tasks across two general RF devices (WiFi and radar), strongly demonstrate that GSAA plays a significant role in advancing SSL-based solutions in RF sensing.},
  archive      = {J_TMC},
  author       = {Ruiyuan Song and Zhi Lu and Dongheng Zhang and Liang Fang and Zhi Wu and Yang Hu and Qibin Sun and Yan Chen},
  doi          = {10.1109/TMC.2024.3497972},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2612-2627},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unleashing the potential of self-supervised RF learning with group shuffle},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative error detection and correction scheme for
safety message in V2X. <em>TMC</em>, <em>24</em>(4), 2594–2611. (<a
href="https://doi.org/10.1109/TMC.2024.3494713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle-to-Everything (V2X) technology plays a pivotal role in enabling real-time traffic coordination and safety, warning, and decision support. Within V2X, the Basic Safety Message (BSM) serves as the core to transmit critical vehicle status, location, and intention information to provide a foundation for ensuring reliable traffic safety and coordination mechanisms. Data accuracy stands as a key to the effectiveness and reliability of the V2X system, in which the transmission of error data can potentially result in severe traffic accidents. During vehicular operation, sensors may generate error data owing to looseness or external conditions. However, immediate sensor replacement is often impractical or infeasible. Therefore, this paper introduces a collaborative scheme involving vehicles, Road Side Units (RSUs), and Data Center (DC) to jointly enhance the accuracy of vehicle-transmitted BSMs. Our scheme involves analyzing statistical features of vehicle driving information to detect error BSMs. Subsequently, these detected errors are corrected by leveraging historical data from the vehicle and its relative relationship with surrounding vehicles. In addition, we propose a time optimization method to reduce the average processing time of each data by RSUs. The extensive experimental results demonstrate that the proposed scheme can accurately detect error BSMs and effectively correct error BSMs. The entire scheme also meets the requisite computational latency requirements.},
  archive      = {J_TMC},
  author       = {Hui Qian and Hongmei Chai and Ammar Hawbani and Yuanguo Bi and Na Lin and Liang Zhao},
  doi          = {10.1109/TMC.2024.3494713},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2594-2611},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A collaborative error detection and correction scheme for safety message in V2X},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum reinforcement learning approach for joint resource
allocation and task offloading in mobile edge computing. <em>TMC</em>,
<em>24</em>(4), 2580–2593. (<a
href="https://doi.org/10.1109/TMC.2024.3496918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) has revolutionized the way computational tasks are offloaded and latency is reduced by leveraging edge servers close to end devices. Efficient resource allocation and task offloading are crucial for enhancing system performance in MEC environments. Traditional reinforcement learning (RL) approaches have shown promise in optimizing resource allocation and task offloading problems. However, they often face challenges such as high computational complexity and the need for extensive training data. Quantum reinforcement learning (QRL) emerges as a promising solution to overcome these limitations by leveraging quantum computing principles to enhance efficiency and scalability. In this paper, we propose a hybrid quantum-classical non-sequential model for joint resource allocation and task offloading in MEC systems. Our model combines the advantages of RL in handling environmental dynamics and quantum computing in reducing adjustable parameters and accelerating the training process. Extensive experiments demonstrate that our proposed algorithm can achieve higher training and inference performance under various parameter settings compared to traditional RL models and previous QRL models.},
  archive      = {J_TMC},
  author       = {Xinliang Wei and Xitong Gao and Kejiang Ye and Cheng-Zhong Xu and Yu Wang},
  doi          = {10.1109/TMC.2024.3496918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2580-2593},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A quantum reinforcement learning approach for joint resource allocation and task offloading in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay- and resource-aware satellite UPF service
optimization. <em>TMC</em>, <em>24</em>(4), 2564–2579. (<a
href="https://doi.org/10.1109/TMC.2024.3494043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Executing 5G core network functions on satellites has become crucial to enhance satellite network management and service capabilities. The User Plane Function (UPF) is responsible for efficient data traffic forwarding and is envisioned as a key and pioneering core network function that will be deployed on satellites. However, managing and providing services with satellite UPFs face dual challenges. Limited satellite resources constrain the user scale that a satellite UPF can service, resulting in an unguaranteed service delay. Moreover, the extremely rapid mobility of satellites renders it difficult for satellite UPFs to provide seamless services. To address the above challenges, this paper presents the first-of-its-kind service optimization scheme for satellite UPFs in terms of switch control, state migration, and traffic routing. To provide guaranteed service delay, we provide a theoretical analysis based on the M/G/1 queue model, demonstrating the service delay-resource consumption trade-off. A satellite UPF switch control scheme is integrated into the service optimization process, which can decrease satellite UPF service delay while saving satellite resources by adjusting the switch control parameters. To provide seamless services, we propose a satellite UPF-oriented state-aware service migration and traffic routing (UPF service optimization) algorithm. A policy network-based reinforcement learning approach is employed to dynamically perceive the satellite network’s state as well as the satellite UPF switch state. Building upon the optimization of service delay through satellite UPF switch control, the processes of state-aware state migration and traffic routing are further employed to reduce delay, ensuring seamless service effectively. Experiments reveal that the proposed algorithm outperforms other benchmark algorithms under different metrics. The service delay is reduced by an average of 23.2% compared with other algorithms.},
  archive      = {J_TMC},
  author       = {Chao Wang and Xiao Ma and Ruolin Xing and Sisi Li and Ao Zhou and Shangguang Wang},
  doi          = {10.1109/TMC.2024.3494043},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2564-2579},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Delay- and resource-aware satellite UPF service optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards communication-efficient cooperative perception via
planning-oriented feature sharing. <em>TMC</em>, <em>24</em>(4),
2551–2563. (<a href="https://doi.org/10.1109/TMC.2024.3496856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving systems are fundamentally composed of sequential modular tasks, i.e., perception, prediction, and planning. For connected autonomous vehicles (CAVs), cooperative perception offers a promising solution to surpass their perception limitations, such as occlusion, by sharing sensing data with each other through wireless communication. Existing works typically prioritize sharing data from potential object-containing areas to maximize object detection accuracy under constrained communication resources. However, such detection-oriented approaches ignore a crucial fact that more accurate detection does not equal safer planning. Sharing large amounts of sensing data for detection accuracy can lead to communication resource wastage and performance degradation of subsequent driving tasks. To address this, we introduce Plan2comm, a communication-efficient cooperative perception framework via planning-oriented feature sharing, which shares only sensing data around planned trajectories to enable safer planning rather than mere detection accuracy. Specifically, a planning-oriented communication mechanism is designed to select and transmit the most valuable features from the perspective of the planning task. Moreover, an uncertainty-aware spatial-temporal feature fusion strategy is proposed to enhance high-quality information aggregation. Comprehensive experiments demonstrate that Plan2comm outperforms all other cooperative perception methods on motion prediction performance, and is more communication-efficient.},
  archive      = {J_TMC},
  author       = {Qi Xie and Xiaobo Zhou and Tianyu Hong and Wenkai Hu and Wenyu Qu and Tie Qiu},
  doi          = {10.1109/TMC.2024.3496856},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2551-2563},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards communication-efficient cooperative perception via planning-oriented feature sharing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of task offloading and resource
allocation of fog network by considering matching externalities and
dynamics. <em>TMC</em>, <em>24</em>(4), 2534–2550. (<a
href="https://doi.org/10.1109/TMC.2024.3494793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to jointly optimize task offloading and resource allocation to minimize the task failure rate and task payments remains an unresolved challenge in fog networks. Focusing on this problem, this research formulates a novel task offloading and resource allocation model with two offloading modes and on-demand virtual resource units (VRUs). This model is decomposed into two sub-problems to solve: a joint task offloading and resource allocation optimization problem and a matching problem with externalities and dynamics. First, for a given terminal node (TN) and fog node (FN), this research theoretically derives the optimal offloading ratio and resource allocation strategy to minimize the payment of TNs for two offloading modes, i.e., immediate and queued offloading. Second, in the multi-TNs and multi-FNs scenario, the problem of making the task offloading decision is transformed into a many-to-one matching game by considering externalities and dynamics. Finally, a Deferred acceptance-based Loss ratio and Payment Minimized task Offloading and resource Allocation optimization (DLPMOA) algorithm is proposed to derive a stable and Pareto-optimal match. The simulation results show that the proposed DLPMOA has better performance in terms of task failure rate, task average payment, fog computing resource utilization, and fairness than the state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Jiahui Xu and Yingbiao Yao and Xin Xu and Wei Feng and Pei Li},
  doi          = {10.1109/TMC.2024.3494793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2534-2550},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of task offloading and resource allocation of fog network by considering matching externalities and dynamics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TaPIN: Reinforcing PIN authentication on smartphones with
tap biometrics. <em>TMC</em>, <em>24</em>(4), 2519–2533. (<a
href="https://doi.org/10.1109/TMC.2024.3502902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PIN authentication is the first line of defense for protecting private data on many smartphone applications, such as lock screens, messengers, and banking apps. However, existing PIN authentication systems have several constraints regarding security, usability, and robustness. To go beyond their limitations, this paper presents TaPIN, a reliable system that authenticates smartphone users with the collaborative use of PINs and tap biometrics. A user is first instructed to enter her PIN by tapping a smartphone screen for authentication. During the PIN entry, the user&#39;s fingertip collides with the screen, producing user-specific vibration and sound signals. TaPIN then senses the tap-induced signals and the collision properties, e.g., pressures and sizes, using the smartphone&#39;s built-in sensors and leverages them as biometric features. That is, it authenticates the user by verifying not only the entered PIN but also the collected features. Our experiments with 20 real-world users demonstrate that this two-factor authentication system is easy to use, more secure than existing methods, and deployable without dedicated hardware. For example, it accurately authenticates users with an average EER of 1.9% in stationary environments and maintains a reasonable level of security regardless of devices, tap styles, and noise.},
  archive      = {J_TMC},
  author       = {Junhyub Lee and Insu Kim and Sangeun Oh and Hyosu Kim},
  doi          = {10.1109/TMC.2024.3502902},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2519-2533},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TaPIN: Reinforcing PIN authentication on smartphones with tap biometrics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent end-to-end deterministic scheduling across
converged networks. <em>TMC</em>, <em>24</em>(4), 2504–2518. (<a
href="https://doi.org/10.1109/TMC.2025.3530486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deterministic network services play a vital role for supporting emerging real-time applications with bounded low latency, jitter, and high reliability. The deterministic guarantee is penetrated into various types of networks, such as 5G, WiFi, satellite, and edge computing networks. From the user’s perspective, the real-time applications require end-to-end deterministic guarantee across the converged network. In this paper, we investigate the end-to-end deterministic guarantee problem across the whole converged network, aiming to provide a scalable method for different kinds of converged networks to meet the bounded end-to-end latency, jitter, and high reliability demands of each flow, while improving the network scheduling QoS. Particularly, we set up the global end-to-end control plane to abstract the deterministic-related resources from converged network, and model the deterministic flow transmission by using the abstracted resources. With the resource abstraction, our model can work well for different underlying technologies. Given large amounts of abstracted resources in our model, it is difficult for traditional algorithms to fully utilize the resources. Thus, we propose a deep reinforcement learning based end-to-end deterministic-related resource scheduling (E2eDRS) algorithm to schedule the network resources from end to end. By setting the action groups, the E2eDRS can support varying network dimensions both in horizontal and vertical end-to-end deterministic-related network architectures. Experimental results show that E2eDRS can averagely increase 1.33x and 6.01x schedulable flow number for horizontal scheduling compared with MultiDRS and MultiNaive algorithms, respectively. The E2eDRS can also optimize 2.65x and 3.87x server load balance than MultiDRS and MultiNaive algorithms, respectively. For vertical scheduling, the E2eDRS can still perform better on schedulable flow number and server load balance.},
  archive      = {J_TMC},
  author       = {Zongrong Cheng and Weiting Zhang and Dong Yang and Chuan Huang and Hongke Zhang and Xuemin Sherman Shen},
  doi          = {10.1109/TMC.2025.3530486},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2504-2518},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Intelligent end-to-end deterministic scheduling across converged networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaEvo: Edge-assisted continuous and timely DNN model
evolution for mobile devices. <em>TMC</em>, <em>24</em>(4), 2485–2503.
(<a href="https://doi.org/10.1109/TMC.2023.3316388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile video applications today have attracted significant attention. Deep learning model (e.g., deep neural network, DNN) compression is widely used to enable on-device inference for facilitating robust and private mobile video applications. The compressed DNN, however, is vulnerable to the agnostic data drift of the live video captured from the dynamically changing mobile scenarios. To combat the data drift, mobile ends rely on edge servers to continuously evolve and re-compress the DNN with freshly collected data. We design a framework, ${\sf AdaEvo}$, that efficiently supports the resource-limited edge server handling mobile DNN evolution tasks from multiple mobile ends. The key goal of ${\sf AdaEvo}$ is to maximize the average quality of experience (QoE), i.e., the proportion of high-quality DNN service time to the entire life cycle, for all mobile ends. Specifically, it estimates the DNN accuracy drops at the mobile end without labels and performs a dedicated video frame sampling strategy to control the size of retraining data. In addition, it balances the limited computing and memory resources on the edge server and the competition between asynchronous tasks initiated by different mobile users. With an extensive evaluation of real-world videos from mobile scenarios and across four diverse mobile tasks, experimental results show that ${\sf AdaEvo}$ enables up to 34% accuracy improvement and 32% average QoE improvement.},
  archive      = {J_TMC},
  author       = {Lehao Wang and Zhiwen Yu and Haoyi Yu and Sicong Liu and Yaxiong Xie and Bin Guo and Yunxin Liu},
  doi          = {10.1109/TMC.2023.3316388},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2485-2503},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaEvo: Edge-assisted continuous and timely DNN model evolution for mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
