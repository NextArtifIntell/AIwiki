<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMODS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simods---14">SIMODS - 14</h2>
<ul>
<li><details>
<summary>
(2025). CA-PCA: Manifold dimension estimation, adapted for
curvature. <em>SIMODS</em>, <em>7</em>(1), 355–383. (<a
href="https://doi.org/10.1137/23M1575135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The success of algorithms in the analysis of high-dimensional data is often attributed to the manifold hypothesis, which supposes that this data lie on or near a manifold of much lower dimension. It is often useful to determine or estimate the dimension of this manifold before performing dimension reduction, for instance. Existing methods for dimension estimation are calibrated using a flat unit ball. In this paper, we develop CA-PCA, a version of local PCA based instead on a calibration of a quadratic embedding, acknowledging the curvature of the underlying manifold. Numerous careful experiments show that this adaptation improves the estimator in a wide range of settings.},
  archive      = {J_SIMODS},
  author       = {Anna C. Gilbert and Kevin O’Neill},
  doi          = {10.1137/23M1575135},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {355-383},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {CA-PCA: Manifold dimension estimation, adapted for curvature},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random fourier signature features. <em>SIMODS</em>,
<em>7</em>(1), 329–354. (<a
href="https://doi.org/10.1137/23M1620478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Tensor algebras give rise to one of the most powerful measures of similarity for sequences of arbitrary length called the signature kernel accompanied with attractive theoretical guarantees from stochastic analysis. Previous algorithms to compute the signature kernel scale quadratically in terms of the length and number of the sequences. To mitigate this severe computational bottleneck, we develop a random Fourier feature-based acceleration of the signature kernel acting on the inherently non-Euclidean domain of sequences. We show uniform approximation guarantees for the proposed unbiased estimator of the signature kernel, while keeping its computation linear in the sequence length and number. In addition, combined with recent advances on tensor projections, we derive two even more scalable time series features with favorable concentration properties and computational complexity both in time and memory. Our empirical results show that the reduction in computational cost comes at a negligible price in terms of accuracy on moderate size datasets, and it enables one to scale to large datasets up to a million time series. We release the code publicly available at https://github.com/tgcsaba/ksig.},
  archive      = {J_SIMODS},
  author       = {Csaba Tóth and Harald Oberhauser and Zoltán Szabó},
  doi          = {10.1137/23M1620478},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {329-354},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Random fourier signature features},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised gromov–wasserstein optimal transport with
metric-preserving constraints. <em>SIMODS</em>, <em>7</em>(1), 301–328.
(<a href="https://doi.org/10.1137/24M1630499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce the supervised Gromov–Wasserstein (sGW) optimal transport, an extension of Gromov–Wasserstein that incorporates potential infinity entries in the cost tensor. These infinity entries enable sGW to enforce application-induced constraints on preserving pairwise distance to a certain extent. A numerical solver is proposed for the sGW problem and the effectiveness is demonstrated in various numerical experiments. The high-order constraints in sGW are transferred to constraints on the coupling matrix by solving a minimal vertex cover problem. The transformed problem is solved by the mirror-C descent iteration coupled with the supervised optimal transport solver. In the numerical experiments, we first validate the proposed framework by applying it to matching synthetic datasets and investigating the impact of the model parameters. Additionally, we apply sGW to aligning single-cell RNA sequencing data where the datasets are partially overlapping and only intra-dataset metrics are used. Through comparisons with other Gromov–Wasserstein variants, we demonstrate that sGW offers an additional utility of controlling distance preservation, leading to automatic estimation of overlapping portions of datasets, which brings improved stability and flexibility in data-driven applications. The codes for sGW and for reproducing the results are available on Github [https://github.com/zcang/supervisedGW].},
  archive      = {J_SIMODS},
  author       = {Zixuan Cang and Yaqi Wu and Yanxiang Zhao},
  doi          = {10.1137/24M1630499},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {301-328},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Supervised Gromov–Wasserstein optimal transport with metric-preserving constraints},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). First-order conditions for optimization in the wasserstein
space. <em>SIMODS</em>, <em>7</em>(1), 274–300. (<a
href="https://doi.org/10.1137/23M156687X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study first-order optimality conditions for constrained optimization in the Wasserstein space, whereby one seeks to minimize a real-valued function over the space of probability measures endowed with the Wasserstein distance. Our analysis combines recent insights on the geometry and the differential structure of the Wasserstein space with more classical calculus of variations. We show that simple rationales such as “setting the derivative to zero” and “gradients are aligned at optimality” carry over to the Wasserstein space. We deploy our tools to study and solve optimization problems in the setting of distributionally robust optimization and statistical inference. The generality of our methodology allows us to naturally deal with functionals, such as mean-variance, Kullback–Leibler divergence, and Wasserstein distance, which are traditionally difficult to study in a unified framework.},
  archive      = {J_SIMODS},
  author       = {Nicolas Lanzetti and Saverio Bolognani and Florian Dörfler},
  doi          = {10.1137/23M156687X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {274-300},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {First-order conditions for optimization in the wasserstein space},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel-based regularized learning with random projections:
Beyond least squares. <em>SIMODS</em>, <em>7</em>(1), 253–273. (<a
href="https://doi.org/10.1137/23M1602954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider kernel-based regularized learning with the help of the random projection technique to ease its computational burden. The random projection approaches we study here include the randomized sketching method and the Nyström approximation method. Current works under the least squares loss demonstrated an optimal learning rate under an appropriate source condition and capacity condition. However, beyond this simplest loss, it seems challenging to appropriately incorporate both conditions due to the unavailability of a closed-form solution. In this work, we consider a sufficiently general class of convex losses which include logistic loss, quantile loss, and hinge loss, for example, and establish the same optimal learning rate as in the least squares case under mild regularity assumptions. Our result also covers the unattainable case where the true function is not in the reproducing kernel Hilbert space. To incorporate the source condition, Young’s inequality for operators is used, while to characterize the capacity, Rademacher complexity is adopted. We illustrate the performances of random projection with some numerical examples.},
  archive      = {J_SIMODS},
  author       = {Jiamin Liu and Junzhuo Gao and Heng Lian},
  doi          = {10.1137/23M1602954},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {253-273},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Kernel-based regularized learning with random projections: Beyond least squares},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating gaussian mixtures using sparse polynomial moment
systems. <em>SIMODS</em>, <em>7</em>(1), 224–252. (<a
href="https://doi.org/10.1137/23M1610082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The method of moments is a classical statistical technique for density estimation that solves a system of moment equations to estimate the parameters of an unknown distribution. A fundamental question critical to understanding identifiability asks how many moment equations are needed to get finitely many solutions and how many solutions there are. We answer this question for classes of Gaussian mixture models using the tools of polyhedral geometry. In addition, we show that a generic Gaussian -mixture model is identifiable from its first moments. Using these results, we present a homotopy algorithm that performs parameter recovery for high-dimensional Gaussian mixture models where the number of paths tracked scales linearly in the dimension.},
  archive      = {J_SIMODS},
  author       = {Julia Lindberg and Carlos Améndola and Jose Israel Rodriguez},
  doi          = {10.1137/23M1610082},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {224-252},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Estimating gaussian mixtures using sparse polynomial moment systems},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multifidelity covariance estimation via regression on the
manifold of symmetric positive definite matrices. <em>SIMODS</em>,
<em>7</em>(1), 189–223. (<a
href="https://doi.org/10.1137/23M159247X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties enabling practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that the MRMF estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.},
  archive      = {J_SIMODS},
  author       = {Aimee Maurais and Terrence Alsup and Benjamin Peherstorfer and Youssef M. Marzouk},
  doi          = {10.1137/23M159247X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {189-223},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Multifidelity covariance estimation via regression on the manifold of symmetric positive definite matrices},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric finite mixture models with possible shape
constraints: A cubic newton approach. <em>SIMODS</em>, <em>7</em>(1),
163–188. (<a href="https://doi.org/10.1137/21M1430972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We explore computational aspects of maximum likelihood estimation of the mixture proportions in a nonparametric finite mixture model—a convex optimization problem with old roots in statistics. Motivated by problems in shape constrained inference, we also consider structured variants of this problem with additional convex polyhedral constraints. We propose a new cubic regularized Newton method for this problem and present novel worst-case and local computational guarantees for our algorithm. We extend earlier work by Nesterov and Polyak to the case of a self-concordant objective with polyhedral constraints, such as the ones considered herein. We propose a Frank–Wolfe method to solve the cubic regularized Newton subproblem and derive efficient solutions for the linear optimization oracles that may be of independent interest. In the particular case of Gaussian mixtures without shape constraints, we derive bounds on how well the finite mixture problem approximates the infinite-dimensional Kiefer–Wolfowitz maximum likelihood estimator. Experiments on synthetic and real datasets suggest that our proposed algorithms exhibit improved runtimes and scalability features over prior benchmarks.},
  archive      = {J_SIMODS},
  author       = {Haoyue Wang and Shibal Ibrahim and Rahul Mazumder},
  doi          = {10.1137/21M1430972},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {163-188},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonparametric finite mixture models with possible shape constraints: A cubic newton approach},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized low-rank approximations beyond gaussian random
matrices. <em>SIMODS</em>, <em>7</em>(1), 136–162. (<a
href="https://doi.org/10.1137/23M1593255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper expands the analysis of randomized low-rank approximation beyond the Gaussian distribution to four classes of random matrices: (1) independent sub-Gaussian entries, (2) independent sub-Gaussian columns, (3) independent bounded columns, and (4) independent columns with bounded second moment. Using a novel interpretation of the low-rank approximation error involving sample covariance matrices, we provide insight into the requirements of a good random matrix for randomized low-rank approximations. Although our bounds involve unspecified absolute constants (a consequence of underlying nonasymptotic theory of random matrices), they allow for qualitative comparisons across distributions. The analysis offers some details on the minimal number of samples (the number of columns of the random matrix ) and the error in the resulting low-rank approximation. We illustrate our analysis in the context of the randomized subspace iteration method as a representative algorithm for low-rank approximation; however, all the results are broadly applicable to other low-rank approximation techniques. We conclude our discussion with numerical examples using both synthetic and real-world test matrices.},
  archive      = {J_SIMODS},
  author       = {Arvind K. Saibaba and Agnieszka Międlar},
  doi          = {10.1137/23M1593255},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {136-162},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Randomized low-rank approximations beyond gaussian random matrices},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Function-space optimality of neural architectures with
multivariate nonlinearities. <em>SIMODS</em>, <em>7</em>(1), 110–135.
(<a href="https://doi.org/10.1137/23M1620971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We investigate the function-space optimality (specifically, the Banach-space optimality) of a large class of shallow neural architectures with multivariate nonlinearities/activation functions. To that end, we construct a new family of Banach spaces defined via a regularization operator, the -plane transform, and a sparsity-promoting norm. We prove a representer theorem that states that the solution sets to learning problems posed over these Banach spaces are completely characterized by neural architectures with multivariate nonlinearities. These optimal architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, both of which have received recent interest in the neural network community. Our framework is compatible with a number of classical nonlinearities including the rectified linear unit activation function, the norm activation function, and the radial basis functions found in the theory of thin-plate/polyharmonic splines. We also show that the underlying spaces are special instances of reproducing kernel Banach spaces and variation spaces. Our results shed light on the regularity of functions learned by neural networks trained on data, particularly with multivariate nonlinearities, and provide new theoretical motivation for several architectural choices found in practice.},
  archive      = {J_SIMODS},
  author       = {Rahul Parhi and Michael Unser},
  doi          = {10.1137/23M1620971},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {110-135},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Function-space optimality of neural architectures with multivariate nonlinearities},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KL convergence guarantees for score diffusion models under
minimal data assumptions. <em>SIMODS</em>, <em>7</em>(1), 86–109. (<a
href="https://doi.org/10.1137/23M1613670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Diffusion models are a new class of generative models that revolve around the estimation of the score function associated with an SDE. Subsequent to its acquisition, the approximated score function is then harnessed to simulate the corresponding time-reversal process, ultimately enabling the generation of approximate data samples. Despite their evident practical significance these models carry, a notable challenge persists in the form of a lack of comprehensive quantitative results, especially in scenarios involving nonregular scores and estimators. In almost all reported bounds in Kullback–Leibler (KL) divergence, it is assumed that either the score function or its approximation is Lipschitz uniformly in time. However, this condition is very restrictive in practice or appears to be difficult to establish. To circumvent this issue, previous works mainly focused on establishing convergence bounds in KL for an early stopped version of the diffusion model and a smoothed version of the data distribution or assuming that the data distribution is supported on a compact manifold. These explorations have led to interesting bounds in either Wasserstein or Fortet–Mourier metrics. However, the question remains about the relevance of such an early stopping procedure or compactness conditions, in particular, if there exists a natural and mild condition ensuring explicit and sharp convergence bounds in KL. In this article, we tackle the aforementioned limitations by focusing on score diffusion models with fixed step size stemming from the Ornstein–Uhlenbeck semigroup and its kinetic counterpart. Our study provides a rigorous analysis, yielding simple, improved, and sharp convergence bounds in KL applicable to any data distribution with finite Fisher information with respect to the standard Gaussian distribution.},
  archive      = {J_SIMODS},
  author       = {Giovanni Conforti and Alain Durmus and Marta Gentiloni Silveri},
  doi          = {10.1137/23M1613670},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {86-109},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {KL convergence guarantees for score diffusion models under minimal data assumptions},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse evolution layers: Physics-informed regularizers for
image segmentation. <em>SIMODS</em>, <em>7</em>(1), 55–85. (<a
href="https://doi.org/10.1137/24M1633662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Traditional image processing methods employing PDEs offer a multitude of meaningful regularizers along with valuable theoretical foundations for a wide range of image-related tasks. This makes their integration into neural networks a promising avenue. In this paper, we introduce a novel regularization approach inspired by the reverse process of PDE-based evolution models. Specifically, we propose inverse evolution layers (IELs), which serve as bad property amplifiers to penalize neural networks of which outputs have undesired characteristics. Using IELs, one can achieve specific regularization objectives and endow neural network outputs with corresponding properties of the PDE models. Our experiments, focusing on semantic segmentation tasks using heat-diffusion IELs, demonstrate their effectiveness in mitigating noisy label effects. Additionally, we develop curve-motion IELs to enforce convex shape regularization in neural network–based segmentation models for preventing the generation of concave outputs. Our results indicate that IELs may offer a potential regularization mechanism for addressing challenges related to noisy labels.},
  archive      = {J_SIMODS},
  author       = {Chaoyu Liu and Zhonghua Qiao and Chao Li and Carola-Bibiane Schönlieb},
  doi          = {10.1137/24M1633662},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {55-85},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Inverse evolution layers: Physics-informed regularizers for image segmentation},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive joint distribution learning. <em>SIMODS</em>,
<em>7</em>(1), 28–54. (<a
href="https://doi.org/10.1137/24M1629900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We develop a new framework for estimating joint probability distributions using tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized, and positive model of a Radon–Nikodym derivative, which we estimate from sample sizes of up to several millions, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. Our proposal is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.},
  archive      = {J_SIMODS},
  author       = {Damir Filipović and Michael D. Multerer and Paul Schneider},
  doi          = {10.1137/24M1629900},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {28-54},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Adaptive joint distribution learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Principles for initialization and architecture selection in
graph neural networks with ReLU activations. <em>SIMODS</em>,
<em>7</em>(1), 1–27. (<a
href="https://doi.org/10.1137/23M1600621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article derives and validates three principles for initialization and architecture selection in finite width graph neural networks (GNNs) with ReLU activations. First, we theoretically derive what is essentially the unique generalization to ReLU GNNs of the well-known He-initialization. Our initialization scheme guarantees that the average scale of network outputs and gradients remains order one at initialization. Second, we prove in finite width vanilla ReLU GNNs that oversmoothing is unavoidable at large depth when using fixed aggregation operators, regardless of initialization. We then prove that using residual aggregation operators, obtained by interpolating a fixed aggregation operator with the identity, provably alleviates oversmoothing at initialization. Finally, we show that the common practice of using residual connections with a fixup-type initialization provably avoids correlation collapse in final layer features at initialization. Through ablation studies, we find that using the correct initialization, residual aggregation operators, and residual connections in the forward pass significantly and reliably speeds up early training dynamics in deep ReLU GNNs on a variety of tasks.},
  archive      = {J_SIMODS},
  author       = {Gage DeZoort and Boris Hanin},
  doi          = {10.1137/23M1600621},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Principles for initialization and architecture selection in graph neural networks with ReLU activations},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
