<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOSTAT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biostat---57">BIOSTAT - 57</h2>
<ul>
<li><details>
<summary>
(2021). Randomization-based confidence intervals for cluster
randomized trials. <em>BIOSTAT</em>, <em>22</em>(4), 913–927. (<a
href="https://doi.org/10.1093/biostatistics/kxaa007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a cluster randomized trial (CRT), groups of people are randomly assigned to different interventions. Existing parametric and semiparametric methods for CRTs rely on distributional assumptions or a large number of clusters to maintain nominal confidence interval (CI) coverage. Randomization-based inference is an alternative approach that is distribution-free and does not require a large number of clusters to be valid. Although it is well-known that a CI can be obtained by inverting a randomization test, this requires testing a non-zero null hypothesis, which is challenging with non-continuous and survival outcomes. In this article, we propose a general method for randomization-based CIs using individual-level data from a CRT. This approach accommodates various outcome types, can account for design features such as matching or stratification, and employs a computationally efficient algorithm. We evaluate this method’s performance through simulations and apply it to the Botswana Combination Prevention Project, a large HIV prevention trial with an interval-censored time-to-event outcome.},
  archive      = {J_BIOSTAT},
  author       = {Rabideau, Dustin J and Wang, Rui},
  doi          = {10.1093/biostatistics/kxaa007},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {913-927},
  shortjournal = {Biostatistics},
  title        = {Randomization-based confidence intervals for cluster randomized trials},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RoBoT: A robust bayesian hypothesis testing method for
basket trials. <em>BIOSTAT</em>, <em>22</em>(4), 897–912. (<a
href="https://doi.org/10.1093/biostatistics/kxaa005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A basket trial in oncology encompasses multiple “baskets” that simultaneously assess one treatment in multiple cancer types or subtypes. It is well-recognized that hierarchical modeling methods, which adaptively borrow strength across baskets, can improve over simple pooling and stratification. We propose a novel Bayesian method, RoBoT ( Ro bust B ayesian Hyp o thesis T esting), for the data analysis and decision-making in phase II basket trials. In contrast to most existing methods that use posterior credible intervals to determine the efficacy of the new treatment, RoBoT builds upon a formal Bayesian hypothesis testing framework that leads to interpretable and robust inference. Specifically, we assume that the baskets belong to several latent subgroups, and within each subgroup, the treatment has similar probabilities of being more efficacious than controls, historical, or concurrent. The number of latent subgroups and subgroup memberships are inferred by the data through a Dirichlet process mixture model. Such model specification helps avoid type I error inflation caused by excessive shrinkage under typical hierarchical models. The operating characteristics of RoBoT are assessed through computer simulations and are compared with existing methods. Finally, we apply RoBoT to data from two recent phase II basket trials of imatinib and vemurafenib, respectively.},
  archive      = {J_BIOSTAT},
  author       = {Zhou, Tianjian and Ji, Yuan},
  doi          = {10.1093/biostatistics/kxaa005},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {897-912},
  shortjournal = {Biostatistics},
  title        = {RoBoT: A robust bayesian hypothesis testing method for basket trials},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The identity of two meta-analytic likelihoods and the
ignorability of double-zero studies. <em>BIOSTAT</em>, <em>22</em>(4),
890–896. (<a
href="https://doi.org/10.1093/biostatistics/kxaa004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In meta-analysis, the conventional two-stage approach computes an effect estimate for each study in the first stage and proceeds with the analysis of effect estimates in the second stage. For counts of events as outcome, the risk ratio is often the effect measure of choice. However, if the meta-analysis includes many studies with no events the conventional method breaks down. As an alternative one-stage approach, a Poisson regression model and a conditional binomial model can be considered where no event studies do not cause problems. The conditional binomial model excludes double-zero studies, whereas this is seemingly not the case for the Poisson regression approach. However, we show here that both models lead to the same likelihood inference and double-zero studies (in contrast to single-zero studies) do not contribute in either case to the likelihood.},
  archive      = {J_BIOSTAT},
  author       = {Böhning, Dankmar and Sangnawakij, Patarawan},
  doi          = {10.1093/biostatistics/kxaa004},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {890-896},
  shortjournal = {Biostatistics},
  title        = {The identity of two meta-analytic likelihoods and the ignorability of double-zero studies},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized additive regression for group testing data.
<em>BIOSTAT</em>, <em>22</em>(4), 873–889. (<a
href="https://doi.org/10.1093/biostatistics/kxaa003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In screening applications involving low-prevalence diseases, pooling specimens (e.g., urine, blood, swabs, etc.) through group testing can be far more cost effective than testing specimens individually. Estimation is a common goal in such applications and typically involves modeling the probability of disease as a function of available covariates. In recent years, several authors have developed regression methods to accommodate the complex structure of group testing data but often under the assumption that covariate effects are linear. Although linearity is a reasonable assumption in some applications, it can lead to model misspecification and biased inference in others. To offer a more flexible framework, we propose a Bayesian generalized additive regression approach to model the individual-level probability of disease with potentially misclassified group testing data. Our approach can be used to analyze data arising from any group testing protocol with the goal of estimating multiple unknown smooth functions of covariates, standard linear effects for other covariates, and assay classification accuracy probabilities. We illustrate the methods in this article using group testing data on chlamydia infection in Iowa.},
  archive      = {J_BIOSTAT},
  author       = {Liu, Yan and McMahan, Christopher S and Tebbs, Joshua M and Gallagher, Colin M and Bilder, Christopher R},
  doi          = {10.1093/biostatistics/kxaa003},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {873-889},
  shortjournal = {Biostatistics},
  title        = {Generalized additive regression for group testing data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bias due to berkson error: Issues when using predicted
values in place of observed covariates. <em>BIOSTAT</em>,
<em>22</em>(4), 858–872. (<a
href="https://doi.org/10.1093/biostatistics/kxaa002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies often want to test for the association between an unmeasured covariate and an outcome. In the absence of a measurement, the study may substitute values generated from a prediction model. Justification for such methods can be found by noting that, with standard assumptions, this is equivalent to fitting a regression model for an outcome variable when at least one covariate is measured with Berkson error. Under this setting, it is known that consistent or nearly consistent inference can be obtained under many linear and nonlinear outcome models. In this article, we focus on the linear regression outcome model and show that this consistency property does not hold when there is unmeasured confounding in the outcome model, in which case the marginal inference based on a covariate measured with Berkson error differs from the same inference based on observed covariates. Since unmeasured confounding is ubiquitous in applications, this severely limits the practical use of such measurements, and, in particular, the substitution of predicted values for observed covariates. These issues are illustrated using data from the National Health and Nutrition Examination Survey to study the joint association of total percent body fat and body mass index with HbA1c. It is shown that using predicted total percent body fat in place of observed percent body fat yields inferences which often differ significantly, in some cases suggesting opposite relationships among covariates.},
  archive      = {J_BIOSTAT},
  author       = {Haber, Gregory and Sampson, Joshua and Graubard, Barry},
  doi          = {10.1093/biostatistics/kxaa002},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {858-872},
  shortjournal = {Biostatistics},
  title        = {Bias due to berkson error: Issues when using predicted values in place of observed covariates},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularized bayesian transfer learning for population-level
etiological distributions. <em>BIOSTAT</em>, <em>22</em>(4), 836–857.
(<a href="https://doi.org/10.1093/biostatistics/kxaa001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-coded verbal autopsy (CCVA) algorithms predict cause of death from high-dimensional family questionnaire data ( verbal autopsy ) of a deceased individual, which are then aggregated to generate national and regional estimates of cause-specific mortality fractions. These estimates may be inaccurate if CCVA is trained on non-local training data different from the local population of interest. This problem is a special case of transfer learning , i.e., improving classification within a target domain (e.g., a particular population) with the classifier trained in a source-domain . Most transfer learning approaches concern individual-level (e.g., a person’s) classification. Social and health scientists such as epidemiologists are often more interested with understanding etiological distributions at the population-level. The sample sizes of their data sets are typically orders of magnitude smaller than those used for common transfer learning applications like image classification, document identification, etc. We present a parsimonious hierarchical Bayesian transfer learning framework to directly estimate population-level class probabilities in a target domain, using any baseline classifier trained on source-domain, and a small labeled target-domain dataset. To address small sample sizes, we introduce a novel shrinkage prior for the transfer error rates guaranteeing that, in absence of any labeled target-domain data or when the baseline classifier is perfectly accurate, our transfer learning agrees with direct aggregation of predictions from the baseline classifier, thereby subsuming the default practice as a special case. We then extend our approach to use an ensemble of baseline classifiers producing an unified estimate. Theoretical and empirical results demonstrate how the ensemble model favors the most accurate baseline classifier. We present data analyses demonstrating the utility of our approach.},
  archive      = {J_BIOSTAT},
  author       = {Datta, Abhirup and Fiksel, Jacob and Amouzou, Agbessi and Zeger, Scott L},
  doi          = {10.1093/biostatistics/kxaa001},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {836-857},
  shortjournal = {Biostatistics},
  title        = {Regularized bayesian transfer learning for population-level etiological distributions},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating disease onset from change points of markers
measured with error. <em>BIOSTAT</em>, <em>22</em>(4), 819–835. (<a
href="https://doi.org/10.1093/biostatistics/kxz068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Huntington disease is an autosomal dominant, neurodegenerative disease without clearly identified biomarkers for when motor-onset occurs. Current standards to determine motor-onset rely on a clinician’s subjective judgment that a patient’s extrapyramidal signs are unequivocally associated with Huntington disease. This subjectivity can lead to error which could be overcome using an objective, data-driven metric that determines motor-onset. Recent studies of motor-sign decline—the longitudinal degeneration of motor-ability in patients—have revealed that motor-onset is closely related to an inflection point in its longitudinal trajectory. We propose a nonlinear location-shift marker model that captures this motor-sign decline and assesses how its inflection point is linked to other markers of Huntington disease progression. We propose two estimating procedures to estimate this model and its inflection point: one is a parametric method using nonlinear mixed effects model and the other one is a multi-stage nonparametric approach, which we developed. In an empirical study, the parametric approach was sensitive to correct specification of the mean structure of the longitudinal data. In contrast, our multi-stage nonparametric procedure consistently produced unbiased estimates regardless of the true mean structure. Applying our multi-stage nonparametric estimator to Neurobiological Predictors of Huntington Disease, a large observational study of Huntington disease, leads to earlier prediction of motor-onset compared to the clinician’s subjective judgment.},
  archive      = {J_BIOSTAT},
  author       = {Lee, Unkyung and Carroll, Raymond J and Marder, Karen and Wang, Yuanjia and Garcia, Tanya P},
  doi          = {10.1093/biostatistics/kxz068},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {819-835},
  shortjournal = {Biostatistics},
  title        = {Estimating disease onset from change points of markers measured with error},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and inference for the population attributable
risk in the presence of misclassification. <em>BIOSTAT</em>,
<em>22</em>(4), 805–818. (<a
href="https://doi.org/10.1093/biostatistics/kxz067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because it describes the proportion of disease cases that could be prevented if an exposure were entirely eliminated from a target population as a result of an intervention, estimation of the population attributable risk (PAR) has become an important goal of public health research. In epidemiologic studies, categorical covariates are often misclassified. We present methods for obtaining point and interval estimates of the PAR and the partial PAR (pPAR) in the presence of misclassification, filling an important existing gap in public health evaluation methods. We use a likelihood-based approach to estimate parameters in the models for the disease and for the misclassification process, under main study/internal validation study and main study/external validation study designs, and various plausible assumptions about transportability. We assessed the finite sample perf ormance of this method via a simulation study, and used it to obtain corrected point and interval estimates of the pPAR for high red meat intake and alcohol intake in relation to colorectal cancer incidence in the HPFS, where we found that the estimated pPAR for the two risk factors increased by up to 317% after correcting for bias due to misclassification.},
  archive      = {J_BIOSTAT},
  author       = {Wong, Benedict H W and Lee, Jooyoung and Spiegelman, Donna and Wang, Molin},
  doi          = {10.1093/biostatistics/kxz067},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {805-818},
  shortjournal = {Biostatistics},
  title        = {Estimation and inference for the population attributable risk in the presence of misclassification},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic borrowing in the presence of treatment effect
heterogeneity. <em>BIOSTAT</em>, <em>22</em>(4), 789–804. (<a
href="https://doi.org/10.1093/biostatistics/kxz066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of statistical approaches have been proposed for incorporating supplemental information in randomized clinical trials. Existing methods often compare the marginal treatment effects to evaluate the degree of consistency between sources. Dissimilar marginal treatment effects would either lead to increased bias or down-weighting of the supplemental data. This represents a limitation in the presence of treatment effect heterogeneity, in which case the marginal treatment effect may differ between the sources solely due to differences between the study populations. We introduce the concept of covariate-adjusted exchangeability, in which differences in the marginal treatment effect can be explained by differences in the distributions of the effect modifiers. The potential outcomes framework is used to conceptualize covariate-adjusted and marginal exchangeability. We utilize a linear model and the existing multisource exchangeability models framework to facilitate borrowing when marginal treatment effects are dissimilar but covariate-adjusted exchangeability holds. We investigate the operating characteristics of our method using simulations. We also illustrate our method using data from two clinical trials of very low nicotine content cigarettes. Our method has the ability to incorporate supplemental information in a wider variety of situations than when only marginal exchangeability is considered.},
  archive      = {J_BIOSTAT},
  author       = {Kotalik, Ales and Vock, David M and Donny, Eric C and Hatsukami, Dorothy K and Koopmeiners, Joseph S},
  doi          = {10.1093/biostatistics/kxz066},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {789-804},
  shortjournal = {Biostatistics},
  title        = {Dynamic borrowing in the presence of treatment effect heterogeneity},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mixed-model approach for powerful testing of genetic
associations with cancer risk incorporating tumor characteristics.
<em>BIOSTAT</em>, <em>22</em>(4), 772–788. (<a
href="https://doi.org/10.1093/biostatistics/kxz065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancers are routinely classified into subtypes according to various features, including histopathological characteristics and molecular markers. Previous genome-wide association studies have reported heterogeneous associations between loci and cancer subtypes. However, it is not evident what is the optimal modeling strategy for handling correlated tumor features, missing data, and increased degrees-of-freedom in the underlying tests of associations. We propose to test for genetic associations using a mixed-effect two-stage polytomous model score test (MTOP). In the first stage, a standard polytomous model is used to specify all possible subtypes defined by the cross-classification of the tumor characteristics. In the second stage, the subtype-specific case–control odds ratios are specified using a more parsimonious model based on the case–control odds ratio for a baseline subtype, and the case–case parameters associated with tumor markers. Further, to reduce the degrees-of-freedom, we specify case–case parameters for additional exploratory markers using a random-effect model. We use the Expectation–Maximization algorithm to account for missing data on tumor markers. Through simulations across a range of realistic scenarios and data from the Polish Breast Cancer Study (PBCS), we show MTOP outperforms alternative methods for identifying heterogeneous associations between risk loci and tumor subtypes. The proposed methods have been implemented in a user-friendly and high-speed R statistical package called TOP ( https://github.com/andrewhaoyu/TOP )},
  archive      = {J_BIOSTAT},
  author       = {Zhang, Haoyu and Zhao, Ni and Ahearn, Thomas U and Wheeler, William and García-Closas, Montserrat and Chatterjee, Nilanjan},
  doi          = {10.1093/biostatistics/kxz065},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {772-788},
  shortjournal = {Biostatistics},
  title        = {A mixed-model approach for powerful testing of genetic associations with cancer risk incorporating tumor characteristics},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous monitoring for regression coefficients and
baseline hazard profile in cox modeling of time-to-event data.
<em>BIOSTAT</em>, <em>22</em>(4), 756–771. (<a
href="https://doi.org/10.1093/biostatistics/kxz064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox model is the most popular tool for analyzing time-to-event data. The nonparametric baseline hazard function can be as important as the regression coefficients in practice, especially when prediction is needed. In the context of stochastic process control, we propose a simultaneous monitoring method that combines a multivariate control chart for the regression coefficients and a profile control chart for the cumulative baseline hazard function that allows for data blocks of possibly different censoring rates and sample sizes. The method can detect changes in either the parametric or the nonparametric part of the Cox model. In simulation studies, the proposed method maintains its size and has substantial power in detecting changes in either part of the Cox model. An application in lymphoma survival analysis in which patients were enrolled by 2-month intervals in the Surveillance, Epidemiology, and End Results program identifies data blocks with structural model changes.},
  archive      = {J_BIOSTAT},
  author       = {Xue, Yishu and Yan, Jun and Schifano, Elizabeth D},
  doi          = {10.1093/biostatistics/kxz064},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {756-771},
  shortjournal = {Biostatistics},
  title        = {Simultaneous monitoring for regression coefficients and baseline hazard profile in cox modeling of time-to-event data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse estimation for case–control studies with multiple
disease subtypes. <em>BIOSTAT</em>, <em>22</em>(4), 738–755. (<a
href="https://doi.org/10.1093/biostatistics/kxz063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of case–control studies with several disease subtypes is increasingly common, e.g. in cancer epidemiology. For matched designs, a natural strategy is based on a stratified conditional logistic regression model. Then, to account for the potential homogeneity among disease subtypes, we adapt the ideas of data shared lasso, which has been recently proposed for the estimation of stratified regression models. For unmatched designs, we compare two standard methods based on |$L_1$| -norm penalized multinomial logistic regression. We describe formal connections between these two approaches, from which practical guidance can be derived. We show that one of these approaches, which is based on a symmetric formulation of the multinomial logistic regression model, actually reduces to a data shared lasso version of the other. Consequently, the relative performance of the two approaches critically depends on the level of homogeneity that exists among disease subtypes: more precisely, when homogeneity is moderate to high, the non-symmetric formulation with controls as the reference is not recommended. Empirical results obtained from synthetic data are presented, which confirm the benefit of properly accounting for potential homogeneity under both matched and unmatched designs, in terms of estimation and prediction accuracy, variable selection and identification of heterogeneities. We also present preliminary results from the analysis of a case–control study nested within the EPIC (European Prospective Investigation into Cancer and nutrition) cohort, where the objective is to identify metabolites associated with the occurrence of subtypes of breast cancer.},
  archive      = {J_BIOSTAT},
  author       = {Ballout, Nadim and Garcia, Cedric and Viallon, Vivian},
  doi          = {10.1093/biostatistics/kxz063},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {738-755},
  shortjournal = {Biostatistics},
  title        = {Sparse estimation for case–control studies with multiple disease subtypes},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive group-regularized logistic elastic net regression.
<em>BIOSTAT</em>, <em>22</em>(4), 723–737. (<a
href="https://doi.org/10.1093/biostatistics/kxz062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional data settings, additional information on the features is often available. Examples of such external information in omics research are: (i) |$p$| -values from a previous study and (ii) omics annotation. The inclusion of this information in the analysis may enhance classification performance and feature selection but is not straightforward. We propose a group-regularized (logistic) elastic net regression method, where each penalty parameter corresponds to a group of features based on the external information. The method, termed gren , makes use of the Bayesian formulation of logistic elastic net regression to estimate both the model and penalty parameters in an approximate empirical–variational Bayes framework. Simulations and applications to three cancer genomics studies and one Alzheimer metabolomics study show that, if the partitioning of the features is informative, classification performance, and feature selection are indeed enhanced.},
  archive      = {J_BIOSTAT},
  author       = {Münch, Magnus M and Peeters, Carel F W and Van Der Vaart, Aad W and Van De Wiel, Mark A},
  doi          = {10.1093/biostatistics/kxz062},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {723-737},
  shortjournal = {Biostatistics},
  title        = {Adaptive group-regularized logistic elastic net regression},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trans-ethnic meta-analysis of rare variants in sequencing
association studies. <em>BIOSTAT</em>, <em>22</em>(4), 706–722. (<a
href="https://doi.org/10.1093/biostatistics/kxz061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trans-ethnic meta-analysis is a powerful tool for detecting novel loci in genetic association studies. However, in the presence of heterogeneity among different populations, existing gene-/region-based rare variants meta-analysis methods may be unsatisfactory because they do not consider genetic similarity or dissimilarity among different populations. In response, we propose a score test under the modified random effects model for gene-/region-based rare variants associations. We adapt the kernel regression framework to construct the model and incorporate genetic similarities across populations into modeling the heterogeneity structure of the genetic effect coefficients. We use a resampling-based copula method to approximate asymptotic distribution of the test statistic, enabling efficient estimation of p-values. Simulation studies show that our proposed method controls type I error rates and increases power over existing approaches in the presence of heterogeneity. We illustrate our method by analyzing T2D-GENES consortium exome sequence data to explore rare variant associations with several traits.},
  archive      = {J_BIOSTAT},
  author       = {Shi, Jingchunzi and Boehnke, Michael and Lee, Seunggeun},
  doi          = {10.1093/biostatistics/kxz061},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {706-722},
  shortjournal = {Biostatistics},
  title        = {Trans-ethnic meta-analysis of rare variants in sequencing association studies},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sufficient dimension reduction for compositional data.
<em>BIOSTAT</em>, <em>22</em>(4), 687–705. (<a
href="https://doi.org/10.1093/biostatistics/kxz060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent efforts to characterize the human microbiome and its relation to chronic diseases have led to a surge in statistical development for compositional data. We develop likelihood-based sufficient dimension reduction methods (SDR) to find linear combinations that contain all the information in the compositional data on an outcome variable, i.e., are sufficient for modeling and prediction of the outcome. We consider several models for the inverse regression of the compositional vector or transformations of it, as a function of outcome. They include normal, multinomial, and Poisson graphical models that allow for complex dependencies among observed counts. These methods yield efficient estimators of the reduction and can be applied to continuous or categorical outcomes. We incorporate variable selection into the estimation via penalties and address important invariance issues arising from the compositional nature of the data. We illustrate and compare our methods and some established methods for analyzing microbiome data in simulations and using data from the Human Microbiome Project. Displaying the data in the coordinate system of the SDR linear combinations allows visual inspection and facilitates comparisons across studies.},
  archive      = {J_BIOSTAT},
  author       = {Tomassi, Diego and Forzani, Liliana and Duarte, Sabrina and Pfeiffer, Ruth M},
  doi          = {10.1093/biostatistics/kxz060},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {687-705},
  shortjournal = {Biostatistics},
  title        = {Sufficient dimension reduction for compositional data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corrigendum to: Neuroconductor: An r platform for medical
imaging analysis. <em>BIOSTAT</em>, <em>22</em>(3), 685. (<a
href="https://doi.org/10.1093/biostatistics/kxaa006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  author       = {Muschelli, John and Gherman, Adrian and Fortin, Jean-Philippe and Avants, Brian and Whitcher, Brandon and Clayden, Jonathan D and Caffo, Brian S and Crainiceanu, Ciprian M},
  doi          = {10.1093/biostatistics/kxaa006},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {685},
  shortjournal = {Biostatistics},
  title        = {Corrigendum to: neuroconductor: an r platform for medical imaging analysis},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corrigendum: Analysis of cluster-randomized test-negative
designs: Cluster-level methods. <em>BIOSTAT</em>, <em>22</em>(3), 684.
(<a href="https://doi.org/10.1093/biostatistics/kxz008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  author       = {Jewell, Nicholas P and Dufault, Suzanne and Cutcher, Zoe and Simmons, Cameron P and Anders, Katherine L},
  doi          = {10.1093/biostatistics/kxz008},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {684},
  shortjournal = {Biostatistics},
  title        = {Corrigendum: analysis of cluster-randomized test-negative designs: cluster-level methods},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring effects of medication adherence on time-varying
health outcomes using bayesian dynamic linear models. <em>BIOSTAT</em>,
<em>22</em>(3), 662–683. (<a
href="https://doi.org/10.1093/biostatistics/kxz059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most significant barriers to medication treatment is patients’ non-adherence to a prescribed medication regimen. The extent of the impact of poor adherence on resulting health measures is often unknown, and typical analyses ignore the time-varying nature of adherence. This article develops a modeling framework for longitudinally recorded health measures modeled as a function of time-varying medication adherence. Our framework, which relies on normal Bayesian dynamic linear models (DLMs), accounts for time-varying covariates such as adherence and non-dynamic covariates such as baseline health characteristics. Standard inferential procedures for DLMs are inefficient when faced with infrequent and irregularly recorded response data. We develop an approach that relies on factoring the posterior density into a product of two terms: a marginal posterior density for the non-dynamic parameters, and a multivariate normal posterior density of the dynamic parameters conditional on the non-dynamic ones. This factorization leads to a two-stage process for inference in which the non-dynamic parameters can be inferred separately from the time-varying parameters. We demonstrate the application of this model to the time-varying effect of antihypertensive medication on blood pressure levels for a cohort of patients diagnosed with hypertension. Our model results are compared to ones in which adherence is incorporated through non-dynamic summaries.},
  archive      = {J_BIOSTAT},
  author       = {Campos, Luis F and Glickman, Mark E and Hunter, Kristen B},
  doi          = {10.1093/biostatistics/kxz059},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {662-683},
  shortjournal = {Biostatistics},
  title        = {Measuring effects of medication adherence on time-varying health outcomes using bayesian dynamic linear models},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A local group differences test for subject-level
multivariate density neuroimaging outcomes. <em>BIOSTAT</em>,
<em>22</em>(3), 646–661. (<a
href="https://doi.org/10.1093/biostatistics/kxz058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A great deal of neuroimaging research focuses on voxel-wise analysis or segmentation of damaged tissue, yet many diseases are characterized by diffuse or non-regional neuropathology. In simple cases, these processes can be quantified using summary statistics of voxel intensities. However, the manifestation of a disease process in imaging data is often unknown, or appears as a complex and nonlinear relationship between the voxel intensities on various modalities. When the relevant pattern is unknown, summary statistics are often unable to capture differences between disease groups, and their use may encourage post hoc searches for the optimal summary measure. In this study, we introduce the multi-modal density testing (MMDT) framework for the naive discovery of group differences in voxel intensity profiles. MMDT operationalizes multi-modal magnetic resonance imaging (MRI) data as multivariate subject-level densities of voxel intensities and utilizes kernel density estimation to develop a local two-sample test for individual points within the density space. Through simulations, we show that this method controls type I error and recovers relevant differences when applied to a specified point. Additionally, we demonstrate the ability to maintain power while controlling the family-wise error rate and false discovery rate when applying the test over a grid of points within the density space. Finally, we apply this method to a study of subjects with either multiple sclerosis (MS) or conditions that tend to mimic MS on MRI, and find significant differences between the two groups in their voxel intensity profiles within the thalamus.},
  archive      = {J_BIOSTAT},
  author       = {Dworkin, Jordan D and Linn, Kristin A and Solomon, Andrew J and Satterthwaite, Theodore D and Raznahan, Armin and Bakshi, Rohit and Shinohara, Russell T},
  doi          = {10.1093/biostatistics/kxz058},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {646-661},
  shortjournal = {Biostatistics},
  title        = {A local group differences test for subject-level multivariate density neuroimaging outcomes},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covariate assisted principal regression for covariance
matrix outcomes. <em>BIOSTAT</em>, <em>22</em>(3), 629–645. (<a
href="https://doi.org/10.1093/biostatistics/kxz057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we consider the problem of regressing covariance matrices on associated covariates. Our goal is to use covariates to explain variation in covariance matrices across units. As such, we introduce Covariate Assisted Principal (CAP) regression, an optimization-based method for identifying components associated with the covariates using a generalized linear model approach. We develop computationally efficient algorithms to jointly search for common linear projections of the covariance matrices, as well as the regression coefficients. Under the assumption that all the covariance matrices share identical eigencomponents, we establish the asymptotic properties. In simulation studies, our CAP method shows higher accuracy and robustness in coefficient estimation over competing methods. In an example resting-state functional magnetic resonance imaging study of healthy adults, CAP identifies human brain network changes associated with subject demographics.},
  archive      = {J_BIOSTAT},
  author       = {Zhao, Yi and Wang, Bingkai and Mostofsky, Stewart H and Caffo, Brian S and Luo, Xi},
  doi          = {10.1093/biostatistics/kxz057},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {629-645},
  shortjournal = {Biostatistics},
  title        = {Covariate assisted principal regression for covariance matrix outcomes},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mapping epileptic directional brain networks using
intracranial EEG data. <em>BIOSTAT</em>, <em>22</em>(3), 613–628. (<a
href="https://doi.org/10.1093/biostatistics/kxz056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain is a directional network system, in which brain regions are network nodes and the influence exerted by one region on another is a network edge. We refer to this directional information flow from one region to another as directional connectivity . Seizures arise from an epileptic directional network; abnormal neuronal activities start from a seizure onset zone and propagate via a network to otherwise healthy brain regions. As such, effective epilepsy diagnosis and treatment require accurate identification of directional connections among regions, i.e., mapping of epileptic patients’ brain networks. This article aims to understand the epileptic brain network using intracranial electroencephalographic data—recordings of epileptic patients’ brain activities in many regions. The most popular models for directional connectivity use ordinary differential equations (ODE). However, ODE models are sensitive to data noise and computationally costly. To address these issues, we propose a high-dimensional state-space multivariate autoregression (SSMAR) model for the brain’s directional connectivity. Different from standard multivariate autoregression and SSMAR models, the proposed SSMAR features a cluster structure, where the brain network consists of several clusters of densely connected brain regions. We develop an expectation–maximization algorithm to estimate the proposed model and use it to map the interregional networks of epileptic patients in different seizure stages. Our method reveals the evolution of brain networks during seizure development.},
  archive      = {J_BIOSTAT},
  author       = {Li, Huazhang and Wang, Yaotian and Tanabe, Seiji and Sun, Yinge and Yan, Guofen and Quigg, Mark S and Zhang, Tingting},
  doi          = {10.1093/biostatistics/kxz056},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {613-628},
  shortjournal = {Biostatistics},
  title        = {Mapping epileptic directional brain networks using intracranial EEG data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of marginal causal effects in the presence of
confounding by cluster. <em>BIOSTAT</em>, <em>22</em>(3), 598–612. (<a
href="https://doi.org/10.1093/biostatistics/kxz054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular way to control for unmeasured confounders is to utilize clusters (e.g. sets of siblings), in which a potentially large set of confounders are constant. By estimating the exposure–outcome association within clusters, rather than between unrelated subjects, all cluster-constant confounders are implicitly controlled for. To analyze such clustered data, it is common to use fixed effects models, which absorb all cluster-constant confounders into a cluster-specific intercept. In this article, we show how linear and log-linear fixed effects models can be used to estimate marginal counterfactual means. These counterfactual means can be estimated and presented for each exposure level separately, or contrasted to form a wide range of marginal causal effects. For binary outcomes, we propose to estimate marginal causal effects with marginal logistic between-within models. These models include a constant intercept common for all clusters, and control for unmeasured cluster-constant confounders by adding the mean exposure level in each cluster to the model. We illustrate the proposed methods by re-analyzing data from a co-twin control study on birth weight and Attention-Deficit/Hyperactivity Disorder.},
  archive      = {J_BIOSTAT},
  author       = {Sjölander, Arvid},
  doi          = {10.1093/biostatistics/kxz054},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {598-612},
  shortjournal = {Biostatistics},
  title        = {Estimation of marginal causal effects in the presence of confounding by cluster},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pair-based likelihood approximations for stochastic epidemic
models. <em>BIOSTAT</em>, <em>22</em>(3), 575–597. (<a
href="https://doi.org/10.1093/biostatistics/kxz053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitting stochastic epidemic models to data is a non-standard problem because data on the infection processes defined in such models are rarely observed directly. This in turn means that the likelihood of the observed data is intractable in the sense that it is very computationally expensive to obtain. Although data-augmented Markov chain Monte Carlo (MCMC) methods provide a solution to this problem, employing a tractable augmented likelihood, such methods typically deteriorate in large populations due to poor mixing and increased computation time. Here, we describe a new approach that seeks to approximate the likelihood by exploiting the underlying structure of the epidemic model. Simulation study results show that this approach can be a serious competitor to data-augmented MCMC methods. Our approach can be applied to a wide variety of disease transmission models, and we provide examples with applications to the common cold, Ebola, and foot-and-mouth disease.},
  archive      = {J_BIOSTAT},
  author       = {Stockdale, Jessica E and Kypraios, Theodore and O’Neill, Philip D},
  doi          = {10.1093/biostatistics/kxz053},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {575-597},
  shortjournal = {Biostatistics},
  title        = {Pair-based likelihood approximations for stochastic epidemic models},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic landmark prediction for mixture data.
<em>BIOSTAT</em>, <em>22</em>(3), 558–574. (<a
href="https://doi.org/10.1093/biostatistics/kxz052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In kin-cohort studies, clinicians want to provide their patients with the most current cumulative risk of death arising from a rare deleterious mutation. Estimating the cumulative risk is difficult when the genetic mutation status is unknown and only estimated probabilities of a patient having the mutation are available. We estimate the cumulative risk for this scenario using a novel nonparametric estimator that incorporates covariate information and dynamic landmark prediction. Our estimator has improved prediction accuracy over existing estimators that ignore covariate information. It is built within a dynamic landmark prediction framework whereby we can obtain personalized dynamic predictions over time. Compared to current standards, a simple transformation of our estimator provides more efficient estimates of marginal distribution functions in settings where patient-specific predictions are not the main goal. We show our estimator is unbiased and has more predictive accuracy compared to methods that ignore covariate information and landmarking. Applying our method to a Huntington disease study of mortality, we develop dynamic survival prediction curves incorporating gender and familial genetic information.},
  archive      = {J_BIOSTAT},
  author       = {Garcia, Tanya P and Parast, Layla},
  doi          = {10.1093/biostatistics/kxz052},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {558-574},
  shortjournal = {Biostatistics},
  title        = {Dynamic landmark prediction for mixture data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical methods for biomarker data pooled from multiple
nested case–control studies. <em>BIOSTAT</em>, <em>22</em>(3), 541–557.
(<a href="https://doi.org/10.1093/biostatistics/kxz051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pooling biomarker data across multiple studies allows for examination of a wider exposure range than generally possible in individual studies, evaluation of population subgroups and disease subtypes with more statistical power, and more precise estimation of biomarker-disease associations. However, circulating biomarker measurements often require calibration to a single reference assay prior to pooling due to assay and laboratory variability across studies. We propose several methods for calibrating and combining biomarker data from nested case–control studies when reference assay data are obtained from a subset of controls in each contributing study. Specifically, we describe a two-stage calibration method and two aggregated calibration methods, named the internalized and full calibration methods, to evaluate the main effect of the biomarker exposure on disease risk and whether that association is modified by a potential covariate. The internalized method uses the reference laboratory measurement in the analysis when available and otherwise uses the estimated value derived from calibration models. The full calibration method uses calibrated biomarker measurements for all subjects, including those with reference laboratory measurements. Under the two-stage method, investigators complete study-specific analyses in the first stage followed by meta-analysis in the second stage. Our results demonstrate that the full calibration method is the preferred aggregated approach to minimize bias in point estimates. We also observe that the two-stage and full calibration methods provide similar effect and variance estimates but that their variance estimates are slightly larger than those from the internalized approach. As an illustrative example, we apply the three methods in a pooling project of nested case–control studies to evaluate (i) the association between circulating vitamin D levels and risk of stroke and (ii) how body mass index modifies the association between circulating vitamin D levels and risk of cardiovascular disease.},
  archive      = {J_BIOSTAT},
  author       = {Sloan, Abigail and Smith-Warner, Stephanie A and Ziegler, Regina G and Wang, Molin},
  doi          = {10.1093/biostatistics/kxz051},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {541-557},
  shortjournal = {Biostatistics},
  title        = {Statistical methods for biomarker data pooled from multiple nested case–control studies},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian zero-inflated negative binomial regression model
for the integrative analysis of microbiome data. <em>BIOSTAT</em>,
<em>22</em>(3), 522–540. (<a
href="https://doi.org/10.1093/biostatistics/kxz050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microbiome omics approaches can reveal intriguing relationships between the human microbiome and certain disease states. Along with identification of specific bacteria taxa associated with diseases, recent scientific advancements provide mounting evidence that metabolism, genetics, and environmental factors can all modulate these microbial effects. However, the current methods for integrating microbiome data and other covariates are severely lacking. Hence, we present an integrative Bayesian zero-inflated negative binomial regression model that can both distinguish differentially abundant taxa with distinct phenotypes and quantify covariate-taxa effects. Our model demonstrates good performance using simulated data. Furthermore, we successfully integrated microbiome taxonomies and metabolomics in two real microbiome datasets to provide biologically interpretable findings. In all, we proposed a novel integrative Bayesian regression model that features bacterial differential abundance analysis and microbiome-covariate effects quantifications, which makes it suitable for general microbiome studies.},
  archive      = {J_BIOSTAT},
  author       = {Jiang, Shuang and Xiao, Guanghua and Koh, Andrew Y and Kim, Jiwoong and Li, Qiwei and Zhan, Xiaowei},
  doi          = {10.1093/biostatistics/kxz050},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {522-540},
  shortjournal = {Biostatistics},
  title        = {A bayesian zero-inflated negative binomial regression model for the integrative analysis of microbiome data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A gaussian copula approach for dynamic prediction of
survival with a longitudinal biomarker. <em>BIOSTAT</em>,
<em>22</em>(3), 504–521. (<a
href="https://doi.org/10.1093/biostatistics/kxz049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic prediction uses patient information collected during follow-up to produce individualized survival predictions at given time points beyond treatment or diagnosis. This allows clinicians to obtain updated predictions of a patient’s prognosis that can be used in making personalized treatment decisions. Two commonly used approaches for dynamic prediction are landmarking and joint modeling. Landmarking does not constitute a comprehensive probability model, and joint modeling often requires strong distributional assumptions and computationally intensive methods for estimation. We introduce an alternative approximate approach for dynamic prediction that aims to overcome the limitations of both methods while achieving good predictive performance. We separately specify the marker and failure time distributions conditional on surviving up to a prediction time of interest and use standard variable selection and goodness-of-fit techniques to identify the best-fitting models. Taking advantage of its analytic tractability and easy two-stage estimation, we use a Gaussian copula to link the marginal distributions smoothly at each prediction time with an association function. With simulation studies, we examine the proposed method’s performance. We illustrate its use for dynamic prediction in an application to predicting death for heart valve transplant patients using longitudinal left ventricular mass index information.},
  archive      = {J_BIOSTAT},
  author       = {Suresh, Krithika and Taylor, Jeremy M G and Tsodikov, Alexander},
  doi          = {10.1093/biostatistics/kxz049},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {504-521},
  shortjournal = {Biostatistics},
  title        = {A gaussian copula approach for dynamic prediction of survival with a longitudinal biomarker},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The illness-death model for family studies.
<em>BIOSTAT</em>, <em>22</em>(3), 482–503. (<a
href="https://doi.org/10.1093/biostatistics/kxz048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Family studies involve the selection of affected individuals from a disease registry who provide right-truncated ages of disease onset. Coarsened disease histories are then obtained from consenting family members, either through examining medical records, retrospective reporting, or clinical examination. Methods for dealing with such biased sampling schemes are available for continuous, binary, and failure time responses, but methods for more complex life history processes are less developed. We consider a simple joint model for clustered illness-death processes which we formulate to study covariate effects on the marginal intensity for disease onset and to study the within-family dependence in disease onset times. We construct likelihoods and composite likelihoods for family data obtained from biased sampling schemes. In settings where the disease is rare and data are insufficient to fit the model of interest, we show how auxiliary data can augment the composite likelihood to facilitate estimation. We apply the proposed methods to analyze data from a family study of psoriatic arthritis carried out at the University of Toronto Psoriatic Arthritis Registry.},
  archive      = {J_BIOSTAT},
  author       = {Lee, Jooyoung and Cook, Richard J},
  doi          = {10.1093/biostatistics/kxz048},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {482-503},
  shortjournal = {Biostatistics},
  title        = {The illness-death model for family studies},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Independence conditions and the analysis of life history
studies with intermittent observation. <em>BIOSTAT</em>, <em>22</em>(3),
455–481. (<a
href="https://doi.org/10.1093/biostatistics/kxz047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistate models provide a powerful framework for the analysis of life history processes when the goal is to characterize transition intensities, transition probabilities, state occupancy probabilities, and covariate effects thereon. Data on such processes are often only available at random visit times occurring over a finite period. We formulate a joint multistate model for the life history process, the recurrent visit process, and a random loss to follow-up time at which the visit process terminates. This joint model is helpful when discussing the independence conditions necessary to justify the use of standard likelihoods involving the life history model alone and provides a basis for analyses that accommodate dependence. We consider settings with disease-driven visits and routinely scheduled visits and develop likelihoods that accommodate partial information on the types of visits. Simulation studies suggest that suitably constructed joint models can yield consistent estimates of parameters of interest even under dependent visit processes, providing the models are correctly specified; identifiability and estimability issues are also discussed. An application is given to a cohort of individuals attending a rheumatology clinic where interest lies in progression of joint damage.},
  archive      = {J_BIOSTAT},
  author       = {Cook, Richard J and Lawless, Jerald F},
  doi          = {10.1093/biostatistics/kxz047},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {455-481},
  shortjournal = {Biostatistics},
  title        = {Independence conditions and the analysis of life history studies with intermittent observation},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A functional mixed model for scalar on function regression
with application to a functional MRI study. <em>BIOSTAT</em>,
<em>22</em>(3), 439–454. (<a
href="https://doi.org/10.1093/biostatistics/kxz046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a functional magnetic resonance imaging (fMRI) study, we propose a new functional mixed model for scalar on function regression. The model extends the standard scalar on function regression for repeated outcomes by incorporating subject-specific random functional effects. Using functional principal component analysis, the new model can be reformulated as a mixed effects model and thus easily fit. A test is also proposed to assess the existence of the subject-specific random functional effects. We evaluate the performance of the model and test via a simulation study, as well as on data from the motivating fMRI study of thermal pain. The data application indicates significant subject-specific effects of the human brain hemodynamics related to pain and provides insights on how the effects might differ across subjects.},
  archive      = {J_BIOSTAT},
  author       = {Ma, Wanying and Xiao, Luo and Liu, Bowen and Lindquist, Martin A},
  doi          = {10.1093/biostatistics/kxz046},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {439-454},
  shortjournal = {Biostatistics},
  title        = {A functional mixed model for scalar on function regression with application to a functional MRI study},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating the surrogacy of multiple vaccine-induced immune
response biomarkers in HIV vaccine trials. <em>BIOSTAT</em>,
<em>22</em>(2), 421–436. (<a
href="https://doi.org/10.1093/biostatistics/kxz039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying biomarkers as surrogates for clinical endpoints in randomized vaccine trials is useful for reducing study duration and costs, relieving participants of unnecessary discomfort, and understanding vaccine-effect mechanism. In this article, we use risk models with multiple vaccine-induced immune response biomarkers to measure the causal association between a vaccine’s effects on these biomarkers and that on the clinical endpoint. In this setup, our main objective is to combine and select markers with high surrogacy from a list of many candidate markers, allowing us to get a more parsimonious model which can potentially increase the predictive quality of the true markers. To address the missing “potential” biomarker value if a subject receives placebo, we utilize the baseline immunogenicity predictor design augmented with a “closeout placebo vaccination” group. We then impute the missing potential marker values and conduct marker selection through a stepwise resampling and imputation method called stability selection. We test our proposed strategy under relevant simulation settings and on (partially simulated) biomarker data from a HIV vaccine trial (RV144).},
  archive      = {J_BIOSTAT},
  author       = {Dasgupta, Sayan and Huang, Ying},
  doi          = {10.1093/biostatistics/kxz039},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {421-436},
  shortjournal = {Biostatistics},
  title        = {Evaluating the surrogacy of multiple vaccine-induced immune response biomarkers in HIV vaccine trials},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Paired test of matrix graphs and brain connectivity
analysis. <em>BIOSTAT</em>, <em>22</em>(2), 402–420. (<a
href="https://doi.org/10.1093/biostatistics/kxz037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring brain connectivity network and quantifying the significance of interactions between brain regions are of paramount importance in neuroscience. Although there have recently emerged some tests for graph inference based on independent samples, there is no readily available solution to test the change of brain network for paired and correlated samples. In this article, we develop a paired test of matrix graphs to infer brain connectivity network when the groups of samples are correlated. The proposed test statistic is both bias corrected and variance corrected, and achieves a small estimation error rate. The subsequent multiple testing procedure built on this test statistic is guaranteed to asymptotically control the false discovery rate at the pre-specified level. Both the methodology and theory of the new test are considerably different from the two independent samples framework, owing to the strong correlations of measurements on the same subjects before and after the stimulus activity. We illustrate the efficacy of our proposal through simulations and an analysis of an Alzheimer’s Disease Neuroimaging Initiative dataset.},
  archive      = {J_BIOSTAT},
  author       = {Ye, Yuting and Xia, Yin and Li, Lexin},
  doi          = {10.1093/biostatistics/kxz037},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {402-420},
  shortjournal = {Biostatistics},
  title        = {Paired test of matrix graphs and brain connectivity analysis},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast divide-and-conquer sparse cox regression.
<em>BIOSTAT</em>, <em>22</em>(2), 381–401. (<a
href="https://doi.org/10.1093/biostatistics/kxz036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a computationally and statistically efficient divide-and-conquer (DAC) algorithm to fit sparse Cox regression to massive datasets where the sample size |$n_0$| is exceedingly large and the covariate dimension |$p$| is not small but |$n_0\gg p$|⁠ . The proposed algorithm achieves computational efficiency through a one-step linear approximation followed by a least square approximation to the partial likelihood (PL). These sequences of linearization enable us to maximize the PL with only a small subset and perform penalized estimation via a fast approximation to the PL. The algorithm is applicable for the analysis of both time-independent and time-dependent survival data. Simulations suggest that the proposed DAC algorithm substantially outperforms the full sample-based estimators and the existing DAC algorithm with respect to the computational speed, while it achieves similar statistical efficiency as the full sample-based estimators. The proposed algorithm was applied to extraordinarily large survival datasets for the prediction of heart failure-specific readmission within 30 days among Medicare heart failure patients.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Yan and Hong, Chuan and Palmer, Nathan and Di, Qian and Schwartz, Joel and Kohane, Isaac and Cai, Tianxi},
  doi          = {10.1093/biostatistics/kxz036},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {381-401},
  shortjournal = {Biostatistics},
  title        = {A fast divide-and-conquer sparse cox regression},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Better-than-chance classification for signal detection.
<em>BIOSTAT</em>, <em>22</em>(2), 365–380. (<a
href="https://doi.org/10.1093/biostatistics/kxz035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimated accuracy of a classifier is a random quantity with variability. A common practice in supervised machine learning, is thus to test if the estimated accuracy is significantly better than chance level. This method of signal detection is particularly popular in neuroimaging and genetics. We provide evidence that using a classifier’s accuracy as a test statistic can be an underpowered strategy for finding differences between populations, compared to a bona fide statistical test. It is also computationally more demanding than a statistical test. Via simulation, we compare test statistics that are based on classification accuracy, to others based on multivariate test statistics. We find that the probability of detecting differences between two distributions is lower for accuracy-based statistics. We examine several candidate causes for the low power of accuracy-tests. These causes include: the discrete nature of the accuracy-test statistic, the type of signal accuracy-tests are designed to detect, their inefficient use of the data, and their suboptimal regularization. When the purpose of the analysis is the evaluation of a particular classifier, not signal detection, we suggest several improvements to increase power. In particular, to replace V-fold cross-validation with the Leave-One-Out Bootstrap.},
  archive      = {J_BIOSTAT},
  author       = {Rosenblatt, Jonathan D and Benjamini, Yuval and Gilron, Roee and Mukamel, Roy and Goeman, Jelle J},
  doi          = {10.1093/biostatistics/kxz035},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {365-380},
  shortjournal = {Biostatistics},
  title        = {Better-than-chance classification for signal detection},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive penalization in high-dimensional regression and
classification with external covariates using variational bayes.
<em>BIOSTAT</em>, <em>22</em>(2), 348–364. (<a
href="https://doi.org/10.1093/biostatistics/kxz034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Penalization schemes like Lasso or ridge regression are routinely used to regress a response of interest on a high-dimensional set of potential predictors. Despite being decisive, the question of the relative strength of penalization is often glossed over and only implicitly determined by the scale of individual predictors. At the same time, additional information on the predictors is available in many applications but left unused. Here, we propose to make use of such external covariates to adapt the penalization in a data-driven manner. We present a method that differentially penalizes feature groups defined by the covariates and adapts the relative strength of penalization to the information content of each group. Using techniques from the Bayesian tool-set our procedure combines shrinkage with feature selection and provides a scalable optimization scheme. We demonstrate in simulations that the method accurately recovers the true effect sizes and sparsity patterns per feature group. Furthermore, it leads to an improved prediction performance in situations where the groups have strong differences in dynamic range. In applications to data from high-throughput biology, the method enables re-weighting the importance of feature groups from different assays. Overall, using available covariates extends the range of applications of penalized regression, improves model interpretability and can improve prediction performance.},
  archive      = {J_BIOSTAT},
  author       = {Velten, Britta and Huber, Wolfgang},
  doi          = {10.1093/biostatistics/kxz034},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {348-364},
  shortjournal = {Biostatistics},
  title        = {Adaptive penalization in high-dimensional regression and classification with external covariates using variational bayes},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive empirical pattern transformation (ADEPT) with
application to walking stride segmentation. <em>BIOSTAT</em>,
<em>22</em>(2), 331–347. (<a
href="https://doi.org/10.1093/biostatistics/kxz033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying gait parameters and ambulatory monitoring of changes in these parameters have become increasingly important in epidemiological and clinical studies. Using high-density accelerometry measurements, we propose adaptive empirical pattern transformation (ADEPT), a fast, scalable, and accurate method for segmentation of individual walking strides. ADEPT computes the covariance between a scaled and translated pattern function and the data, an idea similar to the continuous wavelet transform. The difference is that ADEPT uses a data-based pattern function, allows multiple pattern functions, can use other distances instead of the covariance, and the pattern function is not required to satisfy the wavelet admissibility condition. Compared to many existing approaches, ADEPT is designed to work with data collected at various body locations and is invariant to the direction of accelerometer axes relative to body orientation. The method is applied to and validated on accelerometry data collected during a |$450$| -m outdoor walk of |$32$| study participants wearing accelerometers on the wrist, hip, and both ankles. Additionally, all scripts and data needed to reproduce presented results are included in supplementary material available at Biostatistics online.},
  archive      = {J_BIOSTAT},
  author       = {Karas, Marta and Stra̧czkiewicz, Marcin and Fadel, William and Harezlak, Jaroslaw and Crainiceanu, Ciprian M and Urbanek, Jacek K},
  doi          = {10.1093/biostatistics/kxz033},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {331-347},
  shortjournal = {Biostatistics},
  title        = {Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Copula-based semiparametric regression method for bivariate
data under general interval censoring. <em>BIOSTAT</em>, <em>22</em>(2),
315–330. (<a
href="https://doi.org/10.1093/biostatistics/kxz032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research is motivated by discovering and underpinning genetic causes for the progression of a bilateral eye disease, age-related macular degeneration (AMD), of which the primary outcomes, progression times to late-AMD, are bivariate and interval-censored due to intermittent assessment times. We propose a novel class of copula-based semiparametric transformation models for bivariate data under general interval censoring, which includes the case 1 interval censoring (current status data) and case 2 interval censoring. Specifically, the joint likelihood is modeled through a two-parameter Archimedean copula, which can flexibly characterize the dependence between the two margins in both tails. The marginal distributions are modeled through semiparametric transformation models using sieves, with the proportional hazards or odds model being a special case. We develop a computationally efficient sieve maximum likelihood estimation procedure for the unknown parameters, together with a generalized score test for the regression parameter(s). For the proposed sieve estimators of finite-dimensional parameters, we establish their asymptotic normality and efficiency. Extensive simulations are conducted to evaluate the performance of the proposed method in finite samples. Finally, we apply our method to a genome-wide analysis of AMD progression using the Age-Related Eye Disease Study data, to successfully identify novel risk variants associated with the disease progression. We also produce predicted joint and conditional progression-free probabilities, for patients with different genetic characteristics.},
  archive      = {J_BIOSTAT},
  author       = {Sun, Tao and Ding, Ying},
  doi          = {10.1093/biostatistics/kxz032},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {315-330},
  shortjournal = {Biostatistics},
  title        = {Copula-based semiparametric regression method for bivariate data under general interval censoring},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strategies for validating biomarkers using data from a
reference set. <em>BIOSTAT</em>, <em>22</em>(2), 298–314. (<a
href="https://doi.org/10.1093/biostatistics/kxz031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Candidate biomarkers discovered in the laboratory need to be rigorously validated before advancing to clinical application. However, it is often expensive and time-consuming to collect the high quality specimens needed for validation; moreover, such specimens are often limited in volume. The Early Detection Research Network has developed valuable specimen reference sets that can be used by multiple labs for biomarker validation. To optimize the chance of successful validation, it is critical to efficiently utilize the limited specimens in these reference sets on promising candidate biomarkers. Towards this end, we propose a novel two-stage validation strategy that partitions the samples in the reference set into two groups for sequential validation. The proposed strategy adopts the group sequential testing method to control for the type I error rate and rotates group membership to maximize the usage of available samples. We develop analytical formulas for performance parameters of this strategy in terms of the expected numbers of biomarkers that can be evaluated and the truly useful biomarkers that can be successfully validated, which can provide valuable guidance for future study design. The performance of our proposed strategy for validating biomarkers with respect to the points on the receiver operating characteristic curve are evaluated via extensive simulation studies and compared with the default strategy of validating each biomarker using all samples in the reference set. Different types of early stopping rules and boundary shapes in the group sequential testing method are considered. Compared with the default strategy, our proposed strategy makes more efficient use of the limited resources in the reference set by allowing more candidate biomarkers to be evaluated, giving a better chance of having truly useful biomarkers successfully validated.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Lu and Huang, Ying and Feng, Ziding},
  doi          = {10.1093/biostatistics/kxz031},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {298-314},
  shortjournal = {Biostatistics},
  title        = {Strategies for validating biomarkers using data from a reference set},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimized adaptive enrichment designs for three-arm trials:
Learning which subpopulations benefit from different treatments.
<em>BIOSTAT</em>, <em>22</em>(2), 283–297. (<a
href="https://doi.org/10.1093/biostatistics/kxz030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of designing a confirmatory randomized trial for comparing two treatments versus a common control in two disjoint subpopulations. The subpopulations could be defined in terms of a biomarker or disease severity measured at baseline. The goal is to determine which treatments benefit which subpopulations. We develop a new class of adaptive enrichment designs tailored to solving this problem. Adaptive enrichment designs involve a preplanned rule for modifying enrollment based on accruing data in an ongoing trial. At the interim analysis after each stage, for each subpopulation, the preplanned rule may decide to stop enrollment or to stop randomizing participants to one or more study arms. The motivation for this adaptive feature is that interim data may indicate that a subpopulation, such as those with lower disease severity at baseline, is unlikely to benefit from a particular treatment while uncertainty remains for the other treatment and/or subpopulation. We optimize these adaptive designs to have the minimum expected sample size under power and Type I error constraints. We compare the performance of the optimized adaptive design versus an optimized nonadaptive (single stage) design. Our approach is demonstrated in simulation studies that mimic features of a completed trial of a medical device for treating heart failure. The optimized adaptive design has |$25\%$| smaller expected sample size compared to the optimized nonadaptive design; however, the cost is that the optimized adaptive design has |$8\%$| greater maximum sample size. Open-source software that implements the trial design optimization is provided, allowing users to investigate the tradeoffs in using the proposed adaptive versus standard designs.},
  archive      = {J_BIOSTAT},
  author       = {Steingrimsson, Jon Arni and Betz, Joshua and Qian, Tianchen and Rosenblum, Michael},
  doi          = {10.1093/biostatistics/kxz030},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {283-297},
  shortjournal = {Biostatistics},
  title        = {Optimized adaptive enrichment designs for three-arm trials: Learning which subpopulations benefit from different treatments},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general semiparametric bayesian discrete-time recurrent
events model. <em>BIOSTAT</em>, <em>22</em>(2), 266–282. (<a
href="https://doi.org/10.1093/biostatistics/kxz029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event time variables are often recorded in a discrete fashion, especially in the case of patient-reported outcomes. This work is motivated by a study of illicit drug users, in which time to drug use cessation has been recorded as a number of whole months. Existing approaches for handling such discrete data include treating the survival times as continuous (with adjustments for inevitable tied outcomes), or using discrete models that omit important features like random effects. We provide a general Bayesian discrete-time proportional hazards model, incorporating a number of features popular in continuous-time models such as competing risks and frailties. Our model also provides flexible baseline hazards for time effects, as well as generalized additive models style semiparametric incorporation of other time-varying covariates. Our specific modeling choices enable efficient Markov chain Monte Carlo inference algorithms, which we provide to the user in the form of a freely available R package called |$\texttt{brea}$|⁠ . We demonstrate that our model performs better on our motivating substance abuse application than existing approaches. We also present a reproducible application of the |$\texttt{brea}$| software to a freely available data set from a clinical trial of anesthesia administration methods.},
  archive      = {J_BIOSTAT},
  author       = {King, Adam J and Weiss, Robert E},
  doi          = {10.1093/biostatistics/kxz029},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {266-282},
  shortjournal = {Biostatistics},
  title        = {A general semiparametric bayesian discrete-time recurrent events model},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gamma models for estimating the odds ratio for a skewed
biomarker measured in pools and subject to errors. <em>BIOSTAT</em>,
<em>22</em>(2), 250–265. (<a
href="https://doi.org/10.1093/biostatistics/kxz028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring a biomarker in pooled samples from multiple cases or controls can lead to cost-effective estimation of a covariate-adjusted odds ratio, particularly for expensive assays. But pooled measurements may be affected by assay-related measurement error (ME) and/or pooling-related processing error (PE), which can induce bias if ignored. Building on recently developed methods for a normal biomarker subject to additive errors, we present two related estimators for a right-skewed biomarker subject to multiplicative errors: one based on logistic regression and the other based on a Gamma discriminant function model. Applied to a reproductive health dataset with a right-skewed cytokine measured in pools of size 1 and 2, both methods suggest no association with spontaneous abortion. The fitted models indicate little ME but fairly severe PE, the latter of which is much too large to ignore. Simulations mimicking these data with a non-unity odds ratio confirm validity of the estimators and illustrate how PE can detract from pooling-related gains in statistical efficiency. These methods address a key issue associated with the homogeneous pools study design and should facilitate valid odds ratio estimation at a lower cost in a wide range of scenarios.},
  archive      = {J_BIOSTAT},
  author       = {Van Domelen, Dane R and Mitchell, Emily M and Perkins, Neil J and Schisterman, Enrique F and Manatunga, Amita K and Huang, Yijian and Lyles, Robert H},
  doi          = {10.1093/biostatistics/kxz028},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {250-265},
  shortjournal = {Biostatistics},
  title        = {Gamma models for estimating the odds ratio for a skewed biomarker measured in pools and subject to errors},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast hybrid bayesian integrative learning of multiple gene
regulatory networks for type 1 diabetes. <em>BIOSTAT</em>,
<em>22</em>(2), 233–249. (<a
href="https://doi.org/10.1093/biostatistics/kxz027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the study of the molecular mechanism underlying type 1 diabetes with gene expression data collected from both patients and healthy controls at multiple time points, we propose a hybrid Bayesian method for jointly estimating multiple dependent Gaussian graphical models with data observed under distinct conditions, which avoids inversion of high-dimensional covariance matrices and thus can be executed very fast. We prove the consistency of the proposed method under mild conditions. The numerical results indicate the superiority of the proposed method over existing ones in both estimation accuracy and computational efficiency. Extension of the proposed method to joint estimation of multiple mixed graphical models is straightforward.},
  archive      = {J_BIOSTAT},
  author       = {Jia, Bochao and Liang, Faming and The TEDDY Study Group},
  doi          = {10.1093/biostatistics/kxz027},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {233-249},
  shortjournal = {Biostatistics},
  title        = {Fast hybrid bayesian integrative learning of multiple gene regulatory networks for type 1 diabetes},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On restricted optimal treatment regime estimation for
competing risks data. <em>BIOSTAT</em>, <em>22</em>(2), 217–232. (<a
href="https://doi.org/10.1093/biostatistics/kxz026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well accepted that individualized treatment regimes may improve the clinical outcomes of interest. However, positive treatment effects are often accompanied by certain side effects. Therefore, when choosing the optimal treatment regime for a patient, we need to consider both efficacy and safety issues. In this article, we propose to model time to a primary event of interest and time to severe side effects of treatment by a competing risks model and define a restricted optimal treatment regime based on cumulative incidence functions. The estimation approach is derived using a penalized value search method and investigated through extensive simulations. The proposed method is applied to an HIV dataset obtained from Health Sciences South Carolina, where we minimize the risk of treatment or virologic failures while controlling the risk of serious drug-induced side effects.},
  archive      = {J_BIOSTAT},
  author       = {Zhou, Jie and Zhang, Jiajia and Lu, Wenbin and Li, Xiaoming},
  doi          = {10.1093/biostatistics/kxz026},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {217-232},
  shortjournal = {Biostatistics},
  title        = {On restricted optimal treatment regime estimation for competing risks data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biostatistics reviewer list 2020. <em>BIOSTAT</em>,
<em>22</em>(1), 214–215. (<a
href="https://doi.org/10.1093/biostatistics/kxaa050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxaa050},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {214-215},
  shortjournal = {Biostatistics},
  title        = {Biostatistics reviewer list 2020},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ensemble method for interval-censored time-to-event data.
<em>BIOSTAT</em>, <em>22</em>(1), 198–213. (<a
href="https://doi.org/10.1093/biostatistics/kxz025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored data analysis is important in biomedical statistics for any type of time-to-event response where the time of response is not known exactly, but rather only known to occur between two assessment times. Many clinical trials and longitudinal studies generate interval-censored data; one common example occurs in medical studies that entail periodic follow-up. In this article, we propose a survival forest method for interval-censored data based on the conditional inference framework. We describe how this framework can be adapted to the situation of interval-censored data. We show that the tuning parameters have a non-negligible effect on the survival forest performance and guidance is provided on how to tune the parameters in a data-dependent way to improve the overall performance of the method. Using Monte Carlo simulations, we find that the proposed survival forest is at least as effective as a survival tree method when the underlying model has a tree structure, performs similarly to an interval-censored Cox proportional hazards model fit when the true relationship is linear, and outperforms the survival tree method and Cox model when the true relationship is nonlinear. We illustrate the application of the method on a tooth emergence data set.},
  archive      = {J_BIOSTAT},
  author       = {Yao, Weichi and Frydman, Halina and Simonoff, Jeffrey S},
  doi          = {10.1093/biostatistics/kxz025},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {198-213},
  shortjournal = {Biostatistics},
  title        = {An ensemble method for interval-censored time-to-event data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selection-adjusted inference: An application to confidence
intervals for cis-eQTL effect sizes. <em>BIOSTAT</em>, <em>22</em>(1),
181–197. (<a
href="https://doi.org/10.1093/biostatistics/kxz024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of expression quantitative trait loci (eQTL) studies is to identify the genetic variants that influence the expression levels of the genes in an organism. High throughput technology has made such studies possible: in a given tissue sample, it enables us to quantify the expression levels of approximately 20 000 genes and to record the alleles present at millions of genetic polymorphisms. While obtaining this data is relatively cheap once a specimen is at hand, obtaining human tissue remains a costly endeavor: eQTL studies continue to be based on relatively small sample sizes, with this limitation particularly serious for tissues as brain, liver, etc.—often the organs of most immediate medical relevance. Given the high-dimensional nature of these datasets and the large number of hypotheses tested, the scientific community has adopted early on multiplicity adjustment procedures. These testing procedures primarily control the false discoveries rate for the identification of genetic variants with influence on the expression levels. In contrast, a problem that has not received much attention to date is that of providing estimates of the effect sizes associated with these variants, in a way that accounts for the considerable amount of selection. Yet, given the difficulty of procuring additional samples, this challenge is of practical importance. We illustrate in this work how the recently developed conditional inference approach can be deployed to obtain confidence intervals for the eQTL effect sizes with reliable coverage. The procedure we propose is based on a randomized hierarchical strategy with a 2-fold contribution: (1) it reflects the selection steps typically adopted in state of the art investigations and (2) it introduces the use of randomness instead of data-splitting to maximize the use of available data. Analysis of the GTEx Liver dataset (v6) suggests that naively obtained confidence intervals would likely not cover the true values of effect sizes and that the number of local genetic polymorphisms influencing the expression level of genes might be underestimated.},
  archive      = {J_BIOSTAT},
  author       = {Panigrahi, Snigdha and Zhu, Junjie and Sabatti, Chiara},
  doi          = {10.1093/biostatistics/kxz024},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {181-197},
  shortjournal = {Biostatistics},
  title        = {Selection-adjusted inference: An application to confidence intervals for cis-eQTL effect sizes},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian process regression for survival time prediction
with genome-wide gene expression. <em>BIOSTAT</em>, <em>22</em>(1),
164–180. (<a
href="https://doi.org/10.1093/biostatistics/kxz023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the survival time of a cancer patient based on his/her genome-wide gene expression remains a challenging problem. For certain types of cancer, the effects of gene expression on survival are both weak and abundant, so identifying non-zero effects with reasonable accuracy is difficult. As an alternative to methods that use variable selection, we propose a Gaussian process accelerated failure time model to predict survival time using genome-wide or pathway-wide gene expression data. Using a Monte Carlo expectation–maximization algorithm, we jointly impute censored log-survival time and estimate model parameters. We demonstrate the performance of our method and its advantage over existing methods in both simulations and real data analysis. The real data that we analyze were collected from 513 patients with kidney renal clear cell carcinoma and include survival time, demographic/clinical variables, and expression of more than 20 000 genes. In addition to the right-censored survival time, our method can also accommodate left-censored or interval-censored outcomes; and it provides a natural way to combine multiple types of high-dimensional -omics data. An R package implementing our method is available in the Supplementary material available at Biostatistics online.},
  archive      = {J_BIOSTAT},
  author       = {Molstad, Aaron J and Hsu, Li and Sun, Wei},
  doi          = {10.1093/biostatistics/kxz023},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {164-180},
  shortjournal = {Biostatistics},
  title        = {Gaussian process regression for survival time prediction with genome-wide gene expression},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling local dependence in latent vector autoregressive
models. <em>BIOSTAT</em>, <em>22</em>(1), 148–163. (<a
href="https://doi.org/10.1093/biostatistics/kxz021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian latent vector autoregressive (LVAR) model to analyze multivariate longitudinal data of binary and ordinal variables (items) as a function of a small number of continuous latent variables. We focus on the evolution of the latent variables while taking into account the correlation structure of the responses. Often local independence is assumed in this context. Local independence implies that, given the latent variables, the responses are assumed mutually independent cross-sectionally and longitudinally. However, in practice conditioning on the latent variables may not remove the dependence of the responses. We address local dependence by further conditioning on item-specific random effects. A simulation study shows that wrongly assuming local independence may give biased estimates for the regression coefficients of the LVAR process as well as the item-specific parameters. Novel features of our proposal include (i) correcting biased estimates of the model parameters, especially the regression coefficients of the LVAR process, obtained when local dependence is ignored and (ii) measuring the magnitude of local dependence. We applied our model on data obtained from a registry on the elderly population in Belgium. The purpose was to examine the values of oral health information on top of general health information.},
  archive      = {J_BIOSTAT},
  author       = {Tran, Trung Dung and Lesaffre, Emmanuel and Verbeke, Geert and Duyck, Joke},
  doi          = {10.1093/biostatistics/kxz021},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {148-163},
  shortjournal = {Biostatistics},
  title        = {Modeling local dependence in latent vector autoregressive models},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ACE of space: Estimating genetic components of
high-dimensional imaging data. <em>BIOSTAT</em>, <em>22</em>(1),
131–147. (<a
href="https://doi.org/10.1093/biostatistics/kxz022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of great interest to quantify the contributions of genetic variation to brain structure and function, which are usually measured by high-dimensional imaging data (e.g., magnetic resonance imaging). In addition to the variance, the covariance patterns in the genetic effects of a functional phenotype are of biological importance, and covariance patterns have been linked to psychiatric disorders. The aim of this article is to develop a scalable method to estimate heritability and the nonstationary covariance components in high-dimensional imaging data from twin studies. Our motivating example is from the Human Connectome Project (HCP). Several major big-data challenges arise from estimating the genetic and environmental covariance functions of functional phenotypes extracted from imaging data, such as cortical thickness with 60 000 vertices. Notably, truncating to positive eigenvalues and their eigenfunctions from unconstrained estimators can result in large bias. This motivated our development of a novel estimator ensuring positive semidefiniteness. Simulation studies demonstrate large improvements over existing approaches, both with respect to heritability estimates and covariance estimation. We applied the proposed method to cortical thickness data from the HCP. Our analysis suggests fine-scale differences in covariance patterns, identifying locations in which genetic control is correlated with large areas of the brain and locations where it is highly localized.},
  archive      = {J_BIOSTAT},
  author       = {Risk, Benjamin B and Zhu, Hongtu},
  doi          = {10.1093/biostatistics/kxz022},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {131-147},
  shortjournal = {Biostatistics},
  title        = {ACE of space: Estimating genetic components of high-dimensional imaging data},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified method for improved inference in random effects
meta-analysis. <em>BIOSTAT</em>, <em>22</em>(1), 114–130. (<a
href="https://doi.org/10.1093/biostatistics/kxz020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random effects meta-analyses have been widely applied in evidence synthesis for various types of medical studies. However, standard inference methods (e.g. restricted maximum likelihood estimation) usually underestimate statistical errors and possibly provide highly overconfident results under realistic situations; for instance, coverage probabilities of confidence intervals can be substantially below the nominal level. The main reason is that these inference methods rely on large sample approximations even though the number of synthesized studies is usually small or moderate in practice. In this article, we solve this problem using a unified inference method based on Monte Carlo conditioning for broad application to random effects meta-analysis. The developed method provides improved confidence intervals with coverage probabilities that are closer to the nominal level than standard methods. As specific applications, we provide new inference procedures for three types of meta-analysis: conventional univariate meta-analysis for pairwise treatment comparisons, meta-analysis of diagnostic test accuracy, and multiple treatment comparisons via network meta-analysis. We also illustrate the practical effectiveness of these methods via real data applications and simulation studies.},
  archive      = {J_BIOSTAT},
  author       = {Sugasawa, Shonosuke and Noma, Hisashi},
  doi          = {10.1093/biostatistics/kxz020},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {114-130},
  shortjournal = {Biostatistics},
  title        = {A unified method for improved inference in random effects meta-analysis},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the individual surrogate paradox. <em>BIOSTAT</em>,
<em>22</em>(1), 97–113. (<a
href="https://doi.org/10.1093/biostatistics/kxz019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the primary outcome is difficult to collect, a surrogate endpoint is typically used as a substitute. It is possible that for every individual, the treatment has a positive effect on the surrogate, and the surrogate has a positive effect on the primary outcome, but for some individuals, the treatment has a negative effect on the primary outcome. For example, a treatment may be substantially effective in preventing the stroke for everyone, and lowering the risk of stroke is universally beneficial for life expectancy; however, the treatment may still cause death for some individuals. We define such paradoxical phenomenon as the individual surrogate paradox. The individual surrogate paradox is proposed to capture the treatment effect heterogeneity, which is unable to be described by either the surrogate paradox based on average causal effect or that based on distributional causal effect. We investigate the existing surrogate criteria in terms of whether the individual surrogate paradox could manifest. We find that only the strong binary surrogate can avoid such paradox without additional assumptions. Utilizing the sharp bounds, we propose novel criteria to exclude the individual surrogate paradox. Our methods are illustrated in an application to determine the effect of the intensive glycemia on the risk of development or progression of diabetic retinopathy.},
  archive      = {J_BIOSTAT},
  author       = {Ma, Linquan and Yin, Yunjian and Liu, Lan and Geng, Zhi},
  doi          = {10.1093/biostatistics/kxz019},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {97-113},
  shortjournal = {Biostatistics},
  title        = {On the individual surrogate paradox},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multivariate meta-analysis model for the difference in
restricted mean survival times. <em>BIOSTAT</em>, <em>22</em>(1), 82–96.
(<a href="https://doi.org/10.1093/biostatistics/kxz018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized controlled trials (RCTs) with time-to-event outcomes, the difference in restricted mean survival times (RMSTD) offers an absolute measure of the treatment effect on the time scale. Computation of the RMSTD relies on the choice of a time horizon, |$\tau$|⁠ . In a meta-analysis, varying follow-up durations may lead to the exclusion of RCTs with follow-up shorter than |$\tau$|⁠ . We introduce an individual patient data multivariate meta-analysis model for RMSTD estimated at multiple time horizons. We derived the within-trial covariance for the RMSTD enabling the synthesis of all data by borrowing strength from multiple time points. In a simulation study covering 60 scenarios, we compared the statistical performance of the proposed method to that of two univariate meta-analysis models, based on available data at each time point and based on predictions from flexible parametric models. Our multivariate model yields smaller mean squared error over univariate methods at all time points. We illustrate the method with a meta-analysis of five RCTs comparing transcatheter aortic valve replacement (TAVR) with surgical replacement in patients with aortic stenosis. Over 12, 24, and 36 months of follow-up, those treated by TAVR live 0.28 [95% confidence interval (CI) 0.01 to 0.56], 0.46 (95% CI |$-$| 0.08 to 1.01), and 0.79 (95% CI |$-$| 0.43 to 2.02) months longer on average compared to those treated by surgery, respectively.},
  archive      = {J_BIOSTAT},
  author       = {Weir, Isabelle R and Tian, Lu and Trinquart, Ludovic},
  doi          = {10.1093/biostatistics/kxz018},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {82-96},
  shortjournal = {Biostatistics},
  title        = {Multivariate meta-analysis model for the difference in restricted mean survival times},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The functional false discovery rate with applications to
genomics. <em>BIOSTAT</em>, <em>22</em>(1), 68–81. (<a
href="https://doi.org/10.1093/biostatistics/kxz010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The false discovery rate (FDR) measures the proportion of false discoveries among a set of hypothesis tests called significant. This quantity is typically estimated based on p -values or test statistics. In some scenarios, there is additional information available that may be used to more accurately estimate the FDR. We develop a new framework for formulating and estimating FDRs and q -values when an additional piece of information, which we call an “informative variable”, is available. For a given test, the informative variable provides information about the prior probability a null hypothesis is true or the power of that particular test. The FDR is then treated as a function of this informative variable. We consider two applications in genomics. Our first application is a genetics of gene expression (eQTL) experiment in yeast where every genetic marker and gene expression trait pair are tested for associations. The informative variable in this case is the distance between each genetic marker and gene. Our second application is to detect differentially expressed genes in an RNA-seq study carried out in mice. The informative variable in this study is the per-gene read depth. The framework we develop is quite general, and it should be useful in a broad range of scientific applications.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Xiongzhi and Robinson, David G and Storey, John D},
  doi          = {10.1093/biostatistics/kxz010},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {68-81},
  shortjournal = {Biostatistics},
  title        = {The functional false discovery rate with applications to genomics},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On models for the estimation of the excess mortality hazard
in case of insufficiently stratified life tables. <em>BIOSTAT</em>,
<em>22</em>(1), 51–67. (<a
href="https://doi.org/10.1093/biostatistics/kxz017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cancer epidemiology using population-based data, regression models for the excess mortality hazard is a useful method to estimate cancer survival and to describe the association between prognosis factors and excess mortality. This method requires expected mortality rates from general population life tables: each cancer patient is assigned an expected (background) mortality rate obtained from the life tables, typically at least according to their age and sex, from the population they belong to. However, those life tables may be insufficiently stratified, as some characteristics such as deprivation, ethnicity, and comorbidities, are not available in the life tables for a number of countries. This may affect the background mortality rate allocated to each patient, and it has been shown that not including relevant information for assigning an expected mortality rate to each patient induces a bias in the estimation of the regression parameters of the excess hazard model. We propose two parametric corrections in excess hazard regression models, including a single-parameter or a random effect (frailty), to account for possible mismatches in the life table and thus misspecification of the background mortality rate. In an extensive simulation study, the good statistical performance of the proposed approach is demonstrated, and we illustrate their use on real population-based data of lung cancer patients. We present conditions and limitations of these methods and provide some recommendations for their use in practice.},
  archive      = {J_BIOSTAT},
  author       = {Rubio, Francisco J and Rachet, Bernard and Giorgi, Roch and Maringe, Camille and Belot, Aurélien},
  doi          = {10.1093/biostatistics/kxz017},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {51-67},
  shortjournal = {Biostatistics},
  title        = {On models for the estimation of the excess mortality hazard in case of insufficiently stratified life tables},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MethylSeqDesign: A framework for methyl-seq genome-wide
power calculation and study design issues. <em>BIOSTAT</em>,
<em>22</em>(1), 35–50. (<a
href="https://doi.org/10.1093/biostatistics/kxz016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bisulfite DNA methylation sequencing (Methyl-Seq) becomes one of the most important technologies to study methylation level difference at a genome-wide scale. Due to the complexity and large scale of methyl-Seq data, power calculation and study design method have not been developed. Here, we propose a “MethylSeqDesign” framework for power calculation and study design of Methyl-Seq experiments by utilizing information from pilot data. Differential methylation analysis is based on a beta-binomial model. Power calculation is achieved using mixture model fitting of p-values from pilot data and a parametric bootstrap procedure. To circumvent the issue of existing tens of millions of methylation sites, we focus on the inference of pre-specified targeted regions. The performance of the method was evaluated with simulations. Two real examples are analyzed to illustrate our method. An R package “MethylSeqDesign” to implement this method is publicly available.},
  archive      = {J_BIOSTAT},
  author       = {Liu, Peng and Lin, Chien-Wei and Park, Yongseok and Tseng, George},
  doi          = {10.1093/biostatistics/kxz016},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {35-50},
  shortjournal = {Biostatistics},
  title        = {MethylSeqDesign: A framework for methyl-seq genome-wide power calculation and study design issues},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian adaptive basket trial design using model averaging.
<em>BIOSTAT</em>, <em>22</em>(1), 19–34. (<a
href="https://doi.org/10.1093/biostatistics/kxz014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a Bayesian adaptive design methodology for oncology basket trials with binary endpoints using a Bayesian model averaging framework. Most existing methods seek to borrow information based on the degree of homogeneity of estimated response rates across all baskets. In reality, an investigational product may only demonstrate activity for a subset of baskets, and the degree of activity may vary across the subset. A key benefit of our Bayesian model averaging approach is that it explicitly accounts for the possibility that any subset of baskets may have similar activity and that some may not. Our proposed approach performs inference on the basket-specific response rates by averaging over the complete model space for the response rates, which can include thousands of models. We present results that demonstrate that this computationally feasible Bayesian approach performs favorably compared to existing state-of-the-art approaches, even when held to stringent requirements regarding false positive rates.},
  archive      = {J_BIOSTAT},
  author       = {Psioda, Matthew A and Xu, Jiawei and Jiang, Qi and Ke, Chunlei and Yang, Zhao and Ibrahim, Joseph G},
  doi          = {10.1093/biostatistics/kxz014},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {19-34},
  shortjournal = {Biostatistics},
  title        = {Bayesian adaptive basket trial design using model averaging},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matched case–control data with a misclassified exposure:
What can be done with instrumental variables? <em>BIOSTAT</em>,
<em>22</em>(1), 1–18. (<a
href="https://doi.org/10.1093/biostatistics/kxz012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matched case–control studies are used for finding the association between a disease and an exposure after controlling the effect of important confounding variables. It is a known fact that the disease–exposure association parameter estimators are biased when the exposure is misclassified, and a matched case–control study is of no exception. Any bias correction method relies on validation data that contain the true exposure and the misclassified exposure value, and in turn the validation data help to estimate the misclassification probabilities. The question is what we can do when there are no validation data and no prior knowledge on the misclassification probabilities, but some instrumental variables are observed. To answer this unexplored and unanswered question, we propose two methods of reducing the exposure misclassification bias in the analysis of a matched case–control data when instrumental variables are measured for each subject of the study. The significance of these approaches is that the proposed methods are designed to work without any validation data that often are not available when the true exposure is impossible or too costly to measure. A simulation study explores different types of instrumental variable scenarios and investigates when the proposed methods work, and how much bias can be reduced. For the purpose of illustration, we apply the methods to a nested case–control data sampled from the 1989 US birth registry.},
  archive      = {J_BIOSTAT},
  author       = {Manuel, Christopher M and Sinha, Samiran and Wang, Suojin},
  doi          = {10.1093/biostatistics/kxz012},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Biostatistics},
  title        = {Matched case–control data with a misclassified exposure: What can be done with instrumental variables?},
  volume       = {22},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
