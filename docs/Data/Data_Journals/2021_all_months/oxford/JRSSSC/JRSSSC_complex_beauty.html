<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssc---70">JRSSSC - 70</h2>
<ul>
<li><details>
<summary>
(2021). Contents of volume 70, 2021. <em>JRSSSC</em>,
<em>70</em>(5), 1414–1416. (<a
href="https://doi.org/10.1111/rssc.12534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  doi          = {10.1111/rssc.12534},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1414-1416},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Contents of volume 70, 2021},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corrigendum. <em>JRSSSC</em>, <em>70</em>(5), 1413. (<a
href="https://doi.org/10.1111/rssc.12526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  doi          = {10.1111/rssc.12526},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1413},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Corrigendum},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian sparse mediation analysis with targeted
penalization of natural indirect effects. <em>JRSSSC</em>,
<em>70</em>(5), 1391–1412. (<a
href="https://doi.org/10.1111/rssc.12518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis aims to characterize an exposure&#39;s effect on an outcome and quantify the indirect effect that acts through a given mediator or a group of mediators of interest. With the increasing availability of measurements on a large number of potential mediators, like the epigenome or the microbiome, new statistical methods are needed to simultaneously accommodate high-dimensional mediators while directly target penalization of the natural indirect effect (NIE) for active mediator identification. Here, we develop two novel prior models for identification of active mediators in high-dimensional mediation analysis through penalizing NIEs in a Bayesian paradigm. Both methods specify a joint prior distribution on the exposure-mediator effect and mediator-outcome effect with either (a) a four-component Gaussian mixture prior or (b) a product threshold Gaussian prior. By jointly modelling the two parameters that contribute to the NIE, the proposed methods enable penalization on their product in a targeted way. Resultant inference can take into account the four-component composite structure underlying the NIE. We show through simulations that the proposed methods improve both selection and estimation accuracy compared to other competing methods. We applied our methods for an in-depth analysis of two ongoing epidemiologic studies: the Multi-Ethnic Study of Atherosclerosis (MESA) and the LIFECODES birth cohort. The identified active mediators in both studies reveal important biological pathways for understanding disease mechanisms.},
  archive      = {J_JRSSSC},
  author       = {Song, Yanyi and Zhou, Xiang and Kang, Jian and Aung, Max T. and Zhang, Min and Zhao, Wei and Needham, Belinda L. and Kardia, Sharon L. R. and Liu, Yongmei and Meeker, John D. and Smith, Jennifer A. and Mukherjee, Bhramar},
  doi          = {10.1111/rssc.12518},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1391-1412},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian sparse mediation analysis with targeted penalization of natural indirect effects},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond unidimensional poverty analysis using distributional
copula models for mixed ordered-continuous outcomes. <em>JRSSSC</em>,
<em>70</em>(5), 1365–1390. (<a
href="https://doi.org/10.1111/rssc.12517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poverty is a multidimensional concept often comprising a monetary outcome and other welfare dimensions such as education, subjective well-being or health that are measured on an ordinal scale. In applied research, multidimensional poverty is ubiquitously assessed by studying each poverty dimension independently in univariate regression models or by combining several poverty dimensions into a scalar index. This approach inhibits a thorough analysis of the potentially varying interdependence between the poverty dimensions. We propose a multivariate copula generalized additive model for location, scale and shape (copula GAMLSS or distributional copula model) to tackle this challenge. By relating the copula parameter to covariates, we specifically examine if certain factors determine the dependence between poverty dimensions. Furthermore, specifying the full conditional bivariate distribution allows us to derive several features such as poverty risks and dependence measures coherently from one model for different individuals. We demonstrate the approach by studying two important poverty dimensions: income and education. Since the level of education is measured on an ordinal scale while income is continuous, we extend the bivariate copula GAMLSS to the case of mixed ordered-continuous outcomes. The new model is integrated into the GJRM package in R and applied to data from Indonesia. Particular emphasis is given to the spatial variation of the income–education dependence and groups of individuals at risk of being simultaneously poor in both education and income dimensions.},
  archive      = {J_JRSSSC},
  author       = {Hohberg, Maike and Donat, Francesco and Marra, Giampiero and Kneib, Thomas},
  doi          = {10.1111/rssc.12517},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1365-1390},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Beyond unidimensional poverty analysis using distributional copula models for mixed ordered-continuous outcomes},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reducing the number of experiments required for modelling
the hydrocracking process with kriging through bayesian transfer
learning. <em>JRSSSC</em>, <em>70</em>(5), 1344–1364. (<a
href="https://doi.org/10.1111/rssc.12516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective is to improve the learning of a regression model of the hydrocracking process using a reduced number of observations. When a new catalyst is used for the hydrocracking process, a new model must be fitted. Generating new data is expensive and therefore it is advantageous to limit the amount of new data generation. Our idea is to use a second data set of measurements made on a process using an old catalyst. This second data set is large enough to fit performing models for the old catalyst. In this work, we use the knowledge from this old catalyst to learn a model on the new catalyst. This task is a transfer learning task. We show that the results are greatly improved with a Bayesian approach to transfer linear model and kriging model.},
  archive      = {J_JRSSSC},
  author       = {Iapteff, Loïc and Jacques, Julien and Rolland, Matthieu and Celse, Benoit},
  doi          = {10.1111/rssc.12516},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1344-1364},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Reducing the number of experiments required for modelling the hydrocracking process with kriging through bayesian transfer learning},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian nonparametric analysis of the 2003 outbreak of
highly pathogenic avian influenza in the netherlands. <em>JRSSSC</em>,
<em>70</em>(5), 1323–1343. (<a
href="https://doi.org/10.1111/rssc.12515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infectious diseases on farms pose both public and animal health risks, so understanding how they spread between farms is crucial for developing disease control strategies to prevent future outbreaks. We develop novel Bayesian nonparametric methodology to fit spatial stochastic transmission models in which the infection rate between any two farms is a function that depends on the distance between them, but without assuming a specified parametric form. Making nonparametric inference in this context is challenging since the likelihood function of the observed data is intractable because the underlying transmission process is unobserved. We adopt a fully Bayesian approach by assigning a transformed Gaussian process prior distribution to the infection rate function, and then develop an efficient data augmentation Markov Chain Monte Carlo algorithm to perform Bayesian inference. We use the posterior predictive distribution to simulate the effect of different disease control methods and their economic impact. We analyse a large outbreak of avian influenza in the Netherlands and infer the between-farm infection rate, as well as the unknown infection status of farms which were pre-emptively culled. We use our results to analyse ring-culling strategies, and conclude that although effective, ring-culling has limited impact in high-density areas.},
  archive      = {J_JRSSSC},
  author       = {Seymour, Rowland G. and Kypraios, Theodore and O’Neill, Philip D. and Hagenaars, Thomas J.},
  doi          = {10.1111/rssc.12515},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1323-1343},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian nonparametric analysis of the 2003 outbreak of highly pathogenic avian influenza in the netherlands},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fused graphical lasso for brain networks with symmetries.
<em>JRSSSC</em>, <em>70</em>(5), 1299–1322. (<a
href="https://doi.org/10.1111/rssc.12514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroimaging is the growing area of neuroscience devoted to produce data with the goal of capturing processes and dynamics of the human brain. We consider the problem of inferring the brain connectivity network from time-dependent functional magnetic resonance imaging (fMRI) scans. To this aim we propose the symmetric graphical lasso, a penalized likelihood method with a fused type penalty function that takes into explicit account the natural symmetrical structure of the brain. Symmetric graphical lasso allows one to learn simultaneously both the network structure and a set of symmetries across the two hemispheres. We implement an alternating directions method of multipliers algorithm to solve the corresponding convex optimization problem. Furthermore, we apply our methods to estimate the brain networks of two subjects, one healthy and one affected by mental disorder, and to compare them with respect to their symmetric structure. The method applies once the temporal dependence characterizing fMRI data have been accounted for and we compare the impact on the analysis of different detrending techniques on the estimated brain networks. Although we focus on brain networks, symmetric graphical lasso is a tool which can be more generally applied to learn multiple networks in a context of dependent samples.},
  archive      = {J_JRSSSC},
  author       = {Ranciati, Saverio and Roverato, Alberto and Luati, Alessandra},
  doi          = {10.1111/rssc.12514},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1299-1322},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Fused graphical lasso for brain networks with symmetries},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Response-adaptive rerandomization. <em>JRSSSC</em>,
<em>70</em>(5), 1281–1298. (<a
href="https://doi.org/10.1111/rssc.12513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rerandomization has recently attracted more attention in the literature randomized experiments. It leverages covariate information of participants to achieve a well-balanced allocation, and thus improves the efficiency of inference. However, by only considering covariate information, it may lead to potential ethical issues in clinical trials as a large number of patients might be assigned to the inferior treatment arm. To mitigate this issue, we propose a response-adaptive rerandomization scheme by incorporating response information for two-arm comparative clinical trials. Not only is our method applicable to both continuous and binary outcomes, but it also demonstrates desirable statistical and ethical properties. Extensive simulation studies are performed to illustrate the practicality and superiority of our approach.},
  archive      = {J_JRSSSC},
  author       = {Zhang, Hengtao and Yin, Guosheng},
  doi          = {10.1111/rssc.12513},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1281-1298},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Response-adaptive rerandomization},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic historical data borrowing using weighted average.
<em>JRSSSC</em>, <em>70</em>(5), 1259–1280. (<a
href="https://doi.org/10.1111/rssc.12512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many clinical trials, especially trials in rare diseases or a certain population like paediatric, it is of great interest to incorporate historical data to increase power of evaluating the treatment effect of an experimental drug. In practice, historical data and current data may not be congruent, and borrowing historical data is often associated with bias and Type-1 error rate inflation. It remains a challenge for historical data borrowing methods to control Type-1 error rate inflation at an adequate level and maintain sufficient power at the same time. To address this issue, dynamic historical borrowing methods can borrow historical data more when historical data are similar to current data and less otherwise. This paper proposed to use a weighted average of historical and current control data, with the weight being set as an approximation to the optimal weight that minimizes the mean-squared errors in the treatment effect estimation. Comparing to selected existing methods, the proposed method showed reduced bias, robust gain in power and better control in Type-1 error rate inflation through simulation studies. The proposed method enables the utilization of all possible historical data in the public domain and is readily used by skipping the need for external expert input in some existing approaches.},
  archive      = {J_JRSSSC},
  author       = {Chu, Chenghao and Yi, Bingming},
  doi          = {10.1111/rssc.12512},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1259-1280},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Dynamic historical data borrowing using weighted average},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accounting for missing actors in interaction network
inference from abundance data. <em>JRSSSC</em>, <em>70</em>(5),
1230–1258. (<a href="https://doi.org/10.1111/rssc.12509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network inference aims at unravelling the dependency structure relating jointly observed variables. Graphical models provide a general framework to distinguish between marginal and conditional dependency. Unobserved variables ( missing actors ) may induce apparent conditional dependencies. In the context of count data, we introduce a mixture of Poisson log-normal distributions with tree-shaped graphical models, to recover the dependency structure, including missing actors. We design a variational EM algorithm and assess its performance on synthetic data. We demonstrate the ability of our approach to recover environmental drivers on two ecological data sets. The corresponding R package is available from github.com/Rmomal/nestor .},
  archive      = {J_JRSSSC},
  author       = {Momal, Raphaëlle and Robin, Stéphane and Ambroise, Christophe},
  doi          = {10.1111/rssc.12509},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1230-1258},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Accounting for missing actors in interaction network inference from abundance data},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian phase i/II design for cancer clinical trials
combining an immunotherapeutic agent with a chemotherapeutic agent.
<em>JRSSSC</em>, <em>70</em>(5), 1210–1229. (<a
href="https://doi.org/10.1111/rssc.12508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunotherapy is an innovative treatment approach that harnesses a patient’s immune system to treat cancer. It has provided an alternative and complementary treatment modality to conventional chemotherapy. Combining immunotherapy with cytotoxic chemotherapy agent has become the leading trend and the most active research field in oncology. To accommodate this growing trend, we propose a Bayesian phase I/II dose-finding design to identify the optimal biological dose combination (OBDC), defined as the dose combination with the highest desirability in the risk-benefit trade-off. We propose new statistical models to describe the relationship between the doses and treatment outcomes, including immune response, toxicity and progression-free survival (PFS). During the trial, based on accrued data, we continuously update model estimates and adaptively assign patients to dose combinations with high desirability. The simulation study shows that our design has desirable operating characteristics.},
  archive      = {J_JRSSSC},
  author       = {Guo, Beibei and Garrett-Mayer, Elizabeth and Liu, Suyu},
  doi          = {10.1111/rssc.12508},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1210-1229},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian phase I/II design for cancer clinical trials combining an immunotherapeutic agent with a chemotherapeutic agent},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric model averaging prediction for lifetime data
via hazards regression. <em>JRSSSC</em>, <em>70</em>(5), 1187–1209. (<a
href="https://doi.org/10.1111/rssc.12502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting survival risks for time-to-event data is an essential task in clinical research. Practitioners often rely on well-structured statistical models to make predictions for patient survival outcomes. The nonparametric proportional hazards model, as an extension of the Cox proportional hazards model, involves an additive nonlinear combination of covariate effects for hazards regression and may be more flexible. When there are a large number of predictors, nonparametric smoothing for different variables cannot be simultaneously optimal using the conventional fitting program. To address such a limitation and still maintain the nonparametric flavour, we present a novel model averaging method to produce model-based prediction for survival outcome and our method automatically offers optimal smoothing for individual nonparametric functional estimation. The proposed semiparametric model averaging prediction (SMAP) method basically approximates the underlying unstructured nonparametric regression function by a weighted sum of low-dimensional nonparametric submodels. The weights are obtained from maximizing the partial likelihood constructed for the aggregated model. Theoretical properties are discussed for the estimated model weights. Simulation studies are conducted to examine the performance of SMAP under various evaluation criteria. Two real examples from genetic research studies motivated our work and are analysed by the proposed SMAP to produce new scientific findings.},
  archive      = {J_JRSSSC},
  author       = {Li, Jialiang and Yu, Tonghui and Lv, Jing and Lee, Mei-Ling Ting},
  doi          = {10.1111/rssc.12502},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1187-1209},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Semiparametric model averaging prediction for lifetime data via hazards regression},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the effect of health service delivery
interventions on patient length of stay: A bayesian survival analysis
approach. <em>JRSSSC</em>, <em>70</em>(5), 1164–1186. (<a
href="https://doi.org/10.1111/rssc.12501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health service delivery interventions include a range of hospital ‘quality improvement’ initiatives and broader health system policies. These interventions act through multiple causal pathways to affect patient outcomes and they present distinct challenges for evaluation. In this article, we propose an empirical approach to estimating the effect of service delivery interventions on patient length of stay considering three principle issues: (i) informative censoring of discharge times due to mortality; (ii) post-treatment selection bias if the intervention affects patient admission probabilities; and (iii) decomposition into direct and indirect pathways mediated by quality. We propose a Bayesian structural survival model framework in which results from a subsample in which required assumptions hold, including conditional independence of the intervention, can be applied to the whole sample. We evaluate a policy of increasing specialist intensity in hospitals at the weekend in England and Wales to inform a cost-minimisation analysis. Using data on adverse events from a case note review, we compare various specifications of a structural model that allows for observations of hospital quality. We find that the policy was not implemented as intended but would have likely been cost saving, that this conclusion is sensitive to model specification, and that the direct effect accounts for almost all of the total effect rather than any improvement in hospital quality.},
  archive      = {J_JRSSSC},
  author       = {Watson, Samuel I. and Lilford, Richard J. and Sun, Jianxia and Bion, Julian},
  doi          = {10.1111/rssc.12501},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1164-1186},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimating the effect of health service delivery interventions on patient length of stay: A bayesian survival analysis approach},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixed-frequency bayesian predictive synthesis for economic
nowcasting. <em>JRSSSC</em>, <em>70</em>(5), 1143–1163. (<a
href="https://doi.org/10.1111/rssc.12500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel framework for dynamic modelling of mixed-frequency data using Bayesian predictive synthesis. The proposed framework—unlike other mixed-frequency methods—considers data reported at different frequencies as latent factors, in the form of predictive distributions, which are dynamically synthesized and updated to produce coherent forecast distributions. Time-varying biases and interdependencies between data reported at different frequencies are learnt and effectively mapped onto easily interpretable parameters with associated uncertainty. Furthermore, the proposed framework allows for flexible methodological specifications based on policy goals and utility. A macroeconomic study of nowcasting two decades of quarterly US GDP using monthly macroeconomic and financial indicators is presented. In terms of both point and density forecasts, our proposed method significantly outperforms competing methods throughout the quarter, and is competitive with the aggregate Survey of Professional Forecasters. The study further shows that incorporating information during a quarter, and sequentially updating information throughout, markedly improves the performance, while providing timely insights that are useful for decision-making.},
  archive      = {J_JRSSSC},
  author       = {McAlinn, Kenichiro},
  doi          = {10.1111/rssc.12500},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1143-1163},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Mixed-frequency bayesian predictive synthesis for economic nowcasting},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Applied statistics. <em>JRSSSC</em>, <em>70</em>(5),
1141–1142. (<a href="https://doi.org/10.1111/rssc.12426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  doi          = {10.1111/rssc.12426},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1141-1142},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Applied statistics},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RNN-based counterfactual prediction, with an application to
homestead policy and public schooling. <em>JRSSSC</em>, <em>70</em>(4),
1124–1139. (<a href="https://doi.org/10.1111/rssc.12511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method for estimating the effect of a policy intervention on an outcome over time. We train recurrent neural networks (RNNs) on the history of control unit outcomes to learn a useful representation for predicting future outcomes. The learned representation of control units is then applied to the treated units for predicting counterfactual outcomes. RNNs are specifically structured to exploit temporal dependencies in panel data and are able to learn negative and non-linear interactions between control unit outcomes. We apply the method to the problem of estimating the long-run impact of US homestead policy on public school spending.},
  archive      = {J_JRSSSC},
  author       = {Poulos, Jason and Zeng, Shuxi},
  doi          = {10.1111/rssc.12511},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {1124-1139},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {RNN-based counterfactual prediction, with an application to homestead policy and public schooling},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The application of continuous-time markov chain models in
the analysis of choice flume experiments. <em>JRSSSC</em>,
<em>70</em>(4), 1103–1123. (<a
href="https://doi.org/10.1111/rssc.12510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An inhomogeneous continuous-time Markov chain model is proposed to quantify animal preference and avoidance behaviour in a choice experiment. We develop and apply our model to a choice flume experiment designed to assess the preference or avoidance responses of sea bass ( Dicentrarchus labrax ) exposed to chlorinated seawater. Due to observed fluctuations in chlorine levels, a stochastic process was applied to describe and account for uncertainty in chlorine concentrations. A hierarchical model was implemented to account for differences between eight experimental runs and use Bayesian methods to quantify preference/avoidance after accounting for observed shoaling behaviour. The application of our method not only overcomes the need to track individuals during an experiment but also circumvents temporal autocorrelation and any violations of independence. Our model therefore surpasses current methods in choice chamber studies, incorporating variability in the environment and group-level dynamics to yield results that scale and generalise to the real-world.},
  archive      = {J_JRSSSC},
  author       = {Spence, Michael A. and Muiruri, Evalyne W. and Maxwell, David L. and Davis, Scott and Sheahan, Dave},
  doi          = {10.1111/rssc.12510},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {1103-1123},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {The application of continuous-time markov chain models in the analysis of choice flume experiments},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pricing wind power futures. <em>JRSSSC</em>, <em>70</em>(4),
1083–1102. (<a href="https://doi.org/10.1111/rssc.12499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing wind power (WP) penetration an extensive amount of volatile and weather dependent energy is fed into the German electricity system. To manage the volume risk of windless days and the transfer of revenue risk from wind turbine owners to investors, WP derivatives were introduced. These insurance-like securities allow the hedging of the volume risk of unstable WP production on exchanges such as NASDAQ and EEX. We present a modern and powerful methodology to model weather derivatives, with very skewed underlying assets, incorporating techniques from extreme event modelling to tune seasonal volatility. We compare transformed Gaussian and non-Gaussian CARMA( p , q ) models. Our results indicate that the Gaussian CARMA( p , q ) model is preferred over the non-Gaussian alternative. Out-of-sample backtesting results show good performance, with respect to benchmarks, employing smooth market price of risk (MPR) estimates based on NASDAQ weekly and monthly German WP futures prices. A seasonal MPR of a smile shape is observed, with slightly positive values in times of high volatility, for example, winter months, and negative values, in times of low volatility and production, for example, in summer months. We conclude that producers pay premiums to insure stable revenue steams, while investors pay premiums when weather risk is high.},
  archive      = {J_JRSSSC},
  author       = {Härdle, Wolfgang Karl and López Cabrera, Brenda and Melzer, Awdesch},
  doi          = {10.1111/rssc.12499},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {1083-1102},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Pricing wind power futures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference about complex relationships using peak height data
from DNA mixtures. <em>JRSSSC</em>, <em>70</em>(4), 1049–1082. (<a
href="https://doi.org/10.1111/rssc.12498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In both criminal cases and civil cases, there is an increasing demand for the analysis of DNA mixtures involving relationships. The goal might be, for example, to identify the contributors to a DNA mixture where the donors may be related, or to infer the relationship between individuals based on a mixture. This paper introduces an approach to modelling and computation for DNA mixtures involving contributors with arbitrarily complex relationships. It builds on an extension of Jacquard&#39;s condensed coefficients of identity, to specify and compute with joint relationships, not only pairwise ones, including the possibility of inbreeding. The methodology developed is applied to two casework examples involving a missing person, and simulation studies of performance, in which the ability of the methodology to recover complex relationship information from synthetic data with known ‘true’ family structure is examined. The methods used to analyse the examples are implemented in the new KinMix R package that extends the DNAmixtures package to allow for modelling DNA mixtures with related contributors.},
  archive      = {J_JRSSSC},
  author       = {Green, Peter J. and Mortera, Julia},
  doi          = {10.1111/rssc.12498},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {1049-1082},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Inference about complex relationships using peak height data from DNA mixtures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing the reproducibility of microbiome measurements
based on concordance correlation coefficients. <em>JRSSSC</em>,
<em>70</em>(4), 1027–1048. (<a
href="https://doi.org/10.1111/rssc.12497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the reproducibility or agreement of microbiome measurements is often a crucial step to ensure rigorous downstream analyses in microbiome studies. In this paper, we address this need by developing adaptations of Lin’s concordance correlation coefficient (CCC) tailored to microbiome studies. We introduce a general formulation of the new CCC measures upon the use of a distance function appropriately characterizing the discrepancy between microbiome compositional measurements. We thoroughly study the special cases that adopt the Euclidean distance and Aitchison distance. Our proposals appropriately account for the unique features of microbiome compositional data, including high-dimensionality, dependency among individual relative abundances and the presence of many zeros. We further investigate a practical compound approach to help better understand the sources of data inconsistency. Extensive simulation studies are conducted to evaluate the utility of the proposed methods in realistic scenarios. We also apply the proposed methods to a microbiome validation data set from the Feeding Infants Right.. from the STart (FIRST) study. Our analyses offer useful insight about the extent of data variations resulted from two different experiment procedures as well as their heterogeneous patterns across genera.},
  archive      = {J_JRSSSC},
  author       = {Cui, Ying and Peng, Limin and Hu, Yijuan and Lai, HuiChuan J.},
  doi          = {10.1111/rssc.12497},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {1027-1048},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Assessing the reproducibility of microbiome measurements based on concordance correlation coefficients},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction in high-dimensional linear models and application
to genomic selection under imperfect linkage disequilibrium.
<em>JRSSSC</em>, <em>70</em>(4), 1001–1026. (<a
href="https://doi.org/10.1111/rssc.12496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genomic selection (GS) consists in predicting breeding values of selection candidates, using a large number of genetic markers. An important question in GS is to determine the number of markers required for a good prediction. For this purpose, we introduce new proxies for the accuracy of the prediction. These proxies are suitable under sparse genetic map where it is likely to observe some imperfect linkage disequilibrium, that is, the situation where the alleles at a gene location and at a marker located nearby vary. Moreover, our suggested proxies are helpful for designing cost-effective SNP chips based on a moderate density of markers. We analyse rice data from Los Banos, Philippines and focus on the flowering time collected during the dry season 2012. Using different densities of markers, we show that at least 1553 markers are required to implement GS. Finding the optimal number of markers is crucial in order to optimize the breeding program.},
  archive      = {J_JRSSSC},
  author       = {Rabier, Charles-Elie and Grusea, Simona},
  doi          = {10.1111/rssc.12496},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {1001-1026},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Prediction in high-dimensional linear models and application to genomic selection under imperfect linkage disequilibrium},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse reduced-rank regression for exploratory visualisation
of paired multivariate data. <em>JRSSSC</em>, <em>70</em>(4), 980–1000.
(<a href="https://doi.org/10.1111/rssc.12494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In genomics, transcriptomics, and related biological fields (collectively known as omics ), combinations of experimental techniques can yield multiple sets of features for the same set of biological replicates. One example is Patch-seq, a method combining single-cell RNA sequencing with electrophysiological recordings from the same cells. Here we present a framework based on sparse reduced-rank regression (RRR) for obtaining an interpretable visualisation of the relationship between the transcriptomic and the electrophysiological data. We use elastic net regularisation that yields sparse solutions and allows for an efficient computational implementation. Using several Patch-seq datasets, we show that sparse RRR outperforms both sparse full-rank regression and non-sparse RRR, as well as previous sparse RRR approaches, in terms of predictive performance. We introduce a bibiplot visualisation in order to display the dominant factors determining the relationship between transcriptomic and electrophysiological properties of neurons. We believe that sparse RRR can provide a valuable tool for the exploration and visualisation of paired multivariate datasets.},
  archive      = {J_JRSSSC},
  author       = {Kobak, Dmitry and Bernaerts, Yves and Weis, Marissa A. and Scala, Federico and Tolias, Andreas S. and Berens, Philipp},
  doi          = {10.1111/rssc.12494},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {980-1000},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Sparse reduced-rank regression for exploratory visualisation of paired multivariate data},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian nonparametric analysis for zero-inflated
multivariate count data with application to microbiome study.
<em>JRSSSC</em>, <em>70</em>(4), 961–979. (<a
href="https://doi.org/10.1111/rssc.12493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput sequencing technology has enabled researchers to profile microbial communities from a variety of environments, but analysis of multivariate taxon count data remains challenging. We develop a Bayesian nonparametric (BNP) regression model with zero inflation to analyse multivariate count data from microbiome studies. A BNP approach flexibly models microbial associations with covariates, such as environmental factors and clinical characteristics. The model produces estimates for probability distributions which relate microbial diversity and differential abundance to covariates, and facilitates community comparisons beyond those provided by simple statistical tests. We compare the model to simpler models and popular alternatives in simulation studies, showing, in addition to these additional community-level insights, it yields superior parameter estimates and model fit in various settings. The model&#39;s utility is demonstrated by applying it to a chronic wound microbiome data set and a Human Microbiome Project data set, where it is used to compare microbial communities present in different environments.},
  archive      = {J_JRSSSC},
  author       = {Shuler, Kurtis and Verbanic, Samuel and Chen, Irene A. and Lee, Juhee},
  doi          = {10.1111/rssc.12493},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {961-979},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian nonparametric analysis for zero-inflated multivariate count data with application to microbiome study},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-field geostatistical model combining point and areal
observations—a case study of annual runoff predictions in the voss area.
<em>JRSSSC</em>, <em>70</em>(4), 934–960. (<a
href="https://doi.org/10.1111/rssc.12492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We estimate annual runoff by using a Bayesian geostatistical model for interpolation of hydrological data of different spatial support: streamflow observations from catchments (areal data), and precipitation and evaporation data (point data). The model contains one climatic spatial effect that is common for all years under study, and 1 year specific spatial effect. Hence, the framework enables a quantification of the spatial variability caused by long-term weather patterns and processes. This can contribute to a better understanding of biases and uncertainties in environmental modelling. The suggested model is evaluated by predicting annual runoff for catchments around Voss in Norway and through a simulation study. We find that on average we benefit from combining point and areal data compared to using only one of the data types, and that the interaction between nested areal data and point data gives a spatial model that takes us beyond smoothing. Another finding is that when climatic effects dominate over annual effects, systematic under- and overestimation of runoff can be expected over time. However, a dominating climatic spatial effect also implies that short records of runoff from an otherwise ungauged catchment can lead to large improvements in the predictive performance.},
  archive      = {J_JRSSSC},
  author       = {Roksvåg, Thea and Steinsland, Ingelin and Engeland, Kolbjørn},
  doi          = {10.1111/rssc.12492},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {934-960},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A two-field geostatistical model combining point and areal Observations—A case study of annual runoff predictions in the voss area},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Censored regression for modelling small arms trade volumes
and its “forensic” use for exploring unreported trades. <em>JRSSSC</em>,
<em>70</em>(4), 909–933. (<a
href="https://doi.org/10.1111/rssc.12491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we use a censored regression model to investigate data on the international trade of small arms and ammunition provided by the Norwegian Initiative on Small Arms Transfers. Taking a network-based view on the transfers, we do not only rely on exogenous covariates but also estimate endogenous network effects. We apply a spatial autocorrelation gravity model with multiple weight matrices. The likelihood is maximized employing the Monte Carlo expectation maximization algorithm. Our approach reveals strong and stable endogenous network effects. Furthermore, we find evidence for a substantial path dependence as well as a close connection between exports of civilian and military small arms. The model is then used in a ‘forensic’ manner to analyse latent network structures and thereby to identify countries with higher or lower tendency to export or import than reflected in the data. The approach is also validated using a simulation study.},
  archive      = {J_JRSSSC},
  author       = {Lebacher, Michael and Thurner, Paul W. and Kauermann, Göran},
  doi          = {10.1111/rssc.12491},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {909-933},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Censored regression for modelling small arms trade volumes and its ‘Forensic’ use for exploring unreported trades},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A computationally efficient bayesian seemingly unrelated
regressions model for high-dimensional quantitative trait loci
discovery. <em>JRSSSC</em>, <em>70</em>(4), 886–908. (<a
href="https://doi.org/10.1111/rssc.12490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work is motivated by the search for metabolite quantitative trait loci (QTL) in a cohort of more than 5000 people. There are 158 metabolites measured by NMR spectroscopy in the 31-year follow-up of the Northern Finland Birth Cohort 1966 (NFBC66). These metabolites, as with many multivariate phenotypes produced by high-throughput biomarker technology, exhibit strong correlation structures. Existing approaches for combining such data with genetic variants for multivariate QTL analysis generally ignore phenotypic correlations or make restrictive assumptions about the associations between phenotypes and genetic loci. We present a computationally efficient Bayesian seemingly unrelated regressions model for high-dimensional data, with cell-sparse variable selection and sparse graphical structure for covariance selection. Cell sparsity allows different phenotype responses to be associated with different genetic predictors and the graphical structure is used to represent the conditional dependencies between phenotype variables. To achieve feasible computation of the large model space, we exploit a factorisation of the covariance matrix. Applying the model to the NFBC66 data with 9000 directly genotyped single nucleotide polymorphisms, we are able to simultaneously estimate genotype–phenotype associations and the residual dependence structure among the metabolites. The R package BayesSUR with full documentation is available at https://cran.r-project.org/web/packages/BayesSUR/},
  archive      = {J_JRSSSC},
  author       = {Bottolo, Leonardo and Banterle, Marco and Richardson, Sylvia and Ala-Korpela, Mika and Järvelin, Marjo-Riitta and Lewin, Alex},
  doi          = {10.1111/rssc.12490},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {886-908},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A computationally efficient bayesian seemingly unrelated regressions model for high-dimensional quantitative trait loci discovery},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating bayesian estimation for network poisson models
using frequentist variational estimates. <em>JRSSSC</em>,
<em>70</em>(4), 858–885. (<a
href="https://doi.org/10.1111/rssc.12489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is motivated by the analysis of ecological interaction networks. Poisson stochastic block models are widely used in this field to decipher the structure that underlies a weighted network, while accounting for covariate effects. Efficient algorithms based on variational approximations exist for frequentist inference, but without statistical guaranties as for the resulting estimates. In the absence of variational Bayes estimates, we show that a good proxy of the posterior distribution can be straightforwardly derived from the frequentist variational estimation procedure, using a Laplace approximation. We use this proxy to sample from the true posterior distribution via a sequential Monte Carlo algorithm. As shown in the simulation study, the efficiency of the posterior sampling is greatly improved by the accuracy of the approximate posterior distribution. The proposed procedure can be easily extended to other latent variable models. We use this methodology to assess the influence of available covariates on the organization of several ecological networks, as well as the existence of a residual interaction structure.},
  archive      = {J_JRSSSC},
  author       = {Donnet, Sophie and Robin, Stéphane},
  doi          = {10.1111/rssc.12489},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {858-885},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Accelerating bayesian estimation for network poisson models using frequentist variational estimates},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian criterion-based variable selection.
<em>JRSSSC</em>, <em>70</em>(4), 835–857. (<a
href="https://doi.org/10.1111/rssc.12488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian approaches for criterion based selection include the marginal likelihood based highest posterior model (HPM) and the deviance information criterion (DIC). The DIC is popular in practice as it can often be estimated from sampling-based methods with relative ease and DIC is readily available in various Bayesian software. We find that sensitivity of DIC-based selection can be high, in the range of 90–100\%. However, correct selection by DIC can be in the range of 0–2\%. These performances persist consistently with increase in sample size. We establish that both marginal likelihood and DIC asymptotically disfavour under-fitted models, explaining the high sensitivities of both criteria. However, mis-selection probability of DIC remains bounded below by a positive constant in linear models with g -priors whereas mis-selection probability by marginal likelihood converges to 0 under certain conditions. A consequence of our results is that not only the DIC cannot asymptotically differentiate between the data-generating and an over-fitted model, but, in fact, it cannot asymptotically differentiate between two over-fitted models as well. We illustrate these results in multiple simulation studies and in a biomarker selection problem on cancer cachexia of non-small cell lung cancer patients. We further study the performances of HPM and DIC in generalized linear model as practitioners often choose to use DIC that is readily available in software in such non-conjugate settings.},
  archive      = {J_JRSSSC},
  author       = {Maity, Arnab Kumar and Basu, Sanjib and Ghosh, Santu},
  doi          = {10.1111/rssc.12488},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {835-857},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian criterion-based variable selection},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Phase i clinical trials in adoptive t-cell therapies.
<em>JRSSSC</em>, <em>70</em>(4), 815–834. (<a
href="https://doi.org/10.1111/rssc.12485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop three approaches to phase I dose finding designs for engineered T cells in oncology. Our goal is to address a very particular difficulty in this clinical setting: an inability to fully administer the dose allocated to some patients. Current designs can be biased as a result of this incomplete information being ignored or discarded from the analysis. The performance of the three proposed solutions is largely similar, and all offer an advantage over the currently used design. One of the three methods is supported by theoretical study, and we provide some new results on this approach.},
  archive      = {J_JRSSSC},
  author       = {Devlin, Sean M. and Iasonos, Alexia and O’Quigley, John},
  doi          = {10.1111/rssc.12485},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {815-834},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Phase i clinical trials in adoptive T-cell therapies},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Applied statistics. <em>JRSSSC</em>, <em>70</em>(4),
813–814. (<a href="https://doi.org/10.1111/rssc.12425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  doi          = {10.1111/rssc.12425},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {813-814},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Applied statistics},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multisensor fusion of remotely sensed vegetation indices
using space-time dynamic linear models. <em>JRSSSC</em>, <em>70</em>(3),
793–812. (<a href="https://doi.org/10.1111/rssc.12495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High spatiotemporal resolution maps of surface vegetation from remote sensing data are desirable for vegetation and disturbance monitoring. However, due to the current limitations of imaging spectrometers, remote sensing datasets of vegetation with high temporal frequency of measurements have lower spatial resolution, and vice versa. In this research, we propose a space-time dynamic linear model to fuse high temporal frequency data (MODIS) with high spatial resolution data (Landsat) to create high spatiotemporal resolution data products of a vegetation greenness index. The model incorporates the spatial misalignment of the data and models dependence within and across land cover types with a latent multivariate Matérn process. To handle the large size of the data, we introduce a fast estimation procedure and a moving window Kalman smoother to produce a daily, 30-m resolution data product with associated uncertainty.},
  archive      = {J_JRSSSC},
  author       = {Johnson, Margaret C and Reich, Brian J and Gray, Josh M},
  doi          = {10.1111/rssc.12495},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {793-812},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Multisensor fusion of remotely sensed vegetation indices using space-time dynamic linear models},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Likelihood-free parameter estimation for dynamic queueing
networks: Case study of passenger flow in an international airport
terminal. <em>JRSSSC</em>, <em>70</em>(3), 770–792. (<a
href="https://doi.org/10.1111/rssc.12487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic queueing networks (DQN) model queueing systems where demand varies strongly with time, such as airport terminals. With rapidly rising global air passenger traffic placing increasing pressure on airport terminals, efficient allocation of resources is more important than ever. Parameter inference and quantification of uncertainty are key challenges for developing decision support tools. The DQN likelihood function is, in general, intractable and current approaches to simulation make likelihood-free parameter inference methods, such as approximate Bayesian computation (ABC), infeasible since simulating from these models is computationally expensive. By leveraging a recent advance in computationally efficient queueing simulation, we develop the first parameter inference approach for DQNs. We demonstrate our approach with data of passenger flows in a real airport terminal, and we show that our model accurately recreates the behaviour of the system and is useful for decision support. Special care must be taken in developing the distance for ABC since any useful output must vary with time. We use maximum mean discrepancy, a metric on probability measures, as the distance function for ABC. Prediction intervals of performance measures for decision support tools are easily constructed using draws from posterior samples, which we demonstrate with a scenario of a delayed flight.},
  archive      = {J_JRSSSC},
  author       = {Ebert, Anthony and Dutta, Ritabrata and Mengersen, Kerrie and Mira, Antonietta and Ruggeri, Fabrizio and Wu, Paul},
  doi          = {10.1111/rssc.12487},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {770-792},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Likelihood-free parameter estimation for dynamic queueing networks: Case study of passenger flow in an international airport terminal},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adjusting for population differences using machine learning
methods. <em>JRSSSC</em>, <em>70</em>(3), 750–769. (<a
href="https://doi.org/10.1111/rssc.12486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of real-world data for medical treatment evaluation frequently requires adjusting for population differences. We consider this problem in the context of estimating mean outcomes and treatment differences in a well-defined target population, using clinical data from a study population that overlaps with but differs from the target population in terms of patient characteristics. The current literature on this subject includes a variety of statistical methods, which generally require correct specification of at least one parametric regression model. In this article, we propose to use machine learning methods to estimate nuisance functions and incorporate the machine learning estimates into existing doubly robust estimators. This leads to nonparametric estimators that are n ‾ √ n n -consistent, asymptotically normal and asymptotically efficient under general conditions. Simulation results demonstrate that the proposed methods perform reasonably well in realistic settings. The methods are illustrated with a cardiology example concerning aortic stenosis.},
  archive      = {J_JRSSSC},
  author       = {Cappiello, Lauren and Zhang, Zhiwei and Shen, Changyu and Butala, Neel M. and Cui, Xinping and Yeh, Robert W.},
  doi          = {10.1111/rssc.12486},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {750-769},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Adjusting for population differences using machine learning methods},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mapping malaria by sharing spatial information between
incidence and prevalence data sets. <em>JRSSSC</em>, <em>70</em>(3),
733–749. (<a href="https://doi.org/10.1111/rssc.12484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As malaria incidence decreases and more countries move towards elimination, maps of malaria risk in low-prevalence areas are increasingly needed. For low-burden areas, disaggregation regression models have been developed to estimate risk at high spatial resolution from routine surveillance reports aggregated by administrative unit polygons. However, in areas with both routine surveillance data and prevalence surveys, models that make use of the spatial information from prevalence point-surveys might make more accurate predictions. Using case studies in Indonesia, Senegal and Madagascar, we compare the out-of-sample mean absolute error for two methods for incorporating point-level, spatial information into disaggregation regression models. The first simply fits a binomial-likelihood, logit-link, Gaussian random field to prevalence point-surveys to create a new covariate. The second is a multi-likelihood model that is fitted jointly to prevalence point-surveys and polygon incidence data. We find that in most cases there is no difference in mean absolute error between models. In only one case, did the new models perform the best. More generally, our results demonstrate that combining these types of data has the potential to reduce absolute error in estimates of malaria incidence but that simpler baseline models should always be fitted as a benchmark.},
  archive      = {J_JRSSSC},
  author       = {Lucas, Tim C. D. and Nandi, Anita K. and Chestnutt, Elisabeth G. and Twohig, Katherine A. and Keddie, Suzanne H. and Collins, Emma L. and Howes, Rosalind E. and Nguyen, Michele and Rumisha, Susan F. and Python, Andre and Arambepola, Rohan and Bertozzi-Villa, Amelia and Hancock, Penelope and Amratia, Punam and Battle, Katherine E. and Cameron, Ewan and Gething, Peter W. and Weiss, Daniel J.},
  doi          = {10.1111/rssc.12484},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {733-749},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Mapping malaria by sharing spatial information between incidence and prevalence data sets},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering and automatic labelling within time series of
categorical observations—with an application to marine log messages.
<em>JRSSSC</em>, <em>70</em>(3), 714–732. (<a
href="https://doi.org/10.1111/rssc.12483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {System logs or log files containing textual messages with associated time stamps are generated by many technologies and systems. The clustering technique proposed in this paper provides a tool to discover and identify patterns or macrolevel events in this data. The motivating application is logs generated by frequency converters in the propulsion system on a ship, while the general setting is fault identification and classification in complex industrial systems. The paper introduces an offline approach for dividing a time series of log messages into a series of discrete segments of random lengths. These segments are clustered into a limited set of states. A state is assumed to correspond to a specific operation or condition of the system, and can be a fault mode or a normal operation. Each of the states can be associated with a specific, limited set of messages, where messages appear in a random or semi-structured order within the segments. These structures are in general not defined a priori. We propose a Bayesian hierarchical model where the states are characterised both by the temporal frequency and the type of messages within each segment. An algorithm for inference based on reversible jump MCMC is proposed. The performance of the method is assessed by both simulations and operational data.},
  archive      = {J_JRSSSC},
  author       = {Gramuglia, Emanuele and Storvik, Geir and Stakkeland, Morten},
  doi          = {10.1111/rssc.12483},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {714-732},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Clustering and automatic labelling within time series of categorical Observations—With an application to marine log messages},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Functional data analysis and visualisation of
three-dimensional surface shape. <em>JRSSSC</em>, <em>70</em>(3),
691–713. (<a href="https://doi.org/10.1111/rssc.12482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of high-resolution imaging has made data on surface shape widespread. Methods for the analysis of shape based on landmarks are well established but high-resolution data require a functional approach. The starting point is a systematic and consistent description of each surface shape and a method for creating this is described. Three innovative forms of analysis are then introduced. The first uses surface integration to address issues of registration, principal component analysis and the measurement of asymmetry, all in functional form. Computational issues are handled through discrete approximations to integrals, based in this case on appropriate surface area weighted sums. The second innovation is to focus on sub-spaces where interesting behaviour such as group differences are exhibited, rather than on individual principal components. The third innovation concerns the comparison of individual shapes with a relevant control set, where the concept of a normal range is extended to the highly multivariate setting of surface shape. This has particularly strong applications to medical contexts where the assessment of individual patients is very important. All of these ideas are developed and illustrated in the important context of human facial shape, with a strong emphasis on the effective visual communication of effects of interest.},
  archive      = {J_JRSSSC},
  author       = {Katina, Stanislav and Vittert, Liberty and W. Bowman, Adrian},
  doi          = {10.1111/rssc.12482},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {691-713},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Functional data analysis and visualisation of three-dimensional surface shape},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of the size of informal employment based on
administrative records with non-ignorable selection mechanism.
<em>JRSSSC</em>, <em>70</em>(3), 667–690. (<a
href="https://doi.org/10.1111/rssc.12481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we used company level administrative data from the National Labour Inspectorate and The Polish Social Insurance Institution in order to estimate the prevalence of informal employment in Poland in 2016. Since the selection mechanism is non-ignorable, we employed a generalization of Heckman’s sample selection model assuming non-Gaussian correlation of errors and clustering by incorporation of random effects. We found that 5.7\% (4.6\%, 7.1\%; 95\% CI ) of registered enterprises in Poland, to some extent, take advantage of the informal labour force. Our study exemplifies a new approach to measuring informal employment, which can be implemented in other countries. It also contributes to the existing literature by providing, to the best of our knowledge, the first estimates of informal employment at the level of companies based solely on administrative data.},
  archive      = {J_JRSSSC},
  author       = {Berȩsewicz, Maciej and Nikulin, Dagmara},
  doi          = {10.1111/rssc.12481},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {667-690},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimation of the size of informal employment based on administrative records with non-ignorable selection mechanism},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian modelling for spatially misaligned health areal
data: A multiple membership approach. <em>JRSSSC</em>, <em>70</em>(3),
645–666. (<a href="https://doi.org/10.1111/rssc.12480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes prevalence is on the rise in the United Kingdom, and for public health strategy, estimation of relative disease risk and subsequent mapping is important. We consider an application to London data on diabetes prevalence and mortality. In order to improve the estimation of relative risks, we analyse jointly prevalence and mortality data to ensure borrowing strength over the two outcomes. The available data involve two spatial frameworks, areas (Middle Layer Super Output Areas, MSOAs) and general practices (GPs) recruiting patients from several areas. This raises a spatial misalignment issue that we deal with by employing the multiple membership principle. Specifically, we translate areal spatial effects to explain GP practice prevalence according to proportions of GP populations resident in different areas. A sparse implementation in RStan of both the multivariate conditional autoregressive (MCAR) and generalised MCAR (GMCAR) with multiple membership allows the comparison of these bivariate priors as well as exploring the different implications for the mapping patterns for both outcomes. The necessary causal precedence of diabetes prevalence over mortality allows a specific conditionality assumption in the GMCAR, not always present in the context of disease mapping. Additionally, an area-locality comparison is considered to locate high versus low relative risk clusters.},
  archive      = {J_JRSSSC},
  author       = {Gramatica, Marco and Congdon, Peter and Liverani, Silvia},
  doi          = {10.1111/rssc.12480},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {645-666},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian modelling for spatially misaligned health areal data: A multiple membership approach},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time matters: How default resolution times impact final loss
rates. <em>JRSSSC</em>, <em>70</em>(3), 619–644. (<a
href="https://doi.org/10.1111/rssc.12474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using access to a unique bank loss database, we find positive dependencies of default resolution times (DRTs) of defaulted bank loan contracts and final loan loss rates (losses given default, LGDs). Due to this interconnection, LGD predictions made at the time of default and during resolution are subject to censoring. Pure (standard) LGD models are not able to capture effects of censoring. Accordingly, their LGD predictions may be biased and underestimate loss rates of defaulted loans. In this paper, we develop a Bayesian hierarchical modelling framework for DRTs and LGDs. In comparison to previous approaches, we derive final DRT estimates for loans in default which enables consistent LGD predictions conditional on the time in default. Furthermore, adequate unconditional LGD predictions can be derived. The proposed method is applicable to duration processes in general where the final outcomes depend on the duration of the process and are affected by censoring. By this means, we avoid bias of parameter estimates to ensure adequate predictions.},
  archive      = {J_JRSSSC},
  author       = {Betz, Jennifer and Kellner, Ralf and Rösch, Daniel},
  doi          = {10.1111/rssc.12474},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {619-644},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Time matters: How default resolution times impact final loss rates},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal block designs for experiments on networks.
<em>JRSSSC</em>, <em>70</em>(3), 596–618. (<a
href="https://doi.org/10.1111/rssc.12473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for constructing optimal block designs for experiments on networks. The response model for a given network interference structure extends the linear network effects model to incorporate blocks. The optimality criteria are chosen to reflect the experimental objectives and an exchange algorithm is used to search across the design space for obtaining an efficient design when an exhaustive search is not possible. Our interest lies in estimating the direct comparisons among treatments, in the presence of nuisance network effects that stem from the underlying network interference structure governing the experimental units, or in the network effects themselves. Comparisons of optimal designs under different models, including the standard treatment models, are examined by comparing the variance and bias of treatment effect estimators. We also suggest a way of defining blocks, while taking into account the interrelations of groups of experimental units within a network, using spectral clustering techniques to achieve optimal modularity. We expect connected units within closed-form communities to behave similarly to an external stimulus. We provide evidence that our approach can lead to efficiency gains over conventional designs such as randomised designs that ignore the network structure and we illustrate its usefulness for experiments on networks.},
  archive      = {J_JRSSSC},
  author       = {Koutra, Vasiliki and Gilmour, Steven G. and Parker, Ben M.},
  doi          = {10.1111/rssc.12473},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {596-618},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Optimal block designs for experiments on networks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing daily patterns using home activity sensors and
within period changepoint detection. <em>JRSSSC</em>, <em>70</em>(3),
579–595. (<a href="https://doi.org/10.1111/rssc.12472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of ascertaining daily patterns using passive sensors to establish a baseline for elderly people living alone. The data are whether or not some movement, or human related activity, has occurred in the previous 15 min. We seek to segment the broad patterns within a day, for example, awake/sleep times or potentially more activity around meal-times. To address this problem we use changepoint detection which can segment the day into more/less active times. Traditional changepoint detection methods are inappropriate for these data as they fail to utilize the periodic nature of the data. The traditional assumption of conditional independence of the segments also hampers estimation of the within segment parameters. A new within-period changepoint detection scheme is proposed that instead assumes a circular perspective of the time axis. This permits the pooling of evidence of changepoint events from across multiple days. Inference is performed within the Bayesian framework by utilizing the reversible jump Markov chain Monte Carlo sampler to explore the variable dimension parameter space. Simulations demonstrate that the sampler achieves high accuracy in approximating the posterior while being able to detect small segments. Application to four individuals from our industrial collaborator provides insights to their daily patterns.},
  archive      = {J_JRSSSC},
  author       = {Taylor, Simon A. C. and Killick, Rebecca and Burr, Jonathan and Rogerson, Louise},
  doi          = {10.1111/rssc.12472},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {579-595},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Assessing daily patterns using home activity sensors and within period changepoint detection},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering based on kolmogorov–smirnov statistic with
application to bank card transaction data. <em>JRSSSC</em>,
<em>70</em>(3), 558–578. (<a
href="https://doi.org/10.1111/rssc.12471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid developments in third-party online payment platforms now make it possible to record massive bank card transaction data. Clustering on such transaction data is of great importance for the analysis of merchant behaviours. However, traditional methods based on generated features inevitably lead to much loss of information. To make better use of bank card transaction data, this study investigates the possibility of using the empirical cumulative distribution of transaction amounts. As the distance between two merchants can be measured using the two-sample Kolmogorov–Smirnov test statistic, we propose the Kolmogorov–Smirnov K-means clustering approach based on this distance measure. An approximation step is conducted to ensure the feasibility of the proposed method even for large-scale transaction data, and the associated theoretical properties are investigated. Both simulations and an empirical study demonstrate that our method outperforms feature-based methods and is computationally efficient for large-scale data sets.},
  archive      = {J_JRSSSC},
  author       = {Zhu, Yingqiu and Deng, Qiong and Huang, Danyang and Jing, Bingyi and Zhang, Bo},
  doi          = {10.1111/rssc.12471},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {558-578},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Clustering based on Kolmogorov–Smirnov statistic with application to bank card transaction data},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian hierarchical factor regression models to infer
cause of death from verbal autopsy data. <em>JRSSSC</em>,
<em>70</em>(3), 532–557. (<a
href="https://doi.org/10.1111/rssc.12468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low-resource settings where vital registration of death is not routine it is often of critical interest to determine and study the cause of death (COD) for individuals and the cause-specific mortality fraction (CSMF) for populations. Post-mortem autopsies, considered the gold standard for COD assignment, are often difficult or impossible to implement due to deaths occurring outside the hospital, expense and/or cultural norms. For this reason, verbal autopsies (VAs) are commonly conducted, consisting of a questionnaire administered to next of kin recording demographic information, known medical conditions, symptoms and other factors for the decedent. This article proposes a novel class of hierarchical factor regression models that avoid restrictive assumptions of standard methods, allow both the mean and covariance to vary with COD category, and can include covariate information on the decedent, region or events surrounding death. Taking a Bayesian approach to inference, this work develops an MCMC algorithm and validates the FActor Regression for Verbal Autopsy (FARVA) model in simulation experiments. An application of FARVA to real VA data shows improved goodness-of-fit and better predictive performance in inferring COD and CSMF over competing methods. Code and a user manual are made available at https://github.com/kelrenmor/farva .},
  archive      = {J_JRSSSC},
  author       = {Moran, Kelly R. and Turner, Elizabeth L. and Dunson, David and Herring, Amy H.},
  doi          = {10.1111/rssc.12468},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {532-557},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian hierarchical factor regression models to infer cause of death from verbal autopsy data},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A model-free approach for testing association.
<em>JRSSSC</em>, <em>70</em>(3), 511–531. (<a
href="https://doi.org/10.1111/rssc.12467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The question of association between outcome and feature is generally framed in the context of a model based on functional and distributional forms. Our motivating application is that of identifying serum biomarkers of angiogenesis, energy metabolism, apoptosis and inflammation, predictive of recurrence after lung resection in node-negative non-small cell lung cancer patients with tumour stage T2a or less. We propose an omnibus approach for testing the association that is free of assumptions on functional forms and distributions and can be used as a general method. This proposed maximal permutation test is based on the idea of thresholding, is readily implementable and is computationally efficient. We demonstrate that the proposed omnibus tests maintain their levels and have strong power for detecting linear, nonlinear and quantile-based associations, even with outlier-prone and heavy-tailed error distributions and under nonparametric setting. We additionally illustrate the use of this approach in model-free feature screening and further examine the level and power of these tests for binary outcome. We compare the performance of the proposed omnibus tests with comparator methods in our motivating application to identify the preoperative serum biomarkers associated with non-small cell lung cancer recurrence in early stage patients.},
  archive      = {J_JRSSSC},
  author       = {Chatterjee, Saptarshi and Chowdhury, Shrabanti and Basu, Sanjib},
  doi          = {10.1111/rssc.12467},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {511-531},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A model-free approach for testing association},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Applied statistics. <em>JRSSSC</em>, <em>70</em>(3),
509–510. (<a href="https://doi.org/10.1111/rssc.12424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  doi          = {10.1111/rssc.12424},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {509-510},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Applied statistics},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finding your feet: A gaussian process model for estimating
the abilities of batsmen in test cricket. <em>JRSSSC</em>,
<em>70</em>(2), 481–506. (<a
href="https://doi.org/10.1111/rssc.12470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the sport of cricket, a player’s batting ability is traditionally measured using the batting average. However, the batting average fails to measure both short-term changes in ability that occur during an innings and long-term changes in ability that occur between innings due to factors such as age and experience in various match conditions. We derive and fit a Bayesian parametric model that employs a Gaussian process to measure and predict how the batting abilities of cricket players vary and fluctuate over the course of entire playing careers. The results allow us to better quantify and predict the batting ability of a player, compared with both traditional cricket statistics, such as the batting average, and more complex models, such as the official International Cricket Council ratings.},
  archive      = {J_JRSSSC},
  author       = {Stevenson, Oliver G. and Brewer, Brendon J.},
  doi          = {10.1111/rssc.12470},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {481-506},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Finding your feet: A gaussian process model for estimating the abilities of batsmen in test cricket},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian nonparametric model for textural pattern
heterogeneity. <em>JRSSSC</em>, <em>70</em>(2), 459–480. (<a
href="https://doi.org/10.1111/rssc.12469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer radiomics is an emerging discipline promising to elucidate lesion phenotypes and tumour heterogeneity through patterns of enhancement, texture, morphology and shape. The prevailing technique for image texture analysis relies on the construction and synthesis of grey-level co-occurrence matrices (GLCM). Practice currently reduces the structured count data of a GLCM to reductive and redundant summary statistics for which analysis requires variable selection and multiple comparisons for each application, thus limiting reproducibility. In this article, we develop a Bayesian multivariate probabilistic framework for the analysis and unsupervised clustering of a sample of GLCM objects. By appropriately accounting for skewness and zero inflation of the observed counts and simultaneously adjusting for existing spatial autocorrelation at nearby cells, the methodology facilitates estimation of texture pattern distributions within the GLCM lattice itself. The techniques are applied to cluster images of adrenal lesions obtained from CT scans with and without administration of contrast. We further assess whether the resultant subtypes are clinically oriented by investigating their correspondence with pathological diagnoses. Additionally, we compare performance to a class of machine learning approaches currently used in cancer radiomics with simulation studies.},
  archive      = {J_JRSSSC},
  author       = {Li, Xiao and Guindani, Michele and Ng, Chaan S. and Hobbs, Brian P.},
  doi          = {10.1111/rssc.12469},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {459-480},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian nonparametric model for textural pattern heterogeneity},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Long-term trend analysis of extreme coastal sea levels with
changepoint detection. <em>JRSSSC</em>, <em>70</em>(2), 434–458. (<a
href="https://doi.org/10.1111/rssc.12466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sea level rise can bring disastrous outcomes to people living in coastal regions by increasing flood risk or inducing stronger storm surges. We study long-term linear trends in monthly maximum sea levels by applying extreme value methods. The monthly maximum sea levels are extracted from multiple tide gauges around the coastal regions of the world over a period of as long as 169 years. Due to instrument changes, location changes, earthquakes, land reclamation, dredging, etc., the sea level data could contain inhomogeneous shifts in their means, which can substantially impact trend estimates if ignored. To rigorously quantify the long-term linear trends and return levels for the monthly maximum sea level data, we use a genetic algorithm to estimate the number and times of changepoints in the data. As strong periodicity and temporal correlation are pertinent to the data, bootstrap techniques are used to obtain more realistic confidence intervals to the estimated trends and return levels. We find that the consideration of changepoints changed the estimated linear trends of 89 tide gauges (approximately 30\% of tide gauges considered) by more than 20 cm century − 1 20 cm century - 1 20cmcentury-1 ⁠ . Our results are summarized in maps with estimated extreme sea level trends and 50-year return levels.},
  archive      = {J_JRSSSC},
  author       = {Lee, Mintaek and Lee, Jaechoul},
  doi          = {10.1111/rssc.12466},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {434-458},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Long-term trend analysis of extreme coastal sea levels with changepoint detection},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferring bivariate association from respondent-driven
sampling data. <em>JRSSSC</em>, <em>70</em>(2), 415–433. (<a
href="https://doi.org/10.1111/rssc.12465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respondent-driven sampling (RDS) is an effective method of collecting data from many hard-to-reach populations. Valid statistical inference for these data relies on many strong assumptions. In standard samples, we assume observations from pairs of individuals are independent. In RDS, this assumption is violated by the sampling dependence between individuals. We propose a method to semi-parametrically estimate the null distributions of standard test statistics in the presence of sampling dependence, allowing for more valid statistical testing for dependence between pairs of variables within the sample. We apply our method to study characteristics of young adult illicit opioid users in New York City.},
  archive      = {J_JRSSSC},
  author       = {Kim, Dongah and Gile, Krista J. and Guarino, Honoria and Mateu-Gelabert, Pedro},
  doi          = {10.1111/rssc.12465},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {415-433},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Inferring bivariate association from respondent-driven sampling data},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian semi-parametric g-computation for causal inference
in a cohort study with mnar dropout and death. <em>JRSSSC</em>,
<em>70</em>(2), 398–414. (<a
href="https://doi.org/10.1111/rssc.12464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference with observational longitudinal data and time-varying exposures is often complicated by time-dependent confounding and attrition. The G-computation formula is one approach for estimating a causal effect in this setting. The parametric modelling approach typically used in practice relies on strong modelling assumptions for valid inference and moreover depends on an assumption of missing at random, which is not appropriate when the missingness is missing not at random (MNAR) or due to death. In this work we develop a flexible Bayesian semi-parametric G-computation approach for assessing the causal effect on the subpopulation that would survive irrespective of exposure, in a setting with MNAR dropout. The approach is to specify models for the observed data using Bayesian additive regression trees, and then, use assumptions with embedded sensitivity parameters to identify and estimate the causal effect. The proposed approach is motivated by a longitudinal cohort study on cognition, health and ageing and we apply our approach to study the effect of becoming a widow on memory. We also compare our approach to several standard methods.},
  archive      = {J_JRSSSC},
  author       = {Josefsson, Maria and Daniels, Michael J.},
  doi          = {10.1111/rssc.12464},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {398-414},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian semi-parametric G-computation for causal inference in a cohort study with mnar dropout and death},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale null hypothesis testing for network-valued data:
Analysis of brain networks of patients with autism. <em>JRSSSC</em>,
<em>70</em>(2), 372–397. (<a
href="https://doi.org/10.1111/rssc.12463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks are a natural way of representing the human brain for studying its structure and function and, as such, have been extensively used. In this framework, case–control studies for understanding autism pertain to comparing samples of healthy and autistic brain networks. In order to understand the biological mechanisms involved in the pathology, it is key to localize the differences on the brain network. Motivated by this question, we hereby propose a general non-parametric finite-sample exact statistical framework that allows to test for differences in connectivity within and between prespecified areas inside the brain network, with strong control of the family-wise error rate. We demonstrate unprecedented ability to differentiate children with non-syndromic autism from children with both autism and tuberous sclerosis complex using electroencephalography data. The implementation of the method is available in the R package nevada.},
  archive      = {J_JRSSSC},
  author       = {Lovato, Ilenia and Pini, Alessia and Stamm, Aymeric and Taquet, Maxime and Vantini, Simone},
  doi          = {10.1111/rssc.12463},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {372-397},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Multiscale null hypothesis testing for network-valued data: Analysis of brain networks of patients with autism},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent events modelling of haemophilia bleeding events.
<em>JRSSSC</em>, <em>70</em>(2), 351–371. (<a
href="https://doi.org/10.1111/rssc.12462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A pharmacokinetic–pharmacodynamic (PK-PD) approach is developed for modelling the recurrent bleeding events in patients with severe haemophilia to investigate the relationship between factor VIII plasma activity level and the instantaneous risk of a bleed. The model incorporates patient-level pharmacokinetic (PK) information obtained through measurements taken prior to the study which are used to fit a non-linear mixed-effects two-compartment PK model. Dosing times within the study are combined with the PK model to provide the estimated factor VIII plasma level for all patients, which is used as a time-dependent covariate within the recurrent events model. Methods are developed to correct the attenuation in covariate effects that would otherwise arise due to the discrepancy between estimated and true factor VIII. In contrast to existing methods proposed for such data, such as count data regression or time-to-event analysis, the new method allows all the bleeding times to be used to investigate the relationship between current factor VIII and risk of a bleed. The performance of the proposed estimators are assessed via simulation and found to outperform the naive estimator, which treats the estimated factor VIII levels as if they were measured without error, both in terms of bias and mean squared error.},
  archive      = {J_JRSSSC},
  author       = {Titman, Andrew C. and Wolfsegger, Martin J. and Jaki, Thomas F.},
  doi          = {10.1111/rssc.12462},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {351-371},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Recurrent events modelling of haemophilia bleeding events},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Future proofing a building design using history matching
inspired level-set techniques. <em>JRSSSC</em>, <em>70</em>(2), 335–350.
(<a href="https://doi.org/10.1111/rssc.12461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can one design a building that will be sufficiently protected against overheating and sufficiently energy efficient, whilst considering the expected increases in temperature due to climate change? We successfully manage to address this question—greatly reducing a large set of initial candidate building designs down to a small set of acceptable buildings. We do this using a complex computer model, statistical models of said computer model (emulators), and a modification to the history matching calibration technique. This modification tackles the problem of level-set estimation (rather than calibration), where the goal is to find input settings which lead to the simulated output being below some threshold. The entire procedure allows us to present a practitioner with a set of acceptable building designs, with the final design chosen based on other requirements (subjective or otherwise).},
  archive      = {J_JRSSSC},
  author       = {Baker, Evan and Challenor, Peter and Eames, Matt},
  doi          = {10.1111/rssc.12461},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {335-350},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Future proofing a building design using history matching inspired level-set techniques},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust estimation for small domains in business surveys.
<em>JRSSSC</em>, <em>70</em>(2), 312–334. (<a
href="https://doi.org/10.1111/rssc.12460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small area (or small domain) estimation is still rarely applied in business statistics, because of challenges arising from the skewness and variability of variables such as turnover. We examine a range of small area estimation methods as the basis for estimating the activity of industries within the retail sector in the Netherlands. We use tax register data and a sampling procedure which replicates the sampling for the retail sector of Statistics Netherlands’ Structural Business Survey as a basis for investigating the properties of small area estimators. In particular, we consider the use of the empirical best linear unbiased predictor (EBLUP) under a random effects model and variations of the EBLUP derived under (a) a random effects model that includes a complex specification for the level 1 variance and (b) a random effects model that is fitted by using the survey weights. Although accounting for the survey weights in estimation is important, the impact of influential data points remains the main challenge in this case. The paper further explores the use of outlier robust estimators in business surveys, in particular a robust version of the EBLUP, M-regression-based synthetic estimators and M-quantile small area estimators. The latter family of small area estimators includes robust projective (without and with survey weights) and robust predictive versions. M-quantile methods have the lowest empirical mean squared error and are substantially better than direct estimators, although there is an open question about how to choose the tuning constant for bias adjustment in practice. The paper makes a further contribution by exploring a doubly robust approach comprising the use of survey weights in conjunction with outlier robust methods in small area estimation.},
  archive      = {J_JRSSSC},
  author       = {Smith, Paul A. and Bocci, Chiara and Tzavidis, Nikos and Krieg, Sabine and Smeets, Marc J. E.},
  doi          = {10.1111/rssc.12460},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {312-334},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Robust estimation for small domains in business surveys},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Threshold-based subgroup testing in logistic regression
models in two-phase sampling designs. <em>JRSSSC</em>, <em>70</em>(2),
291–311. (<a href="https://doi.org/10.1111/rssc.12459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effect of treatment on binary disease outcome can differ across subgroups characterised by other covariates. Testing for the existence of subgroups that are associated with heterogeneous treatment effects can provide valuable insight regarding the optimal treatment recommendation in practice. Our research in this paper is motivated by the question of whether host genetics could modify a vaccine&#39;s effect on HIV acquisition risk. To answer this question, we used data from an HIV vaccine trial with a two-phase sampling design and developed a general threshold-based model framework to test for the existence of subgroups associated with the heterogeneity in disease risks, allowing for subgroups based on multivariate covariates. We developed a testing procedure based on maximum of likelihood ratio statistics over change-planes and demonstrated its advantage over alternative methods. We further developed the testing procedure to account for bias sampling of expensive (i.e. resource-intensive to measure) covariates through the incorporation of inverse probability weighting techniques. We used the proposed method to analyse the motivating HIV vaccine trial data. Our proposed testing procedure also has broad applications in epidemiological studies for assessing heterogeneity in disease risk with respect to univariate or multivariate predictors.},
  archive      = {J_JRSSSC},
  author       = {Huang, Ying and Cho, Juhee and Fong, Youyi},
  doi          = {10.1111/rssc.12459},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {291-311},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Threshold-based subgroup testing in logistic regression models in two-phase sampling designs},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantile-frequency analysis and spectral measures for
diagnostic checks of time series with nonlinear dynamics.
<em>JRSSSC</em>, <em>70</em>(2), 270–290. (<a
href="https://doi.org/10.1111/rssc.12458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear dynamic volatility has been observed in many financial time series. The recently proposed quantile periodogram offers an alternative way to examine this phenomena in the frequency domain. The quantile periodogram is constructed from trigonometric quantile regression of time series data at different frequencies and quantile levels, enabling the quantile-frequency analysis (QFA) of nonlinear serial dependence. This paper introduces some spectral measures based on the quantile periodogram for diagnostic checks of financial time series models and for model-based discriminant analysis. A simulation-based parametric bootstrapping technique is employed to compute the p -values of the spectral measures. The usefulness of the proposed method is demonstrated by a simulation study and a motivating application using the daily log returns of the S&amp;P 500 index together with GARCH-type models. The results show that the QFA method is able to provide additional insights into the goodness of fit of these financial time series models that may have been missed by conventional tests. The results also show that the QFA method offers a more informative way of discriminant analysis for detecting regime changes in financial time series.},
  archive      = {J_JRSSSC},
  author       = {Li, Ta-Hsin},
  doi          = {10.1111/rssc.12458},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {270-290},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Quantile-frequency analysis and spectral measures for diagnostic checks of time series with nonlinear dynamics},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite mixtures of semiparametric bayesian survival kernel
machine regressions: Application to breast cancer gene pathway subgroup
analysis. <em>JRSSSC</em>, <em>70</em>(2), 251–269. (<a
href="https://doi.org/10.1111/rssc.12457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A gene pathway is defined as a set of genes that functionally work together to regulate a certain biological process. Gene pathway expression data, which is a special case of highly correlated high-dimensional data, exhibits the ‘small n and large p ’ problem. Pathway analysis can take into account the dependency structures among genes and the possibility that several moderately regulated genes may have significant impacts on the clinical outcomes. To test the significance of gene pathways in the presence of subgroups, we propose a finite mixture model of semiparametric Bayesian survival kernel machine regressions (fm-BKSurv). Within each hidden group, we model the unknown function of gene pathways via a Gaussian kernel machine. We demonstrate how fm-BKSurv excels in terms of true positive rate, false positive rate, accuracy, and precision in a simulation study, and further illustrate the outperformance of fm-BKSurv in detecting significant gene pathways using a gene pathway expression dataset of breast cancer patients.},
  archive      = {J_JRSSSC},
  author       = {Zhang, Lin and Kim, Inyoung},
  doi          = {10.1111/rssc.12457},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {251-269},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Finite mixtures of semiparametric bayesian survival kernel machine regressions: Application to breast cancer gene pathway subgroup analysis},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applied statistics. <em>JRSSSC</em>, <em>70</em>(2),
249–250. (<a href="https://doi.org/10.1111/rssc.12423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  author       = {Friel, N. and Illian, J. and Barber, S. and Brown, P. and Canale, A. and Demiris, N. and Fan, Y. and Huang, X. and Iannario, M. and Jaki, T. and Jermyn, I. H. and Latouche, P. and Libel, D. and Manolopoulou, I. and Marra, G. and Matechou, E. and De Menezes, R. X. and Ntzoufras, I. and Overstall, A. and Paoletti, X. and Papamarkou, T. and Prangle, D. and Radice, R. and Tzavidis, N.},
  doi          = {10.1111/rssc.12423},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {249-250},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Applied statistics},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling time-varying mobility flows using
function-on-function regression: Analysis of a bike sharing system in
the city of milan. <em>JRSSSC</em>, <em>70</em>(1), 226–247. (<a
href="https://doi.org/10.1111/rssc.12456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s world, bike sharing systems are becoming increasingly common in all main cities around the world. To understand the spatiotemporal patterns of how people move by bike through the city of Milan, we apply functional data analysis to study the flows of a bike sharing mobility network. We introduce a complete pipeline to properly analyse and model functional data through a concurrent functional-on-functional model taking into account the effects of weather conditions and calendar on the bike flows. In the end, we develop an interactive interface to explore the results of the analyses.},
  archive      = {J_JRSSSC},
  author       = {Torti, Agostino and Pini, Alessia and Vantini, Simone},
  doi          = {10.1111/rssc.12456},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {226-247},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Modelling time-varying mobility flows using function-on-function regression: Analysis of a bike sharing system in the city of milan},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential aggregation of probabilistic
forecasts—application to wind speed ensemble forecasts. <em>JRSSSC</em>,
<em>70</em>(1), 202–225. (<a
href="https://doi.org/10.1111/rssc.12455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In numerical weather prediction (NWP), the uncertainty about the future state of the atmosphere is described by a set of forecasts (called an ensemble). All ensembles have deficiencies that can be corrected via statistical post-processing methods. Several ensembles, based on different NWP models, exist and may be corrected using different statistical methods. These raw or post-processed ensembles can thus be combined. The theory of prediction with expert advice allows us to build combination algorithms with theoretical guarantees on the forecast performance. We adapt this theory to the case of probabilistic forecasts issued as stepwise cumulative distribution functions, computed from raw and post-processed ensembles. The theory is applied to combine wind speed ensemble forecasts. The second goal of this study is to explore the use of two forecast performance criteria: the continuous ranked probability score (CRPS) and the Jolliffe–Primo test. The usual way to build skilful probabilistic forecasts is to minimize the CRPS. Minimizing the CRPS may not produce reliable forecasts according to the Jolliffe–Primo test. The Jolliffe–Primo test generally selects reliable forecasts, but could lead to issuing suboptimal forecasts in terms of CRPS. We propose to use both criteria to achieve reliable and skilful probabilistic forecasts.},
  archive      = {J_JRSSSC},
  author       = {Zamo, Michaël and Bel, Liliane and Mestre, Olivier},
  doi          = {10.1111/rssc.12455},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {202-225},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Sequential aggregation of probabilistic Forecasts—Application to wind speed ensemble forecasts},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian approach for determining player abilities in
football. <em>JRSSSC</em>, <em>70</em>(1), 174–201. (<a
href="https://doi.org/10.1111/rssc.12454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of determining a football player’s ability for a given event type, for example, scoring a goal. We propose an interpretable Bayesian model which is fit using variational inference methods. We implement a Poisson model to capture occurrences of event types, from which we infer player abilities. Our approach also allows the visualisation of differences between players, for a specific ability, through the marginal posterior variational densities. We then use these inferred player abilities to extend the Bayesian hierarchical model of Baio and Blangiardo (2010, Journal of Applied Statistics , 37(2), 253–264) which captures a team’s scoring rate (the rate at which they score goals). We apply the resulting scheme to the English Premier League, capturing player abilities over the 2013/2014 season, before using output from the hierarchical model to predict whether over or under 2.5 goals will be scored in a given game in the 2014/2015 season. This validates our model as a way of providing insights into team formation and the individual success of sports teams.},
  archive      = {J_JRSSSC},
  author       = {Whitaker, Gavin A. and Silva, Ricardo and Edwards, Daniel and Kosmidis, Ioannis},
  doi          = {10.1111/rssc.12454},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {174-201},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian approach for determining player abilities in football},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correcting misclassification errors in crowdsourced
ecological data: A bayesian perspective. <em>JRSSSC</em>,
<em>70</em>(1), 147–173. (<a
href="https://doi.org/10.1111/rssc.12453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many research domains use data elicited from ‘citizen scientists’ when a direct measure of a process is expensive or infeasible. However, participants may report incorrect estimates or classifications due to their lack of skill. We demonstrate how Bayesian hierarchical models can be used to learn about latent variables of interest, while accounting for the participants’ abilities. The model is described in the context of an ecological application that involves crowdsourced classifications of georeferenced coral-reef images from the Great Barrier Reef, Australia. The latent variable of interest is the proportion of coral cover, which is a common indicator of coral reef health. The participants’ abilities are expressed in terms of sensitivity and specificity of a correctly classified set of points on the images. The model also incorporates a spatial component, which allows prediction of the latent variable in locations that have not been surveyed. We show that the model outperforms traditional weighted-regression approaches used to account for uncertainty in citizen science data. Our approach produces more accurate regression coefficients and provides a better characterisation of the latent process of interest. This new method is implemented in the probabilistic programming language Stan and can be applied to a wide number of problems that rely on uncertain citizen science data.},
  archive      = {J_JRSSSC},
  author       = {Santos-Fernandez, Edgar and Peterson, Erin E. and Vercelloni, Julie and Rushworth, Em and Mengersen, Kerrie},
  doi          = {10.1111/rssc.12453},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {147-173},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Correcting misclassification errors in crowdsourced ecological data: A bayesian perspective},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). M-quantile regression for multivariate longitudinal data
with an application to the millennium cohort study. <em>JRSSSC</em>,
<em>70</em>(1), 122–146. (<a
href="https://doi.org/10.1111/rssc.12452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the analysis of data from the UK Millennium Cohort Study on emotional and behavioural disorders, we develop an M-quantile regression model for multivariate longitudinal responses. M-quantile regression is an appealing alternative to standard regression models; it combines features of quantile and expectile regression and it may produce a detailed picture of the conditional response variable distribution, while ensuring robustness to outlying data. As we deal with multivariate data, we need to specify what it is meant by M-quantile in this context, and how the structure of dependence between univariate profiles may be accounted for. Here, we consider univariate (conditional) M-quantile regression models with outcome-specific random effects for each outcome. Dependence between outcomes is introduced by assuming that the random effects in the univariate models are dependent. The multivariate distribution of the random effects is left unspecified and estimated from the observed data. Adopting this approach, we are able to model dependence both within and between outcomes. We further discuss a suitable model parameterisation to account for potential endogeneity of the observed covariates. An extended EM algorithm is defined to derive estimates under a maximum likelihood approach.},
  archive      = {J_JRSSSC},
  author       = {Alfò, Marco and Marino, Maria Francesca and Ranalli, Maria Giovanna and Salvati, Nicola and Tzavidis, Nikos},
  doi          = {10.1111/rssc.12452},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {122-146},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {M-quantile regression for multivariate longitudinal data with an application to the millennium cohort study},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying the trendiness of trends. <em>JRSSSC</em>,
<em>70</em>(1), 98–121. (<a
href="https://doi.org/10.1111/rssc.12451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News media often report that the trend of some public health outcome has changed. These statements are frequently based on longitudinal data, and the change in trend is typically found to have occurred at the most recent data collection time point—if no change had occurred the story is less likely to be reported. Such claims may potentially influence public health decisions on a national level. We propose two measures for quantifying the trendiness of trends. Assuming that reality evolves in continuous time, we define what constitutes a trend and a change in trend, and introduce a probabilistic Trend Direction Index. This index has the interpretation of the probability that a latent characteristic has changed monotonicity at any given time conditional on observed data. We also define an index of Expected Trend Instability quantifying the expected number of changes in trend on an interval. Using a latent Gaussian process model, we show how the Trend Direction Index and the Expected Trend Instability can be estimated in a Bayesian framework, and use the methods to analyse the proportion of smokers in Denmark during the last 20 years and the development of new COVID-19 cases in Italy from 24 February onwards.},
  archive      = {J_JRSSSC},
  author       = {Jensen, Andreas Kryger and Ekstrøm, Claus Thorn},
  doi          = {10.1111/rssc.12451},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {98-121},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Quantifying the trendiness of trends},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A non-parametric hawkes process model of primary and
secondary accidents on a UK smart motorway. <em>JRSSSC</em>,
<em>70</em>(1), 80–97. (<a
href="https://doi.org/10.1111/rssc.12450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A self-exciting spatiotemporal point process is fitted to incident data from the UK National Traffic Information Service to model the rates of primary and secondary accidents on the M25 motorway in a 12-month period during 2017–2018. This process uses a background component to represent primary accidents, and a self-exciting component to represent secondary accidents. The background consists of periodic daily and weekly components, a spatial component and a long-term trend. The self-exciting components are decaying, unidirectional functions of space and time. These components are determined via kernel smoothing and likelihood estimation. Temporally, the background is stable across seasons with a daily double peak structure reflecting commuting patterns. Spatially, there are two peaks in intensity, one of which becomes more pronounced during the study period. Self-excitation accounts for 6–7\% of the data with associated time and length scales around 100 min and 1 km, respectively. In-sample and out-of-sample validation are performed to assess the model fit. When we restrict the data to incidents that resulted in large speed drops on the network, the results remain coherent.},
  archive      = {J_JRSSSC},
  author       = {Kalair, Kieran and Connaughton, Colm and Alaimo Di Loro, Pierfrancesco},
  doi          = {10.1111/rssc.12450},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {80-97},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A non-parametric hawkes process model of primary and secondary accidents on a UK smart motorway},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Functional ensemble survival tree: Dynamic prediction of
alzheimer’s disease progression accommodating multiple time-varying
covariates. <em>JRSSSC</em>, <em>70</em>(1), 66–79. (<a
href="https://doi.org/10.1111/rssc.12449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the exponential growth in data collection, multiple time-varying biomarkers are commonly encountered in clinical studies, along with a rich set of baseline covariates. This paper is motivated by addressing a critical issue in the field of Alzheimer’s disease (AD) in which we aim to predict the time for AD conversion in people with mild cognitive impairment to inform prevention and early treatment decisions. Conventional joint models of biomarker trajectory with time-to-event data rely heavily on model assumptions and may not be applicable when the number of covariates is large. This motivated us to consider a functional ensemble survival tree framework to characterize the joint effects of both functional and baseline covariates in predicting disease progression. The proposed framework incorporates multivariate functional principal component analysis to characterize the changing patterns of multiple time-varying neurocognitive biomarker trajectories and then nest these features within an ensemble survival tree in predicting the progression of AD. We provide a fast implementation of the algorithm that accommodates personalized dynamic prediction that can be updated as new observations are gathered to reflect the patient’s latest prognosis. The algorithm is empirically shown to perform well in simulation studies and is illustrated through the analysis of data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) ( http://adni.loni.usc.edu/ ). We provide implementation of our proposed method in an R package funest.},
  archive      = {J_JRSSSC},
  author       = {Jiang, Shu and Xie, Yijun and Colditz, Graham A.},
  doi          = {10.1111/rssc.12449},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {66-79},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Functional ensemble survival tree: Dynamic prediction of alzheimer’s disease progression accommodating multiple time-varying covariates},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stacked inverse probability of censoring weighted bagging: A
case study in the InfCareHIV register. <em>JRSSSC</em>, <em>70</em>(1),
51–65. (<a href="https://doi.org/10.1111/rssc.12448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an inverse probability of censoring weighted (IPCW) bagging (bootstrap aggregation) pre-processing that enables the application of any machine learning procedure for classification to be used to predict the cause-specific cumulative incidence, properly accounting for right-censored observations and competing risks. We consider the IPCW area under the time-dependent ROC curve (IPCW-AUC) as a performance evaluation metric. We also suggest a procedure to optimally stack predictions from any set of IPCW bagged methods. We illustrate our proposed method in the Swedish InfCareHIV register by predicting individuals for whom treatment will not maintain an undetectable viral load for at least 2 years following initial suppression. The R package stackBagg that implements our proposed method is available on Github.},
  archive      = {J_JRSSSC},
  author       = {Gonzalez Ginestet, Pablo and Kotalik, Ales and Vock, David M. and Wolfson, Julian and Gabriel, Erin E.},
  doi          = {10.1111/rssc.12448},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {51-65},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Stacked inverse probability of censoring weighted bagging: A case study in the InfCareHIV register},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian varying coefficient model with selection: An
application to functional mapping. <em>JRSSSC</em>, <em>70</em>(1),
24–50. (<a href="https://doi.org/10.1111/rssc.12447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How does the genetic architecture of quantitative traits evolve over time? Answering this question is crucial for many applied fields such as human genetics and plant or animal breeding. In the last decades, high-throughput genome techniques have been used to better understand links between genetic information and quantitative traits. Recently, high-throughput phenotyping methods are also being used to provide huge information at a phenotypic scale. In particular, these methods allow traits to be measured over time, and this, for a large number of individuals. Combining both information might provide evidence on how genetic architecture evolves over time. However, such data raise new statistical challenges related to, among others, high dimensionality, time dependencies, time varying effects. In this work, we propose a Bayesian varying coefficient model allowing, in a single step, the identification of genetic markers involved in the variability of phenotypic traits and the estimation of their dynamic effects. We evaluate the use of spike-and-slab priors for the variable selection with either P-spline interpolation or non-functional techniques to model the dynamic effects. Numerical results are shown on simulations and on a functional mapping study performed on an Arabidopsis thaliana (L. Heynh) data which motivated these developments.},
  archive      = {J_JRSSSC},
  author       = {Heuclin, Benjamin and Mortier, Frédéric and Trottier, Catherine and Denis, Marie},
  doi          = {10.1111/rssc.12447},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {24-50},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian varying coefficient model with selection: An application to functional mapping},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Random effects dynamic panel models for unequally spaced
multivariate categorical repeated measures: An application to
child–parent exchanges of support. <em>JRSSSC</em>, <em>70</em>(1),
3–23. (<a href="https://doi.org/10.1111/rssc.12446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exchanges of practical or financial help between people living in different households are a major component of intergenerational exchanges within families and an increasingly important source of support for individuals in need. Using longitudinal data, bivariate dynamic panel models can be applied to study the effects of changes in individual circumstances on help given to and received from non-coresident parents and the reciprocity of exchanges. However, the use of a rotating module for collection of data on exchanges leads to data where the response measurements are unequally spaced and taken less frequently than for the time-varying covariates. Existing approaches to this problem focus on fixed effects linear models for univariate continuous responses. We propose a random effects estimator for a family of dynamic panel models that can handle continuous, binary or ordinal multivariate responses. The performance of the estimator is assessed in a simulation study. A bivariate probit dynamic panel model is then applied to estimate the effects of partnership and employment transitions in the previous year and the presence and age of children in the current year on an individual’s propensity to give or receive help. Annual data on respondents’ partnership, employment status and dependent children, and data on exchanges of help collected at 2- and 5-year intervals are used in this study.},
  archive      = {J_JRSSSC},
  author       = {Steele, Fiona and Grundy, Emily},
  doi          = {10.1111/rssc.12446},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {3-23},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Random effects dynamic panel models for unequally spaced multivariate categorical repeated measures: An application to Child–Parent exchanges of support},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applied statistics. <em>JRSSSC</em>, <em>70</em>(1), 1–2.
(<a href="https://doi.org/10.1111/rssc.12422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  author       = {Friel, N. and Illian, J. and Barber, S. and Brown, P. and Canale, A. and Demiris, N. and Fan, Y. and Huang, X. and Iannario, M. and Jaki, T. and Jermyn, I. H. and Latouche, P. and Libel, D. and Manolopoulou, I. and Marra, G. and Matechou, E. and De Menezes, R. X. and Ntzoufras, I. and Overstall, A. and Papamarkou, T. and Prangle, D. and Paoletti, X. and Radice, R. and Tzavidis, N.},
  doi          = {10.1111/rssc.12422},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {1-2},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Applied statistics},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
