<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMJNL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comjnl---135">COMJNL - 135</h2>
<ul>
<li><details>
<summary>
(2021). Detection and localization of abnormalities in surveillance
video using timerider-based neural network. <em>COMJNL</em>,
<em>64</em>(12), 1886–1906. (<a
href="https://doi.org/10.1093/comjnl/bxab002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic anomaly detection in surveillance videos is a trending research domain, which assures the detection of the anomalies effectively, relieves the time-consumed by the manual interpretation methods without the requirement of the domain knowledge about the anomalous object. Accordingly, this research work proposes an effective anomaly detection approach, named, TimeRide Neural network (TimeRideNN), by modifying the standard RideNN using the Taylor series such that an extra group of rider, named as timerider, is included in the standard rider optimization algorithm. Initially, the face in the videos is subjected to face detection using the Viola Jones algorithm. Then, the object tracking is performed using the knocker and holoentropy-based Bhattacharya distance, which is a modification of the Bhattacharya distance using the knocker and holoentropy. After that, the features, such as object-level features and speed-level features of the objects, are extracted and the features are employed to the proposed TimeRideNN classifier, which declares the anomalous objects in the video. The experimentation of the proposed anomaly detection method is done using the UCSD dataset (Ped1), subway dataset and QMUL junction dataset, and the analysis is performed based on accuracy, sensitivity and specificity. The proposed TimeRideNN classifier obtains the accuracy, sensitivity and specificity of 0.9724, 0.9894 and 0.9691, respectively.},
  archive      = {J_COMJNL},
  author       = {Veluchamy, S and Karlmarx, L R and Mahesh, K Michael},
  doi          = {10.1093/comjnl/bxab002},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1886-1906},
  shortjournal = {Comput. J.},
  title        = {Detection and localization of abnormalities in surveillance video using timerider-based neural network},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Retrieving semantic image using shape descriptors and
latent-dynamic conditional random fields. <em>COMJNL</em>,
<em>64</em>(12), 1876–1885. (<a
href="https://doi.org/10.1093/comjnl/bxaa118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new approach to semantic image retrieval using shape descriptors as dispersion and moment in conjunction with discriminative classifier model of latent-dynamic conditional random fields (LDCRFs). The target region is firstly localized via the background subtraction model. Then the features of dispersion and moments are employed to ; -means clustering to extract object’s feature as second stage. After that, the learning process is carried out by LDCRFs. Finally, simple protocol and RDF (resource description framework) query language (i.e. SPARQL) on input text or image query is to retrieve semantic image based on sequential processes of query engine, matching module and ontology manager. Experimental findings show that our approach can be successful to retrieve images against the mammal’s benchmark with retrieving rate of 98.11\%. Such outcomes are likely to compare very positively with those accessible in the literature from other researchers.},
  archive      = {J_COMJNL},
  author       = {Elmezain, Mahmoud and Ibrahem, Hani M},
  doi          = {10.1093/comjnl/bxaa118},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1876-1885},
  shortjournal = {Comput. J.},
  title        = {Retrieving semantic image using shape descriptors and latent-dynamic conditional random fields},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble technique for toxicity prediction of small drug
molecules of the antioxidant response element signalling pathway.
<em>COMJNL</em>, <em>64</em>(12), 1861–1875. (<a
href="https://doi.org/10.1093/comjnl/bxaa001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ; toxicity prediction techniques are useful to reduce rodents testing (; ). Authors have proposed a computational method (; ) for the toxicity prediction of small drug molecules using their various physicochemical properties (molecular descriptors), which can bind to the antioxidant response elements (AREs). The software PaDEL-Descriptor is used for extracting the different features of drug molecules. The ARE data set has total 7439 drug molecules, of which 1147 are active and 6292 are inactive, and each drug molecule contains 1444 features. We have proposed a novel ensemble-based model that can efficiently classify active (binding) and inactive (non-binding) compounds of the data set. Initially, we performed feature selection using random forest importance algorithm in R, and subsequently, we have resolved the class imbalance issue by ensemble learning method itself, where we divided the data set into five data frames, which have an almost equal number of active and inactive drug molecules. An ensemble model based upon the votes of four base classifiers is proposed, which gives an accuracy of 97.14\%. The K-fold cross-validation is conducted to measure the consistency of the proposed ensemble model. Finally, the proposed ensemble model is validated on some new drug molecules and compared with some existing models.},
  archive      = {J_COMJNL},
  author       = {Kumar Gupta, Vishan and Singh Rana, Prashant},
  doi          = {10.1093/comjnl/bxaa001},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1861-1875},
  shortjournal = {Comput. J.},
  title        = {Ensemble technique for toxicity prediction of small drug molecules of the antioxidant response element signalling pathway},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Primary emotions and recognition of their intensities.
<em>COMJNL</em>, <em>64</em>(12), 1848–1860. (<a
href="https://doi.org/10.1093/comjnl/bxz162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emotion recognition field has two major issues. On the one hand, it is difficult to find the same emotion state in different persons since they may express the same emotion state in various ways. On the other hand, it is also hard to seek the difference between expressions of the same person because some emotion states are too subtle to discriminate. The focus of this work is to solve these two problems by proposing a new approach of emotion recognition. This novel approach allows our emotion recognition system to classify 18 emotions (primary emotions and their intensities). First, we proposed textual definitions of the intensity emotions. Then, we created our emotion recognition system, which is composed of three stages: pre-treatment, feature extraction and classification. We used the deep learning for the feature extraction and the fuzzy logic for the classification. The experimental test demonstrates the efficiency of our system for primary emotions and their intensities’ classification compared to other methods.},
  archive      = {J_COMJNL},
  author       = {Afdhal, Rim and Ejbali, Ridha and Zaied, Mourad},
  doi          = {10.1093/comjnl/bxz162},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1848-1860},
  shortjournal = {Comput. J.},
  title        = {Primary emotions and recognition of their intensities},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effects of random forest parameters in the selection of
biomarkers. <em>COMJNL</em>, <em>64</em>(12), 1840–1847. (<a
href="https://doi.org/10.1093/comjnl/bxz161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A microarray dataset contains thousands of DNA spots covering almost every gene in the genome. Microarray-based gene expression helps with the diagnosis, prognosis and treatment of cancer. The nature of diseases frequently changes, which in turn generates a considerable volume of data. The main drawback of microarray data is the curse of dimensionality. It hinders useful information and leads to computational instability. The main objective of feature selection is to extract and remove insignificant and irrelevant features to determine the informative genes that cause cancer. Random forest is a well-suited classification algorithm for microarray data. To enhance the importance of the variables, we proposed out-of-bag (OOB) cases in every tree of the forest to count the number of votes for the exact class. The incorporation of random permutation in the variables of these OOB cases enables us to select the crucial features from high-dimensional microarray data. In this study, we analyze the effects of various random forest parameters on the selection procedure. ‘Variable drop fraction’ regulates the forest construction. The higher variable drop fraction value efficiently decreases the dimensionality of the microarray data. Forest built with 800 trees chooses fewer important features under any variable drop fraction value that reduces microarray data dimensionality.},
  archive      = {J_COMJNL},
  author       = {Khaire, Utkarsh Mahadeo and Dhanalakshmi, R},
  doi          = {10.1093/comjnl/bxz161},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1840-1847},
  shortjournal = {Comput. J.},
  title        = {Effects of random forest parameters in the selection of biomarkers},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial intelligence surpassing human intelligence:
Factual or hoax. <em>COMJNL</em>, <em>64</em>(12), 1832–1839. (<a
href="https://doi.org/10.1093/comjnl/bxz156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Khanam, Sana and Tanweer, Safdar and Khalid, Syed},
  doi          = {10.1093/comjnl/bxz156},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1832-1839},
  shortjournal = {Comput. J.},
  title        = {Artificial intelligence surpassing human intelligence: Factual or hoax},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enable traditional laptops with virtual writing capability
leveraging acoustic signals. <em>COMJNL</em>, <em>64</em>(12),
1814–1831. (<a href="https://doi.org/10.1093/comjnl/bxz153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–computer interaction through touch screens plays an increasingly important role in our daily lives. Besides smartphones and tablets, laptops are the most prevalent mobile devices for both work and leisure. To satisfy the requirements of some applications, it is desirable to re-equip a typical laptop with both handwriting and drawing capability. In this paper, we design a virtual writing tablet system, ; , for traditional laptops without touch screens. VPad leverages two speakers and one microphone, which are available in most commodity laptops, to accurately track hand movements and recognize writing characters in the air without additional hardware. Specifically, VPad emits inaudible acoustic signals from two speakers in a laptop and then analyzes energy features and Doppler shifts of acoustic signals received by the microphone to track the trajectory of hand movements. Furthermore, we propose a state machine-based trajectory optimization method to correct the unexpected trajectory and employ a stroke direction sequence model based on probability estimation to recognize characters users write in the air. Experimental results show that VPad achieves the average error of 1.55 ; for trajectory tracking and the accuracy over 90\% of character recognition merely through built-in audio devices on a laptop.},
  archive      = {J_COMJNL},
  author       = {Lu, Li and Liu, Jian and Yu, Jiadi and Chen, Yingying and Zhu, Yanmin and Kong, Linghe and Li, Minglu},
  doi          = {10.1093/comjnl/bxz153},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1814-1831},
  shortjournal = {Comput. J.},
  title        = {Enable traditional laptops with virtual writing capability leveraging acoustic signals},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian optimization approach to compute nash equilibrium
of potential games using bandit feedback. <em>COMJNL</em>,
<em>64</em>(12), 1801–1813. (<a
href="https://doi.org/10.1093/comjnl/bxz146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing a Nash equilibrium for strategic multi-agent systems is challenging for black box systems. Motivated by the ubiquity of games involving exploitation of common resources, this paper considers the above problem for potential games. We use a Bayesian optimization framework to obtain novel algorithms to solve finite (discrete action spaces) and infinite (real interval action spaces) potential games, utilizing the structure of potential games. Numerical results illustrate the efficiency of the approach in computing a Nash equilibrium of static potential games and linear Nash equilibrium of dynamic potential games.},
  archive      = {J_COMJNL},
  author       = {Aprem, Anup and Roberts, Stephen},
  doi          = {10.1093/comjnl/bxz146},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1801-1813},
  shortjournal = {Comput. J.},
  title        = {A bayesian optimization approach to compute nash equilibrium of potential games using bandit feedback},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ultrasound image despeckling and enhancement using modified
multiscale anisotropic diffusion model in non-subsampled shearlet
domain. <em>COMJNL</em>, <em>64</em>(12), 1785–1800. (<a
href="https://doi.org/10.1093/comjnl/bxz131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound imaging is undoubtedly the most used imaging modality for diagnostic purposes. Unfortunately, it is accompanied by speckle which can degrade texture information by obscuring fine details like boundaries and edges. This work presents a method for despeckling ultrasound images by treating them with multiscale modified speckle reduction anisotropic diffusion model and Non-Subsampled shearlet transform (NSST). The method involves division of images using a non-subsampled Laplacian pyramid. This results in low and high frequency image components. Modified anisotropic diffusion is used on the low frequency part. The high frequency component, as subjected to shearlet function, generates noisy coefficients in various directions. These coefficients are further subjected to NSST thresholding. The denoised low and high frequency image components are then recombined to obtain the enhanced image. This multidimensional and multidirectional method improves the qualitative characteristics of ultrasound images by not just removing speckle noise but also by preserving edges, thus resulting in effective image enhancement. Performance of the method is analysed on synthetic and real medical ultrasound images. Results reveal that the proposed method exceeds the state-of-the-art methods in the context of edge preservation and structural similarities, and thus, it is an effective aid to radiologists in their clinical diagnosis by providing an enhanced denoised image.},
  archive      = {J_COMJNL},
  author       = {Bedi, Anterpreet Kaur and Sunkaria, Ramesh Kumar and Mittal, Deepti},
  doi          = {10.1093/comjnl/bxz131},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1785-1800},
  shortjournal = {Comput. J.},
  title        = {Ultrasound image despeckling and enhancement using modified multiscale anisotropic diffusion model in non-subsampled shearlet domain},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computer-aided diagnosis system for chronic obstructive
pulmonary disease using empirical wavelet transform on auscultation
sounds. <em>COMJNL</em>, <em>64</em>(11), 1775–1783. (<a
href="https://doi.org/10.1093/comjnl/bxaa191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, it is aimed to develop computer-aided a diagnosis system for Chronic Obstructive Pulmonary Disease (COPD) which is a completely incurable and chronic disease. The COPD causes obstructions of the airways in the lungs by arising air pollution environments. Contributing analysis of abnormalities in simple ways is very important to shorten the duration of treatment by early diagnosis. The most common diagnostic method for respiratory disorders is auscultation sounds. These sounds are also essential and effective signals for diagnosing the COPD. The analysis was performed using signals from the RespiratoryDatabase@TR which consists of 12-channel lung sounds. In the computerized analysis, Empirical Wavelet Transform (EWT) algorithm was applied to the signals for extracting different modes. Afterwards the statistical features were extracted from each EWT modulation. The highest classification performances were achieved with the rates of 90.41\%, 95.28\%, 90.56\% and 85.78\% for Support Vector Machine, AdaBoost, Random Forest and J48 Decision Tree, respectively. The contribution of the study is reducing the diagnosis time to 5 seconds within higher accuracy rate.},
  archive      = {J_COMJNL},
  author       = {Gökçen, Ahmet},
  doi          = {10.1093/comjnl/bxaa191},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1775-1783},
  shortjournal = {Comput. J.},
  title        = {Computer-aided diagnosis system for chronic obstructive pulmonary disease using empirical wavelet transform on auscultation sounds},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal feature selection and hybrid classification for
autism detection in young children. <em>COMJNL</em>, <em>64</em>(11),
1760–1774. (<a href="https://doi.org/10.1093/comjnl/bxaa156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early detection of autism spectrum disorder acts as a risk in the infants and toddlers as per the increase over the early invention awareness. Hence, this paper has made an effort to introduce a new autism detection technique in young children, which poses three major phases called weighted logarithmic transformation, optimal feature selection and classification. Initially, weighted transformation in the input data is carried out that correctly distinguishes the interclass labels, and it is determined to be the specified features. Because of the presence of numerous amounts of features, the ‘prediction’ becomes a serious issue, and therefore the optimal selection of features is important. Here, for optimal feature selection process, a new Levi Flight Cub Update-based lion algorithm (LFCU-LA) is introduced that can be a modification in lion algorithm. Once the optimal features are selected, they are given to the classification process that exploits a hybrid classifier: deep belief network (DBN) and neural network (NN). Additionally, the most important contributions in the hidden neurons of DBN and NN were optimally selected that paves way for exact detection.},
  archive      = {J_COMJNL},
  author       = {Guruvammal, S and Chellatamilan, T and Deborah, L Jegatha},
  doi          = {10.1093/comjnl/bxaa156},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1760-1774},
  shortjournal = {Comput. J.},
  title        = {Optimal feature selection and hybrid classification for autism detection in young children},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thermal image-based object classification for guiding the
visually impaired. <em>COMJNL</em>, <em>64</em>(11), 1747–1759. (<a
href="https://doi.org/10.1093/comjnl/bxaa097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {.},
  archive      = {J_COMJNL},
  author       = {Nancy, V and Balakrishnan, G},
  doi          = {10.1093/comjnl/bxaa097},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1747-1759},
  shortjournal = {Comput. J.},
  title        = {Thermal image-based object classification for guiding the visually impaired},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corporate bankruptcy prediction: An approach towards better
corporate world. <em>COMJNL</em>, <em>64</em>(11), 1731–1746. (<a
href="https://doi.org/10.1093/comjnl/bxaa056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area of corporate bankruptcy prediction attains high economic importance, as it affects many stakeholders. The prediction of corporate bankruptcy has been extensively studied in economics, accounting and decision sciences over the past two decades. The corporate bankruptcy prediction has been a matter of talk among academic literature and professional researchers throughout the world. Different traditional approaches were suggested based on hypothesis testing and statistical modeling. Therefore, the primary purpose of the research is to come up with a model that can estimate the probability of corporate bankruptcy by evaluating its occurrence of failure using different machine learning models. As the dataset was not well prepared and contains missing values, various data mining and data pre-processing techniques were utilized for data preparation. Within this research, the task of resolving the issues induced by the imbalance between the two classes is approached by applying different data balancing techniques. We address the problem of imbalanced data with the random undersampling and Synthetic Minority Over Sampling Technique (SMOTE). We used five machine learning models (support vector machine, J48 decision tree, Logistic model tree, random forest and decision forest) to predict corporate bankruptcy earlier to the occurrence. We use data from 2009 to 2013 on Poland manufacturing corporates and selected the 64 financial indicators to be broken down. The main finding of the study is a significant improvement in predictive accuracy using machine learning techniques. We also include other economic indicators ratios, along with Altman’s ; -score variables related to profitability, liquidity, leverage and solvency (short/long term) to propose an efficient model. Machine learning models give better results while balancing the data through SMOTE as compared to random undersampling. The machine learning technique related to decision forest led to 99\% accuracy, whereas support vector machine (SVM), J48 decision tree, Logistic Model Tree (LMT) and Random Forest (RF) led to 92\%, 92.3\%, 93.8\% and 98.7\% accuracy, respectively, with all predictive financial indicators. We find that the decision forest outperforms the other techniques and previous techniques discussed in the literature. The proposed method is also deployed on the web to assist regulators, investors, creditors and scholars to predict corporate bankruptcy.},
  archive      = {J_COMJNL},
  author       = {Alam, Talha Mahboob and Shaukat, Kamran and Mushtaq, Mubbashar and Ali, Yasir and Khushi, Matloob and Luo, Suhuai and Wahab, Abdul},
  doi          = {10.1093/comjnl/bxaa056},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1731-1746},
  shortjournal = {Comput. J.},
  title        = {Corporate bankruptcy prediction: An approach towards better corporate world},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance improvement of pre-trained convolutional neural
networks for action recognition. <em>COMJNL</em>, <em>64</em>(11),
1715–1730. (<a href="https://doi.org/10.1093/comjnl/bxaa029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition is a challenging task. Deep learning models have been investigated to solve this problem. Setting up a new neural network model is a crucial and time-consuming process. Alternatively, pre-trained convolutional neural network (CNN) models offer rapid modeling. The selection of the hyperparameters of CNNs is a challenging issue that heavily depends on user experience. The parameters of CNNs should be carefully selected to get effective results. For this purpose, the artificial bee colony (ABC) algorithm is used for tuning the parameters to get optimum results. The proposed method includes three main stages: the image preprocessing stage involves automatic cropping of the meaningful area within the images in the data set, the transfer learning stage includes experiments with six different pre-trained CNN models and the hyperparameter tuning stage using the ABC algorithm. Performance comparison of the pre-trained CNN models involving the use and nonuse of the ABC algorithm for the Stanford 40 data set is presented. The experiments show that the pre-trained CNN models with ABC are more successful than pre-trained CNN models without ABC. Additionally, to the best of our knowledge, the improved NASNet-Large CNN model with the ABC algorithm gives the best accuracy of 87.78\% for the overall success rate-based performance metric.},
  archive      = {J_COMJNL},
  author       = {Ozcan, Tayyip and Basturk, Alper},
  doi          = {10.1093/comjnl/bxaa029},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1715-1730},
  shortjournal = {Comput. J.},
  title        = {Performance improvement of pre-trained convolutional neural networks for action recognition},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Character recognition algorithm based on fusion probability
model and deep learning. <em>COMJNL</em>, <em>64</em>(11), 1705–1714.
(<a href="https://doi.org/10.1093/comjnl/bxaa025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors proposed a character recognition algorithm to improve the recognition efficiency and recognition accuracy in character recognition. The algorithm is based on a deep belief network classifier. In the character recognition, a comprehensive feature model is established firstly by combining the histogram Gabor feature, grid level feature and gray level co-occurrence matrix. Subsequently, the deep belief network trains the feature model. Finally, the probability model is used to judge the recognition symbols. The algorithm is tested with 74 k data set and is compared and analyzed from three indexes: false acceptance rate, false rejection rate and accuracy. The data set simulation and comparison with other algorithms indicate that the recognition system based on the probability model and depth learning has higher accuracy and better performance.},
  archive      = {J_COMJNL},
  author       = {Liu, Zhijun and Pan, Xuefeng and Peng, Yuan},
  doi          = {10.1093/comjnl/bxaa025},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1705-1714},
  shortjournal = {Comput. J.},
  title        = {Character recognition algorithm based on fusion probability model and deep learning},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical categorization and review of recent techniques
on image forgery detection. <em>COMJNL</em>, <em>64</em>(11), 1692–1704.
(<a href="https://doi.org/10.1093/comjnl/bxz148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information in the form of the image conveys more details than any other form of information. Several software packages are available to manipulate the images so that the authenticity of the images is being questioned. Several image processing approaches are available to create fake images without leaving any visual clue about the forging operation. So, proper image forgery detection tools are required to detect such forgery images. Over the past few years, several research papers were published in the digital image forensics domain for detecting fake images, thus escalating the legitimacy of the images. This survey paper attempts to review the recent approaches proposed for detecting image forgery. Accordingly, several research papers related to image forgery detection are reviewed and analyzed. The taxonomy of image forgery detection techniques is presented, and the algorithms related to each technique are discussed. The comprehensive analysis is carried out based on the dataset used, software used for the implementation and the performance achievement. Besides, the research issues associated with every approach were scrutinized together with the recommendation for future work.},
  archive      = {J_COMJNL},
  author       = {Vinolin, V and Sucharitha, M},
  doi          = {10.1093/comjnl/bxz148},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1692-1704},
  shortjournal = {Comput. J.},
  title        = {Hierarchical categorization and review of recent techniques on image forgery detection},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A content-based model for tag recommendation in software
information sites. <em>COMJNL</em>, <em>64</em>(11), 1680–1691. (<a
href="https://doi.org/10.1093/comjnl/bxz144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developers use software information sites such as Stack Overflow to get and give information on various subjects. These sites allow developers to label content with tags as a short description. Tags, then, are used to describe, categorize and search the posted content. However, tags might be noisy, and postings may become poorly categorized since people tag a posting based on their knowledge of its content and other existing tags. To keep the content well organized, tag recommendation systems can help users by suggesting appropriate tags for their posted content. In this paper, we propose a tag recommendation scheme that uses the textual content of already tagged postings to recommend suitable tags for newly posted content. Our approach combines multi-label classification and textual similarity techniques to improve the performance of tag recommendation. We evaluate the performance of the proposed scheme on 11 software information sites from the Stack Exchange network. The results show a significant improvement over TagCombine, TagMulRec and FastTagRec, which are well-known tag recommendation systems. On average, the proposed model outperforms TagCombine, TagMulRec and FastTagRec by 26.2, 15.9 and 13.8\% in terms of Recall@5 and by 16.9, 12.4 and 9.4\% in terms of Recall@10, respectively.},
  archive      = {J_COMJNL},
  author       = {Gharibi, Reza and Safdel, Atefeh and Fakhrahmad, Seyed Mostafa and Sadreddini, Mohammad Hadi},
  doi          = {10.1093/comjnl/bxz144},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1680-1691},
  shortjournal = {Comput. J.},
  title        = {A content-based model for tag recommendation in software information sites},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive IoT empowered smart road traffic congestion control
system using supervised machine learning algorithm. <em>COMJNL</em>,
<em>64</em>(11), 1672–1679. (<a
href="https://doi.org/10.1093/comjnl/bxz129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of smart systems blessed with different technologies can enable many algorithms used in Machine Learning (ML) and the world of the Internet of Things (IoT). In a modern city many different sensors can be used for information collection. Algorithms that are cast-off in Machine Learning improves the capabilities and intelligence of a system when the amount of data collectedincreases. In this research, we propose a TCC-SVM system model to analyse traffic congestion in the environment of a smart city. The proposed model comprises an ML-enabled IoT-based road traffic congestion control system whereby the occurrence of congestion at a specific point is notified.},
  archive      = {J_COMJNL},
  author       = {Ata, Ayesha and Khan, Muhammad Adnan and Abbas, Sagheer and Khan, Muhammad Saleem and Ahmad, Gulzar},
  doi          = {10.1093/comjnl/bxz129},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1672-1679},
  shortjournal = {Comput. J.},
  title        = {Adaptive IoT empowered smart road traffic congestion control system using supervised machine learning algorithm},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust image hashing with singular values of quaternion SVD.
<em>COMJNL</em>, <em>64</em>(11), 1656–1671. (<a
href="https://doi.org/10.1093/comjnl/bxz127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hashing is an efficient technique of many multimedia systems, such as image retrieval, image authentication and image copy detection. Classification between robustness and discrimination is one of the most important performances of image hashing. In this paper, we propose a robust image hashing with singular values of quaternion singular value decomposition (QSVD). The key contribution is the innovative use of QSVD, which can extract stable and discriminative image features from CIE L; a; b; color space. In addition, image features of a block are viewed as a point in the Cartesian coordinates and compressed by calculating the Euclidean distance between its point and a reference point. As the Euclidean distance requires smaller storage than the original block features, this technique helps to make a discriminative and compact hash. Experiments with three open image databases are conducted to validate efficiency of our image hashing. The results demonstrate that our image hashing can resist many digital operations and reaches a good discrimination. Receiver operating characteristic curve comparisons illustrate that our image hashing outperforms some state-of-the-art algorithms in classification performance.},
  archive      = {J_COMJNL},
  author       = {Tang, Zhenjun and Yu, Mengzhu and Yao, Heng and Zhang, Hanyun and Yu, Chunqiang and Zhang, Xianquan},
  doi          = {10.1093/comjnl/bxz127},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1656-1671},
  shortjournal = {Comput. J.},
  title        = {Robust image hashing with singular values of quaternion SVD},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving human action recognition using hierarchical
features and multiple classifier ensembles. <em>COMJNL</em>,
<em>64</em>(11), 1633–1655. (<a
href="https://doi.org/10.1093/comjnl/bxz123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a simple, fast and efficacious system to promote the human action classification outcome using the depth action sequences. Firstly, the motion history images (MHIs) and static history images (SHIs) are created from the front (XOY), side (YOZ) and top (XOZ) projected scenes of each depth sequence in a 3D Euclidean space through engaging the 3D Motion Trail Model (3DMTM). Then, the Local Binary Patterns (LBPs) algorithm is operated on the MHIs and SHIs to learn motion and static hierarchical features to represent the action sequence. The motion and static hierarchical feature vectors are then fed into a classifier ensemble to classify action classes, where the ensemble comprises of two classifiers. Thus, each ensemble includes a pair of Kernel-based Extreme Learning Machine (KELM) or ; -regularized Collaborative Representation Classifier (; -CRC) or Multi-class Support Vector Machine. To extensively assess the framework, we perform experiments on a couple of standard available datasets such as ; , ; and ; . Experimental consequences demonstrate that the proposed approach gains a state-of-the-art recognition performance in comparison with other available approaches. Several statistical measurements on recognition results also indicate that the method achieves superiority when the hierarchical features are adopted with the KELM ensemble. In addition, to ensure real-time processing capability of the algorithm, the running time of major components is investigated. Based on machine dependency of the running time, the computational complexity of the system is also shown and compared with other methods. Experimental results and evaluation of the computational time and complexity reflect real-time compatibility and feasibility of the proposed system.},
  archive      = {J_COMJNL},
  author       = {Bulbul, Mohammad Farhad and Islam, Saiful and Zhou, Yatong and Ali, Hazrat},
  doi          = {10.1093/comjnl/bxz123},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1633-1655},
  shortjournal = {Comput. J.},
  title        = {Improving human action recognition using hierarchical features and multiple classifier ensembles},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on prediction of suicidal ideation using machine
and ensemble learning. <em>COMJNL</em>, <em>64</em>(11), 1617–1632. (<a
href="https://doi.org/10.1093/comjnl/bxz120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suicide is a major health issue nowadays and has become one of the highest reason for deaths. There are many negative emotions like anxiety, depression, stress that can lead to suicide. By identifying the individuals having suicidal ideation beforehand, the risk of them completing suicide can be reduced. Social media is increasingly becoming a powerful platform where people around the world are sharing emotions and thoughts. Moreover, this platform in some way is working as a catalyst for invoking and inciting the suicidal ideation. The objective of this proposal is to use social media as a tool that can aid in preventing the same. Data is collected from Twitter, a social networking site using some features that are related to suicidal ideation. The tweets are preprocessed as per the semantics of the identified features and then it is converted into probabilistic values so that it will be suitably used by machine learning and ensemble learning algorithms. Different machine learning algorithms like Bernoulli Naïve Bayes, Multinomial Naïve Bayes, Decision Tree, Logistic Regression, Support Vector Machine were applied on the data to predict and identify trends of suicidal ideation. Further the proposed work is evaluated with some ensemble approaches like Random Forest, AdaBoost, Voting Ensemble to see the improvement.},
  archive      = {J_COMJNL},
  author       = {Chadha, Akshma and Kaushik, Baijnath},
  doi          = {10.1093/comjnl/bxz120},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1617-1632},
  shortjournal = {Comput. J.},
  title        = {A survey on prediction of suicidal ideation using machine and ensemble learning},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SS6: Online short-code RAID-6 scaling by optimizing new disk
location and data migration. <em>COMJNL</em>, <em>64</em>(10),
1600–1616. (<a href="https://doi.org/10.1093/comjnl/bxab134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to excellent reliability, availability, flexibility and scalability, ; edundant ; rrays of ; ndependent (or ; nexpensive) ; isks (RAID) are widely deployed in large-scale data centers. RAID scaling effectively relieves the storage pressure of the data center and increases both the capacity and I/O parallelism of storage systems. To regain load balancing among all disks including old and new, some data usually are migrated from old disks to new disks. Owing to unique parity layouts of erasure codes, traditional scaling approaches may incur high migration overhead on RAID-6 scaling. This paper proposes an efficient approach based Short-Code for RAID-6 scaling. The approach exhibits three salient features: first, SS6 introduces ; to determine where new disks should be inserted. Second, SS6 minimizes migration overhead by delineating migration areas. Third, SS6 reduces the XOR calculation cost by optimizing parity update. The numerical results and experiment results demonstrate that (i) SS6 reduces the amount of data migration and improves the scaling performance compared with Round-Robin and Semi-RR under offline, (ii) SS6 decreases the total scaling time against Round-Robin and Semi-RR under two real-world I/O workloads (iii) the user average response time of SS6 is better than the other two approaches during scaling and after scaling.},
  archive      = {J_COMJNL},
  author       = {Yuan, Zhu and You, Xindong and Lv, Xueqiang and Xie, Ping},
  doi          = {10.1093/comjnl/bxab134},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1600-1616},
  shortjournal = {Comput. J.},
  title        = {SS6: Online short-code RAID-6 scaling by optimizing new disk location and data migration},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Be scalable and rescue my slices during reconfiguration.
<em>COMJNL</em>, <em>64</em>(10), 1584–1599. (<a
href="https://doi.org/10.1093/comjnl/bxab108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern 5G networks promise more bandwidth, less delay and more flexibility for an ever increasing number of users and applications, with Software Defined Networking, Network Function Virtualization and Network Slicing as key enablers. Within that context, efficiently provisioning the network and cloud resources of a wide variety of applications with dynamic user demand is a real challenge. We study here the network slice reconfiguration problem. Reconfiguring network slices from time to time reduces network operational costs and increases the number of slices that can be managed within the network. However, this affect the ; of users during the reconfiguration step. To solve this issue, we study solutions implementing a ; scheme. We propose new models and scalable algorithms (relying on column generation techniques) that solve large data instances in few seconds.},
  archive      = {J_COMJNL},
  author       = {Gausseran, Adrien and Giroire, Frederic and Jaumard, Brigitte and Moulierac, Joanna},
  doi          = {10.1093/comjnl/bxab108},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1584-1599},
  shortjournal = {Comput. J.},
  title        = {Be scalable and rescue my slices during reconfiguration},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A collaborative learning-based algorithm for task offloading
in UAV-aided wireless sensor networks. <em>COMJNL</em>, <em>64</em>(10),
1575–1583. (<a href="https://doi.org/10.1093/comjnl/bxab100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unmanned aerial vehicles (UAVs) have emerged to enhance data processing, network monitoring, disaster management and other useful applications in many different networks. Due to their flexibility, cost efficiency and powerful capabilities, combining these UAVs with the existing wireless sensor networks (WSNs) could improve network performance and enhance the network lifetime in such networks. In this research, we propose a task offloading mechanism in UAV-aided WSN by implementing a utility-based learning collaborative algorithm that will enhance the service satisfaction rate, taking into account the delay requirements of the submitted tasks. The proposed learning algorithm predicts the queuing delays of all UAVs instead of having a global overview of the system, which reduces the communication overhead in the network. The simulation results showed the effectiveness of our proposed work in terms of service satisfaction ratio compared with the non-collaborative algorithm that only processes the task locally in the WSN cluster.},
  archive      = {J_COMJNL},
  author       = {Al-Share, Rama and Shurman, Mohammad and Alma’aitah, Abdallah},
  doi          = {10.1093/comjnl/bxab100},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1575-1583},
  shortjournal = {Comput. J.},
  title        = {A collaborative learning-based algorithm for task offloading in UAV-aided wireless sensor networks},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contact tracing solution for global community.
<em>COMJNL</em>, <em>64</em>(10), 1565–1574. (<a
href="https://doi.org/10.1093/comjnl/bxab099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are several ; solutions since the outbreak of SARS ; . All these solutions are localized—specific to a country. The Apps supported by these solutions do not interwork with each other. There are no standards to the ; collected by these Apps. Once the international travel restrictions are relaxed, this will become an issue. This paper explores this issue, by addressing one of the key requirements of ; . All the current solutions use an ; , ; (PID), that anonymously represents the user in the proximity data exchanged. The PID used in these applications varies in their structure, management and properties. This paper first identifies the common desirable properties of PID, including some non-obvious ones for its global application. This identification is essential for the design and development of ; solution that can work across boundaries seamlessly. The paper also evaluates representative solutions from two different design classes against these properties.},
  archive      = {J_COMJNL},
  author       = {Narayanan, Hari T S},
  doi          = {10.1093/comjnl/bxab099},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1565-1574},
  shortjournal = {Comput. J.},
  title        = {Contact tracing solution for global community},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pruning of health data in mobile-assisted remote healthcare
service delivery. <em>COMJNL</em>, <em>64</em>(10), 1549–1564. (<a
href="https://doi.org/10.1093/comjnl/bxab083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of cloud computing and mobile devices is increasing in healthcare service delivery primarily because of the huge storage capacity of cloud, the heterogeneous structure of health data and the user-friendly interfaces on mobile devices. We propose a healthcare delivery scheme where a large knowledge base is stored in the cloud and user responses from mobile devices are input to this knowledge base to reach a preliminary diagnosis of diseases based on patients’ symptoms. However, instead of sending every response to the cloud and getting data from cloud server, it may often be desirable to prune a portion of the knowledge base that is stored in a graph form and download in to the mobile devices. Downloading data from cloud depends on the storage, battery power, processor of a mobile device, wireless network bandwidth and cloud processor capacity. In this paper, we focus on developing mathematical expressions involving the above mentioned criteria and show how these parameters are dependent on each other. The expressions built in this paper can be used in real-life scenarios to take decisions regarding the amount of data to be pruned in order to save energy as well as time.},
  archive      = {J_COMJNL},
  author       = {Mondal, Safikureshi and Mukherjee, Nandini},
  doi          = {10.1093/comjnl/bxab083},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1549-1564},
  shortjournal = {Comput. J.},
  title        = {Pruning of health data in mobile-assisted remote healthcare service delivery},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust object detection and localization using semantic
segmentation network. <em>COMJNL</em>, <em>64</em>(10), 1531–1548. (<a
href="https://doi.org/10.1093/comjnl/bxab079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancements in the area of object localization are in great progress for analyzing the spatial relations of different objects from the set of images. Several object localization techniques rely on classification, which decides, if the object exist or not, but does not provide the object information using pixel-wise segmentation. This work introduces an object detection and localization technique using semantic segmentation network (SSN) and deep convolutional neural network (Deep CNN). Here, the proposed technique consists of the following steps: Initially, the image is denoised using the filtering to eliminate the noise present in the image. Then, pre-processed image undergoes sparking process for making the image suitable for the segmentation using SSN for object segmentation. The obtained segments are subjected as the input to the proposed Stochastic-Cat Crow optimization (Stochastic-CCO)-based Deep CNN for the object classification. Here, the proposed Stochastic-CCO, obtained by integrating stochastic gradient descent and the CCO, is used for training the Deep CNN. The CCO is designed by the integration of cat swarm optimization (CSO) and crow search algorithm and takes advantages of both optimization algorithms. The experimentation proves that the proposed Stochastic-CCO-based Deep CNN-based technique acquired maximal accuracy of 98.7.},
  archive      = {J_COMJNL},
  author       = {Francis Alexander Raghu, A and Ananth, J P},
  doi          = {10.1093/comjnl/bxab079},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1531-1548},
  shortjournal = {Comput. J.},
  title        = {Robust object detection and localization using semantic segmentation network},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Severity level classification of brain tumor based on MRI
images using fractional-chicken swarm optimization algorithm.
<em>COMJNL</em>, <em>64</em>(10), 1514–1530. (<a
href="https://doi.org/10.1093/comjnl/bxab057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor classification is highly effective in identifying and diagnosing the exact location of the tumor in the brain. The medical imaging system reported that early diagnosis and classification of the tumor increases the life of the human. Among various imaging modalities, magnetic resonance imaging (MRI) is highly used by clinical experts, as it offers contrast information of brain tumors. An effective classification method named fractional-chicken swarm optimization (fractional-CSO) is introduced to perform the severity-level tumor classification. Here, the chicken swarm behavior is merged with the derivative factor to enhance the accuracy of severity level classification. The optimal solution is obtained by updating the position of the rooster, which updates their location based on better fitness value. The brain images are pre-processed and the features are effectively extracted, and the cancer classification is carried out. Moreover, the severity level of tumor classification is performed using the deep recurrent neural network, which is trained by the proposed fractional-CSO algorithm. Moreover, the performance of the proposed fractional-CSO attained better performance in terms of the evaluation metrics, such as accuracy, specificity and sensitivity with the values of 93.35, 96 and 95\% using simulated BRATS dataset, respectively.},
  archive      = {J_COMJNL},
  author       = {Cristin, Dr R and Kumar, Dr K Suresh and Anbhazhagan, Dr P},
  doi          = {10.1093/comjnl/bxab057},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1514-1530},
  shortjournal = {Comput. J.},
  title        = {Severity level classification of brain tumor based on MRI images using fractional-chicken swarm optimization algorithm},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Underwater image enhancement with optimal histogram using
hybridized particle swarm and dragonfly. <em>COMJNL</em>,
<em>64</em>(10), 1494–1513. (<a
href="https://doi.org/10.1093/comjnl/bxab056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typically, underwater image processing is mainly concerned with balancing the color change distortion or light scattering. Various researches have been processed under these issues. This proposed model incorporates two phases, namely, contrast correction and color correction. Moreover, two processes are involved within the contrast correction model, namely: (i) global contrast correction and (ii) local contrast correction. For the image enhancement, the main target is on the histogram evaluation, and therefore, the optimal selection of histogram limit is very essential. For this optimization purpose, a new hybrid algorithm is introduced namely, swarm updated Dragonfly Algorithm, which is the hybridization of Particle Swarm Optimization (PSO) and Dragonfly Algorithm (DA). Further, this paper mainly focused on Integrated Global and Local Contrast Correction (IGLCC). The proposed model is finally distinguished over the other conventional models like Contrast Limited Adaptive Histogram, IGLCC, dynamic stretching IGLCC-Genetic Algorithm, IGLCC-PSO, IGLCC- Firefly and IGLCC-Cuckoo Search, IGLCC-Distance-Oriented Cuckoo Search and DA, and the superiority of the proposed work is proved.},
  archive      = {J_COMJNL},
  author       = {Prasath, R and Kumanan, T},
  doi          = {10.1093/comjnl/bxab056},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1494-1513},
  shortjournal = {Comput. J.},
  title        = {Underwater image enhancement with optimal histogram using hybridized particle swarm and dragonfly},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy-efficient cluster-based routing protocol for WSN
based on hybrid BSO–TLBO optimization model. <em>COMJNL</em>,
<em>64</em>(10), 1477–1493. (<a
href="https://doi.org/10.1093/comjnl/bxab044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most famous wireless sensor networks is one of the cheapest and rapidly evolving networks in modern communication. It can be used to sense various substantial and environmental specifications by providing cost-effective sensor devices. The development of these sensor networks is exploited to provide an energy-efficient weighted clustering method to increase the lifespan of the network. We propose a novel energy-efficient method, which utilizes the brainstorm algorithm in order to adopt the ideal cluster head (CH) to reduce energy draining. Furthermore, the effectiveness of the BrainStorm Optimization (BSO) algorithm is enhanced with the incorporation of the modified teacher–learner optimized (MTLBO) algorithm with it. The modified BSO–MTLBO algorithm can be used to attain an improved throughput, network lifetime, and to reduce the energy consumption by nodes and CH, death of sensor nodes, routing overhead. The performance of our proposed work is analyzed with other existing approaches and inferred that our approach performs better than all the other approaches.},
  archive      = {J_COMJNL},
  author       = {Krishnan, Kannan and Yamini, B and Alenazy, Wael Mohammad and Nalini, M},
  doi          = {10.1093/comjnl/bxab044},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1477-1493},
  shortjournal = {Comput. J.},
  title        = {Energy-efficient cluster-based routing protocol for WSN based on hybrid BSO–TLBO optimization model},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying influential nodes in complex networks based on
neighborhood entropy centrality. <em>COMJNL</em>, <em>64</em>(10),
1465–1476. (<a href="https://doi.org/10.1093/comjnl/bxab034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying influential nodes is a fundamental and open issue in analysis of the complex networks. The measurement of the spreading capabilities of nodes is an attractive challenge in this field. Node centrality is one of the most popular methods used to identify the influential nodes, which includes the degree centrality (DC), betweenness centrality (BC) and closeness centrality (CC). The DC is an efficient method but not effective. The BC and CC are effective but not efficient. They have high computational complexity. To balance the effectiveness and efficiency, this paper proposes the neighborhood entropy centrality to rank the influential nodes. The proposed method uses the notion of entropy to improve the DC. For evaluating the performance, the susceptible-infected-recovered model is used to simulate the information spreading process of messages on nine real-world networks. The experimental results reveal the accuracy and efficiency of the proposed method.},
  archive      = {J_COMJNL},
  author       = {Qiu, Liqing and Zhang, Jianyi and Tian, Xiangbo and Zhang, Shuang},
  doi          = {10.1093/comjnl/bxab034},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1465-1476},
  shortjournal = {Comput. J.},
  title        = {Identifying influential nodes in complex networks based on neighborhood entropy centrality},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The conditional reliability evaluation of data center
network BCDC. <em>COMJNL</em>, <em>64</em>(9), 1451–1464. (<a
href="https://doi.org/10.1093/comjnl/bxaa078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of servers in a data center network (DCN) increases, the probability of server failures is significantly increased. Traditional connectivity is an important metric to measure the reliability of DCN. However, the traditional connectivity of a DCN based on the condition of arbitrary faulty servers is generally lower. Therefore, it is important to increase the connectivity of a DCN by adding some limited conditions for the faulty server set. As a result, ; -restricted connectivity and ; -extra connectivity, which are two crucial subjects for a DCN’s ability to tolerate faulty servers, were proposed in the literature. In this paper, we study the ; -restricted connectivity and ; -extra connectivity of a new server-centric DCN, called BCDC, based on crossed cube with excellent performance. We prove that the ; -restricted connectivity of BCDC is 4 for ; and ; for ; , where ; , and the ; -extra connectivity of BCDC is 4 for ; and ; for ; , where ; .},
  archive      = {J_COMJNL},
  author       = {Lv, Mengjie and Cheng, Baolei and Fan, Jianxi and Wang, Xi and Zhou, Jingya and Yu, Jia},
  doi          = {10.1093/comjnl/bxaa078},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1451-1464},
  shortjournal = {Comput. J.},
  title        = {The conditional reliability evaluation of data center network BCDC},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LUISA: Decoupling the frequency model from the context model
in prediction-based compression. <em>COMJNL</em>, <em>64</em>(9),
1437–1450. (<a href="https://doi.org/10.1093/comjnl/bxaa074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction-based compression methods, like prediction by partial matching, achieve a remarkable compression ratio, especially for texts written in natural language. However, they are not efficient in terms of speed. Part of the problem concerns the usage of dynamic entropy encoding, which is considerably slower than the static alternatives. In this paper, we propose a prediction-based compression method that decouples the context model from the frequency model. The separation allows static entropy encoding to be used without a significant overhead in the meta-data embedded in the compressed data. The result is a reasonably efficient algorithm that is particularly suited for small textual files, as the experiments show. We also show it is relatively easy to built strategies designed to handle specific cases, like the compression of files whose symbols are only locally frequent.},
  archive      = {J_COMJNL},
  author       = {Fulber-Garcia, Vinicius and Sardi Mergen, Sérgio Luis},
  doi          = {10.1093/comjnl/bxaa074},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1437-1450},
  shortjournal = {Comput. J.},
  title        = {LUISA: Decoupling the frequency model from the context model in prediction-based compression},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliability analysis of alternating group graphs and
split-stars. <em>COMJNL</em>, <em>64</em>(9), 1425–1436. (<a
href="https://doi.org/10.1093/comjnl/bxaa070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a connected graph ; and a positive integer ; , the ; -extra (resp. ; -component) edge connectivity of ; , denoted by ; (resp. ; ), is the minimum number of edges whose removal from ; results in a disconnected graph so that every component has more than ; vertices (resp. so that it contains at least ; components). This naturally generalizes the classical edge connectivity of graphs defined in term of the minimum edge cut. In this paper, we proposed a general approach to derive component (resp. extra) edge connectivity for a connected graph ; . For a connected graph ; , let ; be a vertex subset of ; for ; such that ; , ; is connected and ; , then we prove that ; and ; for ; . By exploring the reliability analysis of ; and ; based on extra (component) edge faults, we obtain the following results: (i) ; , ; and ; (ii) ; , ; and ; . This general approach maybe applied to many diverse networks.},
  archive      = {J_COMJNL},
  author       = {Gu, Mei-Mei and Hao, Rong-Xia and Chang, Jou-Ming},
  doi          = {10.1093/comjnl/bxaa070},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1425-1436},
  shortjournal = {Comput. J.},
  title        = {Reliability analysis of alternating group graphs and split-stars},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Determining exact solutions for structural parameters on
hierarchical networks with density feature. <em>COMJNL</em>,
<em>64</em>(9), 1412–1424. (<a
href="https://doi.org/10.1093/comjnl/bxaa067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of determining closed-form solutions for some structural parameters of great interest on networked models is meaningful and intriguing. In this paper, we propose a family of networked models ; with hierarchical structure where ; represents time step and ; is copy number. And then, we study some structural parameters on the proposed models ; in more detail. The results show that (i) models ; follow power-law distribution with exponent ; and thus exhibit density feature; (ii) models ; have both higher clustering coefficients and an ultra-small diameter and so display small-world property; and (iii) models ; possess rich mixing structure because Pearson-correlated coefficients undergo phase transitions unseen in previously published networked models. In addition, we also consider trapping problem on networked models ; and then precisely derive a solution for average trapping time ; . More importantly, the analytic value for ; can be approximately equal to the theoretical lower bound in the large graph size limit, implying that models ; are capable of having most optimal trapping efficiency. As a result, we also derive exact solution for another significant parameter, Kemeny’s constant. Furthermore, we conduct extensive simulations that are in perfect agreement with all the theoretical deductions.},
  archive      = {J_COMJNL},
  author       = {Ma, Fei and Wang, Ping},
  doi          = {10.1093/comjnl/bxaa067},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1412-1424},
  shortjournal = {Comput. J.},
  title        = {Determining exact solutions for structural parameters on hierarchical networks with density feature},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Persistence of hybrid diagnosability of regular networks
under testing diagnostic model. <em>COMJNL</em>, <em>64</em>(9),
1401–1411. (<a href="https://doi.org/10.1093/comjnl/bxaa065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosability is an important metric to fault tolerance and reliability for multiprocessor systems. However, plenty of research on fault diagnosability focuses on node failure. In practical scenario, not only node failures take place but also link malfunctions may arise. In this work, we investigate the diagnosability of general regular networks with failing nodes as well as missing malfunctional links. Let ; be a set of the missing links and broken-down nodes. We first prove that the diagnosability of the survival graph ; persists ; under the PMC model (Preparata, F.P., Metze, G. and Chien, R.T. (1967) On the connection assignment problem of diagnosable systems. ; ., EC-16, 848–854) for a ; -regular and ; -connected triangle-free network ; subject to ; and ; (; ). Furthermore, we determine the diagnosability of ; for some kinds of extensively explored ; -regular networks with triangles subject to ; (; ).},
  archive      = {J_COMJNL},
  author       = {Lian, Guanqin and Zhou, Shuming and Cheng, Eddie and Liu, Jiafei and Chen, Gaolin},
  doi          = {10.1093/comjnl/bxaa065},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1401-1411},
  shortjournal = {Comput. J.},
  title        = {Persistence of hybrid diagnosability of regular networks under testing diagnostic model},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliability of DQcube based on g-extra conditional fault.
<em>COMJNL</em>, <em>64</em>(9), 1393–1400. (<a
href="https://doi.org/10.1093/comjnl/bxaa058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosability and connectivity are important metrics for the reliability and fault diagnosis capability of interconnection networks, respectively. The g-extra connectivity of a graph G, denoted by ; , is the minimum number of vertices whose deletion will disconnect the network and every remaining component has more than ; vertices. The g-extra conditional diagnosability of graph G, denoted by ; , is the maximum number of faulty vertices that the graph G can guarantee to identify under the condition that every fault-free component contains at least g+1 vertices. In this paper, we first determine that g-extra connectivity of DQcube is ; for ; and then show that the g-extra conditional diagnosability of DQcube under the PMC model ; and the MM; model ; is ; , respectively.},
  archive      = {J_COMJNL},
  author       = {Zhang, Hong and Meng, Jixiang},
  doi          = {10.1093/comjnl/bxaa058},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1393-1400},
  shortjournal = {Comput. J.},
  title        = {Reliability of DQcube based on g-extra conditional fault},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimum status, matching and domination of graphs.
<em>COMJNL</em>, <em>64</em>(9), 1384–1392. (<a
href="https://doi.org/10.1093/comjnl/bxaa057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a graph, the status of a vertex is the sum of the distances between the vertex and all other vertices. The minimum status of a graph is the minimum of statuses of all vertices of this graph. We give a sharp upper bound for the minimum status of a connected graph with fixed order and matching number (domination number, respectively) and characterize the unique trees achieving the bound. We also determine the unique tree such that its minimum status is as small as possible when order and matching number (domination number, respectively) are fixed.},
  archive      = {J_COMJNL},
  author       = {Liang, Caixia and Zhou, Bo and Guo, Haiyan},
  doi          = {10.1093/comjnl/bxaa057},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1384-1392},
  shortjournal = {Comput. J.},
  title        = {Minimum status, matching and domination of graphs},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intra-tile parallelization for two-level perfectly nested
loops with non-uniform dependences. <em>COMJNL</em>, <em>64</em>(9),
1358–1383. (<a href="https://doi.org/10.1093/comjnl/bxaa050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most important scientific and engineering applications have complex computations or large data. In all these applications, a huge amount of time is consumed by nested loops. Therefore, loops are the main source of the parallelization of scientific and engineering programs. Many parallelizing compilers focus on parallelization of nested loops with uniform dependences, and parallelization of nested loops with non-uniform dependences has not been extensively investigated. This paper addresses the problem of parallelizing two-level nested loops with non-uniform dependences. The aim is to minimize the execution time by improving the load balancing and minimizing the inter-processor communication. We propose a new tiling algorithm, k-StepIntraTiling, using bin packing problem to minimize the execution time. We demonstrate the effectiveness of the proposed method in several experiments. Simulation and experimental results show that the algorithm effectively reduces the total execution time of several benchmarks compared to the other tiling methods.},
  archive      = {J_COMJNL},
  author       = {Abdi Reyhan, Zahra and Lotfi, Shahriar and Isazadeh, Ayaz and Karimpour, Jaber},
  doi          = {10.1093/comjnl/bxaa050},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1358-1383},
  shortjournal = {Comput. J.},
  title        = {Intra-tile parallelization for two-level perfectly nested loops with non-uniform dependences},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CutTheTail: An accurate and space-efficient heuristic
algorithm for influence maximization. <em>COMJNL</em>, <em>64</em>(9),
1343–1357. (<a href="https://doi.org/10.1093/comjnl/bxaa049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic problem of computing the most influential nodes in an arbitrary graph (influence maximization) is an important theoretical and practical problem and has been extensively studied for decades. For massive graphs (e.g. modelling huge social networks), randomized algorithms are the answer as the exact computation is prohibitively complex, both for runtime and space. This paper concentrates on developing new accurate and efficient randomized algorithms that drastically cut the memory footprint and scale up the computation of the most influential nodes. Implementing the Reverse Influence Sampling method proposed by Borgs, Brautbar, Chayes and Lucier in 2013, we engineered a novel algorithm, CutTheTail (CTT), which solves the problem of influence maximization (IM) while using up to five orders of magnitude smaller space than the existing renown algorithms. CTT is a heuristic algorithm. We tested the accuracy of CTT on large real-world graphs using Monte Carlo simulation as the benchmark and comparing the quality of CTT solution to the algorithms with theoretically proven guaranteed approximation to optimal. Experiments show that CTT provides solutions with the quality equal to the quality of such algorithms. Savings in required space allow to successfully run CTT on a consumer-grade laptop for a graph with almost a billion of edges. To the best of our knowledge, no other IM algorithm can compute a solution on such a scale using a 16 GB RAM laptop.},
  archive      = {J_COMJNL},
  author       = {Popova, Diana and Kawarabayashi, Ken-ichi and Thomo, Alex},
  doi          = {10.1093/comjnl/bxaa049},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1343-1357},
  shortjournal = {Comput. J.},
  title        = {CutTheTail: An accurate and space-efficient heuristic algorithm for influence maximization},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic QoS-aware cloud service selection using best-worst
method and timeslot weighted satisfaction scores. <em>COMJNL</em>,
<em>64</em>(9), 1326–1342. (<a
href="https://doi.org/10.1093/comjnl/bxaa039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of cloud services has dramatically increased over the past few years. Consequently, finding a service with the most suitable quality of service (QoS) criteria matching the user’s requirements is becoming a challenging task. Although various decision-making methods have been proposed to help users to find their required cloud services, some uncertainties such as dynamic QoS variations hamper the users from employing such methods. Additionally, the current approaches use either static or average QoS values for cloud service selection and do not consider dynamic QoS variations. In this paper, we overcome this drawback by developing a broker-based approach for cloud service selection. In this approach, we use recently monitored QoS values to find a timeslot weighted satisfaction score that represents how well a service satisfies the user’s QoS requirements. The timeslot weighted satisfaction score is then used in Best-Worst Method, which is a multi-criteria decision-making method, to rank the available cloud services. The proposed approach is validated using Amazon’s Elastic Compute Cloud (EC2) cloud services performance data. The results show that the proposed approach leads to the selection of more suitable cloud services and is also efficient in terms of performance compared to the existing analytic hierarchy process-based cloud service selection approaches.},
  archive      = {J_COMJNL},
  author       = {Nawaz, Falak and Janjua, Naeem Khalid},
  doi          = {10.1093/comjnl/bxaa039},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1326-1342},
  shortjournal = {Comput. J.},
  title        = {Dynamic QoS-aware cloud service selection using best-worst method and timeslot weighted satisfaction scores},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Double-layer network negative public opinion information
propagation modeling based on continuous-time markov chain.
<em>COMJNL</em>, <em>64</em>(9), 1315–1325. (<a
href="https://doi.org/10.1093/comjnl/bxaa038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the fact that the existing public opinion propagation aspects are mostly based on single-layer propagation network, these works rarely consider the double-layer network structure and the negative opinion evolution. This paper proposes a new susceptible-infected-vaccinated-susceptible negative opinion information propagation model with preventive vaccination by constructing double-layer network topology. Firstly, the continuous-time Markov chain is used to simulate the negative public opinion information propagation process and the nonlinear dynamic equation of the model is derived; secondly, the steady state condition of the virus propagation in the model is proposed and mathematically proved; finally, Monte Carlo method is applied in the proposed model. The parameters of simulation model have an effect on negative public opinion information propagation, the derivation results are verified by computer simulation. The simulation results show that the proposed model has a larger threshold of public opinion information propagation and has more effective control of the scale of negative public opinion; it also can reduce the density of negative public opinion information propagation and suppress negative public opinion information compared with the traditional susceptible infected susceptible model. It also can provide the scientific method and research approach based on probability statistics for the study of negative public opinion information propagation in complex networks.},
  archive      = {J_COMJNL},
  author       = {Liu, Xiaoyang and Tang, Ting and He, Daobing},
  doi          = {10.1093/comjnl/bxaa038},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1315-1325},
  shortjournal = {Comput. J.},
  title        = {Double-layer network negative public opinion information propagation modeling based on continuous-time markov chain},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unlinkable and revocable secret handshake. <em>COMJNL</em>,
<em>64</em>(8), 1303–1314. (<a
href="https://doi.org/10.1093/comjnl/bxaa181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new construction for unlinkable secret handshake that allows a group of users to perform handshakes anonymously. We define formal security models for the proposed construction and prove that it can achieve session key security, anonymity and affiliation hiding. In particular, the proposed construction ensures that (i) anonymity against protocol participants (including group authority) is achieved since a hierarchical identity-based signature is used in generating group user’s pseudonym-credential pairs and (ii) revocation is achieved using a secret sharing-based revocation mechanism.},
  archive      = {J_COMJNL},
  author       = {Tian, Yangguang and Li, Yingjiu and Mu, Yi and Yang, Guomin},
  doi          = {10.1093/comjnl/bxaa181},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1303-1314},
  shortjournal = {Comput. J.},
  title        = {Unlinkable and revocable secret handshake},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A verifier-based password-authenticated key exchange using
tamper-proof hardware. <em>COMJNL</em>, <em>64</em>(8), 1293–1302. (<a
href="https://doi.org/10.1093/comjnl/bxaa178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Password-based authenticated key exchange (PAKE) allows two parties to compute a common secret key. PAKE offers the advantage of allowing two parties to pre-share only a password. However, when it is executed in a client–server environment, server corruption can expose the clients’ passwords. To be resilient against server compromises, verifier-based authenticated key exchange (VPAKE) is proposed, as an augmented version of PAKE. Thus far, there are two known major VPAKE constructions formally proven secure. However, both involve strong assumptions, such as random oracles. In this paper, we propose a simple and efficient VPAKE using tamper-proof hardware without random oracles to support resilient infrastructures. In particular, we transform Katz–Vaikuntanathan one-round PAKE into two-round VPAKE so as to instill resilience to server compromises. We provide a formal definition of VPAKE using tamper-proof hardware and security proof without random oracles. Finally, we provide a performance analysis and comparisons to previous VPAKE and PAKE protocols. Our transformation supports an efficient VPAKE protocol with six group element communications when the underlying Katz–Vaikuntanathan PAKE is instantiated by Cramer–Shoup ciphertext following the proposal by Benhamouda},
  archive      = {J_COMJNL},
  author       = {Shin, Ji Sun and Jo, Minjae and Hwang, Jung Yeon and Lee, Jaehwan},
  doi          = {10.1093/comjnl/bxaa178},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1293-1302},
  shortjournal = {Comput. J.},
  title        = {A verifier-based password-authenticated key exchange using tamper-proof hardware},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Theoretical estimation on the success rate of the asymptotic
higher order optimal distinguisher. <em>COMJNL</em>, <em>64</em>(8),
1277–1292. (<a href="https://doi.org/10.1093/comjnl/bxaa171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its first publication at ASIACRYPT 2014, higher order optimal distinguisher (HOOD) has been the most efficient style of higher order side channel attacks that can be used to evaluate the physical security of a masking device. In practice, the efficiency of HOOD can be empirically evaluated with the success rate (SR) metric. In the empirical evaluation, a large number of power traces are needed, and HOOD should be repeated thousands of times under the values of different parameters, which can make the evaluation process cumbersome and the evaluation price high. In light of this, the exact relationship between the SR of the asymptotic HOOD and the values of different parameters is theoretically built, and the soundness of the theoretical analysis is empirically verified in both the simulated scenario and the real scenario. Then, by setting the values of different parameters, the SR of the asymptotic HOOD can be theoretically estimated. Here, as the signal-to-noise ratio of a masking device approaches to zero, the SR of the asymptotic HOOD approaches to the SR of HOOD. Overall, this contribution may help evaluators to efficiently evaluate the physical security of a masking device with HOOD.},
  archive      = {J_COMJNL},
  author       = {Zhang, Hailong and Yang, Wei},
  doi          = {10.1093/comjnl/bxaa171},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1277-1292},
  shortjournal = {Comput. J.},
  title        = {Theoretical estimation on the success rate of the asymptotic higher order optimal distinguisher},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved file-injection attacks on searchable encryption
using finite set theory. <em>COMJNL</em>, <em>64</em>(8), 1264–1276. (<a
href="https://doi.org/10.1093/comjnl/bxaa161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable encryption (SE) allows the cloud server to search over the encrypted data and leak information as little as possible. Most existing efficient SE schemes assume that the leakage of search pattern and access pattern is acceptable. A series of work was proposed, instructing malicious users to use this leakage to come up with attacks. Especially, with a devastating attack proposed by Zhang ; , the cloud server can reveal the keywords queried by normal users by using some injected files. From the method of constructing uniform ; -set of a finite set ; proposed by Cao, we put forward a new file-injection attack. In our attack, the server needs fewer injected files than the previous attack when the size of ; is larger than 9 and the size of keyword set is larger than ; , where ; is the threshold of the number of keywords in each injected file. Our attack is more practical and easier to implement in the real scenario.},
  archive      = {J_COMJNL},
  author       = {Wang, Gaoli and Cao, Zhenfu and Dong, Xiaolei},
  doi          = {10.1093/comjnl/bxaa161},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1264-1276},
  shortjournal = {Comput. J.},
  title        = {Improved file-injection attacks on searchable encryption using finite set theory},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved key recovery attacks on simplified version of k2
stream cipher. <em>COMJNL</em>, <em>64</em>(8), 1253–1263. (<a
href="https://doi.org/10.1093/comjnl/bxaa154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The K2 stream cipher, designed for 32-bit words, is an ISO/IEC 18033 standard and is listed as a recommended algorithm used by the Japanese government in the CRYPTREC project. The main feature of the K2 algorithm is the use of a dynamic feedback control mechanism between the two linear feedback shift registers, which makes the analysis of the K2 algorithm more difficult. In this paper, for its simplified version algorithm, a key recovery attack is performed by using differential attacks. Firstly, for the unknown key, the same IV is fixed in two chosen IV differential attacks, and we use the input differences and the output differences of the S-box to recover the input of S-box; the internal state values can be uniquely determined by taking intersection of the input of S-box. This technology is used to improve the key recovery attack of seven-round algorithm proposed by Deike Priemuth-Schmid. Secondly, we find the constraint relationship between the keystream equations and the unknown differences by introducing the guess difference bit and eliminate the impossible differences by the constraint relationship. Thus, we expand the key recovery attack from seven to nine rounds. The time complexity of the attack is ; , the data complexity is ; and the success rate is ; .},
  archive      = {J_COMJNL},
  author       = {Ma, Sudong and Guan, Jie},
  doi          = {10.1093/comjnl/bxaa154},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1253-1263},
  shortjournal = {Comput. J.},
  title        = {Improved key recovery attacks on simplified version of k2 stream cipher},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CCA-almost-full anonymous group signature with verifier
local revocation in the standard model. <em>COMJNL</em>, <em>64</em>(8),
1239–1252. (<a href="https://doi.org/10.1093/comjnl/bxaa153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group signature (GS) allows each member in a group to do signatures anonymously on behalf of the group under management of a group authority. Membership revocation has been a central issue in GS and widely studied so far. The mechanism of verifier local revocation for GS, first formalized by Boneh and Shacham, has an advantage of making the signers free from fetching the up-to-date information of the revoked users and practicality in the scenario of periodically update in the large population of group users. Most of work related to group signature with verifier-local revocation either can only achieve selfless anonymity or have inefficient constructions due to complicate primitives. Aiming to a recent ; notion for GS, this paper presents an efficient GS with verifier local revocation in the standard model by adding a new primitive ; into Groth’s GS under Canard ; .’s framework. We prove that it has backward unlinkability to ensure that all signatures generated by the user before the revocation remain anonymous, even if it is revoked later.},
  archive      = {J_COMJNL},
  author       = {Ma, Sha and Huang, Qiong},
  doi          = {10.1093/comjnl/bxaa153},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1239-1252},
  shortjournal = {Comput. J.},
  title        = {CCA-almost-full anonymous group signature with verifier local revocation in the standard model},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightweight public key encryption with equality test
supporting partial authorization in cloud storage. <em>COMJNL</em>,
<em>64</em>(8), 1226–1238. (<a
href="https://doi.org/10.1093/comjnl/bxaa144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public key encryption with equality test (PKEET) can check whether two ciphertexts are encrypted from the same message or not without decryption. This attribute enables PKEET to be increasingly utilized in cloud storage, where users store their encrypted data on the cloud. In traditional PKEET, the tester is authorized by the data receiver to perform equality test on its ciphertexts. However, the tester can only test one ciphertext or all ciphertexts of one receiver with one authorization. It means that the receiver cannot adaptively authorize the test right of any number of ciphertexts to the tester. A trivial solution is authorizing one ciphertext each time and repeating multiple times. The corresponding size of trapdoor in this method is linear with the number of authorized ciphertexts. This will incur storage burden for the tester. To solve the aforementioned problem, we propose the concept of PKEET supporting partial authentication (PKEET-PA). We then instantiate the concept to a lightweight PKEET-PA, which achieves constant-size trapdoor. Besides, we prove the security of our PKEET-PA scheme against two types of adversaries. Compared with other PKEET schemes that can be used in trivial solution, our PKEET-PA is more efficient in receivers’ computation and has lower trapdoor size.},
  archive      = {J_COMJNL},
  author       = {Lin, Hao and Zhao, Zhen and Gao, Fei and Susilo, Willy and Wen, Qiaoyan and Guo, Fuchun and Shi, Yijie},
  doi          = {10.1093/comjnl/bxaa144},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1226-1238},
  shortjournal = {Comput. J.},
  title        = {Lightweight public key encryption with equality test supporting partial authorization in cloud storage},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CyberEyes: Cybersecurity entity recognition model based on
graph convolutional network. <em>COMJNL</em>, <em>64</em>(8), 1215–1225.
(<a href="https://doi.org/10.1093/comjnl/bxaa141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity has gradually become the public focus between common people and countries with the high development of Internet technology in daily life. The cybersecurity knowledge analysis methods have achieved high evolution with the help of knowledge graph technology, especially a lot of threat intelligence information could be extracted with fine granularity. But named entity recognition (NER) is the primary task for constructing security knowledge graph. Traditional NER models are difficult to determine entities that have a complex structure in the field of cybersecurity, and it is difficult to capture non-local and non-sequential dependencies. In this paper, we propose a cybersecurity entity recognition model ; that uses non-local dependencies extracted by graph convolutional neural networks. The model can capture both local context and graph-level non-local dependencies. In the evaluation experiments, our model reached an F1 score of 90.28\% on the cybersecurity corpus under the gold evaluation standard for NER, which performed better than the 86.49\% obtained by the classic CNN-BiLSTM-CRF model.},
  archive      = {J_COMJNL},
  author       = {Fang, Yong and Zhang, Yuchi and Huang, Cheng},
  doi          = {10.1093/comjnl/bxaa141},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1215-1225},
  shortjournal = {Comput. J.},
  title        = {CyberEyes: Cybersecurity entity recognition model based on graph convolutional network},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new intrusion detection system using the improved
dendritic cell algorithm. <em>COMJNL</em>, <em>64</em>(8), 1193–1214.
(<a href="https://doi.org/10.1093/comjnl/bxaa140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dendritic cell algorithm (DCA) as one of the emerging evolutionary algorithms is based on the behavior of the specific immune agents, known as dendritic cells (DCs). DCA has several potentially beneficial features for binary classification problems. In this paper, we aim at providing a new version of this immune-inspired mechanism acts as a semi-supervised classifier, which can be a defensive shield in network intrusion detection problem. Till now, no strategy or idea has been adopted on the ; function on the detection phase, but random sampling entails the DCA to provide undesirable results in several cycles at each time. This leads to uncertainty. Whereas it must be accomplished by biological behaviors of DCs in peripheral tissues, we have proposed a novel strategy that exactly acts based on its immunological functionalities of dendritic cells. The proposed mechanism focuses on two items: first, to obviate the challenge of needing to have a preordered antigen set for computing danger signal, and the second, to provide a novel immune-inspired idea for nonrandom data sampling. A variable functional migration threshold is also computed cycle by cycle that shows the necessity of the migration threshold flexibility. A significant criterion so-called capability of intrusion detection (CID) is used for tests. All the tests have been performed in a new benchmark dataset named UNSW-NB15. Experimental consequences demonstrate that the present schema as the best version among improved DC algorithms achieves 76.69\% CID by 90\% accuracy and outperforms its counterpart methods.},
  archive      = {J_COMJNL},
  author       = {Farzadnia, Ehsan and Shirazi, Hossein and Nowroozi, Alireza},
  doi          = {10.1093/comjnl/bxaa140},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1193-1214},
  shortjournal = {Comput. J.},
  title        = {A new intrusion detection system using the improved dendritic cell algorithm},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-aware image authentication from cryptographic
primitives. <em>COMJNL</em>, <em>64</em>(8), 1178–1192. (<a
href="https://doi.org/10.1093/comjnl/bxaa127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image authentication is the process of verifying image origin, integrity and authenticity. In many situations, image authentication should allow reasonable image editing, which does not introduce any wrong information against the original one. While it has been studied both extensively and intensively with considerable efforts, there is no satisfactory method supporting region extraction. This paper presents a solution to address the issue of privacy protection in authenticated images. Our scheme allows anyone to extract sub-image blocks from an original image (authenticated by the image producer) and generate a proof tag to prove the credibility of the extracted image blocks. The process of proof tag generation does not require any interaction with the image producer. In addition, the image producer is able to define must-be-preserved image blocks (e.g. producer logo) during the extraction. We define the security property for the authenticated sub-images and give a generic design with two core primitives: an ordinary digital signature scheme and a cryptographic accumulator. The security of our design can be reduced to the underlying cryptographic primitives and its practical performance is demonstrated by a bunch of evaluations. We believe the proposed design, together with other image authentication methods, will further facilitate image relevant services and applications.},
  archive      = {J_COMJNL},
  author       = {Chen, Haixia and Huang, Xinyi and Wu, Wei and Mu, Yi},
  doi          = {10.1093/comjnl/bxaa127},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1178-1192},
  shortjournal = {Comput. J.},
  title        = {Privacy-aware image authentication from cryptographic primitives},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel public-key encryption with continuous leakage
amplification. <em>COMJNL</em>, <em>64</em>(8), 1163–1177. (<a
href="https://doi.org/10.1093/comjnl/bxaa124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leakage of private information, such as the secret keys, has become a threat to the security of computing systems. It has become a common requirement that cryptographic schemes should withstand various leakage attacks, including the continuous leakage attacks. Although some research progresses have been made toward this area, there are still some unsolved issues. In the literature, the public-key encryption (PKE) constructions with (continuous) leakage resilience normally require the upper bound of leakage to be fixed. However, in many real-world applications, this requirement cannot provide sufficient protection against leakage attacks. In order to mitigate these problems, this paper demonstrates how to design a leakage amplified PKE scheme with continuous leakage resilience and chosen-plaintext attacks security. In our proposed PKE scheme, the leakage parameter can have an arbitrary length. Moreover, the length of permitted leakage in our scheme can be flexibly adjusted according to the leakage requirements of application environment. Its security is formally proved under the classic static assumption.},
  archive      = {J_COMJNL},
  author       = {Qiao, Zirui and Yang, Qiliang and Zhou, Yanwei and Xia, Zhe and Zhang, Mingwu},
  doi          = {10.1093/comjnl/bxaa124},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1163-1177},
  shortjournal = {Comput. J.},
  title        = {Novel public-key encryption with continuous leakage amplification},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Private set operations over encrypted cloud dataset and
applications. <em>COMJNL</em>, <em>64</em>(8), 1145–1162. (<a
href="https://doi.org/10.1093/comjnl/bxaa123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the notion of private set operations (PSO) as a symmetric-key primitive in the cloud scenario, where a client securely outsources his dataset to a cloud service provider and later privately issues queries in the form of common set operations. We define a syntax and security notion for PSO and propose a general construction that satisfies it. There are two main ingredients to our PSO scheme: an adjustable join (Adjoin) scheme (MIT-CSAIL-TR-2012-006 (2012) ; . ; ) and a tuple set (TSet) scheme (Cash, D., Jarecki, S., Jutla, C. S., Krawczyk, H., Rosu, M.-C., and Steiner, M. (2013) Highly-Scalable Searchable Symmetric Encryption With Support for Boolean Queries. ; , Santa Barbara, CA, August 18–22, pp. 353–373. Springer, Berlin, Heidelberg). We also propose an Adjoin construction that is substantially more efficient (in computation and storage) than the previous ones (Mironov, I., Segev, G., and Shahaf, I. (2017) Strengthening the Security of Encrypted Databases: Non-Transitive Joins. ; , Baltimore, MD, USA, November 12–15, pp. 631–661. Springer, Cham) due to the hardness assumption that we rely on, while retaining the same security notion. The proposed PSO scheme can be used to perform join queries on encrypted databases without revealing the duplicate patterns in the unqueried columns, which is inherent to an Adjoin scheme. In addition, we also show that the PSO scheme can be used to perform Boolean search queries on a collection of encrypted documents. We also provide standard security proofs for our constructions and present detailed efficiency evaluation and compare them with well-known previous ones.},
  archive      = {J_COMJNL},
  author       = {Rafiee, Mojtaba and Khazaei, Shahram},
  doi          = {10.1093/comjnl/bxaa123},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1145-1162},
  shortjournal = {Comput. J.},
  title        = {Private set operations over encrypted cloud dataset and applications},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimized k-means algorithm based on information entropy.
<em>COMJNL</em>, <em>64</em>(7), 1130–1143. (<a
href="https://doi.org/10.1093/comjnl/bxab078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a widely used technique in data mining applications and various pattern recognition applications, in which data objects are divided into groups. K-means algorithm is one of the most classical clustering algorithms. In this algorithm, the initial clustering centers are randomly selected, this results in unstable clustering results. To solve this problem, an optimized algorithm to select the initial centers is proposed. In the proposed algorithm, dispersion degree is defined, which is based on entropy. In the algorithm, all the objects are firstly grouped into a big cluster, and the object that has the maximum dispersion degree and the object that has the minimum dispersion degree are selected as the initial clustering centers from the initial big cluster. And then other objects in the biggest cluster are partitioned to the initial clusters to which the objects are nearest. The partition process will be repeated until the cluster number is equal to the specified value k. Finally, the partitioned k clusters and their cluster centers are applied to k-means algorithm as initial clusters and centers. Several experiments are conducted on real data sets to evaluate the proposed algorithm. The proposed algorithm is compared with traditional k-means algorithm and max-min distance clustering algorithm, and experimental results show that the improved k-means algorithm is stable in selecting initial clustering, because it can select unique initial clustering centers. The optimized algorithm’s effectiveness and feasibility are also verified by experiments, and the algorithm can reduce the times of iterations and has more stable clustering results and higher accuracy.},
  archive      = {J_COMJNL},
  author       = {Liu, Meiling and Zhang, Beixian and Li, Xi and Tang, Weidong and Zhang, GangQiang},
  doi          = {10.1093/comjnl/bxab078},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1130-1143},
  shortjournal = {Comput. J.},
  title        = {An optimized k-means algorithm based on information entropy},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of privacy solutions using blockchain for
recommender systems: Current status, classification and open issues.
<em>COMJNL</em>, <em>64</em>(7), 1104–1129. (<a
href="https://doi.org/10.1093/comjnl/bxab065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid growth of Internet, E-commerce and Internet of Things, people use Web based services for most of their needs including buying items, reading books, watching online shows etc. Several companies are using recommender systems to influence people’s choices based on their likings, behaviours etc. Hence, people fear that their privacy is violated. Also, some of the online applications are not safe and secure. One way to overcome the privacy related issues is using secured solutions such as incorporating blockchain technologies for privacy-based applications. The decentralized nature of blockchain technologies have resolved several security, and authentication problems of Internet of Things systems. In this paper, we conduct a comprehensive survey on the privacy solutions for recommender systems emphasising current status, classification and open issues. We also discuss blockchain technology, including its structure as well as applications of blockchain technology for privacy solutions of recommender systems. Furthermore, we discuss the limitations and delve into future trends that blockchain technology can be adapted for privacy-base applications in the years to come.},
  archive      = {J_COMJNL},
  author       = {Abduljabbar, Tamara Abdulmunim and Tao, Xiaohui and Zhang, Ji and Zhou, Xujuan and Li, Lin and Cai, Yi},
  doi          = {10.1093/comjnl/bxab065},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1104-1129},
  shortjournal = {Comput. J.},
  title        = {A survey of privacy solutions using blockchain for recommender systems: Current status, classification and open issues},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and dynamic graph convolutional network for
multi-view data classification. <em>COMJNL</em>, <em>64</em>(7),
1093–1103. (<a href="https://doi.org/10.1093/comjnl/bxab064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since graph learning could preserve the structure information of the samples to improve the learning ability, it has been widely applied in both shallow learning and deep learning. However, the current graph learning methods still suffer from the issues such as outlier influence and model robustness. In this paper, we propose a new dynamic graph neural network (DGCN) method to conduct semi-supervised classification on multi-view data by jointly conducting the graph learning and the classification task in a unified framework. Specifically, our method investigates three strategies to improve the quality of the graph before feeding it into the GCN model: (i) employing robust statistics to consider the sample importance for reducing the outlier influence, i.e. assigning every sample with soft weights so that the important samples are with large weights and outliers are with small or even zero weights; (ii) learning the common representation across all views to improve the quality of the graph for every view; and (iii) learning the complementary information from all initial graphs on multi-view data to further improve the learning of the graph for every view. As a result, each of the strategies could improve the robustness of the DGCN model. Moreover, they are complementary for reducing outlier influence from different aspects, i.e. the sample importance reduces the weights of the outliers, both the common representation and the complementary information improve the quality of the graph for every view. Experimental result on real data sets demonstrates the effectiveness of our method, compared to the comparison methods, in terms of multi-class classification performance.},
  archive      = {J_COMJNL},
  author       = {Peng, Liang and Kong, Fei and Liu, Chongzhi and Kuang, Ping},
  doi          = {10.1093/comjnl/bxab064},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1093-1103},
  shortjournal = {Comput. J.},
  title        = {Robust and dynamic graph convolutional network for multi-view data classification},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-negative matrix factorization: A survey.
<em>COMJNL</em>, <em>64</em>(7), 1080–1092. (<a
href="https://doi.org/10.1093/comjnl/bxab103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF) is a powerful tool for data science researchers, and it has been successfully applied to data mining and machine learning community, due to its advantages such as simple form, good interpretability and less storage space. In this paper, we give a detailed survey on existing NMF methods, including a comprehensive analysis of their design principles, characteristics and drawbacks. In addition, we also discuss various variants of NMF methods and analyse properties and applications of these variants. Finally, we evaluate the performance of nine NMF methods through numerical experiments, and the results show that NMF methods perform well in clustering tasks.},
  archive      = {J_COMJNL},
  author       = {Gan, Jiangzhang and Liu, Tong and Li, Li and Zhang, Jilian},
  doi          = {10.1093/comjnl/bxab103},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1080-1092},
  shortjournal = {Comput. J.},
  title        = {Non-negative matrix factorization: A survey},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning representation from concurrence-words graph for
aspect sentiment classification. <em>COMJNL</em>, <em>64</em>(7),
1069–1079. (<a href="https://doi.org/10.1093/comjnl/bxab104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect sentiment classification is an important research topic in natural language processing and computational linguistics, assisting in automatically review analysis and emotional tendency judgement. Different from extant methods that focus on text sequence representations, this paper presents a network framework to learn representation from concurrence-words relation graph (LRCWG), so as to improve the Macro-F1 and accuracy. The LRCWG first employs the multi-head attention mechanism to capture the sentiment representation from the sentences which can learn the importance of text sequence representation. And then, it leverages the priori sentiment dictionary information to construct the concurrence relations of sentiment words with Graph Convolution Network (GCN). This assists in that the learnt context representation can keep both the semantics integrity and the features of sentiment concurrence-words relations. The designed algorithm is experimentally evaluated with all the five benchmark datasets and demonstrated that the proposed aspect sentiment classification can significantly improve the prediction performance of learning task.},
  archive      = {J_COMJNL},
  author       = {Lu, Guangquan and Huang, Jihong},
  doi          = {10.1093/comjnl/bxab104},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1069-1079},
  shortjournal = {Comput. J.},
  title        = {Learning representation from concurrence-words graph for aspect sentiment classification},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive cross-lingual question generation with minimal
resources. <em>COMJNL</em>, <em>64</em>(7), 1056–1068. (<a
href="https://doi.org/10.1093/comjnl/bxab106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of question generation (QG) aims to create valid questions and correlated answers from the given text. Despite the neural QG approaches have achieved promising results, they are typically developed for languages with rich annotated training data. Because of the high annotation cost, it is difficult to deploy to other low-resource languages. Besides, different samples have their own characteristics on the aspects of text contextual structure, question type and correlations. Without capturing these diversified characteristics, the traditional one-size-fits-all model is hard to generate the best results. To address this problem, we study the task of cross-lingual QG from an adaptive learning perspective. Concretely, we first build a basic QG model on a multilingual space using the labelled data. In this way, we can transfer the supervision from the high-resource language to the language lacking labelled data. We then design a task-specific meta-learner to optimize the basic QG model. Each sample and its similar instances are viewed as a pseudo-QG task. The asking patterns and logical forms contained in the similar samples can be used as a guide to fine-tune the model fitly and produce the optimal results accordingly. Considering that each sample contains the text, question and answer, with unknown semantic correlations among them, we propose a context-dependent retriever to measure the similarity of such structured inputs. Experimental results on three languages of three typical data sets show the effectiveness of our approach.},
  archive      = {J_COMJNL},
  author       = {Yu, Jianxing and Wang, Shiqi and Yin, Jian},
  doi          = {10.1093/comjnl/bxab106},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1056-1068},
  shortjournal = {Comput. J.},
  title        = {Adaptive cross-lingual question generation with minimal resources},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Precise point set registration based on feature fusion.
<em>COMJNL</em>, <em>64</em>(7), 1039–1055. (<a
href="https://doi.org/10.1093/comjnl/bxab114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed a novel precise point set registration method based on feature fusion for three-dimensional data. Firstly, for the prominent foreground with dense and continuous cluster structure, we propose an automatic extraction method combining the principal component analysis projection and density-based clustering method. Secondly, for point sets containing noises, we introduce correntropy measurement into registration to weaken their influence. Thirdly, for the precise registration of uneven distribution of points in the same point set, we propose a feature fusion based algorithm which is distribution specific, using point-to-point measurement for densely distributed foreground and point-to-plane measurement for sparsely distributed background, in case that only one measurement method is used for the whole point set the registration gets trapped into local extremum. Finally, we give the optimization algorithm of the proposed method. We conduct experiments on real orthodontics scenes to verify the effectiveness of our proposed feature extraction method and registration algorithm, and experimental results demonstrate that both the proposed solutions are proper for their respective tasks than other existing methods.},
  archive      = {J_COMJNL},
  author       = {Liu, Yuying and Du, Shaoyi and Cui, Wenting and Wang, Xijing and Mou, Qingnan and Zhao, Jiamin and Guo, Yucheng and Zhang, Yong},
  doi          = {10.1093/comjnl/bxab114},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1039-1055},
  shortjournal = {Comput. J.},
  title        = {Precise point set registration based on feature fusion},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LEDet: A single-shot real-time object detector based on
low-light image enhancement. <em>COMJNL</em>, <em>64</em>(7), 1028–1038.
(<a href="https://doi.org/10.1093/comjnl/bxab055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, significant breakthroughs have been achieved in the field of object detection. However, existing methods mostly focus on the generic object detection task. Performance degradation can be unavoidable when applying the existing methods to some specific situations directly, e.g. a low-light environment. To address this issue, we propose a single-shot real-time object Detector based on Low-light image Enhancement, namely LEDet. LEDet adapts itself to the low-light detection task in three aspects. First, a low-light enhancement module is introduced as the image preprocessor, producing the augmented inputs from the low-light images. Second, two modules, i.e. low-light and enhanced features fusion module and the scale-aware channel attention dilated convolution module are designed. These two modules aim at learning robust and discriminative features from objects of various sizes hidden in the darkness. In experiments, we validate the effectiveness of each part of our LEDet model via several ablation studies. We also compare LEDet with various methods on the Exclusively Dark dataset, showing that our model achieves the state-of-the-art performance on the balance between speed and accuracy.},
  archive      = {J_COMJNL},
  author       = {Hao, Shijie and Wang, Zhonghao and Sun, Fuming},
  doi          = {10.1093/comjnl/bxab055},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1028-1038},
  shortjournal = {Comput. J.},
  title        = {LEDet: A single-shot real-time object detector based on low-light image enhancement},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A federated learning approach for privacy protection in
context-aware recommender systems. <em>COMJNL</em>, <em>64</em>(7),
1016–1027. (<a href="https://doi.org/10.1093/comjnl/bxab025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy protection is one of the key concerns of users in recommender system-based consumer markets. Popular recommendation frameworks such as collaborative filtering (CF) suffer from several privacy issues. Federated learning has emerged as an optimistic approach for collaborative and privacy-preserved learning. Users in a federated learning environment train a local model on a self-maintained item log and collaboratively train a global model by exchanging model parameters instead of personalized preferences. In this research, we proposed a federated learning-based privacy-preserving CF model for context-aware recommender systems that work with a user-defined collaboration protocol to ensure users’ privacy. Instead of crawling users’ personal information into a central server, the whole data are divided into two disjoint parts, i.e. user data and sharable item information. The inbuilt power of federated architecture ensures the users’ privacy concerns while providing considerably accurate recommendations. We evaluated the performance of the proposed algorithm with two publicly available datasets through both the prediction and ranking perspectives. Despite the federated cost and lack of open collaboration, the overall performance achieved through the proposed technique is comparable with popular recommendation models and satisfactory while providing significant privacy guarantees.},
  archive      = {J_COMJNL},
  author       = {Ali, Waqar and Kumar, Rajesh and Deng, Zhiyi and Wang, Yansong and Shao, Jie},
  doi          = {10.1093/comjnl/bxab025},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1016-1027},
  shortjournal = {Comput. J.},
  title        = {A federated learning approach for privacy protection in context-aware recommender systems},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive laplacian support vector machine for
semi-supervised learning. <em>COMJNL</em>, <em>64</em>(7), 1005–1015.
(<a href="https://doi.org/10.1093/comjnl/bxab024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Laplacian support vector machine (LapSVM) is an extremely popular classification method and relies on a small number of labels and a Laplacian regularization to complete the training of the support vector machine (SVM). However, the training of SVM model and Laplacian matrix construction are usually two independent process. Therefore, In this paper, we propose a new adaptive LapSVM method to realize semi-supervised learning with a primal solution. Specifically, the hinge loss of unlabelled data is considered to maximize the distance between unlabelled samples from different classes and the process of dealing with labelled data are similar to other LapSVM methods. Besides, the proposed method embeds the Laplacian matrix acquisition into the SVM training process to improve the effectiveness of Laplacian matrix and the accuracy of new SVM model. Moreover, a novel optimization algorithm considering primal solver is proposed to our adaptive LapSVM model. Experimental results showed that our method outperformed all comparison methods in terms of different evaluation metrics on both real datasets and synthetic datasets.},
  archive      = {J_COMJNL},
  author       = {Hu, Rongyao and Zhang, Leyuan and Wei, Jian},
  doi          = {10.1093/comjnl/bxab024},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1005-1015},
  shortjournal = {Comput. J.},
  title        = {Adaptive laplacian support vector machine for semi-supervised learning},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global and local structure preservation for nonlinear
high-dimensional spectral clustering. <em>COMJNL</em>, <em>64</em>(7),
993–1004. (<a href="https://doi.org/10.1093/comjnl/bxab020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is widely applied in real applications, as it utilizes a graph matrix to consider the similarity relationship of subjects. The quality of graph structure is usually important to the robustness of the clustering task. However, existing spectral clustering methods consider either the local structure or the global structure, which can not provide comprehensive information for clustering tasks. Moreover, previous clustering methods only consider the simple similarity relationship, which may not output the optimal clustering performance. To solve these problems, we propose a novel clustering method considering both the local structure and the global structure for conducting nonlinear clustering. Specifically, our proposed method simultaneously considers (i) preserving the local structure and the global structure of subjects to provide comprehensive information for clustering tasks, (ii) exploring the nonlinear similarity relationship to capture the complex and inherent correlation of subjects and (iii) embedding dimensionality reduction techniques and a low-rank constraint in the framework of adaptive graph learning to reduce clustering biases. These constraints are considered in a unified optimization framework to result in one-step clustering. Experimental results on real data sets demonstrate that our method achieved competitive clustering performance in comparison with state-of-the-art clustering methods.},
  archive      = {J_COMJNL},
  author       = {Wen, Guoqiu and Zhu, Yonghua and Chen, Linjun and Zhan, Mengmeng and Xie, Yangcai},
  doi          = {10.1093/comjnl/bxab020},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {993-1004},
  shortjournal = {Comput. J.},
  title        = {Global and local structure preservation for nonlinear high-dimensional spectral clustering},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-preserving multimedia data analysis.
<em>COMJNL</em>, <em>64</em>(7), 991–992. (<a
href="https://doi.org/10.1093/comjnl/bxab095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Zhu, Xiaofeng and Han Thung, Kim and Kim, Minjeong},
  doi          = {10.1093/comjnl/bxab095},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {991-992},
  shortjournal = {Comput. J.},
  title        = {Privacy-preserving multimedia data analysis},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A virtual network embedding algorithm based on double-layer
reinforcement learning. <em>COMJNL</em>, <em>64</em>(6), 973–989. (<a
href="https://doi.org/10.1093/comjnl/bxab040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual network embedding (VNE) algorithms dominate the effectiveness of resource sharing in network virtualization. Heuristic embedding algorithms generally make embedding decisions by artificially specified strategies, in which the node importance is measured by simply summing or multiplying several node attributes. However, the contributions of different attributes may be combined through complex functional relationships. The reinforcement learning-based VNE algorithms can optimize node embedding. However, the existing algorithms only consider the local node attributes, and only simple shortest path-based embedding policy is adopted for link embedding, resulting limited embedding effects. To overcome the above defects, we propose a double-layer reinforcement learning-based VNE algorithm (DRL-VNE). In DRL-VNE, both the global and local node attributes are extracted to represent the status of network nodes, then a policy network is constructed to optimize node embedding, and the other policy network is designed to optimize link embedding. The performance of DRL-VNE is evaluated under different network scenarios and is compared with that of heuristic and machine learning-based VNE algorithms. Simulation results show that in hierarchical network scenario, the request acceptance ratio and the resource utilization of DRL-VNE are respectively improved by 14\% and by 27\% compared with the best performance comparison algorithm.},
  archive      = {J_COMJNL},
  author       = {Li, Meng and Lu, MeiLian},
  doi          = {10.1093/comjnl/bxab040},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {973-989},
  shortjournal = {Comput. J.},
  title        = {A virtual network embedding algorithm based on double-layer reinforcement learning},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal text document clustering enabled by weighed
similarity oriented jaya with grey wolf optimization algorithm.
<em>COMJNL</em>, <em>64</em>(6), 960–972. (<a
href="https://doi.org/10.1093/comjnl/bxab013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to scientific development, a variety of challenges present in the field of information retrieval. These challenges are because of the increased usage of large volumes of data. These huge amounts of data are presented from large-scale distributed networks. Centralization of these data to carry out analysis is tricky. There exists a requirement for novel text document clustering algorithms, which overcomes challenges in clustering. The two most important challenges in clustering are clustering accuracy and quality. For this reason, this paper intends to present an ideal clustering model for text document using term frequency–inverse document frequency, which is considered as feature sets. Here, the initial centroid selection is much concentrated which can automatically cluster the text using weighted similarity measure in the proposed clustering process. In fact, the weighted similarity function involves the inter-cluster, and intra-cluster similarity of both ordered and unordered documents, which is used to minimize weighted similarity among the documents. An advanced model for clustering is proposed by the hybrid optimization algorithm, which is the combination of the Jaya Algorithm (JA) and Grey Wolf Algorithm (GWO), and so the proposed algorithm is termed as JA-based GWO. Finally, the performance of the proposed model is verified through a comparative analysis with the state-of-the-art models. The performance analysis exhibits that the proposed model is 96.56\% better than genetic algorithm, 99.46\% better than particle swarm optimization, 97.09\% superior to Dragonfly algorithm, and 96.21\% better than JA for the similarity index. Therefore, the proposed model has confirmed its efficiency through valuable analysis.},
  archive      = {J_COMJNL},
  author       = {Venkanna, Gugulothu and Bharati, Dr K F},
  doi          = {10.1093/comjnl/bxab013},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {960-972},
  shortjournal = {Comput. J.},
  title        = {Optimal text document clustering enabled by weighed similarity oriented jaya with grey wolf optimization algorithm},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CMT: An efficient algorithm for scalable packet
classification. <em>COMJNL</em>, <em>64</em>(6), 941–959. (<a
href="https://doi.org/10.1093/comjnl/bxab005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Packet classification plays an essential role in diverse network functions such as quality of service, firewall filtering and load balancer. However, implementing an efficient packet classifier is a challenging problem. The problem even gets worse in the era of software-defined network, in which frequent rule updates are performed, and complex flow tables are used. This paper proposes CMT, a new software algorithm named by its novel data structure—common mask tree—to implement an efficient multi-field packet classifier. The core idea of CMT is to combine the strengths of both decision-tree and tuple-space schemes by employing tree-like structures and hash tables simultaneously. The objective of CMT is to achieve both high classification performance and fast rule updates. In the evaluation section, CMT is compared with decision-tree and tuple-space schemes. Compared to the state-of-the-art decision-tree methods, CMT performs rule updates at two orders of magnitude faster. CMT has a stable performance on different rulesets and achieves a 40\% improvement in memory access compared to the state-of-the-art tuple-space method.},
  archive      = {J_COMJNL},
  author       = {Chen, Shuhui and Zhong, Jincheng and Huang, Teng and Wei, Ziling and Zhao, Shuang},
  doi          = {10.1093/comjnl/bxab005},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {941-959},
  shortjournal = {Comput. J.},
  title        = {CMT: An efficient algorithm for scalable packet classification},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trust-aware task allocation in collaborative crowdsourcing
model. <em>COMJNL</em>, <em>64</em>(6), 929–940. (<a
href="https://doi.org/10.1093/comjnl/bxaa202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task allocation plays a vital role in crowd computing by determining its performance. The power of crowd computing stems from a large number of workers potentially available to provide high quality of service and reduce costs. An important challenge in the crowdsourcing market today is the task allocation of crowdsourcing workflows. Task allocation aims to maximize the completion quality of the entire workflow and minimize its total cost. Trust can affect the quality of the produced results and costs. Selecting workers with high levels of trust could provide better solution to the workflow and increase the budget. Crowdsourcing workflow needs to balance the two conflicting objectives. In this paper, we propose an alternative greedy approach with four heuristic strategies to address the issue. In particular, the proposed approach aims to monitor the current status of workflow execution and use heuristic strategies to adjust the parameters of task allocation. We design a two-phase allocation model to accurately match the tasks with workers. ; allocates each task to the worker that maximizes the trust level, while minimizing the cost. We conduct extensive experiments to quantitatively evaluate the proposed algorithms in terms of running time, task failure ratio, trust and cost using a customer objective function on WorkflowSim, a well-known cloud simulation tool. Experimental results based on real-world workflows show that ; outperforms other optimal solutions on finding the tradeoff between trust and cost, which is 3 to 6\% better than the best competitor algorithm.},
  archive      = {J_COMJNL},
  author       = {Donglai, Fu and Yanhua, Liu},
  doi          = {10.1093/comjnl/bxaa202},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {929-940},
  shortjournal = {Comput. J.},
  title        = {Trust-aware task allocation in collaborative crowdsourcing model},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The relationship between the g-extra connectivity and the
g-extra diagnosability of networks under the MM* model. <em>COMJNL</em>,
<em>64</em>(6), 921–928. (<a
href="https://doi.org/10.1093/comjnl/bxaa200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by ; -extra connectivity, the ; -extra diagnosability is proposed as a better and more realistic measurement for fault diagnosis of interconnection networks, which is defined as the maximum number of faulty vertices that can be identified when each remaining component has no fewer ; vertices. Under the MM* model, a variety of interconnection networks’ ; -extra diagnosability have been investigated, such as hypercube, folded hypercube, ; -star network, alternating group graph, etc. These results mostly share similar derivation processes to derive the ; -extra diagnosability of involved networks by using the ; -extra connectivity. Therefore, a general approach to derive the ; -extra diagnosability of a network from its ; -extra connectivity was investigated in (Wang, S. Y. and Wang, M. (2019) The ; -good-neighbor and ; -extra diagnosability of networks. Theor. Comput. Sci., ; , 107–114) and (Huang, Y. Z., Lin, L. M. and Xu, L. (2020) A new proof for exact relationship between extra connectivity and extra diagnosability of regular connected graphs under MM* model. Theor. Comput. Sci., ; , 70–80). However, there are some shortcomings in both references. By summarizing the existing shared practices, we propose a new relationship between the ; -extra connectivity and the ; -extra diagnosability of networks under the MM* model. As applications, we derive the ; -extra diagnosability of bijective connection networks and ; -star graphs.},
  archive      = {J_COMJNL},
  author       = {Yuan, Jun and Liu, Aixia and Wang, Xi},
  doi          = {10.1093/comjnl/bxaa200},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {921-928},
  shortjournal = {Comput. J.},
  title        = {The relationship between the g-extra connectivity and the g-extra diagnosability of networks under the MM* model},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid-based deep belief network model for cement
compressive strength prediction. <em>COMJNL</em>, <em>64</em>(6),
909–920. (<a href="https://doi.org/10.1093/comjnl/bxaa197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive strength is one of the most important qualities of concrete, and most of the conventional regression models for predicting the concrete strength could not achieve an expected result due to the unstructured factors. Moreover, the utilization of machine learning and statistical approaches playing its vital role in predicting the concrete compressive strength based on mixture proportions accounting to its industrial importance as well. In this manner, this paper attempts to introduce a new deep learning-based prediction model that makes the prediction more accurate, hence Deep Belief Network (DBN) is used. Moreover, to make the prediction more precise, it is planned to have the fine-tuning of activation function and weights of DBN, which makes the model efficient in its performance. For this purpose, an improved optimization concept is introduced called Lion Algorithm with new Rate Evaluation, which is the modified Lion Algorithm (LA). Finally, the performance of the proposed model is evaluated over other state-of-the-art models concerning certain error analysis.},
  archive      = {J_COMJNL},
  author       = {Shaswat, Kumar},
  doi          = {10.1093/comjnl/bxaa197},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {909-920},
  shortjournal = {Comput. J.},
  title        = {Hybrid-based deep belief network model for cement compressive strength prediction},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A call center system based on expert systems for the
acquisition of agricultural knowledge transferred from text-to-speech in
china. <em>COMJNL</em>, <em>64</em>(6), 895–908. (<a
href="https://doi.org/10.1093/comjnl/bxaa195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is rich knowledge in expert systems that can be used to solve practical problems, but its promotion and application must rely on information facilities. The application of both computers and the Internet for Chinese farmers are not common, which leads to restrictions on the promotion and application of expert systems in rural areas of China. On the other hand, the existing call centers lack a professional knowledge base and the method of automatically calling the knowledge base in real-time, which makes it difficult to meet the needs of users wanting to obtain knowledge in a timely manner. To address these problems, a call center embedded in an expert system inference algorithm and knowledge base for farmers to obtain agricultural knowledge through mobile phones or fixed-line telephones was established. By studying the event-condition-action-based (ECA-based) database triggering model, remote method invocation-based (RMI-based) communication and iterative dichotomiser 3 algorithm-based (ID3-based) parameter extraction, the cohesion between the call center and the expert system was realized. The agricultural knowledge audio acquisition model was then coupled with the call center and the expert system was constructed, allowing farmers to acquire agricultural knowledge through mobile phones or fixed phones with fast responses. When used for cotton disease diagnosis, it can achieve a high diagnostic success rate (above 75\%) when at least three disease symptoms are input into the expert system via the voice call, which provides an effective channel for Chinese farmers to obtain agricultural knowledge. It presents good application prospects in China, where 5G technology is currently developing rapidly.},
  archive      = {J_COMJNL},
  author       = {Dong, Yuhong and Fu, Zetian and Stankovski, Stevan and Peng, Yaoqi and Li, Xinxing},
  doi          = {10.1093/comjnl/bxaa195},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {895-908},
  shortjournal = {Comput. J.},
  title        = {A call center system based on expert systems for the acquisition of agricultural knowledge transferred from text-to-speech in china},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A proposed framework for cloud-aware multimodal multimedia
big data analysis toward optimal resource allocation. <em>COMJNL</em>,
<em>64</em>(6), 880–894. (<a
href="https://doi.org/10.1093/comjnl/bxaa192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of this paper is to demonstrate the structural design of Multimodal Multimedia Services in Cloud Platform (MMSCP). Thus, our proposed MMSCP architecture is built of three levels such as, Service Platform, Execution Platform and Structural platform. The functionality of service platform is to gather different forms of video files generated by the media creators and to store these files on the local platform. The second execution platform integrates both the Hadoop and Mapreduce functionalities. Finally, QoS based cloud computing functionalities (i.e. load balancing, security, resource allocation and network traffic management) is employed at the third structural platform. Likely, we introduced the Crow Search Algorithm (CSA) in structural platform for optimal allocation of resources. We adapt a Hadoop cluster to perform the experiment. Also, to conduct the resource allocation experiment we used some of the conventional optimization algorithms such as, ABC, GA and PSO for comparison with our proposed CSA algorithm imposed on the structural platform. However, to evaluate the performance of the algorithms we configured the CloudAnalyst tool. The simulation results illustrate that the proposed algorithm can allocate the virtual machine (VM) optimally to attain a minimal response time.},
  archive      = {J_COMJNL},
  author       = {Sasikala, S and Gomathi, S and Geetha, V and Murali, L},
  doi          = {10.1093/comjnl/bxaa192},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {880-894},
  shortjournal = {Comput. J.},
  title        = {A proposed framework for cloud-aware multimodal multimedia big data analysis toward optimal resource allocation},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simulink implementation of the data distribution service for
vehicular controllers on top of GBE and AFDX. <em>COMJNL</em>,
<em>64</em>(6), 860–879. (<a
href="https://doi.org/10.1093/comjnl/bxaa176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular networks have seen major changes in the past few years in order to offer reliable and real-time capable high-speed data transmission between electrical and mechatronic components to map current and future innovative functions into distributed systems within automotive applications. In the same context, the real-time middleware data distribution service (DDS) is an appropriate alternative for the standard vehicular middleware considering that it handles quality of service (QoS) parameters including real-time ones. In this paper, we are proposing a new approach for DDS implementation and integration into the vehicular system by creating a model-based design blocks. To validate this implementation, we have used the case of the Society of Automotive Engineers (SAE) vehicle benchmark as a simulation and test model. Therefore, we designed a Simulink vehicle as specified by SAE benchmark. Then, we have introduced a new methodology to link each module to a Simulink DDS blockset. The goal of this approach is to facilitate the use of DDS with vehicular controllers and to reduce the deployment and configuration complexities associated with DDS. It will also enable distributed real-time embedded systems developers to concentrate more on the business logic of the application instead of the low-level implementation details. The final developed architecture has been tested using three different types of real-time networks: FlexRay, Gigabit Ethernet and AFDX, to demonstrate that real-time application’s QoS are always met using this model.},
  archive      = {J_COMJNL},
  author       = {Takrouni, Manel and Bouhouch, Rim and Hasnaoui, Salem},
  doi          = {10.1093/comjnl/bxaa176},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {860-879},
  shortjournal = {Comput. J.},
  title        = {Simulink implementation of the data distribution service for vehicular controllers on top of GBE and AFDX},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting the performance of DNS queries on a DNS hierarchy
testbed over dual-stack. <em>COMJNL</em>, <em>64</em>(6), 843–859. (<a
href="https://doi.org/10.1093/comjnl/bxaa143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of IoT devices and their need to use IPv6 addresses has the potential to create load stress on the existing DNS infrastructure and it is imperative that DNS servers to be deployed on IPv6 networks. The DNS query latency from a particular Internet vantage point for IPv4 and IPv6 network cannot be compared directly due to variations in the number of hops of query on IPv4 and IPv6 communication networks. Moreover, there is no assurance that DNS server in the hierarchy is hosted on a dual-stack. This work brings out the DNS query resolution latency over the IPv4 and IPv6 protocol stacks with better accuracy. The experiments are carried out by setting up a complete DNS hierarchy (ROOT, TLD, STLD, TTLD and recursive resolver) on dual IP stack (IPv4 and IPv6), enabling both forward and reverse lookup tree on a live testbed, ensuring a constant number of hops between the recursive resolver and each of the DNS servers in the hierarchy. This live testbed is a first of its kind and is made available for Internet researchers. The operational issues encountered during this deployment and service provisioning are discussed and documented in this paper. This paper also gives a clear illustration and provides reference guidelines for the DNS hierarchy setup, and also aims to bridge the knowledge gap required for deploying DNS over IPv6.},
  archive      = {J_COMJNL},
  author       = {Adiwal, Sanjay and Rajendran, Balaji and Shetty D, Pushparaj and Palaniappan, Gopinath},
  doi          = {10.1093/comjnl/bxaa143},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {843-859},
  shortjournal = {Comput. J.},
  title        = {Revisiting the performance of DNS queries on a DNS hierarchy testbed over dual-stack},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Software testing automation of VR-based systems with haptic
interfaces. <em>COMJNL</em>, <em>64</em>(5), 826–841. (<a
href="https://doi.org/10.1093/comjnl/bxaa054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As software systems have increased in complexity, manual testing has become harder or even infeasible. In addition, each test phase and application domain may have its idiosyncrasies in relation to testing automation. Techniques and tools to automate test oracles in domains such as graphical user interfaces are available; nevertheless, they are scarce in the virtual reality (VR) realm. We present an approach to automate software testing in VR-based systems with haptic interfaces—interfaces that allow bidirectional communication during human–computer interaction, capturing movements and providing touch feedback. It deals with the complexity and characteristics of haptic interfaces to apply the record and playback technique. Our approach also provides inference rules to identify possible faulty modules of the system under testing. A case study was performed with three systems: a system with primitive virtual objects, a dental anesthesia simulator and a game. Faulty versions of the systems were created by seeding faults manually and by using mutation operators. The results showed that 100\% of the manually seeded faults and 93\% of mutants were detected. Moreover, the inference rules helped identify the faulty modules of the systems, suggesting that the approach improves the test activity in VR-based systems with haptic interfaces.},
  archive      = {J_COMJNL},
  author       = {Corrêa, Cléber G and Delamaro, Márcio E and Chaim, Marcos L and Nunes, Fátima L S},
  doi          = {10.1093/comjnl/bxaa054},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {826-841},
  shortjournal = {Comput. J.},
  title        = {Software testing automation of VR-based systems with haptic interfaces},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strong menger connectedness of augmented k-ary n-cubes.
<em>COMJNL</em>, <em>64</em>(5), 812–825. (<a
href="https://doi.org/10.1093/comjnl/bxaa037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A connected graph ; is called strongly Menger (edge) connected if for any two distinct vertices ; of ; , there are ; internally disjoint (edge disjoint) paths between ; and ; . Motivated by parallel routing in networks with faults, Oh and Chen (resp., Qiao and Yang) proposed the (fault-tolerant) strong Menger (edge) connectivity as follows. A graph ; is called ; -strongly Menger (edge) connected if ; remains strongly Menger (edge) connected for an arbitrary vertex set ; (resp. edge set ; ) with ; . A graph ; is called ; -conditional strongly Menger (edge) connected if ; remains strongly Menger (edge) connected for an arbitrary vertex set ; (resp. edge set ; ) with ; and ; . In this paper, we consider strong Menger (edge) connectedness of the augmented ; -ary ; -cube ; , which is a variant of ; -ary ; -cube ; . By exploring the topological proprieties of ; , we show that ; (resp. ; , ; ) is ; -strongly (resp. ; -strongly) Menger connected for ; (resp. ; ) and ; is ; -strongly Menger edge connected for ; and ; . Moreover, we obtain that ; is ; -conditional strongly Menger edge connected for ; and ; . These results are all optimal in the sense of the maximum number of tolerated vertex (resp. edge) faults.},
  archive      = {J_COMJNL},
  author       = {Gu, Mei-Mei and Chang, Jou-Ming and Hao, Rong-Xia},
  doi          = {10.1093/comjnl/bxaa037},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {812-825},
  shortjournal = {Comput. J.},
  title        = {Strong menger connectedness of augmented k-ary n-cubes},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From relation algebra to semi-join algebra: An approach to
graph query optimization. <em>COMJNL</em>, <em>64</em>(5), 789–811. (<a
href="https://doi.org/10.1093/comjnl/bxaa031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many graph query languages rely on composition to navigate graphs and select nodes of interest, even though evaluating compositions of relations can be costly. Often, this need for composition can be reduced by rewriting toward queries using semi-joins instead, resulting in a significant reduction of the query evaluation cost. We study techniques to recognize and apply such rewritings. Concretely, we study the relationship between the expressive power of the relation algebras, which heavily rely on composition, and the semi-join algebras, which replace composition in favor of semi-joins. Our main result is that each fragment of the relation algebras where intersection and/or difference is only used on edges (and not on complex compositions) is expressively equivalent to a fragment of the semi-join algebras. This expressive equivalence holds for node queries evaluating to sets of nodes. For practical relevance, we exhibit constructive rules for rewriting relation algebra queries to semi-join algebra queries and prove that they lead to only a well-bounded increase in the number of steps needed to evaluate the rewritten queries. In addition, on sibling-ordered trees, we establish new relationships among the expressive power of Regular XPath, Conditional XPath, FO-logic and the semi-join algebra augmented with restricted fixpoint operators.},
  archive      = {J_COMJNL},
  author       = {Hellings, Jelle and Pilachowski, Catherine L and Van Gucht, Dirk and Gyssens, Marc and Wu, Yuqing},
  doi          = {10.1093/comjnl/bxaa031},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {789-811},
  shortjournal = {Comput. J.},
  title        = {From relation algebra to semi-join algebra: An approach to graph query optimization},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covering array constructors: An experimental analysis of
their interaction coverage and fault detection. <em>COMJNL</em>,
<em>64</em>(5), 762–788. (<a
href="https://doi.org/10.1093/comjnl/bxaa020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinatorial interaction testing (CIT) aims at constructing a ; (CA) of all value combinations at a specific interaction strength, to detect faults that are caused by the interaction of parameters. CIT has been widely used in different applications, with many algorithms and tools having been proposed to support CA construction. To date, however, there appears to have been no studies comparing different CA constructors when only some of the CA test cases are executed. In this paper, we present an investigation of five popular CA constructors: ; , ; , ; , ; and ; . We conducted empirical studies examining the five programs, focusing on interaction coverage and fault detection. The experimental results show that when there is no preference or special justification for using other CA constructors, then ; is recommended—because it achieves better interaction coverage and fault detection than the other four constructors in many cases. Our results also show that when using ; or ; , their CAs must be prioritized before testing. The main reason for this is that these CAs can result in considerable interaction coverage or fault detection capabilities when executing a large number of test cases; however, they may also produce the lowest rates of fault detection and interaction coverage.},
  archive      = {J_COMJNL},
  author       = {Huang, Rubing and Chen, Haibo and Zhou, Yunan and Yueh Chen, Tsong and Towey, Dave and Fai Lau, Man and Ng, Sebastian and Merkel, Robert and Chen, Jinfu},
  doi          = {10.1093/comjnl/bxaa020},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {762-788},
  shortjournal = {Comput. J.},
  title        = {Covering array constructors: An experimental analysis of their interaction coverage and fault detection},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coordinate graph grammar for the specification of spatial
graphs. <em>COMJNL</em>, <em>64</em>(5), 749–761. (<a
href="https://doi.org/10.1093/comjnl/bxaa019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a two-dimensional formal method, graph grammar is widely used in defining various visual programming languages. This paper presents a new graph grammar formalism called ; (CGG). CGG is extended from the edge-based graph grammar (EGG) by introducing the spatial mechanism into the theoretical framework, which consists of continuous coordinate graph grammar (cCGG) and discrete coordinate graph grammar (dCGG). By combining quantitative and qualitative spatial semantics in one framework, CGG provides strong expressiveness and flexibility for specifying various spatial graphs. This paper focuses on several important issues on the new formalism. First, the theoretical framework of CGG is given. Second, two matching algorithms for cCGG and dCGG are proposed, which use the spatial relationships between nodes to narrow down the search space during parsing. Finally, an application of CGG is demonstrated, which generates parsable flowcharts in a uniform layout.},
  archive      = {J_COMJNL},
  author       = {Liu, Yufeng and Zeng, Xiaoqin and Zhang, Kang and Zou, Yang},
  doi          = {10.1093/comjnl/bxaa019},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {749-761},
  shortjournal = {Comput. J.},
  title        = {Coordinate graph grammar for the specification of spatial graphs},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel prediction-based image steganography by support vector
neural network. <em>COMJNL</em>, <em>64</em>(5), 731–748. (<a
href="https://doi.org/10.1093/comjnl/bxaa017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Securing the privacy of the medical information through the image steganography process has gained more research interest nowadays to protect the privacy of the patient. In the existing works, least significant bit (LSB) replacement strategy was most popularly used to hide the sensitive contents. Here, every pixel was replaced for achieving higher privacy, but it increased the complexity. This work introduces a novel pixel prediction scheme-based image steganography to overcome the complexity issues prevailing in the existing works. In the proposed pixel prediction scheme, the support vector neural network (SVNN) classifier is utilized for the construction of a prediction map, which identifies the suitable pixels for the embedding process. Then, in the embedding phase, wavelet coefficients are extracted from the medical image based on discrete wavelet transform (DWT) and embedding strength, and the secret message is embedded into the HL wavelet band. Finally, the secret message is extracted from the medical image on applying the DWT. The experimentation of the proposed pixel prediction scheme is done by utilizing the medical images from the BRATS database. The proposed pixel prediction scheme has achieved high performance with the values of 48.558 dB, 0.50009 and 0.9879 for the peak signal to noise ratio (PSNR), Structural Similarity Index (SSIM) and correlation factor, respectively.},
  archive      = {J_COMJNL},
  author       = {V K, Reshma and R S, Vinod Kumar},
  doi          = {10.1093/comjnl/bxaa017},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {731-748},
  shortjournal = {Comput. J.},
  title        = {Pixel prediction-based image steganography by support vector neural network},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smaller compressed suffix arrays. <em>COMJNL</em>,
<em>64</em>(5), 721–730. (<a
href="https://doi.org/10.1093/comjnl/bxaa016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An alternative to compressed suffix arrays is introduced, based on representing a sequence of integers using Fibonacci encodings, thereby reducing the space requirements of state-of-the-art implementations of the suffix array, while retaining the searching functionalities. Empirical tests support the theoretical space complexity improvements and show that there is no deterioration in the processing times.},
  archive      = {J_COMJNL},
  author       = {Benza, Ekaterina and Klein, Shmuel T and Shapira, Dana},
  doi          = {10.1093/comjnl/bxaa016},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {721-730},
  shortjournal = {Comput. J.},
  title        = {Smaller compressed suffix arrays},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the (k,t)-metric dimension of graphs. <em>COMJNL</em>,
<em>64</em>(5), 707–720. (<a
href="https://doi.org/10.1093/comjnl/bxaa009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMJNL},
  author       = {Estrada-Moreno, A and Yero, I G and Rodríguez-Velázquez, J A},
  doi          = {10.1093/comjnl/bxaa009},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {707-720},
  shortjournal = {Comput. J.},
  title        = {On the (k,t)-metric dimension of graphs},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating context to preferences and goals for
goal-oriented adaptability of software systems. <em>COMJNL</em>,
<em>64</em>(5), 675–706. (<a
href="https://doi.org/10.1093/comjnl/bxz167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern software systems are continuously seeking for adaptability realizations, to generate better fit behaviours in response to domain changes. Requirements variability motivates adaptability; hence, understanding the influence of the domain changes, i.e. context variability, to requirements variability is necessary. In this paper, we propose an approach for context-based requirements variability analysis in the goal-oriented requirements modelling. We define contextual goals and contextual preferences to specify the relationships of contexts with requirements and preferences, respectively. Given a requirements problem represented through a goal model, we use the contextual goals to derive applicable solutions at a given situation. Then, from those applicable solutions, we use the contextual preferences as criteria for evaluating and selecting the ones that would best satisfy stakeholder priorities. To support our variability analysis, we develop a tool to automate the derivation and evaluation of the solutions. We further demonstrate the use of our approach in detecting modelling errors and validating the impact of prioritizations, leading to improvements in the requirements specifications. Our approach broadens the scope of requirements variability by weaving context variability with both stakeholder goals and preferences, in order to sufficiently represent the adaptability needs of software systems where contextual changes are commonplace.},
  archive      = {J_COMJNL},
  author       = {Botangen, Khavee Agustus and Yu, Jian and Yeap, Wai Kiang and Sheng, Quan Z},
  doi          = {10.1093/comjnl/bxz167},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {675-706},
  shortjournal = {Comput. J.},
  title        = {Integrating context to preferences and goals for goal-oriented adaptability of software systems},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient group ID-based encryption with equality test
against insider attack. <em>COMJNL</em>, <em>64</em>(4), 661–674. (<a
href="https://doi.org/10.1093/comjnl/bxaa120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ID-based encryption with equality test (IBEET) allows a tester to compare ciphertexts encrypted under different public keys for checking whether they contain the same message. In this paper, we first introduce group mechanism into IBEET and propose a new primitive, namely group ID-based encryption with equality test (G-IBEET). With the group mechanism: (1) group administrator can authorize a tester to make comparison between ciphertexts of group users, but it cannot compare their ciphertexts with any ciphertext of any user who is not in the group. Such group granularity authorization can make IBEET that adapts to group scenario; (2) for the group granularity authorization, only one trapdoor, named group trapdoor, should be issued to the tester, which can greatly reduce the cost of computation, transmission and storage of trapdoors in traditional IBEET schemes; (3) G-IBEET can resist the insider attack launched by the authorized tester, which is an open problem in IBEET. We give definitions for G-IBEET and propose a concrete construction with an efficient test algorithm. We then give its security analysis in the random oracle model.},
  archive      = {J_COMJNL},
  author       = {Ling, Yunhao and Ma, Sha and Huang, Qiong and Li, Ximing and Zhong, Yijian and Ling, Yunzhi},
  doi          = {10.1093/comjnl/bxaa120},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {661-674},
  shortjournal = {Comput. J.},
  title        = {Efficient group ID-based encryption with equality test against insider attack},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Security analysis of the first certificateless proxy
signature scheme against malicious-but-passive KGC attacks.
<em>COMJNL</em>, <em>64</em>(4), 653–660. (<a
href="https://doi.org/10.1093/comjnl/bxaa105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Yang ; proposed the first certificateless proxy signature scheme against malicious-but-passive key generation center (MKGC) attacks. They proved that their scheme can resist the MKGC attacks in the standard model. In this paper, we point out that their scheme cannot achieve this security because the adversary can forge valid signatures.},
  archive      = {J_COMJNL},
  author       = {Lin, Xi-Jun and Wang, Qihui and Sun, Lin and Yan, Zhen and Liu, Peishun},
  doi          = {10.1093/comjnl/bxaa105},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {653-660},
  shortjournal = {Comput. J.},
  title        = {Security analysis of the first certificateless proxy signature scheme against malicious-but-passive KGC attacks},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A modern view on forward security. <em>COMJNL</em>,
<em>64</em>(4), 639–652. (<a
href="https://doi.org/10.1093/comjnl/bxaa104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forward security ensures that compromise of entities today does not impact the security of cryptographic primitives employed in the past. Such a form of security is regarded as increasingly important in the modern world due to the existence of adversaries with mass storage capabilities and powerful infiltration abilities. Although the idea of forward security has been known for over 30 years, current understanding of what it really should mean is limited due to the prevalence of new techniques and inconsistent terminology. We survey existing methods for achieving forward security for different cryptographic primitives and propose new definitions and terminology aimed at a unified treatment of the notion.},
  archive      = {J_COMJNL},
  author       = {Boyd, Colin and Gellert, Kai},
  doi          = {10.1093/comjnl/bxaa104},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {639-652},
  shortjournal = {Comput. J.},
  title        = {A modern view on forward security},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpolation attacks on round-reduced elephant, kravatte
and xoofff. <em>COMJNL</em>, <em>64</em>(4), 628–638. (<a
href="https://doi.org/10.1093/comjnl/bxaa101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an interpolation attack using the ; . This can reduce the time complexity to get a linear system of equations for specified intermediate state bits, which is general to cryptanalysis of some ciphers with update function of low algebraic degree. Along this line, we perform an interpolation attack against ; , a round 2 submission of the ongoing national institute of standards and technology (NIST) lightweight cryptography project. This is the first third-party cryptanalysis on this cipher. Moreover, we promote the interpolation attack by applying it to the ; pseudo-random constructions ; and ; . Our attacks turn out to be the most efficient method for these ciphers thus far.},
  archive      = {J_COMJNL},
  author       = {Zhou, Haibo and Zong, Rui and Dong, Xiaoyang and Jia, Keting and Meier, Willi},
  doi          = {10.1093/comjnl/bxaa101},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {628-638},
  shortjournal = {Comput. J.},
  title        = {Interpolation attacks on round-reduced elephant, kravatte and xoofff},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving matsui’s search algorithm for the best
differential/linear trails and its applications for DES, DESL and GIFT.
<em>COMJNL</em>, <em>64</em>(4), 610–627. (<a
href="https://doi.org/10.1093/comjnl/bxaa090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic search methods have been widely used for cryptanalysis of block ciphers, especially for the most classic cryptanalysis methods—differential and linear cryptanalysis. However, the automatic search methods, no matter based on MILP, SMT/SAT or CP techniques, can be inefficient when the search space is too large. In this paper, we propose three new methods to improve Matsui’s branch-and-bound search algorithm, which is known as the first generic algorithm for finding the best differential and linear trails. The three methods, named ; , ; and ; , respectively, can efficiently speed up the search process by reducing the search space as much as possible and reducing the cost of executing linear layer operations. We apply our improved algorithm to DESL and GIFT, which are still the hard instances for the automatic search methods. As a result, we find the best differential trails for DESL (up to 14-round) and GIFT-128 (up to 19-round). The best linear trails for DESL (up to 16-round), GIFT-128 (up to 10-round) and GIFT-64 (up to 15-round) are also found. To the best of our knowledge, these security bounds for DESL and GIFT under single-key scenario are given for the first time. Meanwhile, it is the longest exploitable (differential or linear) trails for DESL and GIFT. Furthermore, benefiting from the efficiency of the improved algorithm, we do experiments to demonstrate that the clustering effect of differential trails for 13-round DES and DESL are both weak.},
  archive      = {J_COMJNL},
  author       = {Ji, Fulei and Zhang, Wentao and Ding, Tianyou},
  doi          = {10.1093/comjnl/bxaa090},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {610-627},
  shortjournal = {Comput. J.},
  title        = {Improving matsui’s search algorithm for the best Differential/Linear trails and its applications for DES, DESL and GIFT},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient algorithm to extract control flow-based
features for IoT malware detection. <em>COMJNL</em>, <em>64</em>(4),
599–609. (<a href="https://doi.org/10.1093/comjnl/bxaa087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control flow-based feature extraction method has the ability to detect malicious code with higher accuracy than traditional text-based methods. Unfortunately, this method has been encountered with the NP-hard problem, which is infeasible for the large-sized and high-complexity programs. To tackle this, we propose a control flow-based feature extraction dynamic programming algorithm for fast extraction of control flow-based features with polynomial time O(; ), where ; is the number of basic blocks in decompiled executable codes. From the experimental results, it is demonstrated that the proposed algorithm is more efficient and effective in detecting malware than the existing ones. Applying our algorithm to an Internet of Things dataset gives better results on three measures: Accuracy = 99.05\%, False Positive Rate = 1.31\% and False Negative Rate = 0.66\%.},
  archive      = {J_COMJNL},
  author       = {Nghi Phu, Tran and Dai Tho, Nguyen and Huy Hoang, Le and Ngoc Toan, Nguyen and Ngoc Binh, Nguyen},
  doi          = {10.1093/comjnl/bxaa087},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {599-609},
  shortjournal = {Comput. J.},
  title        = {An efficient algorithm to extract control flow-based features for IoT malware detection},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On enabling attribute-based encryption to be traceable
against traitors. <em>COMJNL</em>, <em>64</em>(4), 575–598. (<a
href="https://doi.org/10.1093/comjnl/bxaa082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based encryption (ABE) is a versatile one-to-many encryption primitive, which enables fine-grained access control over encrypted data. Due to its promising applications in practice, ABE schemes with high efficiency, security and expressivity have been continuously emerging. On the other hand, due to the nature of ABE, a malicious user may abuse its decryption privilege. Therefore, being able to identify such a malicious user is crucial towards the practicality of ABE. Although some specific ABE schemes in the literature enjoys the tracing function, they are only proceeded case by case. Most of the ABE schemes do not support traceability. It is thus meaningful and important to have ; . In this work, we partially solve the aforementioned problem. Namely, we propose a way of transforming (non-traceable) ABE schemes satisfying certain requirements to ; ABE schemes, which adds only ; elements to the ciphertext where ; is the number of users in the system. And to demonstrate the practicability of our transformation, we show how to convert a couple of existing non-traceable ABE schemes to support traceability.},
  archive      = {J_COMJNL},
  author       = {Liu, Zhen and Huang, Qiong and Wong, Duncan S},
  doi          = {10.1093/comjnl/bxaa082},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {575-598},
  shortjournal = {Comput. J.},
  title        = {On enabling attribute-based encryption to be traceable against traitors},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KORGAN: An efficient PKI architecture based on PBFT through
dynamic threshold signatures. <em>COMJNL</em>, <em>64</em>(4), 564–574.
(<a href="https://doi.org/10.1093/comjnl/bxaa081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past decade, several misbehaving certificate authorities (CAs) have issued fraudulent TLS certificates allowing man-in-the-middle (MITM) kinds of attacks that result in serious security incidents. In order to avoid such incidents, Yakubov ; . ((2018) A blockchain-based PKI management framework. NOMS 2018 - 2018 IEEE/IFIP Network Operations and Management Symposium, Taipei, Taiwan, April, pp. 16. IEEE) recently proposed a new public key infrastructure (PKI) architecture where CAs issue, revoke and validate X.509 certificates on a public blockchain. However, in their proposal TLS clients are subject to MITM kinds of attacks, and certificate transparency is not fully provided. In this paper, we eliminate the issues of the Yakubov ; .’s scheme and propose a new PKI architecture based on permissioned blockchain with PBFT consensus mechanism where the consensus nodes utilize a dynamic threshold signature scheme to generate signed blocks. In this way, the trust to the intermediary entities can be completely eliminated during certificate validation. Our scheme enjoys the dynamic property of the threshold signature because TLS clients do not have to change the verification key even if the validator set is dynamic. We implement our proposal on private Ethereum network to demonstrate the experimental results. The results show that our proposal has negligible overhead during TLS handshake. The certificate validation duration is less than the duration in the conventional PKI and Yakubov ; .’s scheme.},
  archive      = {J_COMJNL},
  author       = {Yasin Kubilay, Murat and Sabir Kiraz, Mehmet and Ali Mantar, Haci},
  doi          = {10.1093/comjnl/bxaa081},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {564-574},
  shortjournal = {Comput. J.},
  title        = {KORGAN: An efficient PKI architecture based on PBFT through dynamic threshold signatures},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-factor decryption: A better way to protect data security
and privacy. <em>COMJNL</em>, <em>64</em>(4), 550–563. (<a
href="https://doi.org/10.1093/comjnl/bxaa080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric information is unique to a human, so it would be desirable to use the biometric characteristic as the private key in a cryptographic system to protect data security and privacy. In this paper, we introduce a notion called two-factor decryption (TFD). Informally speaking, a TFD scheme is a variant of the public-key encryption (PKE) scheme. In a TFD scheme, messages are encrypted under public keys as that in a standard PKE scheme, but both private keys (i.e. the first factor) and biometric inputs (i.e. the second factor) are required to decrypt the ciphertexts and obtain the underlying plaintexts. We first describe a framework of TFD, and then define a formal security model for TFD. Thereafter, we present a generic construction on TFD based on the cryptographic primitives of linear sketch and functional encryption (FE) with certain properties and analyse its security. In addition, we give instantiations of TFD by applying concrete FE schemes into the generic construction and show their applications.},
  archive      = {J_COMJNL},
  author       = {Cui, Hui and Paulet, Russell and Nepal, Surya and Yi, Xun and Mbimbi, Butrus},
  doi          = {10.1093/comjnl/bxaa080},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {550-563},
  shortjournal = {Comput. J.},
  title        = {Two-factor decryption: A better way to protect data security and privacy},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-malleable zero-knowledge arguments with lower round
complexity. <em>COMJNL</em>, <em>64</em>(4), 534–549. (<a
href="https://doi.org/10.1093/comjnl/bxaa076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Round complexity is one of the fundamental problems in zero-knowledge (ZK) proof systems. Non-malleable zero-knowledge (NMZK) protocols are ZK protocols that provide security even when man-in-the-middle adversaries interact with a prover and a verifier simultaneously. It is known that the first ; NMZK arguments for NP can be constructed by assuming the existence of ; (Pass, R. and Rosen, A. (2005) New and Improved Constructions of Non-Malleable Cryptographic Protocols. In Gabow, H.N. and Fagin, R. (eds) Proc. 37th Annual ACM Symposium on Theory of Computing, Baltimore, MD, USA, May 2224, 2005, pp. 533542. ACM) and has relatively high round complexity; the first ; NMZK arguments for NP can be constructed in the plain model by assuming the existence of ; (Goyal, V., Richelson, S., Rosen, A. and Vald, M. (2014) An Algebraic Approach to Non-Malleability. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 1821, 2014, pp. 4150. IEEE Computer Society and Ciampi, M., Ostrovsky, R., Siniscalchi, L. and Visconti, I. (2017) Delayed-Input Non-Malleable Zero Knowledge and Multi-Party Coin Tossing in Four Rounds. In Kalai, Y. and Reyzin, L. (eds) Theory of Cryptography15th Int. Conf., TCC 2017. Lecture Notes in Computer Science, Baltimore, MD, USA, November 1215, 2017, Part I, Vol. 10677, pp. 711742. Springer). In this paper, we present a ; NMZK ; system assuming the existence of collision-resistant hash functions and a ; NMZK ; from ; assumption in the keyless setting.},
  archive      = {J_COMJNL},
  author       = {Yan, Zhenbin and Deng, Yi},
  doi          = {10.1093/comjnl/bxaa076},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {534-549},
  shortjournal = {Comput. J.},
  title        = {Non-malleable zero-knowledge arguments with lower round complexity},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large universe CCA2 CP-ABE with equality and validity test
in the standard model. <em>COMJNL</em>, <em>64</em>(4), 509–533. (<a
href="https://doi.org/10.1093/comjnl/bxaa075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based encryption with equality test (ABEET) simultaneously supports fine-grained access control on the encrypted data and plaintext message equality comparison without decrypting the ciphertexts. Recently, there have been several literatures about ABEET proposed. Nevertheless, most of them explore the ABEET schemes in the random oracle model, which has been pointed out to have many defects in practicality. The only existing ABEET scheme in the standard model, proposed by Wang ; , merely achieves the indistinguishable against chosen-plaintext attack security. Considering the aforementioned problems, in this paper, we propose the first direct adaptive chosen-ciphertext security ciphertext-policy ABEET scheme in the standard model. Our method only adopts a chameleon hash function and adds one dummy attribute to the access structure. Compared with the previous works, our scheme achieves the security improvement, ciphertext validity check and large universe. Besides, we further optimize our scheme to support the outsourced decryption. Finally, we first give the detailed theoretical analysis of our constructions in computation and storage costs, then we implement our constructions and carry out a series of experiments. Both results indicate that our constructions are more efficient in Setup and Trapdoor and have the shorter public parameters than the existing ABEET ones do.},
  archive      = {J_COMJNL},
  author       = {Li, Cong and Shen, Qingni and Xie, Zhikang and Feng, Xinyu and Fang, Yuejian and Wu, Zhonghai},
  doi          = {10.1093/comjnl/bxaa075},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {509-533},
  shortjournal = {Comput. J.},
  title        = {Large universe CCA2 CP-ABE with equality and validity test in the standard model},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A random forest classification algorithm based personal
thermal sensation model for personalized conditioning system in office
buildings. <em>COMJNL</em>, <em>64</em>(3), 500–508. (<a
href="https://doi.org/10.1093/comjnl/bxaa165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The personal thermal sensation model is used as the main component for personalized conditioning system, which is an effective method to fulfill thermal comfort requirements of the occupants, considering the energy consumption. The Random Forest classification algorithm based thermal sensation model is developed in this study, which combines indoor air quality parameters, personal information, physiological factors and occupancy preferences on selection of 7-level of sensation: cold, cool, slightly cool, neutral, slightly warm, warm and hot. Our model shows better functionality, as well as performance and factor selection. As a result, our method has achieved 70.2\% accuracy, comparing with the 57.4\% accuracy of support vector machine, and 67.7\% accuracy of neutral network in an ASHRAE RP-884 database. Therefore, our newly developed model can be used in personalized thermal adjustment systems with intelligent control functions.},
  archive      = {J_COMJNL},
  author       = {Li, Qing Yun and Han, Jie and Lu, Lin},
  doi          = {10.1093/comjnl/bxaa165},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {500-508},
  shortjournal = {Comput. J.},
  title        = {A random forest classification algorithm based personal thermal sensation model for personalized conditioning system in office buildings},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning-based sentiment analysis of facebook data: The
case of turkish users. <em>COMJNL</em>, <em>64</em>(3), 473–499. (<a
href="https://doi.org/10.1093/comjnl/bxaa172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis (SA) is an essential task for many domains where it is crucial to know users’ public opinion about events, products, brands, politicians and so on. Existing works on SA have concentrated on English texts including Twitter feeds and user reviews on hotels, movies and products. On the other hand, Facebook, as an online social network (OSN), has attracted quite limited attention from the research community. Among these, SA work on Turkish text obtained from OSNs are extremely scarce. In this paper, our aim is to perform SA on public Facebook data collected from Turkish user accounts. Our study differs from existing studies in terms of the data set scale, the natural language of the texts in the data set and the extent of experimental analyses that include both machine learning and deep learning techniques. We extensively report not only the results of different learning models involving SA but also statistical distribution of metadata of user activities across various user attributes (e.g. gender and age). Our experimental results indicate that recurrent neural networks achieve the best accuracy (i.e. 0.916) with word embeddings. To the best of our knowledge, this is the best result for SA on Facebook data in the context of the Turkish language.},
  archive      = {J_COMJNL},
  author       = {Çoban, Önder and Özel, Selma Ayşe and İnan, Ali},
  doi          = {10.1093/comjnl/bxaa172},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {473-499},
  shortjournal = {Comput. J.},
  title        = {Deep learning-based sentiment analysis of facebook data: The case of turkish users},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generic analogy-centered software cost estimation based on
differential evolution exploration process. <em>COMJNL</em>,
<em>64</em>(3), 462–472. (<a
href="https://doi.org/10.1093/comjnl/bxaa199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software cost estimation is the prediction of development effort and calendar time required to develop any software project. It is considered to be the very fundamental task for successful execution of an on-going project as well as budgetary requirements of futuristic projects. As accuracy in software cost estimation is very hard because of the availability of vague information at the time of inception of the software project, it prompted many researchers to explore in this domain from past decades. Their pioneer works suggest a bulk of techniques for this purpose. However, because of the availability of large number of estimation techniques, it becomes hard for any software practitioners to select an appropriate one. To help the industry practitioners in these situations, a novel analogy-centered model based on differential evolution exploration process is proposed in this research study. The proposed model has been assessed on 676 projects from 5 different data sets and the results achieved are significantly better when compared with other benchmark analogy-based estimation studies. Furthermore, being the very less computational cost of the proposed model, it is suggested that the proposed model be considered as the preliminary stage of any analogy-based software estimation technique.},
  archive      = {J_COMJNL},
  author       = {Wani, Zahid Hussain and Bhat, Javaid Iqbal and Giri, Kaisar Javeed},
  doi          = {10.1093/comjnl/bxaa199},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {462-472},
  shortjournal = {Comput. J.},
  title        = {A generic analogy-centered software cost estimation based on differential evolution exploration process},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Positioning and categorizing mass media using reaction
emojis on facebook. <em>COMJNL</em>, <em>64</em>(3), 451–461. (<a
href="https://doi.org/10.1093/comjnl/bxaa167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of social network services, a paradigm shift in communication between media organizations and the audience has occurred. Numerous mass media agencies established fan pages on social platforms, such as Facebook, Twitter and Instagram, to disseminate breaking news, promote reports and interact with their audience. In this study, we leverage the reaction emojis delivered from users to media fan pages on Facebook to investigate how users react to media organizations and the implications of selective exposure. Using a 1-year-long observation of user activities on mass media pages, we perform a series of quantitative approaches to locate media agencies, measure the distances between them and cluster organizations into groups. A total of 30 fan pages of mass media organizations in Taiwan are investigated. The outcomes suggest that the report genres and topics are key factors to categorize media groups through reaction emojis from the online audience.},
  archive      = {J_COMJNL},
  author       = {Wang, Ming-Hung},
  doi          = {10.1093/comjnl/bxaa167},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {451-461},
  shortjournal = {Comput. J.},
  title        = {Positioning and categorizing mass media using reaction emojis on facebook},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting spam product reviews in roman urdu script.
<em>COMJNL</em>, <em>64</em>(3), 432–450. (<a
href="https://doi.org/10.1093/comjnl/bxaa164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, online customer reviews have become the main source to determine public opinion about offered products and services. Therefore, manufacturers and sellers are extremely concerned with customer reviews, as these can have a direct impact on their businesses. Unfortunately, there is an increasing trend to write spam reviews to promote or demote targeted products or services. This practice, known as review spamming, has posed many questions regarding the authenticity and dependability of customers’ review-based business processes. Although the spam review detection (SRD) problem has gained much attention from researchers, existing studies on SRD have mostly worked on datasets of English, Chinese, Arabic, Persian, and Malay languages. Therefore, the objective of this research is to identify the spam in Roman Urdu reviews using different classification models based on linguistic features and behavioral features. The performance of each classifier is evaluated in a number of perspectives: (i) linguistic features are used to calculate accuracy (F1 score) of each classifier; (ii) behavioral features combined with distributional and non-distributional aspects are used to evaluate accuracy (F1 score) of each classifier; and (iii) the combination of both linguistic and behavioral features (distributional and non-distributional aspects) are used to evaluate the accuracy of each classifier. The experimental evaluations demonstrated an improved accuracy (F1 score: 0.96), which is the result of combinations of linguistic features and behavioral features with the distributional aspect of reviewers. Moreover, behavioral features using distributional characteristic achieve an accuracy (F1 score: 0.86) and linguistic features shows accuracy (F1 score: 0.69). The outcome of this research can be used to increase customers’ confidence in the South Asian region. It can also help to reduce spam reviews in the South Asian region, particularly in Pakistan.},
  archive      = {J_COMJNL},
  author       = {Hussain, Naveed and Mirza, Hamid Turab and Iqbal, Faiza and Hussain, Ibrar and Kaleem, Mohammad},
  doi          = {10.1093/comjnl/bxaa164},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {432-450},
  shortjournal = {Comput. J.},
  title        = {Detecting spam product reviews in roman urdu script},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stable communities detection method for temporal multiplex
graphs: Heterogeneous social network case study. <em>COMJNL</em>,
<em>64</em>(3), 418–431. (<a
href="https://doi.org/10.1093/comjnl/bxaa162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplex graphs have been recently proposed as a model to represent high-level complexity in real-world networks such as heterogeneous social networks where actors could be characterized by heterogeneous properties and could be linked with different types of social interactions. This has brought new challenges in community detection, which aims to identify pertinent groups of nodes in a complex graph. In this context, great efforts have been made to tackle the problem of community detection in multiplex graphs. However, most of the proposed methods until recently deal with static multiplex graph and ignore the temporal dimension, which is a key characteristic of real networks. Even more, the few methods that consider temporal graphs, they just propose to follow communities over time and none of them use the temporal aspect directly to detect stable communities, which are often more meaningful in reality. Thus, this paper proposes a new two-step method to detect stable communities in temporal multiplex graphs. The first step aims to find the best static graph partition at each instant by applying a new hybrid community detection algorithm, which considers both relations heterogeneities and nodes similarities. Then, the second step considers the temporal dimension in order to find final stable communities. Finally, experiments on synthetic graphs and a real social network show that this method is competitive and it is able to extract high-quality communities.},
  archive      = {J_COMJNL},
  author       = {Rebhi, Wala and Ben Yahia, Nesrine and Bellamine Ben Saoud, Narjès},
  doi          = {10.1093/comjnl/bxaa162},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {418-431},
  shortjournal = {Comput. J.},
  title        = {Stable communities detection method for temporal multiplex graphs: Heterogeneous social network case study},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Proposed artificial bee colony algorithm as feature selector
to predict the leadership perception of site managers. <em>COMJNL</em>,
<em>64</em>(3), 408–417. (<a
href="https://doi.org/10.1093/comjnl/bxaa163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets have relevant and irrelevant features whose evaluations are fundamental for classification or clustering processes. The effects of these relevant features make classification accuracy more accurate and stable. At this point, optimization methods are used for feature selection process. This process is a feature reduction process finding the most relevant feature subset without decrement of the accuracy rate obtained by original feature sets. Varied nature inspiration-based optimization algorithms have been proposed as feature selector. The density of data in construction projects and the inability of extracting these data cause various losses in field studies. In this respect, the behaviors of leaders are important in the selection and efficient use of these data. The objective of this study is implementing Artificial Bee Colony (ABC) algorithm as a feature selection method to predict the leadership perception of the construction employees. When Random Forest, Sequential Minimal Optimization and K-Nearest Neighborhood (KNN) are used as classifier, 84.1584\% as highest accuracy result and 0.805 as highest F-Measure result were obtained by using KNN and Random Forest classifier with proposed ABC Algorithm as feature selector. The results show that a nature inspiration-based optimization algorithm like ABC algorithm as feature selector is satisfactory in prediction of the Construction Employee’s Leadership Perception.},
  archive      = {J_COMJNL},
  author       = {Kaya Keles, Mumine and Kilic, Umit and Keles, Abdullah Emre},
  doi          = {10.1093/comjnl/bxaa163},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {408-417},
  shortjournal = {Comput. J.},
  title        = {Proposed artificial bee colony algorithm as feature selector to predict the leadership perception of site managers},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of social influence for provenance of
misinformation in online social network using big data approach.
<em>COMJNL</em>, <em>64</em>(3), 391–407. (<a
href="https://doi.org/10.1093/comjnl/bxaa132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks (OSNs) is a platform that plays an essential role in identifying misinformation like false rumors, insults, pranks, hoaxes, spear phishing and computational propaganda in a better way. Detection of misinformation finds its applications in areas such as law enforcement to pinpoint culprits who spread rumors to harm the society, targeted marketing in e-commerce to identify the user who originates dissatisfaction messages about products or services that harm an organizations reputation. The process of identifying and detecting misinformation is very crucial in complex social networks. As misinformation in social network is identified by designing and placing the monitors, computing the minimum number of monitors for detecting misinformation is a very trivial work in the complex social network. The proposed approach determines the top suspected sources of misinformation using a tweet polarity-based ranking system in tandem with sarcasm detection (both implicit and explicit sarcasm) with optimization approaches on large-scale incomplete network. The algorithm subsequently uses this determined feature to place the minimum set of monitors in the network for detecting misinformation. The proposed work focuses on the timely detection of misinformation by limiting the distance between the suspected sources and the monitors. The proposed work also determines the root cause of misinformation (provenance) by using a combination of network-based and content-based approaches. The proposed work is compared with the state-of-art work and has observed that the proposed algorithm produces better results than existing methods.},
  archive      = {J_COMJNL},
  author       = {P, Kumaran and Sridhar, Rajeswari},
  doi          = {10.1093/comjnl/bxaa132},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {391-407},
  shortjournal = {Comput. J.},
  title        = {Prediction of social influence for provenance of misinformation in online social network using big data approach},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment analysis of fast food companies with deep learning
models. <em>COMJNL</em>, <em>64</em>(3), 383–390. (<a
href="https://doi.org/10.1093/comjnl/bxaa131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern era, Internet usage has become a basic necessity in the lives of people. Nowadays, people can perform online shopping and check the customer’s views about products that purchased online. Social networking services enable users to post opinions on public platforms. Analyzing people’s opinions helps corporations to improve the quality of products and provide better customer service. However, analyzing this content manually is a daunting task. Therefore, we implemented sentiment analysis to make the process automatically. The entire process includes data collection, pre-processing, word embedding, sentiment detection and classification using deep learning techniques. Twitter was chosen as the source of data collection and tweets collected automatically by using Tweepy. In this paper, three deep learning techniques were implemented, which are CNN, Bi-LSTM and CNN-Bi-LSTM. Each of the models trained on three datasets consists of 50K, 100K and 200K tweets. The experimental result revealed that, with the increasing amount of training data size, the performance of the models improved, especially the performance of the Bi-LSTM model. When the model trained on the 200K dataset, it achieved about 3\% higher accuracy than the 100K dataset and achieved about 7\% higher accuracy than the 50K dataset. Finally, the Bi-LSTM model scored the highest performance in all metrics and achieved an accuracy of 95.35\%.},
  archive      = {J_COMJNL},
  author       = {Abdalla, Ghazi and Özyurt, Fatih},
  doi          = {10.1093/comjnl/bxaa131},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {383-390},
  shortjournal = {Comput. J.},
  title        = {Sentiment analysis of fast food companies with deep learning models},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MDER: Multi-dimensional event recommendation in social media
context. <em>COMJNL</em>, <em>64</em>(3), 369–382. (<a
href="https://doi.org/10.1093/comjnl/bxaa126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Events represent a tipping point that affects users’ opinions and vary depending upon their popularity from local to international. Indeed, social media offer users platforms to express their opinions and commitments to events that attract them. However, owing to the volume of data, users are encountering a difficulty to accede to the preferred events according to their features that are stored in their social network profiles. To surmount this limitation, multiple event recommendation systems appeared. Nevertheless, these systems use a limited number of event dimensions and user’s features. Besides, they consider users’ features stored in a single user’s profile and disregard the semantic concept. In this research, an approach for multi-dimensional event recommendation is set forward to recommend events to users resting on several event dimensions (engagement, location, topic, time and popularity) and some user’s features (demographic data, position and user’s/friend’s interests) stored in multi-user’s profiles by considering the semantic relationships between user’s features, specifically user’s interests. The performance of our approach was assessed using error rate measurements (mean absolute error, root mean squared error and cross-validation). Experiment that results on real-world event data sets confirmed that our approach recommends events that fit the user more than the previous approaches with the lowest error rate values.},
  archive      = {J_COMJNL},
  author       = {Troudi, Abir and Ghorbel, Leila and Amel Zayani, Corinne and Jamoussi, Salma and Amous, Ikram},
  doi          = {10.1093/comjnl/bxaa126},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {369-382},
  shortjournal = {Comput. J.},
  title        = {MDER: Multi-dimensional event recommendation in social media context},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An opinion spread prediction model with twitter emotion
analysis during algeria’s hirak. <em>COMJNL</em>, <em>64</em>(3),
358–368. (<a href="https://doi.org/10.1093/comjnl/bxaa117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media is believed to have played a central role in the mobilization of Algerian citizens to peaceful protest against their country’s corrupt regime. Since no one foresaw these protests (called ‘The Revolution of Smiles’ or ‘The Hirak Movement’), this research conducted social media analysis to elicit vital insights about both the intensity of sentiment and the influence of social media on this unexpected instigation of political protest. This work built a deep learning model and analysed the influence of content, sentiment and user features on information spread. The model used the learning capability of a long short-term memory network to predict ‘retweetability’. Experiments were conducted on two real-world datasets (Hirak and Brexit) collected from Twitter. User features were found to be a key element in the diffusion of information. The strongest feelings about event context actively influenced the spread of tweets. The Twitter emotion corpus was found to improve the predictive ability of the model developed in this study.},
  archive      = {J_COMJNL},
  author       = {Drif, Ahlem and Hadjoudj, Khalil},
  doi          = {10.1093/comjnl/bxaa117},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {358-368},
  shortjournal = {Comput. J.},
  title        = {An opinion spread prediction model with twitter emotion analysis during algeria’s hirak},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Related blogs’ summarization with natural language
processing. <em>COMJNL</em>, <em>64</em>(3), 347–357. (<a
href="https://doi.org/10.1093/comjnl/bxaa110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is plethora of information present on the web, on a given topic, in different forms i.e. blogs, articles, websites, etc. However, not all of the information is useful. Perusing and going through all of the information to get the understanding of the topic is a very tiresome and time-consuming task. Most of the time we end up investing in reading content that we later understand was not of importance to us. Due to the lack of capacity of the human to grasp vast quantities of information, relevant and crisp summaries are always desirable. Therefore, in this paper, we focus on generating a new blog entry containing the summary of multiple blogs on the same topic. Different approaches of clustering, modelling, content generation and summarization are applied to reach the intended goal. This system also eliminates the repetitive content giving savings on time and quantity, thereby making learning more comfortable and effective. Overall, a significant reduction in the number of words in the new blog generated by the system is observed by using the proposed novel methodology.},
  archive      = {J_COMJNL},
  author       = {Baliyan, Niyati and Sharma, Aarti},
  doi          = {10.1093/comjnl/bxaa110},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {347-357},
  shortjournal = {Comput. J.},
  title        = {Related blogs’ summarization with natural language processing},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transportation index computation: A development theme
mining-based approach. <em>COMJNL</em>, <em>64</em>(3), 337–346. (<a
href="https://doi.org/10.1093/comjnl/bxaa102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to comprehensively evaluate the achievements of the &#39;Belt and Road&#39; in integrated transportation, researchers need to optimize the method of generating evaluation indices and construct the framework structure of the &#39;Belt and Road&#39; transportation index system. This paper used GDELT database as data source and obtained full text data of English news in 25 countries along ‘the Belt and Road’. The paper also introduced the topic model, combined with the unsupervised method (latent Dirichlet allocation, LDA) and the supervision method (labeled LDA) to mine the topics contained in the news data. It constructed the transportation development model and analyzed the development trend of transportation in various countries. The study found that the development trend of transportation in the countries along the line is unbalanced, which can be divided into four types: rapid development type, stable development type, slow development type and lagging development type. The method of this paper can effectively extract temporal and spatial variation of news events, discover potential risks in various countries, support real-time and dynamic monitoring of the social development situation of the countries along the border and provide auxiliary decision support for implementation of the ‘the Belt and Road’ initiative, which has important application value.},
  archive      = {J_COMJNL},
  author       = {Han, Gang and Li, Menggang and Mei, Yiduo and Li, Deming},
  doi          = {10.1093/comjnl/bxaa102},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {337-346},
  shortjournal = {Comput. J.},
  title        = {Transportation index computation: A development theme mining-based approach},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective link prediction with topological and temporal
information using wavelet neural network embedding. <em>COMJNL</em>,
<em>64</em>(3), 325–336. (<a
href="https://doi.org/10.1093/comjnl/bxaa085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal networks are networks that edges evolve over time, hence link prediction in temporal networks aims at inferring new edges based on a sequence of network snapshots. In this paper, we propose a graph wavelet neural network (TT-GWNN) framework using topological and temporal features for link prediction in temporal networks. To capture topological and temporal features, we develope a second-order weighted random walk sampling algorithm. It combines network snapshots with both first-order and second-order weights into one weighted graph. Moreover, it incorporates a damping factor to assign greater weights to more recent snapshots. Next, we adopt graph wavelet neural networks to embed the vertices and use gated recurrent units for predicting new links. Extensive experiments demonstrate that TT-GWNN can effectively predict links on temporal networks.},
  archive      = {J_COMJNL},
  author       = {Mo, Xian and Pang, Jun and Liu, Zhiming},
  doi          = {10.1093/comjnl/bxaa085},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {325-336},
  shortjournal = {Comput. J.},
  title        = {Effective link prediction with topological and temporal information using wavelet neural network embedding},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent computing: Knowledge acquisition method based on
the management scale transformation. <em>COMJNL</em>, <em>64</em>(3),
314–324. (<a href="https://doi.org/10.1093/comjnl/bxaa077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread scale effect always generates significant changes in the properties or characteristics of management objects with different observation scales. Thus, this paper studies the scale transformation mechanism problem of management objects. The observation scale hierarchy (management scale) with clear management objectives could automatically be recognized through changing the observation scales, in order to improve the practical management efficiency. Firstly, an intelligent computing framework based on the scale transformation is established, which reduces the over-dependency of human involvement in traditional scale transformation methods. Then, the scale characteristic reasoning inference is put forward to improve the knowledge acquisition mechanism of scale transformation. Finally, a knowledge acquisition algorithm based on the variable-scale clustering (KAVSC) is proposed. Experiments selected the multiple products inventory data of a manufacturing enterprise from 1 January 2015 to 31 December 2017. The experiment results illustrate that the proposed algorithm KAVSC is able to accurately recognize different management scale levels and scale characteristics of each product, which could effectively support managers making differentiated inventory management plans.},
  archive      = {J_COMJNL},
  author       = {Wang, Ai and Gao, Xuedong},
  doi          = {10.1093/comjnl/bxaa077},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {314-324},
  shortjournal = {Comput. J.},
  title        = {Intelligent computing: Knowledge acquisition method based on the management scale transformation},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial bee colony–based feature selection algorithm for
cyberbullying. <em>COMJNL</em>, <em>64</em>(3), 305–313. (<a
href="https://doi.org/10.1093/comjnl/bxaa066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a nature-inspired algorithm, artificial bee colony (ABC) is an optimization algorithm that is inspired by the search behaviour of honey bees. The main aim of this study is to examine the effects of the ABC-based feature selection algorithm on classification performance for cyberbullying, which has become a significant worldwide social issue in recent years. With this purpose, the classification performance of the proposed ABC-based feature selection method is compared with three different traditional methods such as information gain, ReliefF and chi square. Experimental results present that ABC-based feature selection method outperforms than three traditional methods for the detection of cyberbullying. The Macro averaged F_measure of the data set is increased from 0.659 to 0.8 using proposed ABC-based feature selection method.},
  archive      = {J_COMJNL},
  author       = {Sarac Essiz, Esra and Oturakci, Murat},
  doi          = {10.1093/comjnl/bxaa066},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {305-313},
  shortjournal = {Comput. J.},
  title        = {Artificial bee Colony–Based feature selection algorithm for cyberbullying},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Destination image recognition and emotion analysis: Evidence
from user-generated content of online travel communities.
<em>COMJNL</em>, <em>64</em>(3), 296–304. (<a
href="https://doi.org/10.1093/comjnl/bxaa064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tourism destination image is an intangible value that enhances the internal and external spiritual value of the region. To improve tourist experiences and provide reference for relevant departments, we applied the GooSeeker web data crawler tool and Python data mining kit to crawl and analyze the representative online tourism community data. We conduct an empirical analysis through data from the online tourist community ‘mafengwo’. The result, based on the user-generated content data analysis of online travel community, shows that the tourists&#39; perception of the destination image, cognitive theme and emotional experience has different effects on the tourist experience. This research offers insights into destination image cognitive theme and traveler behavior habits, which can provide guidance for platform and destination managers.},
  archive      = {J_COMJNL},
  author       = {Huang, Weidong and Zhu, Shuting and Yao, Xinkai},
  doi          = {10.1093/comjnl/bxaa064},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {296-304},
  shortjournal = {Comput. J.},
  title        = {Destination image recognition and emotion analysis: Evidence from user-generated content of online travel communities},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Encoder–decoder couplet generation model based on
“trapezoidal context” character vector. <em>COMJNL</em>, <em>64</em>(3),
286–295. (<a href="https://doi.org/10.1093/comjnl/bxaa048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the couplet generation model which automatically generates the second line of a couplet by giving the first line. Unlike other sequence generation problems, couplet generation not only considers the sequential context within a sentence line but also emphasizes the relationships between the corresponding words of first and second lines. Therefore, a trapezoidal context character embedding the vector model has been developed firstly, which considers the ‘sequence context’ and the ‘corresponding word context’ simultaneously. Afterwards, we chose the typical encoder–decoder framework to solve the sequence–sequence problems, of which the encoder and decoder are used by bi-directional GRU and GRU, respectively. In order to further increase the semantic consistency of the first and second lines of couplets, the pre-trained sentence vector of the first line is added to the attention mechanism in the model. To verify the effectiveness of the method, it is applied to the real data set. Experimental results show that our proposed model can compete with the up-to-date methods, and both adding sentence vectors to attention and using trapezoidal context character vectors can improve the effectiveness of the algorithm.},
  archive      = {J_COMJNL},
  author       = {Gao, Rui and Zhu, Yuanyuan and Li, Mingye and Li, Shoufeng and Shi, Xiaohu},
  doi          = {10.1093/comjnl/bxaa048},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {286-295},
  shortjournal = {Comput. J.},
  title        = {Encoder–Decoder couplet generation model based on ‘Trapezoidal context’ character vector},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advanced data mining tools and methods for social computing.
<em>COMJNL</em>, <em>64</em>(3), 281–285. (<a
href="https://doi.org/10.1093/comjnl/bxab032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {.},
  archive      = {J_COMJNL},
  author       = {Mohammed, Sabah and Fang, Wai Chi and Hassanien, Aboul Ella and Kim, Tai-hoon},
  doi          = {10.1093/comjnl/bxab032},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {281-285},
  shortjournal = {Comput. J.},
  title        = {Advanced data mining tools and methods for social computing},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A profit-maximizing mechanism for query-based data trading
with personalized differential privacy. <em>COMJNL</em>, <em>64</em>(2),
264–280. (<a href="https://doi.org/10.1093/comjnl/bxaa157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data trading has attracted increasing attention over the years as a cost-effective business paradigm, probably producing a tremendous amount of economic value. However, the study of query-based trading in the user data market is still in the initial stage. To design a practical user data trading mechanism, we have to consider three major challenges: privacy concern, compensation cost minimization and revenue maximization in a Bayesian environment. By jointly considering these challenges, we propose a profit-maximizing mechanism for user data trading with personalized differential privacy, called ; , which comprised two components, ; for cost minimization and ; for revenue maximization. Especially, ; adopts personalized differential privacy to satisfy each data owner’s diverse privacy preferences. ; greedily selects the most cost-effective data owner to achieve the sub-optimal data query cost. Given this query cost, ; calculates the maximum expected revenue in a Bayesian setting. Through rigorous theoretical analysis and real-data based experiments, we demonstrate that ; achieves all desired properties and approaches the optimal profit.},
  archive      = {J_COMJNL},
  author       = {Cai, Hui and Zhu, Yanmin and Li, Jie and Yu, Jiadi},
  doi          = {10.1093/comjnl/bxaa157},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {264-280},
  shortjournal = {Comput. J.},
  title        = {A profit-maximizing mechanism for query-based data trading with personalized differential privacy},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SDVoIP—a software-defined VoIP framework for SIP and dynamic
QoS. <em>COMJNL</em>, <em>64</em>(2), 254–263. (<a
href="https://doi.org/10.1093/comjnl/bxaa152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-defined networking (SDN) allows for the decoupling of the control and data planes, enabling more programmability and a global view of the network. Previous research indicates that traditional applications recreated using SDN principles allow for more granularity and customization. In this research, we extend the insights behind SDN to develop a Voice over Internet Protocol (VoIP) framework with the objective to enhance traditional Session Initiation Protocol (SIP) operation and quality of service (QoS) approaches. The contributions of this research are 2-fold: first, an SIP control application is implemented, which communicates with an SDN controller to provide VoIP call registration and call routing capabilities, thereby eliminating the need for specialized SIP proxy hardware devices; second, a dynamic QoS application is developed that provides the ability to make network-wide QoS decisions based on real-time network measurements of latency, bandwidth and packet loss. Functional validation of the framework is performed to verify its operation. The experiment results indicate that the proposed framework allows for enhancements to traditional QoS implementations.},
  archive      = {J_COMJNL},
  author       = {Gandotra, Rahil and Perigo, Levi},
  doi          = {10.1093/comjnl/bxaa152},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {254-263},
  shortjournal = {Comput. J.},
  title        = {SDVoIP—A software-defined VoIP framework for SIP and dynamic QoS},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new algorithm for reconstruction of a computer-generated
hologram (CGH). <em>COMJNL</em>, <em>64</em>(2), 245–253. (<a
href="https://doi.org/10.1093/comjnl/bxaa151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new algorithm based on cell orientation for construction of detour Fourier Transform Computer Generated Hologram (CGH) and reconstruction of original objects from the CGH is presented here. Uniqueness of this investigation is that, besides reconstructing original objects, using this proposed algorithm, these can also be reconstructed without using monochromatic light. This investigation thus finds its importance in encryption applications. The proposed algorithms are faster as compared to Elliptic Curve Cryptography (ECC) algorithm and consumes less power.},
  archive      = {J_COMJNL},
  author       = {Tripathy, A K and Tripathy, S K and Pattanaik, S R and Das, S K},
  doi          = {10.1093/comjnl/bxaa151},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {245-253},
  shortjournal = {Comput. J.},
  title        = {A new algorithm for reconstruction of a computer-generated hologram (CGH)},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-world networks are not always fast mixing.
<em>COMJNL</em>, <em>64</em>(2), 236–244. (<a
href="https://doi.org/10.1093/comjnl/bxaa150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixing time of random walks on a graph has found broad applications across both theoretical and practical aspects of computer science, with the application effects depending on the behavior of mixing time. It is extensively believed that real-world networks, especially social networks, are fast mixing with their mixing time at most ; where ; is the number of vertices. However, the behavior of mixing time in the real-life networks has not been examined carefully, and exactly analytical research for mixing time in models mimicking real networks is still lacking. In this paper, we first experimentally evaluate the mixing time of various real-world networks with scale-free small-world properties and show that their mixing time is much higher than anticipated. To better understand the behavior of the mixing time for real-world networks, we then analytically study the mixing time of the Apollonian network, which is simultaneously scale-free and small-world. To this end, we derive the recursive relations for all eigenvalues, especially the second largest eigenvalue modulus of the transition matrix, based on which we deduce a lower bound for the mixing time of the Apollonian network, which approximately scales sublinearly with ; . Our results indicate that real-world networks are not always fast mixing, which has potential implications in the design of algorithms related to mixing time.},
  archive      = {J_COMJNL},
  author       = {Qi, Yi and Xu, Wanyue and Zhu, Liwang and Zhang, Zhongzhi},
  doi          = {10.1093/comjnl/bxaa150},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {236-244},
  shortjournal = {Comput. J.},
  title        = {Real-world networks are not always fast mixing},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hercules: Intelligent coupling of dual-mode flash memory and
hard disk drive. <em>COMJNL</em>, <em>64</em>(2), 224–235. (<a
href="https://doi.org/10.1093/comjnl/bxaa149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The write performance of multi-level cell (MLC) is several times slower than single-level cell (SLC); however, the cost per bit of MLC is much lower than SLC. Dual-mode flash (the medium can be partially switched to SLC mode by programming only 1 bit in some cells) can combine SLC and MLC to provide trading density opportunity for performance. In this paper, we present ; —a hybrid storage system that couples ; and ; (HDD)—based on the content locality principle for high storage performance. The data are divided into two types: the reference data for read operation and the delta data for write operation. The reference data are stored in SLC and the delta data in MLC or HDD in sequential orders. Hercules organizes the metadata for the mapping of the physical locations of the reference blocks and the delta data of the original blocks, intelligently identifies hot/cold data and performs the data migration between MLC and disk for performance improvements. To validate our findings, we implemented Hercules and made evaluation to show that Hercules can effectively improve the data access speed and reduce the response time, compared with the Flashcache storage structure, and in particular, with Hercules, we can achieve 10\% performance improvement over the system in absence of hot delta data caching.},
  archive      = {J_COMJNL},
  author       = {Cheng, Wen and Zou, Yuqi and Zeng, Lingfang and Wang, Yang},
  doi          = {10.1093/comjnl/bxaa149},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {224-235},
  shortjournal = {Comput. J.},
  title        = {Hercules: Intelligent coupling of dual-mode flash memory and hard disk drive},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance analysis of prioritization and contention
control algorithm in wireless body area networks. <em>COMJNL</em>,
<em>64</em>(2), 211–223. (<a
href="https://doi.org/10.1093/comjnl/bxaa147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Wireless Body Area Network (WBAN) is the composition of a group of energy-efficient, miniature, invasive/non-invasive, light-weighted sensors that monitor human body health conditions for early detection and treatment for life-threatening diseases. Due to the stringent demands of WBAN, such as energy efficiency, reliability and low delay, the development of an efficient contention control algorithm is exceptionally crucial that aims to maximize throughput by reducing collisions. In this context, this paper proposes an adaptive algorithm, namely, Prioritization and Contention Control (PCC) algorithm, to minimize collisions, latency and energy consumption. The first phase of the proposed algorithm prioritizes sensors using run-time metrics to grant channel access only for the potential nodes to send their data. It leads to a lesser number of collisions among sensors, thereby reducing retransmission attempts. In the second phase, the Contention Window (CW) size is predicted using queue length and collision rate that accurately mimic the current channel status. The dynamic estimation of CW aids in minimizing channel access delay, collisions and energy consumption, thereby enhancing overall network performance. The performance of the proposed PCC algorithm is validated with the 2D Markov model and NS2 simulation in terms of throughput, packet delivery ratio, delay and remaining energy.},
  archive      = {J_COMJNL},
  author       = {B, Nithya and Ranjan, Naveen and A, Justin Gopinath},
  doi          = {10.1093/comjnl/bxaa147},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {211-223},
  shortjournal = {Comput. J.},
  title        = {Performance analysis of prioritization and contention control algorithm in wireless body area networks},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A blockchain-based framework for IoT data monetization
services. <em>COMJNL</em>, <em>64</em>(2), 195–210. (<a
href="https://doi.org/10.1093/comjnl/bxaa119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within internet of things (IoT) research, there is a growing interest in leveraging the decentralization properties of blockchains, towards developing IoT authentication and authorization mechanisms that do not inherently require centralized third-party intermediaries. This paper presents a framework for sharing IoT data in a decentralized and private-by-design manner in exchange for monetary services. The framework is built on a tiered blockchain architecture, along with InterPlanetary File System for IoT data storage and transfer. The goal is to enable IoT data users to exercise fine-grained control on how much data they share with entities authenticated through blockchains. To highlight how the framework would be used in real-life scenarios, this paper presents two use cases, namely an IoT data marketplace and a decentralized connected vehicle insurance. These examples showcase how the proposed framework can be used for varying smart contract-based applications involving exchanges of IoT data and cryptocurrency. Following the discussion about the use cases, the paper outlines a detailed security analysis performed on the proposed framework, based on multiple attack scenarios. Finally, it presents and discusses extensive evaluations, in terms of various performance metrics obtained from a real-world implementation.},
  archive      = {J_COMJNL},
  author       = {Ali, Muhammad Salek and Vecchio, Massimo and Antonelli, Fabio},
  doi          = {10.1093/comjnl/bxaa119},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {195-210},
  shortjournal = {Comput. J.},
  title        = {A blockchain-based framework for IoT data monetization services},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multichannel ordered contention MAC protocol for underwater
wireless sensor networks. <em>COMJNL</em>, <em>64</em>(2), 185–194. (<a
href="https://doi.org/10.1093/comjnl/bxaa107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancement in hardware and the availability of bandwidth open scope for multichannel communication in underwater wireless sensor networks. Utilizing multiple channels for data and control packets in bursty traffic networks can reduce collisions due to several contending nodes. The paper presents a synchronous reservation-based multichannel ordered contention MAC protocol for deep underwater high data rate bottom monitoring applications to improve the overall network throughput. This protocol is proposed as an enhancement over ordered contention MAC (OCMAC) protocol to deal with bursty traffic by reducing transmission collisions. It does so by enabling nodes to employ multichannel technique along with the scheduling technique of OCMAC. The protocol uses separate channels for channel reservation and data transmission. The throughput performance of the network has been analyzed and validated through simulation. Simulation-based results have shown that MOC-MAC outperforms OCMAC in terms of throughput.},
  archive      = {J_COMJNL},
  author       = {Roy, Alak and Sarma, Nityananda},
  doi          = {10.1093/comjnl/bxaa107},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {185-194},
  shortjournal = {Comput. J.},
  title        = {Multichannel ordered contention MAC protocol for underwater wireless sensor networks},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wrapper-enabled feature selection and CPLM-based NARX model
for stock market prediction. <em>COMJNL</em>, <em>64</em>(2), 169–184.
(<a href="https://doi.org/10.1093/comjnl/bxaa099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prices in the stock market are dynamic in nature, thereby pretend as a hectic challenge to the sellers and buyers in predicting the trending stocks for the future. To ensure effective prediction of the stock market, the chronological penguin Levenberg–Marquardt-based nonlinear autoregressive network (CPLM-based NARX) is employed, and the prediction is devised on the basis of past and the recent rank of market. Initially, input data are subjected to the features extraction that is based on the technical indicators, such as WILLR, ROCR, MOM, RSI, CCI, ADX, TRIX, MACD, OBV, TSF, ATR and MFI. The technical indicator is adapted for predicting the stock market. The wrapper-enabled feature selection is employed for selecting the highly significant features that are generated using the technical indicators. The highly significant features of the data are fed to the prediction module, which is developed using the NARX model. The NARX model uses the CPLM algorithm that is formed using the integration of the chronological-based penguin search optimization algorithm and the Levenberg–Marquardt algorithm. The prediction using the proposed CPLM-based NARX shows the superior performance in terms of mean absolute percentage error and root mean square error with values of 0.96 and 0.805, respectively.},
  archive      = {J_COMJNL},
  author       = {Gandhmal, Dattatray P and Kumar, K},
  doi          = {10.1093/comjnl/bxaa099},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {169-184},
  shortjournal = {Comput. J.},
  title        = {Wrapper-enabled feature selection and CPLM-based NARX model for stock market prediction},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). A cotton disease diagnosis method using a combined
algorithm of case-based reasoning and fuzzy logic. <em>COMJNL</em>,
<em>64</em>(2), 155–168. (<a
href="https://doi.org/10.1093/comjnl/bxaa098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a cotton disease diagnosis method that uses a combined algorithm of case-based reasoning (CBR) and fuzzy logic was designed and implemented. It focuses on the prevention, diagnosis and control of diseases affecting cotton production in China. Conventional methods of disease diagnosis are primarily based on CBR with reference to user-provided symptoms; however, in most cases, user-provided symptoms do not fully meet the requirements of CBR. To address this problem, fuzzy logic is incorporated into CBR to allow for more flexible and accurate models. With the help of CBR and fuzzy reasoning, three diagnostic results can be obtained by the cotton disease diagnosis system (CDDS) constructed in this study: success, success but not exact and failure. To verify the reliability of the CDDS and its ability to diagnose cotton diseases, its diagnostic accuracy and stability were analyzed and compared with the results obtained by the traditional expert scoring method. The analysis results reveal that the CDDS can achieve a high diagnostic success rate (above 90\%) and better diagnostic stability than the traditional expert scoring method when at least four disease symptoms are input. The CDDS provides an independent and objective source of information to assist farmers in the diagnosis and prevention of cotton diseases.},
  archive      = {J_COMJNL},
  author       = {Dong, Yuhong and Fu, Zetian and Stankovski, Stevan and Peng, Yaoqi and Li, Xinxing},
  doi          = {10.1093/comjnl/bxaa098},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {155-168},
  shortjournal = {Comput. J.},
  title        = {A cotton disease diagnosis method using a combined algorithm of case-based reasoning and fuzzy logic},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Terminating exploration of a grid by an optimal number of
asynchronous oblivious robots. <em>COMJNL</em>, <em>64</em>(1), 132–154.
(<a href="https://doi.org/10.1093/comjnl/bxz166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider swarms of asynchronous oblivious robots evolving into an anonymous grid-shaped network. In this context, we investigate optimal (w.r.t. the number of robots) deterministic solutions for the ; problem. We first show lower bounds in the semi-synchronous model. Precisely, we show that at least three robots are required to explore any grid of at least three nodes, even in the probabilistic case. Then, we show that at least four (resp. five) robots are necessary to deterministically explore a ; -Grid (resp. a ; -Grid). We then propose deterministic algorithms in the asynchronous model. This latter being strictly weakest than the semi-synchronous model, all the aforementioned bounds still hold in that context. Our algorithms actually exhibit the optimal number of robots that is necessary to explore a given grid. Overall, our results show that except in two particular cases, three robots are necessary and sufficient to deterministically explore a grid of at least three nodes and then terminate. The optimal number of robots for the two remaining cases is four for the ; -Grid and five for the ; -Grid, respectively.},
  archive      = {J_COMJNL},
  author       = {Devismes, Stéphane and Lamani, Anissa and Petit, Franck and Raymond, Pascal and Tixeuil, Sébastien},
  doi          = {10.1093/comjnl/bxz166},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {132-154},
  shortjournal = {Comput. J.},
  title        = {Terminating exploration of a grid by an optimal number of asynchronous oblivious robots},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-swarm cuckoo search algorithm with q-learning model.
<em>COMJNL</em>, <em>64</em>(1), 108–131. (<a
href="https://doi.org/10.1093/comjnl/bxz149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a novel swarm intelligence optimization algorithm, cuckoo search (CS) has been successfully applied to solve diverse problems in the real world. Despite its efficiency and wide use, CS has some disadvantages, such as premature convergence, easy to fall into local optimum and poor balance between exploitation and exploration. In order to improve the optimization performance of the CS algorithm, a new CS extension with multi-swarms and ; -Learning namely MP-QL-CS is proposed. The step size strategy of the CS algorithm is that an individual fitness value is examined based on a one-step evolution effect of an individual instead of evaluating the step size from the multi-step evolution effect. In the MP-QL-CS algorithm, a step size control strategy is considered as action, which is used to examine the individual multi-stepping evolution effect and learn the individual optimal step size by calculating the ; function value. In this way, the MP-QL-CS algorithm can increase the adaptability of individual evolution, and a good balance between diversity and intensification can be achieved. Comparing the MP-QL-CS algorithm with various CS algorithms, variants of differential evolution (DE) and improved particle swarm optimization (PSO) algorithms, the results demonstrate that the MP-QL-CS algorithm is a competitive swarm algorithm.},
  archive      = {J_COMJNL},
  author       = {Li, Juan and Xiao, Dan-dan and Zhang, Ting and Liu, Chun and Li, Yuan-xiang and Wang, Gai-ge},
  doi          = {10.1093/comjnl/bxz149},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {108-131},
  shortjournal = {Comput. J.},
  title        = {Multi-swarm cuckoo search algorithm with Q-learning model},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exact distance query in large graphs through fast graph
simplification. <em>COMJNL</em>, <em>64</em>(1), 93–107. (<a
href="https://doi.org/10.1093/comjnl/bxz147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortest path distance query is one of the most fundamental problems in graph theory and applications. Nowadays, the scale of graphs becomes so large that traditional algorithms for shortest path are not available to answer the exact distance query quickly. Many methods based on two-hop labeling have been proposed to solve this problem. However, they cost too much either in preprocessing or query phase to handle large networks containing as many as tens of millions of vertices. In this paper, we propose a novel ; method to address this problem in large networks with less preprocessing cost while keeping the query time in the microsecond level on average. Technically, two types of labels are presented in our construction, one for distance queries when the actual distance is at most ; , which we call ; , and the other for further distance queries, which we call ; . Our approach of ; is essentially different from previous widely used two-hop labeling framework since we construct labels by using ; . We conduct extensive experiments on large real-world networks and the results demonstrate the higher efficiency of our method in preprocessing phase and the much smaller space size of constructed index compared to previous efficient two-hop labeling method, with a comparatively fast query speed.},
  archive      = {J_COMJNL},
  author       = {Liu, Jun and Pan, Yicheng and Hu, Qifu},
  doi          = {10.1093/comjnl/bxz147},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {93-107},
  shortjournal = {Comput. J.},
  title        = {Exact distance query in large graphs through fast graph simplification},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectra, hitting times and resistance distances of q-
subdivision graphs. <em>COMJNL</em>, <em>64</em>(1), 76–92. (<a
href="https://doi.org/10.1093/comjnl/bxz141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subdivision, triangulation, Kronecker product, corona product and many other graph operations or products play an important role in complex networks. In this paper, we study the properties of ; -subdivision graphs, which have been applied to model complex networks. For a simple connected graph ; , its ; -subdivision graph ; is obtained from ; through replacing every edge ; in ; by ; disjoint paths of length 2, with each path having ; and ; as its ends. We derive explicit formulas for many quantities of ; in terms of those corresponding to ; , including the eigenvalues and eigenvectors of normalized adjacency matrix, two-node hitting time, Kemeny constant, two-node resistance distance, Kirchhoff index, additive degree-Kirchhoff index and multiplicative degree-Kirchhoff index. We also study the properties of the iterated ; -subdivision graphs, based on which we obtain the closed-form expressions for a family of hierarchical lattices, which has been used to describe scale-free fractal networks.},
  archive      = {J_COMJNL},
  author       = {Zeng, Yibo and Zhang, Zhongzhi},
  doi          = {10.1093/comjnl/bxz141},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {76-92},
  shortjournal = {Comput. J.},
  title        = {Spectra, hitting times and resistance distances of q- subdivision graphs},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure fault tolerance of recursive interconnection
networks. <em>COMJNL</em>, <em>64</em>(1), 64–75. (<a
href="https://doi.org/10.1093/comjnl/bxz139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by effects caused by structure link faults in networks, we study the following graph theoretical problem. Let ; be a connected subgraph of a graph ; except for ; . The ; -structure edge-connectivity ; (resp. ; -substructure edge-connectivity ; ) of ; is the minimum cardinality of a set of edge-disjoint subgraphs ; (resp. ; ) such that ; is isomorphic to ; (resp. ; is a connected subgraph of ; ) for every ; , and ; ’s removal leaves the remaining graph disconnected. In this paper, we determine both ; and ; for ; the hypercube ; and ; the ; -ary ; -cube ;  ; and ; the balanced hypercube ; and ; . We also extend some known results.},
  archive      = {J_COMJNL},
  author       = {Sabir, Eminjan and Meng, Jixiang},
  doi          = {10.1093/comjnl/bxz139},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {64-75},
  shortjournal = {Comput. J.},
  title        = {Structure fault tolerance of recursive interconnection networks},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing grounded extensions of abstract argumentation
frameworks. <em>COMJNL</em>, <em>64</em>(1), 54–63. (<a
href="https://doi.org/10.1093/comjnl/bxz138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ; is a directed graph ; such that the vertices of ; denote ; and ; represents the ; relation between them. We present a new ; algorithm for computing the ; of an abstract argumentation framework. We show that the new algorithm runs in ; time. In contrast, the existing state-of-the-art algorithm runs in ; time where ; is the grounded extension of the input graph.},
  archive      = {J_COMJNL},
  author       = {Nofal, Samer and Atkinson, Katie and Dunne, Paul E},
  doi          = {10.1093/comjnl/bxz138},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {54-63},
  shortjournal = {Comput. J.},
  title        = {Computing grounded extensions of abstract argumentation frameworks},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relationship between extra connectivity and component
connectivity in networks. <em>COMJNL</em>, <em>64</em>(1), 38–53. (<a
href="https://doi.org/10.1093/comjnl/bxz136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connectivity is a classic measure for reliability of a multiprocessor system in the case of processor failures. Extra connectivity and component connectivity are two important indicators of the reliability of a multiprocessor system in presence of failing processors. The ; -extra connectivity ; of a graph ; is the minimum number of nodes whose removal will disconnect ; , and every remaining component has at least ; nodes. Moreover, the ; -component connectivity ; of ; is the minimum number of nodes whose deletion results in a graph with at least ; components. However, the extra connectivity and component connectivity of many well-known networks have been independently investigated. In this paper, we determine the relationship between extra connectivity and component connectivity of general networks. As applications, the extra connectivity and component connectivity are explored for some well-known networks, including complete cubic networks, hierarchical cubic networks, generalized exchanged hypercubes, dual-cube-like networks, Cayley graphs generated by transposition trees and hierarchical hypercubes as well.},
  archive      = {J_COMJNL},
  author       = {Li, Xiaoyan and Lin, Cheng-Kuan and Fan, Jianxi and Jia, Xiaohua and Cheng, Baolei and Zhou, Jingya},
  doi          = {10.1093/comjnl/bxz136},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {38-53},
  shortjournal = {Comput. J.},
  title        = {Relationship between extra connectivity and component connectivity in networks},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Three types of two-disjoint-cycle-cover pancyclicity and
their applications to cycle embedding in locally twisted cubes.
<em>COMJNL</em>, <em>64</em>(1), 27–37. (<a
href="https://doi.org/10.1093/comjnl/bxz134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph ; is two-disjoint-cycle-cover ; -pancyclic if for any integer ; satisfying ; , there exist two vertex-disjoint cycles ; and ; in ; such that the lengths of ; and ; are ; and ; , respectively, where ; denotes the total number of vertices in ; . On the basis of this definition, we further propose Ore-type conditions for graphs to be two-disjoint-cycle-cover vertex/edge ; -pancyclic. In addition, we study cycle embedding in the ; -dimensional locally twisted cube ; under the consideration of two-disjoint-cycle-cover vertex/edge pancyclicity.},
  archive      = {J_COMJNL},
  author       = {Kung, Tzu-Liang and Chen, Hon-Chan and Lin, Chia-Hui and Hsu, Lih-Hsing},
  doi          = {10.1093/comjnl/bxz134},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {27-37},
  shortjournal = {Comput. J.},
  title        = {Three types of two-disjoint-cycle-cover pancyclicity and their applications to cycle embedding in locally twisted cubes},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A heuristic approach towards drawings of graphs with high
crossing resolution. <em>COMJNL</em>, <em>64</em>(1), 7–26. (<a
href="https://doi.org/10.1093/comjnl/bxz133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ; of a non-planar drawing of a graph is the value of the minimum angle formed by any pair of crossing edges. Recent experiments suggest that the larger the crossing resolution is, the easier it is to read and interpret a drawing of a graph. However, maximizing the crossing resolution turns out to be an NP-hard problem in general, and only heuristic algorithms are known that are mainly based on appropriately adjusting force-directed algorithms. In this paper, we propose a new heuristic algorithm for the crossing resolution maximization problem and we experimentally compare it against the known approaches from the literature. Our experimental evaluation indicates that the new heuristic produces drawings with better crossing resolution, but this comes at the cost of slightly higher edge-length ratio, especially when the input graph is large.},
  archive      = {J_COMJNL},
  author       = {Bekos, Michael A and Förster, Henry and Geckeler, Christian and Holländer, Lukas and Kaufmann, Michael and Spallek, Amadäus M and Splett, Jan},
  doi          = {10.1093/comjnl/bxz133},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {7-26},
  shortjournal = {Comput. J.},
  title        = {A heuristic approach towards drawings of graphs with high crossing resolution},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliability assessment of some regular networks.
<em>COMJNL</em>, <em>64</em>(1), 1–6. (<a
href="https://doi.org/10.1093/comjnl/bxz116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized ; -connectivity of a graph ; is a parameter that can measure the reliability of a network ; to connect any ; vertices in ; , which is a generalization of traditional connectivity. Let ; and ; denote the maximum number ; of edge-disjoint trees ; in ; such that ; for any ; and ; . For an integer ; with ; , the ; of a graph ; is defined as ; and ; . In this paper, we introduce a family of regular graph ; that can be constructed recursively and each vertex with exactly one outside neighbor. The generalized ; -connectivity of the regular graph ; is studied, which attains a previously proven upper bound on ; . As applications of the main result, the generalized ; -connectivity of some important networks including some known results such as the alternating group network ; , the star graph ; and the pancake graphs ; can be obtained directly.},
  archive      = {J_COMJNL},
  author       = {Zhao, Shu-Li and Hao, Rong-Xia and Peng, Sheng-Lung},
  doi          = {10.1093/comjnl/bxz116},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {1-6},
  shortjournal = {Comput. J.},
  title        = {Reliability assessment of some regular networks},
  volume       = {64},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
