<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSB_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssb---47">JRSSSB - 47</h2>
<ul>
<li><details>
<summary>
(2021). Contents of volume 83, 2021. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(5), 1072–1073. (<a
href="https://doi.org/10.1111/rssb.12488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1111/rssb.12488},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {5},
  pages   = {1072-1073},
  title   = {Contents of volume 83, 2021},
  volume  = {83},
  year    = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Erratum: Anchor regression: Heterogeneous data meet
causality. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(5), 1071. (<a
href="https://doi.org/10.1111/rssb.12440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1111/rssb.12440},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {5},
  pages   = {1071},
  title   = {Erratum: anchor regression: heterogeneous data meet causality},
  volume  = {83},
  year    = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On identifiability and consistency of the nugget in gaussian
spatial process models. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>83</em>(5), 1044–1070. (<a
href="https://doi.org/10.1111/rssb.12472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spatial process models popular in geostatistics often represent the observed data as the sum of a smooth underlying process and white noise. The variation in the white noise is attributed to measurement error, or microscale variability, and is called the ‘nugget’. We formally establish results on the identifiability and consistency of the nugget in spatial models based upon the Gaussian process within the framework of in-fill asymptotics, that is the sample size increases within a sampling domain that is bounded. Our work extends results in fixed domain asymptotics for spatial models without the nugget. More specifically, we establish the identifiability of parameters in the Matérn covariogram and the consistency of their maximum likelihood estimators in the presence of discontinuities due to the nugget. We also present simulation studies to demonstrate the role of the identifiable quantities in spatial interpolation.},
  archive  = {J},
  author   = {Wenpin Tang and Lu Zhang and Sudipto Banerjee},
  doi      = {10.1111/rssb.12472},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1044-1070},
  title    = {On identifiability and consistency of the nugget in gaussian spatial process models},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference of heterogeneous treatment effects using
observational data with high-dimensional covariates. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(5), 1016–1043. (<a
href="https://doi.org/10.1111/rssb.12469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study proposes novel estimation and inference approaches for heterogeneous local treatment effects using high-dimensional covariates and observational data without a strong ignorability assumption. To achieve this, with a binary instrumental variable, the parameters of interest are identified on an unobservable subgroup of the population (compliers). Lasso estimation under a non-convex objective function is developed for a two-stage generalized linear model, and a debiased estimator is proposed to construct confidence intervals for treatment effects conditioned on covariates. Notably, this approach simultaneously corrects the biases due to high-dimensional estimation at both stages. The finite sample performance is evaluated via simulation studies, and real data analysis is performed on the Oregon Health Insurance Experiment to illustrate the feasibility of the proposed procedure.},
  archive  = {J},
  author   = {Yumou Qiu and Jing Tao and Xiao-Hua Zhou},
  doi      = {10.1111/rssb.12469},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1016-1043},
  title    = {Inference of heterogeneous treatment effects using observational data with high-dimensional covariates},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-assisted analyses of cluster-randomized experiments.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(5), 994–1015. (<a
href="https://doi.org/10.1111/rssb.12468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cluster-randomized experiments are widely used due to their logistical convenience and policy relevance. To analyse them properly, we must address the fact that the treatment is assigned at the cluster level instead of the individual level. Standard analytic strategies are regressions based on individual data, cluster averages and cluster totals, which differ when the cluster sizes vary. These methods are often motivated by models with strong and unverifiable assumptions, and the choice among them can be subjective. Without any outcome modelling assumption, we evaluate these regression estimators and the associated robust standard errors from the design-based perspective where only the treatment assignment itself is random and controlled by the experimenter. We demonstrate that regression based on cluster averages targets a weighted average treatment effect, regression based on individual data is suboptimal in terms of efficiency and regression based on cluster totals is consistent and more efficient with a large number of clusters. We highlight the critical role of covariates in improving estimation efficiency and illustrate the efficiency gain via both simulation studies and data analysis. The asymptotic analysis also reveals the efficiency-robustness trade-off by comparing the properties of various estimators using data at different levels with and without covariate adjustment. Moreover, we show that the robust standard errors are convenient approximations to the true asymptotic standard errors under the design-based perspective. Our theory holds even when the outcome models are misspecified, so it is model-assisted rather than model-based. We also extend the theory to a wider class of weighted average treatment effects.},
  archive  = {J},
  author   = {Fangzhou Su and Peng Ding},
  doi      = {10.1111/rssb.12468},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {994-1015},
  title    = {Model-assisted analyses of cluster-randomized experiments},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Isotonic distributional regression. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(5), 963–993. (<a
href="https://doi.org/10.1111/rssb.12450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Isotonic distributional regression (IDR) is a powerful non-parametric technique for the estimation of conditional distributions under order restrictions. In a nutshell, IDR learns conditional distributions that are calibrated, and simultaneously optimal relative to comprehensive classes of relevant loss functions, subject to isotonicity constraints in terms of a partial order on the covariate space. Non-parametric isotonic quantile regression and non-parametric isotonic binary regression emerge as special cases. For prediction, we propose an interpolation method that generalizes extant specifications under the pool adjacent violators algorithm. We recommend the use of IDR as a generic benchmark technique in probabilistic forecast problems, as it does not involve any parameter tuning nor implementation choices, except for the selection of a partial order on the covariate space. The method can be combined with subsample aggregation, with the benefits of smoother regression functions and gains in computational efficiency. In a simulation study, we compare methods for distributional regression in terms of the continuous ranked probability score (CRPS) and L 2 estimation error, which are closely linked. In a case study on raw and post-processed quantitative precipitation forecasts from a leading numerical weather prediction system, IDR is competitive with state of the art techniques.},
  archive  = {J},
  author   = {Alexander Henzi and Johanna F. Ziegel and Tilmann Gneiting},
  doi      = {10.1111/rssb.12450},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {963-993},
  title    = {Isotonic distributional regression},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-sample inference for high-dimensional markov networks.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(5), 939–962. (<a
href="https://doi.org/10.1111/rssb.12446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Markov networks are frequently used in sciences to represent conditional independence relationships underlying observed variables arising from a complex system. It is often of interest to understand how an underlying network differs between two conditions. In this paper, we develop methods for comparing a pair of high-dimensional Markov networks where we allow the number of observed variables to increase with the sample sizes. By taking the density ratio approach, we are able to learn the network difference directly and avoid estimating the individual graphs. Our methods are thus applicable even when the individual networks are dense as long as their difference is sparse. We prove finite-sample Gaussian approximation error bounds for the estimator we construct under significantly weaker assumptions than are typically required for model selection consistency. Furthermore, we propose bootstrap procedures for estimating quantiles of a max-type statistics based on our estimator, and show how they can be used to test the equality of two Markov networks or construct simultaneous confidence intervals. The performance of our methods is demonstrated through extensive simulations. The scientific usefulness is illustrated with an analysis of a new fMRI data set.},
  archive  = {J},
  author   = {Byol Kim and Song Liu and Mladen Kolar},
  doi      = {10.1111/rssb.12446},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {939-962},
  title    = {Two-sample inference for high-dimensional markov networks},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conformal inference of counterfactuals and individual
treatment effects. <em>Journal of the Royal Statistical Society: Series
B (Statistical Methodology)</em>, <em>83</em>(5), 911–938. (<a
href="https://doi.org/10.1111/rssb.12445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Evaluating treatment effect heterogeneity widely informs treatment decision making. At the moment, much emphasis is placed on the estimation of the conditional average treatment effect via flexible machine learning algorithms. While these methods enjoy some theoretical appeal in terms of consistency and convergence rates, they generally perform poorly in terms of uncertainty quantification. This is troubling since assessing risk is crucial for reliable decision-making in sensitive and uncertain environments. In this work, we propose a conformal inference-based approach that can produce reliable interval estimates for counterfactuals and individual treatment effects under the potential outcome framework. For completely randomized or stratified randomized experiments with perfect compliance, the intervals have guaranteed average coverage in finite samples regardless of the unknown data generating mechanism. For randomized experiments with ignorable compliance and general observational studies obeying the strong ignorability assumption, the intervals satisfy a doubly robust property which states the following: the average coverage is approximately controlled if either the propensity score or the conditional quantiles of potential outcomes can be estimated accurately. Numerical studies on both synthetic and real data sets empirically demonstrate that existing methods suffer from a significant coverage deficit even in simple models. In contrast, our methods achieve the desired coverage with reasonably short intervals.},
  archive  = {J},
  author   = {Lihua Lei and Emmanuel J. Candès},
  doi      = {10.1111/rssb.12445},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {911-938},
  title    = {Conformal inference of counterfactuals and individual treatment effects},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of networks via the sparse β-model. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(5), 887–910. (<a
href="https://doi.org/10.1111/rssb.12444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data in the form of networks are increasingly available in a variety of areas, yet statistical models allowing for parameter estimates with desirable statistical properties for sparse networks remain scarce. To address this, we propose the Sparse β -Model ( S β M ), a new network model that interpolates the celebrated Erdős–Rényi model and the β -model that assigns one different parameter to each node. By a novel reparameterization of the β -model to distinguish global and local parameters, our S β M can drastically reduce the dimensionality of the β -model by requiring some of the local parameters to be zero. We derive the asymptotic distribution of the maximum likelihood estimator of the S β M when the support of the parameter vector is known. When the support is unknown, we formulate a penalized likelihood approach with the ℓ 0 -penalty. Remarkably, we show via a monotonicity lemma that the seemingly combinatorial computational problem due to the ℓ 0 -penalty can be overcome by assigning non-zero parameters to those nodes with the largest degrees. We further show that a β -min condition guarantees our method to identify the true model and provide excess risk bounds for the estimated parameters. The estimation procedure enjoys good finite sample properties as shown by simulation studies. The usefulness of the S β M is further illustrated via the analysis of a microfinance take-up example.},
  archive  = {J},
  author   = {Mingli Chen and Kengo Kato and Chenlei Leng},
  doi      = {10.1111/rssb.12444},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {887-910},
  title    = {Analysis of networks via the sparse β-model},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Errata to “functional models for time-varying random
objects.” <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(4), 883. (<a
href="https://doi.org/10.1111/rssb.12438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Paromita Dubey and Hans-Georg Müller},
  doi     = {10.1111/rssb.12438},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {4},
  pages   = {883},
  title   = {Errata to “Functional models for time-varying random objects”},
  volume  = {83},
  year    = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Erratum: Optimal control of false discovery criteria in the
two-group model. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(4), 882. (<a
href="https://doi.org/10.1111/rssb.12441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1111/rssb.12441},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {4},
  pages   = {882},
  title   = {Erratum: Optimal control of false discovery criteria in the two-group model},
  volume  = {83},
  year    = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wang and leng (2016), high-dimensional ordinary
least-squares projection for screening variables, journal of the royal
statistical society series b, 78, 589–611. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(4), 880–881. (<a
href="https://doi.org/10.1111/rssb.12427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Xiangyu Wang and Chenlei Leng and Tom Boot},
  doi     = {10.1111/rssb.12427},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {4},
  pages   = {880-881},
  title   = {Wang and leng (2016), high-dimensional ordinary least-squares projection for screening variables, journal of the royal statistical society series b, 78, 589–611},
  volume  = {83},
  year    = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate laplace approximations for scalable model
selection. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(4), 853–879. (<a
href="https://doi.org/10.1111/rssb.12466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose the approximate Laplace approximation (ALA) to evaluate integrated likelihoods, a bottleneck in Bayesian model selection. The Laplace approximation (LA) is a popular tool that speeds up such computation and equips strong model selection properties. However, when the sample size is large or one considers many models the cost of the required optimizations becomes impractical. ALA reduces the cost to that of solving a least-squares problem for each model. Further, it enables efficient computation across models such as sharing pre-computed sufficient statistics and certain operations in matrix decompositions. We prove that in generalized (possibly non-linear) models ALA achieves a strong form of model selection consistency for a suitably-defined optimal model, at the same functional rates as exact computation. We consider fixed- and high-dimensional problems, group and hierarchical constraints, and the possibility that all models are misspecified. We also obtain ALA rates for Gaussian regression under non-local priors, an important example where the LA can be costly and does not consistently estimate the integrated likelihood. Our examples include non-linear regression, logistic, Poisson and survival models. We implement the methodology in the R package mombf .},
  archive  = {J},
  author   = {David Rossell and Oriol Abril and Anirban Bhattacharya},
  doi      = {10.1111/rssb.12466},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {853-879},
  title    = {Approximate laplace approximations for scalable model selection},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint quantile regression for spatial data. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(4), 826–852. (<a
href="https://doi.org/10.1111/rssb.12467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Linear quantile regression is a powerful tool to investigate how predictors may affect a response heterogeneously across different quantile levels. Unfortunately, existing approaches find it extremely difficult to adjust for any dependency between observation units, largely because such methods are not based upon a fully generative model of the data. For analysing spatially indexed data, we address this difficulty by generalizing the joint quantile regression model of Yang and Tokdar ( Journal of the American Statistical Association , 2017, 112(519), 1107–1120) and characterizing spatial dependence via a Gaussian or t -copula process on the underlying quantile levels of the observation units. A Bayesian semiparametric approach is introduced to perform inference of model parameters and carry out spatial quantile smoothing. An effective model comparison criteria is provided, particularly for selecting between different model specifications of tail heaviness and tail dependence. Extensive simulation studies and two real applications to particulate matter concentration and wildfire risk are presented to illustrate substantial gains in inference quality, prediction accuracy and uncertainty quantification over existing alternatives.},
  archive  = {J},
  author   = {Xu Chen and Surya T. Tokdar},
  doi      = {10.1111/rssb.12467},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {826-852},
  title    = {Joint quantile regression for spatial data},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial birth–death–move processes: Basic properties and
estimation of their intensity functions. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(4), 798–825. (<a
href="https://doi.org/10.1111/rssb.12452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many spatiotemporal data record the time of birth and death of individuals, along with their spatial trajectories during their lifetime, whether through continuous-time observations or discrete-time observations. Natural applications include epidemiology, individual-based modelling in ecology, spatiotemporal dynamics observed in bioimaging and computer vision. The aim of this article is to estimate in this context the birth and death intensity functions that depend in full generality on the current spatial configuration of all alive individuals. While the temporal evolution of the population size is a simple birth–death process, observing the lifetime and trajectories of all individuals calls for a new paradigm. To formalise this framework, we introduce spatial birth–death–move processes, where the birth and death dynamics depends on the current spatial configuration of the population and where individuals can move during their lifetime according to a continuous Markov process with possible interactions. We consider non-parametric kernel estimators of their birth and death intensity functions. The setting is original because each observation in time belongs to a non-vectorial, infinite dimensional space and the dependence between observations is barely tractable. We prove the consistency of the estimators in the presence of continuous-time and discrete-time observations, under fairly simple conditions. We moreover discuss how we can take advantage in practice of structural assumptions made on the intensity functions and we explain how data-driven bandwidth selection can be conducted, despite the unknown (and sometimes undefined) second order moments of the estimators. We finally apply our statistical method to the analysis of the spatiotemporal dynamics of proteins involved in exocytosis in cells, providing new insights on this complex mechanism.},
  archive  = {J},
  author   = {Frédéric Lavancier and Ronan Le Guével},
  doi      = {10.1111/rssb.12452},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {798-825},
  title    = {Spatial birth–death–move processes: Basic properties and estimation of their intensity functions},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging the fisher randomization test using confidence
distributions: Inference, combination and fusion learning. <em>Journal
of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(4), 777–797. (<a
href="https://doi.org/10.1111/rssb.12429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The flexibility and wide applicability of the Fisher randomization test (FRT) make it an attractive tool for assessment of causal effects of interventions from modern-day randomized experiments that are increasing in size and complexity. This paper provides a theoretical inferential framework for FRT by establishing its connection with confidence distributions. Such a connection leads to development’s of (i) an unambiguous procedure for inversion of FRTs to generate confidence intervals with guaranteed coverage, (ii) new insights on the effect of size of the Monte Carlo sample on the estimation of a p -value curve and (iii) generic and specific methods to combine FRTs from multiple independent experiments with theoretical guarantees. Our developments pertain to finite sample settings but have direct extensions to large samples. Simulations and a case example demonstrate the benefit of these new developments.},
  archive  = {J},
  author   = {Xiaokang Luo and Tirthankar Dasgupta and Minge Xie and Regina Y. Liu},
  doi      = {10.1111/rssb.12429},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {777-797},
  title    = {Leveraging the fisher randomization test using confidence distributions: Inference, combination and fusion learning},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The confidence interval method for selecting valid
instrumental variables. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>83</em>(4), 752–776. (<a
href="https://doi.org/10.1111/rssb.12449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a new method, the confidence interval (CI) method, to select valid instruments from a larger set of potential instruments for instrumental variable (IV) estimation of the causal effect of an exposure on an outcome. Invalid instruments are such that they fail the exclusion conditions and enter the model as explanatory variables. The CI method is based on the CIs of the per instrument causal effects estimates and selects the largest group with all CIs overlapping with each other as the set of valid instruments. Under a plurality rule, we show that the resulting standard IV, or two-stage least squares (2SLS) estimator has oracle properties. This result is the same as for the hard thresholding with voting (HT) method of Guo et al. ( Journal of the Royal Statistical Society : Series B , 2018, 80 , 793–815). Unlike the HT method, the number of instruments selected as valid by the CI method is guaranteed to be monotonically decreasing for decreasing values of the tuning parameter. For the CI method, we can therefore use a downward testing procedure based on the Sargan ( Econometrica , 1958, 26 , 393–415) test for overidentifying restrictions and a main advantage of the CI downward testing method is that it selects the model with the largest number of instruments selected as valid that passes the Sargan test.},
  archive  = {J},
  author   = {Frank Windmeijer and Xiaoran Liang and Fernando P. Hartwig and Jack Bowden},
  doi      = {10.1111/rssb.12449},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {752-776},
  title    = {The confidence interval method for selecting valid instrumental variables},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covariate powered cross-weighted multiple testing.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(4), 720–751. (<a
href="https://doi.org/10.1111/rssb.12411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A fundamental task in the analysis of data sets with many variables is screening for associations. This can be cast as a multiple testing task, where the objective is achieving high detection power while controlling type I error. We consider m hypothesis tests represented by pairs ( ( P i , X i ) ) 1 ≤ i ≤ m of p -values P i and covariates X i , such that P i ⊥ X i if H i is null. Here, we show how to use information potentially available in the covariates about heterogeneities among hypotheses to increase power compared to conventional procedures that only use the P i . To this end, we upgrade existing weighted multiple testing procedures through the independent hypothesis weighting (IHW) framework to use data-driven weights that are calculated as a function of the covariates. Finite sample guarantees, for example false discovery rate control, are derived from cross-weighting, a data-splitting approach that enables learning the weight-covariate function without overfitting as long as the hypotheses can be partitioned into independent folds, with arbitrary within-fold dependence. IHW has increased power compared to methods that do not use covariate information. A key implication of IHW is that hypothesis rejection in common multiple testing setups should not proceed according to the ranking of the p -values, but by an alternative ranking implied by the covariate-weighted p -values.},
  archive  = {J},
  author   = {Nikolaos Ignatiadis and Wolfgang Huber},
  doi      = {10.1111/rssb.12411},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {720-751},
  title    = {Covariate powered cross-weighted multiple testing},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal statistical inference for individualized treatment
effects in high-dimensional models. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>83</em>(4),
669–719. (<a href="https://doi.org/10.1111/rssb.12426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ability to predict individualized treatment effects (ITEs) based on a given patient&#39;s profile is essential for personalized medicine. We propose a hypothesis testing approach to choosing between two potential treatments for a given individual in the framework of high-dimensional linear models. The methodological novelty lies in the construction of a debiased estimator of the ITE and establishment of its asymptotic normality uniformly for an arbitrary future high-dimensional observation, while the existing methods can only handle certain specific forms of observations. We introduce a testing procedure with the type I error controlled and establish its asymptotic power. The proposed method can be extended to making inference for general linear contrasts, including both the average treatment effect and outcome prediction. We introduce the optimality framework for hypothesis testing from both the minimaxity and adaptivity perspectives and establish the optimality of the proposed procedure. An extension to high-dimensional approximate linear models is also considered. The finite sample performance of the procedure is demonstrated in simulation studies and further illustrated through an analysis of electronic health records data from patients with rheumatoid arthritis.},
  archive  = {J},
  author   = {Tianxi Cai and T. Tony Cai and Zijian Guo},
  doi      = {10.1111/rssb.12426},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {669-719},
  title    = {Optimal statistical inference for individualized treatment effects in high-dimensional models},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference on the history of a randomly growing tree.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(4), 639–668. (<a
href="https://doi.org/10.1111/rssb.12428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The spread of infectious disease in a human community or the proliferation of fake news on social media can be modelled as a randomly growing tree-shaped graph. The history of the random growth process is often unobserved but contains important information such as the source of the infection. We consider the problem of statistical inference on aspects of the latent history using only a single snapshot of the final tree. Our approach is to apply random labels to the observed unlabelled tree and analyse the resulting distribution of the growth process, conditional on the final outcome. We show that this conditional distribution is tractable under a shape exchangeability condition, which we introduce here, and that this condition is satisfied for many popular models for randomly growing trees such as uniform attachment, linear preferential attachment and uniform attachment on a D -regular tree. For inference of the root under shape exchangeability, we propose O ( n log n ) time algorithms for constructing confidence sets with valid frequentist coverage as well as bounds on the expected size of the confidence sets. We also provide efficient sampling algorithms which extend our methods to a wide class of inference problems.},
  archive  = {J},
  author   = {Harry Crane and Min Xu},
  doi      = {10.1111/rssb.12428},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {639-668},
  title    = {Inference on the history of a randomly growing tree},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instrument residual estimator for any response variable with
endogenous binary treatment. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>83</em>(3),
612–635. (<a href="https://doi.org/10.1111/rssb.12442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Given an endogenous/confounded binary treatment D , a response Y with its potential versions ( Y 0 , Y 1 ) and covariates X , finding the treatment effect is difficult if Y is not continuous, even when a binary instrumental variable (IV) Z is available. We show that, for any form of Y (continuous, binary, mixed,…), there exists a decomposition Y = μ 0 ( X ) + μ 1 ( X ) D + error with E ( error | Z , X ) = 0, where μ 1 ( X ) ≡ E ( Y 1 - Y 0 | complier , X ) and ‘compliers’ are those who get treated if and only if Z = 1. First, using the decomposition, instrumental variable estimator (IVE) is applicable with polynomial approximations for μ 0 ( X ) and μ 1 ( X ) to obtain a linear model for Y . Second, better yet, an ‘instrumental residual estimator (IRE)’ with Z − E ( Z | X ) as an IV for D can be applied, and IRE is consistent for the ‘ E ( Z | X )-overlap’ weighted average of μ 1 ( X ), which becomes E ( Y 1 - Y 0 | complier ) for randomized Z . Third, going further, a ‘weighted IRE’ can be done which is consistent for E { μ 1 ( X )}. Empirical analyses as well as a simulation study are provided to illustrate our approaches.},
  archive  = {J},
  author   = {Myoung-jae Lee},
  doi      = {10.1111/rssb.12442},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {612-635},
  title    = {Instrument residual estimator for any response variable with endogenous binary treatment},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling high-dimensional categorical data using nonconvex
fusion penalties. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(3), 579–611. (<a
href="https://doi.org/10.1111/rssb.12432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a method for estimation in high-dimensional linear models with nominal categorical data. Our estimator, called SCOPE, fuses levels together by making their corresponding coefficients exactly equal. This is achieved using the minimax concave penalty on differences between the order statistics of the coefficients for a categorical variable, thereby clustering the coefficients. We provide an algorithm for exact and efficient computation of the global minimum of the resulting nonconvex objective in the case with a single variable with potentially many levels, and use this within a block coordinate descent procedure in the multivariate case. We show that an oracle least squares solution that exploits the unknown level fusions is a limit point of the coordinate descent with high probability, provided the true levels have a certain minimum separation; these conditions are known to be minimal in the univariate case. We demonstrate the favourable performance of SCOPE across a range of real and simulated datasets. An R package CatReg implementing SCOPE for linear models and also a version for logistic regression is available on CRAN.},
  archive  = {J},
  author   = {Benjamin G. Stokell and Rajen D. Shah and Ryan J. Tibshirani},
  doi      = {10.1111/rssb.12432},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {579-611},
  title    = {Modelling high-dimensional categorical data using nonconvex fusion penalties},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of causal quantile effects with a binary
instrumental variable and censored data. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(3), 559–578. (<a
href="https://doi.org/10.1111/rssb.12431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The causal effect of a treatment is of fundamental interest in the social, biological and health sciences. Instrumental variable (IV) methods are commonly used to determine causal treatment effects in the presence of unmeasured confounding. In this work, we study a new binary IV framework with randomly censored outcomes where we propose to quantify the causal treatment effect by the concept of complier quantile causal effect (CQCE). The CQCE is identifiable under weaker conditions than the complier average causal effect when outcomes are subject to censoring, and it can provide useful insight into the dynamics of the causal treatment effect. Employing the special characteristic of the binary IV and adapting the principle of conditional score, we uncover a simple weighting scheme that can be incorporated into the standard censored quantile regression procedure to estimate CQCE. We develop robust non-parametric estimation of the derived weights in the first stage, which permits stable implementation of the second stage estimation based on existing software. We establish rigorous asymptotic properties for the proposed estimator, and confirm its validity and satisfactory finite-sample performance via extensive simulations. The proposed method is applied to a bone marrow transplant data set to evaluate the causal effect of rituximab in diffuse large B-cell lymphoma patients.},
  archive  = {J},
  author   = {Bo Wei and Limin Peng and Mei-Jie Zhang and Jason P. Fine},
  doi      = {10.1111/rssb.12431},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {559-578},
  title    = {Estimation of causal quantile effects with a binary instrumental variable and censored data},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GGM knockoff filter: False discovery rate control for
gaussian graphical models. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>83</em>(3), 534–558. (<a
href="https://doi.org/10.1111/rssb.12430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a new method to learn the structure of a Gaussian graphical model with finite sample false discovery rate control. Our method builds on the knockoff framework of Barber and Candès for linear models. We extend their approach to the graphical model setting by using a local (node-based) and a global (graph-based) step: we construct knockoffs and feature statistics for each node locally, and then solve a global optimization problem to determine a threshold for each node. We then estimate the neighbourhood of each node, by comparing its feature statistics to its threshold, resulting in our graph estimate. Our proposed method is very flexible, in the sense that there is freedom in the choice of knockoffs, feature statistics and the way in which the final graph estimate is obtained. For any given data set, it is not clear a priori what choices of these hyperparameters are optimal. We therefore use a sample-splitting-recycling procedure that first uses half of the samples to select the hyperparameters, and then learns the graph using all samples, in such a way that the finite sample FDR control still holds. We compare our method to several competitors in simulations and on a real data set.},
  archive  = {J},
  author   = {Jinzhou Li and Marloes H. Maathuis},
  doi      = {10.1111/rssb.12430},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {534-558},
  title    = {GGM knockoff filter: False discovery rate control for gaussian graphical models},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AMF: Aggregated mondrian forests for online learning.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(3), 505–533. (<a
href="https://doi.org/10.1111/rssb.12425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Random forest (RF) is one of the algorithms of choice in many supervised learning applications, be it classification or regression. The appeal of such tree-ensemble methods comes from a combination of several characteristics: a remarkable accuracy in a variety of tasks, a small number of parameters to tune, robustness with respect to features scaling, a reasonable computational cost for training and prediction, and their suitability in high-dimensional settings. The most commonly used RF variants, however, are ‘offline’ algorithms, which require the availability of the whole dataset at once. In this paper, we introduce AMF, an online RF algorithm based on Mondrian Forests. Using a variant of the context tree weighting algorithm, we show that it is possible to efficiently perform an exact aggregation over all prunings of the trees; in particular, this enables to obtain a truly online parameter-free algorithm which is competitive with the optimal pruning of the Mondrian tree, and thus adaptive to the unknown regularity of the regression function. Numerical experiments show that AMF is competitive with respect to several strong baselines on a large number of datasets for multi-class classification.},
  archive  = {J},
  author   = {Jaouad Mourtada and Stéphane Gaïffas and Erwan Scornet},
  doi      = {10.1111/rssb.12425},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {505-533},
  title    = {AMF: Aggregated mondrian forests for online learning},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Increasing power for observational studies of aberrant
response: An adaptive approach. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>83</em>(3),
482–504. (<a href="https://doi.org/10.1111/rssb.12424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In many observational studies, the interest is in the effect of treatment on bad, aberrant outcomes rather than the average outcome. For such settings, the traditional approach is to define a dichotomous outcome indicating aberration from a continuous score and use the Mantel–Haenszel test with matched data. For example, studies of determinants of poor child growth use the World Health Organization’s definition of child stunting being height-for-age z-score ≤ − 2. The traditional approach may lose power because it discards potentially useful information about the severity of aberration. We develop an adaptive approach that makes use of this information and asymptotically dominates the traditional approach. We develop our approach in two parts. First, we develop an aberrant rank approach in matched observational studies and prove a novel design sensitivity formula enabling its asymptotic comparison with the Mantel–Haenszel test under various settings. Second, we develop a new, general adaptive approach, the two-stage programming method , and use it to adaptively combine the aberrant rank test and the Mantel–Haenszel test. We apply our approach to a study of the effect of teenage pregnancy on stunting.},
  archive  = {J},
  author   = {Siyu Heng and Hyunseung Kang and Dylan S. Small and Colin B. Fogarty},
  doi      = {10.1111/rssb.12424},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {482-504},
  title    = {Increasing power for observational studies of aberrant response: An adaptive approach},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable selection with ABC bayesian forests. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(3), 453–481. (<a
href="https://doi.org/10.1111/rssb.12423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Few problems in statistics are as perplexing as variable selection in the presence of very many redundant covariates. The variable selection problem is most familiar in parametric environments such as the linear model or additive variants thereof. In this work, we abandon the linear model framework, which can be quite detrimental when the covariates impact the outcome in a non-linear way, and turn to tree-based methods for variable selection. Such variable screening is traditionally done by pruning down large trees or by ranking variables based on some importance measure. Despite heavily used in practice, these ad hoc selection rules are not yet well understood from a theoretical point of view. In this work, we devise a Bayesian tree-based probabilistic method and show that it is consistent for variable selection when the regression surface is a smooth mix of p &gt; n covariates. These results are the first model selection consistency results for Bayesian forest priors. Probabilistic assessment of variable importance is made feasible by a spike-and-slab wrapper around sum-of-trees priors. Sampling from posterior distributions over trees is inherently very difficult. As an alternative to Markov Chain Monte Carlo (MCMC), we propose approximate Bayesian computation (ABC) Bayesian forests, a new ABC sampling method based on data-splitting that achieves higher ABC acceptance rate. We show that the method is robust and successful at finding variables with high marginal inclusion probabilities. Our ABC algorithm provides a new avenue towards approximating the median probability model in non-parametric setups where the marginal likelihood is intractable.},
  archive  = {J},
  author   = {Yi Liu and Veronika Ročková and Yuexi Wang},
  doi      = {10.1111/rssb.12423},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {453-481},
  title    = {Variable selection with ABC bayesian forests},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Valid and approximately valid confidence intervals for
current status data. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>83</em>(3), 438–452. (<a
href="https://doi.org/10.1111/rssb.12422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce a new approach for creating pointwise confidence intervals for the distribution of event times for current status data. Existing methods are based on asymptotics. Our approach is based on binomial properties and motivates confidence intervals that are very simple to apply and are valid that is guarantee nominal coverage. Although these confidence intervals are necessarily conservative for small sample sizes, asymptotically their coverage rate approaches the nominal one. This binomial approach also motivates approximately valid confidence intervals, and simulations show that these approximate intervals generally have coverage rates closer to the nominal level with shorter length than existing intervals, such as the confidence interval based on the likelihood ratio test. Unlike previous asymptotic methods that require different asymptotic distributions for continuous or grid-based assessment, the binomial approach can be applied to either type of assessment distribution.},
  archive  = {J},
  author   = {Sungwook Kim and Michael P. Fay and Michael A. Proschan},
  doi      = {10.1111/rssb.12422},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {438-452},
  title    = {Valid and approximately valid confidence intervals for current status data},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prior sample size extensions for assessing prior impact and
prior-likelihood discordance. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>83</em>(3),
413–437. (<a href="https://doi.org/10.1111/rssb.12414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper outlines a framework for quantifying the prior’s contribution to posterior inference in the presence of prior-likelihood discordance, a broader concept than the usual notion of prior-likelihood conflict. We achieve this dual purpose by extending the classic notion of prior sample size , M , in three directions: (I) estimating M beyond conjugate families; (II) formulating M as a relative notion that is as a function of the likelihood sample size k , M ( k ), which also leads naturally to a graphical diagnosis; and (III) permitting negative M , as a measure of prior-likelihood conflict, that is, harmful discordance. Our asymptotic regime permits the prior sample size to grow with the likelihood data size, hence making asymptotic arguments meaningful for investigating the impact of the prior relative to that of likelihood. It leads to a simple asymptotic formula for quantifying the impact of a proper prior that only involves computing a centrality and a spread measure of the prior and the posterior. We use simulated and real data to illustrate the potential of the proposed framework, including quantifying how weak is a ‘weakly informative’ prior adopted in a study of lupus nephritis. Whereas we take a pragmatic perspective in assessing the impact of a prior on a given inference problem under a specific evaluative metric, we also touch upon conceptual and theoretical issues such as using improper priors and permitting priors with asymptotically non-vanishing influence.},
  archive  = {J},
  author   = {Matthew Reimherr and Xiao-Li Meng and Dan L. Nicolae},
  doi      = {10.1111/rssb.12414},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {413-437},
  title    = {Prior sample size extensions for assessing prior impact and prior-likelihood discordance},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the optimality of randomization in experimental design:
How to randomize for minimax variance and design-based inference.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(2), 404–409. (<a
href="https://doi.org/10.1111/rssb.12412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {I study the minimax-optimal design for a two-arm controlled experiment where conditional mean outcomes vary in a given set and the objective is effect-estimation precision. When this set is permutation symmetric, the optimal design is shown to be complete randomization. Notably, even when the set has structure (i.e., is not permutation symmetric), being minimax-optimal for precision still requires randomization beyond a single partition of units, that is, beyond randomizing the identity of treatment. A single partition is not optimal even when conditional means are linear. Since this only targets precision, it may nonetheless not ensure sufficient uniformity for design-based (i.e., randomization) inference. I therefore propose the inference-constrained mixed-strategy optimal design as the minimax-optimal for precision among designs subject to sufficient-uniformity constraints.},
  archive  = {J},
  author   = {Nathan Kallus},
  doi      = {10.1111/rssb.12412},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {404-409},
  title    = {On the optimality of randomization in experimental design: How to randomize for minimax variance and design-based inference},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On optimal rerandomization designs. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(2), 395–403. (<a
href="https://doi.org/10.1111/rssb.12417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Blocking is commonly used in randomized experiments to increase efficiency of estimation. A generalization of blocking removes allocations with imbalance in covariate distributions between treated and control units, and then randomizes within the remaining set of allocations with balance. This idea of rerandomization was formalized by Morgan and Rubin ( Annals of Statistics , 2012, 40 , 1263–1282), who suggested using Mahalanobis distance between treated and control covariate means as the criterion for removing unbalanced allocations. Kallus ( Journal of the Royal Statistical Society, Series B: Statistical Methodology , 2018, 80 , 85–112) proposed reducing the set of balanced allocations to the minimum. Here we discuss the implication of such an ‘optimal’ rerandomization design for inferences to the units in the sample and to the population from which the units in the sample were randomly drawn. We argue that, in general, it is a bad idea to seek the optimal design for an inference because that inference typically only reflects uncertainty from the random sampling of units, which is usually hypothetical, and not the randomization of units to treatment versus control.},
  archive  = {J},
  author   = {Per Johansson and Donald B. Rubin and Mårten Schultzberg},
  doi      = {10.1111/rssb.12417},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {395-403},
  title    = {On optimal rerandomization designs},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principal manifold estimation via model complexity
selection. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(2), 369–394. (<a
href="https://doi.org/10.1111/rssb.12416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a framework of principal manifolds to model high-dimensional data. This framework is based on Sobolev spaces and designed to model data of any intrinsic dimension. It includes principal component analysis and principal curve algorithm as special cases. We propose a novel method for model complexity selection to avoid overfitting, eliminate the effects of outliers and improve the computation speed. Additionally, we propose a method for identifying the interiors of circle-like curves and cylinder/ball-like surfaces. The proposed approach is compared to existing methods by simulations and applied to estimate tumour surfaces and interiors in a lung cancer study.},
  archive  = {J},
  author   = {Kun Meng and Ani Eloyan},
  doi      = {10.1111/rssb.12416},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {369-394},
  title    = {Principal manifold estimation via model complexity selection},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric density estimation over complicated domains.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(2), 346–368. (<a
href="https://doi.org/10.1111/rssb.12415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a nonparametric method for density estimation over (possibly complicated) spatial domains. The method combines a likelihood approach with a regularization based on a differential operator. We demonstrate the good inferential properties of the method. Moreover, we develop an estimation procedure based on advanced numerical techniques, and in particular making use of finite elements. This ensures high computational efficiency and enables great flexibility. The proposed method efficiently deals with data scattered over regions having complicated shapes, featuring complex boundaries, sharp concavities or holes. Moreover, it captures very well complicated signals having multiple modes with different directions and intensities of anisotropy. We show the comparative advantages of the proposed approach over state of the art methods, in simulation studies and in an application to the study of criminality in the city of Portland, Oregon.},
  archive  = {J},
  author   = {Federico Ferraccioli and Eleonora Arnone and Livio Finos and James O. Ramsay and Laura M. Sangalli},
  doi      = {10.1111/rssb.12415},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {346-368},
  title    = {Nonparametric density estimation over complicated domains},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating optimal treatment rules with an instrumental
variable: A partial identification learning approach. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(2), 318–345. (<a
href="https://doi.org/10.1111/rssb.12413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Individualized treatment rules (ITRs) are considered a promising recipe to deliver better policy interventions. One key ingredient in optimal ITR estimation problems is to estimate the average treatment effect conditional on a subject’s covariate information, which is often challenging in observational studies due to the universal concern of unmeasured confounding. Instrumental variables (IVs) are widely used tools to infer the treatment effect when there is unmeasured confounding between the treatment and outcome. In this work, we propose a general framework of approaching the optimal ITR estimation problem when a valid IV is allowed to only partially identify the treatment effect. We introduce a novel notion of optimality called ‘IV-optimality’. A treatment rule is said to be IV-optimal if it minimizes the maximum risk with respect to the putative IV and the set of IV identification assumptions. We derive a bound on the risk of an IV-optimal rule that illuminates when an IV-optimal rule has favourable generalization performance. We propose a classification-based statistical learning method that estimates such an IV-optimal rule, design computationally efficient algorithms, and prove theoretical guarantees. We contrast our proposed method to the popular outcome weighted learning (OWL) approach via extensive simulations, and apply our method to study which mothers would benefit from travelling to deliver their premature babies at hospitals with high-level neonatal intensive care units. R package ivitr implements the proposed method.},
  archive  = {J},
  author   = {Hongming Pu and Bo Zhang},
  doi      = {10.1111/rssb.12413},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {318-345},
  title    = {Estimating optimal treatment rules with an instrumental variable: A partial identification learning approach},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and clustering in popularity adjusted block
model. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(2), 293–317. (<a
href="https://doi.org/10.1111/rssb.12410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper considers the Popularity Adjusted Block model (PABM) introduced by Sengupta and Chen (Journal of the Royal Statistical Society Series B, 2018, 80 , 365–386). We argue that the main appeal of the PABM is the flexibility of the spectral properties of the graph which makes the PABM an attractive choice for modelling networks that appear in biological sciences. We expand the theory of PABM to the case of an arbitrary number of communities which possibly grows with a number of nodes in the network and is not assumed to be known. We produce estimators of the probability matrix and of the community structure and, in addition, provide non-asymptotic upper bounds for the estimation and the clustering errors. We use the Sparse Subspace Clustering (SSC) approach for partitioning the network into communities, the approach that, to the best of our knowledge, has not been used for the clustering network data. The theory is supplemented by a simulation study. In addition, we show advantages of the PABM for modelling a butterfly similarity network and a human brain functional network.},
  archive  = {J},
  author   = {Majid Noroozi and Ramchandra Rimal and Marianna Pensky},
  doi      = {10.1111/rssb.12410},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {293-317},
  title    = {Estimation and clustering in popularity adjusted block model},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative alpha expansion for estimating gradient-sparse
signals from linear measurements. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>83</em>(2),
271–292. (<a href="https://doi.org/10.1111/rssb.12407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider estimating a piecewise-constant image, or a gradient-sparse signal on a general graph, from noisy linear measurements. We propose and study an iterative algorithm to minimize a penalized least-squares objective, with a penalty given by the “ ℓ 0 -norm” of the signal’s discrete graph gradient. The method uses a non-convex variant of proximal gradient descent, applying the alpha-expansion procedure to approximate the proximal mapping in each iteration, and using a geometric decay of the penalty parameter across iterations to ensure convergence. Under a cut-restricted isometry property for the measurement design, we prove global recovery guarantees for the estimated signal. For standard Gaussian designs, the required number of measurements is independent of the graph structure, and improves upon worst-case guarantees for total-variation (TV) compressed sensing on the 1-D line and 2-D lattice graphs by polynomial and logarithmic factors respectively. The method empirically yields lower mean-squared recovery error compared with TV regularization in regimes of moderate undersampling and moderate to high signal-to-noise, for several examples of changepoint signals and gradient-sparse phantom images.},
  archive  = {J},
  author   = {Sheng Xu and Zhou Fan},
  doi      = {10.1111/rssb.12407},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {271-292},
  title    = {Iterative alpha expansion for estimating gradient-sparse signals from linear measurements},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite sample change point inference and identification for
high-dimensional mean vectors. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>83</em>(2),
247–270. (<a href="https://doi.org/10.1111/rssb.12406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cumulative sum (CUSUM) statistics are widely used in the change point inference and identification. For the problem of testing for existence of a change point in an independent sample generated from the mean-shift model, we introduce a Gaussian multiplier bootstrap to calibrate critical values of the CUSUM test statistics in high dimensions. The proposed bootstrap CUSUM test is fully data dependent and it has strong theoretical guarantees under arbitrary dependence structures and mild moment conditions. Specifically, we show that with a boundary removal parameter the bootstrap CUSUM test enjoys the uniform validity in size under the null and it achieves the minimax separation rate under the sparse alternatives when the dimension p can be larger than the sample size n . Once a change point is detected, we estimate the change point location by maximising the ℓ ∞ -norm of the generalised CUSUM statistics at two different weighting scales corresponding to covariance stationary and non-stationary CUSUM statistics. For both estimators, we derive their rates of convergence and show that dimension impacts the rates only through logarithmic factors, which implies that consistency of the CUSUM estimators is possible when p is much larger than n . In the presence of multiple change points, we propose a principled bootstrap-assisted binary segmentation (BABS) algorithm to dynamically adjust the change point detection rule and recursively estimate their locations. We derive its rate of convergence under suitable signal separation and strength conditions. The results derived in this paper are non-asymptotic and we provide extensive simulation studies to assess the finite sample performance. The empirical evidence shows an encouraging agreement with our theoretical results.},
  archive  = {J},
  author   = {Mengjia Yu and Xiaohui Chen},
  doi      = {10.1111/rssb.12406},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {247-270},
  title    = {Finite sample change point inference and identification for high-dimensional mean vectors},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anchor regression: Heterogeneous data meet causality.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>83</em>(2), 215–246. (<a
href="https://doi.org/10.1111/rssb.12398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the problem of predicting a response variable from a set of covariates on a data set that differs in distribution from the training data. Causal parameters are optimal in terms of predictive accuracy if in the new distribution either many variables are affected by interventions or only some variables are affected, but the perturbations are strong. If the training and test distributions differ by a shift, causal parameters might be too conservative to perform well on the above task. This motivates anchor regression, a method that makes use of exogenous variables to solve a relaxation of the ‘causal’ minimax problem by considering a modification of the least-squares loss. The procedure naturally provides an interpolation between the solutions of ordinary least squares (OLS) and two-stage least squares. We prove that the estimator satisfies predictive guarantees in terms of distributional robustness against shifts in a linear class; these guarantees are valid even if the instrumental variable assumptions are violated. If anchor regression and least squares provide the same answer (‘anchor stability’), we establish that OLS parameters are invariant under certain distributional changes. Anchor regression is shown empirically to improve replicability and protect against distributional shifts.},
  archive  = {J},
  author   = {Dominik Rothenhäusler and Nicolai Meinshausen and Peter Bühlmann and Jonas Peters},
  doi      = {10.1111/rssb.12398},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {215-246},
  title    = {Anchor regression: Heterogeneous data meet causality},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The proximal robbins–monro method. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(1), 188–212. (<a
href="https://doi.org/10.1111/rssb.12405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The need for statistical estimation with large data sets has reinvigorated interest in iterative procedures and stochastic optimization. Stochastic approximations are at the forefront of this recent development as they yield procedures that are simple, general and fast. However, standard stochastic approximations are often numerically unstable. Deterministic optimization, in contrast, increasingly uses proximal updates to achieve numerical stability in a principled manner. A theoretical gap has thus emerged. While standard stochastic approximations are subsumed by the framework Robbins and Monro ( The annals of mathematical statistics , 1951, pp. 400–407), there is no such framework for stochastic approximations with proximal updates. In this paper, we conceptualize a proximal version of the classical Robbins–Monro procedure. Our theoretical analysis demonstrates that the proposed procedure has important stability benefits over the classical Robbins–Monro procedure, while it retains the best known convergence rates. Exact implementations of the proximal Robbins–Monro procedure are challenging, but we show that approximate implementations lead to procedures that are easy to implement, and still dominate standard procedures by achieving numerical stability, practically without trade-offs. Moreover, approximate proximal Robbins–Monro procedures can be applied even when the objective cannot be calculated analytically, and so they generalize stochastic proximal procedures currently in use.},
  archive  = {J},
  author   = {Panos Toulis and Thibaut Horel and Edoardo M. Airoldi},
  doi      = {10.1111/rssb.12405},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {188-212},
  title    = {The proximal Robbins–Monro method},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gibbs flow for approximate transport with applications to
bayesian computation. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>83</em>(1), 156–187. (<a
href="https://doi.org/10.1111/rssb.12404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Let π 0 and π 1 be two distributions on the Borel space ( R d , B ( R d ) ) . Any measurable function T : R d → R d such that Y = T ( X ) ∼ π 1 if X ∼ π 0 is called a transport map from π 0 to π 1 . For any π 0 and π 1 , if one could obtain an analytical expression for a transport map from π 0 to π 1 , then this could be straightforwardly applied to sample from any distribution. One would map draws from an easy-to-sample distribution π 0 to the target distribution π 1 using this transport map. Although it is usually impossible to obtain an explicit transport map for complex target distributions, we show here how to build a tractable approximation of a novel transport map. This is achieved by moving samples from π 0 using an ordinary differential equation with a velocity field that depends on the full conditional distributions of the target. Even when this ordinary differential equation is time-discretised and the full conditional distributions are numerically approximated, the resulting distribution of mapped samples can be efficiently evaluated and used as a proposal within sequential Monte Carlo samplers. We demonstrate significant gains over state-of-the-art sequential Monte Carlo samplers at a fixed computational complexity on a variety of applications.},
  archive  = {J},
  author   = {Jeremy Heng and Arnaud Doucet and Yvo Pokern},
  doi      = {10.1111/rssb.12404},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {156-187},
  title    = {Gibbs flow for approximate transport with applications to bayesian computation},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal control of false discovery criteria in the two-group
model. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(1), 133–155. (<a
href="https://doi.org/10.1111/rssb.12403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The highly influential two-group model in testing a large number of statistical hypotheses assumes that the test statistics are drawn independently from a mixture of a high probability null distribution and a low probability alternative. Optimal control of the marginal false discovery rate (mFDR), in the sense that it provides maximal power (expected true discoveries) subject to mFDR control, is known to be achieved by thresholding the local false discovery rate (locFDR), the probability of the hypothesis being null given the set of test statistics, with a fixed threshold. We address the challenge of controlling optimally the popular false discovery rate (FDR) or positive FDR (pFDR) in the general two-group model, which also allows for dependence between the test statistics. These criteria are less conservative than the mFDR criterion, so they make more rejections in expectation. We derive their optimal multiple testing (OMT) policies, which turn out to be thresholding the locFDR with a threshold that is a function of the entire set of statistics. We develop an efficient algorithm for finding these policies, and use it for problems with thousands of hypotheses. We illustrate these procedures on gene expression studies.},
  archive  = {J},
  author   = {Ruth Heller and Saharon Rosset},
  doi      = {10.1111/rssb.12403},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {133-155},
  title    = {Optimal control of false discovery criteria in the two-group model},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smoothing splines on riemannian manifolds, with applications
to 3D shape space. <em>Journal of the Royal Statistical Society: Series
B (Statistical Methodology)</em>, <em>83</em>(1), 108–132. (<a
href="https://doi.org/10.1111/rssb.12402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There has been increasing interest in statistical analysis of data lying in manifolds. This paper generalizes a smoothing spline fitting method to Riemannian manifold data based on the technique of unrolling, unwrapping and wrapping originally proposed by Jupp and Kent for spherical data. In particular, we develop such a fitting procedure for shapes of configurations in general m -dimensional Euclidean space, extending our previous work for two-dimensional shapes. We show that parallel transport along a geodesic on Kendall shape space is linked to the solution of a homogeneous first-order differential equation, some of whose coefficients are implicitly defined functions. This finding enables us to approximate the procedure of unrolling and unwrapping by simultaneously solving such equations numerically, and so to find numerical solutions for smoothing splines fitted to higher dimensional shape data. This fitting method is applied to the analysis of some dynamic 3D peptide data.},
  archive  = {J},
  author   = {Kwang-Rae Kim and Ian L. Dryden and Huiling Le and Katie E. Severn},
  doi      = {10.1111/rssb.12402},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {108-132},
  title    = {Smoothing splines on riemannian manifolds, with applications to 3D shape space},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Small area estimation with linked data. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(1), 78–107. (<a
href="https://doi.org/10.1111/rssb.12401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data linkage can be used to combine values of the variable of interest from a national survey with values of auxiliary variables obtained from another source, such as a population register, for use in small area estimation. However, linkage errors can induce bias when fitting regression models; moreover, they can create non-representative outliers in the linked data in addition to the presence of potential representative outliers. In this paper, we adopt a secondary analyst’s point of view, assuming that limited information is available on the linkage process, and develop small area estimators based on linear mixed models and M-quantile models to accommodate linked data containing a mix of both types of outliers. We illustrate the properties of these small area estimators, as well as estimators of their mean squared error, by means of model-based and design-based simulation experiments. We further illustrate the proposed methodology by applying it to linked data from the European Survey on Income and Living Conditions and the Italian integrated archive of economic and demographic micro data in order to obtain estimates of the average equivalised income for labour market areas in central Italy.},
  archive  = {J},
  author   = {N. Salvati and E. Fabrizi and M. G. Ranalli and R. L. Chambers},
  doi      = {10.1111/rssb.12401},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {78-107},
  title    = {Small area estimation with linked data},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inferences of linear forms for noisy matrix
completion. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(1), 58–77. (<a
href="https://doi.org/10.1111/rssb.12400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce a flexible framework for making inferences about general linear forms of a large matrix based on noisy observations of a subset of its entries. In particular, under mild regularity conditions, we develop a universal procedure to construct asymptotically normal estimators of its linear forms through double-sample debiasing and low-rank projection whenever an entry-wise consistent estimator of the matrix is available. These estimators allow us to subsequently construct confidence intervals for and test hypotheses about the linear forms. Our proposal was motivated by a careful perturbation analysis of the empirical singular spaces under the noisy matrix completion model which might be of independent interest. The practical merits of our proposed inference procedure are demonstrated on both simulated and real-world data examples.},
  archive  = {J},
  author   = {Dong Xia and Ming Yuan},
  doi      = {10.1111/rssb.12400},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {58-77},
  title    = {Statistical inferences of linear forms for noisy matrix completion},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Use of model reparametrization to improve variational
bayes†. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>83</em>(1), 30–57. (<a
href="https://doi.org/10.1111/rssb.12399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose using model reparametrization to improve variational Bayes inference for hierarchical models whose variables can be classified as global (shared across observations) or local (observation-specific). Posterior dependence between local and global variables is minimized by applying an invertible affine transformation on the local variables. The functional form of this transformation is deduced by approximating the posterior distribution of each local variable conditional on the global variables by a Gaussian density via a second order Taylor expansion. Variational Bayes inference for the reparametrized model is then obtained using stochastic approximation. Our approach can be readily extended to large datasets via a divide and recombine strategy. Using generalized linear mixed models, we demonstrate that reparametrized variational Bayes (RVB) provides improvements in both accuracy and convergence rate compared to state of the art Gaussian variational approximation methods.},
  archive  = {J},
  author   = {Linda S. L. Tan},
  doi      = {10.1111/rssb.12399},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {30-57},
  title    = {Use of model reparametrization to improve variational bayes†},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Construction of blocked factorial designs to estimate main
effects and selected two-factor interactions. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(1), 5–29. (<a
href="https://doi.org/10.1111/rssb.12397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Two-level factorial designs are widely used in industry. For experiments involving n factors, the construction of designs comprising 2 n and 2 n - p factorials, arranged in blocks of size 2 q is investigated. The aim is to estimate all main effects and a selected subset of two-factor interactions. Designs constructed according to minimum aberration criteria are shown to not necessarily be the most appropriate designs in this situation. A design construction approach is proposed which exploits known results on proper vertex colourings in graph theory. Examples are provided to illustrate the results and construction strategies. Particular consideration is given to the special case of designs with blocks of size four and tables of designs are given for this block size.},
  archive  = {J},
  author   = {J. D. Godolphin},
  doi      = {10.1111/rssb.12397},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {5-29},
  title    = {Construction of blocked factorial designs to estimate main effects and selected two-factor interactions},
  volume   = {83},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Report of the editors—2020. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>83</em>(1), 3–4. (<a
href="https://doi.org/10.1111/rssb.12408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Aurore Delaigle and Simon Wood},
  doi     = {10.1111/rssb.12408},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {3-4},
  title   = {Report of the editors—2020},
  volume  = {83},
  year    = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
