<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMTC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomtc---141">BIOMTC - 141</h2>
<ul>
<li><details>
<summary>
(2021). Acknowledgments referees 2021. <em>BIOMTC</em>,
<em>77</em>(4), 1505–1508. (<a
href="https://doi.org/10.1111/biom.13613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13613},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1505-1508},
  shortjournal = {Biometrics},
  title        = {Acknowledgments referees 2021},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data science and machine learning: Mathematical and
statistical methods by dirk
p.kroese,ZdravkoBotev,ThomasTaimre,RadislavVaisman boca raton, FL:
Chapman and hall/CRC, 2019. Pp. 523. <em>BIOMTC</em>, <em>77</em>(4),
1503–1504. (<a href="https://doi.org/10.1111/biom.13598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yin-Ju Lai and Chuhsing Kate Hsiao},
  doi          = {10.1111/biom.13598},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1503-1504},
  shortjournal = {Biometrics},
  title        = {Data science and machine learning: mathematical and statistical methods by dirk P.Kroese,ZdravkoBotev,ThomasTaimre,RadislavVaisman boca raton, FL: chapman and Hall/CRC, 2019. pp. 523.},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data analysis with RStudio: An easygoing introduction by
franz kronthaler and silke zöllner berlin, germany: Springer-verlag
GmbH, DE, 2021. Pp. 131. <em>BIOMTC</em>, <em>77</em>(4), 1502–1503. (<a
href="https://doi.org/10.1111/biom.13567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Mei-Hsien Lee},
  doi          = {10.1111/biom.13567},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1502-1503},
  shortjournal = {Biometrics},
  title        = {Data analysis with RStudio: an easygoing introduction by franz kronthaler and silke zöllner berlin, germany: springer-verlag GmbH, DE, 2021. pp. 131.},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biometry for forestry and environmental data with examples
in r. Lauri mehtätalo and juha lappi. Boca raton, FL: Chapman and
hall/CRC, 2020. Pp. 426. <em>BIOMTC</em>, <em>77</em>(4), 1500–1502. (<a
href="https://doi.org/10.1111/biom.13597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Bianca N.I. Eskelson},
  doi          = {10.1111/biom.13597},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1500-1502},
  shortjournal = {Biometrics},
  title        = {Biometry for forestry and environmental data with examples in r. lauri mehtätalo and juha lappi. boca raton, FL: Chapman and Hall/CRC, 2020. pp. 426},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matrix-based introduction to multivariate data analysis, by
KoheiAdachi 2nd edition. Singapore: Springer nature, 2020. Pp. 457.
<em>BIOMTC</em>, <em>77</em>(4), 1498–1500. (<a
href="https://doi.org/10.1111/biom.13566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Lin Liu},
  doi          = {10.1111/biom.13566},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1498-1500},
  shortjournal = {Biometrics},
  title        = {Matrix-based introduction to multivariate data analysis, by KoheiAdachi 2nd edition. singapore: Springer nature, 2020. pp. 457.},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Replication and evidence factors in observational studies by
paul r.rosenbaum boca raton, FL: Chapman and hall/CRC, 2021. Pp. 276.
<em>BIOMTC</em>, <em>77</em>(4), 1495–1498. (<a
href="https://doi.org/10.1111/biom.13535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Rajarshi Mukherjee},
  doi          = {10.1111/biom.13535},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1495-1498},
  shortjournal = {Biometrics},
  title        = {Replication and evidence factors in observational studies by paul R.Rosenbaum boca raton, FL: Chapman and Hall/CRC, 2021. pp. 276.},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder: Improving precision and power in randomized
trials for COVID-19 treatments using covariate adjustment, for binary,
ordinal, and time-to-event outcomes. <em>BIOMTC</em>, <em>77</em>(4),
1492–1494. (<a href="https://doi.org/10.1111/biom.13495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {David Benkeser and Iván Díaz and Alex Luedtke and Jodi Segal and Daniel Scharfstein and Michael Rosenblum},
  doi          = {10.1111/biom.13495},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1492-1494},
  shortjournal = {Biometrics},
  title        = {Rejoinder: Improving precision and power in randomized trials for COVID-19 treatments using covariate adjustment, for binary, ordinal, and time-to-event outcomes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “improving precision and power in randomized
trials for COVID-19 treatments using covariate adjustment, for binary,
ordinal, and time-to-event outcomes” by david benkeser, ivan diaz, alex
luedtke, jodi segal, daniel scharfstein, and michael rosenblum.
<em>BIOMTC</em>, <em>77</em>(4), 1489–1491. (<a
href="https://doi.org/10.1111/biom.13494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Lisa M. LaVange},
  doi          = {10.1111/biom.13494},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1489-1491},
  shortjournal = {Biometrics},
  title        = {Discussion on “Improving precision and power in randomized trials for COVID-19 treatments using covariate adjustment, for binary, ordinal, and time-to-event outcomes” by david benkeser, ivan diaz, alex luedtke, jodi segal, daniel scharfstein, and michael rosenblum},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of “improving precision and power in randomized
trials for COVID-19 treatments using covariate adjustment, for binary,
ordinal, and time-to-event outcomes.” <em>BIOMTC</em>, <em>77</em>(4),
1485–1488. (<a href="https://doi.org/10.1111/biom.13492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Min Zhang and Baqun Zhang},
  doi          = {10.1111/biom.13492},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1485-1488},
  shortjournal = {Biometrics},
  title        = {Discussion of “Improving precision and power in randomized trials for COVID-19 treatments using covariate adjustment, for binary, ordinal, and time-to-event outcomes”},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “improving precision and power in randomized
trials for COVID-19 treatments using covariate adjustment for binary,
ordinal, and time-to-event outcomes.” <em>BIOMTC</em>, <em>77</em>(4),
1482–1484. (<a href="https://doi.org/10.1111/biom.13493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benkeser et al . present a very informative paper evaluating the efficiency gains of covariate adjustment in settings with binary, ordinal, and time-to-event outcomes. The adjustment method focuses on estimating the marginal treatment effect averaged over the covariate distribution in both arms combined. The authors show that covariate adjustment can achieve power gains that could find answers more quickly. The suggested approach is an important weapon in the armamentarium against epidemics like COVID-19. I recommend evaluating the procedure against more traditional approaches for conditional analyses (e.g., logistic regression) and against blinded methods of building prediction models followed by randomization-based inference.},
  archive      = {J_BIOMTC},
  author       = {Michael A. Proschan},
  doi          = {10.1111/biom.13493},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1482-1484},
  shortjournal = {Biometrics},
  title        = {Discussion on “Improving precision and power in randomized trials for COVID-19 treatments using covariate adjustment for binary, ordinal, and time-to-event outcomes”},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Improving precision and power in randomized trials for
COVID-19 treatments using covariate adjustment, for binary, ordinal, and
time-to-event outcomes. <em>BIOMTC</em>, <em>77</em>(4), 1467–1481. (<a
href="https://doi.org/10.1111/biom.13377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time is of the essence in evaluating potential drugs and biologics for the treatment and prevention of COVID-19. There are currently 876 randomized clinical trials (phase 2 and 3) of treatments for COVID-19 registered on clinicaltrials.gov. Covariate adjustment is a statistical analysis method with potential to improve precision and reduce the required sample size for a substantial number of these trials. Though covariate adjustment is recommended by the U.S. Food and Drug Administration and the European Medicines Agency, it is underutilized, especially for the types of outcomes (binary, ordinal, and time-to-event) that are common in COVID-19 trials. To demonstrate the potential value added by covariate adjustment in this context, we simulated two-arm, randomized trials comparing a hypothetical COVID-19 treatment versus standard of care, where the primary outcome is binary, ordinal, or time-to-event. Our simulated distributions are derived from two sources: longitudinal data on over 500 patients hospitalized at Weill Cornell Medicine New York Presbyterian Hospital and a Centers for Disease Control and Prevention preliminary description of 2449 cases. In simulated trials with sample sizes ranging from 100 to 1000 participants, we found substantial precision gains from using covariate adjustment–equivalent to 4–18\% reductions in the required sample size to achieve a desired power. This was the case for a variety of estimands (targets of inference). From these simulations, we conclude that covariate adjustment is a low-risk, high-reward approach to streamlining COVID-19 treatment trials. We provide an R package and practical recommendations for implementation.},
  archive      = {J_BIOMTC},
  author       = {David Benkeser and Iván Díaz and Alex Luedtke and Jodi Segal and Daniel Scharfstein and Michael Rosenblum},
  doi          = {10.1111/biom.13377},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1467-1481},
  shortjournal = {Biometrics},
  title        = {Improving precision and power in randomized trials for COVID-19 treatments using covariate adjustment, for binary, ordinal, and time-to-event outcomes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Child mortality estimation incorporating summary birth
history data. <em>BIOMTC</em>, <em>77</em>(4), 1456–1466. (<a
href="https://doi.org/10.1111/biom.13383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The United Nations&#39; Sustainable Development Goal 3.2 aims to reduce under-five child mortality to 25 deaths per 1000 live births by 2030. Child mortality tends to be concentrated in developing regions where information needed to assess achievement of this goal often comes from surveys and censuses. In both, women are asked about their birth histories, but with varying degrees of detail. Full birth history (FBH) data contain the reported dates of births and deaths of every surveyed mother&#39;s children. In contrast, summary birth history (SBH) data contain only the total number of children born and total number of children who died for each mother. Specialized methods are needed to accommodate this type of data into analyses of child mortality trends. We develop a data augmentation scheme within a Bayesian framework where for SBH data, birth and death dates are introduced as auxiliary variables. Since we specify a full probability model for the data, many of the well-known biases that exist in this data can be accommodated, along with space-time smoothing on the underlying mortality rates. We illustrate our approach in a simulation, showing robustness to model misspecification and that uncertainty is reduced when incorporating SBH data over simply analyzing all available FBH data. We also apply our approach to data from the Central region of Malawi and compare with the well-known Brass method.},
  archive      = {J_BIOMTC},
  author       = {Katie Wilson and Jon Wakefield},
  doi          = {10.1111/biom.13383},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1456-1466},
  shortjournal = {Biometrics},
  title        = {Child mortality estimation incorporating summary birth history data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using the “hidden” genome to improve classification of
cancer types. <em>BIOMTC</em>, <em>77</em>(4), 1445–1455. (<a
href="https://doi.org/10.1111/biom.13367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is increasingly common clinically for cancer specimens to be examined using techniques that identify somatic mutations. In principle, these mutational profiles can be used to diagnose the tissue of origin, a critical task for the 3\% to 5\% of tumors that have an unknown primary site. Diagnosis of primary site is also critical for screening tests that employ circulating DNA. However, most mutations observed in any new tumor are very rarely occurring mutations, and indeed the preponderance of these may never have been observed in any previous recorded tumor. To create a viable diagnostic tool we need to harness the information content in this “hidden genome” of variants for which no direct information is available. To accomplish this we propose a multilevel meta-feature regression to extract the critical information from rare variants in the training data in a way that permits us to also extract diagnostic information from any previously unobserved variants in the new tumor sample. A scalable implementation of the model is obtained by combining a high-dimensional feature screening approach with a group-lasso penalized maximum likelihood approach based on an equivalent mixed-effect representation of the multilevel model. We apply the method to the Cancer Genome Atlas whole-exome sequencing data set including 3702 tumor samples across seven common cancer sites. Results show that our multilevel approach can harness substantial diagnostic information from the hidden genome.},
  archive      = {J_BIOMTC},
  author       = {Saptarshi Chakraborty and Colin B. Begg and Ronglai Shen},
  doi          = {10.1111/biom.13367},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1445-1455},
  shortjournal = {Biometrics},
  title        = {Using the “Hidden” genome to improve classification of cancer types},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian approach to restricted latent class models for
scientifically structured clustering of multivariate binary outcomes.
<em>BIOMTC</em>, <em>77</em>(4), 1431–1444. (<a
href="https://doi.org/10.1111/biom.13388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a model-based method for clustering multivariate binary observations that incorporates constraints consistent with the scientific context. The approach is motivated by the precision medicine problem of identifying autoimmune disease patient subsets or classes who may require different treatments. We start with a family of restricted latent class models or RLCMs. However, in the motivating example and many others like it, the unknown number of classes and the definition of classes using binary states are among the targets of inference. We use a Bayesian approach to RLCMs in order to use informative prior assumptions on the number and definitions of latent classes to be consistent with scientific knowledge so that the posterior distribution tends to concentrate on smaller numbers of clusters and sparser binary patterns. The paper derives a posterior sampling algorithm based on Markov chain Monte Carlo with split-merge updates to efficiently explore the space of clustering allocations. Through simulations under the assumed model and realistic deviations from it, we demonstrate greater interpretability of results and superior finite-sample clustering performance for our method compared to common alternatives. The methods are illustrated with an analysis of protein data to detect clusters representing autoantibody classes among scleroderma patients.},
  archive      = {J_BIOMTC},
  author       = {Zhenke Wu and Livia Casciola-Rosen and Antony Rosen and Scott L. Zeger},
  doi          = {10.1111/biom.13388},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1431-1444},
  shortjournal = {Biometrics},
  title        = {A bayesian approach to restricted latent class models for scientifically structured clustering of multivariate binary outcomes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Receiver operating characteristic curves and confidence
bands for support vector machines. <em>BIOMTC</em>, <em>77</em>(4),
1422–1430. (<a href="https://doi.org/10.1111/biom.13365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many problems that appear in biomedical decision-making, such as diagnosing disease and predicting response to treatment, can be expressed as binary classification problems. The support vector machine (SVM) is a popular classification technique that is robust to model misspecification and effectively handles high-dimensional data. The relative costs of false positives and false negatives can vary across application domains. The receiving operating characteristic (ROC) curve provides a visual representation of the trade-off between these two types of errors. Because the SVM does not produce a predicted probability, an ROC curve cannot be constructed in the traditional way of thresholding a predicted probability. However, a sequence of weighted SVMs can be used to construct an ROC curve. Although ROC curves constructed using weighted SVMs have great potential for allowing ROC curves analyses that cannot be done by thresholding predicted probabilities, their theoretical properties have heretofore been underdeveloped. We propose a method for constructing confidence bands for the SVM ROC curve and provide the theoretical justification for the SVM ROC curve by showing that the risk function of the estimated decision rule is uniformly consistent across the weight parameter. We demonstrate the proposed confidence band method using simulation studies. We present a predictive model for treatment response in breast cancer as an illustrative example.},
  archive      = {J_BIOMTC},
  author       = {Daniel J. Luckett and Eric B. Laber and Samer S. El-Kamary and Cheng Fan and Ravi Jhaveri and Charles M. Perou and Fatma M. Shebl and Michael R. Kosorok},
  doi          = {10.1111/biom.13365},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1422-1430},
  shortjournal = {Biometrics},
  title        = {Receiver operating characteristic curves and confidence bands for support vector machines},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain connectivity alteration detection via matrix-variate
differential network model. <em>BIOMTC</em>, <em>77</em>(4), 1409–1421.
(<a href="https://doi.org/10.1111/biom.13359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain functional connectivity reveals the synchronization of brain systems through correlations in neurophysiological measures of brain activities. Growing evidence now suggests that the brain connectivity network experiences alterations with the presence of numerous neurological disorders, thus differential brain network analysis may provide new insights into disease pathologies. The data from neurophysiological measurement are often multidimensional and in a matrix form, posing a challenge in brain connectivity analysis. Existing graphical model estimation methods either assume a vector normal distribution that in essence requires the columns of the matrix data to be independent or fail to address the estimation of differential networks across different populations. To tackle these issues, we propose an innovative matrix-variate differential network (MVDN) model. We exploit the D-trace loss function and a Lasso-type penalty to directly estimate the spatial differential partial correlation matrix and use an alternating direction method of multipliers algorithm for the optimization problem. Theoretical and simulation studies demonstrate that MVDN significantly outperforms other state-of-the-art methods in dynamic differential network analysis. We illustrate with a functional connectivity analysis of an attention deficit hyperactivity disorder dataset. The hub nodes and differential interaction patterns identified are consistent with existing experimental studies.},
  archive      = {J_BIOMTC},
  author       = {Jiadong Ji and Yong He and Lei Liu and Lei Xie},
  doi          = {10.1111/biom.13359},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1409-1421},
  shortjournal = {Biometrics},
  title        = {Brain connectivity alteration detection via matrix-variate differential network model},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Histopathological imaging-based cancer heterogeneity
analysis via penalized fusion with model averaging. <em>BIOMTC</em>,
<em>77</em>(4), 1397–1408. (<a
href="https://doi.org/10.1111/biom.13357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneity is a hallmark of cancer. For various cancer outcomes/phenotypes, supervised heterogeneity analysis has been conducted, leading to a deeper understanding of disease biology and customized clinical decisions. In the literature, such analysis has been oftentimes based on demographic, clinical, and omics measurements. Recent studies have shown that high-dimensional histopathological imaging features contain valuable information on cancer outcomes. However, comparatively, heterogeneity analysis based on imaging features has been very limited. In this article, we conduct supervised cancer heterogeneity analysis using histopathological imaging features. The penalized fusion technique, which has notable advantages—such as greater flexibility—over the finite mixture modeling and other techniques, is adopted. A sparse penalization is further imposed to accommodate high dimensionality and select relevant imaging features. To improve computational feasibility and generate more reliable estimation, we employ model averaging. Computational and statistical properties of the proposed approach are carefully investigated. Simulation demonstrates its favorable performance. The analysis of The Cancer Genome Atlas (TCGA) data may provide a new way of defining/examining breast cancer heterogeneity.},
  archive      = {J_BIOMTC},
  author       = {Baihua He and Tingyan Zhong and Jian Huang and Yanyan Liu and Qingzhao Zhang and Shuangge Ma},
  doi          = {10.1111/biom.13357},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1397-1408},
  shortjournal = {Biometrics},
  title        = {Histopathological imaging-based cancer heterogeneity analysis via penalized fusion with model averaging},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A random covariance model for bi-level graphical modeling
with application to resting-state fMRI data. <em>BIOMTC</em>,
<em>77</em>(4), 1385–1396. (<a
href="https://doi.org/10.1111/biom.13364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a novel problem, bi-level graphical modeling, in which multiple individual graphical models can be considered as variants of a common group-level graphical model and inference of both the group- and individual-level graphical models is of interest. Such a problem arises from many applications, including multi-subject neuro-imaging and genomics data analysis. We propose a novel and efficient statistical method, the random covariance model, to learn the group- and individual-level graphical models simultaneously. The proposed method can be nicely interpreted as a random covariance model that mimics the random effects model for mean structures in linear regression. It accounts for similarity between individual graphical models, identifies group-level connections that are shared by individuals, and simultaneously infers multiple individual-level networks. Compared to existing multiple graphical modeling methods that only focus on individual-level graphical modeling, our model learns the group-level structure underlying the multiple individual graphical models and enjoys computational efficiency that is particularly attractive for practical use. We further define a measure of degrees-of-freedom for the complexity of the model useful for model selection. We demonstrate the asymptotic properties of our method and show its finite-sample performance through simulation studies. Finally, we apply the method to our motivating clinical data, a multi-subject resting-state functional magnetic resonance imaging dataset collected from participants diagnosed with schizophrenia, identifying both individual- and group-level graphical models of functional connectivity.},
  archive      = {J_BIOMTC},
  author       = {Lin Zhang and Andrew DiLernia and Karina Quevedo and Jazmin Camchong and Kelvin Lim and Wei Pan},
  doi          = {10.1111/biom.13364},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1385-1396},
  shortjournal = {Biometrics},
  title        = {A random covariance model for bi-level graphical modeling with application to resting-state fMRI data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Poisson PCA: Poisson measurement error corrected PCA, with
application to microbiome data. <em>BIOMTC</em>, <em>77</em>(4),
1369–1384. (<a href="https://doi.org/10.1111/biom.13384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of computing a principal component analysis of data affected by Poisson noise. We assume samples are drawn from independent Poisson distributions. We want to estimate principal components of a fixed transformation of the latent Poisson means. Our motivating example is microbiome data, though the methods apply to many other situations. We develop a semiparametric approach to correct the bias of variance estimators, both for untransformed and transformed (with particular attention to log-transformation) Poisson means. Furthermore, we incorporate methods for correcting different exposure or sequencing depth in the data. In addition to identifying the principal components, we also address the nontrivial problem of computing the principal scores in this semiparametric framework. Most previous approaches tend to take a more parametric line: for example, fitting a log-normal Poisson (PLN) model. We compare our method with the PLN approach and find that in many cases our method is better at identifying the main principal components of the latent log-transformed Poisson means, and as a further major advantage, takes far less time to compute. Comparing methods on real and simulated data, we see that our method also appears to be more robust to outliers than the parametric method.},
  archive      = {J_BIOMTC},
  author       = {Toby Kenney and Hong Gu and Tianshu Huang},
  doi          = {10.1111/biom.13384},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1369-1384},
  shortjournal = {Biometrics},
  title        = {Poisson PCA: Poisson measurement error corrected PCA, with application to microbiome data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized fieller’s confidence interval for the ratio of
bivariate normal means. <em>BIOMTC</em>, <em>77</em>(4), 1355–1368. (<a
href="https://doi.org/10.1111/biom.13363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing a confidence interval for the ratio of bivariate normal means is a classical problem in statistics. Several methods have been proposed in the literature. The Fieller method is known as an exact method, but can produce an unbounded confidence interval if the denominator of the ratio is not significantly deviated from 0; while the delta and some numeric methods are all bounded, they are only first-order correct. Motivated by a real-world problem, we propose the penalized Fieller method, which employs the same principle as the Fieller method, but adopts a penalized likelihood approach to estimate the denominator. The proposed method has a simple closed form, and can always produce a bounded confidence interval by selecting a suitable penalty parameter. Moreover, the new method is shown to be second-order correct under the bivariate normality assumption, that is, its coverage probability will converge to the nominal level faster than other bounded methods. Simulation results show that our proposed method generally outperforms the existing methods in terms of controlling the coverage probability and the confidence width and is particularly useful when the denominator does not have adequate power to reject being 0. Finally, we apply the proposed approach to the interval estimation of the median response dose in pharmacology studies to show its practical usefulness.},
  archive      = {J_BIOMTC},
  author       = {Peng Wang and Siqi Xu and Yi-Xin Wang and Baolin Wu and Wing Kam Fung and Guimin Gao and Zhijiang Liang and Nianjun Liu},
  doi          = {10.1111/biom.13363},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1355-1368},
  shortjournal = {Biometrics},
  title        = {Penalized fieller&#39;s confidence interval for the ratio of bivariate normal means},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A stacked approach for chained equations multiple imputation
incorporating the substantive model. <em>BIOMTC</em>, <em>77</em>(4),
1342–1354. (<a href="https://doi.org/10.1111/biom.13372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation by chained equations (MICE) has emerged as a popular approach for handling missing data. A central challenge for applying MICE is determining how to incorporate outcome information into covariate imputation models, particularly for complicated outcomes. Often, we have a particular analysis model in mind, and we would like to ensure congeniality between the imputation and analysis models. We propose a novel strategy for directly incorporating the analysis model into the handling of missing data. In our proposed approach, multiple imputations of missing covariates are obtained without using outcome information. We then utilize the strategy of imputation stacking, where multiple imputations are stacked on top of each other to create a large data set. The analysis model is then incorporated through weights. Instead of applying Rubin&#39;s combining rules, we obtain parameter estimates by fitting a weighted version of the analysis model on the stacked data set. We propose a novel estimator for obtaining standard errors for this stacked and weighted analysis. Our estimator is based on the observed data information principle in Louis&#39; work and can be applied for analyzing stacked multiple imputations more generally. Our approach for analyzing stacked multiple imputations is the first method that can be easily applied (using R package StackImpute) for a wide variety of standard analysis models and missing data settings.},
  archive      = {J_BIOMTC},
  author       = {Lauren J. Beesley and Jeremy M. G. Taylor},
  doi          = {10.1111/biom.13372},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1342-1354},
  shortjournal = {Biometrics},
  title        = {A stacked approach for chained equations multiple imputation incorporating the substantive model},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling sparse longitudinal data on riemannian manifolds.
<em>BIOMTC</em>, <em>77</em>(4), 1328–1341. (<a
href="https://doi.org/10.1111/biom.13385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data collection often entails longitudinal repeated measurements that assume values on a Riemannian manifold. Analyzing such longitudinal Riemannian data is challenging, because of both the sparsity of the observations and the nonlinear manifold constraint. Addressing this challenge, we propose an intrinsic functional principal component analysis for longitudinal Riemannian data. Information is pooled across subjects by estimating the mean curve with local Fréchet regression and smoothing the covariance structure of the linearized data on tangent spaces around the mean. Dimension reduction and imputation of the manifold-valued trajectories are achieved by utilizing the leading principal components and applying best linear unbiased prediction. We show that the proposed mean and covariance function estimates achieve state-of-the-art convergence rates. For illustration, we study the development of brain connectivity in a longitudinal cohort of Alzheimer&#39;s disease and normal participants by modeling the connectivity on the manifold of symmetric positive definite matrices with the affine-invariant metric. In a second illustration for irregularly recorded longitudinal emotion compositional data for unemployed workers, we show that the proposed method leads to nicely interpretable eigenfunctions and principal component scores. Data used in preparation of this article were obtained from the Alzheimer&#39;s Disease Neuroimaging Initiative database.},
  archive      = {J_BIOMTC},
  author       = {Xiongtao Dai and Zhenhua Lin and Hans-Georg Müller},
  doi          = {10.1111/biom.13385},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1328-1341},
  shortjournal = {Biometrics},
  title        = {Modeling sparse longitudinal data on riemannian manifolds},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating multiple surrogate markers with censored data.
<em>BIOMTC</em>, <em>77</em>(4), 1315–1327. (<a
href="https://doi.org/10.1111/biom.13370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of surrogate markers offers the opportunity to reduce the length of required follow-up time and/or costs of a randomized trial examining the effectiveness of an intervention or treatment. There are many available methods for evaluating the utility of a single surrogate marker including both parametric and nonparametric approaches. However, as the dimension of the surrogate marker increases, a completely nonparametric procedure becomes infeasible due to the curse of dimensionality. In this paper, we define a quantity to assess the value of multiple surrogate markers in a time-to-event outcome setting and propose a robust estimation approach for censored data. We focus on surrogate markers that are measured at some landmark time, t 0 , which occurs earlier than the end of the study. Our approach is based on a dimension reduction procedure with an option to incorporate weights to guard against potential misspecification of the working model, resulting in three different proposed estimators, two of which can be shown to be double robust. We examine the finite sample performance of the estimators under various scenarios using a simulation study. We illustrate the estimation and inference procedures using data from the Diabetes Prevention Program (DPP) to examine multiple potential surrogate markers for diabetes.},
  archive      = {J_BIOMTC},
  author       = {Layla Parast and Tianxi Cai and Lu Tian},
  doi          = {10.1111/biom.13370},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1315-1327},
  shortjournal = {Biometrics},
  title        = {Evaluating multiple surrogate markers with censored data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pairwise pseudo-likelihood approach for left-truncated and
interval-censored data under the cox model. <em>BIOMTC</em>,
<em>77</em>(4), 1303–1314. (<a
href="https://doi.org/10.1111/biom.13394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Left truncation commonly occurs in many areas, and many methods have been proposed in the literature for the analysis of various types of left-truncated failure time data. For the situation, a common approach is to conduct the analysis conditional on truncation times, and the method is relatively simple but may not be efficient. In this paper, we discuss regression analysis of such data arising from the proportional hazards model that also suffer interval censoring. For the problem, a pairwise pseudo-likelihood approach is proposed that aims to recover some missing information in the conditional methods. The resulting estimator is shown to be consistent and asymptotically normal. A simulation study is conducted to assess the performance of the proposed method and suggests that it works well in practical situations and is indeed more efficient than the existing method. The approach is also applied to a set of real data arising from an AIDS cohort study.},
  archive      = {J_BIOMTC},
  author       = {Peijie Wang and Danning Li and Jianguo Sun},
  doi          = {10.1111/biom.13394},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1303-1314},
  shortjournal = {Biometrics},
  title        = {A pairwise pseudo-likelihood approach for left-truncated and interval-censored data under the cox model},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling excess hazard with time-to-cure as a parameter.
<em>BIOMTC</em>, <em>77</em>(4), 1289–1302. (<a
href="https://doi.org/10.1111/biom.13361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cure models have been widely developed to estimate the cure fraction when some subjects never experience the event of interest. However, these models were rarely focused on the estimation of the time-to-cure, that is, the delay elapsed between the diagnosis and “the time from which cure is reached,” an important indicator, for instance, to address the question of access to insurance or loans for subjects with personal history of cancer. We propose a new excess hazard regression model that includes the time-to-cure as a covariate-dependent parameter to be estimated. The model is written similarly to a Beta probability distribution function and is shown to be a particular case of the non–mixture cure models. Parameters are estimated through a maximum likelihood approach and simulation studies demonstrate good performance of the model. Illustrative applications to three cancer data sets are provided and some limitations as well as possible extensions of the model are discussed. The proposed model offers a simple and comprehensive way to estimate more accurately the time-to-cure.},
  archive      = {J_BIOMTC},
  author       = {Olayidé Boussari and Laurent Bordes and Gaëlle Romain and Marc Colonna and Nadine Bossard and Laurent Remontet and Valérie Jooste},
  doi          = {10.1111/biom.13361},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1289-1302},
  shortjournal = {Biometrics},
  title        = {Modeling excess hazard with time-to-cure as a parameter},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating and improving a matched comparison of
antidepressants and bone density. <em>BIOMTC</em>, <em>77</em>(4),
1276–1288. (<a href="https://doi.org/10.1111/biom.13374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching is a common approach to covariate adjustment in estimating causal effects in observational studies. It is important to assess covariate balance of the matched samples. This is usually done informally, in ways that have a number of limitations. First, there are many diagnostics, even if covariates are assessed one at a time, which raises multiplicity issues. In addition, joint distributions of covariates, even bivariate distributions, are often ignored. Further, it is an open question whether diagnostics identify the major problems. To address these issues, a formal assessment of covariate balance is developed in the current paper. Unlike the common informal diagnostics, the proposed method compares both marginal distributions and joint distributions of the matched sample with those of the benchmark, complete randomizations. The method controls the probability of falsely identifying a covariate imbalance among many comparisons, yet it has a high probability of correctly detecting and identifying a major problem. An R package met implementing the proposed method is available on CRAN .},
  archive      = {J_BIOMTC},
  author       = {Ruoqi Yu},
  doi          = {10.1111/biom.13374},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1276-1288},
  shortjournal = {Biometrics},
  title        = {Evaluating and improving a matched comparison of antidepressants and bone density},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A class of proportional win-fractions regression models for
composite outcomes. <em>BIOMTC</em>, <em>77</em>(4), 1265–1275. (<a
href="https://doi.org/10.1111/biom.13382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The win ratio is gaining traction as a simple and intuitive approach to analysis of prioritized composite endpoints in clinical trials. To extend it from two-sample comparison to regression, we propose a novel class of semiparametric models that includes as special cases both the two-sample win ratio and the traditional Cox proportional hazards model on time to the first event. Under the assumption that the covariate-specific win and loss fractions are proportional over time, the regression coefficient is unrelated to the censoring distribution and can be interpreted as the log win ratio resulting from one-unit increase in the covariate. U -statistic estimating functions, in the form of an arbitrary covariate-specific weight process integrated by a pairwise residual process, are constructed to obtain consistent estimators for the regression parameter. The asymptotic properties of the estimators are derived using uniform weak convergence theory for U -processes. Visual inspection of a “score” process provides useful clues as to the plausibility of the proportionality assumption. Extensive numerical studies using both simulated and real data from a major cardiovascular trial show that the regression methods provide valid inference on covariate effects and outperform the two-sample win ratio in both efficiency and robustness. The proposed methodology is implemented in the R-package WR , publicly available from the Comprehensive R Archive Network (CRAN).},
  archive      = {J_BIOMTC},
  author       = {Lu Mao and Tuo Wang},
  doi          = {10.1111/biom.13382},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1265-1275},
  shortjournal = {Biometrics},
  title        = {A class of proportional win-fractions regression models for composite outcomes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Net benefit index: Assessing the influence of a biomarker
for individualized treatment rules. <em>BIOMTC</em>, <em>77</em>(4),
1254–1264. (<a href="https://doi.org/10.1111/biom.13373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One central task in precision medicine is to establish individualized treatment rules (ITRs) for patients with heterogeneous responses to different therapies. Motivated from a randomized clinical trial for Type 2 diabetic patients on a comparison of two drugs, that is, pioglitazone and gliclazide, we consider a problem: utilizing promising candidate biomarkers to improve an existing ITR. This calls for a biomarker evaluation procedure that enables to gauge added values of individual biomarkers. We propose an assessment analytic, termed as net benefit index (NBI) , that quantifies a contrast between the resulting gain and loss of treatment benefits when a biomarker enters ITR to reallocate patients in treatments. We optimize reallocation schemes via outcome weighted learning (OWL), from which the optimal treatment group labels are generated by weighted support vector machine (SVM). To account for sampling uncertainty in assessing a biomarker, we propose an NBI-based test for a significant improvement over the existing ITR, where the empirical null distribution is constructed via the method of stratified permutation by treatment arms. Applying NBI to the motivating diabetes trial, we found that baseline fasting insulin is an important biomarker that leads to an improvement over an existing ITR based only on patient&#39;s baseline fasting plasma glucose (FPG), age, and body mass index (BMI) to reduce FPG over a period of 52 weeks.},
  archive      = {J_BIOMTC},
  author       = {Yiwang Zhou and Peter X.K. Song and Haoda Fu},
  doi          = {10.1111/biom.13373},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1254-1264},
  shortjournal = {Biometrics},
  title        = {Net benefit index: Assessing the influence of a biomarker for individualized treatment rules},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient nonparametric inference on the effects of
stochastic interventions under two-phase sampling, with applications to
vaccine efficacy trials. <em>BIOMTC</em>, <em>77</em>(4), 1241–1253. (<a
href="https://doi.org/10.1111/biom.13375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent and subsequent widespread availability of preventive vaccines has altered the course of public health over the past century. Despite this success, effective vaccines to prevent many high-burden diseases, including human immunodeficiency virus (HIV), have been slow to develop. Vaccine development can be aided by the identification of immune response markers that serve as effective surrogates for clinically significant infection or disease endpoints. However, measuring immune response marker activity is often costly, which has motivated the usage of two-phase sampling for immune response evaluation in clinical trials of preventive vaccines. In such trials, the measurement of immunological markers is performed on a subset of trial participants, where enrollment in this second phase is potentially contingent on the observed study outcome and other participant-level information. We propose nonparametric methodology for efficiently estimating a counterfactual parameter that quantifies the impact of a given immune response marker on the subsequent probability of infection. Along the way, we fill in theoretical gaps pertaining to the asymptotic behavior of nonparametric efficient estimators in the context of two-phase sampling, including a multiple robustness property enjoyed by our estimators. Techniques for constructing confidence intervals and hypothesis tests are presented, and an open source software implementation of the methodology, the txshift R package, is introduced. We illustrate the proposed techniques using data from a recent preventive HIV vaccine efficacy trial.},
  archive      = {J_BIOMTC},
  author       = {Nima S. Hejazi and Mark J. van der Laan and Holly E. Janes and Peter B. Gilbert and David C. Benkeser},
  doi          = {10.1111/biom.13375},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1241-1253},
  shortjournal = {Biometrics},
  title        = {Efficient nonparametric inference on the effects of stochastic interventions under two-phase sampling, with applications to vaccine efficacy trials},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric matrix response regression with application to
brain imaging data analysis. <em>BIOMTC</em>, <em>77</em>(4), 1227–1240.
(<a href="https://doi.org/10.1111/biom.13362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of neuroimaging technologies, a great effort has been dedicated recently to investigate the dynamic changes in brain activity. Examples include time course calcium imaging and dynamic brain functional connectivity. In this paper, we propose a novel nonparametric matrix response regression model to characterize the nonlinear association between 2D image outcomes and predictors such as time and patient information. Our estimation procedure can be formulated as a nuclear norm regularization problem, which can capture the underlying low-rank structure of the dynamic 2D images. We present a computationally efficient algorithm, derive the asymptotic theory, and show that the method outperforms other existing approaches in simulations. We then apply the proposed method to a calcium imaging study for estimating the change of fluorescent intensities of neurons, and an electroencephalography study for a comparison in the dynamic connectivity covariance matrices between alcoholic and control individuals. For both studies, the method leads to a substantial improvement in prediction error.},
  archive      = {J_BIOMTC},
  author       = {Wei Hu and Tianyu Pan and Dehan Kong and Weining Shen},
  doi          = {10.1111/biom.13362},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1227-1240},
  shortjournal = {Biometrics},
  title        = {Nonparametric matrix response regression with application to brain imaging data analysis},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric cluster significance testing with reference to
a unimodal null distribution. <em>BIOMTC</em>, <em>77</em>(4),
1215–1226. (<a href="https://doi.org/10.1111/biom.13376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis is an unsupervised learning strategy that is exceptionally useful for identifying homogeneous subgroups of observations in data sets of unknown structure. However, it is challenging to determine if the identified clusters represent truly distinct subgroups rather than noise. Existing approaches for addressing this problem tend to define clusters based on distributional assumptions, ignore the inherent correlation structure in the data, or are not suited for high-dimension low-sample size (HDLSS) settings. In this paper, we propose a novel method to evaluate the significance of identified clusters by comparing the explained variation due to the clustering from the original data to that produced by clustering a unimodal reference distribution that preserves the covariance structure in the data. The reference distribution is generated using kernel density estimation, and thus, does not require that the data follow a particular distribution. By utilizing sparse covariance estimation, the method is adapted for the HDLSS setting. The approach can be used to test the null hypothesis that the data cannot be partitioned into clusters and to determine the optimal number of clusters. Simulation examples, theoretical evaluations, and applications to temporomandibular disorder research and cancer microarray data illustrate the utility of the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Erika S. Helgeson and David M. Vock and Eric Bair},
  doi          = {10.1111/biom.13376},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1215-1226},
  shortjournal = {Biometrics},
  title        = {Nonparametric cluster significance testing with reference to a unimodal null distribution},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A marginal moment matching approach for fitting
endemic-epidemic models to underreported disease surveillance counts.
<em>BIOMTC</em>, <em>77</em>(4), 1202–1214. (<a
href="https://doi.org/10.1111/biom.13371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data are often subject to underreporting, especially in infectious disease surveillance. We propose an approximate maximum likelihood method to fit count time series models from the endemic-epidemic class to underreported data. The approach is based on marginal moment matching where underreported processes are approximated through completely observed processes from the same class. Moreover, the form of the bias when underreporting is ignored or taken into account via multiplication factors is analyzed. Notably, we show that this leads to a downward bias in model-based estimates of the effective reproductive number. A marginal moment matching approach can also be used to account for reporting intervals which are longer than the mean serial interval of a disease. The good performance of the proposed methodology is demonstrated in simulation studies. An extension to time-varying parameters and reporting probabilities is discussed and applied in a case study on weekly rotavirus gastroenteritis counts in Berlin, Germany.},
  archive      = {J_BIOMTC},
  author       = {Johannes Bracher and Leonhard Held},
  doi          = {10.1111/biom.13371},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1202-1214},
  shortjournal = {Biometrics},
  title        = {A marginal moment matching approach for fitting endemic-epidemic models to underreported disease surveillance counts},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric models and inference for the effect of a
treatment when the outcome is nonnegative with clumping at zero.
<em>BIOMTC</em>, <em>77</em>(4), 1187–1201. (<a
href="https://doi.org/10.1111/biom.13368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outcome in a randomized experiment is sometimes nonnegative with a clump of observations at zero and continuously distributed positive values. One widely used model for a nonnegative outcome with a clump at zero is the Tobit model, which assumes that the treatment has a shift effect on the distribution of a normally distributed latent variable and the observed outcome is the maximum of the latent variable and zero. We develop a class of semiparametric models and inference procedures that extend the Tobit model in two useful directions. First, we consider more flexible models for the treatment effect than the shift effect of the Tobit model; for example, our models allow for the treatment to have a larger in magnitude effect for upper quantiles. Second, we make semiparametric inferences using empirical likelihood that allow the underlying latent variable to have any distribution, unlike the original Tobit model that assumes the latent variable is normally distributed. We apply our approach to data from the RAND Health Insurance Experiment. We also extend our approach to observational studies in which treatment assignment is strongly ignorable.},
  archive      = {J_BIOMTC},
  author       = {Jing Cheng and Dylan S. Small},
  doi          = {10.1111/biom.13368},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1187-1201},
  shortjournal = {Biometrics},
  title        = {Semiparametric models and inference for the effect of a treatment when the outcome is nonnegative with clumping at zero},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric partial common principal component analysis
for covariance matrices. <em>BIOMTC</em>, <em>77</em>(4), 1175–1186. (<a
href="https://doi.org/10.1111/biom.13369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of jointly modeling multiple covariance matrices by partial common principal component analysis (PCPCA), which assumes a proportion of eigenvectors to be shared across covariance matrices and the rest to be individual-specific. This paper proposes consistent estimators of the shared eigenvectors in the PCPCA as the number of matrices or the number of samples to estimate each matrix goes to infinity. We prove such asymptotic results without making any assumptions on the ranks of eigenvalues that are associated with the shared eigenvectors. When the number of samples goes to infinity, our results do not require the data to be Gaussian distributed. Furthermore, this paper introduces a sequential testing procedure to identify the number of shared eigenvectors in the PCPCA. In simulation studies, our method shows higher accuracy in estimating the shared eigenvectors than competing methods. Applied to a motor-task functional magnetic resonance imaging data set, our estimator identifies meaningful brain networks that are consistent with current scientific understandings of motor networks during a motor paradigm.},
  archive      = {J_BIOMTC},
  author       = {Bingkai Wang and Xi Luo and Yi Zhao and Brian Caffo},
  doi          = {10.1111/biom.13369},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1175-1186},
  shortjournal = {Biometrics},
  title        = {Semiparametric partial common principal component analysis for covariance matrices},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder to “causal mediation of semicompeting risks.”
<em>BIOMTC</em>, <em>77</em>(4), 1170–1174. (<a
href="https://doi.org/10.1111/biom.13518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yen-Tsung Huang},
  doi          = {10.1111/biom.13518},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1170-1174},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Causal mediation of semicompeting risks”},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “causal mediation of semicompeting risks” by
yen-tsung huang. <em>BIOMTC</em>, <em>77</em>(4), 1165–1169. (<a
href="https://doi.org/10.1111/biom.13519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Huang proposes a method for assessing the impact of a point treatment on mortality either directly or mediated by occurrence of a nonterminal health event, based on data from a prospective cohort study in which the occurrence of the nonterminal health event may be preemptied by death but not vice versa. The author uses a causal mediation framework to formally define causal quantities known as natural (in)direct effects. The novelty consists of adapting these concepts to a continuous-time modeling framework based on counting processes. In an effort to posit “scientifically interpretable estimands,” statistical and causal assumptions are introduced for identification. In this commentary, we argue that these assumptions are not only difficult to interpret and justify, but are also likely violated in the hepatitis B motivating example and other survival/time to event settings as well.},
  archive      = {J_BIOMTC},
  author       = {Isabel R. Fulcher and Ilya Shpitser and Vanessa Didelez and Kali Zhou and Daniel O. Scharfstein},
  doi          = {10.1111/biom.13519},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1165-1169},
  shortjournal = {Biometrics},
  title        = {Discussion on “Causal mediation of semicompeting risks” by yen-tsung huang},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “causal mediation of semicompeting risks” by
yen-tsung huang. <em>BIOMTC</em>, <em>77</em>(4), 1160–1164. (<a
href="https://doi.org/10.1111/biom.13523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Mats J. Stensrud and Jessica G. Young and Torben Martinussen},
  doi          = {10.1111/biom.13523},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1160-1164},
  shortjournal = {Biometrics},
  title        = {Discussion on “Causal mediation of semicompeting risks” by yen-tsung huang},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “causal mediation of semicompeting risks” by
yen-tsung huang. <em>BIOMTC</em>, <em>77</em>(4), 1155–1159. (<a
href="https://doi.org/10.1111/biom.13520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Kwun Chuen Gary Chan and Fei Gao and Fan Xia},
  doi          = {10.1111/biom.13520},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1155-1159},
  shortjournal = {Biometrics},
  title        = {Discussion on “Causal mediation of semicompeting risks” by yen-tsung huang},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Causal mediation of semicompeting risks. <em>BIOMTC</em>,
<em>77</em>(4), 1143–1154. (<a
href="https://doi.org/10.1111/biom.13525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semi-competing risks problem arises when one is interested in the effect of an exposure or treatment on both intermediate (e.g., having cancer) and primary events (e.g., death) where the intermediate event may be censored by the primary event, but not vice versa. Here we propose a nonparametric approach casting the semi-competing risks problem in the framework of causal mediation modeling. We set up a mediation model with the intermediate and primary events, respectively as the mediator and the outcome, and define an indirect effect as the effect of the exposure on the primary event mediated by the intermediate event and a direct effect as that not mediated by the intermediate event. A nonparametric estimator with time-varying weights is proposed for direct and indirect effects where the counting process at time t of the primary event ◂◽.▸ N 2 ⁢ n 1 ⁡ ( t ) and its compensator ◂◽.▸ A n 1 ⁡ ( t ) are both defined conditional on the status of the intermediate event right before t , ◂=▸ N 1 ⁡ ( t − ) = n 1 . We show that ◂−▸ ◂◽.▸ N 2 ⁢ n 1 ⁡ ( t ) − ◂◽.▸ A n 1 ⁡ ( t ) is a zero-mean martingale. Based on this, we further establish theoretical properties for the proposed estimators. Simulation studies are presented to illustrate the finite sample performance of the proposed method. Its advantage in causal interpretation over existing methods is also demonstrated in a hepatitis study.},
  archive      = {J_BIOMTC},
  author       = {Yen-Tsung Huang},
  doi          = {10.1111/biom.13525},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1143-1154},
  shortjournal = {Biometrics},
  title        = {Causal mediation of semicompeting risks},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handbook of neuroimaging data analysis HernandoOmbao,
MartinLindquist, WesleyThompson, JohnAston boca raton, FL: CRC press,
2017. Hard cover. Pp. 702. 210.00 USD. <em>BIOMTC</em>, <em>77</em>(3),
1135–1137. (<a href="https://doi.org/10.1111/biom.13529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Andrew Whiteman},
  doi          = {10.1111/biom.13529},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1135-1137},
  shortjournal = {Biometrics},
  title        = {Handbook of neuroimaging data analysis HernandoOmbao, MartinLindquist, WesleyThompson, JohnAston boca raton, FL: CRC press, 2017. hard cover. pp. 702. 210.00 USD},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). <em>BIOMTC</em>, <em>77</em>(3), 1132–1135. (<a
href="https://doi.org/10.1111/biom.13522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen},
  doi          = {10.1111/biom.13522},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1132-1135},
  shortjournal = {Biometrics},
  title        = {Statistical foundations of data science jianqing fan, runze li, cun-hui zhang and hui zou boca raton, FL: Chapman and Hall/CRC, 2021. hard cover. pp. 774. USD $130.00},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to data science: Data analysis and algorithms
with r, by rafael irrizarry boca raton, FL: CRC press, 2020. Hard cover.
Pp. 711. <em>BIOMTC</em>, <em>77</em>(3), 1131–1132. (<a
href="https://doi.org/10.1111/biom.13521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Nairanjana Dasgupta},
  doi          = {10.1111/biom.13521},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1131-1132},
  shortjournal = {Biometrics},
  title        = {Introduction to data science: data analysis and algorithms with r, by rafael irrizarry boca raton, FL: CRC press, 2020. hard cover. pp. 711.},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structural equation modeling with partial least squares
using stata and r by mehmet mehmetoglu and sergio venturini boca raton,
FL, USA: Chapman and hall/CRC, 2021. Hard cover. Pp. 382. £ 99.99.
<em>BIOMTC</em>, <em>77</em>(3), 1130–1131. (<a
href="https://doi.org/10.1111/biom.13533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yu-Kang Tu},
  doi          = {10.1111/biom.13533},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1130-1131},
  shortjournal = {Biometrics},
  title        = {Structural equation modeling with partial least squares using stata and r by mehmet mehmetoglu and sergio venturini boca raton, FL, USA: Chapman and Hall/CRC, 2021. hard cover. pp. 382. £ 99.99},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). <em>BIOMTC</em>, <em>77</em>(3), 1129. (<a
href="https://doi.org/10.1111/biom.13534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yu-Chung Wei},
  doi          = {10.1111/biom.13534},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1129},
  shortjournal = {Biometrics},
  title        = {Statistics for making decisions by nicholas t. longford boca raton, FL: Chapman and Hall/CRC, 2021. hard cover. pp. 307. $96},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable and robust latent trajectory class analysis using
artificial likelihood. <em>BIOMTC</em>, <em>77</em>(3), 1118–1128. (<a
href="https://doi.org/10.1111/biom.13366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent trajectory class analysis is a powerful technique to elucidate the structure underlying population heterogeneity. The standard approach relies on fully parametric modeling and is computationally impractical when the data include a large collection of non-Gaussian longitudinal features. We introduce a new approach, the first based on artificial likelihood concepts, that avoids undue modeling assumptions and is computationally tractable. We show that this new method provides reliable estimates of the underlying population structure and is from 20 to 200 times faster than conventional methods when the longitudinal features are non-Gaussian. We apply the approach to explore subgroups among research participants in the early stages of neurodegeneration.},
  archive      = {J_BIOMTC},
  author       = {Kari R. Hart and Teng Fei and John J. Hanfelt},
  doi          = {10.1111/biom.13366},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1118-1128},
  shortjournal = {Biometrics},
  title        = {Scalable and robust latent trajectory class analysis using artificial likelihood},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variance estimation in inverse probability weighted cox
models. <em>BIOMTC</em>, <em>77</em>(3), 1101–1117. (<a
href="https://doi.org/10.1111/biom.13332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability weighted Cox models can be used to estimate marginal hazard ratios under different point treatments in observational studies. To obtain variance estimates, the robust sandwich variance estimator is often recommended to account for the induced correlation among weighted observations. However, this estimator does not incorporate the uncertainty in estimating the weights and tends to overestimate the variance, leading to inefficient inference. Here we propose a new variance estimator that combines the estimation procedures for the hazard ratio and weights using stacked estimating equations, with additional adjustments for the sum of terms that are not independently and identically distributed in a Cox partial likelihood score equation. We prove analytically that the robust sandwich variance estimator is conservative and establish the asymptotic equivalence between the proposed variance estimator and one obtained through linearization by Hajage et al . in 2018. In addition, we extend our proposed variance estimator to accommodate clustered data. We compare the finite sample performance of the proposed method with alternative methods through simulation studies. We illustrate these different variance methods in both independent and clustered data settings, using a bariatric surgery dataset and a multiple readmission dataset, respectively. To facilitate implementation of the proposed method, we have developed an R package ipwCoxCSV .},
  archive      = {J_BIOMTC},
  author       = {Di Shu and Jessica G. Young and Sengwee Toh and Rui Wang},
  doi          = {10.1111/biom.13332},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1101-1117},
  shortjournal = {Biometrics},
  title        = {Variance estimation in inverse probability weighted cox models},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inference for natural language processing
algorithms with a demonstration using type 2 diabetes prediction from
electronic health record notes. <em>BIOMTC</em>, <em>77</em>(3),
1089–1100. (<a href="https://doi.org/10.1111/biom.13338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pointwise mutual information statistic (PMI), which measures how often two words occur together in a document corpus, is a cornerstone of recently proposed popular natural language processing algorithms such as word2vec. PMI and word2vec reveal semantic relationships between words and can be helpful in a range of applications such as document indexing, topic analysis, or document categorization. We use probability theory to demonstrate the relationship between PMI and word2vec. We use the theoretical results to demonstrate how the PMI can be modeled and estimated in a simple and straight forward manner. We further describe how one can obtain standard error estimates that account for within-patient clustering that arises from patterns of repeated words within a patient&#39;s health record due to a unique health history. We then demonstrate the usefulness of PMI on the problem of predictive identification of disease from free text notes of electronic health records. Specifically, we use our methods to distinguish those with and without type 2 diabetes mellitus in electronic health record free text data using over 400 000 clinical notes from an academic medical center.},
  archive      = {J_BIOMTC},
  author       = {Brian L. Egleston and Tian Bai and Richard J. Bleicher and Stanford J. Taylor and Michael H. Lutz and Slobodan Vucetic},
  doi          = {10.1111/biom.13338},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1089-1100},
  shortjournal = {Biometrics},
  title        = {Statistical inference for natural language processing algorithms with a demonstration using type 2 diabetes prediction from electronic health record notes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A batch-effect adjusted simon’s two-stage design for cancer
vaccine clinical studies. <em>BIOMTC</em>, <em>77</em>(3), 1075–1088.
(<a href="https://doi.org/10.1111/biom.13358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the development of cancer treatment vaccines, phase II clinical studies are conducted to examine the efficacy of a vaccine in order to screen out vaccines with minimal activity. Immune responses are commonly used as the primary endpoint for assessing vaccine efficacy. With respect to study design, Simon&#39;s two-stage design is a popular format for phase II cancer clinical studies because of its simplicity and ethical considerations. Nonetheless, it is not straightforward to apply Simon&#39;s two-stage design to cancer vaccine studies when performing immune assays in batches, as outcomes from multiple patients may be correlated with each other in the presence of batch effects. This violates the independence assumption of Simon&#39;s two-stage design. In this paper, we numerically explore the impact of batch effects on Simon&#39;s two-stage design, propose a batch-effect adjusted Simon&#39;s two-stage design, demonstrate the proposed design by both a simulation study and a therapeutic human papillomavirus vaccine trial, and briefly introduce a software that implements the proposed design.},
  archive      = {J_BIOMTC},
  author       = {Chenguang Wang and Zhixin Wang and Gary L. Rosner and Warner K. Huh and Richard B.S. Roden and Sejong Bae},
  doi          = {10.1111/biom.13358},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1075-1088},
  shortjournal = {Biometrics},
  title        = {A batch-effect adjusted simon&#39;s two-stage design for cancer vaccine clinical studies},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint penalized spline modeling of multivariate longitudinal
data, with application to HIV-1 RNA load levels and CD4 cell counts.
<em>BIOMTC</em>, <em>77</em>(3), 1061–1074. (<a
href="https://doi.org/10.1111/biom.13339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the need to jointly model the longitudinal trajectories of HIV viral load levels and CD4 counts during the primary infection stage, we propose a joint penalized spline modeling approach that can be used to model the repeated measurements from multiple biomarkers of various types (eg, continuous, binary) simultaneously. This approach allows for flexible trajectories for each marker, accounts for potentially time-varying correlation between markers, and is robust to misspecification of knots. Despite its advantages, the application of multivariate penalized spline models, especially when biomarkers may be of different data types, has been limited in part due to its seemingly complexity in implementation. To overcome this, we describe a procedure that transforms the multivariate setting to the univariate one, and then makes use of the generalized linear mixed effect model representation of a penalized spline model to facilitate its implementation with standard statistical software. We performed simulation studies to evaluate the validity and efficiency through joint modeling of correlated biomarkers measured longitudinally compared to the univariate modeling approach. We applied this modeling approach to longitudinal HIV-1 RNA load and CD4 count data from Southern African cohorts to estimate features of the joint distributions such as the correlation and the proportion of subjects with high viral load levels and high CD4 cell counts over time.},
  archive      = {J_BIOMTC},
  author       = {Lihui Zhao and Tom Chen and Vladimir Novitsky and Rui Wang},
  doi          = {10.1111/biom.13339},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1061-1074},
  shortjournal = {Biometrics},
  title        = {Joint penalized spline modeling of multivariate longitudinal data, with application to HIV-1 RNA load levels and CD4 cell counts},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum likelihood abundance estimation from
capture-recapture data when covariates are missing at random.
<em>BIOMTC</em>, <em>77</em>(3), 1050–1060. (<a
href="https://doi.org/10.1111/biom.13334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In capture-recapture experiments, individual covariates may be subject to missingness, especially when the number of captures is small. When the covariate information is missing at random, the inverse probability weighting method and the multiple imputation method are widely used to obtain point estimators of the abundance. These estimators are then used to construct Wald-type confidence intervals. However, such intervals may have seriously inaccurate coverage probabilities. In this paper, we propose a maximum empirical likelihood (EL) estimation approach for the abundance in the presence of missing covariates. We show that the maximum EL estimator is asymptotically normal, and that the EL ratio statistic for the abundance has a chi-square limiting distribution with one degree of freedom. Simulations indicate that the proposed estimator has a smaller mean square error than existing estimators, and the proposed EL ratio confidence interval usually has more accurate coverage probabilities than the existing Wald-type confidence intervals. We illustrate the proposed method by analyzing data collected in Hong Kong for the yellow-bellied prinia, a bird species.},
  archive      = {J_BIOMTC},
  author       = {Yang Liu and Yukun Liu and Pengfei Li and Lin Zhu},
  doi          = {10.1111/biom.13334},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1050-1060},
  shortjournal = {Biometrics},
  title        = {Maximum likelihood abundance estimation from capture-recapture data when covariates are missing at random},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Post-selection inference for changepoint detection
algorithms with application to copy number variation data.
<em>BIOMTC</em>, <em>77</em>(3), 1037–1049. (<a
href="https://doi.org/10.1111/biom.13422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changepoint detection methods are used in many areas of science and engineering, for example, in the analysis of copy number variation data to detect abnormalities in copy numbers along the genome. Despite the broad array of available tools, methodology for quantifying our uncertainty in the strength (or the presence) of given changepoints post-selection are lacking. Post-selection inference offers a framework to fill this gap, but the most straightforward application of these methods results in low-powered hypothesis tests and leaves open several important questions about practical usability. In this work, we carefully tailor post-selection inference methods toward changepoint detection, focusing on copy number variation data. To accomplish this, we study commonly used changepoint algorithms: binary segmentation, as well as two of its most popular variants, wild and circular, and the fused lasso. We implement some of the latest developments in post-selection inference theory, mainly auxiliary randomization. This improves the power, which requires implementations of Markov chain Monte Carlo algorithms (importance sampling and hit-and-run sampling) to carry out our tests. We also provide recommendations for improving practical useability, detailed simulations, and example analyses on array comparative genomic hybridization as well as sequencing data.},
  archive      = {J_BIOMTC},
  author       = {Sangwon Hyun and Kevin Z. Lin and Max G&#39;Sell and Ryan J. Tibshirani},
  doi          = {10.1111/biom.13422},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1037-1049},
  shortjournal = {Biometrics},
  title        = {Post-selection inference for changepoint detection algorithms with application to copy number variation data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining primary cohort data with external aggregate
information without assuming comparability. <em>BIOMTC</em>,
<em>77</em>(3), 1024–1036. (<a
href="https://doi.org/10.1111/biom.13356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In comparative effectiveness research (CER) for rare types of cancer, it is appealing to combine primary cohort data containing detailed tumor profiles together with aggregate information derived from cancer registry databases. Such integration of data may improve statistical efficiency in CER. A major challenge in combining information from different resources, however, is that the aggregate information from the cancer registry databases could be incomparable with the primary cohort data, which are often collected from a single cancer center or a clinical trial. We develop an adaptive estimation procedure, which uses the combined information to determine the degree of information borrowing from the aggregate data of the external resource. We establish the asymptotic properties of the estimators and evaluate the finite sample performance via simulation studies. The proposed method yields a substantial gain in statistical efficiency over the conventional method using the primary cohort only, and avoids undesirable biases when the given external information is incomparable to the primary cohort. We apply the proposed method to evaluate the long-term effect of trimodality treatment to inflammatory breast cancer (IBC) by tumor subtypes, while combining the IBC patient cohort at The University of Texas MD Anderson Cancer Center and the external aggregate information from the National Cancer Data Base.},
  archive      = {J_BIOMTC},
  author       = {Ziqi Chen and Jing Ning and Yu Shen and Jing Qin},
  doi          = {10.1111/biom.13356},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1024-1036},
  shortjournal = {Biometrics},
  title        = {Combining primary cohort data with external aggregate information without assuming comparability},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covariate-driven factorization by thresholding for
multiblock data. <em>BIOMTC</em>, <em>77</em>(3), 1011–1023. (<a
href="https://doi.org/10.1111/biom.13352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiblock data, where multiple groups of variables from different sources are observed for a common set of subjects, are routinely collected in many areas of science. Methods for joint factorization of such multiblock data are being developed to explore the potentially joint variation structure of the data. While most of the existing work focuses on delineating joint components, shared across all data blocks, from individual components, which is only relevant to a single data block, we propose to model and estimate partially joint components across some, but not all, data blocks. If covariates, with potential multiblock structures, are available, then the components are further modeled to be driven by the covariate information. To estimate such a covariate-driven, block-structured factor model, we propose an iterative algorithm based on thresholding, by transforming the problem of signal segmentation into a grouped variable selection problem. The proposed factorization provides accurate estimation of individual and (partially) joint structures in multiblock data, as confirmed by simulation studies. In the analysis of a real multiblock genomic dataset from the Cancer Genome Atlas project, we demonstrate that the estimated block structures provide straightforward interpretation and facilitate subsequent analyses.},
  archive      = {J_BIOMTC},
  author       = {Xing Gao and Sungwon Lee and Gen Li and Sungkyu Jung},
  doi          = {10.1111/biom.13352},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1011-1023},
  shortjournal = {Biometrics},
  title        = {Covariate-driven factorization by thresholding for multiblock data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A weak-signal-assisted procedure for variable selection and
statistical inference with an informative subsample. <em>BIOMTC</em>,
<em>77</em>(3), 996–1010. (<a
href="https://doi.org/10.1111/biom.13346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated from an HIV-1 drug resistance study where we encounter three analytical challenges: to analyze data with an informative subsample, to take into account the weak signals, and to detect important signals and also conduct statistical inference. We start with an initial estimation method, which adopts a penalized pairwise conditional likelihood approach for variable selection. This initial estimator incorporates the informative subsample issue. To accounting for the effect of weak signals, we use a key idea of partial ridge regression. We also propose a one-step estimation method for each of the signal coefficients and then construct confidence intervals accordingly. We apply the proposed method to the Stanford HIV-1 drug resistance study and compare the results with existing approaches. We also conduct comprehensive simulation studies to demonstrate the superior performance of our proposed method.},
  archive      = {J_BIOMTC},
  author       = {Fang Fang and Jiwei Zhao and S. Ejaz Ahmed and Annie Qu},
  doi          = {10.1111/biom.13346},
  journal      = {Biometrics},
  number       = {3},
  pages        = {996-1010},
  shortjournal = {Biometrics},
  title        = {A weak-signal-assisted procedure for variable selection and statistical inference with an informative subsample},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compositional knockoff filter for high-dimensional
regression analysis of microbiome data. <em>BIOMTC</em>, <em>77</em>(3),
984–995. (<a href="https://doi.org/10.1111/biom.13336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical task in microbiome data analysis is to explore the association between a scalar response of interest and a large number of microbial taxa that are summarized as compositional data at different taxonomic levels. Motivated by fine-mapping of the microbiome, we propose a two-step compositional knockoff filter to provide the effective finite-sample false discovery rate (FDR) control in high-dimensional linear log-contrast regression analysis of microbiome compositional data. In the first step, we propose a new compositional screening procedure to remove insignificant microbial taxa while retaining the essential sum-to-zero constraint. In the second step, we extend the knockoff filter to identify the significant microbial taxa in the sparse regression model for compositional data. Thereby, a subset of the microbes is selected from the high-dimensional microbial taxa as related to the response under a prespecified FDR threshold. We study the theoretical properties of the proposed two-step procedure, including both sure screening and effective false discovery control. We demonstrate these properties in numerical simulation studies to compare our methods to some existing ones and show power gain of the new method while controlling the nominal FDR. The potential usefulness of the proposed method is also illustrated with application to an inflammatory bowel disease data set to identify microbial taxa that influence host gene expressions.},
  archive      = {J_BIOMTC},
  author       = {Arun Srinivasan and Lingzhou Xue and Xiang Zhan},
  doi          = {10.1111/biom.13336},
  journal      = {Biometrics},
  number       = {3},
  pages        = {984-995},
  shortjournal = {Biometrics},
  title        = {Compositional knockoff filter for high-dimensional regression analysis of microbiome data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of conditional power for cluster-randomized
trials with interval-censored endpoints. <em>BIOMTC</em>,
<em>77</em>(3), 970–983. (<a
href="https://doi.org/10.1111/biom.13360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster-randomized trials (CRTs) of infectious disease preventions often yield correlated, interval-censored data: dependencies may exist between observations from the same cluster, and event occurrence may be assessed only at intermittent study visits. This data structure must be accounted for when conducting interim monitoring and futility assessment for CRTs. In this article, we propose a flexible framework for conditional power estimation when outcomes are correlated and interval-censored. Under the assumption that the survival times follow a shared frailty model, we first characterize the correspondence between the marginal and cluster-conditional survival functions, and then use this relationship to semiparametrically estimate the cluster-specific survival distributions from the available interim data. We incorporate assumptions about changes to the event process over the remainder of the trial—as well as estimates of the dependency among observations in the same cluster—to extend these survival curves through the end of the study. Based on these projected survival functions, we generate correlated interval-censored observations, and then calculate the conditional power as the proportion of times (across multiple full-data generation steps) that the null hypothesis of no treatment effect is rejected. We evaluate the performance of the proposed method through extensive simulation studies, and illustrate its use on a large cluster-randomized HIV prevention trial.},
  archive      = {J_BIOMTC},
  author       = {Kaitlyn Cook and Rui Wang},
  doi          = {10.1111/biom.13360},
  journal      = {Biometrics},
  number       = {3},
  pages        = {970-983},
  shortjournal = {Biometrics},
  title        = {Estimation of conditional power for cluster-randomized trials with interval-censored endpoints},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of noisy survival data with graphical proportional
hazards measurement error models. <em>BIOMTC</em>, <em>77</em>(3),
956–969. (<a href="https://doi.org/10.1111/biom.13331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen and Grace Y. Yi},
  doi          = {10.1111/biom.13331},
  journal      = {Biometrics},
  number       = {3},
  pages        = {956-969},
  shortjournal = {Biometrics},
  title        = {Analysis of noisy survival data with graphical proportional hazards measurement error models},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tailored optimal posttreatment surveillance for cancer
recurrence. <em>BIOMTC</em>, <em>77</em>(3), 942–955. (<a
href="https://doi.org/10.1111/biom.13341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A substantial rise in the number of cancer survivors has led to urgent management questions regarding effective posttreatment imaging-based surveillance strategies for cancer recurrence. Current surveillance guidelines provided by a number of professional societies all warn against overly aggressive surveillance, especially for low-risk patients, but all fail to provide more specific directions to accommodate underlying heterogeneity of cancer recurrence. Therefore it is imperative to develop data-driven strategies that can tailor the surveillance schedules to recurrence risk in this era of stricter insurance regulations, provider shortages, and rising costs of health care. Due to a lack of statistical methods for optimizing surveillance scheduling in presence of competing risks, we propose a general approach that uses an intuitive loss function for optimization of early detection of recurrence before death. The proposed strategies can tailor to patient risks of recurrence, in terms of both intensity and amount of surveillance. Using general three-state Markov models, our method is flexible and includes earlier works as special cases. We illustrate our method in both simulation studies and an application to breast cancer surveillance.},
  archive      = {J_BIOMTC},
  author       = {Rui Chen and Menggang Yu},
  doi          = {10.1111/biom.13341},
  journal      = {Biometrics},
  number       = {3},
  pages        = {942-955},
  shortjournal = {Biometrics},
  title        = {Tailored optimal posttreatment surveillance for cancer recurrence},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of incubation period and generation time based on
observed length-biased epidemic cohort with censoring for COVID-19
outbreak in china. <em>BIOMTC</em>, <em>77</em>(3), 929–941. (<a
href="https://doi.org/10.1111/biom.13325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incubation period and generation time are key characteristics in the analysis of infectious diseases. The commonly used contact-tracing–based estimation of incubation distribution is highly influenced by the individuals&#39; judgment on the possible date of exposure, and might lead to significant errors. On the other hand, interval censoring–based methods are able to utilize a much larger set of traveling data but may encounter biased sampling problems. The distribution of generation time is usually approximated by observed serial intervals. However, it may result in a biased estimation of generation time, especially when the disease is infectious during incubation. In this paper, the theory from renewal process is partially adopted by considering the incubation period as the interarrival time, and the duration between departure from Wuhan and onset of symptoms as the mixture of forward time and interarrival time with censored intervals. In addition, a consistent estimator for the distribution of generation time based on incubation period and serial interval is proposed for incubation-infectious diseases. A real case application to the current outbreak of COVID-19 is implemented. We find that the incubation period has a median of 8.50 days (95\% confidence interval [CI] [7.22; 9.15]). The basic reproduction number in the early phase of COVID-19 outbreak based on the proposed generation time estimation is estimated to be 2.96 (95\% CI [2.15; 3.86]).},
  archive      = {J_BIOMTC},
  author       = {Yuhao Deng and Chong You and Yukun Liu and Jing Qin and Xiao-Hua Zhou},
  doi          = {10.1111/biom.13325},
  journal      = {Biometrics},
  number       = {3},
  pages        = {929-941},
  shortjournal = {Biometrics},
  title        = {Estimation of incubation period and generation time based on observed length-biased epidemic cohort with censoring for COVID-19 outbreak in china},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Poststratification fusion learning in longitudinal data
analysis. <em>BIOMTC</em>, <em>77</em>(3), 914–928. (<a
href="https://doi.org/10.1111/biom.13333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stratification is a very commonly used approach in biomedical studies to handle sample heterogeneity arising from, for examples, clinical units, patient subgroups, or missing-data. A key rationale behind such approach is to overcome potential sampling biases in statistical inference. Two issues of such stratification-based strategy are (i) whether individual strata are sufficiently distinctive to warrant stratification, and (ii) sample size attrition resulted from the stratification may potentially lead to loss of statistical power. To address these issues, we propose a penalized generalized estimating equations approach to reducing the complexity of parametric model structures due to excessive stratification. Specifically, we develop a data-driven fusion learning approach for longitudinal data that improves estimation efficiency by integrating information across similar strata, yet still allows necessary separation for stratum-specific conclusions. The proposed method is evaluated by simulation studies and applied to a motivating example of psychiatric study to demonstrate its usefulness in real world settings.},
  archive      = {J_BIOMTC},
  author       = {Lu Tang and Peter X.-K. Song},
  doi          = {10.1111/biom.13333},
  journal      = {Biometrics},
  number       = {3},
  pages        = {914-928},
  shortjournal = {Biometrics},
  title        = {Poststratification fusion learning in longitudinal data analysis},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ultra high-dimensional semiparametric longitudinal data
analysis. <em>BIOMTC</em>, <em>77</em>(3), 903–913. (<a
href="https://doi.org/10.1111/biom.13348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As ultra high-dimensional longitudinal data are becoming ever more apparent in fields such as public health and bioinformatics, developing flexible methods with a sparse model is of high interest. In this setting, the dimension of the covariates can potentially grow exponentially as exp ( n 1 / 2 ) with respect to the number of clusters n . We consider a flexible semiparametric approach, namely, partially linear single-index models, for ultra high-dimensional longitudinal data. Most importantly, we allow not only the partially linear covariates but also the single-index covariates within the unknown flexible function estimated nonparametrically to be ultra high dimensional. Using penalized generalized estimating equations, this approach can capture correlation within subjects, can perform simultaneous variable selection and estimation with a smoothly clipped absolute deviation penalty, and can capture nonlinearity and potentially some interactions among predictors. We establish asymptotic theory for the estimators including the oracle property in ultra high dimension for both the partially linear and nonparametric components, and we present an efficient algorithm to handle the computational challenges. We show the effectiveness of our method and algorithm via a simulation study and a yeast cell cycle gene expression data.},
  archive      = {J_BIOMTC},
  author       = {Brittany Green and Heng Lian and Yan Yu and Tianhai Zu},
  doi          = {10.1111/biom.13348},
  journal      = {Biometrics},
  number       = {3},
  pages        = {903-913},
  shortjournal = {Biometrics},
  title        = {Ultra high-dimensional semiparametric longitudinal data analysis},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularized matrix data clustering and its application to
image analysis. <em>BIOMTC</em>, <em>77</em>(3), 890–902. (<a
href="https://doi.org/10.1111/biom.13354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel regularized mixture model for clustering matrix-valued data. The proposed method assumes a separable covariance structure for each cluster and imposes a sparsity structure (eg, low rankness, spatial sparsity) for the mean signal of each cluster. We formulate the problem as a finite mixture model of matrix-normal distributions with regularization terms, and then develop an expectation maximization type of algorithm for efficient computation. In theory, we show that the proposed estimators are strongly consistent for various choices of penalty functions. Simulation and two applications on brain signal studies confirm the excellent performance of the proposed method including a better prediction accuracy than the competitors and the scientific interpretability of the solution.},
  archive      = {J_BIOMTC},
  author       = {Xu Gao and Weining Shen and Liwen Zhang and Jianhua Hu and Norbert J. Fortin and Ron D. Frostig and Hernando Ombao},
  doi          = {10.1111/biom.13354},
  journal      = {Biometrics},
  number       = {3},
  pages        = {890-902},
  shortjournal = {Biometrics},
  title        = {Regularized matrix data clustering and its application to image analysis},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal neuroimaging data integration and pathway
analysis. <em>BIOMTC</em>, <em>77</em>(3), 879–889. (<a
href="https://doi.org/10.1111/biom.13351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in technology, the collection of multiple types of measurements on a common set of subjects is becoming routine in science. Some notable examples include multimodal neuroimaging studies for the simultaneous investigation of brain structure and function and multi-omics studies for combining genetic and genomic information. Integrative analysis of multimodal data allows scientists to interrogate new mechanistic questions. However, the data collection and generation of integrative hypotheses is outpacing available methodology for joint analysis of multimodal measurements. In this article, we study high-dimensional multimodal data integration in the context of mediation analysis. We aim to understand the roles that different data modalities play as possible mediators in the pathway between an exposure variable and an outcome. We propose a mediation model framework with two data types serving as separate sets of mediators and develop a penalized optimization approach for parameter estimation. We study both the theoretical properties of the estimator through an asymptotic analysis and its finite-sample performance through simulations. We illustrate our method with a multimodal brain pathway analysis having both structural and functional connectivity as mediators in the association between sex and language processing.},
  archive      = {J_BIOMTC},
  author       = {Yi Zhao and Lexin Li and Brian S. Caffo},
  doi          = {10.1111/biom.13351},
  journal      = {Biometrics},
  number       = {3},
  pages        = {879-889},
  shortjournal = {Biometrics},
  title        = {Multimodal neuroimaging data integration and pathway analysis},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric trend estimation in functional time series
with application to annual mortality rates. <em>BIOMTC</em>,
<em>77</em>(3), 866–878. (<a
href="https://doi.org/10.1111/biom.13353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of trend estimation for functional time series. Existing contributions either deal with detecting a functional trend or assuming a simple model. They consider neither the estimation of a general functional trend nor the analysis of functional time series with a functional trend component. Similarly to univariate time series, we propose an alternative methodology to analyze functional time series, taking into account a functional trend component. We propose to estimate the functional trend by using a tensor product surface that is easy to implement, to interpret, and allows to control the smoothness properties of the estimator. Through a Monte Carlo study, we simulate different scenarios of functional processes to show that our estimator accurately identifies the functional trend component. We also show that the dependency structure of the estimated stationary time series component is not significantly affected by the error approximation of the functional trend component. We apply our methodology to annual mortality rates in France.},
  archive      = {J_BIOMTC},
  author       = {Israel Martínez-Hernández and Marc G. Genton},
  doi          = {10.1111/biom.13353},
  journal      = {Biometrics},
  number       = {3},
  pages        = {866-878},
  shortjournal = {Biometrics},
  title        = {Nonparametric trend estimation in functional time series with application to annual mortality rates},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cluster non-gaussian functional data. <em>BIOMTC</em>,
<em>77</em>(3), 852–865. (<a
href="https://doi.org/10.1111/biom.13349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian distributions have been commonly assumed when clustering functional data. When the normality condition fails, biased results will follow. Additional challenges occur as the number of the clusters is often unknown a priori . This paper focuses on clustering non-Gaussian functional data without the prior information of the number of clusters. We introduce a semiparametric mixed normal transformation model to accommodate non-Gaussian functional data, and propose a penalized approach to simultaneously estimate the parameters, transformation function, and the number of clusters. The estimators are shown to be consistent and asymptotically normal. The practical utility of the methods is confirmed via simulations as well as an application of the analysis of Alzheimer&#39;s disease study. The proposed method yields much less classification error than the existing methods. Data used in preparation of this paper were obtained from the Alzheimer&#39;s Disease Neuroimaging Initiative database.},
  archive      = {J_BIOMTC},
  author       = {Qingzhi Zhong and Huazhen Lin and Yi Li},
  doi          = {10.1111/biom.13349},
  journal      = {Biometrics},
  number       = {3},
  pages        = {852-865},
  shortjournal = {Biometrics},
  title        = {Cluster non-gaussian functional data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-component registration for multivariate functional
data, with application to growth curves. <em>BIOMTC</em>,
<em>77</em>(3), 839–851. (<a
href="https://doi.org/10.1111/biom.13340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate functional data are becoming ubiquitous with advances in modern technology and are substantially more complex than univariate functional data. We propose and study a novel model for multivariate functional data where the component processes are subject to mutual time warping. That is, the component processes exhibit a similar shape but are subject to systematic phase variation across their time domains. To address this previously unconsidered mode of warping, we propose new registration methodology that is based on a shift-warping model. Our method differs from all existing registration methods for functional data in a fundamental way. Namely, instead of focusing on the traditional approach to warping, where one aims to recover individual-specific registration, we focus on shift registration across the components of a multivariate functional data vector on a population-wide level. Our proposed estimates for these shifts are identifiable, enjoy parametric rates of convergence, and often have intuitive physical interpretations, all in contrast to traditional curve-specific registration approaches. We demonstrate the implementation and interpretation of the proposed method by applying our methodology to the Zürich Longitudinal Growth data and study its finite sample properties in simulations.},
  archive      = {J_BIOMTC},
  author       = {Cody Carroll and Hans-Georg Müller and Alois Kneip},
  doi          = {10.1111/biom.13340},
  journal      = {Biometrics},
  number       = {3},
  pages        = {839-851},
  shortjournal = {Biometrics},
  title        = {Cross-component registration for multivariate functional data, with application to growth curves},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian compositional regression with structured priors for
microbiome feature selection. <em>BIOMTC</em>, <em>77</em>(3), 824–838.
(<a href="https://doi.org/10.1111/biom.13335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The microbiome plays a critical role in human health and disease, and there is a strong scientific interest in linking specific features of the microbiome to clinical outcomes. There are key aspects of microbiome data, however, that limit the applicability of standard variable selection methods. In particular, the observed data are compositional, as the counts within each sample have a fixed-sum constraint. In addition, microbiome features, typically quantified as operational taxonomic units, often reflect microorganisms that are similar in function, and may therefore have a similar influence on the response variable. To address the challenges posed by these aspects of the data structure, we propose a variable selection technique with the following novel features: a generalized transformation and z -prior to handle the compositional constraint, and an Ising prior that encourages the joint selection of microbiome features that are closely related in terms of their genetic sequence similarity. We demonstrate that our proposed method outperforms existing penalized approaches for microbiome variable selection in both simulation and the analysis of real data exploring the relationship of the gut microbiome to body mass index.},
  archive      = {J_BIOMTC},
  author       = {Liangliang Zhang and Yushu Shi and Robert R. Jenq and Kim-Anh Do and Christine B. Peterson},
  doi          = {10.1111/biom.13335},
  journal      = {Biometrics},
  number       = {3},
  pages        = {824-838},
  shortjournal = {Biometrics},
  title        = {Bayesian compositional regression with structured priors for microbiome feature selection},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian variable selection for non-gaussian responses: A
marginally calibrated copula approach. <em>BIOMTC</em>, <em>77</em>(3),
809–823. (<a href="https://doi.org/10.1111/biom.13355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new highly flexible and tractable Bayesian approach to undertake variable selection in non-Gaussian regression models. It uses a copula decomposition for the joint distribution of observations on the dependent variable. This allows the marginal distribution of the dependent variable to be calibrated accurately using a nonparametric or other estimator. The family of copulas employed are “implicit copulas” that are constructed from existing hierarchical Bayesian models widely used for variable selection, and we establish some of their properties. Even though the copulas are high dimensional, they can be estimated efficiently and quickly using Markov chain Monte Carlo. A simulation study shows that when the responses are non-Gaussian, the approach selects variables more accurately than contemporary benchmarks. A real data example in the Web Appendix illustrates that accounting for even mild deviations from normality can lead to a substantial increase in accuracy. To illustrate the full potential of our approach, we extend it to spatial variable selection for fMRI. Using real data, we show our method allows for voxel-specific marginal calibration of the magnetic resonance signal at over 6000 voxels, leading to an increase in the quality of the activation maps.},
  archive      = {J_BIOMTC},
  author       = {Nadja Klein and Michael Stanley Smith},
  doi          = {10.1111/biom.13355},
  journal      = {Biometrics},
  number       = {3},
  pages        = {809-823},
  shortjournal = {Biometrics},
  title        = {Bayesian variable selection for non-gaussian responses: A marginally calibrated copula approach},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian adaptive phase i/II clinical trial design with
late-onset competing risk outcomes. <em>BIOMTC</em>, <em>77</em>(3),
796–808. (<a href="https://doi.org/10.1111/biom.13347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early-phase dose-finding clinical trials are often subject to the issue of late-onset outcomes. In phase I/II clinical trials, the issue becomes more intractable because toxicity and efficacy can be competing risk outcomes such that the occurrence of the first outcome will terminate the other one. In this paper, we propose a novel Bayesian adaptive phase I/II clinical trial design to address the issue of late-onset competing risk outcomes. We use the continuation-ratio model to characterize the trinomial response outcomes and the cause-specific hazard rate method to model the competing-risk survival outcomes. We treat the late-onset outcomes as missing data and develop a Bayesian data augmentation method to impute the missing data from the observations. We also propose an adaptive dose-finding algorithm to allocate patients and identify the optimal biological dose during the trial. Simulation studies show that the proposed design yields desirable operating characteristics.},
  archive      = {J_BIOMTC},
  author       = {Yifei Zhang and Sha Cao and Chi Zhang and Ick Hoon Jin and Yong Zang},
  doi          = {10.1111/biom.13347},
  journal      = {Biometrics},
  number       = {3},
  pages        = {796-808},
  shortjournal = {Biometrics},
  title        = {A bayesian adaptive phase I/II clinical trial design with late-onset competing risk outcomes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate bayesian inference for case-crossover models.
<em>BIOMTC</em>, <em>77</em>(3), 785–795. (<a
href="https://doi.org/10.1111/biom.13329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A case-crossover analysis is used as a simple but powerful tool for estimating the effect of short-term environmental factors such as extreme temperatures or poor air quality on mortality. The environment on the day of each death is compared to the one or more “control days” in previous weeks, and higher levels of exposure on death days than control days provide evidence of an effect. Current state-of-the-art methodology and software (integrated nested Laplace approximation [INLA]) cannot be used to fit the most flexible case-crossover models to large datasets, because the likelihood for case-crossover models cannot be expressed in a manner compatible with this methodology. In this paper, we develop a flexible and scalable modeling framework for case-crossover models with linear and semiparametric effects which retains the flexibility and computational advantages of INLA. We apply our method to quantify nonlinear associations between mortality and extreme temperatures in India. An R package implementing our methods is publicly available.},
  archive      = {J_BIOMTC},
  author       = {Alex Stringer and Patrick Brown and Jamie Stafford},
  doi          = {10.1111/biom.13329},
  journal      = {Biometrics},
  number       = {3},
  pages        = {785-795},
  shortjournal = {Biometrics},
  title        = {Approximate bayesian inference for case-crossover models},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction to “a cluster-adjusted rank-based test for a
clinical trial concerning multiple endpoints with application to dietary
intervention assessment,” by zhang, w., liu, a., tang, l.l. And li, q;
75(3), 821–830, 2019. <em>BIOMTC</em>, <em>77</em>(2), 780. (<a
href="https://doi.org/10.1111/biom.13497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13497},
  journal      = {Biometrics},
  number       = {2},
  pages        = {780},
  shortjournal = {Biometrics},
  title        = {Correction to “A cluster-adjusted rank-based test for a clinical trial concerning multiple endpoints with application to dietary intervention assessment,” by zhang, w., liu, a., tang, L.L. and li, q; 75(3), 821–830, 2019},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction to “empirical-likelihood-based criteria for model
selection on marginal analysis of longitudinal data with dropout
missingness,” by chen, c., shen, b., zhang, l., xue, y. And wang, m.;
75(3), 950–965, 2019. <em>BIOMTC</em>, <em>77</em>(2), 779. (<a
href="https://doi.org/10.1111/biom.13496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13496},
  journal      = {Biometrics},
  number       = {2},
  pages        = {779},
  shortjournal = {Biometrics},
  title        = {Correction to “Empirical-likelihood-based criteria for model selection on marginal analysis of longitudinal data with dropout missingness,” by chen, c., shen, b., zhang, l., xue, y. and wang, m.; 75(3), 950–965, 2019},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical methods for survival trial design—with
applications to cancer clinical trials using r by jianrong wu, CRC
press, 2018, ISBN 9780367734329. <em>BIOMTC</em>, <em>77</em>(2),
777–778. (<a href="https://doi.org/10.1111/biom.13485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chen Hu},
  doi          = {10.1111/biom.13485},
  journal      = {Biometrics},
  number       = {2},
  pages        = {777-778},
  shortjournal = {Biometrics},
  title        = {Statistical methods for survival trial design—With applications to cancer clinical trials using r by jianrong wu, CRC press, 2018, ISBN 9780367734329},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive web-based data visualization with r, plotly, and
shiny (carson sievert). <em>BIOMTC</em>, <em>77</em>(2), 776–777. (<a
href="https://doi.org/10.1111/biom.13474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Ran Li and Usama Bilal},
  doi          = {10.1111/biom.13474},
  journal      = {Biometrics},
  number       = {2},
  pages        = {776-777},
  shortjournal = {Biometrics},
  title        = {Interactive web-based data visualization with r, plotly, and shiny (Carson sievert)},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the burden of the opioid epidemic for adults and
adolescents in ohio counties. <em>BIOMTC</em>, <em>77</em>(2), 765–775.
(<a href="https://doi.org/10.1111/biom.13295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying the opioid epidemic at the local level is a challenging problem that has important consequences on resource allocation. Adults and adolescents may exhibit different spatial trends and require different interventions and resources so it is important to examine the problem for each age group. In Ohio, surveillance data are collected at the county level for each age group on measurable outcomes of the opioid epidemic, overdose deaths, and treatment admissions. However, our interest lies in quantifying the unmeasurable construct, representing the burden of the opioid epidemic, which drives rates of the outcomes. We propose jointly modeling adult and adolescent surveillance outcomes through a multivariate spatial factor model. A generalized spatial factor model within each age group quantifies a latent factor related to the number of opioid-associated treatment admissions and deaths. By assuming a multivariate conditional autoregressive model for the spatial factors of adults and adolescents, we allow the adolescent model to borrow strength from the adult model (and vice versa), improving estimation. We also incorporate county-level covariates to help explain spatial heterogeneity in each of the factors. We apply this approach to the state of Ohio and discuss the findings. Our framework provides a coherent approach for synthesizing information across multiple outcomes and age groups to better understand the spatial epidemiology of the opioid epidemic.},
  archive      = {J_BIOMTC},
  author       = {David Kline and Staci A. Hepler},
  doi          = {10.1111/biom.13295},
  journal      = {Biometrics},
  number       = {2},
  pages        = {765-775},
  shortjournal = {Biometrics},
  title        = {Estimating the burden of the opioid epidemic for adults and adolescents in ohio counties},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible link functions in a joint hierarchical gaussian
process model. <em>BIOMTC</em>, <em>77</em>(2), 754–764. (<a
href="https://doi.org/10.1111/biom.13291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many longitudinal studies often require jointly modeling a biomarker and an event outcome, in order to provide more accurate inference and dynamic prediction of disease progression. Cystic fibrosis (CF) studies have illustrated the benefits of these models, primarily examining the joint evolution of lung-function decline and survival. We propose a novel joint model within the shared-parameter framework that accommodates nonlinear lung-function trajectories, in order to provide more accurate inference on lung-function decline over time and to examine the association between evolution of lung function and risk of a pulmonary exacerbation (PE) event recurrence. Specifically, a two-level Gaussian process (GP) is used to estimate the nonlinear longitudinal trajectories and a flexible link function is introduced for a more accurate depiction of the binary process on the event outcome. Bayesian model assessment is used to evaluate each component of the joint model in simulation studies and an application to longitudinal data on patients receiving care from a CF center. A nonlinear structure is suggested by both longitudinal continuous and binary evaluations. Including a flexible link function improves model fit to these data. The proposed hierarchical GP model with a flexible power link function where Laplace distribution is the baseline (spep) has the best fit of all joint models considered, characterizing how accelerated lung-function decline corresponds to increased odds of experiencing another PE.},
  archive      = {J_BIOMTC},
  author       = {Weiji Su and Xia Wang and Rhonda D. Szczesniak},
  doi          = {10.1111/biom.13291},
  journal      = {Biometrics},
  number       = {2},
  pages        = {754-764},
  shortjournal = {Biometrics},
  title        = {Flexible link functions in a joint hierarchical gaussian process model},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parametric g-formula implementations for causal survival
analyses. <em>BIOMTC</em>, <em>77</em>(2), 740–753. (<a
href="https://doi.org/10.1111/biom.13321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The g-formula can be used to estimate the survival curve under a sustained treatment strategy. Two available estimators of the g-formula are noniterative conditional expectation and iterative conditional expectation. We propose a version of the iterative conditional expectation estimator and describe its procedures for deterministic and random treatment strategies. Also, because little is known about the comparative performance of noniterative and iterative conditional expectation estimators, we explore their relative efficiency via simulation studies. Our simulations show that, in the absence of model misspecification and unmeasured confounding, our proposed iterative conditional expectation estimator and the noniterative conditional expectation estimator are similarly efficient, and that both are at least as efficient as the classical iterative conditional expectation estimator. We describe an application of both noniterative and iterative conditional expectation to answer “when to start” treatment questions using data from the HIV-CAUSAL Collaboration.},
  archive      = {J_BIOMTC},
  author       = {Lan Wen and Jessica G. Young and James M. Robins and Miguel A. Hernán},
  doi          = {10.1111/biom.13321},
  journal      = {Biometrics},
  number       = {2},
  pages        = {740-753},
  shortjournal = {Biometrics},
  title        = {Parametric g-formula implementations for causal survival analyses},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the optimal timing of surgery from observational
data. <em>BIOMTC</em>, <em>77</em>(2), 729–739. (<a
href="https://doi.org/10.1111/biom.13311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infants with hypoplastic left heart syndrome require an initial Norwood operation, followed some months later by a stage 2 palliation (S2P). The timing of S2P is critical for the operation&#39;s success and the infant&#39;s survival, but the optimal timing, if one exists, is unknown. We attempt to identify the optimal timing of S2P by analyzing data from the Single Ventricle Reconstruction Trial (SVRT), which randomized patients between two different types of Norwood procedure. In the SVRT, the timing of the S2P was chosen by the medical team; thus with respect to this exposure, the trial constitutes an observational study, and the analysis must adjust for potential confounding. To accomplish this, we propose an extended propensity score analysis that describes the time to surgery as a function of confounders in a discrete competing-risk model. We then apply inverse probability weighting to estimate a spline hazard model for predicting survival from the time of S2P. Our analysis suggests that S2P conducted at 6 months after the Norwood gives the patient the best post-S2P survival. Thus, we place the optimal time slightly later than a previous analysis in the medical literature that did not account for competing risks of death and heart transplantation.},
  archive      = {J_BIOMTC},
  author       = {Xiaofei Chen and Daniel F. Heitjan and Gerald Greil and Haekyung Jeon-Slaughter},
  doi          = {10.1111/biom.13311},
  journal      = {Biometrics},
  number       = {2},
  pages        = {729-739},
  shortjournal = {Biometrics},
  title        = {Estimating the optimal timing of surgery from observational data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Horvitz-thompson–like estimation with distance-based
detection probabilities for circular plot sampling of forests.
<em>BIOMTC</em>, <em>77</em>(2), 715–728. (<a
href="https://doi.org/10.1111/biom.13312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In circular plot sampling, trees within a given distance from the sample plot location constitute a sample, which is used to infer characteristics of interest for the forest area. If the sample is collected using a technical device located at the sampling point, eg, a terrestrial laser scanner, all trees of the sample plot cannot be observed because they hide behind each other. We propose a Horvitz-Thompson–like estimator with distance-based detection probabilities derived from stochastic geometry for estimation of population totals such as stem density and basal area in such situation. We show that our estimator is unbiased for Poisson forests and give estimates of variance and approximate confidence intervals for the estimator, unlike any previous methods. We compare the estimator to two previously published benchmark methods. The comparison is done through a simulation study where several plots are simulated either from field measured data or different marked point processes. The simulations show that the estimator produces lower or comparable error values than the other methods. In the sample plots based on the field measured data, the bias is relatively small—relative mean of errors for stem density, for example, varying from 0.3\% to 2.2\%, depending on the detection condition. The empirical coverage probabilities of the approximate confidence intervals are either similar to the nominal levels or conservative in these sample plots.},
  archive      = {J_BIOMTC},
  author       = {Kasper Kansanen and Petteri Packalen and Matti Maltamo and Lauri Mehtätalo},
  doi          = {10.1111/biom.13312},
  journal      = {Biometrics},
  number       = {2},
  pages        = {715-728},
  shortjournal = {Biometrics},
  title        = {Horvitz-thompson–like estimation with distance-based detection probabilities for circular plot sampling of forests},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensitivity analysis for subsequent treatments in
confirmatory oncology clinical trials: A two-stage stochastic dynamic
treatment regime approach. <em>BIOMTC</em>, <em>77</em>(2), 702–714. (<a
href="https://doi.org/10.1111/biom.13296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsequent treatments can result in a difficulty in interpretation of the overall survival results in confirmatory oncology clinical trials. To complement the intention-to-treat (ITT) analysis affected by subsequent treatment patterns unintentional in the trial protocol, several causal methods targeting the per-protocol effect have been proposed. When two or more types of subsequent treatments are allowed in the trial protocol, however, these methods cannot answer clinical questions such as how sensitive the ITT analysis result is to higher or lower proportions of each subsequent treatment allowed in the trial protocol than observed, and to what extent ITT analysis result is generalizable to subsequent treatment patterns other than observed one. To answer these clinical questions, we propose a sensitivity analysis method for subsequent treatments using the inverse probability of treatment weighting method for stochastic dynamic treatment regimes (DTRs). We formulate oncology clinical trials with subsequent treatments as two-stage designs in which initial treatments are randomized, but subsequent treatments are observational. In this formulation, we use stochastic DTRs to simulate specific proportions of each subsequent treatment and compare an initial experimental treatment with an initial control treatment under various proportions of each subsequent treatment. We applied our proposed method to a motivating randomized noninferiority trial for metastatic breast cancer. Simulation results are also reported to show the usefulness of the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Yasuhiro Hagiwara and Tomohiro Shinozaki and Hirofumi Mukai and Yutaka Matsuyama},
  doi          = {10.1111/biom.13296},
  journal      = {Biometrics},
  number       = {2},
  pages        = {702-714},
  shortjournal = {Biometrics},
  title        = {Sensitivity analysis for subsequent treatments in confirmatory oncology clinical trials: A two-stage stochastic dynamic treatment regime approach},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent ornstein-uhlenbeck models for bayesian analysis of
multivariate longitudinal categorical responses. <em>BIOMTC</em>,
<em>77</em>(2), 689–701. (<a
href="https://doi.org/10.1111/biom.13292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian latent Ornstein-Uhlenbeck (OU) model to analyze unbalanced longitudinal data of binary and ordinal variables, which are manifestations of fewer continuous latent variables. We focus on the evolution of such latent variables when they continuously change over time. Existing approaches are limited to data collected at regular time intervals. Our proposal makes use of an OU process for the latent variables to overcome this limitation. We show that assuming real eigenvalues for the drift matrix of the OU process, as is frequently done in practice, can lead to biased estimates and/or misleading inference when the true process is oscillating. In contrast, our proposal allows for both real and complex eigenvalues. We illustrate our proposed model with a motivating dataset, containing patients with amyotrophic lateral sclerosis disease. We were interested in how bulbar, cervical, and lumbar functions evolve over time.},
  archive      = {J_BIOMTC},
  author       = {Trung Dung Tran and Emmanuel Lesaffre and Geert Verbeke and Joke Duyck},
  doi          = {10.1111/biom.13292},
  journal      = {Biometrics},
  number       = {2},
  pages        = {689-701},
  shortjournal = {Biometrics},
  title        = {Latent ornstein-uhlenbeck models for bayesian analysis of multivariate longitudinal categorical responses},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian multivariate mixture model for skewed
longitudinal data with intermittent missing observations: An application
to infant motor development. <em>BIOMTC</em>, <em>77</em>(2), 675–688.
(<a href="https://doi.org/10.1111/biom.13328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies of infant growth, an important research goal is to identify latent clusters of infants with delayed motor development—a risk factor for adverse outcomes later in life. However, there are numerous statistical challenges in modeling motor development: the data are typically skewed, exhibit intermittent missingness, and are correlated across repeated measurements over time. Using data from the Nurture study, a cohort of approximately 600 mother-infant pairs, we develop a flexible Bayesian mixture model for the analysis of infant motor development. First, we model developmental trajectories using matrix skew-normal distributions with cluster-specific parameters to accommodate dependence and skewness in the data. Second, we model the cluster-membership probabilities using a Pólya-Gamma data-augmentation scheme, which improves predictions of the cluster-membership allocations. Lastly, we impute missing responses from conditional multivariate skew-normal distributions. Bayesian inference is achieved through straightforward Gibbs sampling. Through simulation studies, we show that the proposed model yields improved inferences over models that ignore skewness or adopt conventional imputation methods. We applied the model to the Nurture data and identified two distinct developmental clusters, as well as detrimental effects of food insecurity on motor development. These findings can aid investigators in targeting interventions during this critical early-life developmental window.},
  archive      = {J_BIOMTC},
  author       = {Carter Allen and Sara E. Benjamin-Neelon and Brian Neelon},
  doi          = {10.1111/biom.13328},
  journal      = {Biometrics},
  number       = {2},
  pages        = {675-688},
  shortjournal = {Biometrics},
  title        = {A bayesian multivariate mixture model for skewed longitudinal data with intermittent missing observations: An application to infant motor development},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Batch bayesian optimization design for optimizing a
neurostimulator. <em>BIOMTC</em>, <em>77</em>(2), 661–674. (<a
href="https://doi.org/10.1111/biom.13313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, spinal epidural neurostimulation is being considered for rehabilitation of persons suffering from partial spinal-cord injury. The neurostimulator must be programmed by a neurosurgeon, yet little work has been done to develop rigorous methods for optimally programming the device. We propose an adaptive design to efficiently optimize programming of the neurostimulator based on specified interim evaluations of patient reported preferences. Preferences for the eligible device configurations are estimated after each interim analysis through a conditionally autoregressive model that assumes preference for one configuration is related to preferences for neighboring configurations. Using the adaptively updated preferences, a group of configurations is programmed into the device for the patient to evaluate during the next follow-up period. This selection is based on a balance of device exploration and preference maximization. We repeat this process until a specified stopping rule or the calibration end is reached. We show simulation studies to evaluate the overall quality of the adaptive calibration for various configuration selection strategies and the effects of stopping it early.},
  archive      = {J_BIOMTC},
  author       = {Adam Kaplan and Thomas A. Murray},
  doi          = {10.1111/biom.13313},
  journal      = {Biometrics},
  number       = {2},
  pages        = {661-674},
  shortjournal = {Biometrics},
  title        = {Batch bayesian optimization design for optimizing a neurostimulator},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian hierarchical model for characterizing the
diffusion of new antipsychotic drugs. <em>BIOMTC</em>, <em>77</em>(2),
649–660. (<a href="https://doi.org/10.1111/biom.13324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New prescription medications are a primary driver of spending growth in the United States. For patients with severe mental illnesses, second-generation antipsychotic (SGA) medications feature prominently. However, many SGAs are costly, particularly before generic entry, and some may increase the risk of diabetes. Because physicians play a prominent role in new prescription adoption, understanding their prescribing behaviors is policy-relevant. Several features of prescription data, such as different antipsychotic choice sets over time, variable physician prescription volumes, and correlation among drug choices within physicians, complicate inferences. We propose a multivariate Bayesian hierarchical model with piecewise random effects to characterize the diffusion of new antipsychotic drugs. This model captures the complex prescriber-specific relationships among the different diffusion processes and takes advantage of the Bayesian paradigm to quantify uncertainty for all parameters straightforwardly. To evaluate the prescribing patterns for each physician, we propose various indices to identify early new SGA adopters. A sample of nearly 17,000 US physicians whose antipsychotic drug prescribing information was collected between January 1, 1997 and December 31, 2007 illustrates the methods. Determinants of high prescription rates and adoption speeds of new SGAs included physician sex, age, hospital affiliation, physician specialty, and office location. Large within- and between-provider variations in prescribing patterns of new SGAs were identified. Early adopters for one drug were not early adopters for another drug.},
  archive      = {J_BIOMTC},
  author       = {Chenyang Gu and Haiden Huskamp and Julie Donohue and Sharon-Lise Normand},
  doi          = {10.1111/biom.13324},
  journal      = {Biometrics},
  number       = {2},
  pages        = {649-660},
  shortjournal = {Biometrics},
  title        = {A bayesian hierarchical model for characterizing the diffusion of new antipsychotic drugs},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semiparametric bayesian approach to population finding
with time-to-event and toxicity data in a randomized clinical trial.
<em>BIOMTC</em>, <em>77</em>(2), 634–648. (<a
href="https://doi.org/10.1111/biom.13289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A utility-based Bayesian population finding (BaPoFi) method was proposed by Morita and Müller to analyze data from a randomized clinical trial with the aim of identifying good predictive baseline covariates for optimizing the target population for a future study. The approach casts the population finding process as a formal decision problem together with a flexible probability model using a random forest to define a regression mean function. BaPoFi is constructed to handle a single continuous or binary outcome variable. In this paper, we develop BaPoFi-TTE as an extension of the earlier approach for clinically important cases of time-to-event (TTE) data with censoring, and also accounting for a toxicity outcome. We model the association of TTE data with baseline covariates using a semiparametric failure time model with a Pólya tree prior for an unknown error term and a random forest for a flexible regression mean function. We define a utility function that addresses a trade-off between efficacy and toxicity as one of the important clinical considerations for population finding. We examine the operating characteristics of the proposed method in extensive simulation studies. For illustration, we apply the proposed method to data from a randomized oncology clinical trial. Concerns in a preliminary analysis of the same data based on a parametric model motivated the proposed more general approach.},
  archive      = {J_BIOMTC},
  author       = {Satoshi Morita and Peter Müller and Hiroyasu Abe},
  doi          = {10.1111/biom.13289},
  journal      = {Biometrics},
  number       = {2},
  pages        = {634-648},
  shortjournal = {Biometrics},
  title        = {A semiparametric bayesian approach to population finding with time-to-event and toxicity data in a randomized clinical trial},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-group poisson-dirichlet mixtures for multiple testing.
<em>BIOMTC</em>, <em>77</em>(2), 622–633. (<a
href="https://doi.org/10.1111/biom.13314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous testing of multiple hypotheses is common to the analysis of high-dimensional data sets. The two-group model, first proposed by Efron, identifies significant comparisons by allocating observations to a mixture of an empirical null and an alternative distribution. In the Bayesian nonparametrics literature, many approaches have suggested using mixtures of Dirichlet Processes in the two-group model framework. Here, we investigate employing mixtures of two-parameter Poisson-Dirichlet Processes instead, and show how they provide a more flexible and effective tool for large-scale hypothesis testing. Our model further employs nonlocal prior densities to allow separation between the two mixture components. We obtain a closed-form expression for the exchangeable partition probability function of the two-group model, which leads to a straightforward Markov Chain Monte Carlo implementation. We compare the performance of our method for large-scale inference in a simulation study and illustrate its use on both a prostate cancer data set and a case-control microbiome study of the gastrointestinal tracts in children from underdeveloped countries who have been recently diagnosed with moderate-to-severe diarrhea.},
  archive      = {J_BIOMTC},
  author       = {Francesco Denti and Michele Guindani and Fabrizio Leisen and Antonio Lijoi and William Duncan Wadsworth and Marina Vannucci},
  doi          = {10.1111/biom.13314},
  journal      = {Biometrics},
  number       = {2},
  pages        = {622-633},
  shortjournal = {Biometrics},
  title        = {Two-group poisson-dirichlet mixtures for multiple testing},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantile regression for survival data with covariates
subject to detection limits. <em>BIOMTC</em>, <em>77</em>(2), 610–621.
(<a href="https://doi.org/10.1111/biom.13309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in biomedical research, biomarkers are becoming increasingly important prognostic factors for predicting overall survival, while the measurement of biomarkers is often censored due to instruments&#39; lower limits of detection. This leads to two types of censoring: random censoring in overall survival outcomes and fixed censoring in biomarker covariates, posing new challenges in statistical modeling and inference. Existing methods for analyzing such data focus primarily on linear regression ignoring censored responses or semiparametric accelerated failure time models with covariates under detection limits (DL). In this paper, we propose a quantile regression for survival data with covariates subject to DL. Comparing to existing methods, the proposed approach provides a more versatile tool for modeling the distribution of survival outcomes by allowing covariate effects to vary across conditional quantiles of the survival time and requiring no parametric distribution assumptions for outcome data. To estimate the quantile process of regression coefficients, we develop a novel multiple imputation approach based on another quantile regression for covariates under DL, avoiding stringent parametric restrictions on censored covariates as often assumed in the literature. Under regularity conditions, we show that the estimation procedure yields uniformly consistent and asymptotically normal estimators. Simulation results demonstrate the satisfactory finite-sample performance of the method. We also apply our method to the motivating data from a study of genetic and inflammatory markers of Sepsis.},
  archive      = {J_BIOMTC},
  author       = {Tonghui Yu and Liming Xiang and Huixia Judy Wang},
  doi          = {10.1111/biom.13309},
  journal      = {Biometrics},
  number       = {2},
  pages        = {610-621},
  shortjournal = {Biometrics},
  title        = {Quantile regression for survival data with covariates subject to detection limits},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing and evaluating risk prediction models with panel
current status data. <em>BIOMTC</em>, <em>77</em>(2), 599–609. (<a
href="https://doi.org/10.1111/biom.13317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panel current status data arise frequently in biomedical studies when the occurrence of a particular clinical condition is only examined at several prescheduled visit times. Existing methods for analyzing current status data have largely focused on regression modeling based on commonly used survival models such as the proportional hazards model and the accelerated failure time model. However, these procedures have the limitations of being difficult to implement and performing sub-optimally in relatively small sample sizes. The performance of these procedures is also unclear under model misspecification. In addition, no methods currently exist to evaluate the prediction performance of estimated risk models with panel current status data. In this paper, we propose a simple estimator under a general class of nonparametric transformation (NPT) models by fitting a logistic regression working model and demonstrate that our proposed estimator is consistent for the NPT model parameter up to a scale multiplier. Furthermore, we propose nonparametric estimators for evaluating the prediction performance of the risk score derived from model fitting, which is valid regardless of the adequacy of the fitted model. Extensive simulation results suggest that our proposed estimators perform well in finite samples and the regression parameter estimators outperform existing estimators under various scenarios. We illustrate the proposed procedures using data from the Framingham Offspring Study.},
  archive      = {J_BIOMTC},
  author       = {Stephanie Chan and Xuan Wang and Ina Jazić and Sarah Peskoe and Yingye Zheng and Tianxi Cai},
  doi          = {10.1111/biom.13317},
  journal      = {Biometrics},
  number       = {2},
  pages        = {599-609},
  shortjournal = {Biometrics},
  title        = {Developing and evaluating risk prediction models with panel current status data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimality of testing procedures for survival data in the
nonproportional hazards setting. <em>BIOMTC</em>, <em>77</em>(2),
587–598. (<a href="https://doi.org/10.1111/biom.13315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most statistical tests for treatment effects used in randomized clinical trials with survival outcomes are based on the proportional hazards assumption, which often fails in practice. Data from early exploratory studies may provide evidence of nonproportional hazards, which can guide the choice of alternative tests in the design of practice-changing confirmatory trials. We developed a test to detect treatment effects in a late-stage trial, which accounts for the deviations from proportional hazards suggested by early-stage data. Conditional on early-stage data, among all tests that control the frequentist Type I error rate at a fixed α level, our testing procedure maximizes the Bayesian predictive probability that the study will demonstrate the efficacy of the experimental treatment. Hence, the proposed test provides a useful benchmark for other tests commonly used in the presence of nonproportional hazards, for example, weighted log-rank tests. We illustrate this approach in simulations based on data from a published cancer immunotherapy phase III trial.},
  archive      = {J_BIOMTC},
  author       = {Andrea Arfè and Brian Alexander and Lorenzo Trippa},
  doi          = {10.1111/biom.13315},
  journal      = {Biometrics},
  number       = {2},
  pages        = {587-598},
  shortjournal = {Biometrics},
  title        = {Optimality of testing procedures for survival data in the nonproportional hazards setting},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structural factor equation models for causal network
construction via directed acyclic mixed graphs. <em>BIOMTC</em>,
<em>77</em>(2), 573–586. (<a
href="https://doi.org/10.1111/biom.13322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic mixed graphs (DAMGs) provide a useful representation of network topology with both directed and undirected edges subject to the restriction of no directed cycles in the graph. This graphical framework may arise in many biomedical studies, for example, when a directed acyclic graph (DAG) of interest is contaminated with undirected edges induced by some unobserved confounding factors (eg, unmeasured environmental factors). Directed edges in a DAG are widely used to evaluate causal relationships among variables in a network, but detecting them is challenging when the underlying causality is obscured by some shared latent factors. The objective of this paper is to develop an effective structural equation model (SEM) method to extract reliable causal relationships from a DAMG. The proposed approach, termed structural factor equation model (SFEM) , uses the SEM to capture the network topology of the DAG while accounting for the undirected edges in the graph with a factor analysis model. The latent factors in the SFEM enable the identification and removal of undirected edges, leading to a simpler and more interpretable causal network. The proposed method is evaluated and compared to existing methods through extensive simulation studies, and illustrated through the construction of gene regulatory networks related to breast cancer.},
  archive      = {J_BIOMTC},
  author       = {Yan Zhou and Peter X.-K. Song and Xiaoquan Wen},
  doi          = {10.1111/biom.13322},
  journal      = {Biometrics},
  number       = {2},
  pages        = {573-586},
  shortjournal = {Biometrics},
  title        = {Structural factor equation models for causal network construction via directed acyclic mixed graphs},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric regression calibration for general hazard
models in survival analysis with covariate measurement error; surprising
performance under linear hazard. <em>BIOMTC</em>, <em>77</em>(2),
561–572. (<a href="https://doi.org/10.1111/biom.13318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational epidemiological studies often confront the problem of estimating exposure-disease relationships when the exposure is not measured exactly. Regression calibration (RC) is a common approach to correct for bias in regression analysis with covariate measurement error. In survival analysis with covariate measurement error, it is well known that the RC estimator may be biased when the hazard is an exponential function of the covariates. In the paper, we investigate the RC estimator with general hazard functions, including exponential and linear functions of the covariates. When the hazard is a linear function of the covariates, we show that a risk set regression calibration (RRC) is consistent and robust to a working model for the calibration function. Under exponential hazard models, there is a trade-off between bias and efficiency when comparing RC and RRC. However, one surprising finding is that the trade-off between bias and efficiency in measurement error research is not seen under linear hazard when the unobserved covariate is from a uniform or normal distribution. Under this situation, the RRC estimator is in general slightly better than the RC estimator in terms of both bias and efficiency. The methods are applied to the Nutritional Biomarkers Study of the Women&#39;s Health Initiative.},
  archive      = {J_BIOMTC},
  author       = {Ching-Yun Wang and Xiao Song},
  doi          = {10.1111/biom.13318},
  journal      = {Biometrics},
  number       = {2},
  pages        = {561-572},
  shortjournal = {Biometrics},
  title        = {Semiparametric regression calibration for general hazard models in survival analysis with covariate measurement error; surprising performance under linear hazard},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric estimation of cross-covariance functions for
multivariate random fields. <em>BIOMTC</em>, <em>77</em>(2), 547–560.
(<a href="https://doi.org/10.1111/biom.13323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of spatially referenced multivariate data has impelled researchers to develop procedures for joint modeling of multiple spatial processes. This ordinarily involves modeling marginal and cross-process dependence for any arbitrary pair of locations using a multivariate spatial covariance function. However, building a flexible multivariate spatial covariance function that is nonnegative definite is challenging. Here, we propose a semiparametric approach for multivariate spatial covariance function estimation with approximate Matérn marginals and highly flexible cross-covariance functions via their spectral representations. The flexibility in our cross-covariance function arises due to B-spline–based specification of the underlying coherence functions, which in turn allows us to capture nontrivial cross-spectral features. We then develop a likelihood-based estimation procedure and perform multiple simulation studies to demonstrate the performance of our method, especially on the coherence function estimation. Finally, we analyze particulate matter concentrations (PM 2.5 ) and wind speed data over the West-North-Central climatic region of the United States, where we illustrate that our proposed method outperforms the commonly used full bivariate Matérn model and the linear model of coregionalization for spatial prediction.},
  archive      = {J_BIOMTC},
  author       = {Ghulam A. Qadir and Ying Sun},
  doi          = {10.1111/biom.13323},
  journal      = {Biometrics},
  number       = {2},
  pages        = {547-560},
  shortjournal = {Biometrics},
  title        = {Semiparametric estimation of cross-covariance functions for multivariate random fields},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric analysis of nonhomogeneous multistate
processes with clustered observations. <em>BIOMTC</em>, <em>77</em>(2),
533–546. (<a href="https://doi.org/10.1111/biom.13327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequently, clinical trials and observational studies involve complex event history data with multiple events. When the observations are independent, the analysis of such studies can be based on standard methods for multistate models. However, the independence assumption is often violated, such as in multicenter studies, which makes standard methods improper. This work addresses the issue of nonparametric estimation and two-sample testing for the population-averaged transition and state occupation probabilities under general multistate models with cluster-correlated, right-censored, and/or left-truncated observations. The proposed methods do not impose assumptions regarding the within-cluster dependence, allow for informative cluster size, and are applicable to both Markov and non-Markov processes. Using empirical process theory, the estimators are shown to be uniformly consistent and to converge weakly to tight Gaussian processes. Closed-form variance estimators are derived, rigorous methodology for the calculation of simultaneous confidence bands is proposed, and the asymptotic properties of the nonparametric tests are established. Furthermore, I provide theoretical arguments for the validity of the nonparametric cluster bootstrap, which can be readily implemented in practice regardless of how complex the underlying multistate model is. Simulation studies show that the performance of the proposed methods is good, and that methods that ignore the within-cluster dependence can lead to invalid inferences. Finally, the methods are illustrated using data from a multicenter randomized controlled trial.},
  archive      = {J_BIOMTC},
  author       = {Giorgos Bakoyannis},
  doi          = {10.1111/biom.13327},
  journal      = {Biometrics},
  number       = {2},
  pages        = {533-546},
  shortjournal = {Biometrics},
  title        = {Nonparametric analysis of nonhomogeneous multistate processes with clustered observations},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multiple robust propensity score method for longitudinal
analysis with intermittent missing data. <em>BIOMTC</em>,
<em>77</em>(2), 519–532. (<a
href="https://doi.org/10.1111/biom.13330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal data are very popular in practice, but they are often missing in either outcomes or time-dependent risk factors, making them highly unbalanced and complex. Missing data may contain various missing patterns or mechanisms, and how to properly handle it for unbiased and valid inference still presents a significant challenge. Here, we propose a novel semiparametric framework for analyzing longitudinal data with both missing responses and covariates that are missing at random and intermittent, a general and widely encountered situation in observational studies. Within this framework, we consider multiple robust estimation procedures based on innovative calibrated propensity scores, which offers additional relaxation of the misspecification of missing data mechanisms and shows more satisfactory numerical performance. Also, the corresponding robust information criterion on consistent variable selection for our proposed model is developed based on empirical likelihood-based methods. These advocated methods are evaluated in both theory and extensive simulation studies in a variety of situations, showing competing properties and advantages compared to the existing approaches. We illustrate the utility of our approach by analyzing the data from the HIV Epidemiology Research Study.},
  archive      = {J_BIOMTC},
  author       = {Chixiang Chen and Biyi Shen and Aiyi Liu and Rongling Wu and Ming Wang},
  doi          = {10.1111/biom.13330},
  journal      = {Biometrics},
  number       = {2},
  pages        = {519-532},
  shortjournal = {Biometrics},
  title        = {A multiple robust propensity score method for longitudinal analysis with intermittent missing data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A constrained single-index regression for estimating
interactions between a treatment and covariates. <em>BIOMTC</em>,
<em>77</em>(2), 506–518. (<a
href="https://doi.org/10.1111/biom.13320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a single-index regression model, uniquely constrained to estimate interactions between a set of pretreatment covariates and a treatment variable on their effects on a response variable, in the context of analyzing data from randomized clinical trials. We represent interaction effect terms of the model through a set of treatment-specific flexible link functions on a linear combination of the covariates (a single index), subject to the constraint that the expected value given the covariates equals 0, while leaving the main effects of the covariates unspecified. We show that the proposed semiparametric estimator is consistent for the interaction term of the model, and that the efficiency of the estimator can be improved with an augmentation procedure. The proposed single-index regression provides a flexible and interpretable modeling approach to optimizing individualized treatment rules based on patients&#39; data measured at baseline, as illustrated by simulation examples and an application to data from a depression clinical trial.},
  archive      = {J_BIOMTC},
  author       = {Hyung Park and Eva Petkova and Thaddeus Tarpey and R. Todd Ogden},
  doi          = {10.1111/biom.13320},
  journal      = {Biometrics},
  number       = {2},
  pages        = {506-518},
  shortjournal = {Biometrics},
  title        = {A constrained single-index regression for estimating interactions between a treatment and covariates},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial regression and spillover effects in cluster
randomized trials with count outcomes. <em>BIOMTC</em>, <em>77</em>(2),
490–505. (<a href="https://doi.org/10.1111/biom.13316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes methodology for analyzing data from cluster randomized trials with count outcomes, taking indirect effects as well spatial effects into account. Indirect effects are modeled using a novel application of a measure of depth within the intervention arm. Both direct and indirect effects can be estimated accurately even when the proposed model is misspecified. We use spatial regression models with Gaussian random effects, where the individual outcomes have distributions overdispersed with respect to the Poisson, and the corresponding direct and indirect effects have a marginal interpretation. To avoid spatial confounding, we use orthogonal regression, in which random effects represent spatial dependence using a homoscedastic and dimensionally reduced modification of the intrinsic conditional autoregression model. We illustrate the methodology using spatial data from a pair-matched cluster randomized trial against the dengue mosquito vector Aedes aegypti , done in Trujillo, Venezuela.},
  archive      = {J_BIOMTC},
  author       = {Karim Anaya-Izquierdo and Neal Alexander},
  doi          = {10.1111/biom.13316},
  journal      = {Biometrics},
  number       = {2},
  pages        = {490-505},
  shortjournal = {Biometrics},
  title        = {Spatial regression and spillover effects in cluster randomized trials with count outcomes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of longitudinal surrogate markers.
<em>BIOMTC</em>, <em>77</em>(2), 477–489. (<a
href="https://doi.org/10.1111/biom.13310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of surrogate markers to examine the effectiveness of a treatment has the potential to decrease study length and identify effective treatments more quickly. Most available methods to investigate the usefulness of a surrogate marker involve restrictive parametric assumptions and tend to focus on settings where the surrogate is measured at a single point in time. However, in many clinical settings, the potential surrogate marker is often measured repeatedly over time, and thus, the surrogate marker information is a trajectory of measurements. In addition, it is often difficult in practice to correctly specify the relationship between a treatment, primary outcome, and surrogate marker trajectory. In this paper, we propose a model-free definition for the proportion of the treatment effect on the primary outcome that is explained by the treatment effect on the longitudinal surrogate markers. We propose three novel flexible methods to estimate this proportion, develop the asymptotic properties of our estimators, and investigate the robustness of the estimators under multiple settings via a simulation study. We apply our proposed procedures to an AIDS clinical trial dataset to examine a trajectory of CD4 counts as a potential surrogate.},
  archive      = {J_BIOMTC},
  author       = {Denis Agniel and Layla Parast},
  doi          = {10.1111/biom.13310},
  journal      = {Biometrics},
  number       = {2},
  pages        = {477-489},
  shortjournal = {Biometrics},
  title        = {Evaluation of longitudinal surrogate markers},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Resampling-based confidence intervals for model-free robust
inference on optimal treatment regimes. <em>BIOMTC</em>, <em>77</em>(2),
465–476. (<a href="https://doi.org/10.1111/biom.13337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new procedure for inference on optimal treatment regimes in the model-free setting, which does not require to specify an outcome regression model. Existing model-free estimators for optimal treatment regimes are usually not suitable for the purpose of inference, because they either have nonstandard asymptotic distributions or do not necessarily guarantee consistent estimation of the parameter indexing the Bayes rule due to the use of surrogate loss. We first study a smoothed robust estimator that directly targets the parameter corresponding to the Bayes decision rule for optimal treatment regimes estimation. This estimator is shown to have an asymptotic normal distribution. Furthermore, we verify that a resampling procedure provides asymptotically accurate inference for both the parameter indexing the optimal treatment regime and the optimal value function. A new algorithm is developed to calculate the proposed estimator with substantially improved speed and stability. Numerical results demonstrate the satisfactory performance of the new methods.},
  archive      = {J_BIOMTC},
  author       = {Yunan Wu and Lan Wang},
  doi          = {10.1111/biom.13337},
  journal      = {Biometrics},
  number       = {2},
  pages        = {465-476},
  shortjournal = {Biometrics},
  title        = {Resampling-based confidence intervals for model-free robust inference on optimal treatment regimes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The impact of misclassification on covariate-adaptive
randomized clinical trials. <em>BIOMTC</em>, <em>77</em>(2), 451–464.
(<a href="https://doi.org/10.1111/biom.13308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-adaptive randomization (CAR) is widely used in clinical trials to balance treatment allocation over covariates. Over the past decade, significant progress has been made on the theoretical properties of covariate-adaptive design and associated inference. However, most results are established under the assumption that the covariates are correctly measured. In practice, measurement error is inevitable, resulting in misclassification for discrete covariates. When covariate misclassification is present in a clinical trial conducted using CAR, the impact is twofold: it impairs the intended covariate balance, and raises concerns over the validity of test procedures. In this paper, we consider the impact of misclassification on covariate-adaptive randomized trials from the perspectives of both design and inference. We derive the asymptotic normality, and thereby the convergence rate, of the imbalance of the true covariates for a general family of covariate-adaptive randomization methods, and show that a superior covariate balance can still be attained compared to complete randomization. We also show that the two sample t -test is conservative, with a reduced Type I error, but that this can be corrected using a bootstrap method. Moreover, if the misclassified covariates are adjusted in the model used for analysis, the test maintains its nominal Type I error, with an increased power. Our results support the use of covariate-adaptive randomization in clinical trials, even when the covariates are subject to misclassification.},
  archive      = {J_BIOMTC},
  author       = {Tong Wang and Wei Ma},
  doi          = {10.1111/biom.13308},
  journal      = {Biometrics},
  number       = {2},
  pages        = {451-464},
  shortjournal = {Biometrics},
  title        = {The impact of misclassification on covariate-adaptive randomized clinical trials},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable bayesian matrix normal graphical models for brain
functional networks. <em>BIOMTC</em>, <em>77</em>(2), 439–450. (<a
href="https://doi.org/10.1111/biom.13319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been an explosive growth in graphical modeling approaches for estimating brain functional networks. In a detailed study, we show that surprisingly, standard graphical modeling approaches for fMRI data may not yield accurate estimates of the brain network due to the inability to suitably account for temporal correlations. We propose a novel Bayesian matrix normal graphical model that jointly models the temporal covariance and the brain network under a separable structure for the covariance to obtain improved estimates. The approach is implemented via an efficient optimization algorithm that computes the maximum-a-posteriori network estimates having desirable theoretical properties and which is scalable to high dimensions. The proposed method leads to substantial gains in network estimation accuracy compared to standard brain network modeling approaches as illustrated via extensive simulations. We apply the method to resting state fMRI data from the Human Connectome Project involving a large number of time scans and brain regions, to study the relationships between fluid intelligence and functional connectivity, where it is not computationally feasible to apply existing matrix normal graphical models. Our proposed approach led to the detection of differences in connectivity between high and low fluid intelligence groups, whereas these differences were less pronounced or absent using the graphical lasso.},
  archive      = {J_BIOMTC},
  author       = {Suprateek Kundu and Benjamin B. Risk},
  doi          = {10.1111/biom.13319},
  journal      = {Biometrics},
  number       = {2},
  pages        = {439-450},
  shortjournal = {Biometrics},
  title        = {Scalable bayesian matrix normal graphical models for brain functional networks},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel statistical method for modeling covariate effects in
bisulfite sequencing derived measures of DNA methylation.
<em>BIOMTC</em>, <em>77</em>(2), 424–438. (<a
href="https://doi.org/10.1111/biom.13307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying disease-associated changes in DNA methylation can help us gain a better understanding of disease etiology. Bisulfite sequencing allows the generation of high-throughput methylation profiles at single-base resolution of DNA. However, optimally modeling and analyzing these sparse and discrete sequencing data is still very challenging due to variable read depth, missing data patterns, long-range correlations, data errors, and confounding from cell type mixtures. We propose a regression-based hierarchical model that allows covariate effects to vary smoothly along genomic positions and we have built a specialized EM algorithm, which explicitly allows for experimental errors and cell type mixtures, to make inference about smooth covariate effects in the model. Simulations show that the proposed method provides accurate estimates of covariate effects and captures the major underlying methylation patterns with excellent power. We also apply our method to analyze data from rheumatoid arthritis patients and controls. The method has been implemented in R package SOMNiBUS .},
  archive      = {J_BIOMTC},
  author       = {Kaiqiong Zhao and Karim Oualkacha and Lajmi Lakhal-Chaieb and Aurélie Labbe and Kathleen Klein and Antonio Ciampi and Marie Hudson and Inés Colmegna and Tomi Pastinen and Tieyuan Zhang and Denise Daley and Celia M.T. Greenwood},
  doi          = {10.1111/biom.13307},
  journal      = {Biometrics},
  number       = {2},
  pages        = {424-438},
  shortjournal = {Biometrics},
  title        = {A novel statistical method for modeling covariate effects in bisulfite sequencing derived measures of DNA methylation},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and efficient semi-supervised estimation of average
treatment effects with application to electronic health records data.
<em>BIOMTC</em>, <em>77</em>(2), 413–423. (<a
href="https://doi.org/10.1111/biom.13298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating the average treatment effect (ATE) in a semi-supervised learning setting, where a very small proportion of the entire set of observations are labeled with the true outcome but features predictive of the outcome are available among all observations. This problem arises, for example, when estimating treatment effects in electronic health records (EHR) data because gold-standard outcomes are often not directly observable from the records but are observed for a limited number of patients through small-scale manual chart review. We develop an imputation-based approach for estimating the ATE that is robust to misspecification of the imputation model. This effectively allows information from the predictive features to be safely leveraged to improve efficiency in estimating the ATE. The estimator is additionally doubly-robust in that it is consistent under correct specification of either an initial propensity score model or a baseline outcome model. It is also locally semiparametric efficient under an ideal semi-supervised model where the distribution of the unlabeled data is known. Simulations exhibit the efficiency and robustness of the proposed method compared to existing approaches in finite samples. We illustrate the method by comparing rates of treatment response to two biologic agents for treatment inflammatory bowel disease using EHR data from Partners&#39; Healthcare.},
  archive      = {J_BIOMTC},
  author       = {David Cheng and Ashwin N. Ananthakrishnan and Tianxi Cai},
  doi          = {10.1111/biom.13298},
  journal      = {Biometrics},
  number       = {2},
  pages        = {413-423},
  shortjournal = {Biometrics},
  title        = {Robust and efficient semi-supervised estimation of average treatment effects with application to electronic health records data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterated multisource exchangeability models for
individualized inference with an application to mobile sensor data.
<em>BIOMTC</em>, <em>77</em>(2), 401–412. (<a
href="https://doi.org/10.1111/biom.13294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers are increasingly interested in using sensor technology to collect accurate activity information and make individualized inference about treatments, exposures, and policies. How to optimally combine population data with data from an individual remains an open question. Multisource exchangeability models (MEMs) are a Bayesian approach for increasing precision by combining potentially heterogeneous supplemental data sources into analysis of a primary source. MEMs are a potentially powerful tool for individualized inference but can integrate only a few sources; their model space grows exponentially, making them intractable for high-dimensional applications. We propose iterated MEMs (iMEMs), which identify a subset of the most exchangeable sources prior to fitting a MEM model. iMEM complexity scales linearly with the number of sources, and iMEMs greatly increase precision while maintaining desirable asymptotic and small sample properties. We apply iMEMs to individual-level behavior and emotion data from a smartphone app and show that they achieve individualized inference with up to 99\% efficiency gain relative to standard analyses that do not borrow information.},
  archive      = {J_BIOMTC},
  author       = {Roland Brown and Yingling Fan and Kirti Das and Julian Wolfson},
  doi          = {10.1111/biom.13294},
  journal      = {Biometrics},
  number       = {2},
  pages        = {401-412},
  shortjournal = {Biometrics},
  title        = {Iterated multisource exchangeability models for individualized inference with an application to mobile sensor data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian group selection in logistic regression with
application to MRI data analysis. <em>BIOMTC</em>, <em>77</em>(2),
391–400. (<a href="https://doi.org/10.1111/biom.13290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Bayesian logistic regression models with group-structured covariates. In high-dimensional settings, it is often assumed that only a small portion of groups are significant, and thus, consistent group selection is of significant importance. While consistent frequentist group selection methods have been proposed, theoretical properties of Bayesian group selection methods for logistic regression models have not been investigated yet. In this paper, we consider a hierarchical group spike and slab prior for logistic regression models in high-dimensional settings. Under mild conditions, we establish strong group selection consistency of the induced posterior, which is the first theoretical result in the Bayesian literature. Through simulation studies, we demonstrate that the proposed method outperforms existing state-of-the-art methods in various settings. We further apply our method to a magnetic resonance imaging data set for predicting Parkinson&#39;s disease and show its benefits over other contenders.},
  archive      = {J_BIOMTC},
  author       = {Kyoungjae Lee and Xuan Cao},
  doi          = {10.1111/biom.13290},
  journal      = {Biometrics},
  number       = {2},
  pages        = {391-400},
  shortjournal = {Biometrics},
  title        = {Bayesian group selection in logistic regression with application to MRI data analysis},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating and inferring the maximum degree of
stimulus-locked time-varying brain connectivity networks.
<em>BIOMTC</em>, <em>77</em>(2), 379–390. (<a
href="https://doi.org/10.1111/biom.13297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroscientists have enjoyed much success in understanding brain functions by constructing brain connectivity networks using data collected under highly controlled experimental settings. However, these experimental settings bear little resemblance to our real-life experience in day-to-day interactions with the surroundings. To address this issue, neuroscientists have been measuring brain activity under natural viewing experiments in which the subjects are given continuous stimuli, such as watching a movie or listening to a story. The main challenge with this approach is that the measured signal consists of both the stimulus-induced signal, as well as intrinsic-neural and nonneuronal signals. By exploiting the experimental design, we propose to estimate stimulus-locked brain networks by treating nonstimulus-induced signals as nuisance parameters. In many neuroscience applications, it is often important to identify brain regions that are connected to many other brain regions during cognitive process. We propose an inferential method to test whether the maximum degree of the estimated network is larger than a prespecific number. We prove that the type I error can be controlled and that the power increases to one asymptotically. Simulation studies are conducted to assess the performance of our method. Finally, we analyze a functional magnetic resonance imaging dataset obtained under the Sherlock Holmes movie stimuli.},
  archive      = {J_BIOMTC},
  author       = {Kean Ming Tan and Junwei Lu and Tong Zhang and Han Liu},
  doi          = {10.1111/biom.13297},
  journal      = {Biometrics},
  number       = {2},
  pages        = {379-390},
  shortjournal = {Biometrics},
  title        = {Estimating and inferring the maximum degree of stimulus-locked time-varying brain connectivity networks},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Book review of “disease mapping: From foundations to
multidimensional modeling.” <em>BIOMTC</em>, <em>77</em>(1), 372–373.
(<a href="https://doi.org/10.1111/biom.13425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Virgilio Gómez-Rubio},
  doi          = {10.1111/biom.13425},
  journal      = {Biometrics},
  number       = {1},
  pages        = {372-373},
  shortjournal = {Biometrics},
  title        = {Book review of “Disease mapping: From foundations to multidimensional modeling”},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A penalized structural equation modeling method accounting
for secondary phenotypes for variable selection on genetically regulated
expression from PrediXcan for alzheimer’s disease. <em>BIOMTC</em>,
<em>77</em>(1), 362–371. (<a
href="https://doi.org/10.1111/biom.13286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global burden of mental illness is estimated to become a severe issue in the near future, it demands the development of more effective treatments. Most psychiatric diseases are moderately to highly heritable and believed to involve many genes. Development of new treatment options demands more knowledge on the molecular basis of psychiatric diseases. Toward this end, we propose to develop new statistical methods with improved sensitivity and accuracy to identify disease-related genes specialized for psychiatric diseases. The qualitative psychiatric diagnoses such as case control often suffer from high rates of misdiagnosis and oversimplify the disease phenotypes. Our proposed method utilizes endophenotypes, the quantitative traits hypothesized to underlie disease syndromes, to better characterize the heterogeneous phenotypes of psychiatric diseases. We employ the structural equation modeling using the liability-index model to link multiple genetically regulated expressions from PrediXcan and the manifest variables including endophenotypes and case-control status. The proposed method can be considered as a general method for multivariate regression, which is particularly helpful for psychiatric diseases. We derive penalized retrospective likelihood estimators to deal with the typical small sample size issue. Simulation results demonstrate the advantages of the proposed method and the real data analysis of Alzheimer&#39;s disease illustrates the practical utility of the techniques. Data used in preparation of this article were obtained from the Alzheimer&#39;s Disease Neuroimaging Initiative database.},
  archive      = {J_BIOMTC},
  author       = {Ting-Huei Chen and Hanaa Boughal},
  doi          = {10.1111/biom.13286},
  journal      = {Biometrics},
  number       = {1},
  pages        = {362-371},
  shortjournal = {Biometrics},
  title        = {A penalized structural equation modeling method accounting for secondary phenotypes for variable selection on genetically regulated expression from PrediXcan for alzheimer&#39;s disease},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving inference for nonlinear state-space models of
animal population dynamics given biased sequential life stage data.
<em>BIOMTC</em>, <em>77</em>(1), 352–361. (<a
href="https://doi.org/10.1111/biom.13267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-space models (SSMs) are a popular tool for modeling animal abundances. Inference difficulties for simple linear SSMs are well known, particularly in relation to simultaneous estimation of process and observation variances. Several remedies to overcome estimation problems have been studied for relatively simple SSMs, but whether these challenges and proposed remedies apply for nonlinear stage-structured SSMs, an important class of ecological models, is less well understood. Here we identify improvements for inference about nonlinear stage-structured SSMs fit with biased sequential life stage data. Theoretical analyses indicate parameter identifiability requires covariates in the state processes. Simulation studies show that plugging in externally estimated observation variances, as opposed to jointly estimating them with other parameters, reduces bias and standard error of estimates. In contrast to previous results for simple linear SSMs, strong confounding between jointly estimated process and observation variance parameters was not found in the models explored here. However, when observation variance was also estimated in the motivating case study, the resulting process variance estimates were implausibly low (near-zero). As SSMs are used in increasingly complex ways, understanding when inference can be expected to be successful, and what aids it, becomes more important. Our study illustrates (a) the need for relevant process covariates and (b) the benefits of using externally estimated observation variances for inference about nonlinear stage-structured SSMs.},
  archive      = {J_BIOMTC},
  author       = {Leo Polansky and Ken B. Newman and Lara Mitchell},
  doi          = {10.1111/biom.13267},
  journal      = {Biometrics},
  number       = {1},
  pages        = {352-361},
  shortjournal = {Biometrics},
  title        = {Improving inference for nonlinear state-space models of animal population dynamics given biased sequential life stage data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Repeated measures random forests (RMRF): Identifying factors
associated with nocturnal hypoglycemia. <em>BIOMTC</em>, <em>77</em>(1),
343–351. (<a href="https://doi.org/10.1111/biom.13284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nocturnal hypoglycemia is a common phenomenon among patients with diabetes and can lead to a broad range of adverse events and complications. Identifying factors associated with hypoglycemia can improve glucose control and patient care. We propose a repeated measures random forest (RMRF) algorithm that can handle nonlinear relationships and interactions and the correlated responses from patients evaluated over several nights. Simulation results show that our proposed algorithm captures the informative variable more often than naïvely assuming independence. RMRF also outperforms standard random forest and extremely randomized trees algorithms. We demonstrate scenarios where RMRF attains greater prediction accuracy than generalized linear models. We apply the RMRF algorithm to analyze a diabetes study with 2524 nights from 127 patients with type 1 diabetes. We find that nocturnal hypoglycemia is associated with HbA1c, bedtime blood glucose (BG), insulin on board, time system activated, exercise intensity, and daytime hypoglycemia. The RMRF can accurately classify nights at high risk of nocturnal hypoglycemia.},
  archive      = {J_BIOMTC},
  author       = {Peter Calhoun and Richard A. Levine and Juanjuan Fan},
  doi          = {10.1111/biom.13284},
  journal      = {Biometrics},
  number       = {1},
  pages        = {343-351},
  shortjournal = {Biometrics},
  title        = {Repeated measures random forests (RMRF): Identifying factors associated with nocturnal hypoglycemia},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting nonsystematic covariate monitoring to broaden the
scope of evidence about the causal effects of adaptive treatment
strategies. <em>BIOMTC</em>, <em>77</em>(1), 329–342. (<a
href="https://doi.org/10.1111/biom.13271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies based on electronic health records (EHR), the frequency of covariate monitoring can vary by covariate type, across patients, and over time, which can limit the generalizability of inferences about the effects of adaptive treatment strategies. In addition, monitoring is a health intervention in itself with costs and benefits, and stakeholders may be interested in the effect of monitoring when adopting adaptive treatment strategies. This paper demonstrates how to exploit nonsystematic covariate monitoring in EHR-based studies to both improve the generalizability of causal inferences and to evaluate the health impact of monitoring when evaluating adaptive treatment strategies. Using a real world, EHR-based, comparative effectiveness research (CER) study of patients with type II diabetes mellitus, we illustrate how the evaluation of joint dynamic treatment and static monitoring interventions can improve CER evidence and describe two alternate estimation approaches based on inverse probability weighting (IPW). First, we demonstrate the poor performance of the standard estimator of the effects of joint treatment-monitoring interventions, due to a large decrease in data support and concerns over finite-sample bias from near-violations of the positivity assumption (PA) for the monitoring process. Second, we detail an alternate IPW estimator using a no direct effect assumption. We demonstrate that this estimator can improve efficiency but at the potential cost of increase in bias from violations of the PA for the treatment process.},
  archive      = {J_BIOMTC},
  author       = {Noémi Kreif and Oleg Sofrygin and Julie A. Schmittdiel and Alyce S. Adams and Richard W. Grant and Zheng Zhu and Mark J. van der Laan and Romain Neugebauer},
  doi          = {10.1111/biom.13271},
  journal      = {Biometrics},
  number       = {1},
  pages        = {329-342},
  shortjournal = {Biometrics},
  title        = {Exploiting nonsystematic covariate monitoring to broaden the scope of evidence about the causal effects of adaptive treatment strategies},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayes factor approach with informative prior for rare
genetic variant analysis from next generation sequencing data.
<em>BIOMTC</em>, <em>77</em>(1), 316–328. (<a
href="https://doi.org/10.1111/biom.13278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of rare genetic variants through next generation sequencing is a very challenging issue in the field of human genetics. We propose a novel region-based statistical approach based on a Bayes Factor (BF) to assess evidence of association between a set of rare variants (RVs) located on the same genomic region and a disease outcome in the context of case-control design. Marginal likelihoods are computed under the null and alternative hypotheses assuming a binomial distribution for the RV count in the region and a beta or mixture of Dirac and beta prior distribution for the probability of RV. We derive the theoretical null distribution of the BF under our prior setting and show that a Bayesian control of the false Discovery Rate can be obtained for genome-wide inference. Informative priors are introduced using prior evidence of association from a Kolmogorov-Smirnov test statistic. We use our simulation program, sim1000G, to generate RV data similar to the 1000 genomes sequencing project. Our simulation studies showed that the new BF statistic outperforms standard methods (SKAT, SKAT-O, Burden test) in case-control studies with moderate sample sizes and is equivalent to them under large sample size scenarios. Our real data application to a lung cancer case-control study found enrichment for RVs in known and novel cancer genes. It also suggests that using the BF with informative prior improves the overall gene discovery compared to the BF with noninformative prior.},
  archive      = {J_BIOMTC},
  author       = {Jingxiong Xu and Wei Xu and Laurent Briollais},
  doi          = {10.1111/biom.13278},
  journal      = {Biometrics},
  number       = {1},
  pages        = {316-328},
  shortjournal = {Biometrics},
  title        = {A bayes factor approach with informative prior for rare genetic variant analysis from next generation sequencing data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian analysis of survival data with missing censoring
indicators. <em>BIOMTC</em>, <em>77</em>(1), 305–315. (<a
href="https://doi.org/10.1111/biom.13280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some large clinical studies, it may be impractical to perform the physical examination to every subject at his/her last monitoring time in order to diagnose the occurrence of the event of interest. This gives rise to survival data with missing censoring indicators where the probability of missing may depend on time of last monitoring and some covariates. We present a fully Bayesian semi-parametric method for such survival data to estimate regression parameters of the proportional hazards model of Cox. Theoretical investigation and simulation studies show that our method performs better than competing methods. We apply the proposed method to analyze the survival data with missing censoring indicators from the Orofacial Pain: Prospective Evaluation and Risk Assessment study.},
  archive      = {J_BIOMTC},
  author       = {Naomi C. Brownstein and Veronica Bunn and Luis M. Castro and Debajyoti Sinha},
  doi          = {10.1111/biom.13280},
  journal      = {Biometrics},
  number       = {1},
  pages        = {305-315},
  shortjournal = {Biometrics},
  title        = {Bayesian analysis of survival data with missing censoring indicators},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble clustering for step data via binning.
<em>BIOMTC</em>, <em>77</em>(1), 293–304. (<a
href="https://doi.org/10.1111/biom.13258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the clustering problem of physical step count data recorded on wearable devices. Clustering step data give an insight into an individual&#39;s activity status and further provide the groundwork for health-related policies. However, classical methods, such as K -means clustering and hierarchical clustering, are not suitable for step count data that are typically high-dimensional and zero-inflated. This paper presents a new clustering method for step data based on a novel combination of ensemble clustering and binning. We first construct multiple sets of binned data by changing the size and starting position of the bin, and then merge the clustering results from the binned data using a voting method. The advantage of binning, as a critical component, is that it substantially reduces the dimension of the original data while preserving the essential characteristics of the data. As a result, combining clustering results from multiple binned data can provide an improved clustering result that reflects both local and global structures of the data. Simulation studies and real data analysis were carried out to evaluate the empirical performance of the proposed method and demonstrate its general utility.},
  archive      = {J_BIOMTC},
  author       = {Ja-Yoon Jang and Hee-Seok Oh and Yaeji Lim and Ying Kuen Cheung},
  doi          = {10.1111/biom.13258},
  journal      = {Biometrics},
  number       = {1},
  pages        = {293-304},
  shortjournal = {Biometrics},
  title        = {Ensemble clustering for step data via binning},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing tumors from different anatomic sites for clonal
relatedness using somatic mutation data. <em>BIOMTC</em>,
<em>77</em>(1), 283–292. (<a
href="https://doi.org/10.1111/biom.13256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common task for the cancer pathologist is to determine, in a patient suffering from cancer, whether a new tumor in a distinct anatomic site from the primary is an independent occurrence of cancer or a metastasis. As mutational profiling of tumors becomes more widespread in routine clinical practice, this diagnostic task can be greatly enhanced by comparing mutational profiles of the tumors to determine if they are sufficiently similar to conclude that the tumors are clonally related, that is, one is a metastasis of the other. We present here a likelihood ratio test for clonal relatedness in this setting and provide evidence of its validity. The test is unusual in that there are two possible alternative hypotheses, representing the two anatomic sites from which the single clonal cell could have initially emerged. Although evidence for clonal relatedness is largely provided by the presence of exact mutational matches in the two tumors, we show that it is possible to observe data where the test is statistically significant even when no matches are observed. This can occur when the mutational profile of one of the tumors is closely aligned with the anatomic site of the other tumor, suggesting indirectly that the tumor originated in that other site. We exhibit examples of this phenomenon and recommend a strategy for interpreting the results of these tests in practice.},
  archive      = {J_BIOMTC},
  author       = {Irina Ostrovnaya and Audrey Mauguen and Venkatraman E. Seshan and Colin B. Begg},
  doi          = {10.1111/biom.13256},
  journal      = {Biometrics},
  number       = {1},
  pages        = {283-292},
  shortjournal = {Biometrics},
  title        = {Testing tumors from different anatomic sites for clonal relatedness using somatic mutation data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Marginal analysis of multiple outcomes with informative
cluster size. <em>BIOMTC</em>, <em>77</em>(1), 271–282. (<a
href="https://doi.org/10.1111/biom.13241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In surveillance studies of periodontal disease, the relationship between disease and other health and socioeconomic conditions is of key interest. To determine whether a patient has periodontal disease, multiple clinical measurements (eg, clinical attachment loss, alveolar bone loss, and tooth mobility) are taken at the tooth-level. Researchers often create a composite outcome from these measurements or analyze each outcome separately. Moreover, patients have varying number of teeth, with those who are more prone to the disease having fewer teeth compared to those with good oral health. Such dependence between the outcome of interest and cluster size (number of teeth) is called informative cluster size and results obtained from fitting conventional marginal models can be biased. We propose a novel method to jointly analyze multiple correlated binary outcomes for clustered data with informative cluster size using the class of generalized estimating equations (GEE) with cluster-specific weights. We compare our proposed multivariate outcome cluster-weighted GEE results to those from the convectional GEE using the baseline data from Veterans Affairs Dental Longitudinal Study. In an extensive simulation study, we show that our proposed method yields estimates with minimal relative biases and excellent coverage probabilities.},
  archive      = {J_BIOMTC},
  author       = {A. A. Mitani and E. K. Kaye and K. P. Nelson},
  doi          = {10.1111/biom.13241},
  journal      = {Biometrics},
  number       = {1},
  pages        = {271-282},
  shortjournal = {Biometrics},
  title        = {Marginal analysis of multiple outcomes with informative cluster size},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized reliability based on distances. <em>BIOMTC</em>,
<em>77</em>(1), 258–270. (<a
href="https://doi.org/10.1111/biom.13287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intraclass correlation coefficient (ICC) is a classical index of measurement reliability. With the advent of new and complex types of data for which the ICC is not defined, there is a need for new ways to assess reliability. To meet this need, we propose a new distance-based ICC (dbICC), defined in terms of arbitrary distances among observations. We introduce a bias correction to improve the coverage of bootstrap confidence intervals for the dbICC, and demonstrate its efficacy via simulation. We illustrate the proposed method by analyzing the test-retest reliability of brain connectivity matrices derived from a set of repeated functional magnetic resonance imaging scans. The Spearman-Brown formula, which shows how more intensive measurement increases reliability, is extended to encompass the dbICC.},
  archive      = {J_BIOMTC},
  author       = {Meng Xu and Philip T. Reiss and Ivor Cribben},
  doi          = {10.1111/biom.13287},
  journal      = {Biometrics},
  number       = {1},
  pages        = {258-270},
  shortjournal = {Biometrics},
  title        = {Generalized reliability based on distances},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient screening of predictive biomarkers for individual
treatment selection. <em>BIOMTC</em>, <em>77</em>(1), 249–257. (<a
href="https://doi.org/10.1111/biom.13279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of molecular diagnostic tools to achieve individualized medicine requires identifying predictive biomarkers associated with subgroups of individuals who might receive beneficial or harmful effects from different available treatments. However, due to the large number of candidate biomarkers in the large-scale genetic and molecular studies, and complex relationships among clinical outcome, biomarkers, and treatments, the ordinary statistical tests for the interactions between treatments and covariates have difficulties from their limited statistical powers. In this paper, we propose an efficient method for detecting predictive biomarkers. We employ weighted loss functions of Chen et al . to directly estimate individual treatment scores and propose synthetic posterior inference for effect sizes of biomarkers. We develop an empirical Bayes approach, namely, we estimate unknown hyperparameters in the prior distribution based on data. We then provide efficient screening methods for the candidate biomarkers via optimal discovery procedure with adequate control of false discovery rate. The proposed method is demonstrated in simulation studies and an application to a breast cancer clinical study in which the proposed method was shown to detect the much larger numbers of significant biomarkers than existing standard methods.},
  archive      = {J_BIOMTC},
  author       = {Shonosuke Sugasawa and Hisashi Noma},
  doi          = {10.1111/biom.13279},
  journal      = {Biometrics},
  number       = {1},
  pages        = {249-257},
  shortjournal = {Biometrics},
  title        = {Efficient screening of predictive biomarkers for individual treatment selection},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Upper bound estimators of the population size based on
ordinal models for capture-recapture experiments. <em>BIOMTC</em>,
<em>77</em>(1), 237–248. (<a
href="https://doi.org/10.1111/biom.13265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capture-recapture studies have attracted a lot of attention over the past few decades, especially in applied disciplines where a direct estimate for the size of a population of interest is not available. Epidemiology, ecology, public health, and biodiversity are just a few examples. The estimation of the number of unseen units has been a challenge for theoretical statisticians, and considerable progress has been made in providing lower bound estimators for the population size. In fact, it is well known that consistent estimators for this cannot be provided in the very general case. Considering a case where capture-recapture studies are summarized by a frequency of frequencies distribution, we derive a simple upper bound of the population size based on the cumulative distribution function. We introduce two estimators of this bound, without any specific parametric assumption on the distribution of the observed frequency counts. The behavior of the proposed estimators is investigated using several benchmark datasets and a large-scale simulation experiment based on the scheme discussed by Pledger.},
  archive      = {J_BIOMTC},
  author       = {Marco Alfò and Dankmar Böhning and Irene Rocchetti},
  doi          = {10.1111/biom.13265},
  journal      = {Biometrics},
  number       = {1},
  pages        = {237-248},
  shortjournal = {Biometrics},
  title        = {Upper bound estimators of the population size based on ordinal models for capture-recapture experiments},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive treatment and robust control. <em>BIOMTC</em>,
<em>77</em>(1), 223–236. (<a
href="https://doi.org/10.1111/biom.13268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A control theory perspective on determination of optimal dynamic treatment regimes is considered. The aim is to adapt statistical methodology that has been developed for medical or other biostatistical applications to incorporate powerful control techniques that have been designed for engineering or other technological problems. Data tend to be sparse and noisy in the biostatistical area and interest has tended to be in statistical inference for treatment effects. In engineering fields, experimental data can be more easily obtained and reproduced and interest is more often in performance and stability of proposed controllers rather than modeling and inference per se. We propose that modeling and estimation should be based on standard statistical techniques but subsequent treatment policy should be obtained from robust control. To bring focus, we concentrate on A-learning methodology as developed in the biostatistical literature and H ∞ -synthesis from control theory. Simulations and two applications demonstrate robustness of the H ∞ strategy compared to standard A-learning in the presence of model misspecification or measurement error.},
  archive      = {J_BIOMTC},
  author       = {Q. Clairon and R. Henderson and N. J. Young and E. D. Wilson and C. J. Taylor},
  doi          = {10.1111/biom.13268},
  journal      = {Biometrics},
  number       = {1},
  pages        = {223-236},
  shortjournal = {Biometrics},
  title        = {Adaptive treatment and robust control},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A powerful procedure that controls the false discovery rate
with directional information. <em>BIOMTC</em>, <em>77</em>(1), 212–222.
(<a href="https://doi.org/10.1111/biom.13277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many multiple testing applications in genetics, the signs of the test statistics provide useful directional information, such as whether genes are potentially up- or down-regulated between two experimental conditions. However, most existing procedures that control the false discovery rate (FDR) are P -value based and ignore such directional information. We introduce a novel procedure, the signed-knockoff procedure, to utilize the directional information and control the FDR in finite samples. We demonstrate the power advantage of our procedure through simulation studies and two real applications.},
  archive      = {J_BIOMTC},
  author       = {Zhaoyang Tian and Kun Liang and Pengfei Li},
  doi          = {10.1111/biom.13277},
  journal      = {Biometrics},
  number       = {1},
  pages        = {212-222},
  shortjournal = {Biometrics},
  title        = {A powerful procedure that controls the false discovery rate with directional information},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transporting stochastic direct and indirect effects to new
populations. <em>BIOMTC</em>, <em>77</em>(1), 197–211. (<a
href="https://doi.org/10.1111/biom.13274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transported mediation effects may contribute to understanding how interventions work differently when applied to new populations. However, we are not aware of any estimators for such effects. Thus, we propose two doubly robust, efficient estimators of transported stochastic (also called randomized interventional) direct and indirect effects. We demonstrate their finite sample properties in a simulation study. We then apply the preferred substitution estimator to longitudinal data from the Moving to Opportunity Study, a large-scale housing voucher experiment, to transport stochastic indirect effect estimates of voucher receipt in childhood on subsequent risk of mental health or substance use disorder mediated through parental employment across sites, thereby gaining understanding of drivers of the site differences.},
  archive      = {J_BIOMTC},
  author       = {Kara E. Rudolph and Jonathan Levy and Mark J. van der Laan},
  doi          = {10.1111/biom.13274},
  journal      = {Biometrics},
  number       = {1},
  pages        = {197-211},
  shortjournal = {Biometrics},
  title        = {Transporting stochastic direct and indirect effects to new populations},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parameter estimation for discretely observed linear
birth-and-death processes. <em>BIOMTC</em>, <em>77</em>(1), 186–196. (<a
href="https://doi.org/10.1111/biom.13282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Birth-and-death processes are widely used to model the development of biological populations. Although they are relatively simple models, their parameters can be challenging to estimate, as the likelihood can become numerically unstable when data arise from the most common sampling schemes, such as annual population censuses. A further difficulty arises when the discrete observations are not equi-spaced, for example, when census data are unavailable for some years. We present two approaches to estimating the birth, death, and growth rates of a discretely observed linear birth-and-death process: via an embedded Galton-Watson process and by maximizing a saddlepoint approximation to the likelihood. We study asymptotic properties of the estimators, compare them on numerical examples, and apply the methodology to data on monitored populations.},
  archive      = {J_BIOMTC},
  author       = {A. C. Davison and S. Hautphenne and A. Kraus},
  doi          = {10.1111/biom.13282},
  journal      = {Biometrics},
  number       = {1},
  pages        = {186-196},
  shortjournal = {Biometrics},
  title        = {Parameter estimation for discretely observed linear birth-and-death processes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic inference in general nested case-control designs.
<em>BIOMTC</em>, <em>77</em>(1), 175–185. (<a
href="https://doi.org/10.1111/biom.13259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nested case-control designs are attractive in studies with a time-to-event endpoint if the outcome is rare or if interest lies in evaluating expensive covariates. The appeal is that these designs restrict to small subsets of all patients at risk just prior to the observed event times. Only these small subsets need to be evaluated. Typically, the controls are selected at random and methods for time-simultaneous inference have been proposed in the literature. However, the martingale structure behind nested case-control designs allows for more powerful and flexible non-standard sampling designs. We exploit that structure to find simultaneous confidence bands based on wild bootstrap resampling procedures within this general class of designs. We show in a simulation study that the intended coverage probability is obtained for confidence bands for cumulative baseline hazard functions. We apply our methods to observational data about hospital-acquired infections.},
  archive      = {J_BIOMTC},
  author       = {J. Feifel and D. Dobler},
  doi          = {10.1111/biom.13259},
  journal      = {Biometrics},
  number       = {1},
  pages        = {175-185},
  shortjournal = {Biometrics},
  title        = {Dynamic inference in general nested case-control designs},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted regression analysis to correct for informative
monitoring times and confounders in longitudinal studies.
<em>BIOMTC</em>, <em>77</em>(1), 162–174. (<a
href="https://doi.org/10.1111/biom.13285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address estimation of the marginal effect of a time-varying binary treatment on a continuous longitudinal outcome in the context of observational studies using electronic health records, when the relationship of interest is confounded, mediated, and further distorted by an informative visit process. We allow the longitudinal outcome to be recorded only sporadically and assume that its monitoring timing is informed by patients&#39; characteristics. We propose two novel estimators based on linear models for the mean outcome that incorporate an adjustment for confounding and informative monitoring process through generalized inverse probability of treatment weights and a proportional intensity model, respectively. We allow for a flexible modeling of the intercept function as a function of time. Our estimators have closed-form solutions, and their asymptotic distributions can be derived. Extensive simulation studies show that both estimators outperform standard methods such as the ordinary least squares estimator or estimators that only account for informative monitoring or confounders. We illustrate our methods using data from the Add Health study, assessing the effect of depressive mood on weight in adolescents.},
  archive      = {J_BIOMTC},
  author       = {Janie Coulombe and Erica E. M. Moodie and Robert W. Platt},
  doi          = {10.1111/biom.13285},
  journal      = {Biometrics},
  number       = {1},
  pages        = {162-174},
  shortjournal = {Biometrics},
  title        = {Weighted regression analysis to correct for informative monitoring times and confounders in longitudinal studies},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A joint modeling approach for analyzing marker data in the
presence of a terminal event. <em>BIOMTC</em>, <em>77</em>(1), 150–161.
(<a href="https://doi.org/10.1111/biom.13260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many medical studies, markers are contingent on recurrent events and the cumulative markers are usually of interest. However, the recurrent event process is often interrupted by a dependent terminal event, such as death. In this article, we propose a joint modeling approach for analyzing marker data with informative recurrent and terminal events. This approach introduces a shared frailty to specify the explicit dependence structure among the markers, the recurrent, and terminal events. Estimation procedures are developed for the model parameters and the degree of dependence, and a prediction of the covariate-specific cumulative markers is provided. The finite sample performance of the proposed estimators is examined through simulation studies. An application to a medical cost study of chronic heart failure patients from the University of Virginia Health System is illustrated.},
  archive      = {J_BIOMTC},
  author       = {Jie Zhou and Xin Chen and Xinyuan Song and Liuquan Sun},
  doi          = {10.1111/biom.13260},
  journal      = {Biometrics},
  number       = {1},
  pages        = {150-161},
  shortjournal = {Biometrics},
  title        = {A joint modeling approach for analyzing marker data in the presence of a terminal event},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian inference of causal effects from observational data
in gaussian graphical models. <em>BIOMTC</em>, <em>77</em>(1), 136–149.
(<a href="https://doi.org/10.1111/biom.13281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We assume that multivariate observational data are generated from a distribution whose conditional independencies are encoded in a Directed Acyclic Graph (DAG). For any given DAG, the causal effect of a variable onto another one can be evaluated through intervention calculus. A DAG is typically not identifiable from observational data alone. However, its Markov equivalence class (a collection of DAGs) can be estimated from the data. As a consequence, for the same intervention a set of causal effects, one for each DAG in the equivalence class, can be evaluated. In this paper, we propose a fully Bayesian methodology to make inference on the causal effects of any intervention in the system. Main features of our method are: (a) both uncertainty on the equivalence class and the causal effects are jointly modeled; (b) priors on the parameters of the modified Cholesky decomposition of the precision matrices across all DAG models are constructively assigned starting from a unique prior on the complete (unrestricted) DAG; (c) an efficient algorithm to sample from the posterior distribution on graph space is adopted; (d) an objective Bayes approach, requiring virtually no user specification, is used throughout. We demonstrate the merits of our methodology in simulation studies, wherein comparisons with current state-of-the-art procedures turn out to be highly satisfactory. Finally we examine a real data set of gene expressions for Arabidopsis thaliana .},
  archive      = {J_BIOMTC},
  author       = {Federico Castelletti and Guido Consonni},
  doi          = {10.1111/biom.13281},
  journal      = {Biometrics},
  number       = {1},
  pages        = {136-149},
  shortjournal = {Biometrics},
  title        = {Bayesian inference of causal effects from observational data in gaussian graphical models},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian nonparametric model for zero-inflated outcomes:
Prediction, clustering, and causal estimation. <em>BIOMTC</em>,
<em>77</em>(1), 125–135. (<a
href="https://doi.org/10.1111/biom.13244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers are often interested in predicting outcomes, detecting distinct subgroups of their data, or estimating causal treatment effects. Pathological data distributions that exhibit skewness and zero-inflation complicate these tasks—requiring highly flexible, data-adaptive modeling. In this paper, we present a multipurpose Bayesian nonparametric model for continuous, zero-inflated outcomes that simultaneously predicts structural zeros, captures skewness, and clusters patients with similar joint data distributions. The flexibility of our approach yields predictions that capture the joint data distribution better than commonly used zero-inflated methods. Moreover, we demonstrate that our model can be coherently incorporated into a standardization procedure for computing causal effect estimates that are robust to such data pathologies. Uncertainty at all levels of this model flow through to the causal effect estimates of interest—allowing easy point estimation, interval estimation, and posterior predictive checks verifying positivity, a required causal identification assumption. Our simulation results show point estimates to have low bias and interval estimates to have close to nominal coverage under complicated data settings. Under simpler settings, these results hold while incurring lower efficiency loss than comparator methods. We use our proposed method to analyze zero-inflated inpatient medical costs among endometrial cancer patients receiving either chemotherapy or radiation therapy in the SEER-Medicare database.},
  archive      = {J_BIOMTC},
  author       = {Arman Oganisian and Nandita Mitra and Jason A. Roy},
  doi          = {10.1111/biom.13244},
  journal      = {Biometrics},
  number       = {1},
  pages        = {125-135},
  shortjournal = {Biometrics},
  title        = {A bayesian nonparametric model for zero-inflated outcomes: Prediction, clustering, and causal estimation},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On computation of semiparametric maximum likelihood
estimators with shape constraints. <em>BIOMTC</em>, <em>77</em>(1),
113–124. (<a href="https://doi.org/10.1111/biom.13266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large sample theory of semiparametric models based on maximum likelihood estimation (MLE) with shape constraint on the nonparametric component is well studied. Relatively less attention has been paid to the computational aspect of semiparametric MLE. The computation of semiparametric MLE based on existing approaches such as the expectation-maximization (EM) algorithm can be computationally prohibitive when the missing rate is high. In this paper, we propose a computational framework for semiparametric MLE based on an inexact block coordinate ascent (BCA) algorithm. We show theoretically that the proposed algorithm converges. This computational framework can be applied to a wide range of data with different structures, such as panel count data, interval-censored data, and degradation data, among others. Simulation studies demonstrate favorable performance compared with existing algorithms in terms of accuracy and speed. Two data sets are used to illustrate the proposed computational method. We further implement the proposed computational method in R package BCA1SG , available at CRAN.},
  archive      = {J_BIOMTC},
  author       = {Yudong Wang and Zhi-Sheng Ye and Hongyuan Cao},
  doi          = {10.1111/biom.13266},
  journal      = {Biometrics},
  number       = {1},
  pages        = {113-124},
  shortjournal = {Biometrics},
  title        = {On computation of semiparametric maximum likelihood estimators with shape constraints},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Retrospective versus prospective score tests for genetic
association with case-control data. <em>BIOMTC</em>, <em>77</em>(1),
102–112. (<a href="https://doi.org/10.1111/biom.13270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the seminal work of Prentice and Pyke, the prospective logistic likelihood has become the standard method of analysis for retrospectively collected case-control data, in particular for testing the association between a single genetic marker and a disease outcome in genetic case-control studies. In the study of multiple genetic markers with relatively small effects, especially those with rare variants, various aggregated approaches based on the same prospective likelihood have been developed to integrate subtle association evidence among all the markers considered. Many of the commonly used tests are derived from the prospective likelihood under a common-random-effect assumption, which assumes a common random effect for all subjects. We develop the locally most powerful aggregation test based on the retrospective likelihood under an independent-random-effect assumption, which allows the genetic effect to vary among subjects. In contrast to the fact that disease prevalence information cannot be used to improve efficiency for the estimation of odds ratio parameters in logistic regression models, we show that it can be utilized to enhance the testing power in genetic association studies. Extensive simulations demonstrate the advantages of the proposed method over the existing ones. A real genome-wide association study is analyzed for illustration.},
  archive      = {J_BIOMTC},
  author       = {Yukun Liu and Pengfei Li and Lei Song and Kai Yu and Jing Qin},
  doi          = {10.1111/biom.13270},
  journal      = {Biometrics},
  number       = {1},
  pages        = {102-112},
  shortjournal = {Biometrics},
  title        = {Retrospective versus prospective score tests for genetic association with case-control data},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zero-inflated poisson factor model with application to
microbiome read counts. <em>BIOMTC</em>, <em>77</em>(1), 91–101. (<a
href="https://doi.org/10.1111/biom.13272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimension reduction of high-dimensional microbiome data facilitates subsequent analysis such as regression and clustering. Most existing reduction methods cannot fully accommodate the special features of the data such as count-valued and excessive zero reads. We propose a zero-inflated Poisson factor analysis model in this paper. The model assumes that microbiome read counts follow zero-inflated Poisson distributions with library size as offset and Poisson rates negatively related to the inflated zero occurrences. The latent parameters of the model form a low-rank matrix consisting of interpretable loadings and low-dimensional scores that can be used for further analyses. We develop an efficient and robust expectation-maximization algorithm for parameter estimation. We demonstrate the efficacy of the proposed method using comprehensive simulation studies. The application to the Oral Infections, Glucose Intolerance, and Insulin Resistance Study provides valuable insights into the relation between subgingival microbiome and periodontal disease.},
  archive      = {J_BIOMTC},
  author       = {Tianchen Xu and Ryan T. Demmer and Gen Li},
  doi          = {10.1111/biom.13272},
  journal      = {Biometrics},
  number       = {1},
  pages        = {91-101},
  shortjournal = {Biometrics},
  title        = {Zero-inflated poisson factor model with application to microbiome read counts},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian latent multi-state modeling for nonequidistant
longitudinal electronic health records. <em>BIOMTC</em>, <em>77</em>(1),
78–90. (<a href="https://doi.org/10.1111/biom.13261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large amounts of longitudinal health records are now available for dynamic monitoring of the underlying processes governing the observations. However, the health status progression across time is not typically observed directly: records are observed only when a subject interacts with the system, yielding irregular and often sparse observations. This suggests that the observed trajectories should be modeled via a latent continuous-time process potentially as a function of time-varying covariates. We develop a continuous-time hidden Markov model to analyze longitudinal data accounting for irregular visits and different types of observations. By employing a specific missing data likelihood formulation, we can construct an efficient computational algorithm. We focus on Bayesian inference for the model: this is facilitated by an expectation-maximization algorithm and Markov chain Monte Carlo methods. Simulation studies demonstrate that these approaches can be implemented efficiently for large data sets in a fully Bayesian setting. We apply this model to a real cohort where patients suffer from chronic obstructive pulmonary disease with the outcome being the number of drugs taken, using health care utilization indicators and patient characteristics as covariates.},
  archive      = {J_BIOMTC},
  author       = {Yu Luo and David A. Stephens and Aman Verma and David L. Buckeridge},
  doi          = {10.1111/biom.13261},
  journal      = {Biometrics},
  number       = {1},
  pages        = {78-90},
  shortjournal = {Biometrics},
  title        = {Bayesian latent multi-state modeling for nonequidistant longitudinal electronic health records},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Case contamination in electronic health records based
case-control studies. <em>BIOMTC</em>, <em>77</em>(1), 67–77. (<a
href="https://doi.org/10.1111/biom.13264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinically relevant information from electronic health records (EHRs) permits derivation of a rich collection of phenotypes. Unlike traditionally designed studies where scientific hypotheses are specified a priori before data collection, the true phenotype status of any given individual in EHR-based studies is not directly available. Structured and unstructured data elements need to be queried through preconstructed rules to identify case and control groups. A sufficient number of controls can usually be identified with high accuracy by making the selection criteria stringent. But more relaxed criteria are often necessary for more thorough identification of cases to ensure achievable statistical power. The resulting pool of candidate cases consists of genuine cases contaminated with noncase patients who do not satisfy the control definition. The presence of patients who are neither true cases nor controls among the identified cases is a unique challenge in EHR-based case-control studies. Ignoring case contamination would lead to biased estimation of odds ratio association parameters. We propose an estimating equation approach to bias correction, study its large sample property, and evaluate its performance through extensive simulation studies and an application to a pilot study of aortic stenosis in the Penn medicine EHR. Our method holds the promise of facilitating more efficient EHR studies by accommodating enlarged albeit contaminated case pools.},
  archive      = {J_BIOMTC},
  author       = {Lu Wang and Jill Schnall and Aeron Small and Rebecca A. Hubbard and Jason H. Moore and Scott M. Damrauer and Jinbo Chen},
  doi          = {10.1111/biom.13264},
  journal      = {Biometrics},
  number       = {1},
  pages        = {67-77},
  shortjournal = {Biometrics},
  title        = {Case contamination in electronic health records based case-control studies},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing wearable device data using marked point processes.
<em>BIOMTC</em>, <em>77</em>(1), 54–66. (<a
href="https://doi.org/10.1111/biom.13269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces two sets of measures as exploratory tools to study physical activity patterns: active-to-sedentary/sedentary-to-active rate function (ASRF/SARF) and active/sedentary rate function (ARF/SRF). These two sets of measures are complementary to each other and can be effectively used together to understand physical activity patterns. The specific features are illustrated by an analysis of wearable device data from National Health and Nutrition Examination Survey (NHANES). A two-level semiparametric regression model for ARF and the associated activity magnitude is developed under a unified framework using the marked point process formulation. The inactive and active states measured by accelerometers are treated as a 0-1 point process, and the activity magnitude measured at each active state is defined as a marked variable. The commonly encountered missing data problem due to device nonwear is referred to as “window censoring,” which is handled by a proper estimation approach that adopts techniques from recurrent event data. Large sample properties of the estimator and comparison between two regression models as measurement frequency increases are studied. Simulation and NHANES data analysis results are presented. The statistical inference and analysis results suggest that ASRF/SARF and ARF/SRF provide useful analytical tools to practitioners for future research on wearable device data.},
  archive      = {J_BIOMTC},
  author       = {Yuchen Yang and Mei-Cheng Wang},
  doi          = {10.1111/biom.13269},
  journal      = {Biometrics},
  number       = {1},
  pages        = {54-66},
  shortjournal = {Biometrics},
  title        = {Analyzing wearable device data using marked point processes},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder to discussions on “approval policies for
modifications to machine learning-based software as a medical device: A
study of bio-creep.” <em>BIOMTC</em>, <em>77</em>(1), 52–53. (<a
href="https://doi.org/10.1111/biom.13380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We thank the discussants for sharing their unique perspectives on the problem of designing automatic algorithm change protocols (aACPs) for machine learning-based software as a medical device. Both Pennello et al . and Rose highlighted a number of challenges that arise in real-world settings, and we whole-heartedly agree that substantial extensions of our work are needed to understand if and how aACPs can be safely deployed in practice. Our work demonstrated that aACPs that appear to be harmless may allow for biocreep, even when the data distribution is assumed to be representative and stationary over time. While we investigated two solutions that protect against this specific issue, many more statistical and practical challenges remain and we look forward to future research on this topic.},
  archive      = {J_BIOMTC},
  author       = {Jean Feng and Scott Emerson and Noah Simon},
  doi          = {10.1111/biom.13380},
  journal      = {Biometrics},
  number       = {1},
  pages        = {52-53},
  shortjournal = {Biometrics},
  title        = {Rejoinder to discussions on “Approval policies for modifications to machine learning-based software as a medical device: A study of bio-creep”},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “approval policies for modifications to
machine learning-based software as a medical device: A study of
biocreep” by jean feng, scott emerson, and noah simon. <em>BIOMTC</em>,
<em>77</em>(1), 49–51. (<a
href="https://doi.org/10.1111/biom.13378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Sherri Rose},
  doi          = {10.1111/biom.13378},
  journal      = {Biometrics},
  number       = {1},
  pages        = {49-51},
  shortjournal = {Biometrics},
  title        = {Discussion on “Approval policies for modifications to machine learning-based software as a medical device: A study of biocreep” by jean feng, scott emerson, and noah simon},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “approval policies for modifications to
machine learning-based software as a medical device: A study of
bio-creep” by jean feng, scott emerson, and noah simon. <em>BIOMTC</em>,
<em>77</em>(1), 45–48. (<a
href="https://doi.org/10.1111/biom.13381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Gene Pennello and Berkman Sahiner and Alexej Gossmann and Nicholas Petrick},
  doi          = {10.1111/biom.13381},
  journal      = {Biometrics},
  number       = {1},
  pages        = {45-48},
  shortjournal = {Biometrics},
  title        = {Discussion on “Approval policies for modifications to machine learning-based software as a medical device: A study of bio-creep” by jean feng, scott emerson, and noah simon},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Approval policies for modifications to machine
learning-based software as a medical device: A study of bio-creep.
<em>BIOMTC</em>, <em>77</em>(1), 31–44. (<a
href="https://doi.org/10.1111/biom.13379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Successful deployment of machine learning algorithms in healthcare requires careful assessments of their performance and safety. To date, the FDA approves locked algorithms prior to marketing and requires future updates to undergo separate premarket reviews. However, this negates a key feature of machine learning—the ability to learn from a growing dataset and improve over time. This paper frames the design of an approval policy, which we refer to as an automatic algorithmic change protocol (aACP), as an online hypothesis testing problem. As this process has obvious analogy with noninferiority testing of new drugs, we investigate how repeated testing and adoption of modifications might lead to gradual deterioration in prediction accuracy, also known as “biocreep” in the drug development literature. We consider simple policies that one might consider but do not necessarily offer any error-rate guarantees, as well as policies that do provide error-rate control. For the latter, we define two online error-rates appropriate for this context: bad approval count (BAC) and bad approval and benchmark ratios (BABR). We control these rates in the simple setting of a constant population and data source using policies aACP-BAC and aACP-BABR, which combine alpha-investing, group-sequential, and gate-keeping methods. In simulation studies, bio-creep regularly occurred when using policies with no error-rate guarantees, whereas aACP-BAC and aACP-BABR controlled the rate of bio-creep without substantially impacting our ability to approve beneficial modifications.},
  archive      = {J_BIOMTC},
  author       = {Jean Feng and Scott Emerson and Noah Simon},
  doi          = {10.1111/biom.13379},
  journal      = {Biometrics},
  number       = {1},
  pages        = {31-44},
  shortjournal = {Biometrics},
  title        = {Approval policies for modifications to machine learning-based software as a medical device: A study of bio-creep},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder to “nonparametric variable importance assessment
using machine learning techniques.” <em>BIOMTC</em>, <em>77</em>(1),
28–30. (<a href="https://doi.org/10.1111/biom.13389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Brian D. Williamson and Peter B. Gilbert and Marco Carone and Noah Simon},
  doi          = {10.1111/biom.13389},
  journal      = {Biometrics},
  number       = {1},
  pages        = {28-30},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Nonparametric variable importance assessment using machine learning techniques”},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “nonparametric variable importance assessment
using machine learning techniques” by brian d. Williamson, peter b.
Gilbert, marco carone, and noah simon. <em>BIOMTC</em>, <em>77</em>(1),
23–27. (<a href="https://doi.org/10.1111/biom.13391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Min Lu and Hemant Ishwaran},
  doi          = {10.1111/biom.13391},
  journal      = {Biometrics},
  number       = {1},
  pages        = {23-27},
  shortjournal = {Biometrics},
  title        = {Discussion on “Nonparametric variable importance assessment using machine learning techniques” by brian d. williamson, peter b. gilbert, marco carone, and noah simon},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Nonparametric variable importance assessment using machine
learning techniques. <em>BIOMTC</em>, <em>77</em>(1), 9–22. (<a
href="https://doi.org/10.1111/biom.13392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a regression setting, it is often of interest to quantify the importance of various features in predicting the response. Commonly, the variable importance measure used is determined by the regression technique employed. For this reason, practitioners often only resort to one of a few regression techniques for which a variable importance measure is naturally defined. Unfortunately, these regression techniques are often suboptimal for predicting the response. Additionally, because the variable importance measures native to different regression techniques generally have a different interpretation, comparisons across techniques can be difficult. In this work, we study a variable importance measure that can be used with any regression technique, and whose interpretation is agnostic to the technique used. This measure is a property of the true data-generating mechanism. Specifically, we discuss a generalization of the analysis of variance variable importance measure and discuss how it facilitates the use of machine learning techniques to flexibly estimate the variable importance of a single feature or group of features. The importance of each feature or group of features in the data can then be described individually, using this measure. We describe how to construct an efficient estimator of this measure as well as a valid confidence interval. Through simulations, we show that our proposal has good practical operating characteristics, and we illustrate its use with data from a study of risk factors for cardiovascular disease in South Africa.},
  archive      = {J_BIOMTC},
  author       = {Brian D. Williamson and Peter B. Gilbert and Marco Carone and Noah Simon},
  doi          = {10.1111/biom.13392},
  journal      = {Biometrics},
  number       = {1},
  pages        = {9-22},
  shortjournal = {Biometrics},
  title        = {Nonparametric variable importance assessment using machine learning techniques},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Report of the editors—2020. <em>BIOMTC</em>, <em>77</em>(1),
5–8. (<a href="https://doi.org/10.1111/biom.13443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13443},
  journal      = {Biometrics},
  number       = {1},
  pages        = {5-8},
  shortjournal = {Biometrics},
  title        = {Report of the editors—2020},
  volume       = {77},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
