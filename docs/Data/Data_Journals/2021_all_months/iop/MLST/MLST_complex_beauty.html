<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLST_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mlst---125">MLST - 125</h2>
<ul>
<li><details>
<summary>
(2021). Categorical representation learning: Morphism is all you
need. <em>MLST</em>, <em>3</em>(1), 015016. (<a
href="https://doi.org/10.1088/2632-2153/ac2c5d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a construction for categorical representation learning and introduce the foundations of &#39; categorifier &#39;. The central theme in representation learning is the idea of everything to vector. Every object in a dataset \mathcal{S} can be represented as a vector in \mathbb{R}^n by an encoding map E: \mathcal{O}bj(\mathcal{S})\to\mathbb{R}^n . More importantly, every morphism can be represented as a matrix E: \mathcal{H}om(\mathcal{S})\to\mathbb{R}^{n}_{n} . The encoding map E is generally modeled by a deep neural network . The goal of representation learning is to design appropriate tasks on the dataset to train the encoding map (assuming that an encoding is optimal if it universally optimizes the performance on various tasks). However, the latter is still a set-theoretic approach. The goal of the current article is to promote the representation learning to a new level via a category-theoretic approach. As a proof of concept, we provide an example of a text translator equipped with our technology, showing that our categorical learning model outperforms the current deep learning models by 17 times. The content of the current article is part of a US provisional patent application filed by QGNai, Inc.},
  archive      = {J_MLST},
  author       = {Artan Sheshmani and Yi-Zhuang You},
  doi          = {10.1088/2632-2153/ac2c5d},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Categorical representation learning: Morphism is all you need},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable deep learning for the analysis of MHD
spectrograms in nuclear fusion. <em>MLST</em>, <em>3</em>(1), 015015.
(<a href="https://doi.org/10.1088/2632-2153/ac44aa">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the nuclear fusion community, there are many specialized techniques to analyze the data coming from a variety of diagnostics. One of such techniques is the use of spectrograms to analyze the magnetohydrodynamic (MHD) behavior of fusion plasmas. Physicists look at the spectrogram to identify the oscillation modes of the plasma, and to study instabilities that may lead to plasma disruptions. One of the major causes of disruptions occurs when an oscillation mode interacts with the wall, stops rotating, and becomes a locked mode. In this work, we use deep learning to predict the occurrence of locked modes from MHD spectrograms. In particular, we use a convolutional neural network with class activation mapping to pinpoint the exact behavior that the model thinks is responsible for the locked mode. Surprisingly, we find that, in general, the model explanation agrees quite well with the physical interpretation of the behavior observed in the spectrogram.},
  archive      = {J_MLST},
  author       = {Diogo R Ferreira and Tiago A Martins and Paulo Rodrigues and JET Contributors},
  doi          = {10.1088/2632-2153/ac44aa},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Explainable deep learning for the analysis of MHD spectrograms in nuclear fusion},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial domain adaptation to reduce sample bias of a
high energy physics event classifier *. <em>MLST</em>, <em>3</em>(1),
015014. (<a href="https://doi.org/10.1088/2632-2153/ac3dde">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We apply adversarial domain adaptation in unsupervised setting to reduce sample bias in a supervised high energy physics events classifier training. We make use of a neural network containing event and domain classifier with a gradient reversal layer to simultaneously enable signal versus background events classification on the one hand, while on the other hand minimizing the difference in response of the network to background samples originating from different Monte Carlo models via adversarial domain classification loss. We show the successful bias removal on the example of simulated events at the Large Hadron Collider with t\bar{t}H signal versus t\bar{t}b\bar{b} background classification and discuss implications and limitations of the method.},
  archive      = {J_MLST},
  author       = {J M Clavijo and P Glaysher and J Jitsev and J M Katzy},
  doi          = {10.1088/2632-2153/ac3dde},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Adversarial domain adaptation to reduce sample bias of a high energy physics event classifier *},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building high accuracy emulators for scientific simulations
with deep neural architecture search. <em>MLST</em>, <em>3</em>(1),
015013. (<a href="https://doi.org/10.1088/2632-2153/ac3ffa">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer simulations are invaluable tools for scientific discovery. However, accurate simulations are often slow to execute, which limits their applicability to extensive parameter exploration, large-scale data analysis, and uncertainty quantification. A promising route to accelerate simulations by building fast emulators with machine learning requires large training datasets, which can be prohibitively expensive to obtain with slow simulations. Here we present a method based on neural architecture search to build accurate emulators even with a limited number of training data. The method successfully emulates simulations in 10 scientific cases including astrophysics, climate science, biogeochemistry, high energy density physics, fusion energy, and seismology, using the same super-architecture, algorithm, and hyperparameters. Our approach also inherently provides emulator uncertainty estimation, adding further confidence in their use. We anticipate this work will accelerate research involving expensive simulations, allow more extensive parameters exploration, and enable new, previously unfeasible computational discovery.},
  archive      = {J_MLST},
  author       = {M F Kasim and D Watson-Parris and L Deaconu and S Oliver and P Hatfield and D H Froula and G Gregori and M Jarvis and S Khatiwala and J Korenaga and J Topp-Mugglestone and E Viezzer and S M Vinko},
  doi          = {10.1088/2632-2153/ac3ffa},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Building high accuracy emulators for scientific simulations with deep neural architecture search},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Calibrated uncertainty for molecular property prediction
using ensembles of message passing neural networks. <em>MLST</em>,
<em>3</em>(1), 015012. (<a
href="https://doi.org/10.1088/2632-2153/ac3eb3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven methods based on machine learning have the potential to accelerate computational analysis of atomic structures. In this context, reliable uncertainty estimates are important for assessing confidence in predictions and enabling decision making. However, machine learning models can produce badly calibrated uncertainty estimates and it is therefore crucial to detect and handle uncertainty carefully. In this work we extend a message passing neural network designed specifically for predicting properties of molecules and materials with a calibrated probabilistic predictive distribution. The method presented in this paper differs from previous work by considering both aleatoric and epistemic uncertainty in a unified framework, and by recalibrating the predictive distribution on unseen data. Through computer experiments, we show that our approach results in accurate models for predicting molecular formation energies with well calibrated uncertainty in and out of the training data distribution on two public molecular benchmark datasets, QM9 and PC9. The proposed method provides a general framework for training and evaluating neural network ensemble models that are able to produce accurate predictions of properties of molecules with well calibrated uncertainty estimates.},
  archive      = {J_MLST},
  author       = {Jonas Busk and Peter Bjørn Jørgensen and Arghya Bhowmik and Mikkel N Schmidt and Ole Winther and Tejs Vegge},
  doi          = {10.1088/2632-2153/ac3eb3},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Calibrated uncertainty for molecular property prediction using ensembles of message passing neural networks},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning the derivative discontinuity of
density-functional theory. <em>MLST</em>, <em>3</em>(1), 015011. (<a
href="https://doi.org/10.1088/2632-2153/ac3149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is a powerful tool to design accurate, highly non-local, exchange-correlation functionals for density functional theory. So far, most of those machine learned functionals are trained for systems with an integer number of particles. As such, they are unable to reproduce some crucial and fundamental aspects, such as the explicit dependency of the functionals on the particle number or the infamous derivative discontinuity at integer particle numbers. Here we propose a solution to these problems by training a neural network as the universal functional of density-functional theory that (a) depends explicitly on the number of particles with a piece-wise linearity between the integer numbers and (b) reproduces the derivative discontinuity of the exchange-correlation energy. This is achieved by using an ensemble formalism, a training set containing fractional densities, and an explicitly discontinuous formulation.},
  archive      = {J_MLST},
  author       = {Johannes Gedeon and Jonathan Schmidt and Matthew J P Hodgson and Jack Wetherell and Carlos L Benavides-Riveros and Miguel A L Marques},
  doi          = {10.1088/2632-2153/ac3149},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning the derivative discontinuity of density-functional theory},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Restricted boltzmann machine representation for the
groundstate and excited states of kitaev honeycomb model. <em>MLST</em>,
<em>3</em>(1), 015010. (<a
href="https://doi.org/10.1088/2632-2153/ac3ddf">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the capability of restricted Boltzmann machines (RBMs) to find solutions for the Kitaev honeycomb model with periodic boundary conditions is investigated. The measured groundstate energy of the system is compared and, for small lattice sizes (e.g. 3 \times 3 with 18 spinors), shown to agree with the analytically derived value of the energy up to a deviation of 0.09\% . Moreover, the wave-functions we find have 99.89\% overlap with the exact ground state wave-functions. Furthermore, the possibility of realizing anyons in the RBM is discussed and an algorithm is given to build these anyonic excitations and braid them for possible future applications in quantum computation. Using the correspondence between topological field theories in (2 + 1)d and 2d conformal field theories, we propose an identification between our RBM states with the Moore-Read state and conformal blocks of the 2d Ising model.},
  archive      = {J_MLST},
  author       = {Mohammadreza Noormandipour and Sun Youran and Babak Haghighat},
  doi          = {10.1088/2632-2153/ac3ddf},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Restricted boltzmann machine representation for the groundstate and excited states of kitaev honeycomb model},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deeptime: A python library for machine learning dynamical
models from time series data. <em>MLST</em>, <em>3</em>(1), 015009. (<a
href="https://doi.org/10.1088/2632-2153/ac3de0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generation and analysis of time-series data is relevant to many quantitative fields ranging from economics to fluid mechanics. In the physical sciences, structures such as metastable and coherent sets, slow relaxation processes, collective variables, dominant transition pathways or manifolds and channels of probability flow can be of great importance for understanding and characterizing the kinetic, thermodynamic and mechanistic properties of the system. Deeptime is a general purpose Python library offering various tools to estimate dynamical models based on time-series data including conventional linear learning methods, such as Markov state models (MSMs), Hidden Markov Models and Koopman models, as well as kernel and deep learning approaches such as VAMPnets and deep MSMs. The library is largely compatible with scikit-learn, having a range of Estimator classes for these different models, but in contrast to scikit-learn also provides deep Model classes, e.g. in the case of an MSM, which provide a multitude of analysis methods to compute interesting thermodynamic, kinetic and dynamical quantities, such as free energies, relaxation times and transition paths. The library is designed for ease of use but also easily maintainable and extensible code. In this paper we introduce the main features and structure of the deeptime software. Deeptime can be found under https://deeptime-ml.github.io/ .},
  archive      = {J_MLST},
  author       = {Moritz Hoffmann and Martin Scherer and Tim Hempel and Andreas Mardt and Brian de Silva and Brooke E Husic and Stefan Klus and Hao Wu and Nathan Kutz and Steven L Brunton and Frank Noé},
  doi          = {10.1088/2632-2153/ac3de0},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deeptime: A python library for machine learning dynamical models from time series data},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating stable molecules using imitation and
reinforcement learning. <em>MLST</em>, <em>3</em>(1), 015008. (<a
href="https://doi.org/10.1088/2632-2153/ac3eb4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chemical space is routinely explored by machine learning methods to discover interesting molecules, before time-consuming experimental synthesizing is attempted. However, these methods often rely on a graph representation, ignoring 3D information necessary for determining the stability of the molecules. We propose a reinforcement learning (RL) approach for generating molecules in Cartesian coordinates allowing for quantum chemical prediction of the stability. To improve sample-efficiency we learn basic chemical rules from imitation learning (IL) on the GDB-11 database to create an initial model applicable for all stoichiometries. We then deploy multiple copies of the model conditioned on a specific stoichiometry in a RL setting. The models correctly identify low energy molecules in the database and produce novel isomers not found in the training set. Finally, we apply the model to larger molecules to show how RL further refines the IL model in domains far from the training data.},
  archive      = {J_MLST},
  author       = {Søren Ager Meldgaard and Jonas Köhler and Henrik Lund Mortensen and Mads-Peter V Christiansen and Frank Noé and Bjørk Hammer},
  doi          = {10.1088/2632-2153/ac3eb4},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Generating stable molecules using imitation and reinforcement learning},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistically-informed deep learning for gravitational wave
parameter estimation. <em>MLST</em>, <em>3</em>(1), 015007. (<a
href="https://doi.org/10.1088/2632-2153/ac3843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce deep learning models to estimate the masses of the binary components of black hole mergers, (m_1,m_2) , and three astrophysical properties of the post-merger compact remnant, namely, the final spin, a_\mathrm f , and the frequency and damping time of the ringdown oscillations of the fundamental \ell = m = 2 bar mode, (\omega_\mathrm R, \omega_\mathrm I) . Our neural networks combine a modified WaveNet architecture with contrastive learning and normalizing flow. We validate these models against a Gaussian conjugate prior family whose posterior distribution is described by a closed analytical expression. Upon confirming that our models produce statistically consistent results, we used them to estimate the astrophysical parameters (m_1,m_2, a_\mathrm f, \omega_\mathrm R, \omega_\mathrm I) of five binary black holes: GW150914 , GW170104 , GW170814 , GW190521 and GW190630 . We use PyCBC Inference to directly compare traditional Bayesian methodologies for parameter estimation with our deep learning based posterior distributions. Our results show that our neural network models predict posterior distributions that encode physical correlations, and that our data-driven median results and 90% confidence intervals are similar to those produced with gravitational wave Bayesian analyses. This methodology requires a single V100 NVIDIA GPU to produce median values and posterior distributions within two milliseconds for each event. This neural network, and a tutorial for its use, are available at the Data and Learning Hub for Science .},
  archive      = {J_MLST},
  author       = {Hongyu Shen and E A Huerta and Eamonn O’Shea and Prayush Kumar and Zhizhen Zhao},
  doi          = {10.1088/2632-2153/ac3843},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {1},
  pages        = {015007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Statistically-informed deep learning for gravitational wave parameter estimation},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multi-task mining calabi–yau four-folds. <em>MLST</em>,
<em>3</em>(1), 015006. (<a
href="https://doi.org/10.1088/2632-2153/ac37f7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We continue earlier efforts in computing the dimensions of tangent space cohomologies of Calabi–Yau manifolds using deep learning. In this paper, we consider the dataset of all Calabi–Yau four-folds constructed as complete intersections in products of projective spaces. Employing neural networks inspired by state-of-the-art computer vision architectures, we improve earlier benchmarks and demonstrate that all four non-trivial Hodge numbers can be learned at the same time using a multi-task architecture. With 30% (80%) training ratio, we reach an accuracy of 100% for h^{(1,1)} and 97% for h^{(2,1)} (100% for both), 81% (96%) for h^{(3,1)} , and 49% (83%) for h^{(2,2)} . Assuming that the Euler number is known, as it is easy to compute, and taking into account the linear constraint arising from index computations, we get 100% total accuracy.},
  archive      = {J_MLST},
  author       = {Harold Erbin and Riccardo Finotello and Robin Schneider and Mohamed Tamaazousti},
  doi          = {10.1088/2632-2153/ac37f7},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {1},
  pages        = {015006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep multi-task mining Calabi–Yau four-folds},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gradient domain machine learning with composite kernels:
Improving the accuracy of PES and force fields for large molecules.
<em>MLST</em>, <em>3</em>(1), 015005. (<a
href="https://doi.org/10.1088/2632-2153/ac3845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalization accuracy of machine learning models of potential energy surfaces (PES) and force fields (FF) for large polyatomic molecules can be improved either by increasing the number of training points or by improving the models. In order to build accurate models based on expensive ab initio calculations, much of recent work has focused on the latter. In particular, it has been shown that gradient domain machine learning (GDML) models produce accurate results for high-dimensional molecular systems with a small number of ab initio calculations. The present work extends GDML to models with composite kernels built to maximize inference from a small number of molecular geometries. We illustrate that GDML models can be improved by increasing the complexity of underlying kernels through a greedy search algorithm using Bayesian information criterion as the model selection metric. We show that this requires including anisotropy into kernel functions and produces models with significantly smaller generalization errors. The results are presented for ethanol, uracil, malonaldehyde and aspirin. For aspirin, the model with composite kernels trained by forces at 1000 randomly sampled molecular geometries produces a global 57-dimensional PES with the mean absolute accuracy 0.177 kcal mol −1 (61.9 cm −1 ) and FFs with the mean absolute error 0.457 kcal mol −1 Å −1 .},
  archive      = {J_MLST},
  author       = {K Asnaashari and R V Krems},
  doi          = {10.1088/2632-2153/ac3845},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {1},
  pages        = {015005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Gradient domain machine learning with composite kernels: Improving the accuracy of PES and force fields for large molecules},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Achieving robustness to aleatoric uncertainty with
heteroscedastic bayesian optimisation. <em>MLST</em>, <em>3</em>(1),
015004. (<a href="https://doi.org/10.1088/2632-2153/ac298c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimisation is a sample-efficient search methodology that holds great promise for accelerating drug and materials discovery programs. A frequently-overlooked modelling consideration in Bayesian optimisation strategies however, is the representation of heteroscedastic aleatoric uncertainty. In many practical applications it is desirable to identify inputs with low aleatoric noise, an example of which might be a material composition which displays robust properties in response to a noisy fabrication process. In this paper, we propose a heteroscedastic Bayesian optimisation scheme capable of representing and minimising aleatoric noise across the input space. Our scheme employs a heteroscedastic Gaussian process surrogate model in conjunction with two straightforward adaptations of existing acquisition functions. First, we extend the augmented expected improvement heuristic to the heteroscedastic setting and second, we introduce the aleatoric noise-penalised expected improvement (ANPEI) heuristic. Both methodologies are capable of penalising aleatoric noise in the suggestions. In particular, the ANPEI acquisition yields improved performance relative to homoscedastic Bayesian optimisation and random sampling on toy problems as well as on two real-world scientific datasets. Code is available at: https://github.com/Ryan-Rhys/Heteroscedastic-BO},
  archive      = {J_MLST},
  author       = {Ryan-Rhys Griffiths and Alexander A Aldrick and Miguel Garcia-Ortegon and Vidhi Lalchand and Alpha A Lee},
  doi          = {10.1088/2632-2153/ac298c},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {1},
  pages        = {015004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Achieving robustness to aleatoric uncertainty with heteroscedastic bayesian optimisation},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hessian-based toolbox for reliable and interpretable machine
learning in physics. <em>MLST</em>, <em>3</em>(1), 015002. (<a
href="https://doi.org/10.1088/2632-2153/ac338d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) techniques applied to quantum many-body physics have emerged as a new research field. While the numerical power of this approach is undeniable, the most expressive ML algorithms, such as neural networks, are black boxes: The user does neither know the logic behind the model predictions nor the uncertainty of the model predictions. In this work, we present a toolbox for interpretability and reliability, agnostic of the model architecture. In particular, it provides a notion of the influence of the input data on the prediction at a given test point, an estimation of the uncertainty of the model predictions, and an extrapolation score for the model predictions. Such a toolbox only requires a single computation of the Hessian of the training loss function. Our work opens the road to the systematic use of interpretability and reliability methods in ML applied to physics and, more generally, science.},
  archive      = {J_MLST},
  author       = {Anna Dawid and Patrick Huembeli and Michał Tomza and Maciej Lewenstein and Alexandre Dauphin},
  doi          = {10.1088/2632-2153/ac338d},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {1},
  pages        = {015002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Hessian-based toolbox for reliable and interpretable machine learning in physics},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Miniaturizing neural networks for charge state autotuning in
quantum dots. <em>MLST</em>, <em>3</em>(1), 015001. (<a
href="https://doi.org/10.1088/2632-2153/ac34db">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in scaling quantum computers is the calibration and control of multiple qubits. In solid-state quantum dots (QDs), the gate voltages required to stabilize quantized charges are unique for each individual qubit, resulting in a high-dimensional control parameter space that must be tuned automatically. Machine learning techniques are capable of processing high-dimensional data—provided that an appropriate training set is available—and have been successfully used for autotuning in the past. In this paper, we develop extremely small feed-forward neural networks that can be used to detect charge-state transitions in QD stability diagrams. We demonstrate that these neural networks can be trained on synthetic data produced by computer simulations, and robustly transferred to the task of tuning an experimental device into a desired charge state. The neural networks required for this task are sufficiently small as to enable an implementation in existing memristor crossbar arrays in the near future. This opens up the possibility of miniaturizing powerful control elements on low-power hardware, a significant step towards on-chip autotuning in future QD computers.},
  archive      = {J_MLST},
  author       = {Stefanie Czischek and Victor Yon and Marc-Antoine Genest and Marc-Antoine Roux and Sophie Rochette and Julien Camirand Lemyre and Mathieu Moras and Michel Pioro-Ladrière and Dominique Drouin and Yann Beilliard and Roger G Melko},
  doi          = {10.1088/2632-2153/ac34db},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {1},
  pages        = {015001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Miniaturizing neural networks for charge state autotuning in quantum dots},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based quantum tomography. <em>MLST</em>,
<em>3</em>(1), 01LT01. (<a
href="https://doi.org/10.1088/2632-2153/ac362b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid progress across platforms for quantum systems, the problem of many-body quantum state reconstruction for noisy quantum states becomes an important challenge. There has been a growing interest in approaching the problem of quantum state reconstruction using generative neural network models. Here we propose the &#39;attention-based quantum tomography&#39; (AQT), a quantum state reconstruction using an attention mechanism-based generative network that learns the mixed state density matrix of a noisy quantum state. AQT is based on the model proposed in &#39;Attention is all you need&#39; by Vaswani et al (2017 NIPS ) that is designed to learn long-range correlations in natural language sentences and thereby outperform previous natural language processing (NLP) models. We demonstrate not only that AQT outperforms earlier neural-network-based quantum state reconstruction on identical tasks but that AQT can accurately reconstruct the density matrix associated with a noisy quantum state experimentally realized in an IBMQ quantum computer. We speculate the success of the AQT stems from its ability to model quantum entanglement across the entire quantum system much as the attention model for NLP captures the correlations among words in a sentence.},
  archive      = {J_MLST},
  author       = {Peter Cha and Paul Ginsparg and Felix Wu and Juan Carrasquilla and Peter L McMahon and Eun-Ah Kim},
  doi          = {10.1088/2632-2153/ac362b},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {1},
  pages        = {01LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Attention-based quantum tomography},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning approach to force reconstruction in
photoelastic materials. <em>MLST</em>, <em>2</em>(4), 045030. (<a
href="https://doi.org/10.1088/2632-2153/ac29d5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photoelastic techniques have a long tradition in both qualitative and quantitative analysis of the stresses in granular materials. Over the last two decades, computational methods for reconstructing forces between particles from their photoelastic response have been developed by many different experimental teams. Unfortunately, all of these methods are computationally expensive. This limits their use for processing extensive data sets that capture the time evolution of granular ensembles consisting of a large number of particles. In this paper, we present a novel approach to this problem that leverages the power of convolutional neural networks to recognize complex spatial patterns. The main drawback of using neural networks is that training them usually requires a large labeled data set which is hard to obtain experimentally. We show that this problem can be successfully circumvented by pretraining the networks on a large synthetic data set and then fine-tuning them on much smaller experimental data sets. Due to our current lack of experimental data, we demonstrate the potential of our method by changing the size of the considered particles which alters the exhibited photoelastic patterns more than typical experimental errors.},
  archive      = {J_MLST},
  author       = {Renat Sergazinov and Miroslav Kramár},
  doi          = {10.1088/2632-2153/ac29d5},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045030},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning approach to force reconstruction in photoelastic materials},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting polarizabilities of silicon clusters using local
chemical environments. <em>MLST</em>, <em>2</em>(4), 045029. (<a
href="https://doi.org/10.1088/2632-2153/ac2cfe">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calculating polarizabilities of large clusters with first-principles techniques is challenging because of the unfavorable scaling of computational cost with cluster size. To address this challenge, we demonstrate that polarizabilities of large hydrogenated silicon clusters containing thousands of atoms can be efficiently calculated with machine learning methods. Specifically, we construct machine learning models based on the smooth overlap of atomic positions (SOAP) descriptor and train the models using a database of calculated random-phase approximation polarizabilities for clusters containing up to 110 silicon atoms. We first demonstrate the ability of the machine learning models to fit the data and then assess their ability to predict cluster polarizabilities using k-fold cross validation. Finally, we study the machine learning predictions for clusters that are too large for explicit first-principles calculations and find that they accurately describe the dependence of the polarizabilities on the ratio of hydrogen to silicon atoms and also predict a bulk limit that is in good agreement with previous studies.},
  archive      = {J_MLST},
  author       = {Mario G Zauchner and Stefano Dal Forno and Gábor Cśanyi and Andrew Horsfield and Johannes Lischner},
  doi          = {10.1088/2632-2153/ac2cfe},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045029},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Predicting polarizabilities of silicon clusters using local chemical environments},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decoding the shift-invariant data: Applications for
band-excitation scanning probe microscopy *. <em>MLST</em>,
<em>2</em>(4), 045028. (<a
href="https://doi.org/10.1088/2632-2153/ac28de">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A shift-invariant variational autoencoder (shift-VAE) is developed as an unsupervised method for the analysis of spectral data in the presence of shifts along the parameter axis, disentangling the physically-relevant shifts from other latent variables. Using synthetic data sets, we show that the shift-VAE latent variables closely match the ground truth parameters. The shift VAE is extended towards the analysis of band-excitation piezoresponse force microscopy data, disentangling the resonance frequency shifts from the peak shape parameters in a model-free unsupervised manner. The extensions of this approach towards denoising of data and model-free dimensionality reduction in imaging and spectroscopic data are further demonstrated. This approach is universal and can also be extended to analysis of x-ray diffraction, photoluminescence, Raman spectra, and other data sets.},
  archive      = {J_MLST},
  author       = {Yongtao Liu and Rama K Vasudevan and Kyle K Kelley and Dohyung Kim and Yogesh Sharma and Mahshid Ahmadi and Sergei V Kalinin and Maxim Ziatdinov},
  doi          = {10.1088/2632-2153/ac28de},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045028},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Decoding the shift-invariant data: Applications for band-excitation scanning probe microscopy *},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural predictor based quantum architecture search.
<em>MLST</em>, <em>2</em>(4), 045027. (<a
href="https://doi.org/10.1088/2632-2153/ac28dd">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational quantum algorithms (VQAs) are widely speculated to deliver quantum advantages for practical problems under the quantum–classical hybrid computational paradigm in the near term. Both theoretical and practical developments of VQAs share many similarities with those of deep learning. For instance, a key component of VQAs is the design of task-dependent parameterized quantum circuits (PQCs) as in the case of designing a good neural architecture in deep learning. Partly inspired by the recent success of AutoML and neural architecture search (NAS), quantum architecture search (QAS) is a collection of methods devised to engineer an optimal task-specific PQC. It has been proven that QAS-designed VQAs can outperform expert-crafted VQAs in various scenarios. In this work, we propose to use a neural network based predictor as the evaluation policy for QAS. We demonstrate a neural predictor guided QAS can discover powerful quantum circuit ansatz, yielding state-of-the-art results for various examples from quantum simulation and quantum machine learning. Notably, neural predictor guided QAS provides a better solution than that by the random-search baseline while using an order of magnitude less of circuit evaluations. Moreover, the predictor for QAS as well as the optimal ansatz found by QAS can both be transferred and generalized to address similar problems.},
  archive      = {J_MLST},
  author       = {Shi-Xin Zhang and Chang-Yu Hsieh and Shengyu Zhang and Hong Yao},
  doi          = {10.1088/2632-2153/ac28dd},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045027},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural predictor based quantum architecture search},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Entangled q-convolutional neural nets. <em>MLST</em>,
<em>2</em>(4), 045026. (<a
href="https://doi.org/10.1088/2632-2153/ac2800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a machine learning model, the q-CNN model , sharing key features with convolutional neural networks and admitting a tensor network description. As examples, we apply q-CNN to the MNIST and Fashion MNIST classification tasks. We explain how the network associates a quantum state to each classification label, and study the entanglement structure of these network states. In both our experiments on the MNIST and Fashion-MNIST datasets, we observe a distinct increase in both the left/right as well as the up/down bipartition entanglement entropy (EE) during training as the network learns the fine features of the data. More generally, we observe a universal negative correlation between the value of the EE and the value of the cost function, suggesting that the network needs to learn the entanglement structure in order the perform the task accurately. This supports the possibility of exploiting the entanglement structure as a guide to design the machine learning algorithm suitable for given tasks.},
  archive      = {J_MLST},
  author       = {Vassilis Anagiannis and Miranda C N Cheng},
  doi          = {10.1088/2632-2153/ac2800},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045026},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Entangled q-convolutional neural nets},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving surrogate model accuracy for the LCLS-II injector
frontend using convolutional neural networks and transfer learning.
<em>MLST</em>, <em>2</em>(4), 045025. (<a
href="https://doi.org/10.1088/2632-2153/ac27ff">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) models of accelerator systems (&#39;surrogate models&#39;) are able to provide fast, accurate predictions of accelerator physics phenomena. However, approaches to date typically do not include measured input diagnostics, such as the initial beam distributions, which are critical for accurately representing the beam evolution through the system. In addition, these inputs often vary over time, and models that can account for these changing conditions are needed. As beam time for measurements is often limited, simulations are in some cases needed to provide sufficient training data. These typically represent the designed machine before construction; however, the behavior of the installed components may be quite different due to changes over time or static differences that were not modeled. Therefore, surrogate models that can leverage both simulation and measured data successfully are needed. We introduce an approach based on convolutional neural networks that uses the drive laser distribution and scalar settings as inputs for a photoinjector system model (here, the linac coherent light source II, LCLS-II, injector frontend). The model is able to predict scalar beam parameters and the transverse beam distribution downstream, taking into account the impact of time-varying non-uniformities in the initial transverse laser distribution. We also introduce and evaluate a transfer learning procedure for adapting the surrogate model from the simulation domain to the measurement domain, to account for differences between the two. Applying this approach to our test case results in a model that can predict test sample outputs within a mean absolute percent error of 9%. This is a substantial improvement over the model trained only on simulations, which has an error of 261% when applied to measured data. While we focus on the LCLS-II Injector frontend, these approaches for improving ML-based online modeling of injector systems could be easily adapted to other accelerator facilities.},
  archive      = {J_MLST},
  author       = {Lipi Gupta and Auralee Edelen and Nicole Neveu and Aashwin Mishra and Christopher Mayes and Young-Kee Kim},
  doi          = {10.1088/2632-2153/ac27ff},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045025},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving surrogate model accuracy for the LCLS-II injector frontend using convolutional neural networks and transfer learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rapid parameter estimation of discrete decaying signals
using autoencoder networks. <em>MLST</em>, <em>2</em>(4), 045024. (<a
href="https://doi.org/10.1088/2632-2153/ac1eea">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we demonstrate the use of neural networks for rapid extraction of signal parameters of discretely sampled signals. In particular, we use dense autoencoder networks to extract the parameters of interest from exponentially decaying signals and decaying oscillations. By using a three-stage training method and careful choice of the neural network size, we are able to retrieve the relevant signal parameters directly from the latent space of the autoencoder network at significantly improved rates compared to traditional algorithmic signal-analysis approaches. We show that the achievable precision and accuracy of this method of analysis is similar to conventional algorithm-based signal analysis methods, by demonstrating that the extracted signal parameters are approaching their fundamental parameter estimation limit as provided by the Cramér–Rao bound. Furthermore, we demonstrate that autoencoder networks are able to achieve signal analysis, and, hence, parameter extraction, at rates of 75 kHz, orders-of-magnitude faster than conventional techniques with similar precision. Finally, our exploration of the limitations of our approach in different computational systems suggests that analysis rates of \gt 200 kHz are feasible using neural networks in systems where the transfer time between the data-acquisition system and data-analysis modules can be kept below ∼3 µ s.},
  archive      = {J_MLST},
  author       = {Jim C Visschers and Dmitry Budker and Lykourgos Bougas},
  doi          = {10.1088/2632-2153/ac1eea},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045024},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Rapid parameter estimation of discrete decaying signals using autoencoder networks},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of synthetic and experimental training data in
supervised machine learning applied to charge-state detection of quantum
dots. <em>MLST</em>, <em>2</em>(4), 045023. (<a
href="https://doi.org/10.1088/2632-2153/ac104c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated tuning of gate-defined quantum dots is a requirement for large-scale semiconductor-based qubit initialisation. An essential step of these tuning procedures is charge-state detection based on charge stability diagrams. Using supervised machine learning to perform this task requires a large dataset for models to train on. In order to avoid hand labelling experimental data, synthetic data has been explored as an alternative. While providing a significant increase in the size of the training dataset compared to using experimental data, using synthetic data means that classifiers are trained on data sourced from a different distribution than the experimental data that is part of the tuning process. Here we evaluate the prediction accuracy of a range of machine learning models trained on simulated and experimental data, and their ability to generalise to experimental charge stability diagrams in two-dimensional electron gas and nanowire devices. We find that classifiers perform best on either purely experimental or a combination of synthetic and experimental training data, and that adding common experimental noise signatures to the synthetic data does not dramatically improve the classification accuracy. These results suggest that experimental training data as well as realistic quantum dot simulations and noise models are essential in charge-state detection using supervised machine learning.},
  archive      = {J_MLST},
  author       = {J Darulová and M Troyer and M C Cassidy},
  doi          = {10.1088/2632-2153/ac104c},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045023},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Evaluation of synthetic and experimental training data in supervised machine learning applied to charge-state detection of quantum dots},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new formulation of gradient boosting. <em>MLST</em>,
<em>2</em>(4), 045022. (<a
href="https://doi.org/10.1088/2632-2153/ac1ee9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the setting of regression, the standard formulation of gradient boosting generates a sequence of improvements to a constant model. In this paper, we reformulate gradient boosting such that it is able to generate a sequence of improvements to a nonconstant model, which may contain prior knowledge or physical insight about the data generating process. Moreover, we introduce a simple variant of multi-target stacking that extends our approach to the setting of multi-target regression. An experiment on a real-world superconducting quantum device calibration dataset demonstrates that our approach outperforms the state-of-the-art calibration model even though it only receives a paucity of training examples. Further, it significantly outperforms a well-known gradient boosting algorithm, known as LightGBM, as well as an entirely data-driven reimplementation of the calibration model, which suggests the viability of our approach.},
  archive      = {J_MLST},
  author       = {Alex Wozniakowski and Jayne Thompson and Mile Gu and Felix C Binder},
  doi          = {10.1088/2632-2153/ac1ee9},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045022},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A new formulation of gradient boosting},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An end-to-end trainable hybrid classical-quantum classifier.
<em>MLST</em>, <em>2</em>(4), 045021. (<a
href="https://doi.org/10.1088/2632-2153/ac104d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a hybrid model combining a quantum-inspired tensor network and a variational quantum circuit to perform supervised learning tasks. This architecture allows for the classical and quantum parts of the model to be trained simultaneously, providing an end-to-end training framework. We show that compared to the principal component analysis, a tensor network based on the matrix product state with low bond dimensions performs better as a feature extractor for the input data of the variational quantum circuit in the binary and ternary classification of MNIST and Fashion-MNIST datasets. The architecture is highly adaptable and the classical-quantum boundary can be adjusted according to the availability of the quantum resource by exploiting the correspondence between tensor networks and quantum circuits.},
  archive      = {J_MLST},
  author       = {Samuel Yen-Chi Chen and Chih-Min Huang and Chia-Wei Hsing and Ying-Jer Kao},
  doi          = {10.1088/2632-2153/ac104d},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045021},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An end-to-end trainable hybrid classical-quantum classifier},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph convolutional networks applied to unstructured flow
field data. <em>MLST</em>, <em>2</em>(4), 045020. (<a
href="https://doi.org/10.1088/2632-2153/ac1fc9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific and engineering processes produce spatially unstructured data. However, most data-driven models require a feature matrix that enforces both a set number and order of features for each sample. They thus cannot be easily constructed for an unstructured dataset. Therefore, a graph based data-driven model to perform inference on fields defined on an unstructured mesh, using a graph convolutional neural network (GCNN) is presented. The ability of the method to predict global properties from spatially irregular measurements with high accuracy is demonstrated by predicting the drag force associated with laminar flow around airfoils from scattered velocity measurements. The network can infer from field samples at different resolutions, and is invariant to the order in which the measurements within each sample are presented. The GCNN method, using inductive convolutional layers and adaptive pooling, is able to predict this quantity with a validation R 2 above 0.98, and a Normalized Mean Squared Error below 0.01, without relying on spatial structure.},
  archive      = {J_MLST},
  author       = {Francis Ogoke and Kazem Meidani and Amirreza Hashemi and Amir Barati Farimani},
  doi          = {10.1088/2632-2153/ac1fc9},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045020},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Graph convolutional networks applied to unstructured flow field data},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast 3D particle reconstruction using a convolutional neural
network: Application to dusty plasmas. <em>MLST</em>, <em>2</em>(4),
045019. (<a href="https://doi.org/10.1088/2632-2153/ac1fc8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm to reconstruct the three-dimensional positions of particles in a dense cloud of particles in a dusty plasma using a convolutional neural network. The approach is found to be very fast and yields a relatively high accuracy. In this paper, we describe and examine the approach regarding the particle number and the reconstruction accuracy using synthetic data and experimental data. To show the applicability of the approach the 3D positions of particles in a dense dust cloud in a dusty plasma under weightlessness are reconstructed from stereoscopic camera images using the prescribed neural network.},
  archive      = {J_MLST},
  author       = {Michael Himpel and André Melzer},
  doi          = {10.1088/2632-2153/ac1fc8},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045019},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Fast 3D particle reconstruction using a convolutional neural network: Application to dusty plasmas},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep reinforcement learning for predicting kinetic pathways
to surface reconstruction in a ternary alloy. <em>MLST</em>,
<em>2</em>(4), 045018. (<a
href="https://doi.org/10.1088/2632-2153/ac191c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of computational catalyst design focuses on the screening of material components and alloy composition to optimize selectivity and activity for a given reaction. However, predicting the metastability of the alloy catalyst surface at realistic operating conditions requires an extensive sampling of possible surface reconstructions and their associated kinetic pathways. We present CatGym, a deep reinforcement learning (DRL) environment for predicting the thermal surface reconstruction pathways and their associated kinetic barriers in crystalline solids under reaction conditions. The DRL agent iteratively changes the positions of atoms in the near-surface region to generate kinetic pathways to accessible local minima involving changes in the surface compositions. We showcase our agent by predicting the surface reconstruction pathways of a ternary Ni 3 Pd 3 Au 2 (111) alloy catalyst. Our results show that the DRL agent can not only explore more diverse surface compositions than the conventional minima hopping method, but also generate the kinetic surface reconstruction pathways. We further demonstrate that the kinetic pathway to a global minimum energy surface composition and its associated transition state predicted by our agent is in good agreement with the minimum energy path predicted by nudged elastic band calculations.},
  archive      = {J_MLST},
  author       = {Junwoong Yoon and Zhonglin Cao and Rajesh K Raju and Yuyang Wang and Robert Burnley and Andrew J Gellman and Amir Barati Farimani and Zachary W Ulissi},
  doi          = {10.1088/2632-2153/ac191c},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {4},
  pages        = {045018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep reinforcement learning for predicting kinetic pathways to surface reconstruction in a ternary alloy},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint ptycho-tomography with deep generative priors.
<em>MLST</em>, <em>2</em>(4), 045017. (<a
href="https://doi.org/10.1088/2632-2153/ac1d35">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint ptycho-tomography is a powerful computational imaging framework to recover the refractive properties of a 3D object while relaxing the requirements for probe overlap that is common in conventional phase retrieval. We use an augmented Lagrangian scheme for formulating the constrained optimization problem and employ an alternating direction method of multipliers (ADMM) for the joint solution. ADMM allows the problem to be split into smaller and computationally more efficient subproblems: ptychographic phase retrieval, tomographic reconstruction, and regularization of the solution. We extend our ADMM framework with plug-and-play (PnP) denoisers by replacing the regularization subproblem with a general denoising operator based on machine learning. While the PnP framework enables integrating such learned priors as denoising operators, tuning of the denoiser prior remains challenging. To overcome this challenge, we propose a denoiser parameter to control the effect of the denoiser and to accelerate the solution. In our simulations, we demonstrate that our proposed framework with parameter tuning and learned priors generates high-quality reconstructions under limited and noisy measurement data.},
  archive      = {J_MLST},
  author       = {Selin Aslan and Zhengchun Liu and Viktor Nikitin and Tekin Bicer and Sven Leyffer and Doğa Gürsoy},
  doi          = {10.1088/2632-2153/ac1d35},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {4},
  pages        = {045017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Joint ptycho-tomography with deep generative priors},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Symmetric and antisymmetric kernels for machine learning
problems in quantum physics and chemistry. <em>MLST</em>, <em>2</em>(4),
045016. (<a href="https://doi.org/10.1088/2632-2153/ac14ad">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive symmetric and antisymmetric kernels by symmetrizing and antisymmetrizing conventional kernels and analyze their properties. In particular, we compute the feature space dimensions of the resulting polynomial kernels, prove that the reproducing kernel Hilbert spaces induced by symmetric and antisymmetric Gaussian kernels are dense in the space of symmetric and antisymmetric functions, and propose a Slater determinant representation of the antisymmetric Gaussian kernel, which allows for an efficient evaluation even if the state space is high-dimensional. Furthermore, we show that by exploiting symmetries or antisymmetries the size of the training data set can be significantly reduced. The results are illustrated with guiding examples and simple quantum physics and chemistry applications.},
  archive      = {J_MLST},
  author       = {Stefan Klus and Patrick Gelß and Feliks Nüske and Frank Noé},
  doi          = {10.1088/2632-2153/ac14ad},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {4},
  pages        = {045016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Symmetric and antisymmetric kernels for machine learning problems in quantum physics and chemistry},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast convolutional neural networks on FPGAs with hls4ml.
<em>MLST</em>, <em>2</em>(4), 045015. (<a
href="https://doi.org/10.1088/2632-2153/ac0ea1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an automated tool for deploying ultra low-latency, low-power deep neural networks with convolutional layers on field-programmable gate arrays (FPGAs). By extending the hls4ml library, we demonstrate an inference latency of 5 µ s using convolutional architectures, targeting microsecond latency applications like those at the CERN Large Hadron Collider. Considering benchmark models trained on the Street View House Numbers Dataset, we demonstrate various methods for model compression in order to fit the computational constraints of a typical FPGA device used in trigger and data acquisition systems of particle detectors. In particular, we discuss pruning and quantization-aware training, and demonstrate how resource utilization can be significantly reduced with little to no loss in model accuracy. We show that the FPGA critical resource consumption can be reduced by 97% with zero loss in model accuracy, and by 99% when tolerating a 6% accuracy degradation.},
  archive      = {J_MLST},
  author       = {Thea Aarrestad and Vladimir Loncar and Nicolò Ghielmetti and Maurizio Pierini and Sioni Summers and Jennifer Ngadiuba and Christoffer Petersson and Hampus Linander and Yutaro Iiyama and Giuseppe Di Guglielmo and Javier Duarte and Philip Harris and Dylan Rankin and Sergo Jindariani and Kevin Pedro and Nhan Tran and Mia Liu and Edward Kreinar and Zhenbin Wu and Duc Hoang},
  doi          = {10.1088/2632-2153/ac0ea1},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Fast convolutional neural networks on FPGAs with hls4ml},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anomaly detection in gravitational waves data using
convolutional autoencoders. <em>MLST</em>, <em>2</em>(4), 045014. (<a
href="https://doi.org/10.1088/2632-2153/abf3d0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As of this moment, 50 gravitational wave (GW) detections have been announced, thanks to the observational efforts of the LIGO-Virgo collaboration, working with the Advanced LIGO and the Advanced Virgo interferometers. The detection of signals is complicated by the noise-dominated nature of the data. Conventional approaches in GW detection procedures require either precise knowledge of the GW waveform in the context of matched filtering searches or coincident analysis of data from multiple detectors. Furthermore, the analysis is prone to contamination by instrumental or environmental artifacts called glitches which either mimic astrophysical signals or reduce the overall quality of data. In this paper, we propose an alternative generic method of studying GW data based on detecting anomalies. The anomalies we study are transient signals, different from the slow non-stationary noise of the detector. The anomalies presented in the manuscript are mostly based on the GW emitted by the mergers of binary black hole systems. However, the presented study of anomalies is not limited only to GW alone, but also includes glitches occurring in the real LIGO/Virgo dataset available at the Gravitational Waves Open Science Center. To search for anomalies we employ deep learning algorithms, namely convolutional autoencoders, which are trained on both simulated and real detector data. We demonstrate the capabilities of our deep learning implementation in the reconstruction of injected signals. We study the influence of the GW strength, defined in terms of matched filter signal-to-noise ratio, on the detection of anomalies. Moreover, we present the application of our method for the localization in time of anomalies in the studied time-series data. We validate the results of anomaly searches on real data containing confirmed gravitational wave detections; we thus prove the generalization capabilities of our method, towards detecting GWs unknown to our deep learning models during training.},
  archive      = {J_MLST},
  author       = {Filip Morawski and Michał Bejger and Elena Cuoco and Luigia Petre},
  doi          = {10.1088/2632-2153/abf3d0},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Anomaly detection in gravitational waves data using convolutional autoencoders},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 detection from lung CT-scan images using transfer
learning approach. <em>MLST</em>, <em>2</em>(4), 045013. (<a
href="https://doi.org/10.1088/2632-2153/abf22c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the onset of 2020, the spread of coronavirus disease (COVID-19) has rapidly accelerated worldwide into a state of severe pandemic. COVID-19 has infected more than 29 million people and caused more than 900 thousand deaths at the time of writing. Since it is highly contagious, it causes explosive community transmission. Thus, health care delivery has been disrupted and compromised by the lack of testing kits. COVID-19-infected patients show severe acute respiratory syndrome. Meanwhile, the scientific community has been involved in the implementation of deep learning (DL) techniques to diagnose COVID-19 using computed tomography (CT) lung scans, since CT is a pertinent screening tool due to its higher sensitivity in recognizing early pneumonic changes. However, large datasets of CT-scan images are not publicly available due to privacy concerns and obtaining very accurate models has become difficult. Thus, to overcome this drawback, transfer-learning pre-trained models are used in the proposed methodology to classify COVID-19 (positive) and COVID-19 (negative) patients. We describe the development of a DL framework that includes pre-trained models (DenseNet201, VGG16, ResNet50V2, and MobileNet) as its backbone, known as KarNet. To extensively test and analyze the framework, each model was trained on original (i.e. unaugmented) and manipulated (i.e. augmented) datasets. Among the four pre-trained models of KarNet, the one that used DenseNet201 demonstrated excellent diagnostic ability, with AUC scores of 1.00 and 0.99 for models trained on unaugmented and augmented data sets, respectively. Even after considerable distortion of the images (i.e. the augmented dataset) DenseNet201 achieved an accuracy of 97% for the test dataset, followed by ResNet50V2, MobileNet, and VGG16 (which achieved accuracies of 96%, 95%, and 94%, respectively).},
  archive      = {J_MLST},
  author       = {Arpita Halder and Bimal Datta},
  doi          = {10.1088/2632-2153/abf22c},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {COVID-19 detection from lung CT-scan images using transfer learning approach},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Natural evolutionary strategies for variational quantum
computation. <em>MLST</em>, <em>2</em>(4), 045012. (<a
href="https://doi.org/10.1088/2632-2153/abf3ac">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural evolutionary strategies (NES) are a family of gradient-free black-box optimization algorithms. This study illustrates their use for the optimization of randomly initialized parameterized quantum circuits (PQCs) in the region of vanishing gradients. We show that using the NES gradient estimator the exponential decrease in variance can be alleviated. We implement two specific approaches, the exponential and separable NES, for parameter optimization of PQCs and compare them against standard gradient descent. We apply them to two different problems of ground state energy estimation using variational quantum eigensolver and state preparation with circuits of varying depth and length. We also introduce batch optimization for circuits with larger depth to extend the use of ES to a larger number of parameters. We achieve accuracy comparable to state-of-the-art optimization techniques in all the above cases with a lower number of circuit evaluations. Our empirical results indicate that one can use NES as a hybrid tool in tandem with other gradient-based methods for optimization of deep quantum circuits in regions with vanishing gradients.},
  archive      = {J_MLST},
  author       = {Abhinav Anand and Matthias Degroote and Alán Aspuru-Guzik},
  doi          = {10.1088/2632-2153/abf3ac},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Natural evolutionary strategies for variational quantum computation},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Adaptive partial scanning transmission electron microscopy
with reinforcement learning. <em>MLST</em>, <em>2</em>(4), 045011. (<a
href="https://doi.org/10.1088/2632-2153/abf5b6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed sensing can decrease scanning transmission electron microscopy electron dose and scan time with minimal information loss. Traditionally, sparse scans used in compressed sensing sample a static set of probing locations. However, dynamic scans that adapt to specimens are expected to be able to match or surpass the performance of static scans as static scans are a subset of possible dynamic scans. Thus, we present a prototype for a contiguous sparse scan system that piecewise adapts scan paths to specimens as they are scanned. Sampling directions for scan segments are chosen by a recurrent neural network (RNN) based on previously observed scan segments. The RNN is trained by reinforcement learning to cooperate with a feedforward convolutional neural network that completes the sparse scans. This paper presents our learning policy, experiments, and example partial scans, and discusses future research directions. Source code, pretrained models, and training data is openly accessible at https://github.com/Jeffrey-Ede/adaptive-scans .},
  archive      = {J_MLST},
  author       = {Jeffrey M Ede},
  doi          = {10.1088/2632-2153/abf5b6},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Adaptive partial scanning transmission electron microscopy with reinforcement learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MPGVAE: Improved generation of small organic molecules using
message passing neural nets. <em>MLST</em>, <em>2</em>(4), 045010. (<a
href="https://doi.org/10.1088/2632-2153/abf5b7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph generation is an extremely important task, as graphs are found throughout different areas of science and engineering. In this work, we focus on the modern equivalent of the Erdos–Rényi random graph model: the graph variational autoencoder (GVAE) (Simonovsky and Komodakis 2018 Int. Conf. on Artificial Neural Networks pp 412–22). This model assumes edges and nodes are independent in order to generate entire graphs at a time using a multi-layer perceptron decoder. As a result of these assumptions, GVAE has difficulty matching the training distribution and relies on an expensive graph matching procedure. We improve this class of models by building a message passing neural network into GVAE&#39;s encoder and decoder. We demonstrate our model on the specific task of generating small organic molecules.},
  archive      = {J_MLST},
  author       = {Daniel Flam-Shepherd and Tony C Wu and Alan Aspuru-Guzik},
  doi          = {10.1088/2632-2153/abf5b7},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {MPGVAE: Improved generation of small organic molecules using message passing neural nets},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural message passing on high order paths. <em>MLST</em>,
<em>2</em>(4), 045009. (<a
href="https://doi.org/10.1088/2632-2153/abf5b8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks have achieved impressive results in predicting molecular properties, but they do not directly account for local and hidden structures in the graph such as functional groups and molecular geometry. At each propagation step, graph neural networks aggregate only over first order neighbours and can only learn about important information contained in subsequent neighbours as well as the relationships between those higher order connections—over many propagation steps. In this work, we generalize graph neural nets to pass messages and aggregate across higher order paths. This allows for information to propagate over various levels and substructures of the graph. We demonstrate our model on a few tasks in molecular property prediction.},
  archive      = {J_MLST},
  author       = {Daniel Flam-Shepherd and Tony C Wu and Pascal Friederich and Alan Aspuru-Guzik},
  doi          = {10.1088/2632-2153/abf5b8},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural message passing on high order paths},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Atom cloud detection and segmentation using a deep neural
network. <em>MLST</em>, <em>2</em>(4), 045008. (<a
href="https://doi.org/10.1088/2632-2153/abf5ee">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use a deep neural network (NN) to detect and place region-of-interest (ROI) boxes around ultracold atom clouds in absorption and fluorescence images—with the ability to identify and bound multiple clouds within a single image. The NN also outputs segmentation masks that identify the size, shape and orientation of each cloud from which we extract the clouds&#39; Gaussian parameters. This allows 2D Gaussian fits to be reliably seeded thereby enabling fully automatic image processing. The method developed performs significantly better than a more conventional method based on a standardized image analysis library (Scikit-image) both for identifying ROI and extracting Gaussian parameters.},
  archive      = {J_MLST},
  author       = {Lucas R Hofer and Milan Krstajić and Péter Juhász and Anna L Marchant and Robert P Smith},
  doi          = {10.1088/2632-2153/abf5ee},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Atom cloud detection and segmentation using a deep neural network},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advances in scientific literature mining for interpreting
materials characterization. <em>MLST</em>, <em>2</em>(4), 045007. (<a
href="https://doi.org/10.1088/2632-2153/abf751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using synchrotron light sources, such as the National Synchrotron Light Source II at Brookhaven National Laboratory, scientists in fields as diverse as physics, biology, and materials science, identify the atomic structure, chemical composition, or other important properties of varied specimens. x-ray spectroscopy from light sources is particularly valuable for materials research with vast information available about reference spectra in the scientific literature. However, as the technique is applicable to many science domains, searching for information about select x-ray spectroscopy spectra is impeded by the sheer number of publications. Moreover, useful information about the context of an experiment or figures presented in papers can be buried among the details, which takes time to assess. This work presents a scientific literature mining system that supports data acquisition, information extraction, and user interaction for referencing x-ray spectra identification and spectral interpretation. The goal is to provide efficient access to useful spectral data to researchers who may spend only a few days at a synchrotron light source. With this system, users browse a classification tree for papers arranged according to x-ray spectroscopic methods, chemical elements, and x-ray absorption spectroscopy edges. Relevant figures are extracted with sentences from the paper that explain them, known as &#39;figure explanatory text.&#39; Notably, this system focuses on semantic aspects (logical analysis) to find figure explanatory text using deep contextualized word embeddings techniques and contains an interface to obtain labeled data from domain experts that is used to evaluate and improve the model.},
  archive      = {J_MLST},
  author       = {Gilchan Park and Line Pouchard},
  doi          = {10.1088/2632-2153/abf751},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Advances in scientific literature mining for interpreting materials characterization},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Defence against adversarial attacks using classical and
quantum-enhanced boltzmann machines †. <em>MLST</em>, <em>2</em>(4),
045006. (<a href="https://doi.org/10.1088/2632-2153/abf834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a robust defence to adversarial attacks on discriminative algorithms. Neural networks are naturally vulnerable to small, tailored perturbations in the input data that lead to wrong predictions. On the contrary, generative models attempt to learn the distribution underlying a dataset, making them inherently more robust to small perturbations. We use Boltzmann machines for discrimination purposes as attack-resistant classifiers, and compare them against standard state-of-the-art adversarial defences. We find improvements ranging from 5% to 72% against attacks with Boltzmann machines on the MNIST dataset. We furthermore complement the training with quantum-enhanced sampling from the D-Wave 2000Q annealer, finding results comparable with classical techniques and with marginal improvements in some cases. These results underline the relevance of probabilistic methods in constructing neural networks and highlight a novel scenario of practical relevance where quantum computers, even with limited hardware capabilities, could provide advantages over classical computers.},
  archive      = {J_MLST},
  author       = {Aidan Kehoe and Peter Wittek and Yanbo Xue and Alejandro Pozas-Kerstjens},
  doi          = {10.1088/2632-2153/abf834},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Defence against adversarial attacks using classical and quantum-enhanced boltzmann machines †},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spherical convolutions on molecular graphs for protein model
quality assessment. <em>MLST</em>, <em>2</em>(4), 045005. (<a
href="https://doi.org/10.1088/2632-2153/abf856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing information on three-dimensional (3D) objects requires methods stable to rigid-body transformations, in particular rotations, of the input data. In image processing tasks, convolutional neural networks achieve this property using rotation-equivariant operations. However, contrary to images, graphs generally have irregular topology. This makes it challenging to define a rotation-equivariant convolution operation on these structures. In this work, we propose spherical graph convolutional network that processes 3D models of proteins represented as molecular graphs. In a protein molecule, individual amino acids have common topological elements. This allows us to unambiguously associate each amino acid with a local coordinate system and construct rotation-equivariant spherical filters that operate on angular information between graph nodes. Within the framework of the protein model quality assessment problem, we demonstrate that the proposed spherical convolution method significantly improves the quality of model assessment compared to the standard message-passing approach. It is also comparable to state-of-the-art methods, as we demonstrate on critical assessment of structure prediction benchmarks. The proposed technique operates only on geometric features of protein 3D models. This makes it universal and applicable to any other geometric-learning task where the graph structure allows constructing local coordinate systems. The method is available at https://team.inria.fr/nano-d/software/s-gcn/ .},
  archive      = {J_MLST},
  author       = {Ilia Igashov and Nikita Pavlichenko and Sergei Grudinin},
  doi          = {10.1088/2632-2153/abf856},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Spherical convolutions on molecular graphs for protein model quality assessment},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A free-energy principle for representation learning.
<em>MLST</em>, <em>2</em>(4), 045004. (<a
href="https://doi.org/10.1088/2632-2153/abf984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper employs a formal connection of machine learning with thermodynamics to characterize the quality of learned representations for transfer learning. We discuss how information-theoretic functionals such as rate, distortion and classification loss of a model lie on a convex, so-called, equilibrium surface. We prescribe dynamical processes to traverse this surface under specific constraints; in particular we develop an iso-classification process that trades off rate and distortion to keep the classification loss unchanged. We demonstrate how this process can be used for transferring representations from a source task to a target task while keeping the classification loss constant. Experimental validation of the theoretical results is provided on image-classification datasets.},
  archive      = {J_MLST},
  author       = {Yansong Gao and Pratik Chaudhari},
  doi          = {10.1088/2632-2153/abf984},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A free-energy principle for representation learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural network analysis of neutron and x-ray reflectivity
data: Pathological cases, performance and perspectives. <em>MLST</em>,
<em>2</em>(4), 045003. (<a
href="https://doi.org/10.1088/2632-2153/abf9b1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neutron and x-ray reflectometry (NR and XRR) are powerful techniques to investigate the structural, morphological and even magnetic properties of solid and liquid thin films. While neutrons and x-rays behave similarly in many ways and can be described by the same general theory, they fundamentally differ in certain specific aspects. These aspects can be exploited to investigate different properties of a system, depending on which particular questions need to be answered. Having demonstrated the general applicability of neural networks to analyze XRR and NR data before (Greco et al 2019 J. Appl. Cryst. 52 1342), this study discusses challenges arising from certain pathological cases as well as performance issues and perspectives. These cases include a low signal-to-noise ratio, a high background signal (e.g. from incoherent scattering), as well as a potential lack of a total reflection edge (TRE). By dynamically modifying the training data after every mini batch, a fully-connected neural network was trained to determine thin film parameters from reflectivity curves. We show that noise and background intensity pose no significant problem as long as they do not affect the TRE. However, for curves without strong features the prediction accuracy is diminished. Furthermore, we compare the prediction accuracy for different scattering length density combinations. The results are demonstrated using simulated data of a single-layer system while also discussing challenges for multi-component systems.},
  archive      = {J_MLST},
  author       = {Alessandro Greco and Vladimir Starostin and Alexander Hinderhofer and Alexander Gerlach and Maximilian W A Skoda and Stefan Kowarik and Frank Schreiber},
  doi          = {10.1088/2632-2153/abf9b1},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural network analysis of neutron and x-ray reflectivity data: Pathological cases, performance and perspectives},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Propagation of priors for more accurate and efficient
spectroscopic functional fits and their application to ferroelectric
hysteresis. <em>MLST</em>, <em>2</em>(4), 045002. (<a
href="https://doi.org/10.1088/2632-2153/abfbba">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional spectral-imaging is a mainstay of the scanning probe and electron microscopies, micro-Raman, and various forms of chemical imaging. In many cases, individual spectra can be fit to a specific functional form, with the model parameter maps, providing direct insight into material properties. Since spectra are often acquired across a spatial grid of points, spatially adjacent spectra are likely to be similar to one another; yet, this fact is almost never used when considering parameter estimation for functional fits. On datasets tried here, we show that by utilizing proximal information, whether it be in the spatial or spectral domains, it is possible to improve the reliability and increase the speed of such functional fits by ∼2–3 × , as compared to random priors. We explore and compare three distinct new methods: (a) spatially averaging neighborhood spectra, and propagating priors based on functional fits to the averaged case, (b) hierarchical clustering-based methods where spectra are grouped hierarchically based on response, with the priors propagated progressively down the hierarchy, and (c) regular clustering without hierarchical methods with priors propagated from fits to cluster means. Our results highlight that utilizing spatial and spectral neighborhood information is often critical for accurate parameter estimation in noisy environments, which we show for ferroelectric hysteresis loops acquired on a prototypical PbTiO 3 thin film with piezoresponse spectroscopy. This method is general and applicable to any spatially measured spectra where functional forms are available. Examples include exploring the superconducting gap with tunneling spectroscopy, using the Dynes formula, or current–voltage curve fits in conductive atomic force microscopy mapping. Here we explore the problem for ferroelectric hysteresis, which, given its large parameter space, constitutes a more difficult task than, for example, fitting current–voltage curves with a Schottky emission formula (Chiu 2014 Adv. Mater. Sci. Eng. 2014 578168).},
  archive      = {J_MLST},
  author       = {N Creange and K P Kelley and C Smith and D Sando and O Paull and N Valanoor and S Somnath and S Jesse and S V Kalinin and R K Vasudevan},
  doi          = {10.1088/2632-2153/abfbba},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Propagation of priors for more accurate and efficient spectroscopic functional fits and their application to ferroelectric hysteresis},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large deviations in the perceptron model and consequences
for active learning. <em>MLST</em>, <em>2</em>(4), 045001. (<a
href="https://doi.org/10.1088/2632-2153/abfbbb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning (AL) is a branch of machine learning that deals with problems where unlabeled data is abundant yet obtaining labels is expensive. The learning algorithm has the possibility of querying a limited number of samples to obtain the corresponding labels, subsequently used for supervised learning. In this work, we consider the task of choosing the subset of samples to be labeled from a fixed finite pool of samples. We assume the pool of samples to be a random matrix and the ground truth labels to be generated by a single-layer teacher random neural network. We employ replica methods to analyze the large deviations for the accuracy achieved after supervised learning on a subset of the original pool. These large deviations then provide optimal achievable performance boundaries for any AL algorithm. We show that the optimal learning performance can be efficiently approached by simple message-passing AL algorithms. We also provide a comparison with the performance of some other popular active learning strategies.},
  archive      = {J_MLST},
  author       = {H Cui and L Saglietti and L Zdeborová},
  doi          = {10.1088/2632-2153/abfbbb},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {045001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Large deviations in the perceptron model and consequences for active learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Phases of learning dynamics in artificial neural networks in
the absence or presence of mislabeled data. <em>MLST</em>,
<em>2</em>(4), 043001. (<a
href="https://doi.org/10.1088/2632-2153/abf5b9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the tremendous success of deep neural networks in machine learning, the underlying reason for their superior learning capability remains unclear. Here, we present a framework based on statistical physics to study the dynamics of stochastic gradient descent (SGD), which drives learning in neural networks. Using the minibatch gradient ensemble, we construct order parameters to characterize the dynamics of weight updates in SGD. In the case without mislabeled data, we find that the SGD learning dynamics transitions from a fast learning phase to a slow exploration phase, which is associated with large changes in the order parameters that characterize the alignment of SGD gradients and their mean amplitude. In a more complex case, with randomly mislabeled samples, the SGD learning dynamics falls into four distinct phases. First, the system finds solutions for the correctly labeled samples in phase I; it then wanders around these solutions in phase II until it finds a direction that enables it to learn the mislabeled samples during phase III, after which, it finds solutions that satisfy all training samples during phase IV. Correspondingly, the test error decreases during phase I and remains low during phase II; however, it increases during phase III and reaches a high plateau during phase IV. The transitions between different phases can be understood by examining changes in the order parameters that characterize the alignment of the mean gradients for the two datasets (correctly and incorrectly labeled samples) and their (relative) strengths during learning. We find that individual sample losses for the two datasets are separated the most during phase II, leading to a data cleansing process that eliminates mislabeled samples and improves generalization. Overall, we believe that an approach based on statistical physics and stochastic dynamic systems theory provides a promising framework for describing and understanding learning dynamics in neural networks, which may also lead to more efficient learning algorithms.},
  archive      = {J_MLST},
  author       = {Yu Feng and Yuhai Tu},
  doi          = {10.1088/2632-2153/abf5b9},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {4},
  pages        = {043001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Phases of learning dynamics in artificial neural networks in the absence or presence of mislabeled data},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On application of deep learning to simplified
quantum-classical dynamics in electronically excited states.
<em>MLST</em>, <em>2</em>(3), 035039. (<a
href="https://doi.org/10.1088/2632-2153/abfe3f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) is applied to simulate non-adiabatic molecular dynamics of phenanthrene, using the time-dependent density functional based tight binding (TD-DFTB) approach for excited states combined with mixed quantum–classical propagation. Reference calculations rely on Tully&#39;s fewest-switches surface hopping (FSSH) algorithm coupled to TD-DFTB, which provides electronic relaxation dynamics in fair agreement with various available experimental results. Aiming at describing the coupled electron-nuclei dynamics in large molecular systems, we then examine the combination of DL for excited-state potential energy surfaces (PESs) with a simplified trajectory surface hopping propagation based on the Belyaev–Lebedev (BL) scheme. We start to assess the accuracy of the TD-DFTB approach upon comparison of the optical spectrum with experimental and higher-level theoretical results. Using the recently developed SchNetPack (Schütt et al 2019 J. Chem. Theory Comput. 15 448–55) for DL applications, we train several models and evaluate their performance in predicting excited-state energies and forces. Then, the main focus is given to the analysis of the electronic population of low-lying excited states computed with the aforementioned methods. We determine the relaxation timescales and compare them with experimental data. Our results show that DL demonstrates its ability to describe the excited-state PESs. When coupled to the simplified BL scheme considered in this study, it provides reliable description of the electronic relaxation in phenanthrene as compared with either the experimental data or the higher-level FSSH/TD-DFTB theoretical results. Furthermore, the DL performance allows high-throughput analysis at a negligible cost.},
  archive      = {J_MLST},
  author       = {Evgeny Posenitskiy and Fernand Spiegelman and Didier Lemoine},
  doi          = {10.1088/2632-2153/abfe3f},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035039},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {On application of deep learning to simplified quantum-classical dynamics in electronically excited states},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving sample and feature selection with principal
covariates regression. <em>MLST</em>, <em>2</em>(3), 035038. (<a
href="https://doi.org/10.1088/2632-2153/abfe7c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting the most relevant features and samples out of a large set of candidates is a task that occurs very often in the context of automated data analysis, where it improves the computational performance and often the transferability of a model. Here we focus on two popular subselection schemes applied to this end: CUR decomposition, derived from a low-rank approximation of the feature matrix, and farthest point sampling (FPS), which relies on the iterative identification of the most diverse samples and discriminating features. We modify these unsupervised approaches, incorporating a supervised component following the same spirit as the principal covariates (PCov) regression method. We show how this results in selections that perform better in supervised tasks, demonstrating with models of increasing complexity, from ridge regression to kernel ridge regression and finally feed-forward neural networks. We also present adjustments to minimise the impact of any subselection when performing unsupervised tasks. We demonstrate the significant improvements associated with PCov-CUR and PCov-FPS selections for applications to chemistry and materials science, typically reducing by a factor of two the number of features and samples required to achieve a given level of regression accuracy.},
  archive      = {J_MLST},
  author       = {Rose K Cersonsky and Benjamin A Helfrecht and Edgar A Engel and Sergei Kliavinek and Michele Ceriotti},
  doi          = {10.1088/2632-2153/abfe7c},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035038},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving sample and feature selection with principal covariates regression},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised machine learning of topological phase
transitions from experimental data. <em>MLST</em>, <em>2</em>(3),
035037. (<a href="https://doi.org/10.1088/2632-2153/abffe7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying phase transitions is one of the key challenges in quantum many-body physics. Recently, machine learning methods have been shown to be an alternative way of localising phase boundaries from noisy and imperfect data without the knowledge of the order parameter. Here, we apply different unsupervised machine learning techniques, including anomaly detection and influence functions, to experimental data from ultracold atoms. In this way, we obtain the topological phase diagram of the Haldane model in a completely unbiased fashion. We show that these methods can successfully be applied to experimental data at finite temperatures and to the data of Floquet systems when post-processing the data to a single micromotion phase. Our work provides a benchmark for the unsupervised detection of new exotic phases in complex many-body systems.},
  archive      = {J_MLST},
  author       = {Niklas Käming and Anna Dawid and Korbinian Kottmann and Maciej Lewenstein and Klaus Sengstock and Alexandre Dauphin and Christof Weitenberg},
  doi          = {10.1088/2632-2153/abffe7},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035037},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unsupervised machine learning of topological phase transitions from experimental data},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale tensor network architecture for machine
learning. <em>MLST</em>, <em>2</em>(3), 035036. (<a
href="https://doi.org/10.1088/2632-2153/abffe8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm for supervised learning using tensor networks, employing a step of data pre-processing by coarse-graining through a sequence of wavelet transformations. These transformations are represented as a set of tensor network layers identical to those in a multi-scale entanglement renormalization ansatz tensor network. We perform supervised learning and regression tasks through a model based on a matrix product states (MPSs) acting on the coarse-grained data. Because the entire model consists of tensor contractions (apart from the initial non-linear feature map), we can adaptively fine-grain the optimized MPS model &#39;backwards&#39; through the layers with essentially no loss in performance. The MPS itself is trained using an adaptive algorithm based on the density matrix renormalization group algorithm. We test our methods by performing a classification task on audio data and a regression task on temperature time-series data, studying the dependence of training accuracy on the number of coarse-graining layers and showing how fine-graining through the network may be used to initialize models which access finer-scale features.},
  archive      = {J_MLST},
  author       = {J A Reyes and E M Stoudenmire},
  doi          = {10.1088/2632-2153/abffe8},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035036},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-scale tensor network architecture for machine learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning modeling of materials with a group-subgroup
structure. <em>MLST</em>, <em>2</em>(3), 035035. (<a
href="https://doi.org/10.1088/2632-2153/abffe9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crystal structures connected by continuous phase transitions are linked through mathematical relations between crystallographic groups and their subgroups. In the present study, we introduce group-subgroup machine learning (GS-ML) and show that including materials with small unit cells in the training set decreases out-of-sample prediction errors for materials with large unit cells. GS-ML incurs the least training cost to reach 2%–3% target accuracy compared to other ML approaches. Since available materials datasets are heterogeneous providing insufficient examples for realizing the group-subgroup structure, we present the &#39;FriezeRMQ1D&#39; dataset with 8393 Q1D organometallic materials uniformly distributed across seven frieze groups. Furthermore, by comparing the performances of FCHL and 1-hot representations, we show GS-ML to capture subgroup information efficiently when the descriptor encodes structural information. The proposed approach is generic and extendable to symmetry abstractions such as spin-, valency-, or charge order.},
  archive      = {J_MLST},
  author       = {Prakriti Kayastha and Raghunathan Ramakrishnan},
  doi          = {10.1088/2632-2153/abffe9},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035035},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning modeling of materials with a group-subgroup structure},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast derivation of shapley based feature importances through
feature extraction methods for nanoinformatics. <em>MLST</em>,
<em>2</em>(3), 035034. (<a
href="https://doi.org/10.1088/2632-2153/ac0167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an alternative model-agnostic attribution method to compute feature importance rankings for high dimensional data requiring dimension reduction. We make use of Shapley values within the Shapley additive explanation framework to determine the importance values of each of the feature in the data set. We then demonstrate that it is possible to significantly reduce the computational complexity of ranking features in high dimensional spaces by first applying principal component analysis. This transformation into lower dimensional spaces in conjunction with our normalisation approach does not yield a significant loss of information when performing feature selection tasks beyond a threshold. The efficacy of our approach is demonstrated on several examples of nanomaterial data, in particular graphene oxide. Our approach is ideal for the applied physical science communities where datasets are of high dimensionality and computational complexity is a matter for concern.},
  archive      = {J_MLST},
  author       = {Tommy Liu and Amanda S Barnard},
  doi          = {10.1088/2632-2153/ac0167},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035034},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Fast derivation of shapley based feature importances through feature extraction methods for nanoinformatics},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the probability of coincidental similarity
between atomic displacement parameters with machine learning.
<em>MLST</em>, <em>2</em>(3), 035033. (<a
href="https://doi.org/10.1088/2632-2153/ac022d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution diffraction studies of macromolecules incorporate the tensor form of the anisotropic displacement parameter (ADP) of atoms from their mean position. The comparison of these parameters requires a statistical framework that can handle the experimental and modeling errors linked to structure determination. Here, a Bayesian machine learning model is introduced that approximates ADPs with the random Wishart distribution. This model allows for the comparison of random samples from a distribution that is trained on experimental structures. The comparison revealed that the experimental similarity between atoms is larger than predicted by the random model for a substantial fraction of the comparisons. Different metrics between ADPs were evaluated and categorized based on how useful they are at detecting non-accidental similarity and whether they can be replaced by other metrics. The most complementary comparisons were provided by Euclidean, Riemann and Wasserstein metrics. The analysis of ADP similarity and the positional distance of atoms in bovine trypsin revealed a set of atoms with striking ADP similarity over a long physical distance, and generally the physical distance between atoms and their ADP similarity do not correlate strongly. A substantial fraction of long- and short-range ADP similarities does not form by coincidence and are reproducibly observed in different crystal structures of the same protein.},
  archive      = {J_MLST},
  author       = {Viktor Ahlberg Gagner and Maja Jensen and Gergely Katona},
  doi          = {10.1088/2632-2153/ac022d},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035033},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Estimating the probability of coincidental similarity between atomic displacement parameters with machine learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning from survey propagation: A neural network for
MAX-e-3-SAT. <em>MLST</em>, <em>2</em>(3), 035032. (<a
href="https://doi.org/10.1088/2632-2153/ac0496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many natural optimization problems are NP-hard, which implies that they are probably hard to solve exactly in the worst-case. However, it suffices to get reasonably good solutions for all (or even most) instances in practice. This paper presents a new algorithm for computing approximate solutions in Θ( N ) for the maximum exact 3-satisfiability (MAX-E-3-SAT) problem by using supervised learning methodology. This methodology allows us to create a learning algorithm able to fix Boolean variables by using local information obtained by the Survey Propagation algorithm. By performing an accurate analysis, on random conjunctive normal form instances of the MAX-E-3-SAT with several Boolean variables, we show that this new algorithm, avoiding any decimation strategy, can build assignments better than a random one, even if the convergence of the messages is not found. Although this algorithm is not competitive with state-of-the-art maximum satisfiability solvers, it can solve substantially larger and more complicated problems than it ever saw during training.},
  archive      = {J_MLST},
  author       = {Raffaele Marino},
  doi          = {10.1088/2632-2153/ac0496},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035032},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Learning from survey propagation: A neural network for MAX-E-3-SAT},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning spatio-temporal epidemiological model to
evaluate germany-county-level COVID-19 risk. <em>MLST</em>,
<em>2</em>(3), 035031. (<a
href="https://doi.org/10.1088/2632-2153/ac0314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the COVID-19 pandemic continues to ravage the world, it is critical to assess the COVID-19 risk timely on multi-scale. To implement it and evaluate the public health policies, we develop a machine learning assisted framework to predict epidemic dynamics from the reported infection data. It contains a county-level spatio-temporal epidemiological model, which combines spatial cellular automata (CA) with time sensitive-undiagnosed-infected-removed (SUIR) model, and is compatible with the existing risk prediction models. The CA-SUIR model shows the multi-scale risk to the public and reveals the transmission modes of coronavirus in different scenarios. Through transfer learning, this new toolbox is used to predict the prevalence of multi-scale COVID-19 in all 412 counties in Germany. A t-day-ahead risk forecast as well as assessment of the non-pharmaceutical intervention policies is presented. We analyzed the situation at Christmas of 2020, and found that the most serious death toll could be 34.5. However, effective policy could control it below 21thousand, which provides a quantitative basis for evaluating the public policies implemented by the government. Such intervening evaluation process would help to improve public health policies and restart the economy appropriately in pandemics.},
  archive      = {J_MLST},
  author       = {Lingxiao Wang and Tian Xu and Till Stoecker and Horst Stoecker and Yin Jiang and Kai Zhou},
  doi          = {10.1088/2632-2153/ac0314},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035031},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning spatio-temporal epidemiological model to evaluate germany-county-level COVID-19 risk},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty quantification by ensemble learning for
computational optical form measurements. <em>MLST</em>, <em>2</em>(3),
035030. (<a href="https://doi.org/10.1088/2632-2153/ac0495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification by ensemble learning is explored in terms of an application known from the field of computational optical form measurements. The application requires solving a large-scale, nonlinear inverse problem. Ensemble learning is used to extend the scope of a recently developed deep learning approach for this problem in order to provide an uncertainty quantification of the solution to the inverse problem predicted by the deep learning method. By systematically inserting out-of-distribution errors as well as noisy data, the reliability of the developed uncertainty quantification is explored. Results are encouraging and the proposed application exemplifies the ability of ensemble methods to make trustworthy predictions on the basis of high-dimensional data in a real-world context.},
  archive      = {J_MLST},
  author       = {Lara Hoffmann and Ines Fortmeier and Clemens Elster},
  doi          = {10.1088/2632-2153/ac0495},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035030},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Uncertainty quantification by ensemble learning for computational optical form measurements},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochasticity helps to navigate rough landscapes: Comparing
gradient-descent-based algorithms in the phase retrieval problem.
<em>MLST</em>, <em>2</em>(3), 035029. (<a
href="https://doi.org/10.1088/2632-2153/ac0615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate how gradient-based algorithms such as gradient descent (GD), (multi-pass) stochastic GD, its persistent variant, and the Langevin algorithm navigate non-convex loss-landscapes and which of them is able to reach the best generalization error at limited sample complexity. We consider the loss landscape of the high-dimensional phase retrieval problem as a prototypical highly non-convex example. We observe that for phase retrieval the stochastic variants of GD are able to reach perfect generalization for regions of control parameters where the GD algorithm is not. We apply dynamical mean-field theory from statistical physics to characterize analytically the full trajectories of these algorithms in their continuous-time limit, with a warm start, and for large system sizes. We further unveil several intriguing properties of the landscape and the algorithms such as that the GD can obtain better generalization properties from less informed initializations.},
  archive      = {J_MLST},
  author       = {Francesca Mignacco and Pierfrancesco Urbani and Lenka Zdeborová},
  doi          = {10.1088/2632-2153/ac0615},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035029},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Stochasticity helps to navigate rough landscapes: Comparing gradient-descent-based algorithms in the phase retrieval problem},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum autoencoders with enhanced data encoding.
<em>MLST</em>, <em>2</em>(3), 035028. (<a
href="https://doi.org/10.1088/2632-2153/ac0616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the enhanced feature quantum autoencoder, or EF-QAE, a variational quantum algorithm capable of compressing quantum states of different models with higher fidelity. The key idea of the algorithm is to define a parameterized quantum circuit that depends upon adjustable parameters and a feature vector that characterizes such a model. We assess the validity of the method in simulations by compressing ground states of the Ising model and classical handwritten digits. The results show that EF-QAE improves the performance compared to the standard quantum autoencoder using the same amount of quantum resources, but at the expense of additional classical optimization. Therefore, EF-QAE makes the task of compressing quantum information better suited to be implemented in near-term quantum devices.},
  archive      = {J_MLST},
  author       = {Carlos Bravo-Prieto},
  doi          = {10.1088/2632-2153/ac0616},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035028},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum autoencoders with enhanced data encoding},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point cloud transformers applied to collider physics.
<em>MLST</em>, <em>2</em>(3), 035027. (<a
href="https://doi.org/10.1088/2632-2153/ac07f6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for processing point cloud information have seen a great success in collider physics applications. One recent breakthrough in machine learning is the usage of transformer networks to learn semantic relationships between sequences in language processing. In this work, we apply a modified transformer network called point cloud transformer as a method to incorporate the advantages of the transformer architecture to an unordered set of particles resulting from collision events. To compare the performance with other strategies, we study jet-tagging applications for highly-boosted particles.},
  archive      = {J_MLST},
  author       = {Vinicius Mikuni and Florencia Canelli},
  doi          = {10.1088/2632-2153/ac07f6},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035027},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Point cloud transformers applied to collider physics},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Olympus: A benchmarking framework for noisy optimization and
experiment planning. <em>MLST</em>, <em>2</em>(3), 035021. (<a
href="https://doi.org/10.1088/2632-2153/abedc8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research challenges encountered across science, engineering, and economics can frequently be formulated as optimization tasks. In chemistry and materials science, recent growth in laboratory digitization and automation has sparked interest in optimization-guided autonomous discovery and closed-loop experimentation. Experiment planning strategies based on off-the-shelf optimization algorithms can be employed in fully autonomous research platforms to achieve desired experimentation goals with the minimum number of trials. However, the experiment planning strategy that is most suitable to a scientific discovery task is a priori unknown while rigorous comparisons of different strategies are highly time and resource demanding. As optimization algorithms are typically benchmarked on low-dimensional synthetic functions, it is unclear how their performance would translate to noisy, higher-dimensional experimental tasks encountered in chemistry and materials science. We introduce Olympus , a software package that provides a consistent and easy-to-use framework for benchmarking optimization algorithms against realistic experiments emulated via probabilistic deep-learning models. Olympus includes a collection of experimentally derived benchmark sets from chemistry and materials science and a suite of experiment planning strategies that can be easily accessed via a user-friendly Python interface. Furthermore, Olympus facilitates the integration, testing, and sharing of custom algorithms and user-defined datasets. In brief, Olympus mitigates the barriers associated with benchmarking optimization algorithms on realistic experimental scenarios, promoting data sharing and the creation of a standard framework for evaluating the performance of experiment planning strategies.},
  archive      = {J_MLST},
  author       = {Florian Häse and Matteo Aldeghi and Riley J Hickman and Loïc M Roch and Melodie Christensen and Elena Liles and Jason E Hein and Alán Aspuru-Guzik},
  doi          = {10.1088/2632-2153/abedc8},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035021},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Olympus: A benchmarking framework for noisy optimization and experiment planning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strategies for the construction of machine-learning
potentials for accurate and efficient atomic-scale simulations.
<em>MLST</em>, <em>2</em>(3), 031001. (<a
href="https://doi.org/10.1088/2632-2153/abfd96">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in machine-learning interatomic potentials have enabled the efficient modeling of complex atomistic systems with an accuracy that is comparable to that of conventional quantum-mechanics based methods. At the same time, the construction of new machine-learning potentials can seem a daunting task, as it involves data-science techniques that are not yet common in chemistry and materials science. Here, we provide a tutorial-style overview of strategies and best practices for the construction of artificial neural network (ANN) potentials. We illustrate the most important aspects of (a) data collection, (b) model selection, (c) training and validation, and (d) testing and refinement of ANN potentials on the basis of practical examples. Current research in the areas of active learning and delta learning are also discussed in the context of ANN potentials. This tutorial review aims at equipping computational chemists and materials scientists with the required background knowledge for ANN potential construction and application, with the intention to accelerate the adoption of the method, so that it can facilitate exciting research that would otherwise be challenging with conventional strategies.},
  archive      = {J_MLST},
  author       = {April M Miksch and Tobias Morawietz and Johannes Kästner and Alexander Urban and Nongnuch Artrith},
  doi          = {10.1088/2632-2153/abfd96},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {031001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Strategies for the construction of machine-learning potentials for accurate and efficient atomic-scale simulations},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning inference of molecular dipole moment in
liquid water. <em>MLST</em>, <em>2</em>(3), 03LT03. (<a
href="https://doi.org/10.1088/2632-2153/ac0123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular dipole moment in liquid water is an intriguing property, partly due to the fact that there is no unique way to partition the total electron density into individual molecular contributions. The prevailing method to circumvent this problem is to use maximally localized Wannier functions, which perform a unitary transformation of the occupied molecular orbitals by minimizing the spread function of Boys. Here we revisit this problem using a data-driven approach satisfying two physical constraints, namely: (a) The displacement of the atomic charges is proportional to the Berry phase polarization; (b) Each water molecule has a formal charge of zero. It turns out that the distribution of molecular dipole moments in liquid water inferred from latent variables is surprisingly similar to that obtained from maximally localized Wannier functions. Apart from putting a maximum-likelihood footnote to the established method, this work highlights the capability of graph convolution based charge models and the importance of physical constraints on improving the model interpretability.},
  archive      = {J_MLST},
  author       = {Lisanne Knijff and Chao Zhang},
  doi          = {10.1088/2632-2153/ac0123},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {03LT03},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning inference of molecular dipole moment in liquid water},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep molecular dreaming: Inverse machine learning for
de-novo molecular design and interpretability with surjective
representations. <em>MLST</em>, <em>2</em>(3), 03LT02. (<a
href="https://doi.org/10.1088/2632-2153/ac09d6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-based de-novo design of functional molecules is one of the most prominent challenges in cheminformatics today. As a result, generative and evolutionary inverse designs from the field of artificial intelligence have emerged at a rapid pace, with aims to optimize molecules for a particular chemical property. These models &#39;indirectly&#39; explore the chemical space; by learning latent spaces, policies, and distributions, or by applying mutations on populations of molecules. However, the recent development of the SELFIES (Krenn 2020 Mach. Learn.: Sci. Technol. 1 045024) string representation of molecules, a surjective alternative to SMILES, have made possible other potential techniques. Based on SELFIES, we therefore propose PASITHEA, a direct gradient-based molecule optimization that applies inceptionism (Mordvintsev 2015) techniques from computer vision. PASITHEA exploits the use of gradients by directly reversing the learning process of a neural network, which is trained to predict real-valued chemical properties. Effectively, this forms an inverse regression model, which is capable of generating molecular variants optimized for a certain property. Although our results are preliminary, we observe a shift in distribution of a chosen property during inverse-training, a clear indication of PASITHEA&#39;s viability. A striking property of inceptionism is that we can directly probe the model&#39;s understanding of the chemical space on which it is trained. We expect that extending PASITHEA to larger datasets, molecules and more complex properties will lead to advances in the design of new functional molecules as well as the interpretation and explanation of machine learning models.},
  archive      = {J_MLST},
  author       = {Cynthia Shen and Mario Krenn and Sagi Eppel and Alán Aspuru-Guzik},
  doi          = {10.1088/2632-2153/ac09d6},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {03LT02},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep molecular dreaming: Inverse machine learning for de-novo molecular design and interpretability with surjective representations},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved description of atomic environments using low-cost
polynomial functions with compact support. <em>MLST</em>, <em>2</em>(3),
035026. (<a href="https://doi.org/10.1088/2632-2153/abf817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of chemical properties using machine learning techniques calls for a set of appropriate descriptors that accurately describe atomic and, on a larger scale, molecular environments. A mapping of conformational information on a space spanned by atom-centred symmetry functions (SF) has become a standard technique for energy and force predictions using high-dimensional neural network potentials (HDNNP). An appropriate choice of SFs is particularly crucial for accurate force predictions. Established atom-centred SFs, however, are limited in their flexibility, since their functional form restricts the angular domain that can be sampled without introducing problematic derivative discontinuities. Here, we introduce a class of atom-centred SFs based on polynomials with compact support called polynomial symmetry functions (PSF), which enable a free choice of both, the angular and the radial domain covered. We demonstrate that the accuracy of PSFs is either on par or considerably better than that of conventional, atom-centred SFs. In particular, a generic set of PSFs with an intuitive choice of the angular domain inspired by organic chemistry considerably improves prediction accuracy for organic molecules in the gaseous and liquid phase, with reductions in force prediction errors over a test set approaching 50% for certain systems. Contrary to established atom-centred SFs, computation of PSF does not involve any exponentials, and their intrinsic compact support supersedes use of separate cutoff functions, facilitating the choice of their free parameters. Most importantly, the number of floating point operations required to compute polynomial SFs introduced here is considerably lower than that of other state-of-the-art SFs, enabling their efficient implementation without the need of highly optimised code structures or caching, with speedups with respect to other state-of-the-art SFs reaching a factor of 4.5 to 5. This low-effort performance benefit substantially simplifies their use in new programs and emerging platforms such as graphical processing units. Overall, polynomial SFs with compact support improve accuracy of both, energy and force predictions with HDNNPs while enabling significant speedups compared to their well-established counterparts.},
  archive      = {J_MLST},
  author       = {Martin P Bircher and Andreas Singraber and Christoph Dellago},
  doi          = {10.1088/2632-2153/abf817},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035026},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improved description of atomic environments using low-cost polynomial functions with compact support},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extracting electron scattering cross sections from swarm
data using deep neural networks. <em>MLST</em>, <em>2</em>(3), 035025.
(<a href="https://doi.org/10.1088/2632-2153/abf15a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electron-neutral scattering cross sections are fundamental quantities in simulations of low temperature plasmas used for many technological applications today. From these microscopic cross sections, several macro-scale quantities (called &#39;swarm&#39; parameters) can be calculated. However, measurements as well as theoretical calculations of cross sections are challenging. Since the 1960s, researchers have attempted to solve the inverse swarm problem of obtaining cross sections from swarm data; but the solutions are not necessarily unique. To address these issues, we examine the use of deep learning models which are trained using the previous determinations of elastic momentum transfer, ionization and excitation cross sections for different gases available on the LXCat website and their corresponding swarm parameters calculated using the BOLSIG+ solver for the numerical solution of the Boltzmann equation for electrons in weakly ionized gases. We implement artificial neural network (ANN), convolutional neural network (CNN) and densely connected convolutional network (DenseNet) for this investigation. To the best of our knowledge, there is no study exploring the use of CNN and DenseNet for the inverse swarm problem. We test the validity of predictions by all these trained networks for a broad range of gas species and we deduce that DenseNet effectively extracts both long and short term features from the swarm data and hence, it predicts cross sections with significantly higher accuracy compared to ANN. Further, we apply Monte Carlo dropout as Bayesian approximation to estimate the probability distribution of the cross sections to determine all plausible solutions of this inverse problem.},
  archive      = {J_MLST},
  author       = {Vishrut Jetly and Bhaskar Chaudhury},
  doi          = {10.1088/2632-2153/abf15a},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035025},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Extracting electron scattering cross sections from swarm data using deep neural networks},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Playing optical tweezers with deep reinforcement learning:
In virtual, physical and augmented environments. <em>MLST</em>,
<em>2</em>(3), 035024. (<a
href="https://doi.org/10.1088/2632-2153/abf0f6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning was carried out in a simulated environment to learn continuous velocity control over multiple motor axes. This was then applied to a real-world optical tweezers experiment with the objective of moving a laser-trapped microsphere to a target location whilst avoiding collisions with other free-moving microspheres. The concept of training a neural network in a virtual environment has significant potential in the application of machine learning for experimental optimization and control, as the neural network can discover optimal methods for problem solving without the risk of damage to equipment, and at a speed not limited by movement in the physical environment. As the neural network treats both virtual and physical environments equivalently, we show that the network can also be applied to an augmented environment, where a virtual environment is combined with the physical environment. This technique may have the potential to unlock capabilities associated with mixed and augmented reality, such as enforcing safety limits for machine motion or as a method of inputting observations from additional sensors.},
  archive      = {J_MLST},
  author       = {Matthew Praeger and Yunhui Xie and James A Grant-Jacob and Robert W Eason and Ben Mills},
  doi          = {10.1088/2632-2153/abf0f6},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035024},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Playing optical tweezers with deep reinforcement learning: In virtual, physical and augmented environments},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven discovery of koopman eigenfunctions for control.
<em>MLST</em>, <em>2</em>(3), 035023. (<a
href="https://doi.org/10.1088/2632-2153/abf0f5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven transformations that reformulate nonlinear systems in a linear framework have the potential to enable the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. The Koopman operator has emerged as a principled linear embedding of nonlinear dynamics, and its eigenfunctions establish intrinsic coordinates along which the dynamics behave linearly. Previous studies have used finite-dimensional approximations of the Koopman operator for model-predictive control approaches. In this work, we illustrate a fundamental closure issue of this approach and argue that it is beneficial to first validate eigenfunctions and then construct reduced-order models in these validated eigenfunctions. These coordinates form a Koopman-invariant subspace by design and, thus, have improved predictive power. We show then how the control can be formulated directly in these intrinsic coordinates and discuss potential benefits and caveats of this perspective. The resulting control architecture is termed Koopman Reduced Order Nonlinear Identification and Control (KRONIC). It is further demonstrated that these eigenfunctions can be approximated with data-driven regression and power series expansions, based on the partial differential equation governing the infinitesimal generator of the Koopman operator. Validating discovered eigenfunctions is crucial and we show that lightly damped eigenfunctions may be faithfully extracted from EDMD or an implicit formulation. These lightly damped eigenfunctions are particularly relevant for control, as they correspond to nearly conserved quantities that are associated with persistent dynamics, such as the Hamiltonian. KRONIC is then demonstrated on a number of relevant examples, including (a) a nonlinear system with a known linear embedding, (b) a variety of Hamiltonian systems, and (c) a high-dimensional double-gyre model for ocean mixing.},
  archive      = {J_MLST},
  author       = {Eurika Kaiser and J Nathan Kutz and Steven L Brunton},
  doi          = {10.1088/2632-2153/abf0f5},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035023},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Data-driven discovery of koopman eigenfunctions for control},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient hyperparameter tuning for kernel ridge regression
with bayesian optimization. <em>MLST</em>, <em>2</em>(3), 035022. (<a
href="https://doi.org/10.1088/2632-2153/abee59">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning methods usually depend on internal parameters—so called hyperparameters—that need to be optimized for best performance. Such optimization poses a burden on machine learning practitioners, requiring expert knowledge, intuition or computationally demanding brute-force parameter searches. We here assess three different hyperparameter selection methods: grid search, random search and an efficient automated optimization technique based on Bayesian optimization (BO). We apply these methods to a machine learning problem based on kernel ridge regression in computational chemistry. Two different descriptors are employed to represent the atomic structure of organic molecules, one of which introduces its own set of hyperparameters to the method. We identify optimal hyperparameter configurations and infer entire prediction error landscapes in hyperparameter space that serve as visual guides for the hyperparameter performance. We further demonstrate that for an increasing number of hyperparameters, BO and random search become significantly more efficient in computational time than an exhaustive grid search, while delivering an equivalent or even better accuracy.},
  archive      = {J_MLST},
  author       = {Annika Stuke and Patrick Rinke and Milica Todorović},
  doi          = {10.1088/2632-2153/abee59},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035022},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Efficient hyperparameter tuning for kernel ridge regression with bayesian optimization},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine-learning enhanced dark soliton detection in
bose–einstein condensates. <em>MLST</em>, <em>2</em>(3), 035020. (<a
href="https://doi.org/10.1088/2632-2153/abed1e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most data in cold-atom experiments comes from images, the analysis of which is limited by our preconceptions of the patterns that could be present in the data. We focus on the well-defined case of detecting dark solitons—appearing as local density depletions in a Bose–Einstein condensate (BEC)—using a methodology that is extensible to the general task of pattern recognition in images of cold atoms. Studying soliton dynamics over a wide range of parameters requires the analysis of large datasets, making the existing human-inspection-based methodology a significant bottleneck. Here we describe an automated classification and positioning system for identifying localized excitations in atomic BECs utilizing deep convolutional neural networks to eliminate the need for human image examination. Furthermore, we openly publish our labeled dataset of dark solitons, the first of its kind, for further machine learning research.},
  archive      = {J_MLST},
  author       = {Shangjie Guo and Amilson R Fritsch and Craig Greenberg and I B Spielman and Justyna P Zwolak},
  doi          = {10.1088/2632-2153/abed1e},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035020},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine-learning enhanced dark soliton detection in Bose–Einstein condensates},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep-learning-based quantum vortex detection in atomic
bose–einstein condensates. <em>MLST</em>, <em>2</em>(3), 035019. (<a
href="https://doi.org/10.1088/2632-2153/abea6a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum vortices naturally emerge in rotating Bose–Einstein condensates (BECs) and, similarly to their classical counterparts, allow the study of a range of interesting out-of-equilibrium phenomena, such as turbulence and chaos. However, the study of such phenomena requires the determination of the precise location of each vortex within a BEC, which becomes challenging when either only the density of the condensate is available or sources of noise are present, as is typically the case in experimental settings. Here, we introduce a machine-learning-based vortex detector motivated by state-of-the-art object detection methods that can accurately locate vortices in simulated BEC density images. Our model allows for robust and real-time detection in noisy and non-equilibrium configurations. Furthermore, the network can distinguish between vortices and anti-vortices if the phase profile of the condensate is also available. We anticipate that our vortex detector will be advantageous for both experimental and theoretical studies of the static and dynamic properties of vortex configurations in BECs.},
  archive      = {J_MLST},
  author       = {Friederike Metz and Juan Polo and Natalya Weber and Thomas Busch},
  doi          = {10.1088/2632-2153/abea6a},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035019},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep-learning-based quantum vortex detection in atomic Bose–Einstein condensates},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NNETFIX: An artificial neural network-based denoising engine
for gravitational-wave signals. <em>MLST</em>, <em>2</em>(3), 035018.
(<a href="https://doi.org/10.1088/2632-2153/abea69">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental and environmental transient noise bursts in gravitational-wave (GW) detectors, or glitches, may impair astrophysical observations by adversely affecting the sky localization and the parameter estimation of GW signals. Denoising of detector data is especially relevant during low-latency operations because electromagnetic follow-up of candidate detections requires accurate, rapid sky localization and inference of astrophysical sources. NNETFIX is a machine learning, artificial neural network-based algorithm designed to estimate the data containing a transient GW signal with an overlapping glitch as though the glitch was absent. The sky localization calculated from the denoised data may be significantly more accurate than the sky localization obtained from the original data or by removing the portion of the data impacted by the glitch. We test NNETFIX in simulated scenarios of binary black hole coalescence signals and discuss the potential for its use in future low-latency LIGO-Virgo-KAGRA searches. In the majority of cases for signals with a high signal-to-noise ratio, we find that the overlap of the sky maps obtained with the denoised data and the original data is better than the overlap of the sky maps obtained with the original data and the data with the glitch removed.},
  archive      = {J_MLST},
  author       = {Kentaro Mogushi and Ryan Quitzow-James and Marco Cavaglià and Sumeet Kulkarni and Fergus Hayes},
  doi          = {10.1088/2632-2153/abea69},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {NNETFIX: An artificial neural network-based denoising engine for gravitational-wave signals},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A machine learning based bayesian optimization solution to
non-linear responses in dusty plasmas. <em>MLST</em>, <em>2</em>(3),
035017. (<a href="https://doi.org/10.1088/2632-2153/abe7b7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear frequency response analysis is a widely used method for determining system dynamics in the presence of nonlinearities. In dusty plasmas, the plasma–grain interaction (e.g. grain charging fluctuations) can be characterized by a single-particle non-linear response analysis, while grain–grain non-linear interactions can be determined by a multi-particle non-linear response analysis. Here a machine learning-based method to determine the equation of motion in the non-linear response analysis for dust particles in plasmas is presented. Searching the parameter space in a Bayesian manner allows an efficient optimization of the parameters needed to match simulated non-linear response curves to experimentally measured non-linear response curves.},
  archive      = {J_MLST},
  author       = {Zhiyue Ding and Lorin S Matthews and Truell W Hyde},
  doi          = {10.1088/2632-2153/abe7b7},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A machine learning based bayesian optimization solution to non-linear responses in dusty plasmas},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automation of some macromolecular properties using a machine
learning approach. <em>MLST</em>, <em>2</em>(3), 035016. (<a
href="https://doi.org/10.1088/2632-2153/abe7b6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we employed a newly developed method to predict macromolecular properties using a swarm artificial neural network (ANN) method as a machine learning approach. In this method, the molecular structures are represented by the feature description vectors used as training input data for a neural network. This study aims to develop an efficient approach for training an ANN using either experimental or quantum mechanics data. We aim to introduce an error model controlling the reliability of the prediction confidence interval using a bootstrapping swarm approach. We created different datasets of selected experimental or quantum mechanics results. Using this optimized ANN, we hope to predict properties and their statistical errors for new molecules. There are four datasets used in this study. That includes the dataset of 642 small organic molecules with known experimental hydration free energies, the dataset of 1475 experimental pKa values of ionizable groups in 192 proteins, the dataset of 2693 mutants in 14 proteins with given experimental values of changes in the Gibbs free energy, and a dataset of 7101 quantum mechanics heat of formation calculations. All the data are prepared and optimized using the AMBER force field in the CHARMM macromolecular computer simulation program. The bootstrapping swarm ANN code for performing the optimization and prediction is written in Python computer programming language. The descriptor vectors of the small molecules are based on the Coulomb matrix and sum over bond properties. For the macromolecular systems, they consider the chemical-physical fingerprints of the region in the vicinity of each amino acid.},
  archive      = {J_MLST},
  author       = {Merjem Hoxha and Hiqmet Kamberaj},
  doi          = {10.1088/2632-2153/abe7b6},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {035016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Automation of some macromolecular properties using a machine learning approach},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complete parameter inference for GW150914 using deep
learning. <em>MLST</em>, <em>2</em>(3), 03LT01. (<a
href="https://doi.org/10.1088/2632-2153/abfaed">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The LIGO and Virgo gravitational-wave observatories have detected many exciting events over the past 5 years. To infer the system parameters, iterative sampling algorithms such as MCMC are typically used with Bayes&#39; theorem to obtain posterior samples—by repeatedly generating waveforms and comparing to measured strain data. However, as the rate of detections grows with detector sensitivity, this poses a growing computational challenge. To confront this challenge, as well as that of fast multimessenger alerts, in this study we apply deep learning to learn non-iterative surrogate models for the Bayesian posterior. We train a neural-network conditional density estimator to model posterior probability distributions over the full 15-dimensional space of binary black hole system parameters, given detector strain data from multiple detectors. We use the method of normalizing flows—specifically, a neural spline flow—which allows for rapid sampling and density estimation. Training the network is likelihood-free, requiring samples from the data generative process, but no likelihood evaluations. Through training, the network learns a global set of posteriors: it can generate thousands of independent posterior samples per second for any strain data consistent with the training distribution. We demonstrate our method by performing inference on GW150914, and obtain results in close agreement with standard techniques.},
  archive      = {J_MLST},
  author       = {Stephen R Green and Jonathan Gair},
  doi          = {10.1088/2632-2153/abfaed},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {3},
  pages        = {03LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Complete parameter inference for GW150914 using deep learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Erratum: Improving the generative performance of chemical
autoencoders through transfer learning (2020 mach. Learn.: Sci. Technol.
1 045010). <em>MLST</em>, <em>2</em>(3), 039601. (<a
href="https://doi.org/10.1088/2632-2153/abe194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MLST},
  author       = {Nicolae C Iovanac and Brett M Savoie},
  doi          = {10.1088/2632-2153/abe194},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {039601},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Erratum: improving the generative performance of chemical autoencoders through transfer learning (2020 mach. learn.: sci. technol. 1 045010)},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PASSer: Prediction of allosteric sites server.
<em>MLST</em>, <em>2</em>(3), 035015. (<a
href="https://doi.org/10.1088/2632-2153/abe6d6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Allostery is considered important in regulating protein&#39;s activity. Drug development depends on the understanding of allosteric mechanisms, especially the identification of allosteric sites, which is a prerequisite in drug discovery and design. Many computational methods have been developed for allosteric site prediction using pocket features and protein dynamics. Here, we present an ensemble learning method, consisting of eXtreme gradient boosting and graph convolutional neural network, to predict allosteric sites. Our model can learn physical properties and topology without any prior information, and shows good performance under multiple indicators. Prediction results showed that 84.9% of allosteric pockets in the test set appeared in the top 3 positions. The PASSer: Protein Allosteric Sites Server ( https://passer.smu.edu ), along with a command line interface ( https://github.com/smutaogroup/passerCLI ) provide insights for further analysis in drug discovery.},
  archive      = {J_MLST},
  author       = {Hao Tian and Xi Jiang and Peng Tao},
  doi          = {10.1088/2632-2153/abe6d6},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {PASSer: Prediction of allosteric sites server},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning pipeline for quantum state estimation with
incomplete measurements. <em>MLST</em>, <em>2</em>(3), 035014. (<a
href="https://doi.org/10.1088/2632-2153/abe5f5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-qubit systems typically employ 36 projective measurements for high-fidelity tomographic estimation. The overcomplete nature of the 36 measurements suggests possible robustness of the estimation procedure to missing measurements. In this paper, we explore the resilience of machine-learning-based quantum state estimation techniques to missing measurements by creating a pipeline of stacked machine learning models for imputation, denoising, and state estimation. When applied to simulated noiseless and noisy projective measurement data for both pure and mixed states, we demonstrate quantum state estimation from partial measurement results that outperforms previously developed machine-learning-based methods in reconstruction fidelity and several conventional methods in terms of resource scaling. Notably, our developed model does not require training a separate model for each missing measurement, making it potentially applicable to quantum state estimation of large quantum systems where preprocessing is computationally infeasible due to the exponential scaling of quantum system dimension.},
  archive      = {J_MLST},
  author       = {Onur Danaci and Sanjaya Lohani and Brian T Kirby and Ryan T Glasser},
  doi          = {10.1088/2632-2153/abe5f5},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning pipeline for quantum state estimation with incomplete measurements},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A reinforcement learning application of a guided monte carlo
tree search algorithm for beam orientation selection in radiation
therapy. <em>MLST</em>, <em>2</em>(3), 035013. (<a
href="https://doi.org/10.1088/2632-2153/abe528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current beam orientation optimization algorithms for radiotherapy, such as column generation (CG), are typically heuristic or greedy in nature because of the size of the combinatorial problem, which leads to suboptimal solutions. We propose a reinforcement learning strategy using a Monte Carlo Tree Search (MCTS) that can find a better beam orientation set in less time than CG. We utilize a reinforcement learning structure involving a supervised learning network to guide the MCTS and to explore the decision space of beam orientation selection problems. We previously trained a deep neural network (DNN) that takes in the patient anatomy, organ weights, and current beams, then approximates beam fitness values to indicate the next best beam to add. Here, we use this DNN to probabilistically guide the traversal of the branches of the Monte Carlo decision tree to add a new beam to the plan. To assess the feasibility of the algorithm, we used a test set of 13 prostate cancer patients, distinct from the 57 patients originally used to train and validate the DNN, to solve five-beam plans. To show the strength of the guided MCTS (GTS) compared to other search methods, we also provided the performances of Guided Search, Uniform Tree Search and Random Search algorithms. On average, GTS outperformed all the other methods. It found a better solution than CG in 237 s on average, compared to 360 s for CG, and outperformed all other methods in finding a solution with a lower objective function value in less than 1000 s. Using our GTS method, we could maintain planning target volume (PTV) coverage within 1% error similar to CG, while reducing the organ-at-risk mean dose for body, rectum, left and right femoral heads; the mean dose to bladder was 1% higher with GTS than with CG.},
  archive      = {J_MLST},
  author       = {Azar Sadeghnejad-Barkousaraie and Gyanendra Bohara and Steve Jiang and Dan Nguyen},
  doi          = {10.1088/2632-2153/abe528},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A reinforcement learning application of a guided monte carlo tree search algorithm for beam orientation selection in radiation therapy},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward a theory of machine learning. <em>MLST</em>,
<em>2</em>(3), 035012. (<a
href="https://doi.org/10.1088/2632-2153/abe6d7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define a neural network as a septuple consisting of (1) a state vector, (2) an input projection, (3) an output projection, (4) a weight matrix, (5) a bias vector, (6) an activation map and (7) a loss function. We argue that the loss function can be imposed either on the boundary (i.e. input and/or output neurons) or in the bulk (i.e. hidden neurons) for both supervised and unsupervised systems. We apply the principle of maximum entropy to derive a canonical ensemble of the state vectors subject to a constraint imposed on the bulk loss function by a Lagrange multiplier (or an inverse temperature parameter). We show that in an equilibrium the canonical partition function must be a product of two factors: a function of the temperature, and a function of the bias vector and weight matrix. Consequently, the total Shannon entropy consists of two terms which represent, respectively, a thermodynamic entropy and a complexity of the neural network. We derive the first and second laws of learning: during learning the total entropy must decrease until the system reaches an equilibrium (i.e. the second law), and the increment in the loss function must be proportional to the increment in the thermodynamic entropy plus the increment in the complexity (i.e. the first law). We calculate the entropy destruction to show that the efficiency of learning is given by the Laplacian of the total free energy, which is to be maximized in an optimal neural architecture, and explain why the optimization condition is better satisfied in a deep network with a large number of hidden layers. The key properties of the model are verified numerically by training a supervised feedforward neural network using the stochastic gradient descent method. We also discuss a possibility that the entire Universe at its most fundamental level is a neural network.},
  archive      = {J_MLST},
  author       = {Vitaly Vanchurin},
  doi          = {10.1088/2632-2153/abe6d7},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Toward a theory of machine learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural ordinary differential equation and holographic
quantum chromodynamics. <em>MLST</em>, <em>2</em>(3), 035011. (<a
href="https://doi.org/10.1088/2632-2153/abe527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neural ordinary differential equation (neural ODE) is a novel machine learning architecture whose weights are smooth functions of the continuous depth. We apply the neural ODE to holographic QCD by regarding the weight functions as a bulk metric, and train the machine with lattice QCD data of chiral condensate at finite temperature. The machine finds consistent bulk geometry at various values of temperature and discovers the emergent black hole horizon in the holographic bulk automatically. The holographic Wilson loops calculated with the emergent machine-learned bulk spacetime have consistent temperature dependence of confinement and Debye-screening behavior. In machine learning models with physically interpretable weights, the neural ODE frees us from discretization artifact leading to difficult ingenuity of hyperparameters, and improves numerical accuracy to make the model more trustworthy.},
  archive      = {J_MLST},
  author       = {Koji Hashimoto and Hong-Ye Hu and Yi-Zhuang You},
  doi          = {10.1088/2632-2153/abe527},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural ordinary differential equation and holographic quantum chromodynamics},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revving up 13C NMR shielding predictions across chemical
space: Benchmarks for atoms-in-molecules kernel machine learning with
new data for 134 kilo molecules. <em>MLST</em>, <em>2</em>(3), 035010.
(<a href="https://doi.org/10.1088/2632-2153/abe347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The requirement for accelerated and quantitatively accurate screening of nuclear magnetic resonance spectra across the small molecules chemical compound space is two-fold: (1) a robust &#39;local&#39; machine learning (ML) strategy capturing the effect of the neighborhood on an atom&#39;s &#39;near-sighted&#39; property—chemical shielding; (2) an accurate reference dataset generated with a state-of-the-art first-principles method for training. Herein we report the QM9-NMR dataset comprising isotropic shielding of over 0.8 million C atoms in 134k molecules of the QM9 dataset in gas and five common solvent phases. Using these data for training, we present benchmark results for the prediction transferability of kernel-ridge regression models with popular local descriptors. Our best model, trained on 100k samples, accurately predicts isotropic shielding of 50k &#39;hold-out&#39; atoms with a mean error of less than 1.9 ppm. For the rapid prediction of new query molecules, the models were trained on geometries from an inexpensive theory. Furthermore, by using a Δ-ML strategy, we quench the error below 1.4 ppm. Finally, we test the transferability on non-trivial benchmark sets that include benchmark molecules comprising 10–17 heavy atoms and drugs.},
  archive      = {J_MLST},
  author       = {Amit Gupta and Sabyasachi Chakraborty and Raghunathan Ramakrishnan},
  doi          = {10.1088/2632-2153/abe347},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Revving up 13C NMR shielding predictions across chemical space: Benchmarks for atoms-in-molecules kernel machine learning with new data for 134 kilo molecules},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploration of transferable and uniformly accurate neural
network interatomic potentials using optimal experimental design.
<em>MLST</em>, <em>2</em>(3), 035009. (<a
href="https://doi.org/10.1088/2632-2153/abe294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been proven to have the potential to bridge the gap between the accuracy of ab initio methods and the efficiency of empirical force fields. Neural networks are one of the most frequently used approaches to construct high-dimensional potential energy surfaces. Unfortunately, they lack an inherent uncertainty estimation which is necessary for efficient and automated sampling through the chemical and conformational space to find extrapolative configurations. The identification of the latter is needed for the construction of transferable and uniformly accurate potential energy surfaces. In this paper, we propose an active learning approach that uses the estimated model&#39;s output variance derived in the framework of the optimal experimental design. This method has several advantages compared to the established active learning approaches, e.g. Query-by-Committee, Monte Carlo dropout, feature and latent distances, in terms of the predictive power and computational efficiency. We have shown that the application of the proposed active learning scheme leads to transferable and uniformly accurate potential energy surfaces constructed using only a small fraction of data points. Additionally, it is possible to define a natural threshold value for the proposed uncertainty metric which offers the possibility to generate highly informative training data on-the-fly.},
  archive      = {J_MLST},
  author       = {Viktor Zaverkin and Johannes Kästner},
  doi          = {10.1088/2632-2153/abe294},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Exploration of transferable and uniformly accurate neural network interatomic potentials using optimal experimental design},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matrix and tensor completion using tensor ring decomposition
with sparse representation. <em>MLST</em>, <em>2</em>(3), 035008. (<a
href="https://doi.org/10.1088/2632-2153/abcb4f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Completing a data tensor with structured missing components is a challenging task where the missing components are not distributed randomly but they admit some regular patterns, e.g. missing columns and rows or missing blocks/patches. Many of the existing tensor completion algorithms are not able to handle such scenarios. In this paper, we propose a novel and efficient approach for matrix/tensor completion by applying Hankelization and distributed tensor ring decomposition. Our main idea is first Hankelizing an incomplete data tensor in order to obtain high-order tensors and then completing the data tensor by imposing sparse representation on the core tensors in tensor ring format. We apply an efficient over-complete discrete cosine transform dictionary and sparse representation techniques to learn core tensors. Alternating direction methods of multiplier and accelerated proximal gradient approaches are used to solve the underlying optimization problems. Extensive simulations performed on image, video completions and time series forecasting show the validity and applicability of the method for different kinds of structured and random missing elements.},
  archive      = {J_MLST},
  author       = {Maame G Asante-Mensah and Salman Ahmadi-Asl and Andrzej Cichocki},
  doi          = {10.1088/2632-2153/abcb4f},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Matrix and tensor completion using tensor ring decomposition with sparse representation},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving epidemic testing and containment strategies using
machine learning. <em>MLST</em>, <em>2</em>(3), 035007. (<a
href="https://doi.org/10.1088/2632-2153/abf0f7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containment of epidemic outbreaks entails great societal and economic costs. Cost-effective containment strategies rely on efficiently identifying infected individuals, making the best possible use of the available testing resources. Therefore, quickly identifying the optimal testing strategy is of critical importance. Here, we demonstrate that machine learning can be used to identify which individuals are most beneficial to test, automatically and dynamically adapting the testing strategy to the characteristics of the disease outbreak. Specifically, we simulate an outbreak using the archetypal susceptible-infectious-recovered (SIR) model and we use data about the first confirmed cases to train a neural network that learns to make predictions about the rest of the population. Using these predictions, we manage to contain the outbreak more effectively and more quickly than with standard approaches. Furthermore, we demonstrate how this method can be used also when there is a possibility of reinfection (SIRS model) to efficiently eradicate an endemic disease.},
  archive      = {J_MLST},
  author       = {Laura Natali and Saga Helgadottir and Onofrio M Maragò and Giovanni Volpe},
  doi          = {10.1088/2632-2153/abf0f7},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {3},
  pages        = {035007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving epidemic testing and containment strategies using machine learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bin and hash method for analyzing reference data and
descriptors in machine learning potentials. <em>MLST</em>,
<em>2</em>(3), 037001. (<a
href="https://doi.org/10.1088/2632-2153/abe663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years the development of machine learning potentials (MLPs) has become a very active field of research. Numerous approaches have been proposed, which allow one to perform extended simulations of large systems at a small fraction of the computational costs of electronic structure calculations. The key to the success of modern MLPs is the close-to first principles quality description of the atomic interactions. This accuracy is reached by using very flexible functional forms in combination with high-level reference data from electronic structure calculations. These data sets can include up to hundreds of thousands of structures covering millions of atomic environments to ensure that all relevant features of the potential energy surface are well represented. The handling of such large data sets is nowadays becoming one of the main challenges in the construction of MLPs. In this paper we present a method, the bin-and-hash (BAH) algorithm, to overcome this problem by enabling the efficient identification and comparison of large numbers of multidimensional vectors. Such vectors emerge in multiple contexts in the construction of MLPs. Examples are the comparison of local atomic environments to identify and avoid unnecessary redundant information in the reference data sets that is costly in terms of both the electronic structure calculations as well as the training process, the assessment of the quality of the descriptors used as structural fingerprints in many types of MLPs, and the detection of possibly unreliable data points. The BAH algorithm is illustrated for the example of high-dimensional neural network potentials using atom-centered symmetry functions for the geometrical description of the atomic environments, but the method is general and can be combined with any current type of MLP.},
  archive      = {J_MLST},
  author       = {Martín Leandro Paleico and Jörg Behler},
  doi          = {10.1088/2632-2153/abe663},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {3},
  pages        = {037001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A bin and hash method for analyzing reference data and descriptors in machine learning potentials},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards automated analysis for neutron reflectivity.
<em>MLST</em>, <em>2</em>(3), 035006. (<a
href="https://doi.org/10.1088/2632-2153/abe7b5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a neural network-based tool for the automatic estimation of thin film thicknesses and scattering length densities from neutron reflectivity curves. The neural network sits within a data pipeline, that takes raw data from a neutron reflectometer, and outputs data and parameter estimates into a fitting program for end user analysis. Our tool deals with simple cases, predicting the number of layers and layer parameters up to three layers on a bulk substrate. This provides good accuracy in parameter estimation, while covering a large portion of the use case. By automating steps in data analysis that only require semi-expert knowledge, we lower the barrier to on-experiment data analysis, allowing better utility to be made from large scale facility experiments. Transfer learning showed that our tool works for x-ray reflectivity, and all code is freely available on GitHub (neutron-net 2020, available at: https://github.com/xmironov/neutron-net ) (Accessed: 25 June 2020).},
  archive      = {J_MLST},
  author       = {Daniil Mironov and James H Durant and Rebecca Mackenzie and Joshaniel F K Cooper},
  doi          = {10.1088/2632-2153/abe7b5},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {3},
  pages        = {035006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards automated analysis for neutron reflectivity},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU coprocessors as a service for deep learning inference in
high energy physics. <em>MLST</em>, <em>2</em>(3), 035005. (<a
href="https://doi.org/10.1088/2632-2153/abec21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the next decade, the demands for computing in large scientific experiments are expected to grow tremendously. During the same time period, CPU performance increases will be limited. At the CERN Large Hadron Collider (LHC), these two issues will confront one another as the collider is upgraded for high luminosity running. Alternative processors such as graphics processing units (GPUs) can resolve this confrontation provided that algorithms can be sufficiently accelerated. In many cases, algorithmic speedups are found to be largest through the adoption of deep learning algorithms. We present a comprehensive exploration of the use of GPU-based hardware acceleration for deep learning inference within the data reconstruction workflow of high energy physics. We present several realistic examples and discuss a strategy for the seamless integration of coprocessors so that the LHC can maintain, if not exceed, its current performance throughout its running.},
  archive      = {J_MLST},
  author       = {Jeffrey Krupa and Kelvin Lin and Maria Acosta Flechas and Jack Dinsmore and Javier Duarte and Philip Harris and Scott Hauck and Burt Holzman and Shih-Chieh Hsu and Thomas Klijnsma and Mia Liu and Kevin Pedro and Dylan Rankin and Natchanon Suaysom and Matt Trahms and Nhan Tran},
  doi          = {10.1088/2632-2153/abec21},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {3},
  pages        = {035005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {GPU coprocessors as a service for deep learning inference in high energy physics},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Control of stochastic quantum dynamics by differentiable
programming. <em>MLST</em>, <em>2</em>(3), 035004. (<a
href="https://doi.org/10.1088/2632-2153/abec22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control of the stochastic dynamics of a quantum system is indispensable in fields such as quantum information processing and metrology. However, there is no general ready-made approach to the design of efficient control strategies. Here, we propose a framework for the automated design of control schemes based on differentiable programming. We apply this approach to the state preparation and stabilization of a qubit subjected to homodyne detection. To this end, we formulate the control task as an optimization problem where the loss function quantifies the distance from the target state, and we employ neural networks (NNs) as controllers. The system&#39;s time evolution is governed by a stochastic differential equation (SDE). To implement efficient training, we backpropagate the gradient information from the loss function through the SDE solver using adjoint sensitivity methods. As a first example, we feed the quantum state to the controller and focus on different methods of obtaining gradients. As a second example, we directly feed the homodyne detection signal to the controller. The instantaneous value of the homodyne current contains only very limited information on the actual state of the system, masked by unavoidable photon-number fluctuations. Despite the resulting poor signal-to-noise ratio, we can train our controller to prepare and stabilize the qubit to a target state with a mean fidelity of around 85%. We also compare the solutions found by the NN to a hand-crafted control strategy.},
  archive      = {J_MLST},
  author       = {Frank Schäfer and Pavel Sekatski and Martin Koppenhöfer and Christoph Bruder and Michal Kloc},
  doi          = {10.1088/2632-2153/abec22},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {3},
  pages        = {035004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Control of stochastic quantum dynamics by differentiable programming},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deploying the big data science center at the shanghai
synchrotron radiation facility: The first superfacility platform in
china. <em>MLST</em>, <em>2</em>(3), 035003. (<a
href="https://doi.org/10.1088/2632-2153/abe193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent technological advances, large-scale experimental facilities generate huge datasets, into the petabyte range, every year, thereby creating the Big Data deluge effect. Data management, including the collection, management, and curation of these large datasets, is a significantly intensive precursor step in relation to the data analysis that underpins scientific investigations. The rise of artificial intelligence (AI), machine learning (ML), and robotic automation has changed the landscape for experimental facilities, producing a paradigm shift in how different datasets are leveraged for improved intelligence, operation, and data analysis. Therefore, such facilities, known as superfacilities, which fully enable user science while addressing the challenges of the Big Data deluge, are critical for the scientific community. In this work, we discuss the process of setting up the Big Data Science Center within the Shanghai Synchrotron Radiation Facility (SSRF), China&#39;s first superfacility. We provide details of our initiatives for enabling user science at SSRF, with particular consideration given to recent developments in AI, ML, and robotic automation.},
  archive      = {J_MLST},
  author       = {Chunpeng Wang and Feng Yu and Yiyang Liu and Xiaoyun Li and Jige Chen and Jeyan Thiyagalingam and Alessandro Sepe},
  doi          = {10.1088/2632-2153/abe193},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {3},
  pages        = {035003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deploying the big data science center at the shanghai synchrotron radiation facility: The first superfacility platform in china},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural networks and quantum field theory. <em>MLST</em>,
<em>2</em>(3), 035002. (<a
href="https://doi.org/10.1088/2632-2153/abeca3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a theoretical understanding of neural networks in terms of Wilsonian effective field theory. The correspondence relies on the fact that many asymptotic neural networks are drawn from Gaussian processes (GPs), the analog of non-interacting field theories. Moving away from the asymptotic limit yields a non-Gaussian process (NGP) and corresponds to turning on particle interactions, allowing for the computation of correlation functions of neural network outputs with Feynman diagrams. Minimal NGP likelihoods are determined by the most relevant non-Gaussian terms, according to the flow in their coefficients induced by the Wilsonian renormalization group. This yields a direct connection between overparameterization and simplicity of neural network likelihoods. Whether the coefficients are constants or functions may be understood in terms of GP limit symmetries, as expected from &#39;t Hooft&#39;s technical naturalness. General theoretical calculations are matched to neural network experiments in the simplest class of models allowing the correspondence. Our formalism is valid for any of the many architectures that becomes a GP in an asymptotic limit, a property preserved under certain types of training.},
  archive      = {J_MLST},
  author       = {James Halverson and Anindita Maiti and Keegan Stoner},
  doi          = {10.1088/2632-2153/abeca3},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {3},
  pages        = {035002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural networks and quantum field theory},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning for neutron reflectometry data analysis of
two-layer thin films *. <em>MLST</em>, <em>2</em>(3), 035001. (<a
href="https://doi.org/10.1088/2632-2153/abf257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neutron reflectometry (NR) is a powerful tool for probing thin films at length scales down to nanometers. We investigated the use of a neural network to predict a two-layer thin film structure to model a given measured reflectivity curve. Application of this neural network to predict a thin film structure revealed that it was accurate and could provide an excellent starting point for traditional fitting methods. Employing prediction-guided fitting has considerable potential for more rapidly producing a result compared to the labor-intensive but commonly-used approach of trial and error searches prior to refinement. A deeper look at the stability of the predictive power of the neural network against statistical fluctuations of measured reflectivity profiles showed that the predictions are stable. We conclude that the approach presented here can provide valuable assistance to users of NR and should be further extended for use in studies of more complex n-layer thin film systems. This result also opens up the possibility of developing adaptive measurement systems in the future.},
  archive      = {J_MLST},
  author       = {Mathieu Doucet and Richard K Archibald and William T Heller},
  doi          = {10.1088/2632-2153/abf257},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {3},
  pages        = {035001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning for neutron reflectometry data analysis of two-layer thin films *},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to unknot. <em>MLST</em>, <em>2</em>(2), 025035.
(<a href="https://doi.org/10.1088/2632-2153/abe91f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce natural language processing into the study of knot theory, as made natural by the braid word representation of knots. We study the UNKNOT problem of determining whether or not a given knot is the unknot. After describing an algorithm to randomly generate N -crossing braids and their knot closures and discussing the induced prior on the distribution of knots, we apply binary classification to the UNKNOT decision problem. We find that the Reformer and shared-QK Transformer network architectures outperform fully-connected networks, though all perform at \gtrsim 95% accuracy. Perhaps surprisingly, we find that accuracy increases with the length of the braid word, and that the networks learn a direct correlation between the confidence of their predictions and the degree of the Jones polynomial. Finally, we utilize reinforcement learning (RL) to find sequences of Markov moves and braid relations that simplify knots and can identify unknots by explicitly giving the sequence of unknotting actions. Trust region policy optimization (TRPO) performs consistently well, reducing \gtrsim 80% of the unknots with up to 96 crossings we tested to the empty braid word, and thoroughly outperformed other RL algorithms and random walkers. Studying these actions, we find that braid relations are more useful in simplifying to the unknot than one of the Markov moves.},
  archive      = {J_MLST},
  author       = {Sergei Gukov and James Halverson and Fabian Ruehle and Piotr Sułkowski},
  doi          = {10.1088/2632-2153/abe91f},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025035},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Learning to unknot},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards reflectivity profile inversion through artificial
neural networks. <em>MLST</em>, <em>2</em>(2), 025034. (<a
href="https://doi.org/10.1088/2632-2153/abe564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of specular neutron and x-ray reflectometry is to infer a material&#39;s scattering length density (SLD) profile from its experimental reflectivity curves. This paper focuses on the investigation of an original approach to the ill-posed non-invertible problem which involves the use of artificial neural networks (ANNs). In particular, the numerical experiments described here deal with large data sets of simulated reflectivity curves and SLD profiles, and aim to assess the applicability of data science and machine learning technology to the analysis of data generated at large-scale neutron scattering facilities. It is demonstrated that, under certain circumstances, properly trained deep neural networks are capable of correctly recovering plausible SLD profiles when presented with previously unseen simulated reflectivity curves. When the necessary conditions are met, a proper implementation of the described approach would offer two main advantages over traditional fitting methods when dealing with real experiments, namely (1) sample physical models are described under a new paradigm: detailed layer-by-layer descriptions (SLDs, thicknesses, roughnesses) are replaced by parameter-free curves ρ ( z ), allowing a priori assumptions to be used in terms of the sample family to which a given sample belongs (e.g. &#39;thin film,&#39; &#39;lamellar structure&#39;,etc.); (2) the time required to reach a solution is shrunk by orders of magnitude, enabling faster batch analysis for large datasets.},
  archive      = {J_MLST},
  author       = {Juan Manuel Carmona Loaiza and Zamaan Raza},
  doi          = {10.1088/2632-2153/abe564},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025034},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards reflectivity profile inversion through artificial neural networks},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep dose plugin: Towards real-time monte carlo dose
calculation through a deep learning-based denoising algorithm.
<em>MLST</em>, <em>2</em>(2), 025033. (<a
href="https://doi.org/10.1088/2632-2153/abdbfe">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo (MC) simulation is considered the gold standard method for radiotherapy dose calculation. However, achieving high precision requires a large number of simulation histories, which is time-consuming. The use of computer graphics processing units (GPUs) has greatly accelerated MC simulation and allows dose calculation within a few minutes for a typical radiotherapy treatment plan. However, some clinical applications demand real-time efficiency for MC dose calculation. To tackle this problem, we have developed a real-time, deep learning (DL)-based dose denoiser that can be plugged into a current GPU-based MC dose engine to enable real-time MC dose calculation. We used two different acceleration strategies to achieve this goal: (1) we applied voxel unshuffle and voxel shuffle operators to decrease the input and output sizes without any information loss, and (2) we decoupled the 3D volumetric convolution into a 2D axial convolution and a 1D slice convolution. In addition, we used a weakly supervised learning framework to train the network, which greatly reduces the size of the required training dataset and thus enables fast fine-tuning-based adaptation of the trained model to different radiation beams. Experimental results show that the proposed denoiser can run in as little as 39 ms, which is 11.6 times faster than the baseline model. As a result, the whole MC dose calculation pipeline can be finished within ∼ 0.15 s, including both GPU MC dose calculation and DL-based denoising, achieving the real-time efficiency needed for some radiotherapy applications, such as online adaptive radiotherapy.},
  archive      = {J_MLST},
  author       = {Ti Bai and Biling Wang and Dan Nguyen and Steve Jiang},
  doi          = {10.1088/2632-2153/abdbfe},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025033},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep dose plugin: Towards real-time monte carlo dose calculation through a deep learning-based denoising algorithm},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monochromatic image reconstruction via machine learning.
<em>MLST</em>, <em>2</em>(2), 025032. (<a
href="https://doi.org/10.1088/2632-2153/abdbff">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray computed tomography (CT) is a nondestructive imaging technique to reconstruct cross-sectional images of an object using x-ray measurements taken from different view angles for medical diagnosis, therapeutic planning, security screening, and other applications. In clinical practice, the x-ray tube emits polychromatic x-rays, and the x-ray detector array operates in the energy-integrating mode to acquire energy intensity. This physical process of x-ray imaging is accurately described by an energy-dependent non-linear integral equation on the basis of the Beer–Lambert law. However, the non-linear model is not invertible using a computationally efficient solution and is often approximated as a linear integral model in the form of the Radon transform, which basically loses energy-dependent information. This approximate model produces an inaccurate quantification of attenuation images, suffering from beam-hardening effects. In this paper, a machine learning-based approach is proposed to correct the model mismatch to achieve quantitative CT imaging. Specifically, a one-dimensional network model is proposed to learn a non-linear transform from a training dataset to map a polychromatic CT image to its monochromatic sinogram at a pre-specified energy level, realizing virtual monochromatic (VM) imaging effectively and efficiently. Our results show that the proposed method recovers high-quality monochromatic projections with an average relative error of less than 2%. The resultant x-ray VM imaging can be applied for beam-hardening correction, material differentiation and tissue characterization, and proton therapy treatment planning.},
  archive      = {J_MLST},
  author       = {Wenxiang Cong and Yan Xi and Bruno De Man and Ge Wang},
  doi          = {10.1088/2632-2153/abdbff},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025032},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Monochromatic image reconstruction via machine learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional neural network based non-iterative
reconstruction for accelerating neutron tomography *. <em>MLST</em>,
<em>2</em>(2), 025031. (<a
href="https://doi.org/10.1088/2632-2153/abde8e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neutron computed tomography (NCT), a 3D non-destructive characterization technique, is carried out at nuclear reactor or spallation neutron source-based user facilities. Because neutrons are not severely attenuated by heavy elements and are sensitive to light elements like hydrogen, neutron radiography and computed tomography offer a complementary contrast to x-ray CT conducted at a synchrotron user facility. However, compared to synchrotron x-ray CT, the acquisition time for an NCT scan can be orders of magnitude higher due to lower source flux, low detector efficiency and the need to collect a large number of projection images for a high-quality reconstruction when using conventional algorithms. As a result of the long scan times for NCT, the number and type of experiments that can be conducted at a user facility is severely restricted. Recently, several deep convolutional neural network (DCNN) based algorithms have been introduced in the context of accelerating CT scans that can enable high quality reconstructions from sparse-view data. In this paper, we introduce DCNN algorithms to obtain high-quality reconstructions from sparse-view and low signal-to-noise ratio NCT data-sets thereby enabling accelerated scans. Our method is based on the supervised learning strategy of training a DCNN to map a low-quality reconstruction from sparse-view data to a higher quality reconstruction. Specifically, we evaluate the performance of two popular DCNN architectures—one based on using patches for training and the other on using the full images for training. We observe that both the DCNN architectures offer improvements in performance over classical multi-layer perceptron as well as conventional CT reconstruction algorithms. Our results illustrate that the DCNN can be a powerful tool to obtain high-quality NCT reconstructions from sparse-view data thereby enabling accelerated NCT scans for increasing user-facility throughput or enabling high-resolution time-resolved NCT scans.},
  archive      = {J_MLST},
  author       = {Singanallur Venkatakrishnan and Amirkoushyar Ziabari and Jacob Hinkle and Andrew W Needham and Jeffrey M Warren and Hassina Z Bilheux},
  doi          = {10.1088/2632-2153/abde8e},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025031},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Convolutional neural network based non-iterative reconstruction for accelerating neutron tomography *},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven enhancement of cubic phase stability in
mixed-cation perovskites. <em>MLST</em>, <em>2</em>(2), 025030. (<a
href="https://doi.org/10.1088/2632-2153/abdaf9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixing cations has been a successful strategy in perovskite synthesis by solution-processing, delivering improvements in the thermodynamic stability as well as in the lattice parameter control. Unfortunately, the relation between a given cation mixture and the associated structural deformation is not well-established, a fact that hinders an adequate identification of the optimum chemical compositions. Such difficulty arises since local distortion and microscopic disorder influence structural stability and also determine phase segregation. Hence, the search for an optimum composition is currently based on experimental trial and error, a tedious and high-cost process. Here, we report on a machine-learning-reinforced cubic-phase-perovskite stability predictor that has been constructed over an extensive dataset of first-principles calculations. Such a predictor allows us to determine the cubic phase stability at a given cation mixture regardless of the various cations&#39; pair and concentration, even assessing very dilute concentrations, a notoriously challenging task for first-principles calculations. In particular, we construct machine learning models, predicting multiple target quantities such as the enthalpy of mixing and various octahedral distortions. It is then the combination of these targets that guide the laboratory synthesis. Our theoretical analysis is also validated by the experimental synthesis and characterization of methylammonium–dimethylammonium-mixed perovskite thin films, demonstrating the ability of the stability predictor to drive the chemical design of this class of materials.},
  archive      = {J_MLST},
  author       = {Heesoo Park and Adnan Ali and Raghvendra Mall and Halima Bensmail and Stefano Sanvito and Fedwa El-Mellouhi},
  doi          = {10.1088/2632-2153/abdaf9},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025030},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Data-driven enhancement of cubic phase stability in mixed-cation perovskites},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed deep reinforcement learning for simulation
control. <em>MLST</em>, <em>2</em>(2), 025029. (<a
href="https://doi.org/10.1088/2632-2153/abdaf8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several applications in the scientific simulation of physical systems can be formulated as control/optimization problems. The computational models for such systems generally contain hyperparameters, which control solution fidelity and computational expense. The tuning of these parameters is non-trivial and the general approach is to manually &#39;spot-check&#39; for good combinations. This is because optimal hyperparameter configuration search becomes intractable when the parameter space is large and when they may vary dynamically. To address this issue, we present a framework based on deep reinforcement learning (RL) to train a deep neural network agent that controls a model solve by varying parameters dynamically. First, we validate our RL framework for the problem of controlling chaos in chaotic systems by dynamically changing the parameters of the system. Subsequently, we illustrate the capabilities of our framework for accelerating the convergence of a steady-state computational fluid dynamics solver by automatically adjusting the relaxation factors of the discretized Navier–Stokes equations during run-time. The results indicate that the run-time control of the relaxation factors by the learned policy leads to a significant reduction in the number of iterations for convergence compared to the random selection of the relaxation factors. Our results point to potential benefits for learning adaptive hyperparameter learning strategies across different geometries and boundary conditions with implications for reduced computational campaign expenses 4 .},
  archive      = {J_MLST},
  author       = {Suraj Pawar and Romit Maulik},
  doi          = {10.1088/2632-2153/abdaf8},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025029},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Distributed deep reinforcement learning for simulation control},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of feature space in atomistic learning.
<em>MLST</em>, <em>2</em>(2), 025028. (<a
href="https://doi.org/10.1088/2632-2153/abdaf7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient, physically-inspired descriptors of the structure and composition of molecules and materials play a key role in the application of machine-learning techniques to atomistic simulations. The proliferation of approaches, as well as the fact that each choice of features can lead to very different behavior depending on how they are used, e.g. by introducing non-linear kernels and non-Euclidean metrics to manipulate them, makes it difficult to objectively compare different methods, and to address fundamental questions on how one feature space is related to another. In this work we introduce a framework to compare different sets of descriptors, and different ways of transforming them by means of metrics and kernels, in terms of the structure of the feature space that they induce. We define diagnostic tools to determine whether alternative feature spaces contain equivalent amounts of information, and whether the common information is substantially distorted when going from one feature space to another. We compare, in particular, representations that are built in terms of n -body correlations of the atom density, quantitatively assessing the information loss associated with the use of low-order features. We also investigate the impact of different choices of basis functions and hyperparameters of the widely used SOAP and Behler–Parrinello features, and investigate how the use of non-linear kernels, and of a Wasserstein-type metric, change the structure of the feature space in comparison to a simpler linear feature space.},
  archive      = {J_MLST},
  author       = {Alexander Goscinski and Guillaume Fraux and Giulio Imbalzano and Michele Ceriotti},
  doi          = {10.1088/2632-2153/abdaf7},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025028},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {The role of feature space in atomistic learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scientific intuition inspired by machine learning-generated
hypotheses. <em>MLST</em>, <em>2</em>(2), 025027. (<a
href="https://doi.org/10.1088/2632-2153/abda08">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning with application to questions in the physical sciences has become a widely used tool, successfully applied to classification, regression and optimization tasks in many areas. Research focus mostly lies in improving the accuracy of the machine learning models in numerical predictions, while scientific understanding is still almost exclusively generated by human researchers analysing numerical results and drawing conclusions. In this work, we shift the focus on the insights and the knowledge obtained by the machine learning models themselves. In particular, we study how it can be extracted and used to inspire human scientists to increase their intuitions and understanding of natural systems. We apply gradient boosting in decision trees to extract human-interpretable insights from big data sets from chemistry and physics. In chemistry, we not only rediscover widely know rules of thumb but also find new interesting motifs that tell us how to control solubility and energy levels of organic molecules. At the same time, in quantum physics, we gain new understanding on experiments for quantum entanglement. The ability to go beyond numerics and to enter the realm of scientific insight and hypothesis generation opens the door to use machine learning to accelerate the discovery of conceptual understanding in some of the most challenging domains of science.},
  archive      = {J_MLST},
  author       = {Pascal Friederich and Mario Krenn and Isaac Tamblyn and Alán Aspuru-Guzik},
  doi          = {10.1088/2632-2153/abda08},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025027},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Scientific intuition inspired by machine learning-generated hypotheses},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient training of energy-based models via spin-glass
control. <em>MLST</em>, <em>2</em>(2), 025026. (<a
href="https://doi.org/10.1088/2632-2153/abe807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new family of energy-based probabilistic graphical models for efficient unsupervised learning. Its definition is motivated by the control of the spin-glass properties of the Ising model described by the weights of Boltzmann machines. We use it to learn the Bars and Stripes dataset of various sizes and the MNIST dataset, and show how they quickly achieve the performance offered by standard methods for unsupervised learning. Our results indicate that the standard initialization of Boltzmann machines with random weights equivalent to spin-glass models is an unnecessary bottleneck in the process of training. Furthermore, this new family allows for very easy access to low-energy configurations, which points to new, efficient training algorithms. The simplest variant of such algorithms approximates the negative phase of the log-likelihood gradient with no Markov chain Monte Carlo sampling costs at all, and with an accuracy sufficient to achieve good learning and generalization.},
  archive      = {J_MLST},
  author       = {Alejandro Pozas-Kerstjens and Gorka Muñoz-Gil and Eloy Piñol and Miguel Ángel García-March and Antonio Acín and Maciej Lewenstein and Przemysław R Grzybowski},
  doi          = {10.1088/2632-2153/abe807},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025026},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Efficient training of energy-based models via spin-glass control},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaming the beamlines—employing reinforcement learning to
maximize scientific outcomes at large-scale user facilities.
<em>MLST</em>, <em>2</em>(2), 025025. (<a
href="https://doi.org/10.1088/2632-2153/abc9fc">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beamline experiments at central facilities are increasingly demanding of remote, high-throughput, and adaptive operation conditions. To accommodate such needs, new approaches must be developed that enable on-the-fly decision making for data intensive challenges. Reinforcement learning (RL) is a domain of AI that holds the potential to enable autonomous operations in a feedback loop between beamline experiments and trained agents. Here, we outline the advanced data acquisition and control software of the Bluesky suite, and demonstrate its functionality with a canonical RL problem: cartpole. We then extend these methods to efficient use of beamline resources by using RL to develop an optimal measurement strategy for samples with different scattering characteristics. The RL agents converge on the empirically optimal policy when under-constrained with time. When resource limited, the agents outperform a naive or sequential measurement strategy, often by a factor of 100%. We interface these methods directly with the data storage and provenance technologies at the National Synchrotron Light Source II, thus demonstrating the potential for RL to increase the scientific output of beamlines, and layout the framework for how to achieve this impact.},
  archive      = {J_MLST},
  author       = {Phillip M Maffettone and Joshua K Lynch and Thomas A Caswell and Clara E Cook and Stuart I Campbell and Daniel Olds},
  doi          = {10.1088/2632-2153/abc9fc},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025025},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Gaming the beamlines—employing reinforcement learning to maximize scientific outcomes at large-scale user facilities},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven molecular design for discovery and synthesis of
novel ligands: A case study on SARS-CoV-2. <em>MLST</em>, <em>2</em>(2),
025024. (<a href="https://doi.org/10.1088/2632-2153/abe808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bridging systems biology and drug design, we propose a deep learning framework for de novo discovery of molecules tailored to bind with given protein targets. Our methodology is exemplified by the task of designing antiviral candidates to target SARS-CoV-2 related proteins. Crucially, our framework does not require fine-tuning for specific proteins but is demonstrated to generalize in proposing ligands with high predicted binding affinities against unseen targets. Coupling our framework with the automatic retrosynthesis prediction of IBM RXN for Chemistry , we demonstrate the feasibility of swift chemical synthesis of molecules with potential antiviral properties that were designed against a specific protein target. In particular, we synthesize an antiviral candidate designed against the host protein angiotensin converting enzyme 2 (ACE2); a surface receptor on human respiratory epithelial cells that facilitates SARS-CoV-2 cell entry through its spike glycoprotein. This is achieved as follows. First, we train a multimodal ligand–protein binding affinity model on predicting affinities of bioactive compounds to target proteins and couple this model with pharmacological toxicity predictors. Exploiting this multi-objective as a reward function of a conditional molecular generator that consists of two variational autoencoders (VAE), our framework steers the generation toward regions of the chemical space with high-reward molecules. Specifically, we explore a challenging setting of generating ligands against unseen protein targets by performing a leave-one-out-cross-validation on 41 SARS-CoV-2-related target proteins. Using deep reinforcement learning, it is demonstrated that in 35 out of 41 cases, the generation is biased towards sampling binding ligands, with an average increase of 83% comparing to an unbiased VAE. The generated molecules exhibit favorable properties in terms of target binding affinity, selectivity and drug-likeness. We use molecular retrosynthetic models to provide a synthetic accessibility assessment of the best generated hit molecules. Finally, with this end-to-end framework, we synthesize 3-Bromobenzylamine, a potential inhibitor of the host ACE2 protein, solely based on the recommendations of a molecular retrosynthesis model and a synthesis protocol prediction model. We hope that our framework can contribute towards swift discovery of de novo molecules with desired pharmacological properties.},
  archive      = {J_MLST},
  author       = {Jannis Born and Matteo Manica and Joris Cadow and Greta Markert and Nil Adell Mill and Modestas Filipavicius and Nikita Janakarajan and Antonio Cardinale and Teodoro Laino and María Rodríguez Martínez},
  doi          = {10.1088/2632-2153/abe808},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025024},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Data-driven molecular design for discovery and synthesis of novel ligands: A case study on SARS-CoV-2},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph networks for molecular design. <em>MLST</em>,
<em>2</em>(2), 025023. (<a
href="https://doi.org/10.1088/2632-2153/abcf91">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods applied to chemistry can be used to accelerate the discovery of new molecules. This work introduces GraphINVENT, a platform developed for graph-based molecular design using graph neural networks (GNNs). GraphINVENT uses a tiered deep neural network architecture to probabilistically generate new molecules a single bond at a time. All models implemented in GraphINVENT can quickly learn to build molecules resembling the training set molecules without any explicit programming of chemical rules. The models have been benchmarked using the MOSES distribution-based metrics, showing how GraphINVENT models compare well with state-of-the-art generative models. This work compares six different GNN-based generative models in GraphINVENT, and shows that ultimately the gated-graph neural network performs best against the metrics considered here.},
  archive      = {J_MLST},
  author       = {Rocío Mercado and Tobias Rastemo and Edvard Lindelöf and Günter Klambauer and Ola Engkvist and Hongming Chen and Esben Jannik Bjerrum},
  doi          = {10.1088/2632-2153/abcf91},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025023},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Graph networks for molecular design},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast modeling and understanding fluid dynamics systems with
encoder–decoder networks. <em>MLST</em>, <em>2</em>(2), 025022. (<a
href="https://doi.org/10.1088/2632-2153/abd1cf">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Is a deep learning model capable of representing systems governed by certain first principle physics laws by only observing the system&#39;s output? In an effort to simulate two-dimensional subsurface fluid dynamics in porous media, we found that an accurate deep-learning-based proxy model can be taught efficiently by a computationally expensive finite-volume-based simulator. We pose the problem as an image-to-image regression, running the simulator with different input parameters to furnish a synthetic training dataset upon which we fit the deep learning models. Since the data is spatiotemporal, we compare the performance of three alternative treatments of time; a convolutional LSTM, an autoencoder network that treats time as a direct input and an echo state network. Adversarial methods are adopted to address the sharp spatial gradient in the fluid dynamics problem. Compared to traditional simulation, the proposed deep learning approach enables much faster forward computation, which allows us to explore more scenarios with a much larger parameter space given the same time. It is shown that the improved forward computation efficiency is particularly valuable in solving inversion problems, where the physics model has unknown parameters to be determined by history matching. By computing the pixel-level attention of the trained model, we quantify the sensitivity of the deep learning model to key physical parameters and hence demonstrate that the inverse problem can be solved with great acceleration. We assess the efficacy of the machine learning surrogate in terms of its training speed and accuracy. The network can be trained within minutes using limited training data and achieve accuracy that scales desirably with the amount of training data supplied.},
  archive      = {J_MLST},
  author       = {Rohan Thavarajah and Xiang Zhai and Zheren Ma and David Castineira},
  doi          = {10.1088/2632-2153/abd1cf},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025022},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Fast modeling and understanding fluid dynamics systems with encoder–decoder networks},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multidimensional analysis of excitonic spectra of monolayers
of tungsten disulphide: Toward computer-aided identification of
structural and environmental perturbations of 2D materials.
<em>MLST</em>, <em>2</em>(2), 025021. (<a
href="https://doi.org/10.1088/2632-2153/abd87c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite 2D materials holding great promise for a broad range of applications, the proliferation of devices and their fulfillment of real-life demands are still far from being realized. Experimentally obtainable samples commonly experience a wide range of perturbations (ripples and wrinkles, point and line defects, grain boundaries, strain field, doping, water intercalation, oxidation, edge reconstructions) significantly deviating the properties from idealistic models. These perturbations, in general, can be entangled or occur in groups with each group forming a complex perturbation making the interpretations of observable physical properties and the disentanglement of simultaneously acting effects a highly non-trivial task even for an experienced researcher. Here we generalise statistical correlation analysis of excitonic spectra of monolayer WS 2 , acquired by hyperspectral absorption and photoluminescence imaging, to a multidimensional case, and examine multidimensional correlations via unsupervised machine learning algorithms. Using principal component analysis we are able to identify four dominant components that are correlated with tensile strain, disorder induced by adsorption or intercalation of environmental molecules, multi-layer regions and charge doping, respectively. This approach has the potential to determine the local environment of WS 2 monolayers or other 2D materials from simple optical measurements, and paves the way toward advanced, machine-aided, characterization of monolayer matter.},
  archive      = {J_MLST},
  author       = {Pavel V Kolesnichenko and Qianhui Zhang and Changxi Zheng and Michael S Fuhrer and Jeffrey A Davis},
  doi          = {10.1088/2632-2153/abd87c},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025021},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multidimensional analysis of excitonic spectra of monolayers of tungsten disulphide: Toward computer-aided identification of structural and environmental perturbations of 2D materials},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How isotropic kernels perform on simple invariants.
<em>MLST</em>, <em>2</em>(2), 025020. (<a
href="https://doi.org/10.1088/2632-2153/abd485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how the training curve of isotropic kernel methods depends on the symmetry of the task to be learned, in several settings. (i) We consider a regression task, where the target function is a Gaussian random field that depends only on d_\parallel variables, fewer than the input dimension d . We compute the expected test error that follows \epsilon\sim p^{-\beta} where p is the size of the training set. We find that β ∼ 1/ d independently of d_\parallel , supporting previous findings that the presence of invariants does not resolve the curse of dimensionality for kernel regression. (ii) Next we consider support-vector binary classification and introduce the stripe model , where the data label depends on a single coordinate y(\underline x) = y(x_1) , corresponding to parallel decision boundaries separating labels of different signs, and consider that there is no margin at these interfaces. We argue and confirm numerically that, for large bandwidth, \beta = \frac{d-1+\xi}{3d-3+\xi} , where ξ ∈ (0, 2) is the exponent characterizing the singularity of the kernel at the origin. This estimation improves classical bounds obtainable from Rademacher complexity. In this setting there is no curse of dimensionality since \beta\rightarrow 1/3 as d\rightarrow\infty . (iii) We confirm these findings for the spherical model , for which y(\underline x) = y(||\underline x||) . (iv) In the stripe model, we show that, if the data are compressed along their invariants by some factor λ (an operation believed to take place in deep networks), the test error is reduced by a factor \lambda^{-\frac{2(d-1)}{3d-3+\xi}} .},
  archive      = {J_MLST},
  author       = {Jonas Paccolat and Stefano Spigler and Matthieu Wyart},
  doi          = {10.1088/2632-2153/abd485},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025020},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {How isotropic kernels perform on simple invariants},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inception neural network for complete intersection
calabi–yau 3-folds. <em>MLST</em>, <em>2</em>(2), 02LT03. (<a
href="https://doi.org/10.1088/2632-2153/abda61">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a neural network inspired by Google&#39;s Inception model to compute the Hodge number h 1,1 of complete intersection Calabi–Yau (CICY) 3-folds. This architecture improves largely the accuracy of the predictions over existing results, giving already 97% of accuracy with just 30% of the data for training. Accuracy climbs to 99% when using 80% of the data for training. This proves that neural networks are a valuable resource to study geometric aspects in both pure mathematics and string theory.},
  archive      = {J_MLST},
  author       = {H Erbin and R Finotello},
  doi          = {10.1088/2632-2153/abda61},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {02LT03},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Inception neural network for complete intersection Calabi–Yau 3-folds},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid quantum regression model for the prediction of
molecular atomization energies. <em>MLST</em>, <em>2</em>(2), 025019.
(<a href="https://doi.org/10.1088/2632-2153/abd486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum machine learning is a relatively new research field that aims to combine the dramatic performance advantage offered by quantum computing and the ability of machine learning algorithms to learn complex distributions of high-dimensional data. The primary focus of this domain is the implementation of classical machine learning algorithms in the quantum mechanical domain and study of the speedup due to quantum parallelism, which could enable the development of novel techniques for solving problems such as quantum phase recognition and quantum error correction optimization. In this paper, we propose a hybrid quantum machine learning pipeline for predicting the atomization energies of various molecules using the nuclear charges and atomic positions of the constituent atoms. Firstly, we will be using a deep convolutional auto-encoder model for the feature extraction of data constructed from the eigenvalues and eigenvector centralities of the pairwise distance matrix calculated from atomic positions and the unrolled upper triangle of each Coulomb matrix calculated from nuclear charges, and we will then be using a quantum regression algorithm such as quantum linear regression, quantum radial basis function neural network and, a quantum neural network for estimating the atomization energy. The hybrid quantum neural network models do not seem to provide any speedup over their classical counterparts. Before implementing a quantum algorithm, we will also be using state-of-the-art classical machine learning and deep learning models such as XGBoost, multilayer perceptron, deep convolutional neural network, and a long short-term memory network to study the correlation between the extracted features and corresponding atomization energies of molecules.},
  archive      = {J_MLST},
  author       = {Pranath Reddy and Aranya B Bhattacherjee},
  doi          = {10.1088/2632-2153/abd486},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025019},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A hybrid quantum regression model for the prediction of molecular atomization energies},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel automatic scorpion-detection and -recognition system
based on machine-learning techniques. <em>MLST</em>, <em>2</em>(2),
025018. (<a href="https://doi.org/10.1088/2632-2153/abd51d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All species of scorpions can inject venom, some of them even with the possibility of killing a human. Therefore, early detection and identification are essential to minimize scorpion stings. In this paper, we propose a novel automatic system for the detection and recognition of scorpions using computer vision and machine learning (ML) approaches. Two complementary image-processing techniques were used for the proposed detection method to accurately and reliably detect the presence of scorpions. The first is based on the fluorescent characteristics of scorpions when exposed to ultraviolet light, and the second on the shape features of the scorpions. Also, three models based on ML algorithms for the image recognition and classification of scorpions are compared. In particular, the three species of scorpions found in La Plata city (Argentina): Bothriurus bonariensis (of no sanitary importance), Tityus trivittatus , and Tityus confluence (both of sanitary importance) have been researched using a local binary-pattern histogram algorithm and deep neural networks with transfer learning (DNNs with TL) and data augmentation (DNNs with TL and DA) approaches. A confusion matrix and a receiver operating characteristic curve were used to evaluate the quality of these models. The results obtained show that the model of DNN with TL and DA is the most efficient at simultaneously differentiating between Tityus and Bothriurus (for health security) and between T. trivittatus and T. confluence (for biological research purposes).},
  archive      = {J_MLST},
  author       = {Francisco L Giambelluca and Marcelo A Cappelletti and Jorge R Osio and Luis A Giambelluca},
  doi          = {10.1088/2632-2153/abd51d},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Novel automatic scorpion-detection and -recognition system based on machine-learning techniques},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Atomic permutationally invariant polynomials for fitting
molecular force fields. <em>MLST</em>, <em>2</em>(2), 025017. (<a
href="https://doi.org/10.1088/2632-2153/abd51e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and explore an approach for constructing force fields for small molecules, which combines intuitive low body order empirical force field terms with the concepts of data driven statistical fits of recent machine learned potentials. We bring these two key ideas together to bridge the gap between established empirical force fields that have a high degree of transferability on the one hand, and the machine learned potentials that are systematically improvable and can converge to very high accuracy, on the other. Our framework extends the atomic permutationally invariant polynomials (aPIP) developed for elemental materials in (2019 Mach. Learn.: Sci. Technol. 1 015004) to molecular systems. The body order decomposition allows us to keep the dimensionality of each term low, while the use of an iterative fitting scheme as well as regularisation procedures improve the extrapolation outside the training set. We investigate aPIP force fields with up to generalised 4-body terms, and examine the performance on a set of small organic molecules. We achieve a high level of accuracy when fitting individual molecules, comparable to those of the many-body machine learned force fields. Fitted to a combined training set of short linear alkanes, the accuracy of the aPIP force field still significantly exceeds what can be expected from classical empirical force fields, while retaining reasonable transferability to both configurations far from the training set and to new molecules.},
  archive      = {J_MLST},
  author       = {Alice E A Allen and Geneviève Dusson and Christoph Ortner and Gábor Csányi},
  doi          = {10.1088/2632-2153/abd51e},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Atomic permutationally invariant polynomials for fitting molecular force fields},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data augmentation with mobius transformations.
<em>MLST</em>, <em>2</em>(2), 025016. (<a
href="https://doi.org/10.1088/2632-2153/abd615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation has led to substantial improvements in the performance and generalization of deep models, and remains a highly adaptable method to evolving model architectures and varying amounts of data—in particular, extremely scarce amounts of available training data. In this paper, we present a novel method of applying Möbius transformations to augment input images during training. Möbius transformations are bijective conformal maps that generalize image translation to operate over complex inversion in pixel space. As a result, Möbius transformations can operate on the sample level and preserve data labels. We show that the inclusion of Möbius transformations during training enables improved generalization over prior sample-level data augmentation techniques such as cutout and standard crop-and-flip transformations, most notably in low data regimes.},
  archive      = {J_MLST},
  author       = {Sharon Zhou and Jiequan Zhang and Hang Jiang and Torbjörn Lundh and Andrew Y Ng},
  doi          = {10.1088/2632-2153/abd615},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Data augmentation with mobius transformations},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable programming for online training of a neural
artificial viscosity function within a staggered grid lagrangian
hydrodynamics scheme. <em>MLST</em>, <em>2</em>(2), 025015. (<a
href="https://doi.org/10.1088/2632-2153/abd644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lagrangian methods to solve the inviscid Euler equations produce numerical oscillations near shock waves. A common approach to reducing these oscillations is to add artificial viscosity (AV) to the discrete equations. The AV term acts as a dissipative mechanism that attenuates oscillations by smearing the shock across a finite number of computational cells. However, AV introduces several control parameters that are not determined by the underlying physical model, and hence, in practice are tuned to the characteristics of a given problem. We seek to improve the standard quadratic-linear AV form by replacing it with a learned neural function that reduces oscillations relative to exact solutions of the Euler equations, resulting in a hybrid numerical-neural hydrodynamic solver. Because AV is an artificial construct that exists solely to improve the numerical properties of a hydrodynamic code, there is no offline &#39;viscosity data&#39; against which a neural network can be trained before inserting into a numerical simulation, thus requiring online training. We achieve this via differentiable programming, i.e. end-to-end backpropagation or adjoint solution through both the neural and differential equation code, using automatic differentiation of the hybrid code in the Julia programming language to calculate the necessary loss function gradients. A novel offline pre-training step accelerates training by initializing the neural network to the default numerical AV scheme, which can be learned rapidly by space-filling sampling over the AV input space. We find that online training over early time steps of simulation is sufficient to learn a neural AV function that reduces numerical oscillations in long-term hydrodynamic shock simulations. These results offer an early proof-of-principle that online differentiable training of hybrid numerical schemes with novel neural network components can improve certain performance aspects existing in purely numerical schemes.},
  archive      = {J_MLST},
  author       = {Pake Melland and Jason Albright and Nathan M Urban},
  doi          = {10.1088/2632-2153/abd644},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Differentiable programming for online training of a neural artificial viscosity function within a staggered grid lagrangian hydrodynamics scheme},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of diffraction patterns in single particle
imaging experiments performed at x-ray free-electron lasers using a
convolutional neural network. <em>MLST</em>, <em>2</em>(2), 025014. (<a
href="https://doi.org/10.1088/2632-2153/abd916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single particle imaging (SPI) is a promising method of native structure determination, which has undergone fast progress with the development of x-ray free-electron lasers. Large amounts of data are collected during SPI experiments, driving the need for automated data analysis. The necessary data analysis pipeline has a number of steps including binary object classification (single versus non-single hits). Classification and object detection are areas where deep neural networks currently outperform other approaches. In this work, we use the fast object detector networks YOLOv2 and YOLOv3. By exploiting transfer learning, a moderate amount of data is sufficient to train the neural network. We demonstrate here that a convolutional neural network can be successfully used to classify data from SPI experiments. We compare the results of classification for the two different networks, with different depth and architecture, by applying them to the same SPI data with different data representation. The best results are obtained for diffracted intensity represented by color images on a linear scale using YOLOv2 for classification. It shows an accuracy of about 95% with precision and recall of about 50% and 60%, respectively, in comparison to manual data classification.},
  archive      = {J_MLST},
  author       = {Alexandr Ignatenko and Dameli Assalauova and Sergey A Bobkov and Luca Gelisio and Anton B Teslyuk and Viacheslav A Ilyin and Ivan A Vartanyants},
  doi          = {10.1088/2632-2153/abd916},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Classification of diffraction patterns in single particle imaging experiments performed at x-ray free-electron lasers using a convolutional neural network},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated multi-layer optical design via deep reinforcement
learning. <em>MLST</em>, <em>2</em>(2), 025013. (<a
href="https://doi.org/10.1088/2632-2153/abc327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical multi-layer thin films are widely used in optical and energy applications requiring photonic designs. Engineers often design such structures based on their physical intuition. However, solely relying on human experts can be time-consuming and may lead to sub-optimal designs, especially when the design space is large. In this work, we frame the multi-layer optical design task as a sequence generation problem. A deep sequence generation network is proposed for efficiently generating optical layer sequences. We train the deep sequence generation network with proximal policy optimization to generate multi-layer structures with desired properties. The proposed method is applied to two energy applications. Our algorithm successfully discovered high-performance designs, outperforming structures designed by human experts in task 1, and a state-of-the-art memetic algorithm in task 2.},
  archive      = {J_MLST},
  author       = {Haozhu Wang and Zeyu Zheng and Chengang Ji and L Jay Guo},
  doi          = {10.1088/2632-2153/abc327},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Automated multi-layer optical design via deep reinforcement learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A neural network for determination of latent dimensionality
in non-negative matrix factorization. <em>MLST</em>, <em>2</em>(2),
025012. (<a href="https://doi.org/10.1088/2632-2153/aba372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF) has proven to be a powerful unsupervised learning method for uncovering hidden features in complex and noisy data sets with applications in data mining, text recognition, dimension reduction, face recognition, anomaly detection, blind source separation, and many other fields. An important input for NMF is the latent dimensionality of the data, that is, the number of hidden features, K , present in the explored data set. Unfortunately, this quantity is rarely known a priori . The existing methods for determining latent dimensionality, such as automatic relevance determination (ARD), are mostly heuristic and utilize different characteristics to estimate the number of hidden features. However, all of them require human presence to make a final determination of K . Here we utilize a supervised machine learning approach in combination with a recent method for model determination, called NMFk, to determine the number of hidden features automatically. NMFk performs a set of NMF simulations on an ensemble of matrices, obtained by bootstrapping the initial data set, and determines which K produces stable groups of latent features that reconstruct the initial data set well. We then train a multi-layer perceptron (MLP) classifier network to determine the correct number of latent features utilizing the statistics and characteristics of the NMF solutions, obtained from NMFk. In order to train the MLP classifier, a training set of 58 660 matrices with predetermined latent features were factorized with NMFk. The MLP classifier in conjunction with NMFk maintains a greater than 95% success rate when applied to a held out test set. Additionally, when applied to two well-known benchmark data sets, the swimmer and MIT face data, NMFk/MLP correctly recovered the established number of hidden features. Finally, we compared the accuracy of our method to ARD, AIC and stability-based methods.},
  archive      = {J_MLST},
  author       = {Benjamin T Nebgen and Raviteja Vangara and Miguel A Hombrados-Herrera and Svetlana Kuksova and Boian S Alexandrov},
  doi          = {10.1088/2632-2153/aba372},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A neural network for determination of latent dimensionality in non-negative matrix factorization},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving quantum statistical mechanics with variational
autoregressive networks and quantum circuits. <em>MLST</em>,
<em>2</em>(2), 025011. (<a
href="https://doi.org/10.1088/2632-2153/aba19d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the ability of an unitary quantum circuit by interfacing it with a classical autoregressive neural network. The combined model parametrizes a variational density matrix as a classical mixture of quantum pure states, where the autoregressive network generates bitstring samples as input states to the quantum circuit. We devise an efficient variational algorithm to jointly optimize the classical neural network and the quantum circuit to solve quantum statistical mechanics problems. One can obtain thermal observables such as the variational free energy, entropy, and specific heat. As a byproduct, the algorithm also gives access to low energy excitation states. We demonstrate applications of the approach to thermal properties and excitation spectra of the quantum Ising model with resources that are feasible on near-term quantum computers.},
  archive      = {J_MLST},
  author       = {Jin-Guo Liu and Liang Mao and Pan Zhang and Lei Wang},
  doi          = {10.1088/2632-2153/aba19d},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {025011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Solving quantum statistical mechanics with variational autoregressive networks and quantum circuits},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Watch and learn—a generalized approach for transferrable
learning in deep neural networks via physical principles. <em>MLST</em>,
<em>2</em>(2), 02LT02. (<a
href="https://doi.org/10.1088/2632-2153/abc81b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning refers to the use of knowledge gained while solving a machine learning task and applying it to the solution of a closely related problem. Such an approach has enabled scientific breakthroughs in computer vision and natural language processing where the weights learned in state-of-the-art models can be used to initialize models for other tasks which dramatically improve their performance and save computational time. Here we demonstrate an unsupervised learning approach augmented with basic physical principles that achieves fully transferrable learning for problems in statistical physics across different physical regimes. By coupling a sequence model based on a recurrent neural network to an extensive deep neural network, we are able to learn the equilibrium probability distributions and inter-particle interaction models of classical statistical mechanical systems. Our approach, distribution-consistent learning, DCL, is a general strategy that works for a variety of canonical statistical mechanical models (Ising and Potts) as well as disordered interaction potentials. Using data collected from a single set of observation conditions, DCL successfully extrapolates across all temperatures, thermodynamic phases, and can be applied to different length-scales. This constitutes a fully transferrable physics-based learning in a generalizable approach.},
  archive      = {J_MLST},
  author       = {Kyle Sprague and Juan Carrasquilla and Stephen Whitelam and Isaac Tamblyn},
  doi          = {10.1088/2632-2153/abc81b},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {2},
  pages        = {02LT02},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Watch and learn—a generalized approach for transferrable learning in deep neural networks via physical principles},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An assessment of the structural resolution of various
fingerprints commonly used in machine learning. <em>MLST</em>,
<em>2</em>(1), 015018. (<a
href="https://doi.org/10.1088/2632-2153/abb212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atomic environment fingerprints are widely used in computational materials science, from machine learning potentials to the quantification of similarities between atomic configurations. Many approaches to the construction of such fingerprints, also called structural descriptors, have been proposed. In this work, we compare the performance of fingerprints based on the overlap matrix, the smooth overlap of atomic positions, Behler–Parrinello atom-centered symmetry functions, modified Behler–Parrinello symmetry functions used in the ANI-1ccx potential and the Faber–Christensen–Huang–Lilienfeld fingerprint under various aspects. We study their ability to resolve differences in local environments and in particular examine whether there are certain atomic movements that leave the fingerprints exactly or nearly invariant. For this purpose, we introduce a sensitivity matrix whose eigenvalues quantify the effect of atomic displacement modes on the fingerprint. Further, we check whether these displacements correlate with the variation of localized physical quantities such as forces. Finally, we extend our examination to the correlation between molecular fingerprints obtained from the atomic fingerprints and global quantities of entire molecules.},
  archive      = {J_MLST},
  author       = {Behnam Parsaeifard and Deb Sankar De and Anders S Christensen and Felix A Faber and Emir Kocer and Sandip De and Jörg Behler and O Anatole von Lilienfeld and Stefan Goedecker},
  doi          = {10.1088/2632-2153/abb212},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {015018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An assessment of the structural resolution of various fingerprints commonly used in machine learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving proton dose calculation accuracy by using deep
learning. <em>MLST</em>, <em>2</em>(1), 015017. (<a
href="https://doi.org/10.1088/2632-2153/abb6d5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pencil beam (PB) dose calculation is fast but inaccurate due to the approximations when dealing with inhomogeneities. Monte Carlo (MC) dose calculation is the most accurate method but it is time consuming. The aim of this study was to develop a deep learning model that can boost the accuracy of PB dose calculation to the level of MC dose by converting PB dose to MC dose for different tumor sites. The proposed model uses the PB dose and computed tomography image as inputs to generate the MC dose. We used 290 patients (90 head and neck, 93 liver, 75 prostate and 32 lung) to train, validate, and test the model. For each tumor site, we performed four numerical experiments to explore various combinations of training datasets. Training the model on data from all tumor sites together and using the dose distribution of each individual beam as input yielded the best performance for all four tumor sites. The average gamma passing rate (1 mm/1%) between the converted and the MC dose was 92.8%, 92.7%, 89.7% and 99.6% for head and neck, liver, lung, and prostate test patients, respectively. The average dose conversion time for a single field was less than 4 s. The trained model can be adapted to new datasets through transfer learning. Our deep learning-based approach can quickly boost the accuracy of PB dose to that of MC dose. The developed model can be added to the clinical workflow of proton treatment planning to improve dose calculation accuracy.},
  archive      = {J_MLST},
  author       = {Chao Wu and Dan Nguyen and Yixun Xing and Ana Barragan Montero and Jan Schuemann and Haijiao Shang and Yuehu Pu and Steve Jiang},
  doi          = {10.1088/2632-2153/abb6d5},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {015017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving proton dose calculation accuracy by using deep learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of chemical reaction yields using deep learning.
<em>MLST</em>, <em>2</em>(1), 015016. (<a
href="https://doi.org/10.1088/2632-2153/abc81d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is driving one of the most important revolutions in organic chemistry. Multiple platforms, including tools for reaction prediction and synthesis planning based on machine learning, have successfully become part of the organic chemists&#39; daily laboratory, assisting in domain-specific synthetic problems. Unlike reaction prediction and retrosynthetic models, the prediction of reaction yields has received less attention in spite of the enormous potential of accurately predicting reaction conversion rates. Reaction yields models, describing the percentage of the reactants converted to the desired products, could guide chemists and help them select high-yielding reactions and score synthesis routes, reducing the number of attempts. So far, yield predictions have been predominantly performed for high-throughput experiments using a categorical (one-hot) encoding of reactants, concatenated molecular fingerprints, or computed chemical descriptors. Here, we extend the application of natural language processing architectures to predict reaction properties given a text-based representation of the reaction, using an encoder transformer model combined with a regression layer. We demonstrate outstanding prediction performance on two high-throughput experiment reactions sets. An analysis of the yields reported in the open-source USPTO data set shows that their distribution differs depending on the mass scale, limiting the data set applicability in reaction yields predictions.},
  archive      = {J_MLST},
  author       = {Philippe Schwaller and Alain C Vaucher and Teodoro Laino and Jean-Louis Reymond},
  doi          = {10.1088/2632-2153/abc81d},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {015016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Prediction of chemical reaction yields using deep learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Outlook for artificial intelligence and machine learning at
the NSLS-II. <em>MLST</em>, <em>2</em>(1), 013001. (<a
href="https://doi.org/10.1088/2632-2153/abbd4e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe the current and future plans for using artificial intelligence and machine learning (AI/ML) methods at the National Synchrotron Light Source II (NSLS-II), a scientific user facility at the Brookhaven National Laboratory. We discuss the opportunity for using the AI/ML tools and techniques developed in the data and computational science areas to greatly improve the scientific output of large scale experimental user facilities. We describe our current and future plans in areas including from detecting and recovering from faults, optimizing the source and instrument configurations, streamlining the pipeline from measurement to insight, through data acquisition, processing, analysis. The overall strategy and direction of the NSLS-II facility in relation to AI/ML is presented.},
  archive      = {J_MLST},
  author       = {Stuart I Campbell and Daniel B Allan and Andi M Barbour and Daniel Olds and Maksim S Rakitin and Reid Smith and Stuart B Wilkins},
  doi          = {10.1088/2632-2153/abbd4e},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {013001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Outlook for artificial intelligence and machine learning at the NSLS-II},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Deep learning in electron microscopy. <em>MLST</em>,
<em>2</em>(1), 011004. (<a
href="https://doi.org/10.1088/2632-2153/abd614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.},
  archive      = {J_MLST},
  author       = {Jeffrey M Ede},
  doi          = {10.1088/2632-2153/abd614},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {011004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning in electron microscopy},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum machine learning in high energy physics.
<em>MLST</em>, <em>2</em>(1), 011003. (<a
href="https://doi.org/10.1088/2632-2153/abc17d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for machine learning applications. An interesting question is whether there are ways to apply quantum machine learning to HEP. This paper reviews the first generation of ideas that use quantum machine learning on problems in HEP and provide an outlook on future applications.},
  archive      = {J_MLST},
  author       = {Wen Guan and Gabriel Perdue and Arthur Pesah and Maria Schuld and Koji Terashi and Sofia Vallecorsa and Jean-Roch Vlimant},
  doi          = {10.1088/2632-2153/abc17d},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {1},
  pages        = {011003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum machine learning in high energy physics},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
