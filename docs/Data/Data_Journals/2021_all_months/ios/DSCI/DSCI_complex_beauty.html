<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DSCI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dsci---7">DSCI - 7</h2>
<ul>
<li><details>
<summary>
(2021). A systematic review on privacy-preserving distributed data
mining. <em>DSCI</em>, <em>4</em>(2), 121–150. (<a
href="https://doi.org/10.3233/DS-210036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining and analysing sensitive data from multiple sources offers considerable potential for knowledge discovery. However, there are a number of issues that pose problems for such analyses, including technical barriers, privacy restrictions, security concerns, and trust issues. Privacy-preserving distributed data mining techniques (PPDDM) aim to overcome these challenges by extracting knowledge from partitioned data while minimizing the release of sensitive information. This paper reports the results and findings of a systematic review of PPDDM techniques from 231 scientific articles published in the past 20 years. We summarize the state of the art, compare the problems they address, and identify the outstanding challenges in the field. This review identifies the consequence of the lack of standard criteria to evaluate new PPDDM methods and proposes comprehensive evaluation criteria with 10 key factors. We discuss the ambiguous definitions of privacy and confusion between privacy and security in the field, and provide suggestions of how to make a clear and applicable privacy description for new PPDDM techniques. The findings from our review enhance the understanding of the challenges of applying theoretical PPDDM methods to real-life use cases, and the importance of involving legal-ethical and social experts in implementing PPDDM methods. This comprehensive review will serve as a helpful guide to past research and future opportunities in the area of PPDDM.},
  archive      = {J_DSCI},
  author       = {Sun, Chang and Ippel, Lianne and Dekker, Andre and Dumontier, Michel and van Soest, Johan},
  doi          = {10.3233/DS-210036},
  journal      = {Data Science},
  month        = {10},
  number       = {2},
  pages        = {121-150},
  shortjournal = {Data Sci.},
  title        = {A systematic review on privacy-preserving distributed data mining},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic de-identification of data download packages.
<em>DSCI</em>, <em>4</em>(2), 101–120. (<a
href="https://doi.org/10.3233/DS-210035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The General Data Protection Regulation (GDPR) grants all natural persons the right to access their personal data if this is being processed by data controllers. The data controllers are obliged to share the data in an electronic format and often provide the data in a so called Data Download Package (DDP). These DDPs contain all data collected by public and private entities during the course of a citizens’ digital life and form a treasure trove for social scientists. However, the data can be deeply private. To protect the privacy of research participants while using their DDPs for scientific research, we developed a de-identification algorithm that is able to handle typical characteristics of DDPs. These include regularly changing file structures, visual and textual content, differing file formats, differing file structures and private information like usernames. We investigate the performance of the algorithm and illustrate how the algorithm can be tailored towards specific DDP structures.},
  archive      = {J_DSCI},
  author       = {Boeschoten, Laura and Voorvaart, Roos and Van Den Goorbergh, Ruben and Kaandorp, Casper and De Vos, Martine},
  doi          = {10.3233/DS-210035},
  journal      = {Data Science},
  month        = {10},
  number       = {2},
  pages        = {101-120},
  shortjournal = {Data Sci.},
  title        = {Automatic de-identification of data download packages},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Arangopipe, a tool for machine learning meta-data
management. <em>DSCI</em>, <em>4</em>(2), 85–99. (<a
href="https://doi.org/10.3233/DS-210034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experimenting with different models, documenting results and findings, and repeating these tasks are day-to-day activities for machine learning engineers and data scientists. There is a need to keep control of the machine-learning pipeline and its metadata. This allows users to iterate quickly through experiments and retrieve key findings and observations from historical activity. This is the need that Arangopipe serves. Arangopipe is an open-source tool that provides a data model that captures the essential components of any machine learning life cycle. Arangopipe provides an application programming interface that permits machine-learning engineers to record the details of the salient steps in building their machine learning models. The components of the data model and an overview of the application programming interface is provided. Illustrative examples of basic and advanced machine learning workflows are provided. Arangopipe is not only useful for users involved in developing machine learning models but also useful for users deploying and maintaining them.},
  archive      = {J_DSCI},
  author       = {Schad, Jörg and Sambasivan, Rajiv and Woodward, Christopher},
  doi          = {10.3233/DS-210034},
  journal      = {Data Science},
  month        = {10},
  number       = {2},
  pages        = {85-99},
  shortjournal = {Data Sci.},
  title        = {Arangopipe, a tool for machine learning meta-data management},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning based network similarity for model selection.
<em>DSCI</em>, <em>4</em>(2), 63–83. (<a
href="https://doi.org/10.3233/DS-210033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing data in the form of networks is becoming an increasingly popular approach for modeling, analyzing and visualising complex phenomena, to understand the important properties of the underlying complex processes. Access to many large-scale network datasets is restricted due to the privacy and security concerns. Also for several applications (such as functional connectivity networks), generating large scale real data is expensive. For these reasons, there is a growing need for advanced mathematical and statistical models (also called generative models) that can account for the structure of these large-scale networks, without having to materialize them in the real world. The objective is to provide a comprehensible description of the network properties and to be able to infer previously unobserved properties. Various models have been developed by researchers, which generate synthetic networks that adhere to the structural properties of real networks. However, the selection of the appropriate generative model for a given real-world network remains an important challenge. In this paper, we investigate this problem and provide a novel technique (named as TripletFit) for model selection (or network classification) and estimation of structural similarities of the complex networks. The goal of network model selection is to select a generative model that is able to generate a structurally similar synthetic network for a given real-world (target) network. We consider six outstanding generative models as the candidate models. The existing model selection methods mostly suffer from sensitivity to network perturbations, dependency on the size of the networks, and low accuracy. To overcome these limitations, we considered a broad array of network features, with the aim of representing different structural aspects of the network and employed deep learning techniques such as deep triplet network architecture and simple feed-forward network for model selection and estimation of structural similarities of the complex networks. Our proposed method, outperforms existing methods with respect to accuracy, noise-tolerance, and size independence on a number of gold standard data set used in previous studies.},
  archive      = {J_DSCI},
  author       = {Singh, Kushal Veer and Verma, Ajay Kumar and Vig, Lovekesh},
  doi          = {10.3233/DS-210033},
  journal      = {Data Science},
  month        = {10},
  number       = {2},
  pages        = {63-83},
  shortjournal = {Data Sci.},
  title        = {Deep learning based network similarity for&amp;nbsp;model selection},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BioVenn – an r and python package for the comparison and
visualization of biological lists using area-proportional venn diagrams.
<em>DSCI</em>, <em>4</em>(1), 51–61. (<a
href="https://doi.org/10.3233/DS-210032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most popular methods to visualize the overlap and differences between data sets is the Venn diagram. Venn diagrams are especially useful when they are ‘area-proportional’ i.e. the sizes of the circles and the overlaps correspond to the sizes of the data sets. In 2007, the BioVenn web interface was launched, which is being used by many researchers. However, this web implementation requires users to copy and paste (or upload) lists of IDs into the web browser, which is not always convenient and makes it difficult for researchers to create Venn diagrams ‘in batch’, or to automatically update the diagram when the source data changes. This is only possible by using software such as R or Python. This paper describes the BioVenn R and Python packages, which are very easy-to-use packages that can generate accurate area-proportional Venn diagrams of two or three circles directly from lists of (biological) IDs. The only required input is two or three lists of IDs. Optional parameters include the main title, the subtitle, the printing of absolute numbers or percentages within the diagram, colors and fonts. The function can show the diagram on the screen, or it can export the diagram in one of the supported file formats. The function also returns all thirteen lists. The BioVenn R package and Python package were created for biological IDs, but they can be used for other IDs as well. Finally, BioVenn can map Affymetrix and EntrezGene to Ensembl IDs. The BioVenn R package is available in the CRAN repository, and can be installed by running ‘install.packages(“BioVenn”)’. The BioVenn Python package is available in the PyPI repository, and can be installed by running ‘pip install BioVenn’. The BioVenn web interface remains available at https://www.biovenn.nl .},
  archive      = {J_DSCI},
  author       = {Hulsen, Tim},
  doi          = {10.3233/DS-210032},
  journal      = {Data Science},
  month        = {5},
  number       = {1},
  pages        = {51-61},
  shortjournal = {Data Sci.},
  title        = {BioVenn&amp;nbsp;– an r and python package for the comparison and visualization of biological lists using area-proportional venn diagrams},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WORCS: A workflow for open reproducible code in science.
<em>DSCI</em>, <em>4</em>(1), 29–49. (<a
href="https://doi.org/10.3233/DS-210031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adopting open science principles can be challenging, requiring conceptual education and training in the use of new tools. This paper introduces the Workflow for Open Reproducible Code in Science (WORCS): A step-by-step procedure that researchers can follow to make a research project open and reproducible. This workflow intends to lower the threshold for adoption of open science principles. It is based on established best practices, and can be used either in parallel to, or in absence of, top-down requirements by journals, institutions, and funding bodies. To facilitate widespread adoption, the WORCS principles have been implemented in the R package worcs , which offers an RStudio project template and utility functions for specific workflow steps. This paper introduces the conceptual workflow, discusses how it meets different standards for open science, and addresses the functionality provided by the R implementation, worcs . This paper is primarily targeted towards scholars conducting research projects in R, conducting research that involves academic prose, analysis code, and tabular data. However, the workflow is flexible enough to accommodate other scenarios, and offers a starting point for customized solutions. The source code for the R package and manuscript, and a list of examplesof WORCS projects , are available at https://github.com/cjvanlissa/worcs .},
  archive      = {J_DSCI},
  author       = {Van Lissa, Caspar J. and Brandmaier, Andreas M. and Brinkman, Loek and Lamprecht, Anna-Lena and Peikert, Aaron and Struiksma, Marijn E. and Vreede, Barbara M.I.},
  doi          = {10.3233/DS-210031},
  journal      = {Data Science},
  month        = {5},
  number       = {1},
  pages        = {29-49},
  shortjournal = {Data Sci.},
  title        = {WORCS: A workflow for open reproducible code in science},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of concepts of neighbours to knowledge graph
completion. <em>DSCI</em>, <em>4</em>(1), 1–28. (<a
href="https://doi.org/10.3233/DS-200030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The open nature of Knowledge Graphs (KG) often implies that they are incomplete. Knowledge graph completion (a.k.a. link prediction) consists in inferring new relationships between the entities of a KG based on existing relationships. Most existing approaches rely on the learning of latent feature vectors for the encoding of entities and relations. In general however, latent features cannot be easily interpreted. Rule-based approaches offer interpretability but a distinct ruleset must be learned for each relation. In both latent- and rule-based approaches, the training phase has to be run again when the KG is updated. We propose a new approach that does not need a training phase, and that can provide interpretable explanations for each inference. It relies on the computation of Concepts of Nearest Neighbours (C-NN) to identify clusters of similar entities based on common graph patterns. Different rules are then derived from those graph patterns, and combined to predict new relationships. We evaluate our approach on standard benchmarks for link prediction, where it gets competitive performance compared to existing approaches.},
  archive      = {J_DSCI},
  author       = {Ferré, Sébastien},
  doi          = {10.3233/DS-200030},
  journal      = {Data Science},
  month        = {5},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Data Sci.},
  title        = {Application of concepts of neighbours to&amp;nbsp;knowledge graph completion},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
