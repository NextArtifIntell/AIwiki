<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SW_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sw---39">SW - 39</h2>
<ul>
<li><details>
<summary>
(2021). High-level ETL for semantic data warehouses. <em>SW</em>,
<em>13</em>(1), 85–132. (<a
href="https://doi.org/10.3233/SW-210429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of the Semantic Web (SW) encourages organizations to organize and publish semantic data using the RDF model. This growth poses new requirements to Business Intelligence technologies to enable On-Line Analytical Processing (OLAP)-like analysis over semantic data. The incorporation of semantic data into a Data Warehouse (DW) is not supported by the traditional Extract-Transform-Load (ETL) tools because they do not consider semantic issues in the integration process. In this paper, we propose a layer-based integration process and a set of high-level RDF-based ETL constructs required to define, map, extract, process, transform, integrate, update, and load (multidimensional) semantic data. Different to other ETL tools, we automate the ETL data flows by creating metadata at the schema level. Therefore, it relieves ETL developers from the burden of manual mapping at the ETL operation level. We create a prototype, named Semantic ETL Construct ( SETL CONSTRUCT ), based on the innovative ETL constructs proposed here. To evaluate SETL CONSTRUCT , we create a multidimensional semantic DW by integrating a Danish Business dataset and an EU Subsidy dataset using it and compare it with the previous programmable framework SETL PROG in terms of productivity, development time, and performance. The evaluation shows that 1) SETL CONSTRUCT uses 92% fewer Number of Typed Characters (NOTC) than SETL PROG , and SETL AUTO (the extension of SETL CONSTRUCT for generating ETL execution flows automatically) further reduces the Number of Used Concepts (NOUC) by another 25%; 2) using SETL CONSTRUCT , the development time is almost cut in half compared to SETL PROG , and is cut by another 27% using SETL AUTO ; and 3) SETL CONSTRUCT is scalable and has similar performance compared to SETL PROG . We also evaluate our approach qualitatively by interviewing two ETL experts.},
  archive      = {J_SW},
  author       = {Deb Nath, Rudra Pratap and Romero, Oscar and Pedersen, Torben Bach and Hose, Katja},
  doi          = {10.3233/SW-210429},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {85-132},
  shortjournal = {Semantic Web},
  title        = {High-level ETL for semantic data warehouses},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sampo-UI: A full stack JavaScript framework for developing
semantic portal user interfaces. <em>SW</em>, <em>13</em>(1), 69–84. (<a
href="https://doi.org/10.3233/SW-210428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new software framework, Sampo-UI , for developing user interfaces for semantic portals. The goal is to provide the end-user with multiple application perspectives to Linked Data knowledge graphs, and a two-step usage cycle based on faceted search combined with ready-to-use tooling for data analysis. For the software developer, the Sampo-UI framework makes it possible to create highly customizable, user-friendly, and responsive user interfaces using current state-of-the-art JavaScript libraries and data from SPARQL endpoints, while saving substantial coding effort. Sampo-UI is published on GitHub under the open MIT License and has been utilized in several internal and external projects. The framework has been used thus far in creating six published and five forth-coming portals, mostly related to the Cultural Heritage domain, that have had tens of thousands of end-users on the Web.},
  archive      = {J_SW},
  author       = {Ikkala, Esko and Hyvönen, Eero and Rantala, Heikki and Koho, Mikko},
  doi          = {10.3233/SW-210428},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {69-84},
  shortjournal = {Semantic Web},
  title        = {Sampo-UI: A full stack JavaScript framework for developing semantic portal user interfaces},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The euBusinessGraph ontology: A lightweight ontology for
harmonizing basic company information. <em>SW</em>, <em>13</em>(1),
41–68. (<a href="https://doi.org/10.3233/SW-210424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Company data, ranging from basic company information such as company name(s) and incorporation date to complex balance sheets and personal data about directors and shareholders, are the foundation that many data value chains depend upon in various sectors (e.g., business information, marketing and sales, etc.). Company data becomes a valuable asset when data is collected and integrated from a variety of sources, both authoritative (e.g., national business registers) and non-authoritative (e.g., company websites). Company data integration is however a difficult task primarily due to the heterogeneity and complexity of company data, and the lack of generally agreed upon semantic descriptions of the concepts in this domain. In this article, we introduce the euBusinessGraph ontology as a lightweight mechanism for harmonising company data for the purpose of aggregating, linking, provisioning and analysing basic company data. The article provides an overview of the related work, ontology scope, ontology development process, explanations of core concepts and relationships, and the implementation of the ontology. Furthermore, we present scenarios where the ontology was used, among others, for publishing company data (business knowledge graph) and for comparing data from various company data providers. The euBusinessGraph ontology serves as an asset not only for enabling various tasks related to company data but also on which various extensions can be built upon.},
  archive      = {J_SW},
  author       = {Roman, Dumitru and Alexiev, Vladimir and Paniagua, Javier and Elvesæter, Brian and von Zernichow, Bjørn Marius and Soylu, Ahmet and Simeonov, Boyan and Taggart, Chris},
  doi          = {10.3233/SW-210424},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {41-68},
  shortjournal = {Semantic Web},
  title        = {The euBusinessGraph ontology: A&amp;nbsp;lightweight ontology for harmonizing basic company information},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multidimensional enrichment of spatial RDF data for SOLAP.
<em>SW</em>, <em>13</em>(1), 5–39. (<a
href="https://doi.org/10.3233/SW-210423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large volumes of spatial data and multidimensional data are being published on the Semantic Web, which has led to new opportunities for advanced analysis, such as Spatial Online Analytical Processing (SOLAP). The RDF Data Cube (QB) and QB4OLAP vocabularies have been widely used for annotating and publishing statistical and multidimensional RDF data. Although such statistical data sets might have spatial information, such as coordinates, the lack of spatial semantics and spatial multidimensional concepts in QB4OLAP and QB prevents users from employing SOLAP queries over spatial data using SPARQL. The QB4SOLAP vocabulary, on the other hand, fully supports annotating spatial and multidimensional data on the Semantic Web and enables users to query endpoints with SOLAP operators in SPARQL. To bridge the gap between QB/QB4OLAP and QB4SOLAP, we propose an RDF2SOLAP enrichment model that automatically annotates spatial multidimensional concepts with QB4SOLAP and in doing so enables SOLAP on existing QB and QB4OLAP data on the Semantic Web. Furthermore, we present and evaluate a wide range of enrichment algorithms and apply them on a non-trivial real-world use case involving governmental open data with complex geometry types.},
  archive      = {J_SW},
  author       = {Gür, Nurefşan and Pedersen, Torben Bach and Hose, Katja and Midtgaard, Mikael},
  doi          = {10.3233/SW-210423},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {5-39},
  shortjournal = {Semantic Web},
  title        = {Multidimensional enrichment of spatial RDF data for SOLAP},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diverse data! Diverse schemata? <em>SW</em>, <em>13</em>(1),
1–3. (<a href="https://doi.org/10.3233/SW-210453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key value propositions for knowledge graphs and semantic web technologies is fostering semantic interoperability, i.e., integrating data across different themes and domains. But why do we aim at interoperability in the first place? A common answer to this question is that each individual data source only contains partial information about some phenomenon of interest. Consequently, combining multiple diverse datasets provides a more holistic perspective and enables us to answer more complex questions, e.g., those that span between the physical sciences and the social sciences. Interestingly, while these arguments are well established and go by different names, e.g., variety in the realm of big data, we seem less clear about whether the same arguments apply on the level of schemata. Put differently, we want diverse data, but do we also want diverse schemata or a single one to rule them all?},
  archive      = {J_SW},
  author       = {Janowicz, Krzysztof and Shimizu, Cogan and Hitzler, Pascal and Mai, Gengchen and Stephen, Shirly and Zhu, Rui and Cai, Ling and Zhou, Lu and Schildhauer, Mark and Liu, Zilong and Wang, Zhangyu and Shi, Meilin},
  doi          = {10.3233/SW-210453},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Semantic Web},
  title        = {Diverse data! diverse schemata?},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential linked data: The state of affairs. <em>SW</em>,
<em>12</em>(6), 927–958. (<a
href="https://doi.org/10.3233/SW-210436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequences are among the most important data structures in computer science. In the Semantic Web, however, little attention has been given to Sequential Linked Data. In previous work, we have discussed the data models that Knowledge Graphs commonly use for representing sequences and showed how these models have an impact on query performance and that this impact is invariant to triplestore implementations. However, the specific list operations that the management of Sequential Linked Data requires beyond the simple retrieval of an entire list or a range of its elements – e.g. to add or remove elements from a list –, and their impact in the various list data models, remain unclear. Covering this knowledge gap would be a significant step towards the realization of a Semantic Web list Application Programming Interface (API) that standardizes list manipulation and generalizes beyond specific data models. In order to address these challenges towards the realization of such an API, we build on our previous work in understanding the effects of various sequential data models for Knowledge Graphs, extending our benchmark and proposing a set of read-write Semantic Web list operations in SPARQL, with insert, update and delete support. To do so, we identify five classic list-based computer science sequential data structures ( linked list , double linked list , stack , queue , and array ), from which we derive nine atomic read-write operations for Semantic Web lists. We propose a SPARQL implementation of these operations with five typical RDF data models and compare their performance by executing them against six increasing dataset sizes and four different triplestores. In light of our results, we discuss the feasibility of our devised API and reflect on the state of affairs of Sequential Linked Data.},
  archive      = {J_SW},
  author       = {Daga, Enrico and Meroño-Peñuela, Albert and Motta, Enrico},
  doi          = {10.3233/SW-210436},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {927-958},
  shortjournal = {Semantic Web},
  title        = {Sequential linked data: The state of affairs},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards fully-fledged archiving for RDF datasets.
<em>SW</em>, <em>12</em>(6), 903–925. (<a
href="https://doi.org/10.3233/SW-210434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamicity of RDF data has motivated the development of solutions for archiving, i.e., the task of storing and querying previous versions of an RDF dataset. Querying the history of a dataset finds applications in data maintenance and analytics. Notwithstanding the value of RDF archiving, the state of the art in this field is under-developed: (i) most existing systems are neither scalable nor easy to use, (ii) there is no standard way to query RDF archives, and (iii) solutions do not exploit the evolution patterns of real RDF data. On these grounds, this paper surveys the existing works in RDF archiving in order to characterize the gap between the state of the art and a fully-fledged solution. It also provides RDFev , a framework to study the dynamicity of RDF data. We use RDFev to study the evolution of YAGO, DBpedia, and Wikidata, three dynamic and prominent datasets on the Semantic Web. These insights set the ground for the sketch of a fully-fledged archiving solution for RDF data.},
  archive      = {J_SW},
  author       = {Pelgrin, Olivier and Galárraga, Luis and Hose, Katja},
  doi          = {10.3233/SW-210434},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {903-925},
  shortjournal = {Semantic Web},
  title        = {Towards fully-fledged archiving for RDF datasets},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing virtual ontology based access over tabular data
with morph-CSV. <em>SW</em>, <em>12</em>(6), 869–902. (<a
href="https://doi.org/10.3233/SW-210432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology-Based Data Access (OBDA) has traditionally focused on providing a unified view of heterogeneous datasets (e.g., relational databases, CSV and JSON files), either by materializing integrated data into RDF or by performing on-the-fly querying via SPARQL query translation. In the specific cas e of tabular datasets represented as several CSV or Excel files, query translation approaches have been applied by considering each source as a single table that can be loaded into a relational database management system (RDBMS). Nevertheless, constraints over these tables are not represented (e.g., referential integrity among sources, datatypes, or data integrity); thus, neither consistency among attributes nor indexes over tables are enforced. As a consequence, efficiency of the SPARQL-to-SQL translation process may be affected, as well as the completeness of the answers produced during the evaluation of the generated SQL query. Our work is focused on applying implicit constraints on the OBDA query translation process over tabular data. We propose Morph-CSV, a framework for querying tabular data that exploits information from typical OBDA inputs (e.g., mappings, queries) to enforce constraints that can be used together with any SPARQL-to-SQL OBDA engine. Morph-CSV relies on both a constraint component and a set of constraint operators. For a given set of constraints, the operators are applied to each type of constraint with the aim of enhancing query completeness and performance. We evaluate Morph-CSV in several domains: e-commerce with the BSBM benchmark; transportation with the GTFS-Madrid benchmark; and biology with a use case extracted from the Bio2RDF project. We compare and report the performance of two SPARQL-to-SQL OBDA engines, without and with the incorporation of Morph-CSV. The observed results suggest that Morph-CSV is able to speed up the total query execution time by up to two orders of magnitude, while it is able to produce all the query answers.},
  archive      = {J_SW},
  author       = {Chaves-Fraga, David and Ruckhaus, Edna and Priyatna, Freddy and Vidal, Maria-Esther and Corcho, Oscar},
  doi          = {10.3233/SW-210432},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {869-902},
  shortjournal = {Semantic Web},
  title        = {Enhancing virtual ontology based access over tabular data with morph-CSV},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An empirical evaluation of cost-based federated SPARQL query
processing engines. <em>SW</em>, <em>12</em>(6), 843–868. (<a
href="https://doi.org/10.3233/SW-200420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a good query plan is key to the optimization of query runtime. This holds in particular for cost-based federation engines, which make use of cardinality estimations to achieve this goal. A number of studies compare SPARQL federation engines across different performance metrics, including qu ery runtime, result set completeness and correctness, number of sources selected and number of requests sent. Albeit informative, these metrics are generic and unable to quantify and evaluate the accuracy of the cardinality estimators of cost-based federation engines. To thoroughly evaluate cost-based federation engines, the effect of estimated cardinality errors on the overall query runtime performance must be measured. In this paper, we address this challenge by presenting novel evaluation metrics targeted at a fine-grained benchmarking of cost-based federated SPARQL query engines. We evaluate five cost-based federated SPARQL query engines using existing as well as novel evaluation metrics by using LargeRDFBench queries. Our results provide a detailed analysis of the experimental outcomes that reveal novel insights, useful for the development of future cost-based federated SPARQL query processing engines.},
  archive      = {J_SW},
  author       = {Qudus, Umair and Saleem, Muhammad and Ngonga Ngomo, Axel-Cyrille and Lee, Young-Koo},
  doi          = {10.3233/SW-200420},
  journal      = {Semantic Web},
  month        = {10},
  number       = {6},
  pages        = {843-868},
  shortjournal = {Semantic Web},
  title        = {An empirical evaluation of cost-based federated SPARQL query processing engines},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MMoOn core – the multilingual morpheme ontology.
<em>SW</em>, <em>12</em>(5), 813–841. (<a
href="https://doi.org/10.3233/SW-200412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last years a rapid emergence of lexical resources has evolved in the Semantic Web. Whereas most of the linguistic information is already machine-readable, we found that morphological information is mostly absent or only contained in semi-structured strings. An integration of morphemic data has not yet been undertaken due to the lack of existing domain-specific ontologies and explicit morphemic data. In this paper, we present the Multilingual Morpheme Ontology called MMoOn Core which can be regarded as the first comprehensive ontology for the linguistic domain of morphological language data. It will be described how crucial concepts like morphs, morphemes, word forms and meanings are represented and interrelated and how language-specific morpheme inventories can be created as a new possibility of morphological datasets. The aim of the MMoOn Core ontology is to serve as a shared semantic model for linguists and NLP researchers alike to enable the creation, conversion, exchange, reuse and enrichment of morphological language data across different data-dependent language sciences. Therefore, various use cases are illustrated to draw attention to the cross-disciplinary potential which can be realized with the MMoOn Core ontology in the context of the existing Linguistic Linked Data research landscape.},
  archive      = {J_SW},
  author       = {Klimek, Bettina and Ackermann, Markus and Brümmer, Martin and Hellmann, Sebastian},
  doi          = {10.3233/SW-200412},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {813-841},
  shortjournal = {Semantic Web},
  title        = {MMoOn core&amp;nbsp;– the multilingual morpheme ontology},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Charaterizing RDF graphs through graph-based measures –
framework and assessment. <em>SW</em>, <em>12</em>(5), 789–812. (<a
href="https://doi.org/10.3233/SW-200409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topological structure of RDF graphs inherently differs from other types of graphs, like social graphs, due to the pervasive existence of hierarchical relations (TBox), which complement transversal relations (ABox). Graph measures capture such particularities through descriptive statistics. Besides the classical set of measures established in the field of network analysis, such as size and volume of the graph or the type of degree distribution of its vertices, there has been some effort to define measures that capture some of the aforementioned particularities RDF graphs adhere to. However, some of them are redundant, computationally expensive, and not meaningful enough to describe RDF graphs. In particular, it is not clear which of them are efficient metrics to capture specific distinguishing characteristics of datasets in different knowledge domains (e.g., Cross Domain vs. Linguistics ). In this work, we address the problem of identifying a minimal set of measures that is efficient, essential (non-redundant), and meaningful. Based on 54 measures and a sample of 280 graphs of nine knowledge domains from the Linked Open Data Cloud, we identify an essential set of 13 measures, having the capacity to describe graphs concisely. These measures have the capacity to present the topological structures and differences of datasets in established knowledge domains.},
  archive      = {J_SW},
  author       = {Zloch, Matthäus and Acosta, Maribel and Hienert, Daniel and Conrad, Stefan and Dietze, Stefan},
  doi          = {10.3233/SW-200409},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {789-812},
  shortjournal = {Semantic Web},
  title        = {Charaterizing RDF graphs through graph-based measures – framework and assessment},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic evaluation of complex alignments: An
instance-based approach. <em>SW</em>, <em>12</em>(5), 767–787. (<a
href="https://doi.org/10.3233/SW-210437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology matching is the task of generating a set of correspondences (i.e., an alignment) between the entities of different ontologies. While most efforts on alignment evaluation have been dedicated to the evaluation of simple alignments (i.e., those linking one single entity of a source ontology t o one single entity of a target ontology), the emergence of matchers providing complex alignments (i.e., those composed of correspondences involving logical constructors or transformation functions) requires new strategies for addressing the problem of automatically evaluating complex alignments. This paper proposes (i) a benchmark for complex alignment evaluation composed of an automatic evaluation system that relies on queries and instances, and (ii) a dataset about conference organisation. This dataset is composed of populated ontologies and a set of competency questions for alignment as SPARQL queries. State-of-the-art alignments are evaluated and a discussion on the difficulties of the evaluation task is provided.},
  archive      = {J_SW},
  author       = {Thiéblin, Elodie and Haemmerlé, Ollivier and Trojahn, Cássia},
  doi          = {10.3233/SW-210437},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {767-787},
  shortjournal = {Semantic Web},
  title        = {Automatic evaluation of complex alignments: An instance-based approach},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable zero-shot learning via attentive graph
convolutional network and knowledge graphs. <em>SW</em>, <em>12</em>(5),
741–765. (<a href="https://doi.org/10.3233/SW-210435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) which aims to deal with new classes that have never appeared in the training data (i.e., unseen classes) has attracted massive research interests recently. Transferring of deep features learned from training classes (i.e., seen classes) are often used, but most current meth ods are black-box models without any explanations, especially textual explanations that are more acceptable to not only machine learning specialists but also common people without artificial intelligence expertise. In this paper, we focus on explainable ZSL, and present a knowledge graph (KG) based framework that can explain the transferability of features in ZSL in a human understandable manner. The framework has two modules: an attentive ZSL learner and an explanation generator. The former utilizes an Attentive Graph Convolutional Network (AGCN) to match class knowledge from WordNet with deep features learned from CNNs (i.e., encode inter-class relationship to predict classifiers), in which the features of unseen classes are transferred from seen classes to predict the samples of unseen classes, with impressive (important) seen classes detected, while the latter generates human understandable explanations for the transferability of features with class knowledge that are enriched by external KGs, including a domain-specific Attribute Graph and DBpedia. We evaluate our method on two benchmarks of animal recognition. Augmented by class knowledge from KGs, our framework generates promising explanations for the transferability of features, and at the same time improves the recognition accuracy.},
  archive      = {J_SW},
  author       = {Geng, Yuxia and Chen, Jiaoyan and Ye, Zhiquan and Yuan, Zonggang and Zhang, Wei and Chen, Huajun},
  doi          = {10.3233/SW-210435},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {741-765},
  shortjournal = {Semantic Web},
  title        = {Explainable zero-shot learning via attentive graph convolutional network and knowledge graphs},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recursion in SPARQL. <em>SW</em>, <em>12</em>(5), 711–740.
(<a href="https://doi.org/10.3233/SW-200401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for recursive queries in the Semantic Web setting is becoming more and more apparent with the emergence of datasets where different pieces of information are connected by complicated patterns. This was acknowledged by the W3C committee by the inclusion of property paths in the SPARQL standard. However, as more data becomes available, it is becoming clear that property paths alone are not enough to capture all recursive queries that the users are interested in, and the literature has already proposed several extensions to allow searching for more complex patterns. We propose a rather different, but simpler approach: add a general purpose recursion operator directly to SPARQL. In this paper we provide a formal syntax and semantics for this proposal, study its theoretical properties, and develop algorithms for evaluating it in practical scenarios. We also show how to implement this extension as a plug-in on top of existing systems, and test its performance on several synthetic and real world datasets, ranging from small graphs, up to the entire Wikidata database.},
  archive      = {J_SW},
  author       = {Reutter, Juan and Soto, Adrián and Vrgoč, Domagoj},
  doi          = {10.3233/SW-200401},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {711-740},
  shortjournal = {Semantic Web},
  title        = {Recursion in SPARQL},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UCQ-rewritings for disjunctive knowledge and queries with
negated atoms. <em>SW</em>, <em>12</em>(4), 685–709. (<a
href="https://doi.org/10.3233/SW-200399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of query rewriting for disjunctive existential rules. Query rewriting is a well-known approach for query answering on knowledge bases with incomplete data. We propose a rewriting technique that uses negative constraints and conjunctive queries to remove the disju nctive components of disjunctive existential rules. This process eventually generates new non-disjunctive rules, i.e., existential rules. The generated rules can then be used to produce new rewritings using existing rewriting approaches for existential rules. With the proposed technique we are able to provide complete UCQ-rewritings for union of conjunctive queries with universally quantified negation. We implemented the proposed algorithm in the Completo system and performed experiments that evaluate the viability of the proposed solution.},
  archive      = {J_SW},
  author       = {Matos Alfonso, Enrique and Chortaras, Alexandros and Stamou, Giorgos},
  doi          = {10.3233/SW-200399},
  journal      = {Semantic Web},
  month        = {6},
  number       = {4},
  pages        = {685-709},
  shortjournal = {Semantic Web},
  title        = {UCQ-rewritings for disjunctive knowledge and queries with negated atoms},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge graph OLAP. <em>SW</em>, <em>12</em>(4), 649–683.
(<a href="https://doi.org/10.3233/SW-200419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A knowledge graph (KG) represents real-world entities and their relationships. The represented knowledge is often context-dependent, leading to the construction of contextualized KGs. The multidimensional and hierarchical nature of context invites comparison with the OLAP cube model from multidimensional data analysis. Traditional systems for online analytical processing (OLAP) employ multidimensional models to represent numeric values for further analysis using dedicated query operations. In this paper, along with an adaptation of the OLAP cube model for KGs, we introduce an adaptation of the traditional OLAP query operations for the purposes of performing analysis over KGs. In particular, we decompose the roll-up operation from traditional OLAP into a merge and an abstraction operation. The merge operation corresponds to the selection of knowledge from different contexts whereas abstraction replaces entities with more general entities. The result of such a query is a more abstract, high-level view – a management summary – of the knowledge.},
  archive      = {J_SW},
  author       = {Schuetz, Christoph G. and Bozzato, Loris and Neumayr, Bernd and Schrefl, Michael and Serafini, Luciano},
  doi          = {10.3233/SW-200419},
  journal      = {Semantic Web},
  month        = {6},
  number       = {4},
  pages        = {649-683},
  shortjournal = {Semantic Web},
  title        = {Knowledge graph OLAP},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on knowledge graph embeddings with literals: Which
model links better literal-ly? <em>SW</em>, <em>12</em>(4), 617–647. (<a
href="https://doi.org/10.3233/SW-200404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs (KGs) are composed of structured information about a particular domain in the form of entities and relations. In addition to the structured information KGs help in facilitating interconnectivity and interoperability between different resources represented in the Linked Data Cloud. KGs have been used in a variety of applications such as entity linking, question answering, recommender systems, etc. However, KG applications suffer from high computational and storage costs. Hence, there arises the necessity for a representation able to map the high dimensional KGs into low dimensional spaces, i.e., embedding space, preserving structural as well as relational information. This paper conducts a survey of KG embedding models which not only consider the structured information contained in the form of entities and relations in a KG but also its unstructured information represented as literals such as text, numerical values, images, etc. Along with a theoretical analysis and comparison of the methods proposed so far for generating KG embeddings with literals, an empirical evaluation of the different methods under identical settings has been performed for the general task of link prediction.},
  archive      = {J_SW},
  author       = {Gesese, Genet Asefa and Biswas, Russa and Alam, Mehwish and Sack, Harald},
  doi          = {10.3233/SW-200404},
  journal      = {Semantic Web},
  month        = {6},
  number       = {4},
  pages        = {617-647},
  shortjournal = {Semantic Web},
  title        = {A survey on knowledge graph embeddings with literals: Which model links better literal-ly?},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EUCISE-OWL: An ontology-based representation of the common
information sharing environment (CISE) for the maritime domain.
<em>SW</em>, <em>12</em>(4), 603–615. (<a
href="https://doi.org/10.3233/SW-200403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The timely and efficient cooperation across sectors and borders during maritime crises is paramount for the safety of human lives. Maritime monitoring authorities are now realizing the grave importance of cross-sector and cross-border information sharing. However, this cooperation is compromised by the diversity of existing systems and the vast volumes of heterogeneous data generated and exchanged during maritime operations. In order to address these challenges, the EU has been driving several initiatives, including several EU-funded projects, for facilitating information exchange across sectors and borders. A key outcome from these efforts is the Common Information Sharing Environment (CISE), which constitutes a collaborative initiative for promoting automated information sharing between maritime monitoring authorities. However, the adoption of CISE is substantially limited by its existing serialization as an XML Schema only, which facilitates information sharing and exchange to some extent, but fails to deliver the fundamental additional benefits provided by ontologies, like the richer semantics, enhanced semantic interoperability and semantic reasoning capabilities. Thus, this paper presents EUCISE-OWL, an ontology representation of the CISE data model that capitalizes on the benefits provided by ontologies and aims to encourage the adoption of CISE. EUCISE-OWL is an outcome from close collaboration in an EU-funded project with domain experts with extensive experience in deploying CISE in practice. The paper also presents a representative example for handling information exchange during a maritime crisis as well as performance results for specific querying tasks that can demonstrate and evaluate the use of the proposed ontology in practice.},
  archive      = {J_SW},
  author       = {Riga, Marina and Kontopoulos, Efstratios and Ioannidis, Konstantinos and Kintzios, Spyridon and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  doi          = {10.3233/SW-200403},
  journal      = {Semantic Web},
  month        = {6},
  number       = {4},
  pages        = {603-615},
  shortjournal = {Semantic Web},
  title        = {EUCISE-OWL: An ontology-based representation of the common information sharing environment (CISE) for the maritime domain},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RDFRules: Making RDF rule mining easier and even more
efficient. <em>SW</em>, <em>12</em>(4), 569–602. (<a
href="https://doi.org/10.3233/SW-200413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMIE+ is a state-of-the-art algorithm for learning rules from RDF knowledge graphs (KGs). Based on association rule learning, AMIE+ constituted a breakthrough in terms of speed on large data compared to the previous generation of ILP-based systems. In this paper we present several algorithmic exten sions to AMIE+, which make it faster, and the support for data pre-processing and model post-processing, which provides a more comprehensive coverage of the linked data mining process than does the original AMIE+ implementation. The main contributions are related to performance improvement: (1) the top-k approach, which addresses the problem of combinatorial explosion often resulting from a hand-set minimum support threshold, (2) a grammar that allows to define fine-grained patterns reducing the size of the search space, and (3) a faster projection binding reducing the number of repetitive calculations. Other enhancements include the possibility to mine across multiple graphs, the support for discretization of continuous values, and the selection of the most representative rules using proven rule pruning and clustering algorithms. Benchmarks show reductions in mining time of up to several orders of magnitude compared to AMIE+. An open-source implementation is available under the name RDFRules at https://github.com/propi/rdfrules.},
  archive      = {J_SW},
  author       = {Zeman, Václav and Kliegr, Tomáš and Svátek, Vojtěch},
  doi          = {10.3233/SW-200413},
  journal      = {Semantic Web},
  month        = {6},
  number       = {4},
  pages        = {569-602},
  shortjournal = {Semantic Web},
  title        = {RDFRules: Making RDF rule mining easier and even more efficient},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the relation between keys and link keys for data
interlinking. <em>SW</em>, <em>12</em>(4), 547–567. (<a
href="https://doi.org/10.3233/SW-200414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both keys and their generalisation, link keys, may be used to perform data interlinking, i.e. finding identical resources in different RDF datasets. However, the precise relationship between keys and link keys has not been fully determined yet. A common formal framework encompassing both keys and link keys is necessary to ensure the correctness of data interlinking tools based on them, and to determine their scope and possible overlapping. In this paper, we provide a semantics for keys and link keys within description logics. We determine under which conditions they are legitimate to generate links. We provide conditions under which link keys are logically equivalent to keys. In particular, we show that data interlinking with keys and ontology alignments can be reduced to data interlinking with link keys, but not the other way around.},
  archive      = {J_SW},
  author       = {Atencia, Manuel and David, Jérôme and Euzenat, Jérôme},
  doi          = {10.3233/SW-200414},
  journal      = {Semantic Web},
  month        = {6},
  number       = {4},
  pages        = {547-567},
  shortjournal = {Semantic Web},
  title        = {On the relation between keys and link keys for data interlinking},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advancing agriculture through semantic data management.
<em>SW</em>, <em>12</em>(4), 543–545. (<a
href="https://doi.org/10.3233/SW-210433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Hitzler, Pascal and Janowicz, Krzysztof and Sharda, Ajay and Shimizu, Cogan},
  doi          = {10.3233/SW-210433},
  journal      = {Semantic Web},
  month        = {6},
  number       = {4},
  pages        = {543-545},
  shortjournal = {Semantic Web},
  title        = {Advancing agriculture through semantic data management},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Link maintenance for integrity in linked open data
evolution: Literature survey and open challenges. <em>SW</em>,
<em>12</em>(3), 517–541. (<a
href="https://doi.org/10.3233/SW-200398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDF data has been extensively deployed describing various types of resources in a structured way. Links between data elements described by RDF models stand for the core of Semantic Web. The rising amount of structured data published in public RDF repositories, also known as Linked Open Data, elucid ates the success of the global and unified dataset proposed by the vision of the Semantic Web. Nowadays, semi-automatic algorithms build connections among these datasets by exploring a variety of methods. Interconnected open data demands automatic methods and tools to maintain their consistency over time. The update of linked data is considered as key process due to the evolutionary characteristic of such structured datasets. However, data changing operations might influence well-formed links, which turns difficult to maintain the consistencies of connections over time. In this article, we propose a thorough survey that provides a systematic review of the state of the art in link maintenance in linked open data evolution scenario. We conduct a detailed analysis of the literature for characterising and understanding methods and algorithms responsible for detecting, fixing and updating links between RDF data. Our investigation provides a categorisation of existing approaches as well as describes and discusses existing studies. The results reveal an absence of comprehensive solutions suited to fully detect, warn and automatically maintain the consistency of linked data over time.},
  archive      = {J_SW},
  author       = {Regino, Andre Gomes and dos Reis, Julio Cesar and Bonacin, Rodrigo and Morshed, Ahsan and Sellis, Timos},
  doi          = {10.3233/SW-200398},
  journal      = {Semantic Web},
  month        = {3},
  number       = {3},
  pages        = {517-541},
  shortjournal = {Semantic Web},
  title        = {Link maintenance for integrity in linked open data evolution: Literature survey and open challenges},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Challenge-derived design practices for a semantic gazetteer
for medieval and early modern places. <em>SW</em>, <em>12</em>(3),
493–515. (<a href="https://doi.org/10.3233/SW-200394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years gazetteers based on semantic web technologies were discussed as an effective way to describe, formalize and standardize place data by using contextual information as a method to structure and distinguish places from each other. While research concerning semantic gazetteers with rega rd to historical places has pointed out the importance of enabling the creation of a global and epoch-spanning gazetteer, we want to emphasize the importance of taking a domain oriented approach as well – in our case, focusing on places set in medieval and early modern times. By discussing the topic from the historians’ perspective, we will be able to identify a number of challenges that are specific to the semantic representation of places set in these time periods. We will then do a survey of existing gazetteer projects that are taking historical places into account. This will enable us to find out which technologies and practices already exist, that can meet the demands of a gazetteer that considers the time specific geographic, social and administrative structures of medieval and early modern times. Finally we will develop a catalogue of design practices for such a semantic gazetteer. Our recommendations will be derived from these existing solutions as well as from our epoch-specific challenges identified before.},
  archive      = {J_SW},
  author       = {Schneider, Philipp and Jones, Jim and Hiltmann, Torsten and Kauppinen, Tomi},
  doi          = {10.3233/SW-200394},
  journal      = {Semantic Web},
  month        = {3},
  number       = {3},
  pages        = {493-515},
  shortjournal = {Semantic Web},
  title        = {Challenge-derived design practices for a semantic gazetteer for medieval and early modern places},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video representation and suspicious event detection using
semantic technologies. <em>SW</em>, <em>12</em>(3), 467–491. (<a
href="https://doi.org/10.3233/SW-200393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storage and analysis of video surveillance data is a significant challenge, requiring video interpretation and event detection in the relevant context. To perform this task, the low-level features including shape, texture, and color information are extracted and represented in symbolic forms. In th is work, a methodology is proposed, which extracts the salient features and properties using machine learning techniques and represent this information as Linked Data using a domain ontology that is explicitly tailored for detection of certain activities. An ontology is also developed to include concepts and properties which may be applicable in the domain of surveillance and its applications. The proposed approach is validated with actual implementation and is thus evaluated by recognizing suspicious activity in an open parking space. The suspicious activity detection is formalized through inference rules and SPARQL queries. Eventually, Semantic Web Technology has proven to be a remarkable toolchain to interpret videos, thus opening novel possibilities for video scene representation, and detection of complex events, without any human involvement. The proposed novel approach can thus have representation of frame-level information of a video in structured representation and perform event detection while reducing storage and enhancing semantically-aided retrieval of video data.},
  archive      = {J_SW},
  author       = {Patel, Ashish Singh and Merlino, Giovanni and Bruneo, Dario and Puliafito, Antonio and Vyas, O.P. and Ojha, Muneendra},
  doi          = {10.3233/SW-200393},
  journal      = {Semantic Web},
  month        = {3},
  number       = {3},
  pages        = {467-491},
  shortjournal = {Semantic Web},
  title        = {Video representation and suspicious event detection using semantic technologies},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparative study of methods for a priori prediction of
MCQ difficulty. <em>SW</em>, <em>12</em>(3), 449–465. (<a
href="https://doi.org/10.3233/SW-200390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Successful exams require a balance of easy, medium, and difficult questions. Question difficulty is generally either estimated by an expert or determined after an exam is taken. The latter provides no utility for the generation of new questions and the former is expensive both in terms of time and cost. Additionally, it is not known whether expert prediction is indeed a good proxy for estimating question difficulty. In this paper, we analyse and compare two ontology-based measures for difficulty prediction of multiple choice questions, as well as comparing each measure with expert prediction (by 15 experts) against the exam performance of 12 residents over a corpus of 231 medical case-based questions that are in multiple choice format. We find one ontology-based measure (relation strength indicativeness) to be of comparable performance (accuracy = 47%) to expert prediction (average accuracy = 49%).},
  archive      = {J_SW},
  author       = {Kurdi, Ghader and Leo, Jared and Matentzoglu, Nicolas and Parsia, Bijan and Sattler, Uli and Forge, Sophie and Donato, Gina and Dowling, Will},
  doi          = {10.3233/SW-200390},
  journal      = {Semantic Web},
  month        = {3},
  number       = {3},
  pages        = {449-465},
  shortjournal = {Semantic Web},
  title        = {A comparative study of methods for a priori prediction of MCQ difficulty},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unsupervised approach to disjointness learning based on
terminological cluster trees. <em>SW</em>, <em>12</em>(3), 423–447. (<a
href="https://doi.org/10.3233/SW-200391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the Semantic Web regarded as a Web of Data, research efforts have been devoted to improving the quality of the ontologies that are used as vocabularies to enable complex services based on automated reasoning. From various surveys it emerges that many domains would require better o ntologies that include non-negligible constraints for properly conveying the intended semantics. In this respect, disjointness axioms are representative of this general problem: these axioms are essential for making the negative knowledge about the domain of interest explicit yet they are often overlooked during the modeling process (thus affecting the efficacy of the reasoning services). To tackle this problem, automated methods for discovering these axioms can be used as a tool for supporting knowledge engineers in modeling new ontologies or evolving existing ones. The current solutions, either based on statistical correlations or relying on external corpora, often do not fully exploit the terminology. Stemming from this consideration, we have been investigating on alternative methods to elicit disjointness axioms from existing ontologies based on the induction of terminological cluster trees, which are logic trees in which each node stands for a cluster of individuals which emerges as a sub-concept. The growth of such trees relies on a divide-and-conquer procedure that assigns, for the cluster representing the root node, one of the concept descriptions generated via a refinement operator and selected according to a heuristic based on the minimization of the risk of overlap between the candidate sub-clusters (quantified in terms of the distance between two prototypical individuals). Preliminary works have showed some shortcomings that are tackled in this paper. To tackle the task of disjointness axioms discovery we have extended the terminological cluster tree induction framework with various contributions: 1) the adoption of different distance measures for clustering the individuals of a knowledge base; 2) the adoption of different heuristics for selecting the most promising concept descriptions; 3) a modified version of the refinement operator to prevent the introduction of inconsistency during the elicitation of the new axioms. A wide empirical evaluation showed the feasibility of the proposed extensions and the improvement with respect to alternative approaches.},
  archive      = {J_SW},
  author       = {Rizzo, Giuseppe and d’Amato, Claudia and Fanizzi, Nicola},
  doi          = {10.3233/SW-200391},
  journal      = {Semantic Web},
  month        = {3},
  number       = {3},
  pages        = {423-447},
  shortjournal = {Semantic Web},
  title        = {An unsupervised approach to disjointness learning based on terminological cluster trees},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical methodology for crowdsourcing ground truth.
<em>SW</em>, <em>12</em>(3), 403–421. (<a
href="https://doi.org/10.3233/SW-200415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of gathering ground truth data through human annotation is a major bottleneck in the use of information extraction methods for populating the Semantic Web. Crowdsourcing-based approaches are gaining popularity in the attempt to solve the issues related to volume of data and lack of anno tators. Typically these practices use inter-annotator agreement as a measure of quality. However, in many domains, such as event detection, there is ambiguity in the data, as well as a multitude of perspectives of the information examples. We present an empirically derived methodology for efficiently gathering of ground truth data in a diverse set of use cases covering a variety of domains and annotation tasks. Central to our approach is the use of CrowdTruth metrics that capture inter-annotator disagreement. We show that measuring disagreement is essential for acquiring a high quality ground truth. We achieve this by comparing the quality of the data aggregated with CrowdTruth metrics with majority vote, over a set of diverse crowdsourcing tasks: Medical Relation Extraction, Twitter Event Identification, News Event Extraction and Sound Interpretation. We also show that an increased number of crowd workers leads to growth and stabilization in the quality of annotations, going against the usual practice of employing a small number of annotators.},
  archive      = {J_SW},
  author       = {Dumitrache, Anca and Inel, Oana and Timmermans, Benjamin and Ortiz, Carlos and Sips, Robert-Jan and Aroyo, Lora and Welty, Chris},
  doi          = {10.3233/SW-200415},
  journal      = {Semantic Web},
  month        = {3},
  number       = {3},
  pages        = {403-421},
  shortjournal = {Semantic Web},
  title        = {Empirical methodology for crowdsourcing ground truth},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Open science data and the semantic web journal. <em>SW</em>,
<em>12</em>(3), 401–402. (<a
href="https://doi.org/10.3233/SW-210427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Hitzler, Pascal and Janowicz, Krzysztof and Shimizu, Cogan and Zhou, Lu and Eells, Andrew},
  doi          = {10.3233/SW-210427},
  journal      = {Semantic Web},
  month        = {3},
  number       = {3},
  pages        = {401-402},
  shortjournal = {Semantic Web},
  title        = {Open science data and the Semantic&amp;nbsp;Web&amp;nbsp;journal},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gravsearch: Transforming SPARQL to query humanities data.
<em>SW</em>, <em>12</em>(2), 379–400. (<a
href="https://doi.org/10.3233/SW-200386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDF triplestores have become an appealing option for storing and publishing humanities data, but available technologies for querying this data have drawbacks that make them unsuitable for many applications. Gravsearch (Virtual Graph Search), a SPARQL transformer developed as part of a web-based API, is designed to support complex searches that are desirable in humanities research, while avoiding these disadvantages. It does this by introducing server software that mediates between the client and the triplestore, transforming an input SPARQL query into one or more queries executed by the triplestore. This design suggests a practical way to go beyond some limitations of the ways that RDF data has generally been made available.},
  archive      = {J_SW},
  author       = {Schweizer, Tobias and Geer, Benjamin},
  doi          = {10.3233/SW-200386},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {379-400},
  shortjournal = {Semantic Web},
  title        = {Gravsearch: Transforming SPARQL to query humanities data},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applying and developing semantic web technologies for
exploiting a corpus in history of science: The case study of the henri
poincaré correspondence. <em>SW</em>, <em>12</em>(2), 359–378. (<a
href="https://doi.org/10.3233/SW-200400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Henri Poincaré correspondence is a corpus of letters sent and received by this mathematician. The edition of this correspondence is a long-term project begun during the 1990s. Since 1999, a website is devoted to publish online this correspondence with digitized letters. In 2017, it has been dec ided to reforge this website using Omeka S. This content management system offers useful services but some user needs have led to the development of an RDFS infrastructure associated to it. Approximate and explained searches are managed thanks to SPARQL query transformations. A prototype for efficient RDF annotation of this corpus (and similar corpora) has been designed and implemented. This article deals with these three research issues and how they are addressed.},
  archive      = {J_SW},
  author       = {Bruneau, Olivier and Lasolle, Nicolas and Lieber, Jean and Nauer, Emmanuel and Pavlova, Siyana and Rollet, Laurent},
  doi          = {10.3233/SW-200400},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {359-378},
  shortjournal = {Semantic Web},
  title        = {Applying and developing semantic web technologies for exploiting a corpus in history of science: The case study of the henri poincar&amp;eacute; correspondence},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pattern-based design applied to cultural heritage knowledge
graphs. <em>SW</em>, <em>12</em>(2), 313–357. (<a
href="https://doi.org/10.3233/SW-200422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology Design Patterns (ODPs) have become an established and recognised practice for guaranteeing good quality ontology engineering. There are several ODP repositories where ODPs are shared as well as ontology design methodologies recommending their reuse. Performing rigorous testing is recommend ed as well for supporting ontology maintenance and validating the resulting resource against its motivating requirements. Nevertheless, it is less than straightforward to find guidelines on how to apply such methodologies for developing domain-specific knowledge graphs. ArCo is the knowledge graph of Italian Cultural Heritage and has been developed by using eXtreme Design (XD), an ODP- and test-driven methodology. During its development, XD has been adapted to the need of the CH domain e.g. gathering requirements from an open, diverse community of consumers, a new ODP has been defined and many have been specialised to address specific CH requirements. This paper presents ArCo and describes how to apply XD to the development and validation of a CH knowledge graph, also detailing the (intellectual) process implemented for matching the encountered modelling problems to ODPs. Relevant contributions also include a novel web tool for supporting unit-testing of knowledge graphs, a rigorous evaluation of ArCo, and a discussion of methodological lessons learned during ArCo’s development.},
  archive      = {J_SW},
  author       = {Carriero, Valentina Anita and Gangemi, Aldo and Mancinelli, Maria Letizia and Nuzzolese, Andrea Giovanni and Presutti, Valentina and Veninata, Chiara},
  doi          = {10.3233/SW-200422},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {313-357},
  shortjournal = {Semantic Web},
  title        = {Pattern-based design applied to cultural heritage knowledge graphs},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OntoAndalus: An ontology of islamic artefacts for
terminological purposes. <em>SW</em>, <em>12</em>(2), 295–311. (<a
href="https://doi.org/10.3233/SW-200387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OntoAndalus aims at constituting a shared conceptualisation of the domain within a future multilingual terminological resource targeted at experts and students of Islamic archaeology. The present version of OntoAndalus is aligned with DOLCE+DnS Ultralite (DUL), an established top-level ontology for the Semantic Web. This article describes the modelling assumptions underlying OntoAndalus, as well as the more relevant design patterns (i.e. artefact types, events and individuals). The latter are exemplified through relevant case studies in the domain, namely those of lighting artefacts, the life cycle of pottery and the several descriptions of Vaso de Tavira.},
  archive      = {J_SW},
  author       = {Almeida, Bruno and Costa, Rute},
  doi          = {10.3233/SW-200387},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {295-311},
  shortjournal = {Semantic Web},
  title        = {OntoAndalus: An ontology of islamic artefacts for terminological purposes},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A challenge for historical research: Making data FAIR using
a collaborative ontology management environment (OntoME). <em>SW</em>,
<em>12</em>(2), 279–294. (<a
href="https://doi.org/10.3233/SW-200416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the issue of interoperability of data generated by historical research and heritage institutions in order to make them re-usable for new research agendas according to the FAIR principles. After introducing the symogih.org project’s ontology, it proposes a description of the essential aspects of the process of historical knowledge production. It then develops an epistemological and semantic analysis of conceptual data modelling applied to factual historical information, based on the foundational ontologies Constructive Descriptions and Situations and DOLCE, and discusses the reasons for adopting the CIDOC CRM as a core ontology for the field of historical research, but extending it with some relevant, missing high-level classes. Finally, it shows how collaborative data modelling carried out in the ontology management environment OntoME makes it possible to elaborate a communal fine-grained and adaptive ontology of the domain, provided an active research community engages in this process. With this in mind, the Data for history consortium was founded in 2017 and promotes the adoption of a shared conceptualization in the field of historical research.},
  archive      = {J_SW},
  author       = {Beretta, Francesco},
  doi          = {10.3233/SW-200416},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {279-294},
  shortjournal = {Semantic Web},
  title        = {A&amp;nbsp;challenge for historical research: Making data FAIR using a collaborative ontology management environment (OntoME)},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WarSampo knowledge graph: Finland in the second world war as
linked open data. <em>SW</em>, <em>12</em>(2), 265–278. (<a
href="https://doi.org/10.3233/SW-200392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Second World War (WW2) is arguably the most devastating catastrophe of human history, a topic of great interest to not only researchers but the general public. However, data about the Second World War is heterogeneous and distributed in various organizations and countries making it hard to util ize. In order to create aggregated global views of the war, a shared ontology and data infrastructure is needed to harmonize information in various data silos. This makes it possible to share data between publishers and application developers, to support data analysis in Digital Humanities research, and to develop data-driven intelligent applications. As a first step towards these goals, this article presents the WarSampo knowledge graph (KG), a shared semantic infrastructure, and a Linked Open Data (LOD) service for publishing data about WW2, with a focus on Finnish military history. The shared semantic infrastructure is based on the idea of representing war as a spatio-temporal sequence of events that soldiers, military units, and other actors participate in. The used metadata schema is an extension of CIDOC CRM, supplemented by various military history domain ontologies. With an infrastructure containing shared ontologies, maintaining the interlinked data brings upon new challenges, as one change in an ontology can propagate across several datasets that use it. To support sustainability, a repeatable automatic data transformation and linking pipeline has been created for rebuilding the whole WarSampo KG from the individual source datasets. The WarSampo KG is hosted on a data service based on W3C Semantic Web standards and best practices, including content negotiation, SPARQL API, download, automatic documentation, and other services supporting the reuse of the data. The WarSampo KG, a part of the international LOD Cloud and totalling ca. 14 million triples, is in use in nine end-user application views of the WarSampo portal, which has had over 690 000 end users since its opening in 2015.},
  archive      = {J_SW},
  author       = {Koho, Mikko and Ikkala, Esko and Leskinen, Petri and Tamper, Minna and Tuominen, Jouni and Hyvönen, Eero},
  doi          = {10.3233/SW-200392},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {265-278},
  shortjournal = {Semantic Web},
  title        = {WarSampo knowledge graph: Finland in the second world war as linked open data},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Representing narratives in digital libraries: The narrative
ontology. <em>SW</em>, <em>12</em>(2), 241–264. (<a
href="https://doi.org/10.3233/SW-200421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital Libraries (DLs), especially in the Cultural Heritage domain, are rich in narratives. Every digital object in a DL tells some kind of story, regardless of the medium, the genre, or the type of the object. However, DLs do not offer services about narratives, for example it is not possible to discover a narrative, to create one, or to compare two narratives. Certainly, DLs offer discovery functionalities over their contents, but these services merely address the objects that carry the narratives (e.g. books, images, audiovisual objects), without regard for the narratives themselves. The present work aims at introducing narratives as first-class citizens in DLs, by providing a formal expression of what a narrative is. In particular, this paper presents a conceptualisation of the domain of narratives, and its specification through the Narrative Ontology (NOnt for short), expressed in first-order logic. NOnt has been implemented as an extension of three standard vocabularies, i.e. the CIDOC CRM, FRBRoo, and OWL Time, and using the SWRL rule language to express the axioms. On the basis of NOnt, we have developed the Narrative Building and Visualising (NBVT) tool, and applied it in four case studies to validate the ontology. NOnt is also being validated in the context of the Mingei European project, in which it is applied to the representation of knowledge about Craft Heritage.},
  archive      = {J_SW},
  author       = {Meghini, Carlo and Bartalesi, Valentina and Metilli, Daniele},
  doi          = {10.3233/SW-200421},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {241-264},
  shortjournal = {Semantic Web},
  title        = {Representing narratives in digital libraries: The narrative ontology},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Of lions and yakshis. <em>SW</em>, <em>12</em>(2), 219–239.
(<a href="https://doi.org/10.3233/SW-200417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vladimir Propp’s theory Morphology of the Folktale identifies 31 invariant functions, subfunctions, and seven classes of folktale characters to describe the narrative structure of the Russian magic tale. Since it was first published in 1928, Propp’s approach has been used on various folktales of di fferent cultural backgrounds. ProppOntology models Propp’s theory by describing narrative functions using a combination of a function class hierarchy and characteristic relationships between the Dramatis Personae for each function. A special focus lies on the restrictions Propp defined regarding which Dramatis Personae fulfill a certain function. This paper investigates how an ontology can assist traditional Humanities research in examining how well Propp’s theory fits for folktales outside of the Russian–European folktale culture. For this purpose, a lightweight query system has been implemented. To determine how well both the annotation schema and the query system works, twenty African tales and fifteen tales from the Kerala region in India were annotated. The system is evaluated by examining two case studies regarding the representation of characters and the use of Proppian functions in African and Indian tales. The findings are in line with traditional analogous Humanities research. This project shows how carefully modelled ontologies can be utilized as a knowledge base for comparative folklore research.},
  archive      = {J_SW},
  author       = {Pannach, Franziska and Sporleder, Caroline and May, Wolfgang and Krishnan, Aravind and Sewchurran, Anusharani},
  doi          = {10.3233/SW-200417},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {219-239},
  shortjournal = {Semantic Web},
  title        = {Of lions and yakshis},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding the phenomenology of reading through
modelling. <em>SW</em>, <em>12</em>(2), 191–217. (<a
href="https://doi.org/10.3233/SW-200396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale cultural heritage datasets and computational methods for the Humanities research framework are the two pillars of Digital Humanities (DH), a research field aiming to expand Humanities studies beyond specific sources and periods to address macro-scale research questions on broad human ph enomena. In this regard, the development of machine-readable semantically enriched data models based on a cross-disciplinary “language” of phenomena is critical for achieving the interoperability of research data. This paper reports on, documents, and discusses the development of a model for the study of reading experiences as part of the EU JPI-CH project Reading Europe Advanced Data Investigation Tool (READ-IT). Through the discussion of the READ-IT ontology of reading experience, this contribution will highlight and address three challenges emerging from the development of a conceptual model for the support of research on cultural heritage. Firstly, this contribution addresses modelling for multi-disciplinary research. Secondly, this work describes the development of an ontology of reading experience, under the light of the experience of previous projects, and of ongoing and future research developments. Lastly, this contribution addresses the validation of a conceptual model in the context of ongoing research, the lack of a consolidated set of theories and of a consensus of domain experts.},
  archive      = {J_SW},
  author       = {Antonini, Alessio and Suárez-Figueroa, Mari Carmen and Adamou, Alessandro and Benatti, Francesca and Vignale, François and Gravier, Guillaume and Lupi, Lucia},
  doi          = {10.3233/SW-200396},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {191-217},
  shortjournal = {Semantic Web},
  title        = {Understanding the phenomenology of reading through modelling},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling execution techniques of inscriptions. <em>SW</em>,
<em>12</em>(2), 181–190. (<a
href="https://doi.org/10.3233/SW-200395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper discusses a small ontology to describe the features of the execution techniques of inscriptions, based on a recent contribution discussing the classification methodologies. The modeling is done on the basis of existing recent attempts to model epigraphic documents and not on a general eva luation of existing ontologies. The ontology described is used in parallel to enrich and further structure the EAGLE Vocabularies for Execution Technique, which uses SKOS, with possibly immediate impact on the many projects using the concepts contained there.},
  archive      = {J_SW},
  author       = {Liuzzo, Pietro Maria and Evangelisti, Silvia},
  doi          = {10.3233/SW-200395},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {181-190},
  shortjournal = {Semantic Web},
  title        = {Modeling execution techniques of inscriptions},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ce qui est écrit et ce qui est parlé. CRMtex for modelling
textual entities on the semantic web. <em>SW</em>, <em>12</em>(2),
169–180. (<a href="https://doi.org/10.3233/SW-200418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the new developments of CRMtex, an ontological model based on CIDOC CRM, created to describe ancient texts and other semiotic features appearing on inscriptions, papyri, manuscripts and other similar supports. The model is also designed to describe in a formal way the phenomena related to the production, use, conservation, study and interpretation of textual entities. CRMtex was originally intended to detect the close relationship linking ancient texts with the physical objects on which they are supported, the tools and writing systems used for their production, and the various scientific investigations and readings carried out on the text by modern scholars. It eventually evolved to provide researchers with the fundamental concepts for the correct and complete rendering of textual objects, the events representing their history and the cultural and social environments in and for which they were created. The full compatibility of CRMtex with the CIDOC CRM ontology and its extensions ensures persistent interoperability of data encoded by means of its entities with other semantic information produced in cultural heritage and digital humanities. The new entities presented in this paper deal more closely with textual and intertextual structures and try to deepen the close relationships existing between fragments of text or sequences of signs and the underlying meaning they were originally intended to convey.},
  archive      = {J_SW},
  author       = {Felicetti, Achille and Murano, Francesca},
  doi          = {10.3233/SW-200418},
  journal      = {Semantic Web},
  month        = {1},
  number       = {2},
  pages        = {169-180},
  shortjournal = {Semantic Web},
  title        = {Ce qui est &amp;eacute;crit et ce qui est parl&amp;eacute;. CRMtex for modelling textual entities on the semantic web},
  volume       = {12},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
