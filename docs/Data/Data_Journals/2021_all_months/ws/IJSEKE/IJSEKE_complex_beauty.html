<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJSEKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijseke---74">IJSEKE - 74</h2>
<ul>
<li><details>
<summary>
(2021). A fully parallel approach of model checking via probe
machine. <em>IJSEKE</em>, <em>31</em>(11n12), 1761–1781. (<a
href="https://doi.org/10.1142/S0218194021400210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model checking is a verification technique that explores all possible system states in a brute-force manner. However, the state space can be extremely large for many practical systems and the verification time grows exponentially with the size of systems. It is a major limitation for state-space search algorithms of model checking. This paper presents a novel approach to perform Linear Temporal Logic (LTL) and Computation Tree Logic (CTL) model checking by using the connective probe machine, which is a fully parallel computing model. Our state-space search algorithm is based on the semantics of CTL properties and we design transformation algorithms to transform the model of a system into the structure that can run on the existing probe machine. We propose another approach to find multiple accepting cycles in linear time, which greatly shortens the verification time of LTL model checking. Compared to the traditional model checker, our approach can find multiple counterexamples according to the given property, which can trace as many system defects as possible. Simultaneously, it can greatly reduce the verification time for systems with large state spaces. We develop a model checker called MC2PROBE based on our approach and prove the feasibility and efficiency of our checker by experiments.},
  archive      = {J_IJSEKE},
  author       = {Dong Wang and Jing Liu and Haiying Sun and Jin Xu and Jiexiang Kang},
  doi          = {10.1142/S0218194021400210},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1761-1781},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A fully parallel approach of model checking via probe machine},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Remaining activity sequence prediction for ongoing process
instances. <em>IJSEKE</em>, <em>31</em>(11n12), 1741–1760. (<a
href="https://doi.org/10.1142/S0218194021400209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remaining activity sequence prediction (i.e. activity suffix prediction) aims at recommending the most likely future behaviors for ongoing process instances (i.e. traces), which enables process managers to rationally allocate resources and detect process deviations in advance. Recently, techniques of neural networks have found promising applications in activity suffix prediction by training a prediction model for next activity and iteratively performing the model to achieve the whole sequence prediction. However, the iterative prediction accumulates the deviations of each iteration and the result also lacks interpretability. In this paper, we propose a novel method to predict activity suffixes from the perspective of control flow and data flow for ongoing traces, where process discovery and trace replay techniques are employed to simulate executions of traces under real conditions and Long Short-Term Memory (LSTM) is applied to characterize the correlation between executed information and future execution. Sequence matching between historical prefix traces and ongoing traces is performed based on the above information to select the optimal-matched (i.e. most similar) activity suffix for ongoing process instances. Experiments on real-life datasets demonstrate that the proposed method outperforms other methods.},
  archive      = {J_IJSEKE},
  author       = {Xiaoxiao Sun and Yuke Ying and Siqing Yang and Hujun Shen},
  doi          = {10.1142/S0218194021400209},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1741-1760},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Remaining activity sequence prediction for ongoing process instances},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatically generating release notes with content
classification models. <em>IJSEKE</em>, <em>31</em>(11n12), 1721–1740.
(<a href="https://doi.org/10.1142/S0218194021400192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Release notes are admitted as an essential technical document in software maintenance. They summarize the main changes, e.g. bug fixes and new features, that have happened in the software since the previous release. Manually producing release notes is a time-consuming and challenging task. For that reason, sometimes developers neglect to write release notes. For example, we collect data from GitHub with over 1900 releases, and among them, 37% of the release notes are empty. To mitigate this problem, we propose an automatic release notes generation approach by applying the text summarization techniques, i.e. TextRank. To improve the keyword extraction method of traditional TextRank, we integrate the GloVe word embedding technique with TextRank. After generating release notes automatically, we apply machine learning algorithms to classify the release note contents (or sentences). We classify the contents into six categories, e.g. bug fixes and performance improvements, to represent the release notes better for users. We use the evaluation metric, e.g. ROUGE, to evaluate the automatically generated release notes. We also compare the performance of our technique with two popular extractive algorithms, e.g. Luhn’s and latent semantic analysis (LSA). Our evaluation results show that the improved TextRank method outperforms the two algorithms.},
  archive      = {J_IJSEKE},
  author       = {Sristy Sumana Nath and Banani Roy},
  doi          = {10.1142/S0218194021400192},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1721-1740},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automatically generating release notes with content classification models},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing the cloud performance using different user
subscription times. <em>IJSEKE</em>, <em>31</em>(11n12), 1699–1720. (<a
href="https://doi.org/10.1142/S0218194021400180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud providers face the challenge of managing large amounts of heterogeneous resources in real time. It is usually very costly to conduct experiments with real cloud systems. Therefore, tools to analyze and evaluate cloud scenarios and experimental studies are very useful for them. In this paper, we model cloud systems and the user interactions with the cloud provider using the UML2Cloud profile. In general, users request virtual machines according to their needs, but they can also subscribe to the cloud provider and wait to be notified when the requested resources are not available. In this case, users indicate a maximum subscription time, so once this time elapses without being notified, users leave the system unattended. Thus, we present an exhaustive experimental study to measure how the user subscription times affect the overall system responsiveness. To this end, four different cloud configurations are analyzed, and the workloads for these studies are produced by using three distribution functions for the user arrivals, namely, a uniform, a normal, and a cyclic normal distribution. Furthermore, we also analyze the cloud performance with a workload obtained from a real trace. The purpose of this study is to find out the inflection point for the waiting time of the users, from which the cloud responsiveness and its performance do not improve. The obtained information is, therefore, useful for the cloud provider to improve the configuration of the cloud.},
  archive      = {J_IJSEKE},
  author       = {Adrián Bernal and M. Emilia Cambronero and Pablo C. Cañizares and Alberto Núñez and Valentín Valero and Hernán-Indibil de la Cruz},
  doi          = {10.1142/S0218194021400180},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1699-1720},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Analyzing the cloud performance using different user subscription times},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiclass classification of UML diagrams from images using
deep learning. <em>IJSEKE</em>, <em>31</em>(11n12), 1683–1698. (<a
href="https://doi.org/10.1142/S0218194021400179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unified Modeling Language (UML) diagrams are a recognized standard modeling language for representing design of software systems. For academic research, large cases containing UML diagrams are needed. One of the challenges in collecting such datasets is automatically determining whether an image is a UML diagram or not and what type of UML diagram an image contains. In this work, we collected UML diagrams from open datasets and manually labeled them into 10 types of UML diagrams (i.e. class diagrams, activity diagrams, use case diagrams, sequence diagrams, communication diagrams, component diagrams, deployment diagrams, object diagrams, package diagrams, and state machine diagrams) and non-UML images. We evaluated the performance of seven popular neural network architectures using transfer learning on the dataset of 4706 images, including 700 class diagrams, 454 activity diagrams, 651 use case diagrams, 706 sequence diagrams, 204 communication diagrams, 208 component diagrams, 287 deployment diagrams, 207 object diagrams, 246 package diagrams, 323 state machine diagrams, and 720 non-UML images, respectively. We also proposed our neural network architecture for multiclass classification of UML diagrams. The experiment results show that Xception achieved the best performance amongst the algorithms we evaluated with a precision of 93.03%, a recall of 92.44%, and an F 1-score of 92.73%. Moreover, it is possible to develop small and almost the same efficient neural network architectures, that our proposed architecture has the least parameters (around 2.4 millions) and spends the least time per image (0.0135 s per image using graphics processing unit) for classifying UML diagrams with a precision of 91.25%, a recall of 90.34%, and an F 1-score of 90.79%.},
  archive      = {J_IJSEKE},
  author       = {Sergei Shcherban and Peng Liang and Zengyang Li and Chen Yang},
  doi          = {10.1142/S0218194021400179},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1683-1698},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Multiclass classification of UML diagrams from images using deep learning},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An approach of automated anomalous microservice ranking in
cloud-native environments. <em>IJSEKE</em>, <em>31</em>(11n12),
1661–1681. (<a href="https://doi.org/10.1142/S0218194021400167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, more and more developers have been building applications based on the cloud-native architecture. Container and microservice are two essential components in the cloud-native architecture. Container technologies like Docker and Kubernetes can help developers achieve a consistent and scalable delivery for complex software applications. On the other hand, microservice technologies can facilitate the division of complex applications into multiple functionality-independent and composable components, which further increases the flexibility of applications. With the support of cloud computing platforms, cloud-native applications will be easier to manage and maintain, together with higher scalability. However, it is challenging to identify performance issues on microservices due to the complex runtime environments and the numerous monitoring metrics. Towards this issue, this paper proposes a novel root cause analysis approach. Our approach firstly constructs a service dependency graph based on the metrics collected in real time. Next, the anomaly weight of each microservice is automatically updated by extending the mRank algorithm. Finally, a PageRank-based random walk is adopted to rank root causes further, i.e. to rank potential problematic services. Experiments conducted on Kubernetes clusters show that the proposed approach achieves a good analysis result, which outperforms several baseline methods.},
  archive      = {J_IJSEKE},
  author       = {Zekun Zhang and Bing Li and Jian Wang and Yongqiang Liu},
  doi          = {10.1142/S0218194021400167},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1661-1681},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An approach of automated anomalous microservice ranking in cloud-native environments},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Patterns for reuse in production systems engineering.
<em>IJSEKE</em>, <em>31</em>(11n12), 1623–1659. (<a
href="https://doi.org/10.1142/S0218194021400155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Production Systems Engineering (PSE), domain experts aim at reusing partial system designs implemented as Industry 4.0 assets and software. However, the knowledge on assets is often scattered across engineering artifacts from multiple disciplines and domain experts, making it difficult to find reusable assets and map them to requirements. In this paper, we (i) identify challenges and requirements for the representation of reuse knowledge in PSE, based on the results of a domain analysis in automotive manufacturing; (ii) refine the Industry 4.0 Asset Network (I4AN) meta-model that integrates multi-disciplinary dependencies between the assets; (iii) introduce the I4AN reference model that exposes recurring patterns; and (iv) present basic and applied patterns for reuse in PSE that aim at improving reuse efficiency and lowering risks. We evaluate the I4AN reference model and patterns with reuse scenarios in a feasibility study in automotive manufacturing. The study results indicate that the I4AN reference model and patterns satisfy the elicited requirements and enable PSE domain experts to identify patterns for reuse and sufficiently complete sets of reusable assets in their contexts.},
  archive      = {J_IJSEKE},
  author       = {Kristof Meixner and Arndt Lüder and Jan Herzog and Dietmar Winkler and Stefan Biffl},
  doi          = {10.1142/S0218194021400155},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1623-1659},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Patterns for reuse in production systems engineering},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient sorting algorithm for non-volatile memory.
<em>IJSEKE</em>, <em>31</em>(11n12), 1603–1621. (<a
href="https://doi.org/10.1142/S0218194021400143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-volatile memory (NVM) has emerged as an alternative of the next-generation memory due to its non-volatility, byte addressability, high storage-density, and low-energy consumption. However, NVM also has some limitations, e.g. asymmetric read and write latency. Therefore, at present, it is not realistic to completely replace DRAM with NVM in computer systems. A more feasible scheme is to adopt the hybrid memory architecture composed of NVM and DRAM. Following the assumption of hybrid memory architecture, in this paper, we propose an NVM-friendly sorting algorithm called NVMSorting. Particularly, we introduce a new concept called Natural Run to improve the existing MONTRES algorithm. Further, we apply the proposed NVMSorting to database join algorithms to improve the performance of the existing sort-merge join. To verify the performance of our proposal, we implement six existing sorting algorithms as baselines, including the MONTRES algorithm, and conduct comparative experiments on real Intel Optane DC persistent memory. The results show that NVMSorting outperforms other sorting algorithms in terms of execution time and NVM writes. In addition, the results of the join experiment show that the NVMSorting algorithm achieves the highest performance among all schemes. Especially, in the partially ordered data, the execution time of NVMSorting is 2.9%, 2.7%, and 4.2% less than MONTRES, external sort, and quick sort, respectively. Also, the amount of NVM writes of the NVMSorting is 26.1%, 43.6%, 96.2% less than MONTRES, external sort, and quick sort, respectively.},
  archive      = {J_IJSEKE},
  author       = {Zhaole Chu and Yongping Luo and Peiquan Jin},
  doi          = {10.1142/S0218194021400143},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1603-1621},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An efficient sorting algorithm for non-volatile memory},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid model combining formulae with keywords for
mathematical information retrieval. <em>IJSEKE</em>, <em>31</em>(11n12),
1583–1602. (<a href="https://doi.org/10.1142/S0218194021400131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formula retrieval is an important research topic in Mathematical Information Retrieval (MIR). Most studies have focused on formula comparison to determine the similarity between mathematical documents. However, two similar formulae may appear in entirely different knowledge domains and have different meanings. Based on N-ary Tree-based Formula Embedding Model (NTFEM, our previous work in [Y. Dai, L. Chen, and Z. Zhang, An N-ary tree-based model for similarity evaluation on mathematical formulae, in Proc. 2020 IEEE Int. Conf. Systems, Man, and Cybernetics , 2020, pp. 2578–2584.], we introduce a new hybrid retrieval model, NTFEM-K, which combines formulae with their surrounding keywords for more accurate retrieval. By using keywords extraction technology, we extract keywords from context, which can supplement the semantic information of the formula. Then, we get the vector representations of keywords by FastText N-gram embedding model and the vector representations of formulae by NTFEM. Finally, documents are sorted according to the similarity between keywords, and then the ranking results are optimized by formula similarity. For performance evaluation, NTFEM-K is not only compared with NTFEM but also hybrid retrieval models combining formulae with long text and hybrid retrieval models combining formulae with their keywords using other keyword extraction algorithms. Experimental results show that the accuracy of top-10 results of NTFEM-K is at least 20% higher than that of NTFEM and can be 50% in some specific topics.},
  archive      = {J_IJSEKE},
  author       = {Yuqi Shen and Cheng Chen and Yifan Dai and Jinfang Cai and Liangyu Chen},
  doi          = {10.1142/S0218194021400131},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1583-1602},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A hybrid model combining formulae with keywords for mathematical information retrieval},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BSR-TC: Adaptively sampling for accurate triangle counting
over evolving graph streams. <em>IJSEKE</em>, <em>31</em>(11n12),
1561–1581. (<a href="https://doi.org/10.1142/S021819402140012X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle counting is a fundamental graph mining problem, widely employed in various real-world application scenarios. Given the large scale of graph streams and limited memory space, it is feasible to achieve the estimation of global and local triangles by sampling. Existing streaming algorithms for triangle counting can be generalized into two categories: Reservoir-based methods and Bernoulli-based methods. The former use a fixed memory budget, whose size is difficult to set for accurate estimation without any prior knowledge about graph streams. The latter sample edges by a specified probability, but memory budget is uncontrollable for following a binomial distribution. In this work, we propose a novel and bounded-sampling-ratio algorithm for both global and local triangle counting, called BSR-TC, by adaptively resizing memory budget upwards over evolving graph streams. Specifically, our proposed single-pass BSR-TC can gain more advantage than the state-of-the-art algorithms over the continuous growth of graph streams. Experimental results show that BSR-TC achieves accuracy of at least 99.8% for global triangles, when the ratio of initial memory budget against whole graph streams ≥ 0 . 0 0 2 % and given threshold = 2 0 % , respectively.},
  archive      = {J_IJSEKE},
  author       = {Wei Xuan and Huawei Cao and Mingyu Yan and Zhimin Tang and Xiaochun Ye and Dongrui Fan},
  doi          = {10.1142/S021819402140012X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1561-1581},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {BSR-TC: Adaptively sampling for accurate triangle counting over evolving graph streams},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Formal verification of multitask hybrid systems by the
OTS/CafeOBJ method. <em>IJSEKE</em>, <em>31</em>(11n12), 1541–1559. (<a
href="https://doi.org/10.1142/S0218194021400118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid systems combine both continuous and discrete behaviors, which occur frequently in safety-critical applications in various domains including Internet-of-Things (IoT) and Cyber-Physical Systems (CPS) applications such as health care, transportation, and robotics. For safe and reliable information society with IoT and CPS technologies, it is important to establish a way to specify and verify hybrid systems formally. Formal descriptions of hybrid systems may help us to verify desired properties of a given system formally with computer supports. We propose a way to describe a formal specification of a given multitask hybrid system as an observational transition system (OTS) in CafeOBJ algebraic specification language. OTSs are models where systems behaviors are described through observations. CafeOBJ supports specification execution based on a rewrite theory. We verify that OTS/CafeOBJ specifications of hybrid systems satisfy desired property by the proof score method based on equational reasoning implemented in CafeOBJ interpreter. In this paper, we specify a signal control system with an arbitrary number of vehicles by our proposed method, and verify the system satisfies a safety property by the proof score method.},
  archive      = {J_IJSEKE},
  author       = {Masaki Nakamura and Kazutoshi Sakakibara and Yuki Okura and Kazuhiro Ogata},
  doi          = {10.1142/S0218194021400118},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1541-1559},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Formal verification of multitask hybrid systems by the OTS/CafeOBJ method},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editor’s introduction. <em>IJSEKE</em>,
<em>31</em>(11n12), 1539. (<a
href="https://doi.org/10.1142/S0218194021020034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Shi-Kuo Chang},
  doi          = {10.1142/S0218194021020034},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1539},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editor’s introduction},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-aware conversational recommendation of
trigger-action rules in IoT programming. <em>IJSEKE</em>,
<em>31</em>(10), 1517–1538. (<a
href="https://doi.org/10.1142/S0218194021500510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trigger-action (TA) programming is a programming paradigm that allows end-users to automate and connect IoT devices and online services using if-trigger-then-action rules. Early studies have demonstrated this paradigms usability, but more recent work has also highlighted complexities that arise in realistic scenarios. To facilitate end-users in TA programming, we propose AutoTAR, a context-aware conversational recommendation technique for recommending TA rules. AutoTAR leverages a TA knowledge graph to encode semantic features and abstract functionalities of rules, and then takes a two-phase method to recommend TA rules to end-users: during the context-aware recommendation phase, it elicits user preferences from programming context and recommends the top-N rules using a mixed content and collaborative technique; during the conversational recommendation phase, it justifies recommendations by iteratively raising questions and collecting feedback from end-users. We evaluate AutoTAR on Mturk and real data collected from the IFTTT community. The results show that our method outperforms state-of-the-arts significantly — its context-aware recommendation outperforms RecRules by 26% on R@5 and 21% on NDCG@5; its conversational recommendation outperforms LARecommender (a conversational recommender with the LA model) by 67.64% on accuracy. In addition, AutoTAR is effective in solving three problems frequently occurring in TA rule recommendations, i.e. , the cold-start problem, the repeat-consumption problem, and the incomplete-intent problem.},
  archive      = {J_IJSEKE},
  author       = {Mingxin Zhao and Qinyue Wu and Enze Ma and Beijun Shen and Yuting Chen},
  doi          = {10.1142/S0218194021500510},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1517-1538},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Context-aware conversational recommendation of trigger-action rules in IoT programming},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embodying the number of an entity’s relations for knowledge
representation learning. <em>IJSEKE</em>, <em>31</em>(10), 1495–1515.
(<a href="https://doi.org/10.1142/S0218194021500509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge representation learning (knowledge graph embedding) plays a critical role in the application of knowledge graph construction. The multi-source information knowledge representation learning, which is one class of the most promising knowledge representation learning at present, mainly focuses on learning a large number of useful additional information of entities and relations in the knowledge graph into their embeddings, such as the text description information, entity type information, visual information, graph structure information, etc. However, there is a kind of simple but very common information — the number of an entity’s relations which means the number of an entity’s semantic types has been ignored. This work proposes a multi-source knowledge representation learning model KRL-NER, which embodies information of the number of an entity’s relations between entities into the entities’ embeddings through the attention mechanism. Specifically, first of all, we design and construct a submodel of the KRL-NER LearnNER which learns an embedding including the information on the number of an entity’s relations; then, we obtain a new embedding by exerting attention onto the embedding learned by the models such as TransE with this embedding; finally, we translate based onto the new embedding. Experiments, such as related tasks on knowledge graph: entity prediction, entity prediction under different relation types, and triple classification, are carried out to verify our model. The results show that our model is effective on the large-scale knowledge graphs, e.g. FB15K.},
  archive      = {J_IJSEKE},
  author       = {Xinhua Suo and Bing Guo and Yan Shen and Wei Wang and Yaosen Chen and Zhen Zhang},
  doi          = {10.1142/S0218194021500509},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1495-1515},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Embodying the number of an entity’s relations for knowledge representation learning},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Test oracle generation based on BPNN by using the values of
variables at different breakpoints for programs. <em>IJSEKE</em>,
<em>31</em>(10), 1469–1494. (<a
href="https://doi.org/10.1142/S0218194021500492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic test oracle generation is a bottleneck in realizing full automation of the entire software testing process. This study proposes a new method for automatically generating a test oracle for a new test input on the basis of several historical test cases by using a backpropagation neural network (BPNN) model. The new method is different from existing test oracle techniques. Specifically, our method has two steps. First, the values of variables are collected as training data when several historical test inputs are used to execute the program at different breakpoints. The test oracles (pass or fail) of these test cases are utilized to classify and label the training data. Second, a new test input is used to execute the program at different breakpoints, where the trained BPNN prediction model automatically generates its test oracle on the basis of the collected values of the variables involved. We conduct an experiment to validate our method. In the experiment, 113 faulty versions of seven types of programs are used as experimental objects. Results show that the average prediction accuracy rate of 74,651 test oracles is 95.8%. Although the failed test cases in the training data account for less than 5%, the overall average recall rate (prediction accuracy of test case execution failure) of all programs is 78.9%. Furthermore, the trained BPNN can reveal not only the impact of the values of variables but also the impact of the logical correspondence between variables in test oracle generation.},
  archive      = {J_IJSEKE},
  author       = {Chunyan Ma and Shaoying Liu and Jinglan Fu and Tao Zhang},
  doi          = {10.1142/S0218194021500492},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1469-1494},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Test oracle generation based on BPNN by using the values of variables at different breakpoints for programs},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the use of generative deep learning approaches for
generating hidden test scripts. <em>IJSEKE</em>, <em>31</em>(10),
1447–1468. (<a href="https://doi.org/10.1142/S0218194021500480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of web 2.0, web application architectures have been evolved, and their complexity has grown enormously. Due to the complexity, testing of web applications is getting time-consuming and intensive process. In today’s web applications, users can achieve the same goal by performing different actions. To ensure that the entire system is safe and robust, developers try to test all possible user action sequences in the testing phase. Since the space of all the possibilities is enormous, covering all user action sequences can be impossible. To automate the test script generation task and reduce the space of the possible user action sequences, we propose a novel method based on long short-term memory (LSTM) network for generating test scripts from user clickstream data. The experiment results clearly show that generated hidden test sequences are user-like sequences, and the process of generating test scripts with the proposed model is less time-consuming than writing them manually.},
  archive      = {J_IJSEKE},
  author       = {Mert Oz and Caner Kaya and Erdi Olmezogullari and Mehmet S. Aktas},
  doi          = {10.1142/S0218194021500480},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1447-1468},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {On the use of generative deep learning approaches for generating hidden test scripts},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Author reputation measurement on question and answer sites
by the classification of author-generated content. <em>IJSEKE</em>,
<em>31</em>(10), 1421–1445. (<a
href="https://doi.org/10.1142/S0218194021500479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of software engineering, practitioners’ share in the constructed knowledge cannot be underestimated and is mostly in the form of grey literature (GL). GL is a valuable resource though it is subjective and lacks an objective quality assurance methodology. In this paper, a quality assessment scheme is proposed for question and answer (Q&amp;A) sites. In particular, we target stack overflow (SO) and stack exchange (SE) sites. We model the problem of author reputation measurement as a classification task on the author-provided answers. The authors’ mean, median, and total answer scores are used as inputs for class labeling. State-of-the-art language models (BERT and DistilBERT) with a softmax layer on top are utilized as classifiers and compared to SVM and random baselines. Our best model achieves 6 3 . 8 % accuracy in binary classification in SO design patterns tag and 7 1 . 6 % accuracy in SE software engineering category. Superior performance in SE software engineering can be explained by its larger dataset size. In addition to quantitative evaluation, we provide qualitative evidence, which supports that the system’s predicted reputation labels match the quality of provided answers.},
  archive      = {J_IJSEKE},
  author       = {Erhan Sezerer and Samet Tenekeci and Ali Acar and Bora Baloğlu and Selma Tekir},
  doi          = {10.1142/S0218194021500479},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1421-1445},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Author reputation measurement on question and answer sites by the classification of author-generated content},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding the working habits of GH-SO users on GitHub
commit activity and stack overflow post activity. <em>IJSEKE</em>,
<em>31</em>(10), 1399–1419. (<a
href="https://doi.org/10.1142/S0218194021500467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GitHub and Stack Overflow are often used together for software development. GH-SO users, who use both GitHub and Stack Overflow, contribute to the development of various software projects in GitHub and share their knowledge and experience on software development in Stack Overflow. To widely understand the interests and working habits of GH-SO users on software development, it is important to investigate how GH-SO users utilize GitHub and Stack Overflow. In this paper, we present an exploratory study on GitHub commit and Stack Overflow post activities of GH-SO users. Specifically, we investigate the working habits of GH-SO users on GitHub commit and Stack Overflow post activities. We randomly selected 19,756 of GH-SO users as our target sample and collected 2,819,483 and 2,147,317 of commit activity data and post activity data of the GH-SO users. We then categorized the collected commit and post activity datasets into specific categories on programming languages and statistically analyzed the categorized commit and post activity datasets. As the results of our analysis, we found the following: (1) The overall commit and post activities of the GH-SO users share some similarity. (2) The commit activities gradually change while the post activities drastically change over time. (3) The commit activities of the GH-SO users are broadly distributed while the post activities are narrowly distributed and the commit activity can be better predictor for post activity. (4) The commit activity of the GH-SO users tends to be performed prior post activity. We believe that our findings can contribute to finding the ways to better support commit and post activities of GitHub and Stack Overflow users.},
  archive      = {J_IJSEKE},
  author       = {Jungil Kim and Eunjoo Lee},
  doi          = {10.1142/S0218194021500467},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1399-1419},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Understanding the working habits of GH-SO users on GitHub commit activity and stack overflow post activity},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Methods on detecting closely related topics and spatial
events. <em>IJSEKE</em>, <em>31</em>(10), 1377–1398. (<a
href="https://doi.org/10.1142/S0218194021500455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic detection is a hot issue that many researchers are interested in. The previous researches focused on the single data stream, they did not consider the topic detection from different data streams in a harmonious way, so they cannot detect closely related topics from different data streams. Recently, Twitter, along with other SNS such as Weibo, and Yelp, began backing position services in their texts. Previous approaches are either complex to be conducted or oversimplified that cannot achieve better performance on detecting spatial topics. In our paper, we introduce a probabilistic method which can precisely detect closely related bursty topics and their bursty periods across different data streams in a unified way. We also introduce a probabilistic method called Latent Spatial Events Model (LSEM) that can find areas as well as to detect the spatial events, it can also predict positions of the texts. We evaluate LSEM on different datasets and reflect that our approach outperforms other baseline approaches in different indexes such as perplexity, entropy of topic and KL-divergence, range error. Evaluation of our first proposed approach on different datasets shows that it can detect closely related topics and meaningful bursty time periods from different datasets.},
  archive      = {J_IJSEKE},
  author       = {Zehao Yu},
  doi          = {10.1142/S0218194021500455},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1377-1398},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Methods on detecting closely related topics and spatial events},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metaheuristics and software engineering: Past, present, and
future. <em>IJSEKE</em>, <em>31</em>(9), 1349–1375. (<a
href="https://doi.org/10.1142/S0218194021500443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims at giving an updated vision on the successful combination between Metaheuristics and Software Engineering (SE). Mostly during the 90s, varied groups of researchers dealing with search, optimization, and learning (SOL) met SE researchers, all of them looking for a quantified manner of modeling and solving problems in the software field. This paper will discuss on the construction, assessment, and exploitation tasks that help in making software programs a scientific object, subject to automatic study and control. We also want to show with several case studies how the quantification of software features and the automatic search for bugs can improve the software quality process, which eases compliance to ISO/IEEE standards. In short, we want to build intelligent automatic tools that will upgrade the quality of software products and services. Since we approach this new field as a cross-fertilization between two research domains, we then need to talk not only on metaheuristics for SE (well known by now), but also on SE for metaheuristics (not so well known nowadays). In summary, we will discuss here with three time horizons in mind: the old times [before the term search-based SE (SBSE) was used for this], the recent years on SBSE, and the many avenues for future research/development. A new body of knowledge in SOL and SE exists internationally, which is resulting in a new class of researchers able of building intelligent techniques for the benefit of software, that is, of modern societies.},
  archive      = {J_IJSEKE},
  author       = {Enrique Alba and Javier Ferrer and Ignacio Villalobos},
  doi          = {10.1142/S0218194021500443},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1349-1375},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Metaheuristics and software engineering: Past, present, and future},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Python code smell refactoring route generation based on
association rule and correlation. <em>IJSEKE</em>, <em>31</em>(9),
1329–1347. (<a href="https://doi.org/10.1142/S0218194021500431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code smell is a software quality problem caused by software design flaws. Refactoring code smells can improve software maintainability. While prior works mostly focused on Java code smells, only a few prior researches detect and refactor code smells of Python. Therefore, we intend to outline a route (i.e. sequential refactoring operation) for refactoring Python code smells, including LC, LM, LMC, LPL, LSC, LBCL, LLF, MNC, CCC and LTCE. The route could instruct developers to save effort by refactoring the smell strongly correlated with other smells in advance. As a result, more smells could be resolved by a single refactoring. First, we reveal the co-occurrence and the inter-causation between smells. Then, we evaluate the smells’ correlation. Results highlight seven groups of smells with high co-occurrence. Meanwhile, 10 groups of smells correlate with each other in a significant level of Spearman’s correlation coefficient at 0.01. Finally, we generate the refactoring route based on the association rules, we exploit an empirical verification with 10 developers involved. The results of Kendall’s Tau show that the proposed refactoring route has a high inter-agreement with the developer’s perception. In conclusion, we propose four refactoring routes to provide guidance for practitioners, i.e. {LPL → LLF}, {LPL → LBCL}, {LPL → LMC} and {LPL → LM → LC → CCC → MNC}.},
  archive      = {J_IJSEKE},
  author       = {Guanglei Wang and Junhua Chen and Jianhua Gao and Zijie Huang},
  doi          = {10.1142/S0218194021500431},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1329-1347},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Python code smell refactoring route generation based on association rule and correlation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personalized API recommendations. <em>IJSEKE</em>,
<em>31</em>(9), 1299–1327. (<a
href="https://doi.org/10.1142/S021819402150042X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application Programming Interfaces (APIs) play an important role in modern software development. Developers interact with APIs on a daily basis and thus need to learn and memorize those APIs suitable for implementing the required functions. This can be a burden even for experienced developers since there exists a mass of available APIs. API recommendation techniques focus on assisting developers in selecting suitable APIs. However, existing API recommendation techniques have not taken the developers personal characteristics into account. As a result, they cannot provide developers with personalized API recommendation services. Meanwhile, they lack the support for self-defined APIs in the recommendation. To this end, we aim to propose a personalized API recommendation method that considers developers’ differences. Our API recommendation method is based on statistical language. We propose a model structure that combines the N -gram model and the long short-term memory (LSTM) neural network and train predictive models using API invoking sequences extracted from GitHub code repositories. A general language model trained on all sorts of code data is first acquired, based on which two personalized language models that recommend personalized library APIs and self-defined APIs are trained using the code data of the developer who needs personalized services. We evaluate our personalized API recommendation method on real-world developers, and the experimental results show that our approach achieves better accuracy in recommending both library APIs and self-defined APIs compared with the state-of-the-art. The experimental results also confirm the effectiveness of our hybrid model structure and the choice of the LSTM’s size.},
  archive      = {J_IJSEKE},
  author       = {Wenhua Yang and Yu Zhou and Zhiqiu Huang},
  doi          = {10.1142/S021819402150042X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1299-1327},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Personalized API recommendations},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel overlapping method to alleviate the cold-start
problem in recommendation systems. <em>IJSEKE</em>, <em>31</em>(9),
1277–1297. (<a href="https://doi.org/10.1142/S0218194021500418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems (RSs) are tools for interacting with large and complex information spaces. They provide a personalized view of such spaces, prioritizing items likely to be of interest to the user. The main objective of RSs is to tool up users with desired items that meet their preferences. A major problem in RSs is called: “cold-start”; it is a potential problem called so in computer-based information systems which comprises a degree of automated data modeling. Particularly, it concerns the issue in which the system cannot draw any inferences nor have it yet gathered sufficient information about users or items. Since RSs performance is substantially limited by cold-start users and cold-start items problems; this research study takes the route for a major aim to attenuate users’ cold-start problem. Still in the process of researching, sundry studies have been conducted to tackle this issue by using clustering techniques to group users according to their social relations, their ratings or both. However, a clustering technique disregards a variety of users’ tastes. In this case, the researcher has adopted the overlapping technique as a tool to deal with the clustering technique’s defects. The advantage of the overlapping technique excels over others by allowing users to belong to multi-clusters at the same time according to their behavior in the social network and ratings feedback. On that account, a novel overlapping method is presented and applied. This latter is executed by using the partitioning around medoids (PAM) algorithm to implement the clustering, which is achieved by means of exploiting social relations and confidence values. After acquiring users’ clusters, the average distances are computed in each cluster. Thereafter, a content comparison is made regarding the distances between every user and the computed distances of the clusters. If the comparison result is less than or equal to the average distance of a cluster, a new user is added to this cluster. The singular value decomposition plus (SVD + + ) method is then applied to every cluster to compute predictions values. The outcome is calculated by computing the average of mean absolute error (MAE) and root mean square error (RMSE) for every cluster. The model is tested by two real world datasets: Ciao and FilmTrust. Ultimately, findings have exhibited a great deal of insights on how the proposed model outperformed a number of the state-of-the-art studies in terms of prediction accuracy.},
  archive      = {J_IJSEKE},
  author       = {Ali M. Ahmed Al-Sabaawi and Hacer Karacan and Yusuf Erkan Yenice},
  doi          = {10.1142/S0218194021500418},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1277-1297},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A novel overlapping method to alleviate the cold-start problem in recommendation systems},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recommending relevant tutorial fragments for API-related
natural language questions. <em>IJSEKE</em>, <em>31</em>(9), 1251–1275.
(<a href="https://doi.org/10.1142/S0218194021500406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application Programming Interface (API) tutorial is an important API learning resource. To help developers learn APIs, an API tutorial is often split into a number of consecutive units that describe the same topic (i.e. tutorial fragment ). We regard a tutorial fragment explaining an API as a relevant fragment of the API. Automatically recommending relevant tutorial fragments can help developers learn how to use an API. However, existing approaches often employ supervised or unsupervised manner to recommend relevant fragments, which suffers from much manual annotation effort or inaccurate recommended results. Furthermore, these approaches only support developers to input exact API names. In practice, developers often do not know which APIs to use so that they are more likely to use natural language to describe API-related questions. In this paper, we propose a novel approach, called Tu torial Fra gment Rec ommendation (TuFraRec), to effectively recommend relevant tutorial fragments for API-related natural language questions, without much manual annotation effort. For an API tutorial, we split it into fragments and extract APIs from each fragment to build API-fragment pairs. Given a question, TuFraRec first generates several clarification APIs that are related to the question. We use clarification APIs and API-fragment pairs to construct candidate API-fragment pairs. Then, we design a semi-supervised metric learning (SML)-based model to find relevant API-fragment pairs from the candidate list, which can work well with a few labeled API-fragment pairs and a large number of unlabeled API-fragment pairs. In this way, the manual effort for labeling the relevance of API-fragment pairs can be reduced. Finally, we sort and recommend relevant API-fragment pairs based on the recommended strategy. We evaluate TuFraRec on 200 API-related natural language questions and two public tutorial datasets (Java and Android). The results demonstrate that on average TuFraRec improves NDCG@5 by 0.06 and 0.09, and improves Mean Reciprocal Rank (MRR) by 0.07 and 0.09 on two tutorial datasets as compared with the state-of-the-art approach.},
  archive      = {J_IJSEKE},
  author       = {Di Wu and Xiao-Yuan Jing and Haowen Chen and Xiaohui Kong and Jifeng Xuan},
  doi          = {10.1142/S0218194021500406},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1251-1275},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Recommending relevant tutorial fragments for API-related natural language questions},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MTKeras: An automated metamorphic testing platform.
<em>IJSEKE</em>, <em>31</em>(9), 1235–1249. (<a
href="https://doi.org/10.1142/S021819402150039X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an automated, domain-independent, metamorphic testing platform called MTKeras. In this paper, we report on an investigation demonstrating the effectiveness and usability of MTKeras through five case studies in the four domains of image classification, sentiment analysis, search engines and database management systems. We also report on the effectiveness of combining metamorphic relation (input) patterns in individual metamorphic relations, enhancing the failure-finding abilities of the individual relations. The results of our experiments support combining patterns, and the use of MTKeras. The research reported in this paper shows the applicability of metamorphic relation patterns, and introduces a practical tool for the research community.},
  archive      = {J_IJSEKE},
  author       = {Yelin Liu and Zhi Quan Zhou and Tsong Yueh Chen and Yang Liu and Dave Towey},
  doi          = {10.1142/S021819402150039X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1235-1249},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {MTKeras: An automated metamorphic testing platform},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual channel among task and contribution on OSS communities:
An empirical study. <em>IJSEKE</em>, <em>31</em>(8), 1213–1234. (<a
href="https://doi.org/10.1142/S0218194021500388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open Source Software (OSS) community has attracted a large number of distributed developers to work together, e.g. reporting and discussing issues as well as submitting and reviewing code. OSS developers create links among development units (e.g. issues and pull requests in GitHub), share their opinions and promote the resolution of development units. Although previous work has examined the role of links in recommending high-priority tasks and reducing resource waste, the understanding of the actual usage of links in practice is still limited. To address the research gap, we conduct an empirical study based on the 5W1H model and data mining from five popular OSS projects on GitHub. We find that links originating from a PR are more common than the other three types of links, and links are more frequently created in Documentation. We also find that average duration between development units’ create time in a link is half a year. We observed that link behaviors are very complex and the duration of link increases with the complexity of link structure. We also observe that the reasons of link are very different, especially in P–P and I–I. Finally, future works are discussed in conclusion.},
  archive      = {J_IJSEKE},
  author       = {Yu Zhang and Yue Yu and Tao Wang and Zhixing Li and Xiaochuan Wang},
  doi          = {10.1142/S0218194021500388},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1213-1234},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Dual channel among task and contribution on OSS communities: An empirical study},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on problem formulations and (meta)heuristic-based
solutions in automated assembly of parallel test forms. <em>IJSEKE</em>,
<em>31</em>(8), 1171–1212. (<a
href="https://doi.org/10.1142/S0218194021500376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel test forms are ubiquitous in the educational, aptitude, achievement, and licensure testing. The problem of automated assembling of parallel test forms has been extensively explored for almost 40 years. Many different mathematical models of the problem formulations and a plenty of different solutions have emerged over the last two decades, indicating that the problem has matured. However, its investigation is still challenging, especially today, when the importance of distance learning and remote knowledge testing is rapidly growing. The diversity of proposed approaches originated notably from the variety of scientific fields involved such as psychometrics, applied mathematics, operations research, and artificial intelligence. Majority of solutions of the problem are (meta)heuristics-based, since they consider the problem as a combinatorial optimization problem which is NP-hard. In this paper, a comprehensive review of this research field, referring to related works since 1985, is conducted. Problem formulations and solutions of the problem are separately classified. Possible avenues of future research are pointed out.},
  archive      = {J_IJSEKE},
  author       = {Miroslava M. Ignjatović and Dragan M. Bojić and Igor I. Tartalja},
  doi          = {10.1142/S0218194021500376},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1171-1212},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A survey on problem formulations and (Meta)Heuristic-based solutions in automated assembly of parallel test forms},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel effort measure method for effort-aware just-in-time
software defect prediction. <em>IJSEKE</em>, <em>31</em>(8), 1145–1169.
(<a href="https://doi.org/10.1142/S0218194021500364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just-in-time software defect prediction (JIT-SDP) is a fine-grained software defect prediction technology, which aims to identify the defective code changes in software systems. Effort-aware software defect prediction is a software defect prediction technology that takes into consideration the cost of code inspection, which can find more defective code changes in limited test resources. The traditional effort-aware defect prediction model mainly measures the effort based on the number of lines of code (LOC) and rarely considers additional factors. This paper proposes a novel effort measure method called Multi-Metric Joint Calculation (MMJC). When measuring the effort, MMJC takes into account not only LOC, but also the distribution of modified code across different files (Entropy), the number of developers that changed the files (NDEV) and the developer experience (EXP). In the simulation experiment, MMJC is combined with Linear Regression, Decision Tree, Random Forest, LightGBM, Support Vector Machine and Neural Network, respectively, to build the software defect prediction model. Several comparative experiments are conducted between the models based on MMJC and baseline models. The results show that indicators ACC and P opt of the models based on MMJC are improved by 35.3% and 15.9% on average in the three verification scenarios, respectively, compared with the baseline models.},
  archive      = {J_IJSEKE},
  author       = {Liqiong Chen and Shilong Song and Can Wang},
  doi          = {10.1142/S0218194021500364},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1145-1169},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A novel effort measure method for effort-aware just-in-time software defect prediction},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global sensitivity analysis for fuzzy RDF data.
<em>IJSEKE</em>, <em>31</em>(8), 1119–1144. (<a
href="https://doi.org/10.1142/S0218194021500352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The resource description framework (RDF) was adopted by the World Wide Web (W3C) as an essential semantic web standard and the RDF scheme. It accords the hard semantics in the description and wields the crisp metadata. However, it usually produces vague or ambiguous information. Consequently, fuzzy RDF helps deal with such special data by transforming the crisp values into a fuzzy set. A method for analyzing fuzzy RDF data is proposed in this paper. To this end, first, we decompose the RDF into fuzzy RDF variables. Second, we are designing a model for global sensitivity analysis based on the decomposition of fuzzy RDF. It figures out the ambiguities of fuzzy RDF data. The proposed global sensitivity analysis model provides the importance of fuzzy RDF data by considering the response function’s structure and reselects it to a certain degree. A practical tool for sensitivity analysis of fuzzy RDF data has also been implemented based on the proposed model.},
  archive      = {J_IJSEKE},
  author       = {Hatem Soliman and Izhar Ahmed Khan and Yasir Hussain},
  doi          = {10.1142/S0218194021500352},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1119-1144},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Global sensitivity analysis for fuzzy RDF data},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supporting requirements to code traceability creation by
code comments. <em>IJSEKE</em>, <em>31</em>(8), 1099–1118. (<a
href="https://doi.org/10.1142/S0218194021500340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Requirements-to-code tracing is an important and costly task that creates trace links from requirements to source code. These trace links help engineers reduce the time and complexity of software maintenance. Code comments play an important role in software maintenance tasks. However, few studies have focused intensively on the impact of code comments on requirements-to-code trace links creation. Different types of comments have different purposes, so how different types of code comments provide different improvements for requirements-to-code trace links creation? We focus on learning whether code comments and different types of comments can improve the quality of trace links creation. This paper presents a study to evaluate the contribution of code comments and different types of code comments to the creation of trace links. More specifically, this paper first experimentally evaluates the impact of code comments on requirements-to-code trace links creation, and then divides code comments into six categories to evaluate its impact on trace links creation. The results show that the precision increases by an average of 15% (based on the same recall) after adding code comments (even for different trace links creation techniques), and the type of Purpose comments contributes more to the tracing task than the other five. This empirical study provides evidence that code comments are effective in tracing links creation, and different types of code comments contribute differently. Purpose comments can be used to improve the accuracy of requirements-to-code trace links creation.},
  archive      = {J_IJSEKE},
  author       = {Guohua Shen and Haijuan Wang and Zhiqiu Huang and YaoShen Yu and Kai Chen},
  doi          = {10.1142/S0218194021500340},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1099-1118},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Supporting requirements to code traceability creation by code comments},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Construction and analysis of scientific and technological
personnel relational graph for group recognition. <em>IJSEKE</em>,
<em>31</em>(8), 1069–1098. (<a
href="https://doi.org/10.1142/S0218194021500339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to the in-depth management of science and technology is to model the behavior characteristics of scientific and technological personnel and then find groups by analyzing the diverse associations among them. Aiming at the analysis of team relationship among scientific and technological personnel, this paper proposed a method to recognize the group of scientific and technological personnel based on relational graph. The relationship model of scientific and technological personnel was designed, and based on this, the entity and relationship recognition and extraction are performed on the structured and unstructured source data to construct a relational graph. An improved frequent item mining algorithm based on Hadoop was proposed, which enabled getting the group of scientific and technological personnel by mining and analyzing the data in the relational graph. In this paper, the proposed method was experimented on both open and private datasets, and compared with several classical algorithms. The results showed that the method proposed in this paper has a significant improvement in execution efficiency.},
  archive      = {J_IJSEKE},
  author       = {Dongju Yang and Xiaojian Wang and Hanshuo Zhang},
  doi          = {10.1142/S0218194021500339},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1069-1098},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Construction and analysis of scientific and technological personnel relational graph for group recognition},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An extensible compiler for implementing software design
patterns as concise language constructs. <em>IJSEKE</em>,
<em>31</em>(7), 1043–1067. (<a
href="https://doi.org/10.1142/S0218194021500327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design patterns are generic solutions to common programming problems. Design patterns represent a typical example of design reuse. However, implementing design patterns can lead to several problems, such as programming overhead and traceability. Existing research introduced several approaches to alleviate the implementation issues of design patterns. Nevertheless, existing approaches pose different implementation restrictions and require programmers to be aware of how design patterns should be implemented. Such approaches make the source code more prone to faults and defects. In addition, existing design pattern implementation approaches limit programmers to apply specific scenarios of design patterns (e.g. class-level), while other approaches require scattering implementation code snippets throughout the program. Such restrictions negatively impact understanding, tracing, or reusing design patterns. In this paper, we propose a novel approach to support the implementation of software design patterns as an extensible Java compiler. Our approach allows developers to use concise, easy-to-use language constructs to apply design patterns in their code. In addition, our approach allows the application of design patterns in different scenarios. We illustrate our approach using three commonly used design patterns, namely Singleton, Observer and Decorator. We show, through illustrative examples, how our design pattern constructs can significantly simplify implementing design patterns in a flexible, reusable and traceable manner. Moreover, our design pattern constructs allow class-level and instance-level implementations of design patterns.},
  archive      = {J_IJSEKE},
  author       = {Taher Ahmed Ghaleb and Khalid Aljasser and Musab A. Alturki},
  doi          = {10.1142/S0218194021500327},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1043-1067},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An extensible compiler for implementing software design patterns as concise language constructs},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Verifiable model construction for business processes.
<em>IJSEKE</em>, <em>31</em>(7), 1017–1042. (<a
href="https://doi.org/10.1142/S0218194021500315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business process specified in Business Process Execution Language (BPEL), which integrates existing services to develop composite service for offering more complicated function, is error-prone. Verification and testing are necessary to ensure the correctness of business processes. SPIN, for which the input language is PROcess MEta-LAnguage (Promela), is one of the most popular tools for detecting software defects and can be used both in verification and testing. In this paper, an automatic approach is proposed to construct the verifiable model for BPEL-based business process with Promela language. Business process is translated to an intermediate two-level representation, in which eXtended Control Flow Graph (XCFG) describes the behavior of BPEL process in the first level and Web Service Description Models (WSDM) depict the interface information of composite service and partner services in the second level. With XCFG of BPEL process, XCFGs for partner services are generated to describe their behavior. Promela model is constructed by defining data types based on WSDM and defining channels, variables and processes based on XCFGs. The constructed Promela model is closed, containing not only the BPEL process but also its execution environment. Case study shows that the proposed approach is effective.},
  archive      = {J_IJSEKE},
  author       = {Shunhui Ji and Liming Hu and Yihan Cao and Pengcheng Zhang and Jerry Gao},
  doi          = {10.1142/S0218194021500315},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1017-1042},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Verifiable model construction for business processes},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Static privacy analysis by flow reconstruction of tainted
data. <em>IJSEKE</em>, <em>31</em>(7), 973–1016. (<a
href="https://doi.org/10.1142/S0218194021500303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software security vulnerabilities and leakages of private information are two of the main issues in modern software systems. Several different approaches, ranging from design techniques to run-time monitoring, have been applied to prevent, detect and isolate such vulnerabilities. Static taint analysis has been particularly successful in detecting injection vulnerabilities at compile time. However, its extension to detect leakages of sensitive data has been only partially investigated. In this paper, we introduce BackFlow, a backward flow reconstructor that, starting from the results of a generic taint analysis engine, reconstructs the flow of tainted data. If successful, BackFlow provides full information about the flow that such data (e.g. private information or user input) traversed inside the program before reaching a sensitive point (e.g. Internet communication or execution of an SQL query). Such information is needed to extend taint analysis to privacy analyses, since in such a scenario it is important to know which exact type of sensitive data flows to what type of communication channels. BackFlow has been implemented in Julia (an industrial static analyzer for Java, Android and .NET programs), and applied to WebGoat and different benchmarks to detect both injections and privacy issues. The experimental results prove that BackFlow is able to reconstruct the flow of tainted data for most of the true positives, it scales up to industrial applications, and it can be effectively applied to privacy analysis, such as the detection of sensitive data leaks or compliance with a data regulation.},
  archive      = {J_IJSEKE},
  author       = {Pietro Ferrara and Luca Olivieri and Fausto Spoto},
  doi          = {10.1142/S0218194021500303},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {973-1016},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Static privacy analysis by flow reconstruction of tainted data},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DHLBT: Efficient cross-modal hashing retrieval method based
on deep learning using large batch training. <em>IJSEKE</em>,
<em>31</em>(7), 949–971. (<a
href="https://doi.org/10.1142/S0218194021500297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing has attracted considerable attention as it can implement rapid cross-modal retrieval through mapping data of different modalities into a common Hamming space. With the development of deep learning, more and more cross-modal hashing methods based on deep learning are proposed. However, most of these methods use a small batch to train a model. The large batch training can get better gradients and can improve training efficiency. In this paper, we propose the DHLBT method, which uses the large batch training and introduces orthogonal regularization to improve the generalization ability of the DHLBT model. Moreover, we consider the discreteness of hash codes and add the distance between hash codes and features to the objective function. Extensive experiments on three benchmarks show that our method achieves better performance than several existing hashing methods.},
  archive      = {J_IJSEKE},
  author       = {Xuewang Zhang and Jinzhao Lin and Yin Zhou},
  doi          = {10.1142/S0218194021500297},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {949-971},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {DHLBT: Efficient cross-modal hashing retrieval method based on deep learning using large batch training},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DSubSign: Classification of instance-feature data using
discriminative subgraphs as class signatures. <em>IJSEKE</em>,
<em>31</em>(7), 917–947. (<a
href="https://doi.org/10.1142/S0218194021500285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications like customer identification from their peculiar purchase patterns require class-wise discriminative feature subsets called as class signatures for classification. If the classifiers like KNN, SVM, etc. which require to work with a complete feature set, are applied to such applications, then the entire feature set may introduce errors in the classification. Decision tree classifier generates class-wise prominent feature subsets and hence, can be employed for such applications. However, all of these classifiers fail to model the relationship between features present in vector data. Thus, we propose to model the features and their interrelationships as graphs. Graphs occur naturally in protein molecules, chemical compounds, etc. for which several graph classifiers exist. However, multivariate data do not exhibit the graphs naturally. Thus, the proposed work focuses on (1) modeling multivariate data as graphs and (2) obtaining class-wise prominent subgraph signatures which are then used to train classifiers like SVM for decision making. The proposed method dSubSign can also classify multivariate data with missing values without performing imputation or case deletion. The performance analysis of both real-world and synthetic datasets shows that the accuracy of dSubSign is either higher or comparable to other existing methods.},
  archive      = {J_IJSEKE},
  author       = {Parnika N. Paranjape and Meera M. Dhabu and Parag S. Deshpande},
  doi          = {10.1142/S0218194021500285},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {917-947},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {DSubSign: Classification of instance-feature data using discriminative subgraphs as class signatures},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised heterogeneous defect prediction with
open-source projects on GitHub. <em>IJSEKE</em>, <em>31</em>(6),
889–916. (<a href="https://doi.org/10.1142/S0218194021500273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneous defect prediction (HDP) technique can predict defects in a target company using heterogeneous metric data from external company, which has received substantial research attention. However, existing HDP methods assume that source data is labeled but labeling data is expensive. Semi-supervised defect prediction technique can perform defect prediction with few labeled data. In this paper, we investigate a new problem — semi-supervised HDP (SHDP). To solve this problem, we propose a new approach named cost-sensitive kernel semi-supervised correlation analysis (CKSCA) as a solution of SHDP problem. It introduces unified metric representation and canonical correlation analysis to make the data distributions of different company projects more similar. CKSCA also designs a cost-sensitive kernel semi-supervised discriminant analysis mechanism to utilize the limited labeled data and sufficient real-life unlabeled data from different companies. Besides we collect lots of open-source projects from GitHub website to construct a new large-scale unlabeled dataset called GITHUB dataset. It contains 26,407 modules and is greater than each public project dataset. It has been public online and can be extended continuously. Experiments on the GITHUB dataset and other public datasets indicate that unlabeled GITHUB data can help prediction model improve prediction performance, and CKSCA is effective and efficient for solving SHDP problem.},
  archive      = {J_IJSEKE},
  author       = {Ying Sun and Xiao-Yuan Jing and Fei Wu and Xiwei Dong and Yanfei Sun and Ruchuan Wang},
  doi          = {10.1142/S0218194021500273},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {889-916},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Semi-supervised heterogeneous defect prediction with open-source projects on GitHub},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On representing resilience requirements of microservice
architecture systems. <em>IJSEKE</em>, <em>31</em>(6), 863–888. (<a
href="https://doi.org/10.1142/S0218194021500261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Together with the spread of DevOps practices and container technologies, Microservice Architecture has become a mainstream architecture style in recent years. Resilience is a key characteristic in Microservice Architecture (MSA) Systems, and it shows the ability to cope with various kinds of system disturbances which cause degradations of services. However, due to lack of consensus definition of resilience in the software field, although a lot of work has been done on resilience for MSA Systems, developers still do not have a clear idea on how resilient an MSA System should be, and what resilience mechanisms are needed. In this paper, by referring to existing systematic studies on resilience in other scientific areas, the definition of microservice resilience is provided and a Microservice Resilience Measurement Model is proposed to measure service resilience. And a requirement model to represent resilience requirements of MSA Systems is given. The requirement model uses elements in KAOS to represent notions in the measurement model, and decompose service resilience goals into system behaviors that can be executed by system components. As a proof of concept, a case study is conducted on an MSA System to illustrate how the proposed models are applied.},
  archive      = {J_IJSEKE},
  author       = {Kanglin Yin and Qingfeng Du},
  doi          = {10.1142/S0218194021500261},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {863-888},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {On representing resilience requirements of microservice architecture systems},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Code clone detection with hierarchical attentive graph
embedding. <em>IJSEKE</em>, <em>31</em>(6), 837–861. (<a
href="https://doi.org/10.1142/S021819402150025X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code clone serves as a typical programming manner that reuses the existing code to solve similar programming problems, which greatly facilitates software development but recurs program bugs and maintenance costs. Recently, deep learning-based detection approaches gradually present their effectiveness on feature representation and detection performance. Among them, deep learning approaches based on abstract syntax tree (AST) construct models relying on the node embedding technique. In AST, the semantic of nodes is obviously hierarchical, and the importance of nodes is quite different to determine whether the two code fragments are cloned or not. However, some approaches do not fully consider the hierarchical structure information of source code. Some approaches ignore the different importance of nodes when generating the features of source code. Thirdly, when the tree is very large and deep, many approaches are vulnerable to the gradient vanishing problem during training. In order to properly address these challenges, we propose a hierarchical attentive graph neural network embedding model-HAG for the code clone detection. Firstly, the attention mechanism is applied on nodes in AST to distinguish the importance of different nodes during the model training. In addition, the HAG adopts graph convolutional network (GCN) to propagate the code message on AST graph and then exploits a hierarchical differential pooling GCN to sufficiently capture the code semantics at different structure level. To evaluate the effectiveness of HAG, we conducted extensive experiments on public clone dataset and compared it with seven state-of-the-art clone detection models. The experimental results demonstrate that the HAG achieves superior detection performance compared with baseline models. Especially, in the detection of moderately Type-3 or Type-4 clones, the HAG particularly outperforms baselines, indicating the strong detection capability of HAG for semantic clones. Apart from that, the impacts of the hierarchical pooling, attention mechanism and critical model parameters are systematically discussed.},
  archive      = {J_IJSEKE},
  author       = {Xiujuan Ji and Lei Liu and Jingwen Zhu},
  doi          = {10.1142/S021819402150025X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {837-861},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Code clone detection with hierarchical attentive graph embedding},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying the optimal refactoring dependencies using
heuristic search algorithms to maximize maintainability.
<em>IJSEKE</em>, <em>31</em>(6), 803–835. (<a
href="https://doi.org/10.1142/S0218194021500248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bad smells represent imperfection in the design of the software system and trigger the urge to refactor the source code. The quality of object-oriented software has always been a major concern for the developer team and refactoring techniques help them to focus on this aspect by transforming the code in a way such that the behavior of the software can be preserved. Rigorous research has been done in this field to improve the quality of the software using various techniques. But, one of the issues still remains unsettled, i.e. the overhead effort to refactor the code in order to yield the maximum maintainability value. In this paper, a quantitative evaluation method has been proposed to improve the maintainability value by identifying the most optimum refactoring dependencies in advance with the help of various meta-heuristic algorithms, including A * , AO * , Hill-Climbing and Greedy approaches. A comparison has been done between the maintainability values of the software used, before and after applying the proposed methodology. The results of this study show that the Greedy algorithm is the most promising algorithm amongst all the algorithms in determining the most optimum refactoring sequence resulting in 18.56% and 9.90% improvements in the maintainability values of jTDS and ArtOfIllusion projects, respectively. Further, this study would be beneficial for the software maintenance team as refactoring sequences will be available beforehand, thereby helping the team in maintaining the software with much ease to enhance the maintainability of the software. The proposed methodology will help the maintenance team to focus on a limited portion of the software due to prioritization of the classes, in turn helping them in completing their work within the budget and time constraints.},
  archive      = {J_IJSEKE},
  author       = {Anuradha Chug and Sandhya Tarwani},
  doi          = {10.1142/S0218194021500248},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {803-835},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Identifying the optimal refactoring dependencies using heuristic search algorithms to maximize maintainability},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep understanding of runtime configuration intention.
<em>IJSEKE</em>, <em>31</em>(6), 775–802. (<a
href="https://doi.org/10.1142/S0218194021500236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The runtime environment and workload of software are constantly changing, requiring users to make appropriate adjustments to accommodate these changes. The runtime configuration, however, as the interface for users to manipulate software behavior often requires domain-specific knowledge to understand. This usually results in users spending a considerable amount of time wading through document and user manuals trying to understand the runtime configuration. In this paper, we study the possibility of understanding the intention of runtime configuration options through their documents, even sometimes it is difficult for users to understand. Based on these studies, we classify the runtime configuration option’s intention into six categories. Accordingly, we design runtime Configuration Intention Classifier (CIC), a supervised approach based on CNN to classify the runtime configuration option’s intention according to its document. CIC integrates the features of runtime configuration names and descriptions according to different levels of granularity and predicts the intention of runtime configuration options accordingly. Extensive experiments show that our approach can achieve an accuracy of 85.6% and outperform nine comparative approaches by up to 16.6% over the dataset we customized.},
  archive      = {J_IJSEKE},
  author       = {Chenglong Zhou and Haoran Liu and Yuanliang Zhang and Zhipeng Xue and Qing Liao and JinJing Zhao and Ji Wang},
  doi          = {10.1142/S0218194021500236},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {775-802},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Deep understanding of runtime configuration intention},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reverse engineering of object oriented systems to ALF.
<em>IJSEKE</em>, <em>31</em>(5), 745–774. (<a
href="https://doi.org/10.1142/S0218194021500224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Model Driven Software Engineering (MDSE), Action Language for Foundational UML (ALF) is a new standard for specifying the structure and behavior of a system textually. To update/transform existing systems with respect to advance business needs and/or by the change in the dependent technology, this standard can play a vital role in reverse engineering a system for technology change. In this paper, using ALF, we propose a reverse engineering approach for transforming object oriented system. Our work is the first attempt to use ALF in reverse engineering. Using a case study (an ATM system) of significant size developed in C + + , we validate the feasibility of our approach. In this paper, to support our approach by a computer application, we created a tool CPP2ALF; this tool converts the C + + code to srcML code by using a third party srcML-tool and then generates the ALF code by using the generated srcML code.},
  archive      = {J_IJSEKE},
  author       = {Asad Nawaz and Tauseef Rana and Farooque Azam and Muhammad Waseem Anwar},
  doi          = {10.1142/S0218194021500224},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {745-774},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Reverse engineering of object oriented systems to ALF},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MuSim: Mutation-based fault localization using test case
proximity. <em>IJSEKE</em>, <em>31</em>(5), 725–744. (<a
href="https://doi.org/10.1142/S0218194021500212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault localization techniques aim to localize faulty statements using the information gathered from both passed and failed test cases. We present a mutation-based fault localization technique called MuSim. MuSim identifies the faulty statement based on its computed proximity to different mutants. We study the performance of MuSim by using four different similarity metrics. To satisfactorily measure the effectiveness of our proposed approach, we present a new evaluation metric called Mut_Score. Based on this metric, on an average, MuSim is 33.21% more effective than existing fault localization techniques such as DStar, Tarantula, Crosstab, Ochiai.},
  archive      = {J_IJSEKE},
  author       = {Arpita Dutta and Amit Jha and Rajib Mall},
  doi          = {10.1142/S0218194021500212},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {725-744},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {MuSim: Mutation-based fault localization using test case proximity},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handling quantity in variability models for
system-of-systems. <em>IJSEKE</em>, <em>31</em>(5), 693–724. (<a
href="https://doi.org/10.1142/S0218194021500200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem: Modern systems contain parts that are themselves systems. Such complex systems thus have sets of subsystems that have their own variability. These subsystems contribute to the functionality of a whole system-of-systems (SoS). Such systems have a very high degree of variability. Therefore, a modeling technique for the variability of an entire SoS is required to express two different levels of variability: variability of the SoS as a whole and variability of subsystems. If these levels are described together, the model becomes hard to understand. When the variability model of the SoS is described separately, each variability model is represented by a tree structure and these models are combined in a further tree structure. For each node in a variability model, a quantity is assigned to express the multiplicity of its instances per one instance of its parent node. Quantities of the whole system may refer to the number of subsystem instances in the system. From the viewpoint of the entire system, constraints and requirements written in natural language are often ambiguous regarding the quantities of subsystems. Such ambiguous constraints and requirements may lead to misunderstandings or conflicts in an SoS configuration. Approach: A separate notion is proposed for variability of an SoS; one model considers the SoS as an undivided entity, while the other considers it as a combination of subsystems. Moreover, a domain-specific notation is proposed to express relationships among the variability properties of systems, to solve the ambiguity of quantities and establish the total validity. This notation adapts an approach, named Pincer Movement, which can then be used to automatically deduce the quantities for the constraints and requirements. Validation: The descriptive capability of the proposed notation was validated with four examples of cloud providers. In addition, the proposed method and description tool were validated through a simple experiment on describing variability models with real practitioners.},
  archive      = {J_IJSEKE},
  author       = {Daisuke Shimbara and Motoshi Saeki and Shinpei Hayashi and Øystein Haugen},
  doi          = {10.1142/S0218194021500200},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {693-724},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Handling quantity in variability models for system-of-systems},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using an experimental framework to support variables
selection: An exploratory study. <em>IJSEKE</em>, <em>31</em>(5),
677–692. (<a href="https://doi.org/10.1142/S0218194021500194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of variables in a given experiment is crucial, since it is the theoretical foundation that guides how data should be collected and analyzed. However, selecting variables is an intricate activity, especially considering areas such as Software Engineering and Education, whose studies should also consider human-related variables in the design. In this scenario, we aim to investigate how a support mechanism helps in the variables selection activity of the experiment process. To do so, we conducted a preliminary study on the use of an experimental framework composed of a catalog of variables. We explored the domain of the integration of software testing into programming education. Participants were divided into two groups (ad hoc and framework support) and asked to select variables for a given experiment goal. We analyzed the results by identifying threats to validity in their experimental design drafts. Results show a significant number of threats of type inadequate explication of constructs for both groups. Nonetheless, the framework helped to increase the clarity of concepts selected as variables. The cause of most raised threats, even with the framework support, was an inaccuracy in selecting the values of such variables (i.e. treatments and fixed values).},
  archive      = {J_IJSEKE},
  author       = {Lilian Passos Scatalon and Rogério Eduardo Garcia and Ellen Francine Barbosa},
  doi          = {10.1142/S0218194021500194},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {677-692},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Using an experimental framework to support variables selection: An exploratory study},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VeRA: Verifying RBAC and authorization constraints models of
web applications. <em>IJSEKE</em>, <em>31</em>(5), 655–675. (<a
href="https://doi.org/10.1142/S0218194021500182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The software security issue is being paid great attention from the software development community as security violations have emerged variously. Developers often use access control techniques to restrict some security breaches to software systems’ resources. The addition of authorization constraints to the role-based access control model increases the ability to express access rules in real-world problems. However, the complexity of combining components, libraries and programming languages during the implementation stage of web systems’ access control policies may arise potential flaws that make applications’ access control policies inconsistent with their specifications. In this paper, we introduce an approach to review the implementation of these models in web applications written by Java EE according to the MVC architecture under the support of the Spring Security framework. The approach can help developers in detecting flaws in the assignment implementation process of the models. First, the approach focuses on extracting the information about users and roles from the database of the web application. We then analyze policy configuration files to establish the access analysis tree of the application. Next, algorithms are introduced to validate the correctness of the implemented user-role and role-permission assignments in the application system. Lastly, we developed a tool called VeRA , to automatically support the verification process. The tool is also experimented with a number of access violation scenarios in the medical record management system.},
  archive      = {J_IJSEKE},
  author       = {Thanh-Nhan Luong and Hanh-Phuc Nguyen and Ninh-Thuan Truong},
  doi          = {10.1142/S0218194021500182},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {655-675},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {VeRA: Verifying RBAC and authorization constraints models of web applications},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging SPARQL queries for UML consistency checking.
<em>IJSEKE</em>, <em>31</em>(4), 635–654. (<a
href="https://doi.org/10.1142/S0218194021500170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context and motivation : Multiple-viewed requirements modeling method describes the system to-be from different perspectives. Some requirements models are then specified in various UML diagrams. Question/problem : Managing those models can be tedious and error-prone, since a lot of CASE tools provide poor support for reasoning and consistency checking. Principal ideas/results : Ontology is a formal notation for describing concepts and their relations in a domain. Since software requirements are a kind of knowledge, we propose to adopt a knowledge engineering approach for managing the consistency of requirements models. In this paper, an ontology for three most commonly used UML diagrams is developed in Web Ontology Language (OWL). The transformation of UML class, sequence and state diagrams to OWL knowledge base is presented. Owing to the underlying logical reasoning capability of OWL, a semantic query language, SPARQL (SPARQL Protocol and RDF Query Language), is used to query the knowledge base for consistency checking. Contribution : This paper introduces a semantic web-based knowledge engineering approach to represent and manage software requirements knowledge in OWL. By experimenting with a concrete software system, we demonstrate the feasibility and applicability of this knowledge approach.},
  archive      = {J_IJSEKE},
  author       = {Bingyang Wei and Jing Sun},
  doi          = {10.1142/S0218194021500170},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {635-654},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Leveraging SPARQL queries for UML consistency checking},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memory-aware scheduling parallel real-time tasks for
multicore systems. <em>IJSEKE</em>, <em>31</em>(4), 613–634. (<a
href="https://doi.org/10.1142/S0218194021400106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared resources on the multicore chip, such as main memory, are increasingly becoming a point of contention. Traditional real-time task scheduling policies focus on solely on the CPU, and do not take in account memory access and cache effects. In this paper, we propose parallel real-time tasks scheduling (PRTTS) policy on multicore platforms. Each set of tasks is represented as a directed acyclic graph (DAG). The priorities of tasks are assigned according to task periods Rate Monotonic (RM). Each task is composed of three phases. The first phase is read memory stage, the second phase is execution phase and the third phase is write memory phase. The tasks use locks and critical sections to protect data access. The global scheduler maintains the task pool in which tasks are ready to be executed which can run on any core. PRTTS scheduling policy consists of two levels: the first level scheduling schedules ready real-time tasks in the task pool to cores, and the second level scheduling schedules real-time tasks on cores. Tasks can preempt the core on running tasks of low priority. The priorities of tasks which want to access memory are dynamically increased above all tasks that do not access memory. When the data accessed by a task is in the cache, the priority of the task is raised to the highest priority, and the task is scheduled immediately to preempt the core on running the task not accessing memory. After accessing memory, the priority of these tasks is restored to the original priority and these tasks are pended, the preempted task continues to run on the core. This paper analyzes the schedulability of PRTTS scheduling policy. We derive an upper-bound on the worst-case response-time for parallel real-time tasks. A series of extensive simulation experiments have been performed to evaluate the performance of proposed PRTTS scheduling policy. The results of simulation experiment show that PRTTS scheduling policy offers better performance in terms of core utilization and schedulability rate of tasks.},
  archive      = {J_IJSEKE},
  author       = {Zhenyang Lei and Xiangdong Lei and Jun Long},
  doi          = {10.1142/S0218194021400106},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {613-634},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Memory-aware scheduling parallel real-time tasks for multicore systems},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using API call sequences for IoT malware classification
based on convolutional neural networks. <em>IJSEKE</em>, <em>31</em>(4),
587–612. (<a href="https://doi.org/10.1142/S021819402140009X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) devices built on different processor architectures have increasingly become targets of adversarial attacks. In this paper, we propose an algorithm for the malware classification problem of the IoT domain to deal with the increasingly severe IoT security threats. Application executions are represented by sequences of consecutive API calls. The time series of data is analyzed and filtered based on the improved information gains. It performs more effectively than chi-square statistics, in reducing the sequence lengths of input data meanwhile keeping the important information, according to the experimental results. We use a multi-layer convolutional neural network to classify various types of malwares, which is suitable for processing time series data. When the convolution window slides down the time sequence, it can obtain higher-level positions by collecting different sequence features, thereby understanding the characteristics of the corresponding sequence position. By comparing the iterative efficiency of different optimization algorithms in the model, we select an algorithm that can approximate the optimal solution to a small number of iterations to speed up the convergence of the model training. The experimental results from real world IoT malware sample show that the classification accuracy of this approach can reach more than 98%. Overall, our method has demonstrated practical suitability for IoT malware classification with high accuracies and low computational overheads by undergoing a comprehensive evaluation.},
  archive      = {J_IJSEKE},
  author       = {Qianguang Lin and Ni Li and Qi Qi and Jiabin Hu},
  doi          = {10.1142/S021819402140009X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {587-612},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Using API call sequences for IoT malware classification based on convolutional neural networks},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Urban region function mining service based on social media
text analysis. <em>IJSEKE</em>, <em>31</em>(4), 563–586. (<a
href="https://doi.org/10.1142/S0218194021400088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban region functions are the types of potential activities in an urban region, such as residence, commerce, transportation, entertainment, etc. A service which mines urban region functions is of great value for various applications, including urban planning and transportation management, etc. Many studies have been carried out to dig out different regions’ functions, but few studies are based on social media text analysis. Considering that the semantic information embedded in social media texts is very useful to infer an urban region’s main functions, we design a service which extracts human activities using Sina Weibo ( www.weibo.com ; the largest microblog system in Chinese, similar to Twitter) with location information and further describes a region’s main functions with a function vector based on the human activities. First, we predefine a variety of human activities to get the related activities corresponding to each Weibo post using an urban function classification model. Second, urban regions’ function vectors are generated, with which we can easily do some high-level work such as similar place recommendation. At last, with the function vectors generated, we develop a Web application for urban region function querying. We also conduct a case study among the urban regions in Beijing, and the experiment results demonstrate the feasibility of our method.},
  archive      = {J_IJSEKE},
  author       = {Yanchun Sun and Hang Yin and Jiu Wen and Zhiyu Sun},
  doi          = {10.1142/S0218194021400088},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {563-586},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Urban region function mining service based on social media text analysis},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lecture information service based on multiple features
fusion. <em>IJSEKE</em>, <em>31</em>(4), 545–562. (<a
href="https://doi.org/10.1142/S0218194021400076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information service is always a hot topic especially when the Web is accessible anywhere. In university, lecture information is very important for students and teachers who want to take part in academic meetings. Therefore, lecture news extraction is an important and imperative task. Many open information extraction methods have been proposed, but due to the high heterogeneity of websites, this task is still a challenge. In this paper, we propose a method based on fusing multiple features to locate lecture news on the university website. These features include the linked relationship between parent webpage and child webpages, the visual similarity, and the semantics of webpages. Additionally, this paper provides an information service based on a main content extraction algorithm for extracting the lecture information. Stable and invariant features enable the proposed method to adapt to various kinds of campus websites. The experiments conducted on 50 websites show the effectiveness and efficiency of the provided service.},
  archive      = {J_IJSEKE},
  author       = {Zhongguo Yang and Mingzhu Zhang and Zhongmei Zhang and Han Li and Chen Liu and Sikandar Ali},
  doi          = {10.1142/S0218194021400076},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {545-562},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Lecture information service based on multiple features fusion},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective method of evaluating pension service quality
using multi-dimension attention convolutional neural networks.
<em>IJSEKE</em>, <em>31</em>(4), 533–543. (<a
href="https://doi.org/10.1142/S0218194021400064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to make an accurate evaluation of the quality of pension service has become the most important task. However, in the real world, many customs always forget to rate pension service. They only leave a few short, less semantic, and discontinuous review words below the service. This paper will propose an effective multi-dimension attention convolutional neural networks (MACNNs) model to analyze customer review texts and predict the pension service quality. In MACNN, the emoticon feature, sentiment feature, and word feature can be extracted together to construct feature space. And then attention layer and convolution layer work together to predict the service quality. Compared with the traditional machine learning methods and neural network methods, this method is more objective and accurate to reflect consumers’ real evaluation of pension service.},
  archive      = {J_IJSEKE},
  author       = {Chunshan Li and Yuanyuan Wang and Dongmei Li and Dianhui Chu and Mingxiao Ma},
  doi          = {10.1142/S0218194021400064},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {533-543},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An effective method of evaluating pension service quality using multi-dimension attention convolutional neural networks},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic discovery of composite GIS services.
<em>IJSEKE</em>, <em>31</em>(4), 507–531. (<a
href="https://doi.org/10.1142/S0218194021400052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an increasing number of Geographical Information System (GIS) services publicly available on the Web, the discovery of composite GIS services is promising when novel requirements are to be satisfied. GIS services in the repository like ArcGIS software are organized in a tree hierarchy, where a parent node represents a categorial GIS service with a coarser -granularity than its child GIS services, while leaf nodes correspond to atomic and exercisable GIS services. In this setting, discovering appropriate atomic GIS services is challenging. To remedy this issue, this paper proposes a composite GIS service discovery mechanism. Specifically, for the given requirement, select the parent nodes that take the given input parameters as input and remove their inactivated children. Use remaining children to build the network and repeat the previous operation until finding the services that contain the required output. Then record the semantic similarity degree, calculated by services functional description, in this network. By using the simulated annealing algorithm, a composite GIS services solution will be recommended from this semantic network. Evaluation results demonstrate that our approach could give more significant solution compared with the state-of-the-art techniques.},
  archive      = {J_IJSEKE},
  author       = {Jiaqi Zheng and Yongli Xing and Fei Lei and Jin Diao and Zhangbing Zhou},
  doi          = {10.1142/S0218194021400052},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {507-531},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Semantic discovery of composite GIS services},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editors’ introduction. <em>IJSEKE</em>,
<em>31</em>(4), 503–505. (<a
href="https://doi.org/10.1142/S0218194021020022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Zhongjie Wang and Zhuofeng Zhao and Guobing Zou},
  doi          = {10.1142/S0218194021020022},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {503-505},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editors’ introduction},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Search for compatible source code. <em>IJSEKE</em>,
<em>31</em>(3), 477–502. (<a
href="https://doi.org/10.1142/S0218194021500169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Third-party libraries always evolve and produce multiple versions. Lucene, for example, released ten new versions (from version 7.7.0 to 8.4.0) in 2019. These versions confuse the existing code search methods to retrieve the source code that is not compatible with local programming language. To solve this issue, we propose DCSE, a deep code search model based on evolving information (i.e. evolved code tokens and evolution description). DCSE first deeply excavates evolved code tokens and evolution description in the code evolution process; then it takes evolved code tokens and evolution description as one feature of source code and code description, respectively. With such fuller representation, DCSE embeds source code and its code description into a high-dimensional shared vector space, and makes the cosine distance of their vectors closer. For the ever-evolving third-party libraries like Lucene, the experimental results show that DCSE could retrieve the source code that is compatible with local programming language, it outperforms the state-of-the-art methods (e.g. CODEnn) by 56.9–60.9 % in RFVersion. For the rarely-evolving third-party libraries, DCSE outperforms the state-of-the-art methods (e.g. CODEnn) by 4–11 % in Precision.},
  archive      = {J_IJSEKE},
  author       = {Fuqi Cai and Changjing Wang and Qing Huang and Zhengkang Zuo and Yunyan Liao},
  doi          = {10.1142/S0218194021500169},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {477-502},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Search for compatible source code},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IMER-FM: Iterative process of system feature model
extraction from the requirements. <em>IJSEKE</em>, <em>31</em>(3),
435–475. (<a href="https://doi.org/10.1142/S0218194021500157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software product line engineering (SPLE) is a paradigm to promote systematic software reuse. A Feature Model (FM) is a common means to illustrate the commonality and variability of software products in a family. In most existing FM extraction approaches, keywords in the requirement document or certain types of system behavior or external events are considered features. The resulting FM is a combination of user activities and system actions (SAs), making it hard to understand. In this paper, we present an automatic approach to generate a product line FM from multiple requirement documents. We consider user activity and SAs separately in our approach and focus on the expected behaviors of the software system, together with the data being processed. The resulting FM clearly illustrates the expected functionalities of the software system and their variability in the product line. We also compared our approach with existing techniques by processing the same textual documents, and noted improvements in our results.},
  archive      = {J_IJSEKE},
  author       = {Muhammad Javed and Yuqing Lin},
  doi          = {10.1142/S0218194021500157},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {435-475},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {IMER-FM: Iterative process of system feature model extraction from the requirements},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applying class distance to decide similarity on information
models for automated data interoperability. <em>IJSEKE</em>,
<em>31</em>(3), 405–434. (<a
href="https://doi.org/10.1142/S0218194021500145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the world of the Internet of Things (IoT), heterogeneous systems and devices need to be connected and exchange data with others. How data exchange can be automatically realized becomes a critical issue. An information model (IM) is frequently adopted and utilized to solve the data interoperability problem. Meanwhile, as IoT systems and devices can have different IMs with different modeling methodologies and formats such as UML, IEC 61360, etc., automated data interoperability based on various IMs is recognized as an urgent problem. In this paper, we propose an approach to automate the data interoperability, i.e. data exchange among similar entities in different IMs. First, similarity scores among entities are calculated based on their syntactic and semantic features. Then, in order to precisely get similar candidates to exchange data, a concept of class distance calculated with a Virtual Distance Graph (VDG) is proposed to narrow down obtained similar properties for data exchange. Through analyzing the results of a case study, the class distance based on VDG can effectively improve the precisions of calculated similar properties. Furthermore, data exchange rules can be generated automatically. The results reveal that the approach of this research can efficiently contribute to resolving the data interoperability problem.},
  archive      = {J_IJSEKE},
  author       = {Lan Wang and Shinpei Hayashi and Motoshi Saeki},
  doi          = {10.1142/S0218194021500145},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {405-434},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Applying class distance to decide similarity on information models for automated data interoperability},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unifying coalgebraic semantics framework for quantum
systems. <em>IJSEKE</em>, <em>31</em>(3), 381–403. (<a
href="https://doi.org/10.1142/S0218194021500133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a quantum counterpart of labeled transition system (LTS), quantum labeled transition system (QLTS) is a powerful formalism for modeling quantum programs or protocols, and gives a categorical understanding for quantum computation. With the help of quantum branching monad, QLTS provides a framework extending some ideas in non-deterministic or probabilistic systems to quantum systems. On the other hand, quantum finite automata (QFA) emerged as a very elegant and simple model for resolving some quantum computational problems. In this paper, we propose the notion of reactive quantum system (RQS), a variant of QLTS capturing reactive system behavior, and develop a coalgebraic semantics for QLTS, RQS and QFA by an endofunctor on the category of convex sets, which has a final coalgebra. Such a coalgebraic semantics provides a unifying abstract interpretation for QLTS, RQS and QFA. The notions of bisimulation and simulation can be employed to compare the behavior of different types of quantum systems and judge whether a coalgebra can be behaviorally simulated by another.},
  archive      = {J_IJSEKE},
  author       = {Ai Liu and Meng Sun},
  doi          = {10.1142/S0218194021500133},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {381-403},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A unifying coalgebraic semantics framework for quantum systems},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel execution of programs as a support for mutation
testing: A replication study. <em>IJSEKE</em>, <em>31</em>(3), 337–380.
(<a href="https://doi.org/10.1142/S0218194021500121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutation testing is well known as one of the most effective approaches to create test cases, which can detect software faults. However, its drawback is the low scalability — if no special attention is given to improve efficiency — that directly affects its application in practice. This paper shows a replication study focused on emphasizing evidence in which the use of distributed processing structures can improve mutation testing. For this purpose, an architecture that enables mutation testing concurrent execution was designed. Five load balancing algorithms responsible for controlling the distribution and execution of data while carrying out mutation testing were evaluated. Experiments were conducted in order to evaluate the scalability and performance of the architecture considering homogeneous and heterogeneous setups. A time reduction of 50% was observed when executing mutants in parallel in relation to the conventional sequential application of mutation testing. The performance gain was above 95% when there was a higher number of nodes in the distributed architecture.},
  archive      = {J_IJSEKE},
  author       = {Márcio E. Delamaro and Stevão A. Andrade and Simone R. S. de Souza and Paulo S. L. de Souza},
  doi          = {10.1142/S0218194021500121},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {337-380},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Parallel execution of programs as a support for mutation testing: A replication study},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigating different metrics for evaluation and selection
of mutation operators for java. <em>IJSEKE</em>, <em>31</em>(3),
311–336. (<a href="https://doi.org/10.1142/S021819402150011X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutation testing is a successful and powerful technique, specifically designed for injecting the artificial faults. Although it is effective at revealing the faults, test suite assessment and its reduction, however, suffer from the expense of executing a large number of mutants. The researchers have proposed different types of cost reduction techniques in the literature. These techniques highly depend on the inspection of mutation operators. Several metrics have been evolved for the same. The selective mutation technique is most frequently used by the researchers. In this paper, the authors investigate different metrics for evaluating the traditional mutation operators for Java. Results on 13 Java programs indicate how grouping few operators can impact the effectiveness of an adequate and minimal test suite, and how this could provide several cost benefits.},
  archive      = {J_IJSEKE},
  author       = {Shweta Rani and Bharti Suri},
  doi          = {10.1142/S021819402150011X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {311-336},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Investigating different metrics for evaluation and selection of mutation operators for java},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DEJIT: A differential evolution algorithm for effort-aware
just-in-time software defect prediction. <em>IJSEKE</em>,
<em>31</em>(3), 289–310. (<a
href="https://doi.org/10.1142/S0218194021500108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction is an effective approach to save testing resources and improve software quality, which is widely studied in the field of software engineering. The effort-aware just-in-time software defect prediction (JIT-SDP) aims to identify defective software changes in limited software testing resources. Although many methods have been proposed to solve the JIT-SDP, the effort-aware prediction performance of the existing models still needs to be further improved. To this end, we propose a differential evolution (DE) based supervised method DEJIT to build JIT-SDP models. Specifically, first we propose a metric called density-percentile-average (DPA), which is used as optimization objective on the training set. Then, we use logistic regression (LR) to build a prediction model. To make the LR obtain the maximum DPA on the training set, we use the DE algorithm to determine the coefficients of the LR. The experiment uses defect data sets from six open source projects. We compare the proposed method with state-of-the-art four supervised models and four unsupervised models in cross-validation, cross-project-validation and timewise-cross-validation scenarios. The empirical results demonstrate that the DEJIT method can significantly improve the effort-aware prediction performance in the three evaluation scenarios. Therefore, the DEJIT method is promising for the effort-aware JIT-SDP.},
  archive      = {J_IJSEKE},
  author       = {Xingguang Yang and Huiqun Yu and Guisheng Fan and Kang Yang},
  doi          = {10.1142/S0218194021500108},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {289-310},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {DEJIT: A differential evolution algorithm for effort-aware just-in-time software defect prediction},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Refactoring cost estimation for architectural technical
debt. <em>IJSEKE</em>, <em>31</em>(2), 269–288. (<a
href="https://doi.org/10.1142/S021819402150008X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paying-off the Architectural Technical Debt by refactoring the flawed code is important to control the debt and to keep it as low as possible. Project Managers tend to delay paying off this debt because they face difficulties in comparing the cost of the refactoring against the benefits gained. These managers need to estimate the cost and the efforts required to conduct these refactoring activities as well as to decide which flaws have higher priority to be refactored. Our research is based on a dataset used by other researchers that study the technical debt. It includes more than 18,000 refactoring operations performed on 33 apache java projects. We applied the COCOMO II:2000 model to calculate the refactoring cost in person-months units per release. Furthermore, we investigated the correlation between the refactoring efforts and two static code metrics of the refactored code. The research revealed a weak correlation between the refactoring efforts and the size of the project, and a moderate correlation with the code complexity. Finally, we applied the DesigniteJava tool to verify our research results. From the analysis we found a significant correlation between the ranking of the architecture smells and the ranking of refactoring efforts for each package. Using machine learning practices, we took the architecture smells level and the code metrics of each release as an input to predict the levels of the refactoring effort of the next release. We calculated the results using our model and found that we can predict the ‘High’ and ‘Very High’ levels, the most significant levels from managers’ perspective, with 9 3 % accuracy.},
  archive      = {J_IJSEKE},
  author       = {Samir Deeb and Mrwan BenIdris and Hany Ammar and Dale Dzielski},
  doi          = {10.1142/S021819402150008X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {269-288},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Refactoring cost estimation for architectural technical debt},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Positive influence maximization in the signed social
networks considering polarity relationship and propagation probability.
<em>IJSEKE</em>, <em>31</em>(2), 249–267. (<a
href="https://doi.org/10.1142/S0218194021500078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of influence maximization problem is to select a small seed set to maximize the number of nodes influenced by the seed set. For viral marketing, the problem of influence maximization plays a vital role. Current works mainly focus on the unsigned social networks, which include only positive relationship between users. However, the influence maximization in the signed social networks including positive and negative relationships between users is still a challenging issue. Moreover, the existing works pay more attention to the positive influence. Therefore, this paper first analyzes the positive maximization influence in the signed social networks. The purpose of this problem is to select the seed set with the most positive influence in the signed social networks. Afterwards, this paper proposes a model that incorporates the state of node, the preference of individual and polarity relationship, called Independent Cascade with the Negative and Polarity (ICWNP) propagation model. On the basis of the ICWNP model, this paper proposes a Greedy with ICWNP algorithm. Finally, on four real social networks, experimental results manifest that the proposed algorithm has higher accuracy and efficiency than the related methods.},
  archive      = {J_IJSEKE},
  author       = {Liqing Qiu and Shuang Zhang and Jinfeng Yu},
  doi          = {10.1142/S0218194021500078},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {249-267},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Positive influence maximization in the signed social networks considering polarity relationship and propagation probability},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Refactoring legacy software for layer separation.
<em>IJSEKE</em>, <em>31</em>(2), 217–247. (<a
href="https://doi.org/10.1142/S0218194021500066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main aims in the layered software architecture is to divide the code into different layers so that each layer contains related modules and serves its upper layers. Although layered software architecture is matured now; many legacy information systems do not benefit from the advantages of this architecture and their code for the process/business and data access are mostly in a single layer. In many legacy systems, due to the integration of the code in one layer, changes to the software and its maintenance are mostly difficult. In addition, the big size of a single layer causes the load concentration and turns the server into a bottleneck where all requests must be executed on it. In order to eliminate these deficiencies, this paper presents a refactoring mechanism for the automatic separation of the business and data access layers by detecting the data access code based on a series of patterns in the input code and transferring it to a new layer. For this purpose, we introduce a code scanner which detects the target points based on these patterns and hence automatically makes the changes required for the layered architecture. According to the experimental evaluation results, the performance of the system is increased for the layer separated software using the proposed approach. Furthermore, it is examined that the application of the proposed approach provides additional benefits considering the qualitative criteria such as loosely coupling and tightly coherency.},
  archive      = {J_IJSEKE},
  author       = {Alireza Khalilipour and Moharram Challenger and Mehmet Onat and Hale Gezgen and Geylani Kardas},
  doi          = {10.1142/S0218194021500066},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {217-247},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Refactoring legacy software for layer separation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting software defects for object-oriented software
using search-based techniques. <em>IJSEKE</em>, <em>31</em>(2), 193–215.
(<a href="https://doi.org/10.1142/S0218194021500054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Development without any defect is unsubstantial. Timely detection of software defects favors the proper resource utilization saving time, effort and money. With the increasing size and complexity of software, demand for accurate and efficient prediction models is increasing. Recently, search-based techniques (SBTs) have fascinated many researchers for Software Defect Prediction (SDP). The goal of this study is to conduct an empirical evaluation to assess the applicability of SBTs for predicting software defects in object-oriented (OO) softwares. In this study, 16 SBTs are exploited to build defect prediction models for 13 OO software projects. Stable performance measures — GMean, Balance and Receiver Operating Characteristic-Area Under Curve (ROC-AUC) are employed to probe into the predictive capability of developed models, taking into consideration the imbalanced nature of software datasets. Proper measures are taken to handle the stochastic behavior of SBTs. The significance of results is statistically validated using the Friedman test complied with Wilcoxon post hoc analysis. The results confirm that software defects can be detected in the early phases of software development with help of SBTs. This paper identifies the effective subset of SBTs that will aid software practitioners to timely detect the probable software defects, therefore, saving resources and bringing up good quality softwares. Eight SBTs — sUpervised Classification System (UCS), Bioinformatics-oriented hierarchical evolutionary learning (BIOHEL), CHC, Genetic Algorithm-based Classifier System with Adaptive Discretization Intervals (GA_ADI), Genetic Algorithm-based Classifier System with Intervalar Rule (GA_INT), Memetic Pittsburgh Learning Classifier System (MPLCS), Population-Based Incremental Learning (PBIL) and Steady-State Genetic Algorithm for Instance Selection (SGA) are found to be statistically good defect predictors.},
  archive      = {J_IJSEKE},
  author       = {Ruchika Malhotra and Juhi Jain},
  doi          = {10.1142/S0218194021500054},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {193-215},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Predicting software defects for object-oriented software using search-based techniques},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated visual testing of application user interfaces
using static analysis of screenshots. <em>IJSEKE</em>, <em>31</em>(2),
167–191. (<a href="https://doi.org/10.1142/S0218194021500042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile and web applications must operate and be displayed correctly on many different devices and browsers. The visual testing of web or mobile applications is usually a manual process that requires a significant amount of testing time, meaning that applications are tested only on a few devices. It is then assumed that the applications will be displayed correctly on other compatible or similar devices. This paper presents an automated visual testing method for user interfaces. The main contributions of this paper are a classification scheme for visual defects of user interfaces and the definition of an automatic visual testing method that tests applications on many different devices with varying hardware and software parameters. The method is based on an automated search for defects using heuristic and expected state prediction algorithms, which involves analyzing the resources used by applications and screenshots. The testing method works by executing applications on a full set of devices, taking a screenshot at every execution step, and analyzing each of these screenshots. The manual as well as automated testing approaches were validated on 781 of Android applications. The experimental results show that the proposed method has advantages over manual testing.},
  archive      = {J_IJSEKE},
  author       = {Šarūnas Packevičius and Greta Rudžionienė and Eduardas Bareiša},
  doi          = {10.1142/S0218194021500042},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {167-191},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automated visual testing of application user interfaces using static analysis of screenshots},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting of open source software component reusability
level using object-oriented metrics by taguchi approach.
<em>IJSEKE</em>, <em>31</em>(2), 147–166. (<a
href="https://doi.org/10.1142/S0218194021500030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Component-based software development (CBSD) is an efficient approach used by software developers to develop new software. The commercial off the shelf (COTS) and open-source software (OSS) are two styles to implement CBSD. The COTS provides the interface and depicts the black-box behavior, but does not support several software quality characteristics. On the other hard, OSS is a more efficient approach compared to COTS due to its source code availability. This research aims to identify the reusability level of OSS components from an online repository of OSS. The OSS components are classified based on Chidamber and Kemerer reusability metrics (CK-metrics). This paper proposed a mathematical model to establish the relationship between the reusability of CK-metrics. Reusability level of OSS component has been measured and most effective CK-metrics obtained by applying the Taguchi design and analysis of variance (ANOVA). The input parameters for the experimental design are evaluated based on the OSS repository. Performance analysis has been carried out based upon the interaction effect between the reusability of CK-metrics. Main effect plots are created to identify the most reusable component of the OSS. The genetic algorithm (GA) is used to predict the optimized value of the different control parameters. The results indicate that the OSS component reusability level is 0.698194. The reusability of software has a significant effect on the quality of software. The quality of software can be improved by increasing the reusability of software components.},
  archive      = {J_IJSEKE},
  author       = {G. L. Saini and Deepak Panwar and Sandeep Kumar and Vijander Singh and Ramesh Chandra Poonia},
  doi          = {10.1142/S0218194021500030},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {147-166},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Predicting of open source software component reusability level using object-oriented metrics by taguchi approach},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven persona retrospective based on persona
significance index in b-to-b software development. <em>IJSEKE</em>,
<em>31</em>(1), 117–146. (<a
href="https://doi.org/10.1142/S0218194021500029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business-to-Business (B-to-B) software development companies develop services to satisfy their customers’ requirements. Developers should prioritize customer satisfaction because customers greatly influence agile software development. However, satisfying current customer’s requirements may not fulfill actual users or future customers’ requirements because customers’ requirements are not always derived from actual users. To reconcile these differences, developers should identify conflicts in their strategic plan. This plan should consider current commitments to end users and their intentions as well as employ a data-driven approach to adapt to rapid market changes. A persona models an end user representation in human-centered design. Although previous works have applied personas to software development and proposed data-driven software engineering frameworks with gap analysis between the effectiveness of commitments and expectations, the significance of developers’ commitment and quantitative decision-making are not considered. Developers often do not achieve their business goal due to conflicts. Hence, the target of commitments should be validated. To address these issues, we propose Data-Driven Persona Retrospective (DDR) to help developers plan future releases. DDR, which includes the Persona Significance Index (PerSI) to reflect developers’ commitments to end users’ personas, helps developers identify a gap between developers’ commitments to personas and expectations. In addition, DDR identifies release situations with conflicts based on PerSI. Specifically, we define four release cases, which include different situations and issues, and provide a method to determine the release case based on PerSI. Then we validate the release cases and their determinations through a case study involving a Japanese cloud application and discuss the effectiveness of DDR.},
  archive      = {J_IJSEKE},
  author       = {Yasuhiro Watanabe and Hironori Washizaki and Yoshiaki Fukazawa and Kiyoshi Honda and Masahiro Taga and Akira Matsuzaki and Takayoshi Suzuki},
  doi          = {10.1142/S0218194021500029},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {117-146},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Data-driven persona retrospective based on persona significance index in B-to-B software development},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The technical debt density over multiple releases and the
refactoring story. <em>IJSEKE</em>, <em>31</em>(1), 99–116. (<a
href="https://doi.org/10.1142/S0218194021500017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Do developers postpone fixing Technical Debt (TD) in software systems? TD is a metaphor that refers to short-term decisions in software development that may affect the cost of the software development life cycle. The bad smell is an imperfect solution in the software system that negatively impacts the internal software quality and maintainability. In this paper, we will study five open-source software projects (OSSPs) that have several releases and also estimate the numbers of architecture smells (ASs), design smells (DSs), and code smells (CSs) for every release. Designite will be used to detect smells. We describe a case study conducted to explore the following: (1) What is the average smells density for architecture, design, and code smells in an OSSP? (2) Does the density of each smell type increase over multiple releases? (3) What percentage of each smell-type density is eliminated by refactoring? We collected around 2 million LOC from five OSSPs that have multiple releases from the GitHub repository to statistically analyze the software concerning the smells as indicators of TD. We find 36% of Architecture Technical Debt (ATD) is Cyclic Dependency, while 33% of Design Debt (DD) is Cyclically-dependent Modularization. More than 70% of Code Debt (CD) is Magic Number. Even though the developers do refactoring between releases, the TD density in general increases. On average, by refactoring, developers remove around 48%, 16%, and 22% from the introduced ATD, DD, and CD from their next release, respectively.},
  archive      = {J_IJSEKE},
  author       = {Mrwan BenIdris and Hany Ammar and Dale Dzielski},
  doi          = {10.1142/S0218194021500017},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {99-116},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {The technical debt density over multiple releases and the refactoring story},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constructing bug knowledge graph as a service for bug
search. <em>IJSEKE</em>, <em>31</em>(1), 81–98. (<a
href="https://doi.org/10.1142/S0218194021400040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When encountering bug issues, developers tend to search the bug repository and commit repository for references. However, the links between bug reports and commits in version control systems are often missed, and the information in bug repository and commit repository can provide is simple. When developers search a bug issue, they can only get the information of bug reports or commits, which are loose and difficult for developers to refer. What’s more, many searching results are not accurate. To deal with these problems, this paper proposes an approach to deal with the bug and commit information with the topic model, and construct bug knowledge graph as a service to assist in bug search. In addition, as the amount of bug related information continuously increase, it is time-consuming to update the data. We can automatically update the bug knowledge graph with the LTM topic model (a lifelong topic model). Finally, the experiment with the bug reports from Bugzilla@Mozilla and the corresponding commits from Github was conducted. The experiment results show that our approach is effective and efficient to help developers search relevant bugs for reference by constructing the bug knowledge as a service.},
  archive      = {J_IJSEKE},
  author       = {Ying Chen},
  doi          = {10.1142/S0218194021400040},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {81-98},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Constructing bug knowledge graph as a service for bug search},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved entity linking for simple question answering over
knowledge graph. <em>IJSEKE</em>, <em>31</em>(1), 55–80. (<a
href="https://doi.org/10.1142/S0218194021400039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question Answering systems over Knowledge Graphs (KG) answer natural language questions using facts contained in a knowledge graph, and Simple Question Answering over Knowledge Graphs (KG-SimpleQA) means that the question can be answered by a single fact. Entity linking, which is a core component of KG-SimpleQA, detects the entities mentioned in questions, and links them to the actual entity in KG. However, traditional methods ignore some information of entities, especially entity types, which leads to the emergence of entity ambiguity problem. Besides, entity linking suffers from out-of-vocabulary (OOV) problem due to the limitation of pre-trained word embeddings. To address these problems, we encode questions in a novel way and encode the features contained in the entities in a multilevel way. To evaluate the enhancement of the whole KG-SimpleQA brought by our improved entity linking, we utilize a relatively simple approach for relation prediction. Besides, to reduce the impact of losing the feature during the encoding procedure, we utilize a ranking algorithm to re-rank (entity, relation) pairs. According to the experimental results, our method for entity linking achieves an accuracy of 81.8% that beats the state-of-the-art methods, and our improved entity linking brings a boost of 5.6% for the whole KG-SimpleQA.},
  archive      = {J_IJSEKE},
  author       = {Kai Chen and Guohua Shen and Zhiqiu Huang and Haijuan Wang},
  doi          = {10.1142/S0218194021400039},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {55-80},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Improved entity linking for simple question answering over knowledge graph},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general framework to detect design patterns by combining
static and dynamic analysis techniques. <em>IJSEKE</em>, <em>31</em>(1),
21–54. (<a href="https://doi.org/10.1142/S0218194021400027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design pattern detection can provide useful insights to support software comprehension. Accurate and complete detection of pattern instances are extremely important to enable software usability improvements. However, existing design pattern detection approaches and tools suffer from the following problems: incomplete description of design pattern instances, inaccurate behavioral constraint checking, and inability to support novel design patterns. This paper presents a general framework to detect design patterns while solving these issues by combining static and dynamic analysis techniques. The framework has been instantiated for typical behavioral and creational patterns, such as the observer pattern, state pattern, strategy pattern, and singleton pattern to demonstrate the applicability. Based on the open-source process mining toolkit ProM, we have developed an integrated tool that supports the whole detection process for these patterns. We applied and evaluated the framework using software execution data containing around 1,000,000 method calls generated from eight synthetic software systems and three open-source software systems. The evaluation results show that our approach can guarantee a higher precision and recall than existing approaches and can distinguish state and strategy patterns that are indistinguishable by the state-of-the-art.},
  archive      = {J_IJSEKE},
  author       = {Cong Liu},
  doi          = {10.1142/S0218194021400027},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {21-54},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A general framework to detect design patterns by combining static and dynamic analysis techniques},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trusted service provider discovery based on data,
information, knowledge, and wisdom. <em>IJSEKE</em>, <em>31</em>(1),
3–19. (<a href="https://doi.org/10.1142/S0218194021400015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data, information, knowledge, and wisdom forms a progressive relationship. Information is formed by data collation. Knowledge is filtered, refined, and processed from relevant information. Wisdom is based on knowledge and is accumulated through experience. This paper uses the progressive relationship of service data, information, knowledge, and wisdom to explain the expression of service knowledge graph. It is an increasingly challenging demand to discover trusted Cloud service providers with service data, information, and knowledge. We propose an efficient method of trusted service provider discovery based on service knowledge graphs, called PDG (Provider Discovery based on Graphs), to ensure that each service instance of composite services in Cloud systems is trustworthy. PDG evaluates the outputs of service providers in service classes with the help of additional service information. According to the additional service information, service knowledge is generated and trusted service providers can be found easily. PDG improves the accuracy of processing results by automatically replacing data provided by untrusted service providers with results provided by trusted service providers.},
  archive      = {J_IJSEKE},
  author       = {Yu Lei and Duan Yucong},
  doi          = {10.1142/S0218194021400015},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {3-19},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Trusted service provider discovery based on data, information, knowledge, and wisdom},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editor’s introduction. <em>IJSEKE</em>,
<em>31</em>(1), 1–2. (<a
href="https://doi.org/10.1142/S0218194021020010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Yucong Duan},
  doi          = {10.1142/S0218194021020010},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editor’s introduction},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
